{"context": "\n\n\u6982\u8981\n\u307f\u3093\u306a\u5927\u597d\u304dJupyter notebook(python)\u4e0a\u3067\u3001Pyspark/Cython\u3092\u4f7f\u3063\u3066\u3044\u308d\u3093\u306a\u3053\u3068\u3092\u3084\u308b\u3002\u3068\u304b\u3044\u3046\u8a18\u4e8b\u3092\u66f8\u3053\u3046\u3068\u601d\u3063\u305f\u3051\u3069\u30011\u8a18\u4e8b\u306b\u8a70\u3081\u8fbc\u307f\u3059\u304e\u3066\u3082\u919c\u3044\u3057\u3001\u6642\u9593\u304b\u304b\u3063\u3066\u66f8\u304d\u304b\u3051\u3067\u653e\u7f6e\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u5206\u5272\u3057\u3066\u521d\u6b69\u7684\u306a\u3053\u3068\u304b\u3089\u306f\u3058\u3081\u3088\u3046\u3068\u304a\u3082\u3063\u305f\u3002\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u4eca\u56de\u306f\u3001Jupyter\u8d77\u52d5\u3057\u3066\u3001sparkSession\u4f5c\u308b\u3060\u3051\u306b\u3057\u3066\u307f\u308b\u3002\n\n\u4f7f\u7528\u30d0\u30fc\u30b8\u30e7\u30f3\n\nPython == 3.5.1\nSpark == 2.0\u7cfb\u6700\u65b0\uff08branch-2.0\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3082\u306e\uff09\nnotebook == 4.2.1\n\nSpark\u306e\u6700\u65b0\u5b89\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u30012016-07-01\u73fe\u57281.6.2\u306a\u3093\u3060\u3051\u3069\u3001\u3082\u3046github\u306b\u306f2.0.0-rc1\u51fa\u3066\u305f\u308a\u3059\u308b\u3002\u3057\u304b\u3082rc1\u51fa\u3066\u4ee5\u964d\u3082\u3001\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u3068\u304bcommit\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7d50\u5c40\u4eca\u4f7f\u3063\u3066\u3044\u308b\u306e\u306f\u3001branch-2.0\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3082\u306e\u3002\n\u3061\u306a\u307f\u306b\u30012.0\u3067\u7d50\u69cbAPI\u304c\u5909\u308f\u3063\u3066\u3044\u307e\u3059\u3002\n\nJupyter\u8d77\u52d5\u306e\u524d\u306b\u3084\u308b\u3053\u3068\nJupyter\u8d77\u52d5\u524d\u306b\u3001\u3044\u308d\u3044\u308d\u74b0\u5883\u5909\u6570\u3092\u30bb\u30c3\u30c8\u3057\u3066\u304a\u304f\u3002Jupyter\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u66f8\u3044\u3068\u3044\u3066\u3082\u3044\u3044\u3051\u3069\u3001\u66f8\u304d\u65b9\u3088\u304f\u308f\u304b\u3063\u3066\u3044\u306a\u3044\u3057\u3001\u6bce\u56de\u8a2d\u5b9a\u5909\u3048\u305f\u308a\u3059\u308b\u306e\u3067\u3001\u74b0\u5883\u5909\u6570\u3067\u3084\u3063\u3066\u3057\u307e\u3046\u3002\n$ export SPARK_HOME=/opt/local/spark\n$ export PYSPARK_PYTHON=/opt/local/python-3.5.1/bin/python3\n$ export PYSPARK_DRIVER_PYTHON=/opt/local/python-3.5.1/bin/python3\nexport PYTHONPATH=$(ls -a ${SPARK_HOME}/python/lib/py4j-*-src.zip):${SPARK_HOME}/python:$PYTHONPATH\n$ export PYSPARK_SUBMIT_ARGS=\"\n--packages com.amazonaws:aws-java-sdk-pom:1.11.8,org.apache.hadoop:hadoop-aws:2.7.2\n --conf 'spark.local.dir=/mnt/ephemeral/tmp/spark'\n --driver-java-options '-XX:+UseG1GC -XX:G1HeapRegionSize=32m -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=300 -XX:InitiatingHeapOccupancyPercent=35'\n --driver-library-path '/opt/local/hadoop/lib/native'\n --conf 'spark.driver.memory=2g'\n --conf 'spark.driver.maxResultSize=2g'\n --conf 'spark.executor.memory=45g'\n --conf 'spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:G1HeapRegionSize=32m -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=300 -XX:InitiatingHeapOccupancyPercent=35'\n --conf 'spark.executor.extraLibraryPath=/opt/local/hadoop/lib/native'\n --conf 'spark.executorEnv.LD_PRELOAD=/usr/lib/libjemalloc.so'\n --conf 'spark.network.timeout=600s'\n --conf 'spark.io.compression.codec=lz4'\n --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n --conf 'spark.kryo.referenceTracking=false'\n --conf 'spark.shuffle.io.numConnectionsPerPeer=4'\n --conf 'spark.sql.inMemoryColumnarStorage.batchSize=20000'\n --conf 'spark.sql.autoBroadcastJoinThreshold=104857600'\n --conf 'spark.sql.shuffle.partitions=800'\n pyspark-shell\n\"\n\n\n\u74b0\u5883\u5909\u6570\u8aac\u660e\nSpark\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u898b\u308c\u3070\u308f\u304b\u308b\u3051\u3069\u4e00\u5fdc\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30d1\u30b9\u3068\u304b\u306f\u3001\u81ea\u5206\u306e\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u306d\u3002\u3053\u308c\u4ee5\u5916\u306b\u3082\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066HADOOP_HOME\u3068\u304b\u3082\u3002\n\n\n\n\u74b0\u5883\u5909\u6570\u540d\n\u9069\u7528\n\n\n\n\nSPARK_HOME\n\u307e\u3001\u3053\u308c\u306f\u305d\u306e\u307e\u3093\u307e\u3002spark-env.sh\u3068\u304b\u3067\u8a2d\u5b9a\u3057\u3066\u3082\u3044\u3044\u3002\n\n\nPYSPARK_PYTHON\nWorker\u304c\u4f7f\u3046Python executable\u3002\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070OS\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\n\n\nPYSPARK_DRIVER_PYTHON\nDriver\u304c\u4f7f\u3046Python executable\u3002\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070OS\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\n\n\nPYTHONPATH\nJupyter\u4e0a\u3067import pyspark\u3067\u304d\u308b\u3088\u3046\u306b\u3002py4j\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u5909\u308f\u3063\u305f\u308a\u3059\u308b\u306e\u3067\u308f\u3056\u308f\u3056ls\u3057\u3066\u307e\u3059\n\n\nPYSPARK_SUBMIT_ARGS\npyspark\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3002aws\u95a2\u9023\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8aad\u3093\u3060\u308a\u3057\u3066\u3044\u308b\u3002\u597d\u304d\u306a\u3088\u3046\u306b\u5909\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u30e1\u30e2\u30ea\u3092\u305f\u304f\u3055\u3093\u4f7f\u3046\u8a2d\u5b9a\u306b\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u306e\u307e\u307e\u5f35\u308a\u4ed8\u3051\u305f\u308a\u3057\u3066\u3082\u3001\u30e1\u30e2\u30ea\u8db3\u308a\u306a\u3044\u3068\u52d5\u304d\u307e\u305b\u3093\u3002\u6700\u5f8c\u306epyspark-shell\u306f\u5fc5\u8981\u3002\n\n\n\n\u8907\u6570notebook\u4f7f\u3046\u6642\u3001\u30e1\u30e2\u30ea\u306a\u3069\u306e\u8a2d\u5b9a\u3092notebook\u3054\u3068\u306b\u5909\u3048\u305f\u3044\u5834\u5408\u306f\u3001notebook\u4e0a\u3067sparkSession\u3092\u4f5c\u308b\u524d\u306b\u3001os.environ\u3092\u4f7f\u3063\u3066PYSPARK_SUBMIT_ARGS\u3092\u4e0a\u66f8\u304d\u3057\u3066\u3082\u3044\u3044\u3088\u3002\n\nJupyter\u8d77\u52d5\n\u4e0a\u8a18\u74b0\u5883\u5909\u6570\u3068\u3068\u3082\u306b\u3001\u3053\u3093\u306a\u611f\u3058\u3067\u3002\n$ /opt/local/python-3.5/bin/jupyter notebook --ip=0.0.0.0 --no-browser\n\n\u3053\u308c\u4ee5\u964d\u306f\u3001Jupyter\u4e0a\u3067\u4f5c\u696d\u3002\u4ee5\u4e0b\u306f\u3001Jupyter\u3067\u3064\u304f\u3063\u305fnotebook\u3092markdown\u5909\u63db\u3057\u3066\u5f35\u308a\u4ed8\u3051\u305f\u3060\u3051\u3002\n\n\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306eimport\nimport os\n\nimport pyspark\nfrom pyspark import StorageLevel\nfrom pyspark.sql import (\n    SparkSession,\n    functions as sql_funcs,\n)\nfrom pyspark.sql.types import *\n\n\nSparkSession\u4f5c\u6210\n2.0.0\u304b\u3089\u306f\u3001pyspark.sql.SparkSession\u304c\u3053\u3046\u3044\u3046\u6642\u306e\u30d5\u30ed\u30f3\u30c8API\u306b\u306a\u3063\u3066\u3044\u308b\u307f\u305f\u3044\u306a\u306e\u3067\u3001\u305d\u308c\u306b\u5f93\u3046\u3002\nSparkSession\u4f7f\u7528\u6642\u306b\u3001SparkContext\u306eAPI\u306b\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u3001spark_session.sparkContext\u3067SparkContext\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002\ntry:\n    spark_session.stop()\nexcept NameError:\n    pass\n\nspark_session = SparkSession.builder.appName(\n    name='spark-tips-1',\n).master(\n    master=os.environ.get('X_SPARK_MASTER', 'local[*]'),\n).enableHiveSupport().getOrCreate()\n\n\nDataFrame\u4f5c\u6210\npython\u306e\u6b20\u70b9\u306f\u9045\u3044\u3068\u3053\u308d\u3002pyspark\u306e\u30bd\u30fc\u30b9\u898b\u308c\u3070\u308f\u304b\u308b\u3051\u3069\u3001\u7279\u306brdd\u306eAPI\u306f\u3001\u300c\u51e6\u7406\u3092\u901f\u304f\u3057\u3088\u3046\u300d\u3068\u3044\u3046\u610f\u601d\u3092\u5fae\u5875\u3082\u611f\u3058\u3055\u305b\u306a\u3044\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u305f\u308a\u3059\u308b\u3002\n\u306a\u306e\u3067\u3001DataFrame\uff08\u5c06\u6765\u7684\u306b\u306fDataSet?\uff09\u3067\u5b8c\u7d50\u3067\u304d\u308b\u51e6\u7406\u306f\u3001\u6975\u529bDataFrame\u3067\u3084\u308d\u3046\u3002\n\u4eca\u56de\u306f\u3001\u6700\u521d\u306e\u4e00\u6b69\u306a\u306e\u3067\u3001\u304a\u624b\u8efd\u306b\u30d7\u30ed\u30bb\u30b9\u5185\u306elist\u304b\u3089DataFrame\u4f5c\u6210\u3002\nlocal_list = (\n    ('2016-07-01 00:00:00', 'jiba-nyan', 1,),\n    ('2016-07-01 00:01:00', 'bushi-nyan', 1,),\n    ('2016-07-01 00:02:00', 'koma-san', 1,),\n    ('2016-07-01 00:03:00', 'komajiro', 1,),\n)\n\nschema = StructType(\n    fields=(\n        # \u65e5\u6642\n        StructField(\n            name='dt',\n            dataType=StringType(),\n            nullable=False,\n        ),\n        # user_name\n        StructField(\n            name='user_name',\n            dataType=StringType(),\n            nullable=False,\n        ),\n        # rate\n        StructField(\n            name='rate',\n            dataType=LongType(),\n            nullable=True,\n        ),\n    ),\n)\n\n\ndf = spark_session.createDataFrame(\n    data=local_list,\n    schema=schema,\n).persist(\n    storageLevel=StorageLevel.MEMORY_ONLY_SER,\n)\n\n\nprintSchema()\u3067\u30b9\u30ad\u30fc\u30de\u78ba\u8a8d\ndf.printSchema()\n\nroot\n |-- dt: string (nullable = false)\n |-- user_name: string (nullable = false)\n |-- rate: long (nullable = true)\n\n\nshow()\u3067\u4e2d\u8eab\u3092\u78ba\u8a8d\ndf.show()\n\n+-------------------+----------+----+\n|                 dt| user_name|rate|\n+-------------------+----------+----+\n|2016-07-01 00:00:00| jiba-nyan|   1|\n|2016-07-01 00:01:00|bushi-nyan|   1|\n|2016-07-01 00:02:00|  koma-san|   1|\n|2016-07-01 00:03:00|  komajiro|   1|\n+-------------------+----------+----+\n\n\ntake()\u3067\u4e2d\u8eab\u3092\u78ba\u8a8d\ndf.take(4)\n\n[Row(dt='2016-07-01 00:00:00', user_name='jiba-nyan', rate=1),\n Row(dt='2016-07-01 00:01:00', user_name='bushi-nyan', rate=1),\n Row(dt='2016-07-01 00:02:00', user_name='koma-san', rate=1),\n Row(dt='2016-07-01 00:03:00', user_name='komajiro', rate=1)]\n\n\n\u65e5\u4ed8\u306e\u30ab\u30e9\u30e0\u304c\u6587\u5b57\u5217\u306b\u306a\u3063\u3066\u308b\u306e\u3067\u3001Timestamp\u578b\u306b\u5909\u63db\n\u3053\u306e\u5834\u5408\u306f\u3001\u3046\u307e\u3044\u5177\u5408\u306b\u65e5\u6642\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u306a\u3063\u3066\u308b\u306e\u3067\u3001cast(TimestampType())\u3059\u308b\u3060\u3051\u3002\n\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u304c\u9055\u3046\u5834\u5408\u3082\u3001\u6587\u5b57\u5217\u64cd\u4f5c\u306a\u3069\u306eSQL\u95a2\u6570\u3067\u3001\uff08python\u4f7f\u308f\u305a\u306b\uff09\u5927\u4f53\u4f55\u3068\u304b\u306a\u308a\u307e\u3059\u3002\ndf_parsed = df.withColumn(\n    'dt',\n    df['dt'].cast(TimestampType()),\n).persist(\n    storageLevel=StorageLevel.MEMORY_ONLY_SER,\n)\n\ndf_parsed.printSchema()\n\nroot\n |-- dt: timestamp (nullable = true)\n |-- user_name: string (nullable = false)\n |-- rate: long (nullable = true)\n\ndf_parsed.show()\n\n+--------------------+----------+----+\n|                  dt| user_name|rate|\n+--------------------+----------+----+\n|2016-07-01 00:00:...| jiba-nyan|   1|\n|2016-07-01 00:01:...|bushi-nyan|   1|\n|2016-07-01 00:02:...|  koma-san|   1|\n|2016-07-01 00:03:...|  komajiro|   1|\n+--------------------+----------+----+\n\ndf_parsed.take(4)\n\n[Row(dt=datetime.datetime(2016, 7, 1, 0, 0), user_name='jiba-nyan', rate=1),\n Row(dt=datetime.datetime(2016, 7, 1, 0, 1), user_name='bushi-nyan', rate=1),\n Row(dt=datetime.datetime(2016, 7, 1, 0, 2), user_name='koma-san', rate=1),\n Row(dt=datetime.datetime(2016, 7, 1, 0, 3), user_name='komajiro', rate=1)]\n\n\n\u307e\u3068\u3081\n\u4eca\u56de\u306f\u6700\u521d\u306e\u4e00\u6b69\u306a\u306e\u3067\u3001\u7c21\u5358\u3060\u3088\u306d\u3093\u3002\n\n# \u6982\u8981\n\u307f\u3093\u306a\u5927\u597d\u304dJupyter notebook(python)\u4e0a\u3067\u3001Pyspark/Cython\u3092\u4f7f\u3063\u3066\u3044\u308d\u3093\u306a\u3053\u3068\u3092\u3084\u308b\u3002\u3068\u304b\u3044\u3046\u8a18\u4e8b\u3092\u66f8\u3053\u3046\u3068\u601d\u3063\u305f\u3051\u3069\u30011\u8a18\u4e8b\u306b\u8a70\u3081\u8fbc\u307f\u3059\u304e\u3066\u3082\u919c\u3044\u3057\u3001\u6642\u9593\u304b\u304b\u3063\u3066\u66f8\u304d\u304b\u3051\u3067\u653e\u7f6e\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u5206\u5272\u3057\u3066\u521d\u6b69\u7684\u306a\u3053\u3068\u304b\u3089\u306f\u3058\u3081\u3088\u3046\u3068\u304a\u3082\u3063\u305f\u3002\n\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u4eca\u56de\u306f\u3001Jupyter\u8d77\u52d5\u3057\u3066\u3001sparkSession\u4f5c\u308b\u3060\u3051\u306b\u3057\u3066\u307f\u308b\u3002\n\n# \u4f7f\u7528\u30d0\u30fc\u30b8\u30e7\u30f3\n- Python == 3.5.1\n- Spark == 2.0\u7cfb\u6700\u65b0\uff08branch-2.0\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3082\u306e\uff09\n- notebook == 4.2.1\n\nSpark\u306e\u6700\u65b0\u5b89\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u30012016-07-01\u73fe\u57281.6.2\u306a\u3093\u3060\u3051\u3069\u3001\u3082\u3046github\u306b\u306f2.0.0-rc1\u51fa\u3066\u305f\u308a\u3059\u308b\u3002\u3057\u304b\u3082rc1\u51fa\u3066\u4ee5\u964d\u3082\u3001\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u3068\u304bcommit\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u7d50\u5c40\u4eca\u4f7f\u3063\u3066\u3044\u308b\u306e\u306f\u3001branch-2.0\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3082\u306e\u3002\n\u3061\u306a\u307f\u306b\u30012.0\u3067\u7d50\u69cbAPI\u304c\u5909\u308f\u3063\u3066\u3044\u307e\u3059\u3002\n\n# Jupyter\u8d77\u52d5\u306e\u524d\u306b\u3084\u308b\u3053\u3068\nJupyter\u8d77\u52d5\u524d\u306b\u3001\u3044\u308d\u3044\u308d\u74b0\u5883\u5909\u6570\u3092\u30bb\u30c3\u30c8\u3057\u3066\u304a\u304f\u3002Jupyter\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u66f8\u3044\u3068\u3044\u3066\u3082\u3044\u3044\u3051\u3069\u3001\u66f8\u304d\u65b9\u3088\u304f\u308f\u304b\u3063\u3066\u3044\u306a\u3044\u3057\u3001\u6bce\u56de\u8a2d\u5b9a\u5909\u3048\u305f\u308a\u3059\u308b\u306e\u3067\u3001\u74b0\u5883\u5909\u6570\u3067\u3084\u3063\u3066\u3057\u307e\u3046\u3002\n\n```bash\n$ export SPARK_HOME=/opt/local/spark\n$ export PYSPARK_PYTHON=/opt/local/python-3.5.1/bin/python3\n$ export PYSPARK_DRIVER_PYTHON=/opt/local/python-3.5.1/bin/python3\nexport PYTHONPATH=$(ls -a ${SPARK_HOME}/python/lib/py4j-*-src.zip):${SPARK_HOME}/python:$PYTHONPATH\n$ export PYSPARK_SUBMIT_ARGS=\"\n--packages com.amazonaws:aws-java-sdk-pom:1.11.8,org.apache.hadoop:hadoop-aws:2.7.2\n --conf 'spark.local.dir=/mnt/ephemeral/tmp/spark'\n --driver-java-options '-XX:+UseG1GC -XX:G1HeapRegionSize=32m -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=300 -XX:InitiatingHeapOccupancyPercent=35'\n --driver-library-path '/opt/local/hadoop/lib/native'\n --conf 'spark.driver.memory=2g'\n --conf 'spark.driver.maxResultSize=2g'\n --conf 'spark.executor.memory=45g'\n --conf 'spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:G1HeapRegionSize=32m -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=300 -XX:InitiatingHeapOccupancyPercent=35'\n --conf 'spark.executor.extraLibraryPath=/opt/local/hadoop/lib/native'\n --conf 'spark.executorEnv.LD_PRELOAD=/usr/lib/libjemalloc.so'\n --conf 'spark.network.timeout=600s'\n --conf 'spark.io.compression.codec=lz4'\n --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n --conf 'spark.kryo.referenceTracking=false'\n --conf 'spark.shuffle.io.numConnectionsPerPeer=4'\n --conf 'spark.sql.inMemoryColumnarStorage.batchSize=20000'\n --conf 'spark.sql.autoBroadcastJoinThreshold=104857600'\n --conf 'spark.sql.shuffle.partitions=800'\n pyspark-shell\n\"\n```\n\n\n## \u74b0\u5883\u5909\u6570\u8aac\u660e\nSpark\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u898b\u308c\u3070\u308f\u304b\u308b\u3051\u3069\u4e00\u5fdc\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30d1\u30b9\u3068\u304b\u306f\u3001\u81ea\u5206\u306e\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u306d\u3002\u3053\u308c\u4ee5\u5916\u306b\u3082\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066HADOOP_HOME\u3068\u304b\u3082\u3002\n\n| \u74b0\u5883\u5909\u6570\u540d | \u9069\u7528 |\n|:-----------|:------------|\n| SPARK_HOME | \u307e\u3001\u3053\u308c\u306f\u305d\u306e\u307e\u3093\u307e\u3002spark-env.sh\u3068\u304b\u3067\u8a2d\u5b9a\u3057\u3066\u3082\u3044\u3044\u3002 | \n| PYSPARK_PYTHON | Worker\u304c\u4f7f\u3046Python executable\u3002\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070OS\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython |\n| PYSPARK_DRIVER_PYTHON | Driver\u304c\u4f7f\u3046Python executable\u3002\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070OS\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython |\n| PYTHONPATH | Jupyter\u4e0a\u3067`import pyspark`\u3067\u304d\u308b\u3088\u3046\u306b\u3002py4j\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u5909\u308f\u3063\u305f\u308a\u3059\u308b\u306e\u3067\u308f\u3056\u308f\u3056ls\u3057\u3066\u307e\u3059 |\n| PYSPARK_SUBMIT_ARGS | pyspark\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3002aws\u95a2\u9023\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8aad\u3093\u3060\u308a\u3057\u3066\u3044\u308b\u3002\u597d\u304d\u306a\u3088\u3046\u306b\u5909\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u30e1\u30e2\u30ea\u3092\u305f\u304f\u3055\u3093\u4f7f\u3046\u8a2d\u5b9a\u306b\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u306e\u307e\u307e\u5f35\u308a\u4ed8\u3051\u305f\u308a\u3057\u3066\u3082\u3001\u30e1\u30e2\u30ea\u8db3\u308a\u306a\u3044\u3068\u52d5\u304d\u307e\u305b\u3093\u3002\u6700\u5f8c\u306e`pyspark-shell`\u306f\u5fc5\u8981\u3002 |\n\n\u8907\u6570notebook\u4f7f\u3046\u6642\u3001\u30e1\u30e2\u30ea\u306a\u3069\u306e\u8a2d\u5b9a\u3092notebook\u3054\u3068\u306b\u5909\u3048\u305f\u3044\u5834\u5408\u306f\u3001notebook\u4e0a\u3067sparkSession\u3092\u4f5c\u308b\u524d\u306b\u3001`os.environ`\u3092\u4f7f\u3063\u3066`PYSPARK_SUBMIT_ARGS`\u3092\u4e0a\u66f8\u304d\u3057\u3066\u3082\u3044\u3044\u3088\u3002\n\n\n# Jupyter\u8d77\u52d5\n\u4e0a\u8a18\u74b0\u5883\u5909\u6570\u3068\u3068\u3082\u306b\u3001\u3053\u3093\u306a\u611f\u3058\u3067\u3002\n\n```bash\n$ /opt/local/python-3.5/bin/jupyter notebook --ip=0.0.0.0 --no-browser\n```\n\n\u3053\u308c\u4ee5\u964d\u306f\u3001Jupyter\u4e0a\u3067\u4f5c\u696d\u3002\u4ee5\u4e0b\u306f\u3001Jupyter\u3067\u3064\u304f\u3063\u305fnotebook\u3092markdown\u5909\u63db\u3057\u3066\u5f35\u308a\u4ed8\u3051\u305f\u3060\u3051\u3002\n\n\n## \u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306eimport\n\n\n```python\nimport os\n\nimport pyspark\nfrom pyspark import StorageLevel\nfrom pyspark.sql import (\n    SparkSession,\n    functions as sql_funcs,\n)\nfrom pyspark.sql.types import *\n```\n\n## SparkSession\u4f5c\u6210\n2.0.0\u304b\u3089\u306f\u3001pyspark.sql.SparkSession\u304c\u3053\u3046\u3044\u3046\u6642\u306e\u30d5\u30ed\u30f3\u30c8API\u306b\u306a\u3063\u3066\u3044\u308b\u307f\u305f\u3044\u306a\u306e\u3067\u3001\u305d\u308c\u306b\u5f93\u3046\u3002\n\nSparkSession\u4f7f\u7528\u6642\u306b\u3001SparkContext\u306eAPI\u306b\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u3001spark_session.sparkContext\u3067SparkContext\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002\n\n\n```python\ntry:\n    spark_session.stop()\nexcept NameError:\n    pass\n\nspark_session = SparkSession.builder.appName(\n    name='spark-tips-1',\n).master(\n    master=os.environ.get('X_SPARK_MASTER', 'local[*]'),\n).enableHiveSupport().getOrCreate()\n```\n\n## DataFrame\u4f5c\u6210\npython\u306e\u6b20\u70b9\u306f\u9045\u3044\u3068\u3053\u308d\u3002pyspark\u306e\u30bd\u30fc\u30b9\u898b\u308c\u3070\u308f\u304b\u308b\u3051\u3069\u3001\u7279\u306brdd\u306eAPI\u306f\u3001\u300c\u51e6\u7406\u3092\u901f\u304f\u3057\u3088\u3046\u300d\u3068\u3044\u3046\u610f\u601d\u3092\u5fae\u5875\u3082\u611f\u3058\u3055\u305b\u306a\u3044\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u305f\u308a\u3059\u308b\u3002\n\u306a\u306e\u3067\u3001DataFrame\uff08\u5c06\u6765\u7684\u306b\u306fDataSet?\uff09\u3067\u5b8c\u7d50\u3067\u304d\u308b\u51e6\u7406\u306f\u3001\u6975\u529bDataFrame\u3067\u3084\u308d\u3046\u3002\n\n\u4eca\u56de\u306f\u3001\u6700\u521d\u306e\u4e00\u6b69\u306a\u306e\u3067\u3001\u304a\u624b\u8efd\u306b\u30d7\u30ed\u30bb\u30b9\u5185\u306elist\u304b\u3089DataFrame\u4f5c\u6210\u3002\n\n\n```python\nlocal_list = (\n    ('2016-07-01 00:00:00', 'jiba-nyan', 1,),\n    ('2016-07-01 00:01:00', 'bushi-nyan', 1,),\n    ('2016-07-01 00:02:00', 'koma-san', 1,),\n    ('2016-07-01 00:03:00', 'komajiro', 1,),\n)\n\nschema = StructType(\n    fields=(\n        # \u65e5\u6642\n        StructField(\n            name='dt',\n            dataType=StringType(),\n            nullable=False,\n        ),\n        # user_name\n        StructField(\n            name='user_name',\n            dataType=StringType(),\n            nullable=False,\n        ),\n        # rate\n        StructField(\n            name='rate',\n            dataType=LongType(),\n            nullable=True,\n        ),\n    ),\n)\n\n\ndf = spark_session.createDataFrame(\n    data=local_list,\n    schema=schema,\n).persist(\n    storageLevel=StorageLevel.MEMORY_ONLY_SER,\n)\n```\n\n## printSchema()\u3067\u30b9\u30ad\u30fc\u30de\u78ba\u8a8d\n\n\n```python\ndf.printSchema()\n```\n\n    root\n     |-- dt: string (nullable = false)\n     |-- user_name: string (nullable = false)\n     |-- rate: long (nullable = true)\n    \n\n\n## show()\u3067\u4e2d\u8eab\u3092\u78ba\u8a8d\n\n\n```python\ndf.show()\n```\n\n    +-------------------+----------+----+\n    |                 dt| user_name|rate|\n    +-------------------+----------+----+\n    |2016-07-01 00:00:00| jiba-nyan|   1|\n    |2016-07-01 00:01:00|bushi-nyan|   1|\n    |2016-07-01 00:02:00|  koma-san|   1|\n    |2016-07-01 00:03:00|  komajiro|   1|\n    +-------------------+----------+----+\n    \n\n\n## take()\u3067\u4e2d\u8eab\u3092\u78ba\u8a8d\n\n\n```python\ndf.take(4)\n```\n\n\n\n\n    [Row(dt='2016-07-01 00:00:00', user_name='jiba-nyan', rate=1),\n     Row(dt='2016-07-01 00:01:00', user_name='bushi-nyan', rate=1),\n     Row(dt='2016-07-01 00:02:00', user_name='koma-san', rate=1),\n     Row(dt='2016-07-01 00:03:00', user_name='komajiro', rate=1)]\n\n\n\n## \u65e5\u4ed8\u306e\u30ab\u30e9\u30e0\u304c\u6587\u5b57\u5217\u306b\u306a\u3063\u3066\u308b\u306e\u3067\u3001Timestamp\u578b\u306b\u5909\u63db\n\u3053\u306e\u5834\u5408\u306f\u3001\u3046\u307e\u3044\u5177\u5408\u306b\u65e5\u6642\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u306a\u3063\u3066\u308b\u306e\u3067\u3001`cast(TimestampType())`\u3059\u308b\u3060\u3051\u3002\n\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u304c\u9055\u3046\u5834\u5408\u3082\u3001\u6587\u5b57\u5217\u64cd\u4f5c\u306a\u3069\u306eSQL\u95a2\u6570\u3067\u3001\uff08python\u4f7f\u308f\u305a\u306b\uff09\u5927\u4f53\u4f55\u3068\u304b\u306a\u308a\u307e\u3059\u3002\n\n\n```python\ndf_parsed = df.withColumn(\n    'dt',\n    df['dt'].cast(TimestampType()),\n).persist(\n    storageLevel=StorageLevel.MEMORY_ONLY_SER,\n)\n```\n\n\n```python\ndf_parsed.printSchema()\n```\n\n    root\n     |-- dt: timestamp (nullable = true)\n     |-- user_name: string (nullable = false)\n     |-- rate: long (nullable = true)\n    \n\n\n\n```python\ndf_parsed.show()\n```\n\n    +--------------------+----------+----+\n    |                  dt| user_name|rate|\n    +--------------------+----------+----+\n    |2016-07-01 00:00:...| jiba-nyan|   1|\n    |2016-07-01 00:01:...|bushi-nyan|   1|\n    |2016-07-01 00:02:...|  koma-san|   1|\n    |2016-07-01 00:03:...|  komajiro|   1|\n    +--------------------+----------+----+\n    \n\n\n\n```python\ndf_parsed.take(4)\n```\n\n\n\n\n    [Row(dt=datetime.datetime(2016, 7, 1, 0, 0), user_name='jiba-nyan', rate=1),\n     Row(dt=datetime.datetime(2016, 7, 1, 0, 1), user_name='bushi-nyan', rate=1),\n     Row(dt=datetime.datetime(2016, 7, 1, 0, 2), user_name='koma-san', rate=1),\n     Row(dt=datetime.datetime(2016, 7, 1, 0, 3), user_name='komajiro', rate=1)]\n\n\n# \u307e\u3068\u3081\n\u4eca\u56de\u306f\u6700\u521d\u306e\u4e00\u6b69\u306a\u306e\u3067\u3001\u7c21\u5358\u3060\u3088\u306d\u3093\u3002\n", "tags": ["Spark", "Pyspark"]}