{"context": "100 input, 100 output\u306e\u5b66\u7fd2\u3092\u8a66\u884c\u3057\u3066\u3044\u3066\u3001\u3042\u307e\u308a\u9032\u5c55\u304c\u3088\u308d\u3057\u304f\u306a\u3044\u3002\n\u95a2\u9023\u8ad6\u6587\u3092Deep Learning\u4e2d\u3002\n\u305d\u306e\u4e2d\u3067\u3001\u6c17\u306b\u306a\u3063\u305f\u306e\u304c\u4ee5\u4e0b\u3002\nhttp://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/\n\n\u30ad\u30fc\u30ef\u30fc\u30c9\n\nthe classic Mixture Density Networks (Bishop \u201994) model\n\n\na Mixture Density Network (MDN)\n\n\nto fit a noisy sinusoidal data\nepsilon:  standard gaussian random noise\nthe fancy RMSProp gradient descent optimisation method\na one-to-one, or many-to-one function\n\nWhat we want is a model that has the capacity to predict a range of different output values for each input \n\n\u3053\u3053\u307e\u3067\u8aad\u3093\u3067\u6982\u8981\u3068\u3057\u3066\u63b4\u3093\u3060\u3053\u3068\u306f\u3001\n1\u3064\u306e\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u8907\u6570\u306e\u51fa\u529b\u5024\u304c\u3042\u308b\u5834\u5408(\u4f8b\u3068\u3057\u3066\u3001sine curve\u3092\u6a2a\u8ee2\u3057\u305f\u3082\u306e\uff09\u3067\u306f\u5f93\u6765\u306eneural network\u3067\u306f\u5b66\u7fd2\u304c\u5931\u6557\u3059\u308b\u3002\nMixture Density Networks\u3060\u3068\u300c1\u3064\u306e\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u8907\u6570\u306e\u51fa\u529b\u5024\u304c\u3042\u308b\u5834\u5408\u300d\u306e\u5b66\u7fd2\u304c\u3067\u304d\u3066\u3044\u308b\u3088\u3046\u3060\u3002\n\nthe network is to predict an entire probability distribution for the output\nMDNs can also used to model handwriting, where the next stroke is drawn from a probability distribution of multiple possibilities, rather than sticking to one prediction.\nMixture Gaussian distributions\n\n\nthe output value is modelled as a sum of many gaussian random values\neach with different means and standard deviations\n\n\na probability distribution function (pdf) of P(Y = y | X = z)\n\na restriction that the sum of PI_k(x) add up to one\nwe will use a neural network of one hidden later with 24 nodes, and also generate 24 mixtures, hence there will be 72 actual outputs of our neural network of a single input.\nZ is a vector of 72 values\n\n\ninto three equal parts, Z{0to23}, Z{24to43}, Z{44to71}\n\n\nthe softmax and exponential terms have some theoretical interpretations from a Bayesian framework way of looking at probability\nloss function\n\n\nA more suitable loss function is to minimise the logarithm of the likelihood of the distribution vs the training data:\nfor every  (x, y)point in the training data set, we can compute a cost function\na similar approach, but with non-discretised states.\ndef get_lossfunc(\n\n\n there would be room for performance improvements by building a custom operator into TensorFlow with the pre-optimised gradient formulas for this loss function\n\n\u3053\u3053\u307e\u3067\u306e\u7406\u89e3\u3068\u3057\u3066\u306f\uff08\u9593\u9055\u3063\u3066\u3044\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff09\n\n1\u3064\u306ex\u306b\u5bfe\u3057\u3066\u8907\u6570\u306ey\u304c\u4e0e\u3048\u3089\u308c\u308b\u5834\u5408\u306b\u5bfe\u3057\u3066\u3001MDN\u3092\u4f7f\u3063\u305f\nMDN\u306b\u3088\u308a\u3001(x,y)\u5ea7\u6a19\u306e\u5404\u70b9\u306b\u5bfe\u3057\u3066loss function\u3092\u6700\u5c0f\u5316\u3059\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3059\u308b\n\n\u5ea7\u6a19\u306e\u5206\u89e3\u80fd\u3092\u3069\u308c\u304f\u3089\u3044\u53d6\u308b\u304b\u3067\u7d50\u679c\u306e\u7cbe\u5ea6\u3082\u5909\u308f\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n100 input, 100 output\u306e\u5b66\u7fd2\u3092\u8a66\u884c\u3057\u3066\u3044\u3066\u3001\u3042\u307e\u308a\u9032\u5c55\u304c\u3088\u308d\u3057\u304f\u306a\u3044\u3002\n\n\u95a2\u9023\u8ad6\u6587\u3092Deep Learning\u4e2d\u3002\n\n\u305d\u306e\u4e2d\u3067\u3001\u6c17\u306b\u306a\u3063\u305f\u306e\u304c\u4ee5\u4e0b\u3002\nhttp://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/\n\n### \u30ad\u30fc\u30ef\u30fc\u30c9\n\n- the classic Mixture Density Networks (Bishop \u201994) model\n  - a Mixture Density Network (MDN)\n- to fit a noisy sinusoidal data\n- epsilon:  standard gaussian random noise\n- the fancy RMSProp gradient descent optimisation method\n- a one-to-one, or many-to-one function\n- **What we want is a model that has the capacity to predict a range of different output values for each input** \n\n\u3053\u3053\u307e\u3067\u8aad\u3093\u3067\u6982\u8981\u3068\u3057\u3066\u63b4\u3093\u3060\u3053\u3068\u306f\u3001\n1\u3064\u306e\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u8907\u6570\u306e\u51fa\u529b\u5024\u304c\u3042\u308b\u5834\u5408(\u4f8b\u3068\u3057\u3066\u3001sine curve\u3092\u6a2a\u8ee2\u3057\u305f\u3082\u306e\uff09\u3067\u306f\u5f93\u6765\u306eneural network\u3067\u306f\u5b66\u7fd2\u304c\u5931\u6557\u3059\u308b\u3002\nMixture Density Networks\u3060\u3068\u300c1\u3064\u306e\u5165\u529b\u5024\u306b\u5bfe\u3057\u3066\u8907\u6570\u306e\u51fa\u529b\u5024\u304c\u3042\u308b\u5834\u5408\u300d\u306e\u5b66\u7fd2\u304c\u3067\u304d\u3066\u3044\u308b\u3088\u3046\u3060\u3002\n\n- the network is to predict an entire probability distribution for the output\n- MDNs can also used to model handwriting, where the next stroke is drawn from a probability distribution of multiple possibilities, rather than sticking to one prediction.\n- Mixture Gaussian distributions\n  - the output value is modelled as a sum of many gaussian random values\n  - each with different means and standard deviations\n- a probability distribution function (pdf) of `P(Y = y | X = z)`\n- a restriction that the sum of `PI_k(x)` add up to one\n- we will use a neural network of one hidden later with 24 nodes, and also generate 24 mixtures, hence there will be **72 actual outputs** of our neural network of a single input.\n- Z is a vector of 72 values\n  - into three equal parts, Z{0to23}, Z{24to43}, Z{44to71}\n- the softmax and exponential terms have some theoretical interpretations from a Bayesian framework way of looking at probability\n- loss function\n   - A more suitable loss function is to minimise the logarithm of the likelihood of the distribution vs the training data:\n   - **for every  (x, y)point in the training data set, we can compute a cost function**\n   - a similar approach, but with non-discretised states.\n   - `def get_lossfunc(`\n-  there would be room for performance improvements by building a custom operator into TensorFlow with the pre-optimised gradient formulas for this loss function\n\n\u3053\u3053\u307e\u3067\u306e\u7406\u89e3\u3068\u3057\u3066\u306f\uff08\u9593\u9055\u3063\u3066\u3044\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff09\n\n- 1\u3064\u306ex\u306b\u5bfe\u3057\u3066\u8907\u6570\u306ey\u304c\u4e0e\u3048\u3089\u308c\u308b\u5834\u5408\u306b\u5bfe\u3057\u3066\u3001MDN\u3092\u4f7f\u3063\u305f\n- MDN\u306b\u3088\u308a\u3001(x,y)\u5ea7\u6a19\u306e\u5404\u70b9\u306b\u5bfe\u3057\u3066loss function\u3092\u6700\u5c0f\u5316\u3059\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3059\u308b\n\n\u5ea7\u6a19\u306e\u5206\u89e3\u80fd\u3092\u3069\u308c\u304f\u3089\u3044\u53d6\u308b\u304b\u3067\u7d50\u679c\u306e\u7cbe\u5ea6\u3082\u5909\u308f\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\n", "tags": ["TensorFlow", "link", "borgWarp"]}