{"context": " More than 1 year has passed since last update.Japanese version of the article is also available\n\nTL;DR\nMaking Augmented Reality app easily with Scenekit + Vuforia\n\n\uff08See here for full movie\uff09\nAugmented Reality (AR) app are usualy made with OpenGL or Unity. However it's difficult to create complex 3D scene such as scene with physics simulation and so on. With SceneKit, you can create your own AR app with complex scene easily and it can be embedded in usual app using UIKit.\n\nSceneKit\n\nSceneKit is a high-level 3D graphics framework that helps you create 3D animated scenes and effects in your apps. It incorporates a physics engine, a particle generator, and easy ways to script the actions of 3D objects so you can describe your scene in terms of its content \u2014 geometry, materials, lights, and cameras \u2014 then animate it by describing changes to those objects.\n\nSceneKit - Apple Developer\nSceneKit is a 3D framework which allows you to create something like 3D video games. It was only for Mac but since iOS8 you can use it in iOS. Add SCNView as subview to your view and add models, lights, camera to SCNView's SCNScene property, The 3D scene is automatically drawn by the framework. It's really easy. And also it provides a way to combine the scene with OpenGL or Metal. So let's take advangtage of that and add SceneKit object over the video camera input from Vuforia.\n\nVuforia\nVuforiais a mobile vision platform for AR developed by Qualcomm and now it was acquired by PTC. Vuforia provides AR library for iOS, Android and Unity and you can use all fundamental features without a fee (with watermark shown only once a day) Details are here\u3002\nThe SDK is provided as a static library (.a) so you can't use Swift. How sad.\n\nBulid samples and run.\n\nVisit Vuforia Developer Portal, then register and login.\nDownload SDK and Sample projects.\nMove the sample project to samples directory in SDK. (official really short description are here\uff09.\nIssue Licence Key which will be used in the app at LicenceManager.\nOpen SampleApplicationSession.mm in the sample and replace with QCAR::setInitParameters(mQCARInitFalgs, *Your License Key*). (Thanks to the detailed explanation in [Log] Xcode + vuforia\u3067AR\u30a2\u30d7\u30ea\u3092\u52d5\u304b\u3057\u3066\u307f\u308b - \u3057\u3081\u9bd6\u65e5\u8a18 In Japanese).\nBuild the target with real device (If build a target with iOS Simulator, it fails).\nVoil\u00e0! Now you can try the sample app at your own iPhone. There are many samples in the app:\n\n\nImage Targets - AR with pictures already registered in the Vuforia cloud. It enables highly precise tracking by image processing in the cloud.\nCylinder Targets, Multi Targets - Use cylinder or cuboid as a marker. Even solid objects can be used as a marker.\nUser Defined Targets - Users can take a picture includes planar object and Use that as a marker. It's awesome! From now on, let's integrate SceneKit into User Defined Targets sample.\n\n\n\nAR principles\n\nTheory\nThe AR in this context means reconstructing the marker's relative 3D position towards a camera from the 2D image which taken by the camera. The process is:\n\nConvert Marker Coordinate (World Coordinate; its origin is a maker's position. 3D.) to Camera Coordinate(its origin is camera's position. 3D), which means get the marker's relative position seen by the camera.\nConvert Camera Coordinate (3D) to final 2D coordinate.\n\nThis process can be represented as a formula (using homogeneous coordinates):\n\n\nIntrinsic parameter matrix (intrinsic matrix)\nIntrinsic parameter matrix represents the camera's information such as focal length and center coordinate of the image. It's  peculiar to the camera. So your iPhone 6s and my iPhone6s has same intrinsic parameter matrix. Vuforia has a bunch of cameras' intrinsic matrix.\nIf intrinsic matrix isn't right, drawn virtual objects and image from camera's perspective does not match and result in weird looking.\nIf you interested in calculate intrinsic matrix of unknown camera, look up Zhang's method, which is implemented in OpenCV, too.\n\nExtrinsic parameter matrix (extrinsic matrix)\nExtrinsic parameter matrix represents 3-dimentional rotation and translation. With extrinsic matrix, we can convert a coordinate of the object in Marker Coordinate to Camera Coordinate.\nExtrinsic matrix differs by how user hold his/her camera or where marker located so we have to calculate every frame. Vuforia is really good at that. very precise.\nTheoretically, extrinsic parameter matrix can be obtained by solving PnP(Perspective n-Point Problem), which is implemented in OpenCV, too.\n\nImplementation\nIn reality, you don't have to do the matrix calculation every frame and leave that to the 3D library. For example, Using OpenGL, set perspective projection matrix using intrinsic matrix and model view matrix using extrins matrix, OpenGL reproduces the relation between the marker and the camera. Samples in Vuforia uses OpenGL and be implemented in this way.\nThis process is located at renderFrameQCAR method in UserDefinedTargetsEAGLView.mm\nThe process obtaining model view matrix from extrinsic parameter which caluclated by Vuforia is:\nQCAR::Matrix44F modelViewMatrix = QCAR::Tool::convertPose2GLMatrix(result->getPose());`\n\nAnd obtaining perspective projection matrix from intrinsic matrix is:\n&vapp.projectionMatrix.data[0]\n\nvapp is a instance of ApplicationSession. And rest of the codes are OpenGL mumbo jumbo.\n\nAR with SceneKit!\nFinally, let's draw 3D objects in renderFrameQCAR with SceneKit instead of OpenGL.\n\nAdd properties\nFirst, add Scenekit.framework to the project.\nTo draw Scenekit's objects into Vuforia's OpenGL context which draws camera input, We can use SCNRenderer. So add it as property to UserDefinedTargetsEAGLView.h.\n\n#import <SceneKit/SceneKit.h> // Import SceneKit\n\n@interface UserDefinedTargetsEAGLView : UIView <UIGLViewProtocol, GLResourceHandler>\n\n// so much codes...\n\n@property (nonatomic, strong) SCNRenderer *renderer; // renderer\n@property (nonatomic, strong) SCNNode *cameraNode; // node which holds camera\n@property (nonatomic, assign) CFAbsoluteTime startTime; // elapsed time in 3D scene\n\nAnd add datasouce which holds SCNScene (An object which represents scene drawn in SceneKit).\n@protocol UserDefinedTargetsEAGLViewSceneSource <NSObject>\n\n- (SCNScene *)sceneForEAGLView:(UIView *)view;\n\n@end\n\n@property (weak, nonatomic) id<UserDefinedTargetsEAGLViewSceneSource> sceneSource;\n\n\nAbout SCNScene, there are a bunch of the resources on the Internet. So play around with it.\nOne thing, Scenekit's objects size should be 10~100 so it fits Vuforia's extrinsic parameter scale.\n\nInitiate SCNRenderer\n- (void)setupRenderer {\n    self.renderer = [SCNRenderer rendererWithContext:context options:nil];\n    self.renderer.autoenablesDefaultLighting = YES;\n    self.renderer.playing = YES;\n\n    if (self.sceneSource != nil) {\n        self.renderer.scene = [self.sceneSource sceneForEAGLView:self];\n\n        SCNCamera *camera = [SCNCamera camera];\n        self.cameraNode = [SCNNode node];\n        self.cameraNode.camera = camera;\n        [self.renderer.scene.rootNode addChildNode:self.cameraNode];\n        self.renderer.pointOfView = self.cameraNode;\n    }\n\n}\n\n\nCall this method after UserDefinedTargetsEAGLView's - (id)initWithFrame:appSession:. The method is for initiate SCNRenderer and set OpenGL context which is created in the sample and add camera to the scene.\nThe rest is setting intrinsic matrix and extrinsic matrix to the camera. But. Here's a problem. The extrinsic matrix given by Vuforia converts object's coordinate to camera coordinate. In OpenGL, you can use it directly as a model view matrix. But SceneKit does not expose such matrix. So we have to caluclate the camera's position and rotation with respect to the marker. I really tried hard to work it out... and it turns out really simple solution. Just caluclate inverse matrix of the extrinsic matrix and set it to SCNCamera's transform property. That's it.\nCaution: OpenGL and Scenekit matrix's are column-major. You have to be careful about that when you work with element of the matrix.\n\n// Converts Vuforia matrix to SceneKit matrix\n- (SCNMatrix4)SCNMatrix4FromQCARMatrix44:(QCAR::Matrix44F)matrix {\n    GLKMatrix4 glkMatrix;\n\n    for(int i=0; i<16; i++) {\n        glkMatrix.m[i] = matrix.data[i];\n    }\n\n    return SCNMatrix4FromGLKMatrix4(glkMatrix);\n\n}\n\n// Calculate inverse matrix and assign it to cameraNode\n- (void)setCameraMatrix:(QCAR::Matrix44F)matrix {\n    SCNMatrix4 extrinsic = [self SCNMatrix4FromQCARMatrix44:matrix];\n    SCNMatrix4 inverted = SCNMatrix4Invert(extrinsic); // inverse matrix!\n    self.cameraNode.transform = inverted; // assign it to the camera node's transform property.\n}\n\nThen set up perspective projection matrix from the intrinsic matrix.\n- (void)setProjectionMatrix:(QCAR::Matrix44F)matrix {\n    self.cameraNode.camera.projectionTransform = [self SCNMatrix4FromQCARMatrix44:matrix];\n}\n\n\nFinally, lets call these methods in rendeFrameQCAR and render the scene. lets replace renderFrameQCAR like below:\n// *** QCAR will call this method periodically on a background thread ***\n- (void)renderFrameQCAR\n{\n    [self setFramebuffer];\n\n    // Clear colour and depth buffers\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n\n    // Render video background and retrieve tracking state\n    QCAR::State state = QCAR::Renderer::getInstance().begin();\n    QCAR::Renderer::getInstance().drawVideoBackground();\n\n    glEnable(GL_DEPTH_TEST);\n    // We must detect if background reflection is active and adjust the culling direction.\n    // If the reflection is active, this means the pose matrix has been reflected as well,\n    // therefore standard counter clockwise face culling will result in \"inside out\" models.\n    glEnable(GL_CULL_FACE);\n    glCullFace(GL_BACK);\n    if(QCAR::Renderer::getInstance().getVideoBackgroundConfig().mReflection == QCAR::VIDEO_BACKGROUND_REFLECTION_ON)\n        glFrontFace(GL_CW);  //Front camera\n    else\n        glFrontFace(GL_CCW);   //Back camera\n\n    // Render the RefFree UI elements depending on the current state\n    refFreeFrame->render();\n\n    [self setProjectionMatrix:vapp.projectionMatrix]; // Actually you don't have to call that every frame. Because projection matrix does not change.\n\n\n    for (int i = 0; i < state.getNumTrackableResults(); ++i) {\n        // Get the trackable\n        const QCAR::TrackableResult* result = state.getTrackableResult(i);\n        //const QCAR::Trackable& trackable = result->getTrackable();\n        QCAR::Matrix44F modelViewMatrix = QCAR::Tool::convertPose2GLMatrix(result->getPose()); // Obtain model view matrix\n\n        ApplicationUtils::translatePoseMatrix(0.0f, 0.0f, kObjectScale, &modelViewMatrix.data[0]);\n        ApplicationUtils::scalePoseMatrix(kObjectScale, kObjectScale, kObjectScale, &modelViewMatrix.data[0]);\n\n        [self setCameraMatrix:modelViewMatrix]; // SCNCamera\u306b\u30bb\u30c3\u30c8\n        [self.renderer renderAtTime:CFAbsoluteTimeGetCurrent() - self.startTime]; // Render objects into OpenGL context\n\n        ApplicationUtils::checkGlError(\"EAGLView renderFrameQCAR\");\n    }\n\n    glDisable(GL_DEPTH_TEST);\n    glDisable(GL_CULL_FACE);\n\n    QCAR::Renderer::getInstance().end();\n    [self presentFramebuffer];\n\n}\n\n\nVoil\u00e0! With SceneKit, you cand do so much things, lighting, materials, physics simulation, user interaction...\n\nResponse to user tap\nSCNRenderer conforms to SCNSceneRenderer protocol so to get user touch, you can use hitTest:options method. Give touch position as CGPoint to it, it returns SCNNode which corresponds to the point. But be careful, origin of coordinate system in OpenGL is at bottom left and in Retina display, the size of Viewports is doubled. so when you use UITapGestureRecognizer or something like that, you have to double x and y and subtract y from Viewports height, and then pass the point to hitTest:options. \n\nReferences\n\u85e4\u672c \u96c4\u4e00\u90ce, \u9752\u7825 \u9686\u4ec1, \u6d66\u897f \u53cb\u6a39, \u5927\u5009 \u53f2\u751f, \u5c0f\u679d \u6b63\u76f4, \u4e2d\u5cf6 \u60a0\u592a, \u5c71\u672c \u8c6a\u5fd7\u6717, OpenCV3\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30d6\u30c3\u30af\n[Japanese version of the article is also available](http://qiita.com/akira108/items/e5b43810b64cd921a947)\n\n#TL;DR\n\n**Making Augmented Reality app easily with Scenekit + Vuforia**\n\n![Sample GIF movie.gif](https://dl.dropboxusercontent.com/s/fvj9nly9kq4rr5k/scenekit_vuforia_ar_example.gif)\n\n\uff08See [here](https://www.dropbox.com/s/1nuve9mwem1680h/scenekit_vuforia_ar_example.mov?dl=0) for full movie\uff09\n\nAugmented Reality (AR) app are usualy made with OpenGL or Unity. However it's difficult to create complex 3D scene such as scene with physics simulation and so on. With SceneKit, you can create your own AR app with complex scene easily and it can be embedded in usual app using UIKit.\n\n#SceneKit\n> SceneKit is a high-level 3D graphics framework that helps you create 3D animated scenes and effects in your apps. It incorporates a physics engine, a particle generator, and easy ways to script the actions of 3D objects so you can describe your scene in terms of its content \u2014 geometry, materials, lights, and cameras \u2014 then animate it by describing changes to those objects.\n\n[SceneKit - Apple Developer](https://developer.apple.com/scenekit/)\n\nSceneKit is a 3D framework which allows you to create something like 3D video games. It was only for Mac but since iOS8 you can use it in iOS. Add `SCNView` as subview to your view and add models, lights, camera to `SCNView`'s `SCNScene` property, The 3D scene is automatically drawn by the framework. It's really easy. And also it provides a way to combine the scene with OpenGL or Metal. So let's take advangtage of that and add SceneKit object over the video camera input from Vuforia.\n\n#Vuforia\n[Vuforia](http://www.qualcomm.co.jp/products/vuforia)is a mobile vision platform for AR developed by Qualcomm and now it was acquired by PTC. Vuforia provides AR library for iOS, Android and Unity and you can use all fundamental features without a fee (with watermark shown only once a day) Details are [here](https://developer.vuforia.com/pricing)\u3002\n\n**The SDK is provided as a static library (.a) so you can't use Swift. How sad.**\n\n##Bulid samples and run.\n\n1. Visit [Vuforia Developer Portal](https://developer.vuforia.com/pricing), then register and login.\n2. Download [SDK](https://developer.vuforia.com/downloads/sdk) and [Sample projects](https://developer.vuforia.com/downloads/samples).\n3. Move the sample project to `samples` directory in SDK. (official really short description are [here](http://developer.vuforia.com/library/articles/Solution/Installing-the-Vuforia-iOS-Samples)\uff09.\n\n4. Issue Licence Key which will be used in the app at [LicenceManager](https://developer.vuforia.com/targetmanager/licenseManager/licenseListing).\n5. Open `SampleApplicationSession.mm ` in the sample and replace with `QCAR::setInitParameters(mQCARInitFalgs, *Your License Key*)`. (Thanks to the detailed explanation in [[Log] Xcode + vuforia\u3067AR\u30a2\u30d7\u30ea\u3092\u52d5\u304b\u3057\u3066\u307f\u308b - \u3057\u3081\u9bd6\u65e5\u8a18](http://llcc.hatenablog.com/entry/2015/09/26/163950) In Japanese).\n\n6. Build the target with real device (If build a target with iOS Simulator, it fails).\n\n7. Voil\u00e0! Now you can try the sample app at your own iPhone. There are many samples in the app:\n\n###\n* Image Targets - AR with pictures already registered in the Vuforia cloud. It enables highly precise tracking by image processing in the cloud.\n* Cylinder Targets, Multi Targets - Use cylinder or cuboid as a marker. Even solid objects can be used as a marker.\n* User Defined Targets - Users can take a picture includes planar object and Use that as a marker. It's awesome! **From now on, let's integrate SceneKit into User Defined Targets sample.**\n\n#AR principles\n\n##Theory\nThe AR in this context means reconstructing the marker's relative 3D position towards a camera from the 2D image which taken by the camera. The process is:\n\n1. Convert Marker Coordinate (World Coordinate; its origin is a maker's position. 3D.) to Camera Coordinate(its origin is camera's position. 3D), which means get the marker's relative position seen by the camera.\n2. Convert Camera Coordinate (3D) to final 2D coordinate.\n\nThis process can be represented as a formula (using homogeneous coordinates):\n\n![AR_formula.png](https://qiita-image-store.s3.amazonaws.com/0/8012/4f83116f-390d-69db-b6b8-0a60fada97b2.png)\n\n###Intrinsic parameter matrix (intrinsic matrix)\n\nIntrinsic parameter matrix represents the camera's information such as focal length and center coordinate of the image. It's  peculiar to the camera. So your iPhone 6s and my iPhone6s has same intrinsic parameter matrix. Vuforia has a bunch of cameras' intrinsic matrix.\n\nIf intrinsic matrix isn't right, drawn virtual objects and image from camera's perspective does not match and result in weird looking.\n\nIf you interested in calculate intrinsic matrix of unknown camera, look up **Zhang's method**, which is implemented in OpenCV, too.\n\n###Extrinsic parameter matrix (extrinsic matrix)\nExtrinsic parameter matrix represents 3-dimentional rotation and translation. With extrinsic matrix, we can convert a coordinate of the object in Marker Coordinate to Camera Coordinate.\n\nExtrinsic matrix differs by how user hold his/her camera or where marker located so we have to calculate every frame. Vuforia is really good at that. very precise.\n\nTheoretically, extrinsic parameter matrix can be obtained by solving **PnP(Perspective n-Point Problem)**, which is implemented in OpenCV, too.\n\n##Implementation\n\nIn reality, you don't have to do the matrix calculation every frame and leave that to the 3D library. For example, Using OpenGL, set perspective projection matrix using intrinsic matrix and model view matrix using extrins matrix, OpenGL reproduces the relation between the marker and the camera. Samples in Vuforia uses OpenGL and be implemented in this way.\n\nThis process is located at `renderFrameQCAR` method in `UserDefinedTargetsEAGLView.mm`\n\nThe process obtaining model view matrix from extrinsic parameter which caluclated by Vuforia is:\n\n```Objective-C++\nQCAR::Matrix44F modelViewMatrix = QCAR::Tool::convertPose2GLMatrix(result->getPose());`\n```\n\nAnd obtaining perspective projection matrix from intrinsic matrix is:\n\n```Objective-C++\n&vapp.projectionMatrix.data[0]\n```\n\n`vapp` is a instance of `ApplicationSession`. And rest of the codes are OpenGL mumbo jumbo.\n\n#AR with SceneKit!\n\nFinally, let's draw 3D objects in `renderFrameQCAR` with SceneKit instead of OpenGL.\n\n## Add properties\n\nFirst, add Scenekit.framework to the project.\n\nTo draw Scenekit's objects into Vuforia's OpenGL context which draws camera input, We can use `SCNRenderer`. So add it as property to `UserDefinedTargetsEAGLView.h`.\n\n```objc\n\n#import <SceneKit/SceneKit.h> // Import SceneKit\n\n@interface UserDefinedTargetsEAGLView : UIView <UIGLViewProtocol, GLResourceHandler>\n\n// so much codes...\n\n@property (nonatomic, strong) SCNRenderer *renderer; // renderer\n@property (nonatomic, strong) SCNNode *cameraNode; // node which holds camera\n@property (nonatomic, assign) CFAbsoluteTime startTime; // elapsed time in 3D scene\n```\n\nAnd add datasouce which holds SCNScene (An object which represents scene drawn in SceneKit).\n\n```objc\n@protocol UserDefinedTargetsEAGLViewSceneSource <NSObject>\n\n- (SCNScene *)sceneForEAGLView:(UIView *)view;\n\n@end\n\n@property (weak, nonatomic) id<UserDefinedTargetsEAGLViewSceneSource> sceneSource;\n\n```\n\nAbout `SCNScene`, there are a bunch of the resources on the Internet. So play around with it.\nOne thing, Scenekit's objects size should be 10~100 so it fits Vuforia's extrinsic parameter scale.\n\n## Initiate SCNRenderer\n\n```objc\n- (void)setupRenderer {\n    self.renderer = [SCNRenderer rendererWithContext:context options:nil];\n    self.renderer.autoenablesDefaultLighting = YES;\n    self.renderer.playing = YES;\n    \n    if (self.sceneSource != nil) {\n        self.renderer.scene = [self.sceneSource sceneForEAGLView:self];\n        \n        SCNCamera *camera = [SCNCamera camera];\n        self.cameraNode = [SCNNode node];\n        self.cameraNode.camera = camera;\n        [self.renderer.scene.rootNode addChildNode:self.cameraNode];\n        self.renderer.pointOfView = self.cameraNode;\n    }\n    \n}\n\n```\n\nCall this method after `UserDefinedTargetsEAGLView`'s `- (id)initWithFrame:appSession:`. The method is for initiate `SCNRenderer` and set OpenGL context which is created in the sample and add camera to the scene.\n\nThe rest is setting intrinsic matrix and extrinsic matrix to the camera. But. Here's a problem. The extrinsic matrix given by Vuforia converts object's coordinate to camera coordinate. In OpenGL, you can use it directly as a model view matrix. But SceneKit does not expose such matrix. So we have to caluclate the camera's position and rotation with respect to the marker. I really tried hard to work it out... and it turns out really simple solution. Just caluclate **inverse matrix** of the extrinsic matrix and set it to `SCNCamera`'s transform property. That's it.\n\nCaution: OpenGL and Scenekit matrix's are column-major. You have to be careful about that when you work with element of the matrix.\n\n```objc\n\n// Converts Vuforia matrix to SceneKit matrix\n- (SCNMatrix4)SCNMatrix4FromQCARMatrix44:(QCAR::Matrix44F)matrix {\n    GLKMatrix4 glkMatrix;\n    \n    for(int i=0; i<16; i++) {\n        glkMatrix.m[i] = matrix.data[i];\n    }\n    \n    return SCNMatrix4FromGLKMatrix4(glkMatrix);\n    \n}\n\n// Calculate inverse matrix and assign it to cameraNode\n- (void)setCameraMatrix:(QCAR::Matrix44F)matrix {\n    SCNMatrix4 extrinsic = [self SCNMatrix4FromQCARMatrix44:matrix];\n    SCNMatrix4 inverted = SCNMatrix4Invert(extrinsic); // inverse matrix!\n    self.cameraNode.transform = inverted; // assign it to the camera node's transform property.\n}\n```\n\nThen set up perspective projection matrix from the intrinsic matrix.\n\n```objc\n- (void)setProjectionMatrix:(QCAR::Matrix44F)matrix {\n    self.cameraNode.camera.projectionTransform = [self SCNMatrix4FromQCARMatrix44:matrix];\n}\n\n```\n\nFinally, lets call these methods in `rendeFrameQCAR` and render the scene. lets replace `renderFrameQCAR` like below:\n\n```objc\n// *** QCAR will call this method periodically on a background thread ***\n- (void)renderFrameQCAR\n{\n    [self setFramebuffer];\n    \n    // Clear colour and depth buffers\n    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);\n    \n    // Render video background and retrieve tracking state\n    QCAR::State state = QCAR::Renderer::getInstance().begin();\n    QCAR::Renderer::getInstance().drawVideoBackground();\n    \n    glEnable(GL_DEPTH_TEST);\n    // We must detect if background reflection is active and adjust the culling direction.\n    // If the reflection is active, this means the pose matrix has been reflected as well,\n    // therefore standard counter clockwise face culling will result in \"inside out\" models.\n    glEnable(GL_CULL_FACE);\n    glCullFace(GL_BACK);\n    if(QCAR::Renderer::getInstance().getVideoBackgroundConfig().mReflection == QCAR::VIDEO_BACKGROUND_REFLECTION_ON)\n        glFrontFace(GL_CW);  //Front camera\n    else\n        glFrontFace(GL_CCW);   //Back camera\n    \n    // Render the RefFree UI elements depending on the current state\n    refFreeFrame->render();\n    \n    [self setProjectionMatrix:vapp.projectionMatrix]; // Actually you don't have to call that every frame. Because projection matrix does not change.\n    \n    \n    for (int i = 0; i < state.getNumTrackableResults(); ++i) {\n        // Get the trackable\n        const QCAR::TrackableResult* result = state.getTrackableResult(i);\n        //const QCAR::Trackable& trackable = result->getTrackable();\n        QCAR::Matrix44F modelViewMatrix = QCAR::Tool::convertPose2GLMatrix(result->getPose()); // Obtain model view matrix\n        \n        ApplicationUtils::translatePoseMatrix(0.0f, 0.0f, kObjectScale, &modelViewMatrix.data[0]);\n        ApplicationUtils::scalePoseMatrix(kObjectScale, kObjectScale, kObjectScale, &modelViewMatrix.data[0]);\n        \n        [self setCameraMatrix:modelViewMatrix]; // SCNCamera\u306b\u30bb\u30c3\u30c8\n        [self.renderer renderAtTime:CFAbsoluteTimeGetCurrent() - self.startTime]; // Render objects into OpenGL context\n        \n        ApplicationUtils::checkGlError(\"EAGLView renderFrameQCAR\");\n    }\n    \n    glDisable(GL_DEPTH_TEST);\n    glDisable(GL_CULL_FACE);\n    \n    QCAR::Renderer::getInstance().end();\n    [self presentFramebuffer];\n\n}\n\n```\n\nVoil\u00e0! With SceneKit, you cand do so much things, lighting, materials, physics simulation, user interaction...\n\n\n##Response to user tap\n`SCNRenderer` conforms to `SCNSceneRenderer` protocol so to get user touch, you can use `hitTest:options` method. Give touch position as CGPoint to it, it returns `SCNNode` which corresponds to the point. But be careful, origin of coordinate system in OpenGL is at **bottom left** and in Retina display, the size of Viewports is doubled. so when you use `UITapGestureRecognizer` or something like that, you have to double x and y and subtract y from Viewports height, and then pass the point to `hitTest:options`. \n\n#References\n\n[\u85e4\u672c \u96c4\u4e00\u90ce, \u9752\u7825 \u9686\u4ec1, \u6d66\u897f \u53cb\u6a39, \u5927\u5009 \u53f2\u751f, \u5c0f\u679d \u6b63\u76f4, \u4e2d\u5cf6 \u60a0\u592a, \u5c71\u672c \u8c6a\u5fd7\u6717, OpenCV3\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30d6\u30c3\u30af](http://www.amazon.co.jp/OpenCV-3-\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30d6\u30c3\u30af-\u85e4\u672c-\u96c4\u4e00\u90ce/dp/4839952965)\n\n", "tags": ["iOS", "vuforia", "AR", "Objective-C"]}