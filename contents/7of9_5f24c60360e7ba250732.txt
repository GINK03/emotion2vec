{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n\n\nsine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias\u3092C\u8a00\u8a9e\u306e\u30c4\u30fc\u30eb\u3067\u4f7f\u3046\u305f\u3081\u306b\u51fa\u529b\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\nslim.fully_connected()\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u3070\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n\u4e0a\u8a18\u306eslim.fully_connected\u306e\u30ea\u30f3\u30af\u5148\u304c\u4ee5\u4e0b\u3068\u306a\u3063\u3066\u3044\u308b (TensorFlow v0.12)\u3002\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/contrib/layers/python/layers/layers.py\n\nlayers.py\n...\n@add_arg_scope\ndef fully_connected(inputs,\n                    num_outputs,\n                    activation_fn=nn.relu,\n                    normalizer_fn=None,\n                    normalizer_params=None,\n                    weights_initializer=initializers.xavier_initializer(),\n                    weights_regularizer=None,\n                    biases_initializer=init_ops.zeros_initializer,\n                    biases_regularizer=None,\n                    reuse=None,\n                    variables_collections=None,\n                    outputs_collections=None,\n                    trainable=True,\n                    scope=None):\n  \"\"\"Adds a fully connected layer.\n  `fully_connected` creates a variable called `weights`, representing a fully\n  connected weight matrix, which is multiplied by the `inputs` to produce a\n  `Tensor` of hidden units. If a `normalizer_fn` is provided (such as\n  `batch_norm`), it is then applied. Otherwise, if `normalizer_fn` is\n  None and a `biases_initializer` is provided then a `biases` variable would be\n  created and added the hidden units. Finally, if `activation_fn` is not `None`,\n  it is applied to the hidden units as well.\n\n\n\u3053\u3061\u3089\u3067\u4f7f\u3063\u3066\u3044\u308b\u306e\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\nhiddens = slim.stack(input_ph, slim.fully_connected, [7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n...\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n...\n\nslim.stack()\u306f\u8907\u6570\u306eslim.fully_connected()\u306e\u7c21\u7565\u8868\u8a18\u3002\nnormalizer_fn, weights_regularizer, biases_regularizer\u306f\u4f7f\u3063\u3066\u3044\u306a\u3044\u3053\u3068\u306b\u306a\u308b\u3002\nactivation_fn\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eReLU\u3067\u306a\u304fsigmoid\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\u307e\u3068\u3081\u308b\u3068,3\u3064\u306e\u4e2d\u9593\u5c64\u3068\u6700\u5f8c\u306e\u51fa\u529b\u5c64\u3001\u305d\u308c\u305e\u308c\u306b\u304a\u3044\u3066weight\u304c\u304b\u3051\u3089\u308c\u3066\u3001bias\u304c\u52a0\u7b97\u3055\u308c\u3001sigmoid\u3092\u9069\u7528\u3055\u308c\u3066\u3044\u308b\u3002\nweight, bias\u306e\u5024\u3092\u4f7f\u3063\u3066C\u5b9f\u88c5(python\u5b9f\u88c5?)\u3067\u51fa\u529b\u306e\u8a08\u7b97\u3092\u3059\u308b\u3068\u304d\u306f\u540c\u3058\u3088\u3046\u306b\u51e6\u7406\u3092\u3059\u308c\u3070\u3044\u3044\u306f\u305a\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n```\n\nsine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias\u3092C\u8a00\u8a9e\u306e\u30c4\u30fc\u30eb\u3067\u4f7f\u3046\u305f\u3081\u306b\u51fa\u529b\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\n\nslim.fully_connected()\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u3070\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n\u4e0a\u8a18\u306eslim.fully_connected\u306e\u30ea\u30f3\u30af\u5148\u304c\u4ee5\u4e0b\u3068\u306a\u3063\u3066\u3044\u308b (TensorFlow v0.12)\u3002\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/contrib/layers/python/layers/layers.py\n\n```layers.py\n...\n@add_arg_scope\ndef fully_connected(inputs,\n                    num_outputs,\n                    activation_fn=nn.relu,\n                    normalizer_fn=None,\n                    normalizer_params=None,\n                    weights_initializer=initializers.xavier_initializer(),\n                    weights_regularizer=None,\n                    biases_initializer=init_ops.zeros_initializer,\n                    biases_regularizer=None,\n                    reuse=None,\n                    variables_collections=None,\n                    outputs_collections=None,\n                    trainable=True,\n                    scope=None):\n  \"\"\"Adds a fully connected layer.\n  `fully_connected` creates a variable called `weights`, representing a fully\n  connected weight matrix, which is multiplied by the `inputs` to produce a\n  `Tensor` of hidden units. If a `normalizer_fn` is provided (such as\n  `batch_norm`), it is then applied. Otherwise, if `normalizer_fn` is\n  None and a `biases_initializer` is provided then a `biases` variable would be\n  created and added the hidden units. Finally, if `activation_fn` is not `None`,\n  it is applied to the hidden units as well.\n```\n\n\u3053\u3061\u3089\u3067\u4f7f\u3063\u3066\u3044\u308b\u306e\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\n```py\nhiddens = slim.stack(input_ph, slim.fully_connected, [7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n...\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n...\n```\n\nslim.stack()\u306f\u8907\u6570\u306eslim.fully_connected()\u306e\u7c21\u7565\u8868\u8a18\u3002\n\nnormalizer_fn, weights_regularizer, biases_regularizer\u306f\u4f7f\u3063\u3066\u3044\u306a\u3044\u3053\u3068\u306b\u306a\u308b\u3002\nactivation_fn\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eReLU\u3067\u306a\u304fsigmoid\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\n\u307e\u3068\u3081\u308b\u3068,3\u3064\u306e\u4e2d\u9593\u5c64\u3068\u6700\u5f8c\u306e\u51fa\u529b\u5c64\u3001\u305d\u308c\u305e\u308c\u306b\u304a\u3044\u3066weight\u304c\u304b\u3051\u3089\u308c\u3066\u3001bias\u304c\u52a0\u7b97\u3055\u308c\u3001sigmoid\u3092\u9069\u7528\u3055\u308c\u3066\u3044\u308b\u3002\n\nweight, bias\u306e\u5024\u3092\u4f7f\u3063\u3066C\u5b9f\u88c5(python\u5b9f\u88c5?)\u3067\u51fa\u529b\u306e\u8a08\u7b97\u3092\u3059\u308b\u3068\u304d\u306f\u540c\u3058\u3088\u3046\u306b\u51e6\u7406\u3092\u3059\u308c\u3070\u3044\u3044\u306f\u305a\u3002\n\n", "tags": ["TF-Slim", "borgWarp", "link"]}