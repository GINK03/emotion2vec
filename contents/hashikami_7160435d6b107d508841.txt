{"context": "\n\nTask 1\n\nThis task is to build a predictive model which is able to distinguish 0 or 1 based on an input value. \"training-data-small.txt.bz2\" file contains training dataset that consists of output/input pairs (tab-separated). By using the constructed model, generate predictions for each line in \"test-data-small.txt.bz2\" file containing test dataset. Submit your predictions and a report explaining the process for making the model.\n\n\nTechnique and learning process in task 1\nIn order to generate a learning model, I use support vector machine (SVM) 1 which is one of the pattern recognition techniques in supervised learning. For binary classification problem of this task, K-nearest neighbor method, neural network, and deep learning (Restricted boltzmann machine or Deep Belief Networks) techniques are able to applied. In this task, SVM is used from the following reason:\n\nLearning to result in a convex quadratic programming problems by the lagrange multipliers method, obtained an uniquely global optimal solution without optimizing local solution. Therefore, it is possible to stably and also used in the business.\nFeature vector is mapped to the high-dimensional space by the kernel trick that is able to separate non-linear.\nSoft margin SVM is able to deal with training data which contains the noise.\n\n\nProcedure of learning model generated by SVM\n\nPreprocessing\nDesign of SVM\nOptimization and accuracy evaluation of the learning model parameters\n\n\nPreprocessing\nThe training data consists words. It is necessary to binarize words to form of SVM input. A each element of feature vector is represented by boolean whether or not containing the word. Since the data consists 125 words, a dimensional of feature space is 125. In addition, because set of feature vector is sparse, by applying feature hashing method, the dimensional of feature space is compressed. Also, test data is binarized in the same way.\n\nDesign of SVM\nIn order to generate a learning model, I apply non-linear SVM with RBF (Radial Basis Function) kernel to training data. The learning model includes a penalty term to prevent over-fitting. For given set of the $d$ dimensional feature vector $\\mathbf{x}_l$ and label $y_l$ pair, define\n(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_l, y_l) \\\\\n\\forall i, \\mathbf{x}_i \\in \\mathbf{R}_d,y_i \\in \\{\u22121,1\\},\n\nwhere $l$ is the number of set. Linear inseparable SVM constructs maximum-margin hyperplane to maximize the margin is the distance between the hyperplane and the feature vector. The hyperplane is represented by\n\\forall i, g(\\mathbf{x}) = \\mathbf{w}^t\\mathbf{x}_i+b \\left\\{\n\\begin{array}{ll}\n\\geq 1 - \\xi_i & \\mathbf{x}_i \\in X_1 \\\\\n\\leq -1 + \\xi_i & \\mathbf{x}_i \\in X_2\n\\end{array}\n\\right.\n\nIn case of the inseparable non-linear separatable, above formula $\\xi_i$ is $0$ satisfying the formula of $\\mathbf{w}$ does not exist, positive variable $\\xi_i(i=1,\\ldots,l)$ introduced to loosen the conditions. To achieve the maximum margin is the same value to minimize the norm $|w|$. The problem is\nmin. \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{i=l}^{n} \\xi_i \\\\\ns.t. \\forall i, y_i \\cdot (\\mathbf{w}^t\\mathbf{x}_i+b)\u2212(1-\\xi_i) \\geq 0, xi_i \\geq 0\n\nThe first term is for a large margin, the second term is a penalty term for feature vector protruding from the margin, where $C \\in \\mathbf{R}$ is a constant which determines the balance of the first term and the second term. The smaller $g(\\mathbf{x})=\\pm1$ distance between the growing. The solution is obtained by solving by using the method of lagrange multipliers the dual problem 1. Also it obtained a uniquely global optimal solution without falling into local optimal solution to result in a convex quadratic programming problem.\nSVM is limited because of a linear classifier. So combining the kernel method 2. Inner product in minimization problem is replaced by a kernel function kernel method. In this challenge RBF kernel $exp(\u2212\\frac{1}{2\\sigma^2}|x_i \u2212 x_j|^2)=exp(\u2212\\gamma|x_i \u2212 x_j|^2)$ was used. By the kernel function, feature vectors are mapped to higher-dimensional space is linearly separable in the space. The SVM allowing nonlinear separated by this mapping.\n\nOptimization and accuracy evaluation of the learning model parameters\nCalculate parameters of the constant $C$ and RBF kernel of the penalty term $\\gamma\\in\\mathbf{R}$. The optimization of parameters using cross-validation (K-fold method), an output classification accuracy to employ a set of highest parameter. In this task, 3-fold.\nThe results are evaluated using the correct rate (accuracy). The accuracy is a measure which is represented by recall and precision.\n\nResult of task 1\nI experimented on the performance of the learning model measured by accuracy to verify the possibility of applying SVM to training data. The hyperplane penalty term constant $C$ was $2$, and the RBF kernel parameter $\\gamma$ was $0.5$. Accuracy was 67.5%. The results of cross-validation is shown in following table.\n\n\n\nC\n\u03b3\nAccuracy\n\n\n\n\n1\n0.5\n0.675\n\n\n1\n0.707\n0.668\n\n\n1\n1.0\n0.658\n\n\n1\n1.414\n0.655\n\n\n1\n2.0\n0.651\n\n\n1\n2.828\n0.648\n\n\n2\n0.5\n0.675\n\n\n2\n0.707\n0.666\n\n\n2\n1\n0.661\n\n\n2\n1.414\n0.656\n\n\n2\n2\n0.653\n\n\n2\n2.828\n0.648\n\n\n4\n0.5\n0.671\n\n\n4\n0.707\n0.666\n\n\n4\n1\n0.660\n\n\n4\n1.414\n0.657\n\n\n4\n2\n0.653\n\n\n4\n2.828\n0.648\n\n\n\n\nResult of predictions\nThe results of applying the learning model to the test data are shown in result-for-small-test-data.txt which are stored in Dropbox. In 100,000 test data, 81,775 data classified to 0, and 18,225 data classified to 1. Processing time of learning and reasoning was 326 [sec].\n\nTask 2\n\nThe task is similar with the exception of the dataset (note: the dataset is significantly larger than the earlier one). Training dataset is in \"training-data-large.txt.bz2\", and test dataset is in \"test-data-large.txt.bz2\".\n\n\nPreprocessing for task 2\nThe learning of SVM have to solve a quadratic programming problem, requires a huge amount of calculations, there is a problem that not suitable for learning of the large scale data. Platt has proposed SMO (Sequential Minimal Optimization) performing weight update repeating selection by limiting the training data to be updated in two. In addition, for the unbalanced training data, the under-sampling 3 often is applied such as One-Sided Selection. In Task 2, by sampling, deleting redundant data, and compressing the dimension of feature space by feature hashing, learn efficiently.\n\nResult of task 2\nThe hyperplane penalty term constant $C$ was $1$, and the RBF kernel parameter $\\gamma$ was $2.8284$. Accuracy was 71.1%. The results of cross-validation is shown in following table.\n\n\n\nC\n\u03b3\nAccuracy\n\n\n\n\n1\n0.5\n0.672\n\n\n1\n0.707\n0.686\n\n\n1\n1.0\n0.697\n\n\n1\n1.414\n0.706\n\n\n1\n2.0\n0.711\n\n\n1\n2.828\n0.711\n\n\n2\n0.5\n0.691\n\n\n2\n0.707\n0.703\n\n\n2\n1\n0.709\n\n\n2\n1.414\n0.707\n\n\n2\n2\n0.706\n\n\n2\n2.828\n0.711\n\n\n4\n0.5\n0.706\n\n\n4\n0.707\n0.710\n\n\n4\n1\n0.709\n\n\n4\n1.414\n0.707\n\n\n4\n2\n0.706\n\n\n4\n2.828\n0.711\n\n\n\n\nResult of predictions\nThe results for test data are shown in result-for-large-test-data.txt which are stored in Dropbox. In 100,000 test data, 81,775 data classified to 0, and 5,481 data classified to 1. Processing time of learning and reasoning was 2720 [sec].\n\nExperiment environment\nThe experiment was conducted using Intel Core i5 2.5GHz CPU with 8GB memory size running OS X, with the algorithm implemented in Python language. scikit-learn4 was used as library of SVM.\n\nwork.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nprint(__doc__)\n\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn import cross_validation\nfrom sklearn import grid_search\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\nimport argparse\nimport logging\nimport time\nimport csv\n\n\ndef init_logger():\n    global logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    log_fmt = '%(asctime)s/%(name)s[%(levelname)s]: %(message)s'\n    logging.basicConfig(format=log_fmt)\n\ndef __pad_feature_vector(x):\n    x = x.rstrip('\\n').split(',')\n    x = {i: x.count(i) for i in set(x)}\n    return x\n\ndef load_train_data(file):\n    X = []\n    y = []\n    f = open(file, 'r')\n    for l in f.readlines():\n        train = l.split()\n        y.append(int(train[0]))\n        X.append(__pad_feature_vector(train[1]))\n    f.close()\n    return X, y\n\ndef load_test_data(file):\n    X = []\n    f = open(file, 'r')\n    for l in f.readlines():\n        X.append(__pad_feature_vector(l))\n    f.close()\n    return X\n\ndef get_cross_validation_score(clf, train_features, train_labels, test_size):\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(train_features, train_labels, test_size=test_size, random_state=0)\n    clf.fit(X_train, y_train)\n    print clf.score(X_test, y_test)\n\ndef get_cross_validation_accuracy(clf, train_features, train_labels):\n    scores = cross_validation.cross_val_score(clf, train_features, train_labels, cv=10)\n    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\ndef get_best_estimator_by_grid_search(train_X, train_y):\n    u\"\"\"grid_search\n    Grid search to estimate kernel params\n    \"\"\"\n    params = [\n        {'C': [2**i for i in range(0, 3, 1)], 'gamma': [2**i for i in np.arange(-1, 2, 0.5)], 'kernel': ['rbf']},\n    ]\n    method = svm.SVC()\n    gscv = grid_search.GridSearchCV(method, params, scoring='accuracy', n_jobs=9)\n    gscv.fit(train_X, train_y)\n    for params, mean_score, all_scores in gscv.grid_scores_:\n        logger.info('{:.3f} (+/- {:.3f}) for {}'.format(mean_score, all_scores.std() / 2, params))\n    logger.info('params:{params}'.format(params=gscv.best_params_))\n    logger.info('score:{params}'.format(params=gscv.best_score_))\n    # pred = gscv.best_estimator_.predict_proba(test_X)\n\ndef work2(X, y, test_X):\n    from unbalanced_dataset import UnderSampler, OneSidedSelection\n    sampling_interval = 50\n    X = X[::sampling_interval]\n    y = y[::sampling_interval]\n    # dvec = DictVectorizer(sparse=True)\n    # X = dvec.fit_transform(X)\n    # test_X = dvec.transform(test_X)\n    # X = np.array(X.toarray())\n    # y = np.array(y)\n    # verbose = False\n    # US = UnderSampler(verbose=verbose)\n    # X, y = US.fit_transform(X, y)\n    # X = dvec.inverse_transform(X)\n    # y = y.tolist()\n    return X, y, test_X\n\ndef main(args):\n    # Load training data and format training vectors X and class labels y\n    X, y = load_train_data(args.train_file)\n    # Load test data\n    test_X = load_test_data(args.test_file)\n\n    # For work. Under sampling\n    # X, y, __test_X = work2(X, y, test_X)\n\n    # Feature hashing\n    hasher = FeatureHasher(n_features=16)\n    X = hasher.transform(X)\n    test_X = hasher.transform(test_X)\n    logger.debug(u'\u4e0d\u5747\u8861\u30c7\u30fc\u30bf 0: ' + str(y.count(0)) + ' 1: ' + str(y.count(1)))\n\n    # SVM by RBF (Gaussian) kernel with optimized C and gamma.\n    # Use grid search to estimate params\n    get_best_estimator_by_grid_search(X, y) \n    clf = svm.SVC(kernel='rbf', C=2, gamma=0.5)\n    clf.fit(X, y)\n\n    # Predict class labels of test data\n    result = clf.predict(test_X)\n    logger.debug(result)\n    logger.debug('0: ' + str(list(result).count(0)) + ' 1: ' + str(list(result).count(1)))\n    return result\n\n\nif __name__ == '__main__':\n    u\"\"\"__main__\n     Usage: python work.py training-data-small.txt test-data-small.txt -o result.txt\n    \"\"\"\n    init_logger()\n    parser = argparse.ArgumentParser()\n    parser.add_argument('train_file')\n    parser.add_argument('test_file')\n    parser.add_argument('-o', '--output', required=True)\n    args = parser.parse_args()\n\n    start_time = time.time()\n\n    result = main(args)\n\n    elapsed_time = time.time() - start_time\n    logger.info((\"{0}\".format(elapsed_time)) + '[sec]')\n\n    test_file = open(args.test_file, 'r')\n    test_data = test_file.readlines()\n    test_file.close()\n    output_file = open(args.output, 'w')\n    for i, class_label in enumerate(result):\n        output_file.write(str(class_label) + '\\t' + test_data[i])\n    output_file.close()\n\n\n\nConclusions\nAccuracy was as low as around 70% in the learning of the task. Because the identity of data was unknown, the preprocessing was only dimension compression by hash trick. By cleaning such as deletion of unnecessary words and weighting of the word in accordance with the data characteristics, it is able to improve performance. Further, Kamei proposed sampling technique 5, considered it can be learned without compromising performance. In the learning algorithm, recent active research such as Transfer learning (technique which can solve the problem of deep learning that is falling to a local optimal solution) may improve performance.\n\nReferences\n\n\n\n\n\u524d\u7530 \u82f1\u4f5c, ``\u75db\u5feb\uff01\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30c8\u30eb\u30de\u30b7\u30f3 -\u53e4\u304f\u3066\u65b0\u3057\u3044\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u624b\u6cd5-'', \u60c5\u5831\u51e6\u7406, 42(7), 676--683, 2001.\u00a0\u21a9\n\n\nH. Takamura, ``Constructive Induction and Text Categorization with SVMs'', 44, J.IPS Japan, 2003.\u00a0\u21a9\n\n\nUnbalancedDataset\u00a0\u21a9\n\n\nscikit-learn\u00a0\u21a9\n\n\nY. Kamei, ``Applying Sampling Methods to Fault-prone Module Detection,'' J.IPS Japan, Vol.48, No.8, pp.2651-2662, 2007.\u00a0\u21a9\n\n\n\n# Task 1\n> This task is to build a predictive model which is able to distinguish 0 or 1 based on an input value. \"training-data-small.txt.bz2\" file contains training dataset that consists of output/input pairs (tab-separated). By using the constructed model, generate predictions for each line in \"test-data-small.txt.bz2\" file containing test dataset. Submit your predictions and a report explaining the process for making the model.\n\n# Technique and learning process in task 1\nIn order to generate a learning model, I use support vector machine (SVM) [^1] which is one of the pattern recognition techniques in supervised learning. For binary classification problem of this task, K-nearest neighbor method, neural network, and deep learning (Restricted boltzmann machine or Deep Belief Networks) techniques are able to applied. In this task, SVM is used from the following reason:\n\n- Learning to result in a convex quadratic programming problems by the lagrange multipliers method, obtained an uniquely global optimal solution without optimizing local solution. Therefore, it is possible to stably and also used in the business.\n- Feature vector is mapped to the high-dimensional space by the kernel trick that is able to separate non-linear.\n- Soft margin SVM is able to deal with training data which contains the noise.\n\n## Procedure of learning model generated by SVM\n1. Preprocessing\n2. Design of SVM\n3. Optimization and accuracy evaluation of the learning model parameters\n\n### Preprocessing\nThe training data consists words. It is necessary to binarize words to form of SVM input. A each element of feature vector is represented by boolean whether or not containing the word. Since the data consists 125 words, a dimensional of feature space is 125. In addition, because set of feature vector is sparse, by applying feature hashing method, the dimensional of feature space is compressed. Also, test data is binarized in the same way.\n\n### Design of SVM\nIn order to generate a learning model, I apply non-linear SVM with RBF (Radial Basis Function) kernel to training data. The learning model includes a penalty term to prevent over-fitting. For given set of the $d$ dimensional feature vector $\\mathbf{x}_l$ and label $y_l$ pair, define\n\n```math\n(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_l, y_l) \\\\\n\\forall i, \\mathbf{x}_i \\in \\mathbf{R}_d,y_i \\in \\{\u22121,1\\},\n```\n\nwhere $l$ is the number of set. Linear inseparable SVM constructs maximum-margin hyperplane to maximize the margin is the distance between the hyperplane and the feature vector. The hyperplane is represented by\n\n```math\n\\forall i, g(\\mathbf{x}) = \\mathbf{w}^t\\mathbf{x}_i+b \\left\\{\n\\begin{array}{ll}\n\\geq 1 - \\xi_i & \\mathbf{x}_i \\in X_1 \\\\\n\\leq -1 + \\xi_i & \\mathbf{x}_i \\in X_2\n\\end{array}\n\\right.\n```\n\nIn case of the inseparable non-linear separatable, above formula $\\xi_i$ is $0$ satisfying the formula of $\\mathbf{w}$ does not exist, positive variable $\\xi_i(i=1,\\ldots,l)$ introduced to loosen the conditions. To achieve the maximum margin is the same value to minimize the norm $\\|w\\|$. The problem is\n\n```math\nmin. \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{i=l}^{n} \\xi_i \\\\\ns.t. \\forall i, y_i \\cdot (\\mathbf{w}^t\\mathbf{x}_i+b)\u2212(1-\\xi_i) \\geq 0, xi_i \\geq 0\n```\n\nThe first term is for a large margin, the second term is a penalty term for feature vector protruding from the margin, where $C \\in \\mathbf{R}$ is a constant which determines the balance of the first term and the second term. The smaller $g(\\mathbf{x})=\\pm1$ distance between the growing. The solution is obtained by solving by using the method of lagrange multipliers the dual problem [^1]. Also it obtained a uniquely global optimal solution without falling into local optimal solution to result in a convex quadratic programming problem.\nSVM is limited because of a linear classifier. So combining the kernel method [^3]. Inner product in minimization problem is replaced by a kernel function kernel method. In this challenge RBF kernel $exp(\u2212\\frac{1}{2\\sigma^2}|x_i \u2212 x_j|^2)=exp(\u2212\\gamma|x_i \u2212 x_j|^2)$ was used. By the kernel function, feature vectors are mapped to higher-dimensional space is linearly separable in the space. The SVM allowing nonlinear separated by this mapping.\n\n\n### Optimization and accuracy evaluation of the learning model parameters\nCalculate parameters of the constant $C$ and RBF kernel of the penalty term $\\gamma\\in\\mathbf{R}$. The optimization of parameters using cross-validation (K-fold method), an output classification accuracy to employ a set of highest parameter. In this task, 3-fold.\nThe results are evaluated using the correct rate (accuracy). The accuracy is a measure which is represented by recall and precision.\n\n\n# Result of task 1\nI experimented on the performance of the learning model measured by accuracy to verify the possibility of applying SVM to training data. The hyperplane penalty term constant $C$ was $2$, and the RBF kernel parameter $\\gamma$ was $0.5$. Accuracy was 67.5%. The results of cross-validation is shown in following table.\n\n|  **C** | **\u03b3** | **Accuracy** |\n|  :------: | :------: | :------: |\n|  1 | 0.5 | 0.675 |\n|  1 | 0.707 | 0.668 |\n|  1 | 1.0 | 0.658 |\n|  1 | 1.414 | 0.655 |\n|  1 | 2.0 | 0.651 |\n|  1 | 2.828 | 0.648 |\n| **2** | **0.5** | **0.675** |\n|  2 |  0.707 | 0.666 |\n|  2 | 1 | 0.661 |\n|  2 | 1.414 | 0.656 |\n|  2 | 2 | 0.653 |\n|  2 | 2.828 | 0.648 |\n|  4 | 0.5 | 0.671 |\n|  4 |  0.707 | 0.666 |\n|  4 | 1 | 0.660 |\n|  4 | 1.414 | 0.657 |\n|  4 | 2 | 0.653 |\n|  4 | 2.828 | 0.648 |\n\n### Result of predictions\nThe results of applying the learning model to the test data are shown in result-for-small-test-data.txt which are stored in Dropbox. In 100,000 test data, 81,775 data classified to 0, and 18,225 data classified to 1. Processing time of learning and reasoning was 326 [sec].\n\n# Task 2\n> The task is similar with the exception of the dataset (note: the dataset is significantly larger than the earlier one). Training dataset is in \"training-data-large.txt.bz2\", and test dataset is in \"test-data-large.txt.bz2\".\n\n# Preprocessing for task 2\nThe learning of SVM have to solve a quadratic programming problem, requires a huge amount of calculations, there is a problem that not suitable for learning of the large scale data. Platt has proposed SMO (Sequential Minimal Optimization) performing weight update repeating selection by limiting the training data to be updated in two. In addition, for the unbalanced training data, the under-sampling [^6] often is applied such as One-Sided Selection. In Task 2, by sampling, deleting redundant data, and compressing the dimension of feature space by feature hashing, learn efficiently.\n\n# Result of task 2\nThe hyperplane penalty term constant $C$ was $1$, and the RBF kernel parameter $\\gamma$ was $2.8284$. Accuracy was 71.1%. The results of cross-validation is shown in following table.\n\n|  **C** | **\u03b3** | **Accuracy** |\n|  :------: | :------: | :------: |\n|  1 | 0.5 | 0.672 |\n|  1 | 0.707 | 0.686 |\n|  1 | 1.0 | 0.697 |\n|  1 | 1.414 | 0.706 |\n|  1 | 2.0 | 0.711 |\n|  **1** | **2.828** | **0.711** |\n| 2 | 0.5 | 0.691 |\n|  2 |  0.707 | 0.703 |\n|  2 | 1 | 0.709 |\n|  2 | 1.414 | 0.707 |\n|  2 | 2 | 0.706 |\n|  2 | 2.828 | 0.711 |\n|  4 | 0.5 | 0.706 |\n|  4 |  0.707 | 0.710 |\n|  4 | 1 | 0.709 |\n|  4 | 1.414 | 0.707 |\n|  4 | 2 | 0.706 |\n|  4 | 2.828 | 0.711 |\n\n### Result of predictions\nThe results for test data are shown in result-for-large-test-data.txt which are stored in Dropbox. In 100,000 test data, 81,775 data classified to 0, and 5,481 data classified to 1. Processing time of learning and reasoning was 2720 [sec].\n\n\n# Experiment environment\nThe experiment was conducted using Intel Core i5 2.5GHz CPU with 8GB memory size running OS X, with the algorithm implemented in Python language. scikit-learn[^4] was used as library of SVM.\n\n```py:work.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nprint(__doc__)\n\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn import cross_validation\nfrom sklearn import grid_search\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\nimport argparse\nimport logging\nimport time\nimport csv\n\n\ndef init_logger():\n    global logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    log_fmt = '%(asctime)s/%(name)s[%(levelname)s]: %(message)s'\n    logging.basicConfig(format=log_fmt)\n\ndef __pad_feature_vector(x):\n    x = x.rstrip('\\n').split(',')\n    x = {i: x.count(i) for i in set(x)}\n    return x\n\ndef load_train_data(file):\n    X = []\n    y = []\n    f = open(file, 'r')\n    for l in f.readlines():\n        train = l.split()\n        y.append(int(train[0]))\n        X.append(__pad_feature_vector(train[1]))\n    f.close()\n    return X, y\n\ndef load_test_data(file):\n    X = []\n    f = open(file, 'r')\n    for l in f.readlines():\n        X.append(__pad_feature_vector(l))\n    f.close()\n    return X\n\ndef get_cross_validation_score(clf, train_features, train_labels, test_size):\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(train_features, train_labels, test_size=test_size, random_state=0)\n    clf.fit(X_train, y_train)\n    print clf.score(X_test, y_test)\n\ndef get_cross_validation_accuracy(clf, train_features, train_labels):\n    scores = cross_validation.cross_val_score(clf, train_features, train_labels, cv=10)\n    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\ndef get_best_estimator_by_grid_search(train_X, train_y):\n    u\"\"\"grid_search\n    Grid search to estimate kernel params\n    \"\"\"\n    params = [\n        {'C': [2**i for i in range(0, 3, 1)], 'gamma': [2**i for i in np.arange(-1, 2, 0.5)], 'kernel': ['rbf']},\n    ]\n    method = svm.SVC()\n    gscv = grid_search.GridSearchCV(method, params, scoring='accuracy', n_jobs=9)\n    gscv.fit(train_X, train_y)\n    for params, mean_score, all_scores in gscv.grid_scores_:\n        logger.info('{:.3f} (+/- {:.3f}) for {}'.format(mean_score, all_scores.std() / 2, params))\n    logger.info('params:{params}'.format(params=gscv.best_params_))\n    logger.info('score:{params}'.format(params=gscv.best_score_))\n    # pred = gscv.best_estimator_.predict_proba(test_X)\n\ndef work2(X, y, test_X):\n    from unbalanced_dataset import UnderSampler, OneSidedSelection\n    sampling_interval = 50\n    X = X[::sampling_interval]\n    y = y[::sampling_interval]\n    # dvec = DictVectorizer(sparse=True)\n    # X = dvec.fit_transform(X)\n    # test_X = dvec.transform(test_X)\n    # X = np.array(X.toarray())\n    # y = np.array(y)\n    # verbose = False\n    # US = UnderSampler(verbose=verbose)\n    # X, y = US.fit_transform(X, y)\n    # X = dvec.inverse_transform(X)\n    # y = y.tolist()\n    return X, y, test_X\n\ndef main(args):\n    # Load training data and format training vectors X and class labels y\n    X, y = load_train_data(args.train_file)\n    # Load test data\n    test_X = load_test_data(args.test_file)\n\n    # For work. Under sampling\n    # X, y, __test_X = work2(X, y, test_X)\n\n    # Feature hashing\n    hasher = FeatureHasher(n_features=16)\n    X = hasher.transform(X)\n    test_X = hasher.transform(test_X)\n    logger.debug(u'\u4e0d\u5747\u8861\u30c7\u30fc\u30bf 0: ' + str(y.count(0)) + ' 1: ' + str(y.count(1)))\n\n    # SVM by RBF (Gaussian) kernel with optimized C and gamma.\n    # Use grid search to estimate params\n    get_best_estimator_by_grid_search(X, y) \n    clf = svm.SVC(kernel='rbf', C=2, gamma=0.5)\n    clf.fit(X, y)\n\n    # Predict class labels of test data\n    result = clf.predict(test_X)\n    logger.debug(result)\n    logger.debug('0: ' + str(list(result).count(0)) + ' 1: ' + str(list(result).count(1)))\n    return result\n\n\nif __name__ == '__main__':\n    u\"\"\"__main__\n     Usage: python work.py training-data-small.txt test-data-small.txt -o result.txt\n    \"\"\"\n    init_logger()\n    parser = argparse.ArgumentParser()\n    parser.add_argument('train_file')\n    parser.add_argument('test_file')\n    parser.add_argument('-o', '--output', required=True)\n    args = parser.parse_args()\n\n    start_time = time.time()\n    \n    result = main(args)\n    \n    elapsed_time = time.time() - start_time\n    logger.info((\"{0}\".format(elapsed_time)) + '[sec]')\n    \n    test_file = open(args.test_file, 'r')\n    test_data = test_file.readlines()\n    test_file.close()\n    output_file = open(args.output, 'w')\n    for i, class_label in enumerate(result):\n        output_file.write(str(class_label) + '\\t' + test_data[i])\n    output_file.close()\n```\n\n# Conclusions\nAccuracy was as low as around 70% in the learning of the task. Because the identity of data was unknown, the preprocessing was only dimension compression by hash trick. By cleaning such as deletion of unnecessary words and weighting of the word in accordance with the data characteristics, it is able to improve performance. Further, Kamei proposed sampling technique [^7], considered it can be learned without compromising performance. In the learning algorithm, recent active research such as Transfer learning (technique which can solve the problem of deep learning that is falling to a local optimal solution) may improve performance.\n\n\n# References\n[^1]: \u524d\u7530 \u82f1\u4f5c, ``\u75db\u5feb\uff01\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30c8\u30eb\u30de\u30b7\u30f3 -\u53e4\u304f\u3066\u65b0\u3057\u3044\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u624b\u6cd5-'', \u60c5\u5831\u51e6\u7406, 42(7), 676--683, 2001.\n[^2]: K. Weinberger, ``Featurehashing for large scale multitask learning'', ICML, 2009.\n[^3]: H. Takamura, ``Constructive Induction and Text Categorization with SVMs'', 44, J.IPS Japan, 2003.\n[^4]: [scikit-learn](http://scikit-learn.org/)\n[^5]: Jason Yosinski, ``How transferable are features in deep neural networks?'', 2014. \n[^6]: [UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)\n[^7]: Y. Kamei, ``Applying Sampling Methods to Fault-prone Module Detection,'' J.IPS Japan, Vol.48, No.8, pp.2651-2662, 2007.\n[^8]: [SVM\u3092\u4f7f\u3044\u3053\u306a\u3059\uff01\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c88\u3064](http://qiita.com/pika_shi/items/5e59bcf69e85fdd9edb2)\n[^9]: [\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30c7\u30fc\u30bf\u5206\u6790\u307e\u308f\u308a\u306e\u304a\u8a71](http://www.slideshare.net/canard0328/ss-44288984)\n", "tags": ["LINE,", "svm"]}