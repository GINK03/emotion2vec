{"context": " More than 1 year has passed since last update.Run Scrapy from a script\nScrapy\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3042\u308b\u3088\u3046\u306a\u30b9\u30af\u30ea\u30d7\u30c8\u3060\u3068\u3001\u306a\u305c\u304b\u3061\u3083\u3093\u3068\u52d5\u304b\u306a\u304b\u3063\u305f\u3002\n\uff08\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u5bfe\u8c61\u306f\u65e5\u672c\u8a9e\u306e\u30b5\u30a4\u30c8\uff09\nspider = FollowAllSpider(domain='scrapinghub.com')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run()\n\n\u305d\u3053\u3067\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30c4\u30fc\u30eb(scrapy crawl hogehoge)\u304b\u3089\u306e\u5b9f\u884c\u3068\u540c\u3058\u3088\u3046\u306b\u3001Log\u5468\u308a\u306e\u8a18\u8ff0\u3092\u4fee\u6b63\u3057\u305f\u3089\u52d5\u3044\u305f\u3002\nspider = FollowAllSpider(domain='scrapinghub.com')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\nlog.start_from_crawler(crawler)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nreactor.run()\n\n\u539f\u56e0\u306f\u3001log.start()\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001log\u304ccrawler\u3092\u53d7\u3051\u53d6\u3089\u306a\u3044\u304b\u3089\u3063\u307d\u3044\u3002\n\u304c\u3001crawler\u304c\u306a\u3044\u3068\u52d5\u304b\u306a\u3044\u7406\u7531\u306f\u4e0d\u660e\u3002\n\nscrapy/log.py\ndef start_from_crawler(crawler):\n    return start_from_settings(crawler.settings, crawler)\n\n\n[Run Scrapy from a script](http://doc.scrapy.org/en/latest/topics/practices.html)\n\nScrapy\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3042\u308b\u3088\u3046\u306a\u30b9\u30af\u30ea\u30d7\u30c8\u3060\u3068\u3001\u306a\u305c\u304b\u3061\u3083\u3093\u3068\u52d5\u304b\u306a\u304b\u3063\u305f\u3002\n\uff08\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u5bfe\u8c61\u306f\u65e5\u672c\u8a9e\u306e\u30b5\u30a4\u30c8\uff09\n\n```py\nspider = FollowAllSpider(domain='scrapinghub.com')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run()\n```\n\n\u305d\u3053\u3067\u3001\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30c4\u30fc\u30eb(scrapy crawl hogehoge)\u304b\u3089\u306e\u5b9f\u884c\u3068\u540c\u3058\u3088\u3046\u306b\u3001Log\u5468\u308a\u306e\u8a18\u8ff0\u3092\u4fee\u6b63\u3057\u305f\u3089\u52d5\u3044\u305f\u3002\n\n```py\nspider = FollowAllSpider(domain='scrapinghub.com')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\nlog.start_from_crawler(crawler)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nreactor.run()\n```\n\n\u539f\u56e0\u306f\u3001log.start()\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001log\u304ccrawler\u3092\u53d7\u3051\u53d6\u3089\u306a\u3044\u304b\u3089\u3063\u307d\u3044\u3002\n\u304c\u3001crawler\u304c\u306a\u3044\u3068\u52d5\u304b\u306a\u3044\u7406\u7531\u306f\u4e0d\u660e\u3002\n\n```py:scrapy/log.py\ndef start_from_crawler(crawler):\n    return start_from_settings(crawler.settings, crawler)\n```", "tags": ["Python2.7", "Scrapy0.22"]}