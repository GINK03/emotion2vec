{"context": "\u524d\u56de (Optimizer : \u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u52fe\u914d\u6cd5\u306b\u3064\u3044\u3066) \u306e\u7d9a\u304d\u3067\u3059\u3002\n\n\u306f\u3058\u3081\u306b\n\u4eca\u56de\u306f\u524d\u56de\u7d39\u4ecb\u3057\u305f\u5404\u6700\u9069\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\u3092\u3001\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u691c\u8a3c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u4f7f\u7528\u3057\u305f\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u304a\u306a\u3058\u307fMNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u753b\u50cf\u3067\u3001\u7c21\u5358\u306aCNN (Convolutional Neural Network) \u306b\u3088\u3063\u3066\u5206\u985e\u7cbe\u5ea6\u3092\u6307\u6a19\u306b\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002\u524d\u56de\u3082\u8efd\u304f\u89e6\u308c\u307e\u3057\u305f\u304c\u3001\u3042\u304f\u307e\u3067\u3053\u306e\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u6700\u9069\u5316\u624b\u6cd5\u3092\u691c\u8a3c\u3059\u308b\u5185\u5bb9\u306b\u306a\u308b\u305f\u3081\u3001\u4ed6\u306e\u30e2\u30c7\u30eb\u306b\u7528\u3044\u308b\u5834\u5408\u306f\u53c2\u8003\u306b\u3059\u308b\u7a0b\u5ea6\u306b\u7559\u3081\u305f\u65b9\u304c\u826f\u3044\u3068\u81ea\u5206\u306f\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\nMNIST\u306b\u3064\u3044\u3066\n\u4eca\u56de\u7528\u3044\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf\u306fMNIST (Mixed National Institute of Standards and Technology database) \u3068\u3044\u3046\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf\u304c\u4fdd\u7ba1\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u306f8bit (\u753b\u7d20\u5024 : 0-255) \u306e\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb28\u00d728\u30d4\u30af\u30bb\u30eb\u306e\u753b\u50cf\u304c70000\u30b5\u30f3\u30d7\u30eb\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u6570\u5b57\u306e\u7a2e\u985e\u306f0-9\u306e10\u30af\u30e9\u30b9\u5b58\u5728\u3057\u3001\u753b\u50cf\u306f\u4e00\u6b21\u5143\u914d\u5217\u3000(784\u8981\u7d20)\u3000\u3068\u3057\u3066\u7528\u610f\u3055\u308c\u3066\u304a\u308a\u3001\u5168\u3066\u306b\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u4ed8\u52a0\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3069\u306e\u3088\u3046\u306a\u753b\u50cf\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001MNIST\u306e\u4e2d\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u308a\u51fa\u3057\u305f\u753b\u50cf\u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3059\u3002\u753b\u50cf\u306e\u4e0a\u306b\u8868\u793a\u3055\u308c\u305f\u5024\u304c\u6b63\u89e3\u30e9\u30d9\u30eb\u3067\u3059\u3002\n\n\u624b\u66f8\u304d\u6570\u5b57\u3068\u3060\u3051\u3042\u3063\u3066\u3001\u7d50\u69cb\u4eba\u306e\u76ee\u3067\u3082\u5224\u5225\u3057\u306b\u304f\u3044\u753b\u50cf\u304c\u3042\u3063\u305f\u308a\u3057\u307e\u3059\u3002\n\u3053\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u5206\u985e\u30bf\u30b9\u30af\u3092\u5404\u6700\u9069\u5316\u624b\u6cd5\u3092\u7528\u3044\u305fCNN\u3067\u884c\u3044\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nCNN\u306e\u69cb\u9020\u306b\u3064\u3044\u3066\n\u4eca\u56de\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u306b\u3064\u3044\u3066\u3067\u3059\u3002\u6982\u7565\u56f3\u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3057\u305f\u3002\n\n\u6d3b\u6027\u5316\u95a2\u6570\u306fLeRU\u3001\u51fa\u529b\u95a2\u6570\u306fSoftMax\u3092\u7528\u3044\u3066\u3044\u307e\u3059\u3002\u7573\u307f\u8fbc\u307f\u5c64(Convolutional Layer)\u306f\u3001\u3069\u3061\u3089\u3082filter = 28, kernel size = 5\u3001\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64(Pooling Layer)\u306f\u3001\u4e00\u5c64\u76ee\u304ckernel size = 2, stride = 2\u3001\u4e8c\u5c64\u76ee\u304ckernel size = 3, stride = 3\u3067\u3059\u3002\u5168\u7d50\u5408\u5c64(fully-connected Layer)\u306f1\u5c64\u3067252units\u3067\u3059\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u9020\u3092\u3069\u306e\u3088\u3046\u306b\u6c7a\u3081\u308b\u304b\u3068\u3044\u3046\u3053\u3068\u306b\u95a2\u3057\u3066\u3001\u672a\u3060\u306b\u3054\u308a\u3054\u308a\u306e\u30ed\u30fc\u30e9\u30fc\u4f5c\u6226\u3067\u6700\u9069\u5316\u3057\u3066\u307e\u3059\u3002\u3002\u753b\u50cf\u306e\u5927\u304d\u3055\u3084\u7279\u5fb4\u306b\u3088\u3063\u3066\u3001\u7573\u307f\u8fbc\u307f\u5c64\u3084\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u306f\u3042\u308b\u7a0b\u5ea6\u7d4c\u9a13\u7684\u306b\u6c7a\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u304c\u3001\u5c64\u6570\u3084\u30e6\u30cb\u30c3\u30c8\u6570\u306a\u3069\u306b\u95a2\u3057\u3066\u306f\u672a\u3060\u306b\u3053\u308c\u3068\u3044\u3063\u305f\u7d4c\u9a13\u7684\u306a\u6307\u6a19\u3082\u63b4\u3081\u3066\u3044\u306a\u3044\u3067\u3059\u3002\u6700\u8fd1\u69d8\u3005\u306a\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u624b\u6cd5\u304c\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u52c9\u5f37\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u8a55\u4fa1\n\u5404Optimizer (SGD, Momentum SGD, AdaGrad, RMSprop, AdaDelta, Adam)\u306b\u3064\u3044\u30665\u4ea4\u5dee\u691c\u5b9a\u306b\u3088\u308b\u8a55\u4fa1\u3092\u884c\u3044\u307e\u3057\u305f\u3002\u5404Optimizer\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3092\u7528\u3044\u3066\u3044\u307e\u3059\u3002\u7d50\u679c\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\n\n\n\nOptimizers\nAccuracy\nVariance\n\n\n\n\nSGD\n0.989\n3.571443e-07\u3000\u3000\n\n\nMomentumSGD\n0.998685714286\n6.785714e-08\u3000\u3000\n\n\nAdaGrad\n0.974685714286\n8.229592e-07\u3000\u3000\n\n\nRMSprop\n0.981014285714\n3.247449e-06\u3000\u3000\n\n\nAdaDelta\n0.999828571429\n6.632653e-09\u3000\u3000\n\n\nAdam\n0.999471428571\n7.918367e-07\u3000\u3000\n\n\n\n\u7d50\u679c\u306fAdaDelta\u304c\u4e00\u756a\u9ad8\u3044\u7cbe\u5ea6\u3092\u51fa\u3057\u3001\u6b21\u70b9\u3067Adam\u3067\u3057\u305f\u3002\u4e00\u5fdc\u5206\u6563\u3082\u51fa\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u3053\u308c\u3082AdaDelta\u304c\u4e00\u756a\u5c0f\u3055\u3044\u3053\u3068\u304b\u3089\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u4ee5\u4e0a\u306e\u3053\u3068\u304b\u3089\u3001\u4eca\u56de\u306e\u624b\u66f8\u304d\u6570\u5b5710\u30af\u30e9\u30b9\u5206\u985e\u3068\u3044\u3046\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u306f\u3001\u5206\u6563\u30fb\u7cbe\u5ea6\u3068\u3082\u306bAdaDelta\u304c\u6700\u3082\u9069\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\n\u3053\u306e\u7d50\u679c\u304c\u51fa\u308b\u307e\u3067\u306f\u3001\u3069\u3046\u305bAdam\u306e\u5727\u52dd\u3060\u308d\u30fc\u306a\u30fc\u3063\u3066\u8efd\u304f\u8003\u3048\u3066\u3044\u305f\u3060\u3051\u306b\u9762\u767d\u3044\u7d50\u679c\u304c\u51fa\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u4eca\u56de\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306e\u5834\u5408\u3001Adam\u306e\u30d0\u30a4\u30a2\u30b9\u3092\u6253\u3061\u6d88\u3059\u9805\u304c\u9006\u306b\u5b66\u7fd2\u306e\u53ce\u675f\u307e\u3067\u8db3\u3092\u5f15\u3063\u5f35\u3063\u3066\u3057\u307e\u3046\u306e\u3067\u3057\u3087\u3046\u304b\u3002\u305d\u3082\u305d\u3082Chainer\u306eAdam\u304c\u81ea\u5206\u304c\u8003\u3048\u3066\u3044\u308b\u52d5\u304d\u65b9\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u3082\u4e0d\u5b89\u306a\u306e\u3067\u3059\u304c\u3001\u3001\n\u5358\u7d14\u306bAdam\u3092\u4f7f\u3063\u3066\u304a\u3051\u3070\u3044\u3044\u3068\u3044\u3046\u8a33\u3067\u306f\u306a\u3055\u305d\u3046\u3067\u3059\u306d\u3002\n\n\u307e\u3068\u3081\n\u6700\u521d\u306b\u3082\u8ff0\u3079\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306e\u624b\u66f8\u304d\u6570\u5b57\u5206\u985e\u554f\u984c\u306b\u95a2\u3057\u3066\u306f\u3053\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u51fa\u307e\u3057\u305f\u304c\u3001\u3042\u304f\u307e\u3067\u3053\u306e\u554f\u984c\u306b\u304a\u3051\u308b\u7d50\u679c\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u5ff5\u982d\u306b\u7f6e\u304b\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4ed6\u306e\u554f\u984c\u306b\u9069\u7528\u3059\u308b\u969b\u306b\u3001\u53c2\u8003\u306b\u306a\u308c\u3070\u3044\u3044\u304b\u306a\u3068\u3002Optimizer\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3082\u3075\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u306e\u3067\u3002\n\u3061\u306a\u307f\u306b\u4eca\u56de\u306e\u5b9f\u884c\u7d50\u679c\u306f\u51fa\u529b\u3057\u7d42\u308f\u308b\u307e\u30671\u9031\u9593\u4ee5\u4e0a\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002GPU\u306b\u5bfe\u5fdc\u3057\u306a\u3051\u308c\u3070\u3068\u3044\u3046\u3053\u3068\u3082\u3042\u308b\u306e\u3067\u3059\u304c\u3001\u6700\u9069\u5316\u3092\u3069\u3053\u307e\u3067\u884c\u3046\u304b\u3001\u3072\u3044\u3066\u306f\u3069\u306e\u7a0b\u5ea6\u3067\u59a5\u5354\u3059\u308b\u304b\u3001\u3068\u3044\u3046\u3053\u3068\u3082\u5b66\u7fd2\u3055\u305b\u308b\u4e0a\u3067\u6c17\u3092\u3064\u3051\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u30dd\u30a4\u30f3\u30c8\u3060\u3068\u601d\u3044\u307e\u3059\u3002\u5b66\u7fd2\u306f\u3001\u6b63\u7b54\u7387\u3092\u3042\u3052\u3088\u3046\u3068\u601d\u3048\u3070\u3044\u304f\u3089\u3067\u3082\u3075\u308c\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u3001\u5b66\u7fd2\u3055\u305b\u308b\u4e0a\u3067\u4f55\u304c\u91cd\u8981\u3067\u3042\u308b\u304b\u3057\u3063\u304b\u308a\u3068\u898b\u6975\u3081\u308b\u3053\u3068\u304c\u5927\u4e8b\u3060\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u6642\u9593\u304c\u8db3\u308a\u306a\u3055\u3059\u304e\u308b\u3002\u3002\n\u6700\u5f8c\u306b\u3001\u9593\u9055\u3044\u3084\u6307\u6458\u3059\u308b\u70b9\u304c\u4f55\u304b\u3042\u308a\u307e\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\nAppendix\n\u4eca\u56de\u8a55\u4fa1\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u3092\u4ee5\u4e0b\u306b\u8f09\u305b\u307e\u3059\u3002Chainer1.1.1\u306e\u77e5\u8b58\u3057\u304b\u306a\u3044\u305f\u3081\u5b9f\u88c5\u304c\u9045\u308c\u3066\u3044\u308b\u90e8\u5206\u304c\u591a\u3044\u3067\u3059\u3002\u3002\nhttps://github.com/tokkuman/MNIST_Opitmizer_Evaluation\n\noptimizer_evalution.py\n\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom sklearn.datasets import fetch_mldata\nfrom chainer import cuda, Variable, FunctionSet, optimizers\nimport chainer.functions as F\nimport sys\nimport cv2\nimport copy\nimport csv\nimport pylab as plt\n\nplt.style.use('ggplot')\n\ndef cross_split(data, label, k_cross, n, perm, N, length):\n    x, y = [], []\n    for i in range(k_cross):\n        x.append(mnist.data[perm[i*N/cross:(i+1)*N/cross]])\n        y.append(mnist.target[perm[i*N/cross:(i+1)*N/cross]])\n    x_train, y_train = [], []\n    for i in range(k_cross):\n        if i == n:\n            x_test = copy.deepcopy(x[i])\n            y_test = copy.deepcopy(y[i])\n        else:\n            x_train.append(copy.deepcopy(x[i]))\n            y_train.append(copy.deepcopy(y[i]))\n    x_train = np.array(x_train).reshape(N*(k_cross-1)/k_cross, 1, length, length)\n    y_train = np.array(y_train).reshape(N*(k_cross-1)/k_cross)\n    x_test = x_test.reshape(N/k_cross, 1, length, length)\n    return copy.deepcopy(x_train), copy.deepcopy(x_test), copy.deepcopy(y_train), copy.deepcopy(y_test)\n\n\ndef forward(x_data, y_data, train=True):\n    x, t = Variable(x_data), Variable(y_data)\n    h1 = F.max_pooling_2d(F.relu(model.conv1(x)), ksize=2, stride=2)\n    h2 = F.max_pooling_2d(F.relu(model.conv2(h1)), ksize=3, stride=3)\n    h3 = F.dropout(F.relu(model.l3(h2)), train=train)\n    y  = model.l4(h3)\n\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n\ndef cross_optimizers(opt):\n    if opt == 'SGD':\n        optimizer = optimizers.SGD()\n    elif opt == 'MomentumSGD':\n        optimizer = optimizers.MomentumSGD()\n    elif opt == 'AdaGrad':\n        optimizer = optimizers.AdaGrad()\n    elif opt == 'RMSprop':\n        optimizer = optimizers.RMSprop()\n    elif opt == 'AdaDelta':\n        optimizer = optimizers.AdaDelta()\n    elif opt == 'Adam':\n        optimizer = optimizers.Adam()\n    return copy.deepcopy(optimizer)\n\n\nif __name__ == '__main__':\n    mnist = fetch_mldata('MNIST original')\n    mnist.data = mnist.data.astype(np.float32)\n    mnist.target = mnist.target.astype(np.int32)\n    mnist.data /= mnist.data.max()\n    n_epoch = 50\n    batchsize = 100\n    cross = 5   # 5 Cross Validation\n    optimizer_list = ['SGD', 'MomentumSGD', 'AdaGrad', 'RMSprop', 'AdaDelta', 'Adam']   # Optimizers List\n\n    opt_train_loss = []\n    opt_train_acc  = []\n    opt_test_loss = []\n    opt_test_acc  = []\n    N, imagesize = mnist.data.shape\n    length = int(np.sqrt(imagesize))\n    cross_perm = np.random.permutation(N)\n    for opt in optimizer_list:\n        print '========================='\n        print 'Set Optimizer : ' + opt\n        cross_acc_sum = 0\n        cross_train_loss = []\n        cross_train_acc  = []\n        cross_test_loss = []\n        cross_test_acc  = []\n        for k in range(cross):\n            print '-------------------------'\n            print 'Cross Validation : ' + str(k + 1)\n            model = FunctionSet(\n                conv1 = F.Convolution2D(1, 28, 5),\n                conv2 = F.Convolution2D(28, 28, 5),\n                l3 = F.Linear(252, 252),\n                l4 = F.Linear(252, 10) )\n            optimizer = cross_optimizers(opt)\n            optimizer.setup(model)\n            x_train, x_test, y_train, y_test = cross_split(mnist.data, mnist.target, cross, k, cross_perm, N, length)\n            N_train = x_train.shape[0]\n            cross_acc = 0\n            train_loss = []\n            train_acc  = []\n            test_loss = []\n            test_acc  = []\n            for epoch in range(1, n_epoch+1):\n                print 'epoch' + str(epoch)\n                loss_sum, acc_sum = 0, 0\n                perm = np.random.permutation(N_train)\n                for i in range(0, N_train, batchsize):\n                    x_batch = x_train[perm[i:i+batchsize]]\n                    y_batch = y_train[perm[i:i+batchsize]]\n                    optimizer.zero_grads()\n                    loss, acc = forward(x_batch, y_batch)\n                    loss.backward()\n                    optimizer.update()\n                    real_batchsize = len(x_batch)\n                    loss_sum += float(cuda.to_cpu(loss.data)) * real_batchsize\n                    acc_sum += float(cuda.to_cpu(acc.data)) * real_batchsize\n                print 'Train Mean Loss={}, Accuracy={}'.format(loss_sum / N_train, acc_sum / N_train)\n                train_loss.append(loss_sum / N_train)\n                train_acc.append(1 - (acc_sum / N_train))\n                N_test = x_test.shape[0]\n                loss_sum, acc_sum = 0, 0\n                for i in range(0, N_test):\n                    x_batch = x_train[i].reshape(1, 1, length, length)\n                    y_batch = np.array(y_train[i]).reshape(1)\n                    loss, acc = forward(x_batch, y_batch)\n                    loss_sum += float(cuda.to_cpu(loss.data))\n                    acc_sum += float(cuda.to_cpu(acc.data))\n                print 'Test Mean Loss={}, Accuracy={}'.format(loss_sum / N_test, acc_sum / N_test)\n                test_loss.append(loss_sum / N_test)\n                test_acc.append(1 - (acc_sum / N_test))\n                if cross_acc <= acc_sum / N_test:\n                    cross_acc = acc_sum / N_test\n            cross_acc_sum += cross_acc\n            cross_train_loss.append(train_loss)\n            cross_train_acc.append(train_acc)\n            cross_test_loss.append(test_loss)\n            cross_test_acc.append(test_acc)            \n        print '====Cross Validation===='\n        print opt + ' 5 Cross Validation Mean Accuracy : ' + str(cross_acc_sum / cross)\n        opt_train_loss.append(cross_train_loss)\n        opt_train_acc.append(cross_train_acc)\n        opt_test_loss.append(cross_test_loss)\n        opt_test_acc.append(cross_test_acc)\n\n    f = open('opt_train_loss.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_train_loss)\n    f.close()\n    f = open('opt_train_acc.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_train_acc)\n    f.close()\n    f = open('opt_test_loss.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_test_loss)\n    f.close()\n    f = open('opt_test_acc.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_test_acc)\n    f.close()\n\n\n[\u524d\u56de (Optimizer : \u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u52fe\u914d\u6cd5\u306b\u3064\u3044\u3066) ](http://qiita.com/tokkuman/items/1944c00415d129ca0ee9)\u306e\u7d9a\u304d\u3067\u3059\u3002\n\n# \u306f\u3058\u3081\u306b\n\u4eca\u56de\u306f\u524d\u56de\u7d39\u4ecb\u3057\u305f\u5404\u6700\u9069\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\u3092\u3001\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u691c\u8a3c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u4f7f\u7528\u3057\u305f\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u304a\u306a\u3058\u307fMNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u753b\u50cf\u3067\u3001\u7c21\u5358\u306aCNN (Convolutional Neural Network) \u306b\u3088\u3063\u3066\u5206\u985e\u7cbe\u5ea6\u3092\u6307\u6a19\u306b\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002\u524d\u56de\u3082\u8efd\u304f\u89e6\u308c\u307e\u3057\u305f\u304c\u3001\u3042\u304f\u307e\u3067\u3053\u306e\u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u6700\u9069\u5316\u624b\u6cd5\u3092\u691c\u8a3c\u3059\u308b\u5185\u5bb9\u306b\u306a\u308b\u305f\u3081\u3001\u4ed6\u306e\u30e2\u30c7\u30eb\u306b\u7528\u3044\u308b\u5834\u5408\u306f\u53c2\u8003\u306b\u3059\u308b\u7a0b\u5ea6\u306b\u7559\u3081\u305f\u65b9\u304c\u826f\u3044\u3068\u81ea\u5206\u306f\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\n# MNIST\u306b\u3064\u3044\u3066\n\u4eca\u56de\u7528\u3044\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f[MNIST (Mixed National Institute of Standards and Technology database)](http://yann.lecun.com/exdb/mnist/) \u3068\u3044\u3046\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf\u304c\u4fdd\u7ba1\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u306f8bit (\u753b\u7d20\u5024 : 0-255) \u306e\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb28\u00d728\u30d4\u30af\u30bb\u30eb\u306e\u753b\u50cf\u304c70000\u30b5\u30f3\u30d7\u30eb\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u6570\u5b57\u306e\u7a2e\u985e\u306f0-9\u306e10\u30af\u30e9\u30b9\u5b58\u5728\u3057\u3001\u753b\u50cf\u306f\u4e00\u6b21\u5143\u914d\u5217\u3000(784\u8981\u7d20)\u3000\u3068\u3057\u3066\u7528\u610f\u3055\u308c\u3066\u304a\u308a\u3001\u5168\u3066\u306b\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u4ed8\u52a0\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3069\u306e\u3088\u3046\u306a\u753b\u50cf\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001MNIST\u306e\u4e2d\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u308a\u51fa\u3057\u305f\u753b\u50cf\u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3059\u3002\u753b\u50cf\u306e\u4e0a\u306b\u8868\u793a\u3055\u308c\u305f\u5024\u304c\u6b63\u89e3\u30e9\u30d9\u30eb\u3067\u3059\u3002\n![mnist.png](https://qiita-image-store.s3.amazonaws.com/0/137574/fd312ced-89d6-90d7-6273-27f8bd3278c9.png)\n\n\u624b\u66f8\u304d\u6570\u5b57\u3068\u3060\u3051\u3042\u3063\u3066\u3001\u7d50\u69cb\u4eba\u306e\u76ee\u3067\u3082\u5224\u5225\u3057\u306b\u304f\u3044\u753b\u50cf\u304c\u3042\u3063\u305f\u308a\u3057\u307e\u3059\u3002\n\u3053\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u5206\u985e\u30bf\u30b9\u30af\u3092\u5404\u6700\u9069\u5316\u624b\u6cd5\u3092\u7528\u3044\u305fCNN\u3067\u884c\u3044\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n# CNN\u306e\u69cb\u9020\u306b\u3064\u3044\u3066\n\u4eca\u56de\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u306b\u3064\u3044\u3066\u3067\u3059\u3002\u6982\u7565\u56f3\u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3057\u305f\u3002\n![kouzou.001.png](https://qiita-image-store.s3.amazonaws.com/0/137574/816dc0a1-2e57-90f4-8eb0-05667380534b.png)\n\u6d3b\u6027\u5316\u95a2\u6570\u306fLeRU\u3001\u51fa\u529b\u95a2\u6570\u306fSoftMax\u3092\u7528\u3044\u3066\u3044\u307e\u3059\u3002\u7573\u307f\u8fbc\u307f\u5c64(Convolutional Layer)\u306f\u3001\u3069\u3061\u3089\u3082filter = 28, kernel size = 5\u3001\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64(Pooling Layer)\u306f\u3001\u4e00\u5c64\u76ee\u304ckernel size = 2, stride = 2\u3001\u4e8c\u5c64\u76ee\u304ckernel size = 3, stride = 3\u3067\u3059\u3002\u5168\u7d50\u5408\u5c64(fully-connected Layer)\u306f1\u5c64\u3067252units\u3067\u3059\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u9020\u3092\u3069\u306e\u3088\u3046\u306b\u6c7a\u3081\u308b\u304b\u3068\u3044\u3046\u3053\u3068\u306b\u95a2\u3057\u3066\u3001\u672a\u3060\u306b\u3054\u308a\u3054\u308a\u306e\u30ed\u30fc\u30e9\u30fc\u4f5c\u6226\u3067\u6700\u9069\u5316\u3057\u3066\u307e\u3059\u3002\u3002\u753b\u50cf\u306e\u5927\u304d\u3055\u3084\u7279\u5fb4\u306b\u3088\u3063\u3066\u3001\u7573\u307f\u8fbc\u307f\u5c64\u3084\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u306f\u3042\u308b\u7a0b\u5ea6\u7d4c\u9a13\u7684\u306b\u6c7a\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u304c\u3001\u5c64\u6570\u3084\u30e6\u30cb\u30c3\u30c8\u6570\u306a\u3069\u306b\u95a2\u3057\u3066\u306f\u672a\u3060\u306b\u3053\u308c\u3068\u3044\u3063\u305f\u7d4c\u9a13\u7684\u306a\u6307\u6a19\u3082\u63b4\u3081\u3066\u3044\u306a\u3044\u3067\u3059\u3002\u6700\u8fd1\u69d8\u3005\u306a\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u624b\u6cd5\u304c\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u52c9\u5f37\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n# \u8a55\u4fa1\n\u5404Optimizer (SGD, Momentum SGD, AdaGrad, RMSprop, AdaDelta, Adam)\u306b\u3064\u3044\u30665\u4ea4\u5dee\u691c\u5b9a\u306b\u3088\u308b\u8a55\u4fa1\u3092\u884c\u3044\u307e\u3057\u305f\u3002\u5404Optimizer\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3092\u7528\u3044\u3066\u3044\u307e\u3059\u3002\u7d50\u679c\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\n![acc.png](https://qiita-image-store.s3.amazonaws.com/0/137574/62263fdf-4a6c-2012-a7b1-0f13901dab32.png)\n\n| Optimizers | Accuracy    | Variance |\n|:-----------|:------------|:------------:|\n| SGD        |0.989          |3.571443e-07\u3000\u3000|\n| MomentumSGD|0.998685714286 |6.785714e-08\u3000\u3000|\n| AdaGrad    |0.974685714286 |8.229592e-07\u3000\u3000|\n| RMSprop    |0.981014285714 |3.247449e-06\u3000\u3000|\n| AdaDelta   |0.999828571429 |6.632653e-09\u3000\u3000|\n| Adam       |0.999471428571 |7.918367e-07\u3000\u3000|\n\n\u7d50\u679c\u306fAdaDelta\u304c\u4e00\u756a\u9ad8\u3044\u7cbe\u5ea6\u3092\u51fa\u3057\u3001\u6b21\u70b9\u3067Adam\u3067\u3057\u305f\u3002\u4e00\u5fdc\u5206\u6563\u3082\u51fa\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u3053\u308c\u3082AdaDelta\u304c\u4e00\u756a\u5c0f\u3055\u3044\u3053\u3068\u304b\u3089\u5b89\u5b9a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u4ee5\u4e0a\u306e\u3053\u3068\u304b\u3089\u3001\u4eca\u56de\u306e\u624b\u66f8\u304d\u6570\u5b5710\u30af\u30e9\u30b9\u5206\u985e\u3068\u3044\u3046\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u306f\u3001\u5206\u6563\u30fb\u7cbe\u5ea6\u3068\u3082\u306bAdaDelta\u304c\u6700\u3082\u9069\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\n\u3053\u306e\u7d50\u679c\u304c\u51fa\u308b\u307e\u3067\u306f\u3001\u3069\u3046\u305bAdam\u306e\u5727\u52dd\u3060\u308d\u30fc\u306a\u30fc\u3063\u3066\u8efd\u304f\u8003\u3048\u3066\u3044\u305f\u3060\u3051\u306b\u9762\u767d\u3044\u7d50\u679c\u304c\u51fa\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u4eca\u56de\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306e\u5834\u5408\u3001Adam\u306e\u30d0\u30a4\u30a2\u30b9\u3092\u6253\u3061\u6d88\u3059\u9805\u304c\u9006\u306b\u5b66\u7fd2\u306e\u53ce\u675f\u307e\u3067\u8db3\u3092\u5f15\u3063\u5f35\u3063\u3066\u3057\u307e\u3046\u306e\u3067\u3057\u3087\u3046\u304b\u3002\u305d\u3082\u305d\u3082Chainer\u306eAdam\u304c\u81ea\u5206\u304c\u8003\u3048\u3066\u3044\u308b\u52d5\u304d\u65b9\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u3082\u4e0d\u5b89\u306a\u306e\u3067\u3059\u304c\u3001\u3001\n\u5358\u7d14\u306bAdam\u3092\u4f7f\u3063\u3066\u304a\u3051\u3070\u3044\u3044\u3068\u3044\u3046\u8a33\u3067\u306f\u306a\u3055\u305d\u3046\u3067\u3059\u306d\u3002\n\n\n# \u307e\u3068\u3081\n\u6700\u521d\u306b\u3082\u8ff0\u3079\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306e\u624b\u66f8\u304d\u6570\u5b57\u5206\u985e\u554f\u984c\u306b\u95a2\u3057\u3066\u306f\u3053\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u51fa\u307e\u3057\u305f\u304c\u3001\u3042\u304f\u307e\u3067\u3053\u306e\u554f\u984c\u306b\u304a\u3051\u308b\u7d50\u679c\u3067\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u306f\u5ff5\u982d\u306b\u7f6e\u304b\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4ed6\u306e\u554f\u984c\u306b\u9069\u7528\u3059\u308b\u969b\u306b\u3001\u53c2\u8003\u306b\u306a\u308c\u3070\u3044\u3044\u304b\u306a\u3068\u3002Optimizer\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3082\u3075\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u306e\u3067\u3002\n\u3061\u306a\u307f\u306b\u4eca\u56de\u306e\u5b9f\u884c\u7d50\u679c\u306f\u51fa\u529b\u3057\u7d42\u308f\u308b\u307e\u30671\u9031\u9593\u4ee5\u4e0a\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002GPU\u306b\u5bfe\u5fdc\u3057\u306a\u3051\u308c\u3070\u3068\u3044\u3046\u3053\u3068\u3082\u3042\u308b\u306e\u3067\u3059\u304c\u3001\u6700\u9069\u5316\u3092\u3069\u3053\u307e\u3067\u884c\u3046\u304b\u3001\u3072\u3044\u3066\u306f\u3069\u306e\u7a0b\u5ea6\u3067\u59a5\u5354\u3059\u308b\u304b\u3001\u3068\u3044\u3046\u3053\u3068\u3082\u5b66\u7fd2\u3055\u305b\u308b\u4e0a\u3067\u6c17\u3092\u3064\u3051\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u30dd\u30a4\u30f3\u30c8\u3060\u3068\u601d\u3044\u307e\u3059\u3002\u5b66\u7fd2\u306f\u3001\u6b63\u7b54\u7387\u3092\u3042\u3052\u3088\u3046\u3068\u601d\u3048\u3070\u3044\u304f\u3089\u3067\u3082\u3075\u308c\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u5b58\u5728\u3059\u308b\u305f\u3081\u3001\u5b66\u7fd2\u3055\u305b\u308b\u4e0a\u3067\u4f55\u304c\u91cd\u8981\u3067\u3042\u308b\u304b\u3057\u3063\u304b\u308a\u3068\u898b\u6975\u3081\u308b\u3053\u3068\u304c\u5927\u4e8b\u3060\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\u6642\u9593\u304c\u8db3\u308a\u306a\u3055\u3059\u304e\u308b\u3002\u3002\n\n\u6700\u5f8c\u306b\u3001\u9593\u9055\u3044\u3084\u6307\u6458\u3059\u308b\u70b9\u304c\u4f55\u304b\u3042\u308a\u307e\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\n\n# Appendix\n\u4eca\u56de\u8a55\u4fa1\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u3092\u4ee5\u4e0b\u306b\u8f09\u305b\u307e\u3059\u3002Chainer1.1.1\u306e\u77e5\u8b58\u3057\u304b\u306a\u3044\u305f\u3081\u5b9f\u88c5\u304c\u9045\u308c\u3066\u3044\u308b\u90e8\u5206\u304c\u591a\u3044\u3067\u3059\u3002\u3002\nhttps://github.com/tokkuman/MNIST_Opitmizer_Evaluation\n\n```py:optimizer_evalution.py\n\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom sklearn.datasets import fetch_mldata\nfrom chainer import cuda, Variable, FunctionSet, optimizers\nimport chainer.functions as F\nimport sys\nimport cv2\nimport copy\nimport csv\nimport pylab as plt\n\nplt.style.use('ggplot')\n\ndef cross_split(data, label, k_cross, n, perm, N, length):\n    x, y = [], []\n    for i in range(k_cross):\n        x.append(mnist.data[perm[i*N/cross:(i+1)*N/cross]])\n        y.append(mnist.target[perm[i*N/cross:(i+1)*N/cross]])\n    x_train, y_train = [], []\n    for i in range(k_cross):\n        if i == n:\n            x_test = copy.deepcopy(x[i])\n            y_test = copy.deepcopy(y[i])\n        else:\n            x_train.append(copy.deepcopy(x[i]))\n            y_train.append(copy.deepcopy(y[i]))\n    x_train = np.array(x_train).reshape(N*(k_cross-1)/k_cross, 1, length, length)\n    y_train = np.array(y_train).reshape(N*(k_cross-1)/k_cross)\n    x_test = x_test.reshape(N/k_cross, 1, length, length)\n    return copy.deepcopy(x_train), copy.deepcopy(x_test), copy.deepcopy(y_train), copy.deepcopy(y_test)\n\n\ndef forward(x_data, y_data, train=True):\n    x, t = Variable(x_data), Variable(y_data)\n    h1 = F.max_pooling_2d(F.relu(model.conv1(x)), ksize=2, stride=2)\n    h2 = F.max_pooling_2d(F.relu(model.conv2(h1)), ksize=3, stride=3)\n    h3 = F.dropout(F.relu(model.l3(h2)), train=train)\n    y  = model.l4(h3)\n    \n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n\ndef cross_optimizers(opt):\n    if opt == 'SGD':\n        optimizer = optimizers.SGD()\n    elif opt == 'MomentumSGD':\n        optimizer = optimizers.MomentumSGD()\n    elif opt == 'AdaGrad':\n        optimizer = optimizers.AdaGrad()\n    elif opt == 'RMSprop':\n        optimizer = optimizers.RMSprop()\n    elif opt == 'AdaDelta':\n        optimizer = optimizers.AdaDelta()\n    elif opt == 'Adam':\n        optimizer = optimizers.Adam()\n    return copy.deepcopy(optimizer)\n\n\nif __name__ == '__main__':\n    mnist = fetch_mldata('MNIST original')\n    mnist.data = mnist.data.astype(np.float32)\n    mnist.target = mnist.target.astype(np.int32)\n    mnist.data /= mnist.data.max()\n    n_epoch = 50\n    batchsize = 100\n    cross = 5   # 5 Cross Validation\n    optimizer_list = ['SGD', 'MomentumSGD', 'AdaGrad', 'RMSprop', 'AdaDelta', 'Adam']   # Optimizers List\n\n    opt_train_loss = []\n    opt_train_acc  = []\n    opt_test_loss = []\n    opt_test_acc  = []\n    N, imagesize = mnist.data.shape\n    length = int(np.sqrt(imagesize))\n    cross_perm = np.random.permutation(N)\n    for opt in optimizer_list:\n        print '========================='\n        print 'Set Optimizer : ' + opt\n        cross_acc_sum = 0\n        cross_train_loss = []\n        cross_train_acc  = []\n        cross_test_loss = []\n        cross_test_acc  = []\n        for k in range(cross):\n            print '-------------------------'\n            print 'Cross Validation : ' + str(k + 1)\n            model = FunctionSet(\n                conv1 = F.Convolution2D(1, 28, 5),\n                conv2 = F.Convolution2D(28, 28, 5),\n                l3 = F.Linear(252, 252),\n                l4 = F.Linear(252, 10) )\n            optimizer = cross_optimizers(opt)\n            optimizer.setup(model)\n            x_train, x_test, y_train, y_test = cross_split(mnist.data, mnist.target, cross, k, cross_perm, N, length)\n            N_train = x_train.shape[0]\n            cross_acc = 0\n            train_loss = []\n            train_acc  = []\n            test_loss = []\n            test_acc  = []\n            for epoch in range(1, n_epoch+1):\n                print 'epoch' + str(epoch)\n                loss_sum, acc_sum = 0, 0\n                perm = np.random.permutation(N_train)\n                for i in range(0, N_train, batchsize):\n                    x_batch = x_train[perm[i:i+batchsize]]\n                    y_batch = y_train[perm[i:i+batchsize]]\n                    optimizer.zero_grads()\n                    loss, acc = forward(x_batch, y_batch)\n                    loss.backward()\n                    optimizer.update()\n                    real_batchsize = len(x_batch)\n                    loss_sum += float(cuda.to_cpu(loss.data)) * real_batchsize\n                    acc_sum += float(cuda.to_cpu(acc.data)) * real_batchsize\n                print 'Train Mean Loss={}, Accuracy={}'.format(loss_sum / N_train, acc_sum / N_train)\n                train_loss.append(loss_sum / N_train)\n                train_acc.append(1 - (acc_sum / N_train))\n                N_test = x_test.shape[0]\n                loss_sum, acc_sum = 0, 0\n                for i in range(0, N_test):\n                    x_batch = x_train[i].reshape(1, 1, length, length)\n                    y_batch = np.array(y_train[i]).reshape(1)\n                    loss, acc = forward(x_batch, y_batch)\n                    loss_sum += float(cuda.to_cpu(loss.data))\n                    acc_sum += float(cuda.to_cpu(acc.data))\n                print 'Test Mean Loss={}, Accuracy={}'.format(loss_sum / N_test, acc_sum / N_test)\n                test_loss.append(loss_sum / N_test)\n                test_acc.append(1 - (acc_sum / N_test))\n                if cross_acc <= acc_sum / N_test:\n                    cross_acc = acc_sum / N_test\n            cross_acc_sum += cross_acc\n            cross_train_loss.append(train_loss)\n            cross_train_acc.append(train_acc)\n            cross_test_loss.append(test_loss)\n            cross_test_acc.append(test_acc)            \n        print '====Cross Validation===='\n        print opt + ' 5 Cross Validation Mean Accuracy : ' + str(cross_acc_sum / cross)\n        opt_train_loss.append(cross_train_loss)\n        opt_train_acc.append(cross_train_acc)\n        opt_test_loss.append(cross_test_loss)\n        opt_test_acc.append(cross_test_acc)\n\n    f = open('opt_train_loss.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_train_loss)\n    f.close()\n    f = open('opt_train_acc.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_train_acc)\n    f.close()\n    f = open('opt_test_loss.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_test_loss)\n    f.close()\n    f = open('opt_test_acc.csv', 'ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(opt_test_acc)\n    f.close()\n```\n", "tags": ["MachineLearning", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "\u6df1\u5c64\u5b66\u7fd2", "Chainer"]}