{"context": "\n\nSlim\u3068\u306f\n\nTF-Slim is a lightweight library for defining, training and evaluating complex models in TensorFlow. Components of tf-slim can be freely mixed with native tensorflow, as well as other frameworks, such as tf.contrib.learn.\n\n\uff08\u610f\u8a33\uff09\nTF-Slim\u306fTensorflow\u3067\u8907\u96d1\u306a\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u30fb\u8a13\u7df4\u30fb\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\u751f\u306eTensorflow\u3084TF-Learn\uff08tf.contrib.learn\uff09\u3068\u6df7\u305c\u3066\u3082\u5b89\u5168\u3002\nGithub\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u958b\u767a\u304c\u9032\u3093\u3067\u3044\u308b\u3002\nr0.10\u306e\u30d6\u30e9\u30f3\u30c1\u304b\u3089Tensorflow\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3089\u4f7f\u3048\u305d\u3046\u3060\u3051\u3069\u30010.9.0\u3067\u3082\u4e00\u90e8\u6a5f\u80fd\u306f\u5b9f\u88c5\u3055\u308c\u3066\u3044\u305f\u306e\u3067\u7121\u7406\u3084\u308a\u4f7f\u3063\u3066\u307f\u305f\u3002\n\u984c\u6750\u306fCIFAR10\n\n\u30a4\u30f3\u30dd\u30fc\u30c8\nfrom tensorflow.contrib import slim\n\n\n\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\n0.10\u3060\u3068slim.data\u306b\u8272\u3005\u306a\u30e1\u30bd\u30c3\u30c9\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u305d\u3046\u3060\u3063\u305f\u3051\u3069\u30010.9\u3067\u306fslim.data\u304c\u5b58\u5728\u3057\u306a\u3044\u306e\u3067\u8ae6\u3081\u308b\u3002\n\n\u30e2\u30c7\u30eb\u5b9a\u7fa9\nconv*2, pool*1, norm*1\u306e\u30bb\u30c3\u30c8\u30922\u56de\u2192fc*3\nwith slim.arg_scope([slim.layers.conv2d, slim.layers.fully_connected],\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                    weights_regularizer=slim.regularizers.l2_regularizer(0.0005)):\n  with slim.arg_scope([slim.layers.max_pool2d], padding='SAME'):\n      net = slim.layers.repeat(inputs, 2, slim.layers.conv2d, 64, [3, 3], scope='conv1')\n      net = slim.layers.max_pool2d(net, [3, 3], scope='pool1')\n      net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n      net = slim.layers.repeat(net, 2, slim.layers.conv2d, 64, [5, 5], scope='conv2')\n      net = slim.layers.max_pool2d(net, [3, 3], scope='pool2')\n      net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n      net = slim.layers.flatten(net, scope='flatten')\n      net = slim.layers.stack(net, slim.layers.fully_connected, [384, 192], scope='fc1')\n      net = slim.layers.fully_connected(net, NUM_CLASSES, activation_fn=None, scope='fc2')\n\nrepeat\u306f0.9\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u3051\u3069\u3001\u3053\u308c\u3060\u3051\u306f\u306a\u3044\u3068\u4e0d\u4fbf\u306a\u306e\u30670.10\u304b\u3089\u30b3\u30d4\u30fc\u3002\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import variable_scope\n\ndef repeat(inputs, repetitions, layer, *args, **kwargs):\n  scope = kwargs.pop('scope', None)\n  with variable_scope.variable_op_scope([inputs], scope, 'Repeat'):\n    inputs = ops.convert_to_tensor(inputs)\n    if scope is None:\n      if hasattr(layer, '__name__'):\n        scope = layer.__name__\n      elif hasattr(layer, 'func') and hasattr(layer.func, '__name__'):\n        scope = layer.func.__name__  # In case layer is a functools.partial.\n      else:\n        scope = 'repeat'\n    outputs = inputs\n    for i in range(repetitions):\n      kwargs['scope'] = scope + '_' + str(i+1)\n      outputs = layer(outputs, *args, **kwargs)\n    return outputs\n\nslim.layers.repeat = repeat\n\n\n\u640d\u5931\u95a2\u6570\nloss = slim.losses.softmax_cross_entropy(predictions, labels)\n\n\n\u6700\u9069\u5316\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n\n\n\u5b66\u7fd2\ntrain_op = slim.learning.create_train_op(loss, optimizer)\n\nresult = slim.learning.train(train_op,\n                             FLAGS.log_dir,\n                             number_of_steps=1000,\n                             save_summaries_secs=300,\n                             save_interval_secs=600)\n\n\n\u8a55\u4fa1\nslim.evaluation\u306b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3046\u307e\u304f\u4f7f\u3048\u306a\u304b\u3063\u305f\u306e\u3067\u5272\u611b\u3002\n\n\u611f\u60f3\nREADME\u306b\u66f8\u3044\u3066\u3042\u308b\u3053\u3068\u3082\u4fe1\u7528\u3067\u304d\u306a\u3044\u3057\u30010.9\u3060\u3068\u672a\u5b9f\u88c5\u306a\u3082\u306e\u30840.10\u3067\u306f\u30e1\u30bd\u30c3\u30c9\u540d\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u3082\u306e\u3082\u3042\u3063\u3066\u30b3\u30fc\u30c9\u3092\u76f4\u63a5\u898b\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u3001\u7d20\u76f4\u306b0.10\u3092\u5f85\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\u30a6\u30a7\u30a4\u30c8\u30fb\u30d0\u30a4\u30a2\u30b9\u306e\u30ea\u30b9\u30c8\u30a2\u3068\u30e2\u30c7\u30eb\u8a55\u4fa1\u306e\u3068\u3053\u308d\u304c\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u8ab0\u304b\u3067\u304d\u305f\u3089\u6559\u3048\u3066\u6b32\u3057\u3044\u2026\u2026\n\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3060\u3051\u306f\u5229\u7528\u3059\u308b\u3068\u7c21\u6f54\u3067\u3044\u3044\u3002\u5909\u6570\u306e\u30ea\u30b9\u30c8\u30a2\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u3051\u3069\u2026\u2026\n\n\u30b3\u30fc\u30c9\n\ncifar10.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nimport os, sys, tarfile\nfrom six.moves import urllib, range\n\nfrom tensorflow.models.image.cifar10 import cifar10_input\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_data_dir', '/tmp/cifar10_data',\n                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\ntf.app.flags.DEFINE_string('log_dir', '/tmp/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\")\n\nDATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nLEARNING_RATE = 0.1\n\ndef main(argv=None):  # pylint: disable=unused-argument\n    maybe_download_and_extract()\n\n    if tf.gfile.Exists(FLAGS.log_dir):\n        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n    tf.gfile.MakeDirs(FLAGS.log_dir)\n\n    with tf.Graph().as_default():\n        images, labels = distorted_inputs()\n        labels = slim.layers.one_hot_encoding(labels, NUM_CLASSES)\n\n        predictions = my_model(images)\n        loss = slim.losses.softmax_cross_entropy(predictions, labels)\n\n        tf.scalar_summary('loss', loss)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n        train_op = slim.learning.create_train_op(loss, optimizer)\n\n        result = slim.learning.train(train_op,\n                                     FLAGS.log_dir,\n                                     number_of_steps=1000,\n                                     save_summaries_secs=300,\n                                     save_interval_secs=600)\n        print('loss: %f' % result)\n\ndef my_model(inputs):\n    with slim.arg_scope([slim.layers.conv2d, slim.layers.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                        weights_regularizer=slim.regularizers.l2_regularizer(0.0005)):\n        with slim.arg_scope([slim.layers.max_pool2d], padding='SAME'):\n            net = slim.layers.repeat(inputs, 2, slim.layers.conv2d, 64, [3, 3], scope='conv1')\n            net = slim.layers.max_pool2d(net, [3, 3], scope='pool1')\n            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n            net = slim.layers.repeat(net, 2, slim.layers.conv2d, 64, [5, 5], scope='conv2')\n            net = slim.layers.max_pool2d(net, [3, 3], scope='pool2')\n            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n            net = slim.layers.flatten(net, scope='flatten')\n            net = slim.layers.stack(net, slim.layers.fully_connected, [384, 192], scope='fc1')\n            net = slim.layers.fully_connected(net, NUM_CLASSES, activation_fn=None, scope='fc2')\n    return net\n\ndef distorted_inputs():\n    if not FLAGS.tmp_data_dir:\n        raise ValueError('Please supply a tmp_data_dir')\n    data_dir = os.path.join(FLAGS.tmp_data_dir, 'cifar-10-batches-bin')\n    images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                    batch_size=FLAGS.batch_size)\n    return images, labels\n\ndef maybe_download_and_extract():\n    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n    dest_directory = FLAGS.tmp_data_dir\n    if not os.path.exists(dest_directory):\n        os.makedirs(dest_directory)\n    filename = DATA_URL.split('/')[-1]\n    filepath = os.path.join(dest_directory, filename)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n                             float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n        print()\n        statinfo = os.stat(filepath)\n        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n\n# define slim.layers.repeat\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import variable_scope\n\ndef repeat(inputs, repetitions, layer, *args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with variable_scope.variable_op_scope([inputs], scope, 'Repeat'):\n        inputs = ops.convert_to_tensor(inputs)\n        if scope is None:\n          if hasattr(layer, '__name__'):\n            scope = layer.__name__\n          elif hasattr(layer, 'func') and hasattr(layer.func, '__name__'):\n            scope = layer.func.__name__  # In case layer is a functools.partial.\n          else:\n            scope = 'repeat'\n        outputs = inputs\n        for i in range(repetitions):\n          kwargs['scope'] = scope + '_' + str(i+1)\n          outputs = layer(outputs, *args, **kwargs)\n        return outputs\n\nslim.layers.repeat = repeat\n\nif __name__ == '__main__':\n    tf.app.run()\n\n\n## Slim\u3068\u306f\n\n> TF-Slim is a lightweight library for defining, training and evaluating complex models in TensorFlow. Components of tf-slim can be freely mixed with native tensorflow, as well as other frameworks, such as tf.contrib.learn.\n\n\uff08\u610f\u8a33\uff09\nTF-Slim\u306fTensorflow\u3067\u8907\u96d1\u306a\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u30fb\u8a13\u7df4\u30fb\u8a55\u4fa1\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\u751f\u306eTensorflow\u3084TF-Learn\uff08tf.contrib.learn\uff09\u3068\u6df7\u305c\u3066\u3082\u5b89\u5168\u3002\n\nGithub\u306e[\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/tensorflow/tensorflow/tree/r0.10/tensorflow/contrib/slim)\u3067\u958b\u767a\u304c\u9032\u3093\u3067\u3044\u308b\u3002\n\nr0.10\u306e\u30d6\u30e9\u30f3\u30c1\u304b\u3089Tensorflow\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3089\u4f7f\u3048\u305d\u3046\u3060\u3051\u3069\u30010.9.0\u3067\u3082\u4e00\u90e8\u6a5f\u80fd\u306f\u5b9f\u88c5\u3055\u308c\u3066\u3044\u305f\u306e\u3067\u7121\u7406\u3084\u308a\u4f7f\u3063\u3066\u307f\u305f\u3002\n\u984c\u6750\u306fCIFAR10\n\n## \u30a4\u30f3\u30dd\u30fc\u30c8\n\n```python:\nfrom tensorflow.contrib import slim\n```\n\n## \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\n\n0.10\u3060\u3068slim.data\u306b\u8272\u3005\u306a\u30e1\u30bd\u30c3\u30c9\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u305d\u3046\u3060\u3063\u305f\u3051\u3069\u30010.9\u3067\u306fslim.data\u304c\u5b58\u5728\u3057\u306a\u3044\u306e\u3067\u8ae6\u3081\u308b\u3002\n\n## \u30e2\u30c7\u30eb\u5b9a\u7fa9\n\nconv*2, pool*1, norm*1\u306e\u30bb\u30c3\u30c8\u30922\u56de\u2192fc*3\n\n\n```python:\nwith slim.arg_scope([slim.layers.conv2d, slim.layers.fully_connected],\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                    weights_regularizer=slim.regularizers.l2_regularizer(0.0005)):\n  with slim.arg_scope([slim.layers.max_pool2d], padding='SAME'):\n      net = slim.layers.repeat(inputs, 2, slim.layers.conv2d, 64, [3, 3], scope='conv1')\n      net = slim.layers.max_pool2d(net, [3, 3], scope='pool1')\n      net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n      net = slim.layers.repeat(net, 2, slim.layers.conv2d, 64, [5, 5], scope='conv2')\n      net = slim.layers.max_pool2d(net, [3, 3], scope='pool2')\n      net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n      net = slim.layers.flatten(net, scope='flatten')\n      net = slim.layers.stack(net, slim.layers.fully_connected, [384, 192], scope='fc1')\n      net = slim.layers.fully_connected(net, NUM_CLASSES, activation_fn=None, scope='fc2')\n```\n\nrepeat\u306f0.9\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u3051\u3069\u3001\u3053\u308c\u3060\u3051\u306f\u306a\u3044\u3068\u4e0d\u4fbf\u306a\u306e\u30670.10\u304b\u3089\u30b3\u30d4\u30fc\u3002\n\n```python:\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import variable_scope\n\ndef repeat(inputs, repetitions, layer, *args, **kwargs):\n  scope = kwargs.pop('scope', None)\n  with variable_scope.variable_op_scope([inputs], scope, 'Repeat'):\n    inputs = ops.convert_to_tensor(inputs)\n    if scope is None:\n      if hasattr(layer, '__name__'):\n        scope = layer.__name__\n      elif hasattr(layer, 'func') and hasattr(layer.func, '__name__'):\n        scope = layer.func.__name__  # In case layer is a functools.partial.\n      else:\n        scope = 'repeat'\n    outputs = inputs\n    for i in range(repetitions):\n      kwargs['scope'] = scope + '_' + str(i+1)\n      outputs = layer(outputs, *args, **kwargs)\n    return outputs\n\nslim.layers.repeat = repeat\n```\n\n## \u640d\u5931\u95a2\u6570\n\n```python:\nloss = slim.losses.softmax_cross_entropy(predictions, labels)\n```\n\n## \u6700\u9069\u5316\n\n```python:\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n```\n\n## \u5b66\u7fd2\n\n```python:\ntrain_op = slim.learning.create_train_op(loss, optimizer)\n\nresult = slim.learning.train(train_op,\n                             FLAGS.log_dir,\n                             number_of_steps=1000,\n                             save_summaries_secs=300,\n                             save_interval_secs=600)\n```\n\n## \u8a55\u4fa1\n\nslim.evaluation\u306b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u3046\u307e\u304f\u4f7f\u3048\u306a\u304b\u3063\u305f\u306e\u3067\u5272\u611b\u3002\n\n## \u611f\u60f3\n\nREADME\u306b\u66f8\u3044\u3066\u3042\u308b\u3053\u3068\u3082\u4fe1\u7528\u3067\u304d\u306a\u3044\u3057\u30010.9\u3060\u3068\u672a\u5b9f\u88c5\u306a\u3082\u306e\u30840.10\u3067\u306f\u30e1\u30bd\u30c3\u30c9\u540d\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u3082\u306e\u3082\u3042\u3063\u3066\u30b3\u30fc\u30c9\u3092\u76f4\u63a5\u898b\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u3001\u7d20\u76f4\u306b0.10\u3092\u5f85\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\u30a6\u30a7\u30a4\u30c8\u30fb\u30d0\u30a4\u30a2\u30b9\u306e\u30ea\u30b9\u30c8\u30a2\u3068\u30e2\u30c7\u30eb\u8a55\u4fa1\u306e\u3068\u3053\u308d\u304c\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u8ab0\u304b\u3067\u304d\u305f\u3089\u6559\u3048\u3066\u6b32\u3057\u3044\u2026\u2026\n\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3060\u3051\u306f\u5229\u7528\u3059\u308b\u3068\u7c21\u6f54\u3067\u3044\u3044\u3002\u5909\u6570\u306e\u30ea\u30b9\u30c8\u30a2\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u3051\u3069\u2026\u2026\n\n## \u30b3\u30fc\u30c9\n\n```python:cifar10.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import slim\n\nimport os, sys, tarfile\nfrom six.moves import urllib, range\n\nfrom tensorflow.models.image.cifar10 import cifar10_input\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_data_dir', '/tmp/cifar10_data',\n                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\ntf.app.flags.DEFINE_string('log_dir', '/tmp/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\")\n\nDATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nLEARNING_RATE = 0.1\n\ndef main(argv=None):  # pylint: disable=unused-argument\n    maybe_download_and_extract()\n\n    if tf.gfile.Exists(FLAGS.log_dir):\n        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n    tf.gfile.MakeDirs(FLAGS.log_dir)\n\n    with tf.Graph().as_default():\n        images, labels = distorted_inputs()\n        labels = slim.layers.one_hot_encoding(labels, NUM_CLASSES)\n\n        predictions = my_model(images)\n        loss = slim.losses.softmax_cross_entropy(predictions, labels)\n\n        tf.scalar_summary('loss', loss)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n        train_op = slim.learning.create_train_op(loss, optimizer)\n\n        result = slim.learning.train(train_op,\n                                     FLAGS.log_dir,\n                                     number_of_steps=1000,\n                                     save_summaries_secs=300,\n                                     save_interval_secs=600)\n        print('loss: %f' % result)\n\ndef my_model(inputs):\n    with slim.arg_scope([slim.layers.conv2d, slim.layers.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n                        weights_regularizer=slim.regularizers.l2_regularizer(0.0005)):\n        with slim.arg_scope([slim.layers.max_pool2d], padding='SAME'):\n            net = slim.layers.repeat(inputs, 2, slim.layers.conv2d, 64, [3, 3], scope='conv1')\n            net = slim.layers.max_pool2d(net, [3, 3], scope='pool1')\n            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n            net = slim.layers.repeat(net, 2, slim.layers.conv2d, 64, [5, 5], scope='conv2')\n            net = slim.layers.max_pool2d(net, [3, 3], scope='pool2')\n            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n            net = slim.layers.flatten(net, scope='flatten')\n            net = slim.layers.stack(net, slim.layers.fully_connected, [384, 192], scope='fc1')\n            net = slim.layers.fully_connected(net, NUM_CLASSES, activation_fn=None, scope='fc2')\n    return net\n\ndef distorted_inputs():\n    if not FLAGS.tmp_data_dir:\n        raise ValueError('Please supply a tmp_data_dir')\n    data_dir = os.path.join(FLAGS.tmp_data_dir, 'cifar-10-batches-bin')\n    images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                    batch_size=FLAGS.batch_size)\n    return images, labels\n\ndef maybe_download_and_extract():\n    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n    dest_directory = FLAGS.tmp_data_dir\n    if not os.path.exists(dest_directory):\n        os.makedirs(dest_directory)\n    filename = DATA_URL.split('/')[-1]\n    filepath = os.path.join(dest_directory, filename)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n                             float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n        print()\n        statinfo = os.stat(filepath)\n        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n\n# define slim.layers.repeat\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import variable_scope\n\ndef repeat(inputs, repetitions, layer, *args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with variable_scope.variable_op_scope([inputs], scope, 'Repeat'):\n        inputs = ops.convert_to_tensor(inputs)\n        if scope is None:\n          if hasattr(layer, '__name__'):\n            scope = layer.__name__\n          elif hasattr(layer, 'func') and hasattr(layer.func, '__name__'):\n            scope = layer.func.__name__  # In case layer is a functools.partial.\n          else:\n            scope = 'repeat'\n        outputs = inputs\n        for i in range(repetitions):\n          kwargs['scope'] = scope + '_' + str(i+1)\n          outputs = layer(outputs, *args, **kwargs)\n        return outputs\n\nslim.layers.repeat = repeat\n\nif __name__ == '__main__':\n    tf.app.run()\n```\n", "tags": ["TensorFlow", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2", "DeepLearning", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0"]}