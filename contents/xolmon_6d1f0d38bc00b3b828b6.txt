{"context": "Chainer1.11.0\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3001Trainer\u3068\u3044\u3046\u5b66\u7fd2\u30eb\u30fc\u30d7\u3092\u62bd\u8c61\u5316\u3059\u308b\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u72ec\u81ea\u306eAV\u5973\u512a\u9854\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3092\u3057\u3066\u307f\u307e\u3059\u3002\n\u9854\u753b\u50cf\u306e\u62bd\u51fa\u3084\u30c7\u30fc\u30bf\u62e1\u5f35\u306b\u3064\u3044\u3066\u306f\u3001Qiita - chainer\u306b\u3088\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067AV\u5973\u512a\u306e\u985e\u4f3c\u753b\u50cf\u691c\u7d22\u30b5\u30fc\u30d3\u30b9\u3092\u3064\u304f\u3063\u305f\u30ce\u30a6\u30cf\u30a6\u3092\u516c\u958b\u3059\u308b\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u8a18\u4e8b\u3067\u306fnumpy\u5f62\u5f0f\u306b\u5909\u63db\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u5b66\u7fd2\u6642\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u3001numpy\u5f62\u5f0f\u306b\u5909\u63db\u3057\u307e\u305b\u3093\u3002\n\u3053\u3053\u3067\u4f7f\u3046\u9854\u753b\u50cf\u306f\u5404\u5973\u512a\u306b\u3064\u304d1000\u679a\u305a\u3064\u753b\u50cf\u304c\u3042\u308a\u300164\u00d764\u306e\u30b5\u30a4\u30ba\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3001\u4ee5\u4e0b\u69cb\u6210\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5206\u3051\u3089\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n./root\n    |\n    |--- /actress1\n    |        |--- image1.jpg\n    |        |--- image2.jpg\n    |        |--- image3.jpg\n    |\n    |--- /actress2\n    |        .\n    |        .\n    |--- /actress3\n    .\n    .\n    .\n\n\n\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u3051\u308b\n\u307e\u305a\u3001\u9854\u753b\u50cf\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u5272\u3057\u307e\u3059\u3002\u5b66\u7fd2\u306e\u969b\u306e\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u6642\u306b\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u5272\u3057\u306a\u304c\u3089\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u3069\u3093\u306a\u30c7\u30fc\u30bf\u304c\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u3066\u3044\u3066\u3069\u3093\u306a\u30c7\u30fc\u30bf\u304c\u691c\u8a3c\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u306e\u304b\u5206\u304b\u308a\u3065\u3089\u3044\u305f\u3081\u3001\u4e8b\u524d\u306b\u5206\u5272\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n#!/usr/bin/env python\n#-*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport random\nimport shutil\n\ndef separate_train_val(args):\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n\n    if not os.path.exists(os.path.join(args.output_dir, 'train')):\n        os.mkdir(os.path.join(args.output_dir, 'train'))\n\n    if not os.path.exists(os.path.join(args.output_dir, 'val')):\n        os.mkdir(os.path.join(args.output_dir, 'val'))\n\n    directories = os.listdir(args.root)\n\n    for dir_index, dir_name in enumerate(directories):\n        files = glob.glob(os.path.join(args.root, dir_name, '*.jpg'))\n        random.shuffle(files)\n        if len(files) == 0: continue\n\n        for file_index, file_path in enumerate(files):\n            if file_index % args.val_freq != 0:\n                target_dir = os.path.join(args.output_dir, 'train', dir_name)\n                if not os.path.exists(target_dir):\n                    os.mkdir(target_dir)\n                shutil.copy(file_path, target_dir)\n                logging.info('Copied {} => {}'.format(file_path, target_dir))\n            else:\n                target_dir = os.path.join(args.output_dir, 'val', dir_name)\n                if not os.path.exists(target_dir):\n                    os.mkdir(target_dir)\n                shutil.copy(file_path, target_dir)\n                logging.info('Copied {} => {}'.format(file_path, target_dir))\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n\n    parser = argparse.ArgumentParser(description='converter')\n    parser.add_argument('--root', default='.')\n    parser.add_argument('--output_dir', default='.')\n    parser.add_argument('--val_freq', type=int, default=10)\n    args = parser.parse_args()\n\n    separate_train_val(args)\n\n\u5206\u5272\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69cb\u6210\u306b\u306a\u308a\u307e\u3059\u3002\n./train_val_root\n    |\n    |--- /train\n    |       |--- actress1\n    |       |       |--- image1.jpg\n    |       |       |--- image2.jpg\n    |       |       |--- image3.jpg\n    |       |              \u30fb\n    |       |              \u30fb\n    |       |--- actress2\n    |       |      \u30fb\n    |\u3000 \u3000\u3000 | \u3000\u3000\u3000\u30fb         \n    |\n    |--- /val\n    |       |--- actress1\n    |       |\n    |       |--- actress2\n    .\n    .\n\n\n\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306eDataset\u3092\u4f5c\u6210\nchainer.dataset.DatasetMixin\u3092\u7d99\u627f\u3057\u3066\u6307\u5b9a\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u30af\u30e9\u30b9\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u8a8d\u8b58\u306e\u969b\u306b\u4f7f\u7528\u3059\u308b\u30af\u30e9\u30b9(0\uff5e9\u306e\u6570\u5b57)\u3068\u30e9\u30d9\u30eb(\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d)\u3092\u51fa\u529b\u3059\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u5b9a\u7fa9(create_label_file)\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u6c17\u6301\u3061\u60aa\u3044\u306e\u3067\u771f\u4f3c\u3057\u306a\u3044\u3067\u4e0b\u3055\u3044\u3002\nclass DatasetFromDirectory(chainer.dataset.DatasetMixin):\n\n    def __init__(self, root='.', label_out='', dtype=np.float32, label_dtype=np.int32):\n        directories = os.listdir(root)\n        label_table = []\n        pairs = [] # tuple (filepath, label) list\n        for dir_index, dir_name in enumerate(directories):\n            label_table.append((dir_index, dir_name))\n            file_paths = glob.glob(os.path.join(root, dir_name, '*.jpg'))\n            for file_path in file_paths:\n                pairs.append((file_path, dir_index))\n\n        self._pairs = pairs\n        self._root = root\n        self._label_out = label_out\n        self._label_table = label_table\n        self._dtype = dtype\n        self._label_dtype = label_dtype\n\n        if label_out != '':\n            self.create_label_file()\n\n    def __len__(self):\n        return len(self._pairs)\n\n    def get_example(self, i):\n        path, int_label = self._pairs[i]\n        with Image.open(path) as f:\n            image = np.asarray(f, dtype=self._dtype)\n        image = image.transpose(2, 0, 1)\n        label = np.array(int_label, dtype=self._label_dtype)\n        return image, label\n\n    def create_label_file(self):\n        with open(self._label_out, \"w\") as f:\n            for (label_index, label_name) in self._label_table:\n                f.write('{},{}\\n'.format(label_index, label_name))\n\n\u516c\u5f0f\u306eimagenet\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u898b\u308b\u3068\u3001\u4f5c\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30af\u30e9\u30b9\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u5b66\u7fd2\u6642\u306b\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\u5b66\u7fd2\u6642\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u753b\u50cf\u3092\u5c11\u3057\u56de\u8ee2\u3055\u305b\u305f\u308a\u3001\u753b\u50cf\u3092\u5c11\u3057\u305a\u3089\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3067\u3001\u5b8c\u5168\u306b\u540c\u3058\u30c7\u30fc\u30bf\u304b\u3089\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u5c11\u306a\u304f\u306a\u308b\u305f\u3081\u3001\u6c4e\u5316\u6027\u80fd\u306e\u5411\u4e0a\u304c\u671f\u5f85\u3067\u304d\u307e\u3059\u3002\n\nTrainer\u3067\u72ec\u81ea\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\n\u5b9f\u969b\u306b\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\u3092\u3057\u307e\u3059\u3002Chainer Trainer\u3092\u4f7f\u3063\u305f\u5b9f\u88c5\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u5143\u30b3\u30fc\u30c9\u306e\u534a\u5206\u7a0b\u5ea6\u306e\u91cf\u3067\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\nclass CNN(chainer.Chain):\n    \"\"\"\n    CNN (CCPCCPCP)\n    \"\"\"\n    def __init__(self, n_classes):\n        super(CNN, self).__init__(\n            conv1_1=L.Convolution2D(3, 32, 3, pad=1),\n            bn1_1=L.BatchNormalization(32),\n            conv1_2=L.Convolution2D(32, 32, 3, pad=1),\n            bn1_2=L.BatchNormalization(32),\n\n            conv2_1=L.Convolution2D(32, 64, 3, pad=1),\n            bn2_1=L.BatchNormalization(64),\n            conv2_2=L.Convolution2D(64, 64, 3, pad=1),\n            bn2_2=L.BatchNormalization(64),\n\n            conv3_1=L.Convolution2D(64, 128, 3, pad=1),\n            bn3_1=L.BatchNormalization(128),\n\n            fc4=L.Linear(8192, 1024),\n            fc5=L.Linear(1024, n_classes),\n        )\n        self.train = True\n\n    def __call__(self, x, t):\n        h = F.relu(self.bn1_1(self.conv1_1(x), test=not self.train))\n        h = F.relu(self.bn1_2(self.conv1_2(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.relu(self.bn2_1(self.conv2_1(h), test=not self.train))\n        h = F.relu(self.bn2_2(self.conv2_2(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.relu(self.bn3_1(self.conv3_1(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.dropout(F.relu(self.fc4(h)), ratio=0.3, train=self.train)\n        h = self.fc5(h)\n\n        loss = F.softmax_cross_entropy(h, t)\n        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)\n        return loss\n\nmodel = CNN(10)\noptimizer = chainer.optimizers.Adam()\noptimizer.setup(model)\n\nmean = np.load(args.mean)\ntrain_data = datasets.DatasetFromDirectory(args.train_root, label_out=label_file)\nval_data = datasets.DatasetFromDirectory(args.val_root)\n\ntrain_iter = chainer.iterators.SerialIterator(train_data, args.batch_size)\nval_iter = chainer.iterators.SerialIterator(val_data, args.batch_size, repeat=False, shuffle=False)\n\n# Set up a trainer\nupdater = training.StandardUpdater(train_iter, optimizer, device=args.gpu)\ntrainer = training.Trainer(updater, (args.n_epoch, 'epoch'), out=args.output_dir)\n\nsnapshot_interval = (args.snapshot_interval, 'iteration')\n\n# Copy the chain with shared parameters to flip 'train' flag only in test\neval_model = model.copy()\neval_model.train = False\n\ntrainer.extend(extensions.Evaluator(val_iter, eval_model, device=args.gpu))\ntrainer.extend(extensions.dump_graph('main/loss'))\ntrainer.extend(extensions.snapshot(), trigger=snapshot_interval)\ntrainer.extend(extensions.snapshot_object(\n    model, 'model_iter_{.updater.iteration}'), trigger=snapshot_interval)\ntrainer.extend(extensions.snapshot_object(\n    optimizer, 'optimizer_iter_{.updater.iteration}'), trigger=snapshot_interval)\ntrainer.extend(extensions.LogReport())\ntrainer.extend(extensions.PrintReport(\n    ['epoch', 'main/loss', 'validation/main/loss',\n     'main/accuracy', 'validation/main/accuracy']))\ntrainer.extend(extensions.ProgressBar(update_interval=10))\n\nif args.resume:\n    if not os.path.exists(args.resume):\n        raise IOError('Resume file is not exists.')\n    logging.info('Load optimizer state from {}'.format(args.resume))\n    chainer.serializers.load_npz(args.resume, trainer)\n\ntrainer.run()\n\n# Save the trained model\nchainer.serializers.save_npz(os.path.join(args.output_dir, 'model_final'), model)\nchainer.serializers.save_npz(os.path.join(args.output_dir, 'optimizer_final'), optimizer)\n\nprint()\nlogging.info('Saved the model and the optimizer')\nlogging.info('Training is finished!')\n\nextensions.snapshot()\u3067\u4fdd\u5b58\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306ftrainer\u7528\u306e\u3082\u306e\u306e\u305f\u3081\u3001\u5b9f\u969b\u306b\u4e88\u6e2c\u3059\u308b\u969b\u306b\u8aad\u307f\u8fbc\u3080model\u3068optimizer\u3092extensions.snapshot_object()\u3067\u5225\u9014\u4fdd\u5b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u307e\u3068\u3081\nChainer Trainer\u3092\u4f7f\u3044\u72ec\u81ea\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\nTrainer\u3092\u4f7f\u3063\u305f\u611f\u60f3\u3068\u3057\u3066\u306f\u3001\u4e88\u60f3\u901a\u308aKeras\u306b\u8fd1\u3044\u3068\u3044\u3046\u5370\u8c61\u3067\u3059\u3002\u521d\u3081\u3066Chainer\u3092\u4f7f\u3063\u3066\u307f\u305f\u3068\u304d\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u6bce\u306b\u8aad\u307f\u8fbc\u3080\u7b87\u6240\u3067\u624b\u9593\u304c\u304b\u304b\u3063\u3066\u3057\u307e\u3063\u305f\u8a18\u61b6\u304c\u3042\u308b\u305f\u3081\u3001\u305d\u306e\u3088\u3046\u306a\u90e8\u5206\u304c\u62bd\u8c61\u5316\u3055\u308c\u3066\u3044\u308bTrainer\u306f\u308f\u304b\u308a\u3084\u3059\u3044\u5b9f\u88c5\u3060\u3068\u611f\u3058\u307e\u3057\u305f\u3002\n\u3057\u304b\u3057\u3001Keras\u3067\u306fImageDataGenerator\u30af\u30e9\u30b9\u306eflow_from_directory\u3092\u4f7f\u3046\u3053\u3068\u3067Dataset\u30af\u30e9\u30b9\u306e\u5b9f\u88c5\u3092\u305b\u305a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u308b\u305f\u3081\u3001\u3088\u308a\u7c21\u5358\u306b\u4f5c\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\u6700\u5f8c\u306b\u5ba3\u4f1d\u306b\u306a\u308a\u307e\u3059\u304c\u3001CNN\u3092\u4f7f\u3063\u3066AV\u5973\u512a\u306e\u985e\u4f3c\u753b\u50cf\u691c\u7d22\u3092\u3057\u305f\u30b5\u30a4\u30c8\u3092\u4f5c\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3088\u304b\u3063\u305f\u3089\u898b\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\nBabelink - \u985e\u4f3cAV\u5973\u512a\u691c\u7d22\u30b5\u30fc\u30d3\u30b9\n\u203b\u30a2\u30c0\u30eb\u30c8\u30b5\u30a4\u30c8\u306e\u305f\u3081\u3001\u95b2\u89a7\u306b\u306f\u5341\u5206\u6ce8\u610f\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nChainer1.11.0\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3001Trainer\u3068\u3044\u3046\u5b66\u7fd2\u30eb\u30fc\u30d7\u3092\u62bd\u8c61\u5316\u3059\u308b\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u72ec\u81ea\u306eAV\u5973\u512a\u9854\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3092\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u9854\u753b\u50cf\u306e\u62bd\u51fa\u3084\u30c7\u30fc\u30bf\u62e1\u5f35\u306b\u3064\u3044\u3066\u306f\u3001[Qiita - chainer\u306b\u3088\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067AV\u5973\u512a\u306e\u985e\u4f3c\u753b\u50cf\u691c\u7d22\u30b5\u30fc\u30d3\u30b9\u3092\u3064\u304f\u3063\u305f\u30ce\u30a6\u30cf\u30a6\u3092\u516c\u958b\u3059\u308b](http://qiita.com/xolmon/items/0b82f4861cf93fd28e33)\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u8a18\u4e8b\u3067\u306fnumpy\u5f62\u5f0f\u306b\u5909\u63db\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u5b66\u7fd2\u6642\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u3001numpy\u5f62\u5f0f\u306b\u5909\u63db\u3057\u307e\u305b\u3093\u3002\n\n\u3053\u3053\u3067\u4f7f\u3046\u9854\u753b\u50cf\u306f\u5404\u5973\u512a\u306b\u3064\u304d1000\u679a\u305a\u3064\u753b\u50cf\u304c\u3042\u308a\u300164\u00d764\u306e\u30b5\u30a4\u30ba\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3001\u4ee5\u4e0b\u69cb\u6210\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5206\u3051\u3089\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\n```\n./root\n    |\n    |--- /actress1\n    |        |--- image1.jpg\n    |        |--- image2.jpg\n    |        |--- image3.jpg\n    |\n    |--- /actress2\n    |        .\n    |        .\n    |--- /actress3\n    .\n    .\n    .\n```\n\n## \u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u3051\u308b\n\n\u307e\u305a\u3001\u9854\u753b\u50cf\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u5272\u3057\u307e\u3059\u3002\u5b66\u7fd2\u306e\u969b\u306e\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u6642\u306b\u5b66\u7fd2\u7528\u3068\u691c\u8a3c\u7528\u306b\u5206\u5272\u3057\u306a\u304c\u3089\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u3069\u3093\u306a\u30c7\u30fc\u30bf\u304c\u5b66\u7fd2\u3067\u4f7f\u308f\u308c\u3066\u3044\u3066\u3069\u3093\u306a\u30c7\u30fc\u30bf\u304c\u691c\u8a3c\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u306e\u304b\u5206\u304b\u308a\u3065\u3089\u3044\u305f\u3081\u3001\u4e8b\u524d\u306b\u5206\u5272\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```\n#!/usr/bin/env python\n#-*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport random\nimport shutil\n\ndef separate_train_val(args):\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n\n    if not os.path.exists(os.path.join(args.output_dir, 'train')):\n        os.mkdir(os.path.join(args.output_dir, 'train'))\n\n    if not os.path.exists(os.path.join(args.output_dir, 'val')):\n        os.mkdir(os.path.join(args.output_dir, 'val'))\n\n    directories = os.listdir(args.root)\n\n    for dir_index, dir_name in enumerate(directories):\n        files = glob.glob(os.path.join(args.root, dir_name, '*.jpg'))\n        random.shuffle(files)\n        if len(files) == 0: continue\n\n        for file_index, file_path in enumerate(files):\n            if file_index % args.val_freq != 0:\n                target_dir = os.path.join(args.output_dir, 'train', dir_name)\n                if not os.path.exists(target_dir):\n                    os.mkdir(target_dir)\n                shutil.copy(file_path, target_dir)\n                logging.info('Copied {} => {}'.format(file_path, target_dir))\n            else:\n                target_dir = os.path.join(args.output_dir, 'val', dir_name)\n                if not os.path.exists(target_dir):\n                    os.mkdir(target_dir)\n                shutil.copy(file_path, target_dir)\n                logging.info('Copied {} => {}'.format(file_path, target_dir))\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n\n    parser = argparse.ArgumentParser(description='converter')\n    parser.add_argument('--root', default='.')\n    parser.add_argument('--output_dir', default='.')\n    parser.add_argument('--val_freq', type=int, default=10)\n    args = parser.parse_args()\n\n    separate_train_val(args)\n```\n\n\u5206\u5272\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69cb\u6210\u306b\u306a\u308a\u307e\u3059\u3002\n\n```\n./train_val_root\n    |\n    |--- /train\n    |       |--- actress1\n    |       |       |--- image1.jpg\n    |       |       |--- image2.jpg\n    |       |       |--- image3.jpg\n    |       |              \u30fb\n    |       |              \u30fb\n    |       |--- actress2\n    |       |      \u30fb\n    |\u3000 \u3000\u3000 | \u3000\u3000\u3000\u30fb         \n    |\n    |--- /val\n    |       |--- actress1\n    |       |\n    |       |--- actress2\n    .\n    .\n```\n\n## \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306eDataset\u3092\u4f5c\u6210\n\n`chainer.dataset.DatasetMixin`\u3092\u7d99\u627f\u3057\u3066\u6307\u5b9a\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u30af\u30e9\u30b9\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u8a8d\u8b58\u306e\u969b\u306b\u4f7f\u7528\u3059\u308b\u30af\u30e9\u30b9(0\uff5e9\u306e\u6570\u5b57)\u3068\u30e9\u30d9\u30eb(\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d)\u3092\u51fa\u529b\u3059\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u5b9a\u7fa9(`create_label_file`)\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u6c17\u6301\u3061\u60aa\u3044\u306e\u3067\u771f\u4f3c\u3057\u306a\u3044\u3067\u4e0b\u3055\u3044\u3002\n\n```\nclass DatasetFromDirectory(chainer.dataset.DatasetMixin):\n\n    def __init__(self, root='.', label_out='', dtype=np.float32, label_dtype=np.int32):\n        directories = os.listdir(root)\n        label_table = []\n        pairs = [] # tuple (filepath, label) list\n        for dir_index, dir_name in enumerate(directories):\n            label_table.append((dir_index, dir_name))\n            file_paths = glob.glob(os.path.join(root, dir_name, '*.jpg'))\n            for file_path in file_paths:\n                pairs.append((file_path, dir_index))\n\n        self._pairs = pairs\n        self._root = root\n        self._label_out = label_out\n        self._label_table = label_table\n        self._dtype = dtype\n        self._label_dtype = label_dtype\n\n        if label_out != '':\n            self.create_label_file()\n\n    def __len__(self):\n        return len(self._pairs)\n\n    def get_example(self, i):\n        path, int_label = self._pairs[i]\n        with Image.open(path) as f:\n            image = np.asarray(f, dtype=self._dtype)\n        image = image.transpose(2, 0, 1)\n        label = np.array(int_label, dtype=self._label_dtype)\n        return image, label\n\n    def create_label_file(self):\n        with open(self._label_out, \"w\") as f:\n            for (label_index, label_name) in self._label_table:\n                f.write('{},{}\\n'.format(label_index, label_name))\n```\n\n[\u516c\u5f0f\u306eimagenet\u306e\u30b5\u30f3\u30d7\u30eb](https://github.com/pfnet/chainer/blob/master/examples/imagenet/train_imagenet.py)\u3092\u898b\u308b\u3068\u3001\u4f5c\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30af\u30e9\u30b9\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u5b66\u7fd2\u6642\u306b\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\u5b66\u7fd2\u6642\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u753b\u50cf\u3092\u5c11\u3057\u56de\u8ee2\u3055\u305b\u305f\u308a\u3001\u753b\u50cf\u3092\u5c11\u3057\u305a\u3089\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3067\u3001\u5b8c\u5168\u306b\u540c\u3058\u30c7\u30fc\u30bf\u304b\u3089\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u5c11\u306a\u304f\u306a\u308b\u305f\u3081\u3001\u6c4e\u5316\u6027\u80fd\u306e\u5411\u4e0a\u304c\u671f\u5f85\u3067\u304d\u307e\u3059\u3002\n\n## Trainer\u3067\u72ec\u81ea\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\n\n\u5b9f\u969b\u306b\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\u3092\u3057\u307e\u3059\u3002Chainer Trainer\u3092\u4f7f\u3063\u305f\u5b9f\u88c5\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u5143\u30b3\u30fc\u30c9\u306e\u534a\u5206\u7a0b\u5ea6\u306e\u91cf\u3067\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n```\nclass CNN(chainer.Chain):\n    \"\"\"\n    CNN (CCPCCPCP)\n    \"\"\"\n    def __init__(self, n_classes):\n        super(CNN, self).__init__(\n            conv1_1=L.Convolution2D(3, 32, 3, pad=1),\n            bn1_1=L.BatchNormalization(32),\n            conv1_2=L.Convolution2D(32, 32, 3, pad=1),\n            bn1_2=L.BatchNormalization(32),\n\n            conv2_1=L.Convolution2D(32, 64, 3, pad=1),\n            bn2_1=L.BatchNormalization(64),\n            conv2_2=L.Convolution2D(64, 64, 3, pad=1),\n            bn2_2=L.BatchNormalization(64),\n\n            conv3_1=L.Convolution2D(64, 128, 3, pad=1),\n            bn3_1=L.BatchNormalization(128),\n\n            fc4=L.Linear(8192, 1024),\n            fc5=L.Linear(1024, n_classes),\n        )\n        self.train = True\n\n    def __call__(self, x, t):\n        h = F.relu(self.bn1_1(self.conv1_1(x), test=not self.train))\n        h = F.relu(self.bn1_2(self.conv1_2(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.relu(self.bn2_1(self.conv2_1(h), test=not self.train))\n        h = F.relu(self.bn2_2(self.conv2_2(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.relu(self.bn3_1(self.conv3_1(h), test=not self.train))\n        h = F.max_pooling_2d(h, 2, 2)\n\n        h = F.dropout(F.relu(self.fc4(h)), ratio=0.3, train=self.train)\n        h = self.fc5(h)\n\n        loss = F.softmax_cross_entropy(h, t)\n        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)\n        return loss\n```\n\n```\nmodel = CNN(10)\noptimizer = chainer.optimizers.Adam()\noptimizer.setup(model)\n\nmean = np.load(args.mean)\ntrain_data = datasets.DatasetFromDirectory(args.train_root, label_out=label_file)\nval_data = datasets.DatasetFromDirectory(args.val_root)\n\ntrain_iter = chainer.iterators.SerialIterator(train_data, args.batch_size)\nval_iter = chainer.iterators.SerialIterator(val_data, args.batch_size, repeat=False, shuffle=False)\n\n# Set up a trainer\nupdater = training.StandardUpdater(train_iter, optimizer, device=args.gpu)\ntrainer = training.Trainer(updater, (args.n_epoch, 'epoch'), out=args.output_dir)\n\nsnapshot_interval = (args.snapshot_interval, 'iteration')\n\n# Copy the chain with shared parameters to flip 'train' flag only in test\neval_model = model.copy()\neval_model.train = False\n\ntrainer.extend(extensions.Evaluator(val_iter, eval_model, device=args.gpu))\ntrainer.extend(extensions.dump_graph('main/loss'))\ntrainer.extend(extensions.snapshot(), trigger=snapshot_interval)\ntrainer.extend(extensions.snapshot_object(\n    model, 'model_iter_{.updater.iteration}'), trigger=snapshot_interval)\ntrainer.extend(extensions.snapshot_object(\n    optimizer, 'optimizer_iter_{.updater.iteration}'), trigger=snapshot_interval)\ntrainer.extend(extensions.LogReport())\ntrainer.extend(extensions.PrintReport(\n    ['epoch', 'main/loss', 'validation/main/loss',\n     'main/accuracy', 'validation/main/accuracy']))\ntrainer.extend(extensions.ProgressBar(update_interval=10))\n\nif args.resume:\n    if not os.path.exists(args.resume):\n        raise IOError('Resume file is not exists.')\n    logging.info('Load optimizer state from {}'.format(args.resume))\n    chainer.serializers.load_npz(args.resume, trainer)\n\ntrainer.run()\n\n# Save the trained model\nchainer.serializers.save_npz(os.path.join(args.output_dir, 'model_final'), model)\nchainer.serializers.save_npz(os.path.join(args.output_dir, 'optimizer_final'), optimizer)\n\nprint()\nlogging.info('Saved the model and the optimizer')\nlogging.info('Training is finished!')\n```\n\n`extensions.snapshot()`\u3067\u4fdd\u5b58\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306ftrainer\u7528\u306e\u3082\u306e\u306e\u305f\u3081\u3001\u5b9f\u969b\u306b\u4e88\u6e2c\u3059\u308b\u969b\u306b\u8aad\u307f\u8fbc\u3080`model`\u3068`optimizer`\u3092`extensions.snapshot_object()`\u3067\u5225\u9014\u4fdd\u5b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n## \u307e\u3068\u3081\n\nChainer Trainer\u3092\u4f7f\u3044\u72ec\u81ea\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5b66\u7fd2\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\nTrainer\u3092\u4f7f\u3063\u305f\u611f\u60f3\u3068\u3057\u3066\u306f\u3001\u4e88\u60f3\u901a\u308aKeras\u306b\u8fd1\u3044\u3068\u3044\u3046\u5370\u8c61\u3067\u3059\u3002\u521d\u3081\u3066Chainer\u3092\u4f7f\u3063\u3066\u307f\u305f\u3068\u304d\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u6bce\u306b\u8aad\u307f\u8fbc\u3080\u7b87\u6240\u3067\u624b\u9593\u304c\u304b\u304b\u3063\u3066\u3057\u307e\u3063\u305f\u8a18\u61b6\u304c\u3042\u308b\u305f\u3081\u3001\u305d\u306e\u3088\u3046\u306a\u90e8\u5206\u304c\u62bd\u8c61\u5316\u3055\u308c\u3066\u3044\u308bTrainer\u306f\u308f\u304b\u308a\u3084\u3059\u3044\u5b9f\u88c5\u3060\u3068\u611f\u3058\u307e\u3057\u305f\u3002\n\n\u3057\u304b\u3057\u3001Keras\u3067\u306f[ImageDataGenerator\u30af\u30e9\u30b9\u306eflow_from_directory](http://keras.io/preprocessing/image/)\u3092\u4f7f\u3046\u3053\u3068\u3067Dataset\u30af\u30e9\u30b9\u306e\u5b9f\u88c5\u3092\u305b\u305a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3082\u3067\u304d\u308b\u305f\u3081\u3001\u3088\u308a\u7c21\u5358\u306b\u4f5c\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n\u6700\u5f8c\u306b\u5ba3\u4f1d\u306b\u306a\u308a\u307e\u3059\u304c\u3001CNN\u3092\u4f7f\u3063\u3066AV\u5973\u512a\u306e\u985e\u4f3c\u753b\u50cf\u691c\u7d22\u3092\u3057\u305f\u30b5\u30a4\u30c8\u3092\u4f5c\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3088\u304b\u3063\u305f\u3089\u898b\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n\n[Babelink - \u985e\u4f3cAV\u5973\u512a\u691c\u7d22\u30b5\u30fc\u30d3\u30b9](http://www.babelink.net/)\n\u203b\u30a2\u30c0\u30eb\u30c8\u30b5\u30a4\u30c8\u306e\u305f\u3081\u3001\u95b2\u89a7\u306b\u306f\u5341\u5206\u6ce8\u610f\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n\n", "tags": ["Chainer", "Python", "DeepLearning"]}