{"context": "\u3053\u306e\u6587\u66f8\u306f Dmitriy Selivanov \u306b\u3088\u308bR\u30d1\u30c3\u30b1\u30fc\u30b8text2vec (version 0.3.0) \u306e\u30d3\u30cd\u30c3\u30c8 \"Advanced topics\" \u306e\u65e5\u672c\u8a9e\u8a33\u3067\u3059\uff0e\n\u305f\u3060\u3057\u6587\u4e2d\u306e\u6ce8\u306f\u5168\u3066\u8a33\u8005\u306b\u3088\u308b\u3082\u306e\u3067\u30591\uff0e\nLicense: MIT\n\u95a2\u9023\u6587\u66f8\n\ntext2vec vignette: text2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790\ntext2vec vignette: GloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\ntext2vec vignette: \u767a\u5c55\u7684\u306a\u8a71\u984c\n\n\n\n\u30d5\u30a1\u30a4\u30eb\u306e\u53d6\u308a\u6271\u3044\n\u3053\u306e\u6bb5\u843d\u3067\u306f\uff0ctext2vec\u3092\u4f7f\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3055\u308c\u305f\u5de8\u5927\u306a\u30c6\u30ad\u30b9\u30c8\u306e\u96c6\u5408\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\uff0e\nlda\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u305f\u3044\u3068\u3057\u307e\u3057\u3087\u3046\uff0e\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u306e\u96c6\u307e\u308a\u3092\uff0c\u8907\u6570\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3067\u30c7\u30a3\u30b9\u30af\u306b\u4fdd\u5b58\u3057\u3066\u6301\u3063\u3066\u3044\u308b\u3068\u3057\u307e\u3059\uff0e\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u306e\u305f\u3081\u306b\uff0c\u7d44\u307f\u8fbc\u307f\u306emovie_review\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u305d\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\uff0e\nlibrary(text2vec)\nlibrary(magrittr)\ndata(\"movie_review\")\n# \u8aad\u307f\u8fbc\u307f\u3092\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\u306b\uff0c\u5168\u3066\u306e\u6539\u884c\u3092\u9664\u3044\u3066\u304a\u304f\nmovie_review$review <- gsub(pattern = '\\n', replacement = ' ', \n                            x = movie_review$review, fixed = TRUE)\nN_FILES <- 10\nCHUNK_LEN <- nrow(movie_review) / N_FILES\nfiles <- sapply(1:N_FILES, function(x) tempfile())\nchunks <- split(movie_review, rep(1:N_FILES, each = nrow(movie_review) / N_FILES ))\nfor (i in 1:N_FILES ) {\n  write.table(chunks[[i]], files[[i]], quote = T, row.names = F, col.names = T, sep = '|')\n}\n# \u3069\u3093\u306a\u30c7\u30fc\u30bf\u304b\u898b\u3066\u304a\u304f\nstr(movie_review, strict.width = 'cut')\n\n## 'data.frame':    5000 obs. of  3 variables:\n##  $ id       : chr  \"5814_8\" \"2381_9\" \"7759_3\" \"3630_4\" ...\n##  $ sentiment: int  1 1 0 0 1 1 0 0 0 1 ...\n##  $ review   : chr  \"With all this stuff going down at the moment with MJ\"..\n\ntext2vec\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u5bb9\u6613\u306b\u6271\u3046\u305f\u3081\u306e\u95a2\u6570\u7fa4\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\n\u30e6\u30fc\u30b6\u306f\u307b\u3093\u306e\u3044\u304f\u3064\u304b\u306e\u4f5c\u696d\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u3060\u3051\u3067\u3059\uff0e\n\n\nifiles\u95a2\u6570\u3067\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u69cb\u7bc9\u3059\u308b\uff0e\n\n\n\nifiles\u306b\u30d5\u30a1\u30a4\u30eb\u8aad\u8fbc\u95a2\u6570\u3092\u4e0e\u3048\u308b\uff0etext2vec\u306f\u80cc\u5f8c\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066\u306f\u4f55\u3082\u77e5\u308a\u307e\u305b\u3093\uff0e\u30d7\u30ec\u30fc\u30f3\u30c6\u30ad\u30b9\u30c8\u3067\u3082\u4f55\u3089\u304b\u306e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u3067\u3082\u69cb\u3044\u307e\u305b\u3093\uff0e\n\n\n\nitoken\u95a2\u6570\u306b\u3088\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u69cb\u7bc9\u3059\u308b\uff0e\n\n\u3069\u3046\u3084\u308b\u306e\u304b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\nlibrary(data.table)\nreader <- function(x, ...) {\n  # \u8aad\u307f\u8fbc\u307f\n  chunk <- fread(x, header = T, sep = '|')\n  # review\u5217\u3092\u9078\u629e\n  res <- chunk$review\n  # \u30ec\u30d3\u30e5\u30fc\u306bid\u3092\u632f\u308b\n  names(res) <- chunk$id\n  res\n}\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f5c\u6210\nit_files  <- ifiles(files, reader_function = reader)\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u304b\u3089\u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f5c\u6210\nit_tokens <- itoken(it_files, preprocess_function = tolower, \n                    tokenizer = word_tokenizer, progessbar = FALSE)\n\nvocab <- create_vocabulary(it_tokens)\n\n\u3053\u308c\u3067DTM\u3092\uff08lda\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u5fc5\u8981\u306a\uff09lda_c\u5f62\u5f0f\u3067\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n# \u30a4\u30c6\u30ec\u30fc\u30bf\u306f\u518d\u521d\u671f\u5316\u304c\u5fc5\u8981\uff01\n# \u30a4\u30c6\u30ec\u30fc\u30bf\u306f\u5909\u66f4\u53ef\u80fd\uff08mutable\uff09\u3067\u65e2\u306b\u7a7a\u306b\u306a\u3063\u3066\u3044\u308b\uff01\n# try(it_files$nextElem())\nit_files  <- ifiles(files, reader_function = reader)\nit_tokens <- itoken(it_files, preprocess_function = tolower, \n                    tokenizer = word_tokenizer, progessbar = FALSE)\n\ndtm <- create_dtm(it_tokens, vectorizer = vocab_vectorizer(vocab), type = 'lda_c')\nstr(dtm, list.len = 5)\n\n## List of 5000\n##  $ 5814_8  : int [1:2, 1:228] 7489 1 7835 1 8174 3 8365 1 8816 4 ...\n##  $ 2381_9  : int [1:2, 1:109] 7835 1 8174 2 8816 2 9560 1 9562 1 ...\n##  $ 7759_3  : int [1:2, 1:253] 7593 1 7790 1 8174 1 8634 1 8708 1 ...\n##  $ 3630_4  : int [1:2, 1:218] 7410 1 7841 1 8174 1 8258 1 8572 1 ...\n##  $ 9495_8  : int [1:2, 1:256] 7489 1 7659 1 7835 1 7878 1 8174 1 ...\n##   [list output truncated]\n\nDTM\u306b\u6587\u66f8ID\u304c\u4ed8\u3044\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\u3053\u308c\u3089\u306eID\u306freader\u95a2\u6570\u3067\u5272\u308a\u5f53\u3066\u305f\u540d\u524d\u304b\u3089\u5f15\u304d\u7d99\u304c\u308c\u3066\u3044\u307e\u3059\uff0e\u30d5\u30a1\u30a4\u30eb\u3092\u6271\u3046\u3068\u304d\u306b\u6587\u66f8ID\u3092\u5272\u308a\u5f53\u3066\u308b\u4fbf\u5229\u306a\u65b9\u6cd5\u3067\u3059\uff0e\n\u3053\u308c\u3067lda::lda.collapsed.gibbs.sampler()\u95a2\u6570\u3092\u4f7f\u3063\u3066LDA\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\nlibrary(lda)\n# \u30c8\u30d4\u30c3\u30af\u306e\u4e8b\u524d\u5206\u5e03\nalpha = 0.1\n# \u5358\u8a9e\u306e\u4e8b\u524d\u5206\u5e03\neta = 0.001\n# \u30c8\u30d4\u30c3\u30af30\u500b\u306e\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\uff0eGibbs\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306f30\u56de\nlda_fit <- lda.collapsed.gibbs.sampler(documents = dtm, K = 30, \n                                       vocab = vocab$vocab$terms, \n                                       alpha = alpha, \n                                       eta = eta,\n                                       num.iterations = 30, \n                                       trace = 2L)\n\n\n\u30de\u30eb\u30c1\u30b3\u30a2\u3092\u4f7f\u3063\u305f\u4e26\u5217\u30e2\u30fc\u30c9\ncreate_dtm, create_tcm, create_vocabulary\u306f\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u30de\u30b7\u30f3\u3092\u900f\u904e\u7684\u306a\u3084\u308a\u65b9\u3067\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0eGloVe\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3067\u306fRcppParallel\u306b\u3088\u308b\u4f4e\u30ec\u30d9\u30eb\u30b9\u30ec\u30c3\u30c9\u306e\u4e26\u5217\u51e6\u7406\u3092\u4f7f\u7528\u3059\u308b\u306e\u3068\u306f\u5bfe\u7167\u7684\u306b\uff0c\u3053\u308c\u3089\u306e\u95a2\u6570\u306fforeach\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u901a\u3058\u3066\u6a19\u6e96\u7684\u306aR\u306e\u9ad8\u30ec\u30d9\u30eb\u4e26\u5217\u51e6\u7406\u3092\u4f7f\u7528\u3057\u307e\u3059\uff0e\u3053\u308c\u3089\u306e\u95a2\u6570\u306f\u67d4\u8edf\u3067\u3042\u308a\uff0cdoParallel\u3084doRedis\u7b49\uff0c\u3044\u308d\u3044\u308d\u306a\u4e26\u5217\u51e6\u7406\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\u305f\u3060\u3057\uff0c\u3053\u306e\u3088\u3046\u306a\u9ad8\u30ec\u30d9\u30eb\u306e\u4e26\u5217\u51e6\u7406\u306f\u5927\u304d\u306a\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u3092\u3082\u305f\u3089\u3059\u53ef\u80fd\u6027\u304c\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u30e6\u30fc\u30b6\u306f\u899a\u3048\u3066\u304a\u304f\u3079\u304d\u3067\u3059\uff0e\n\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u30de\u30b7\u30f3\u3092\u5229\u7528\u3059\u308b\u306e\u306b\u30e6\u30fc\u30b6\u304c\u624b\u52d5\u3067\u884c\u3046\u3079\u304d\u64cd\u4f5c\u306f2\u3064\u3060\u3051\u3067\u3059\uff0e\n\n\u4e26\u5217\u51e6\u7406\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u767b\u9332\u3059\u308b\uff0e\n\n\n\nitoken\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u306e\u5f62\u5f0f\u3067\u5206\u5272\u3055\u308c\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3059\u308b\uff0e\n\n\u4ee5\u4e0b\u306f\u7c21\u5358\u306a\u4f8b\u3067\u30592\uff0e\nN_WORKERS <- 4\nlibrary(doParallel)\n\n## Loading required package: foreach\n\n## Loading required package: iterators\n\n## Loading required package: parallel\n\n# \u4e26\u5217\u51e6\u7406\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u767b\u9332\nregisterDoParallel(N_WORKERS)\n\n# \u5206\u5272\u3092\u7528\u610f\u3059\u308b\n# \"jobs\"\u306fitoken\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\uff01\nN_SPLITS <- 4\n\njobs <- files %>% \n  split_into(N_SPLITS) %>% \n  lapply(ifiles, reader_function = reader) %>% \n  # \u65e2\u306b\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u3042\u308b\u306e\u3067\uff0cchunks_number\u306f1\u306b\u3059\u308b\u306e\u304c\u3088\u3044\n  lapply(itoken, chunks_number = 1, preprocess_function = tolower, \n         tokenizer = word_tokenizer, progessbar = FALSE)\n\n# \u3042\u308b\u3044\u306f\uff0c\u30c7\u30fc\u30bf\u304c\u30e1\u30e2\u30ea\u4e0a\u306b\u3042\u308b\u6642\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u5206\u5272\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\n#\n# review_chunks <- split_into(movie_review$review, N_SPLITS)\n# review_ids <- split_into(movie_review$id, N_SPLITS)\n#\n# jobs <- Map(function(doc, ids) {\n#  itoken(iterable = doc, ids = ids, preprocess_function = tolower, \n#         tokenizer = word_tokenizer, chunks_number = 1, progessbar = FALSE) \n# }, review_chunks, review_ids)\n\n# \u4ee5\u964d\u306e\u95a2\u6570\u547c\u3073\u51fa\u3057\u306f\u5168\u3066\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u6069\u6075\u3092\u53d7\u3051\u308b\n# \u305d\u308c\u305e\u308c\u306ejob\u304c\u5225\u3005\u306e\u30d7\u30ed\u30bb\u30b9\u3067\u8a55\u4fa1\u3055\u308c\u308b\n\n# \u8a9e\u5f59\u306e\u4f5c\u6210\nvocab <- create_vocabulary(jobs)\n\n# \u8a9e\u5f59\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u305fDTM\nv_vectorizer <- vocab_vectorizer(vocab)\nvocab_dtm_parallel <- create_dtm(jobs, vectorizer = v_vectorizer)\n\n# \u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u305fDTM\nh_vectorizer <- hash_vectorizer()\nhash_dtm_parallel <- create_dtm(jobs, vectorizer = h_vectorizer)\n\n# \u5171\u8d77\u56de\u6570\u306e\u7d71\u8a08\ntcm_vectorizer <- vocab_vectorizer(vocab, grow_dtm = FALSE, skip_grams_window = 5)\ntcm_parallel <- create_tcm(jobs, vectorizer = tcm_vectorizer)\n\n\n\n\n\n\n\u8a33\u8005\u306e\u74b0\u5883\u3092\u793a\u3057\u3066\u304a\u304f\uff0e\u00a0\u21a9\ndevtools::session_info()\n\n## Session info --------------------------------------------------------------\n\n##  setting  value                       \n##  version  R version 3.3.1 (2016-06-21)\n##  system   x86_64, linux-gnu           \n##  ui       X11                         \n##  language (EN)                        \n##  collate  ja_JP.UTF-8                 \n##  tz       <NA>                        \n##  date     2016-09-04\n\n## Packages ------------------------------------------------------------------\n\n##  package      * version date       source        \n##  Matrix         1.2-7.1 2016-09-01 CRAN (R 3.3.1)\n##  Rcpp           0.12.3  2016-01-10 CRAN (R 3.2.3)\n##  RcppParallel   4.3.20  2016-08-16 CRAN (R 3.3.1)\n##  chron          2.3-47  2015-06-24 CRAN (R 3.2.1)\n##  codetools      0.2-14  2015-07-15 CRAN (R 3.3.1)\n##  data.table   * 1.9.6   2015-09-19 CRAN (R 3.3.1)\n##  devtools       1.12.0  2016-06-24 CRAN (R 3.3.1)\n##  digest         0.6.9   2016-01-08 CRAN (R 3.2.3)\n##  doParallel   * 1.0.10  2015-10-14 CRAN (R 3.3.1)\n##  evaluate       0.9     2016-04-29 CRAN (R 3.3.1)\n##  foreach      * 1.4.3   2015-10-13 CRAN (R 3.3.1)\n##  formatR        1.2.1   2015-09-18 CRAN (R 3.2.3)\n##  htmltools      0.3.5   2016-03-21 CRAN (R 3.3.1)\n##  iterators    * 1.0.8   2015-10-13 CRAN (R 3.2.2)\n##  knitr          1.14    2016-08-13 CRAN (R 3.3.1)\n##  lattice        0.20-33 2015-07-14 CRAN (R 3.2.1)\n##  lda          * 1.4.2   2015-11-22 CRAN (R 3.3.1)\n##  magrittr     * 1.5     2014-11-22 CRAN (R 3.3.1)\n##  memoise        1.0.0   2016-01-29 CRAN (R 3.3.1)\n##  rmarkdown      1.0     2016-07-08 CRAN (R 3.3.1)\n##  stringi        1.0-1   2015-10-22 CRAN (R 3.2.2)\n##  stringr        1.0.0   2015-04-30 CRAN (R 3.2.2)\n##  text2vec     * 0.3.0   2016-03-31 CRAN (R 3.3.1)\n##  withr          1.0.2   2016-06-20 CRAN (R 3.3.1)\n##  yaml           2.1.13  2014-06-12 CRAN (R 3.2.3)\n\n\n\ndoParallel\u306e\u4ed5\u69d8\u306b\u3088\u308a\uff0c\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306fWindows\u3067\u306f\u52d5\u4f5c\u3057\u306a\u3044\uff0eWindows\u3067\u52d5\u304b\u3059\u306b\u306f\uff0c\u8a33\u8005\u306f\u672a\u691c\u8a3c\u3060\u304c\u3053\u306e\u3042\u305f\u308a\u304c\u53c2\u8003\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff0e\u00a0\u21a9\n\n\n\n\u3053\u306e\u6587\u66f8\u306f Dmitriy Selivanov \u306b\u3088\u308bR\u30d1\u30c3\u30b1\u30fc\u30b8[`text2vec`](https://cran.r-project.org/web/packages/text2vec/) (version 0.3.0) \u306e\u30d3\u30cd\u30c3\u30c8 \"[Advanced topics](https://cran.r-project.org/web/packages/text2vec/vignettes/advanced.html)\" \u306e\u65e5\u672c\u8a9e\u8a33\u3067\u3059\uff0e\n\u305f\u3060\u3057\u6587\u4e2d\u306e\u6ce8\u306f\u5168\u3066\u8a33\u8005\u306b\u3088\u308b\u3082\u306e\u3067\u3059[^translater_env]\uff0e\n\nLicense: [MIT](https://opensource.org/licenses/MIT)\n\n**\u95a2\u9023\u6587\u66f8**\n\n+ [text2vec vignette: text2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790](http://qiita.com/nakamichi/items/1bf2ee393cb407d9fb74)\n+ [text2vec vignette: GloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f](http://qiita.com/nakamichi/items/69142b6d19f307ec8d9d)\n+ text2vec vignette: \u767a\u5c55\u7684\u306a\u8a71\u984c\n\n----\n\n<!-- ## Working with files -->\n# \u30d5\u30a1\u30a4\u30eb\u306e\u53d6\u308a\u6271\u3044\n\n<!-- It this paragraph I will show how to use `text2vec` for vectorization of large collections of text stored in files. -->\n\u3053\u306e\u6bb5\u843d\u3067\u306f\uff0c`text2vec`\u3092\u4f7f\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3055\u308c\u305f\u5de8\u5927\u306a\u30c6\u30ad\u30b9\u30c8\u306e\u96c6\u5408\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u307e\u3059\uff0e\n\n<!-- Imagine we want to build a topic model with [lda](http://cran.r-project.org/package=lda) package. We have a collection of movie reviews stored in multiple text files on disk.  -->\n[lda](http://cran.r-project.org/package=lda)\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u305f\u3044\u3068\u3057\u307e\u3057\u3087\u3046\uff0e\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u306e\u96c6\u307e\u308a\u3092\uff0c\u8907\u6570\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3067\u30c7\u30a3\u30b9\u30af\u306b\u4fdd\u5b58\u3057\u3066\u6301\u3063\u3066\u3044\u308b\u3068\u3057\u307e\u3059\uff0e\n\n<!-- For this vignette we will create files from embedded `movie_review` dataset: -->\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u306e\u305f\u3081\u306b\uff0c\u7d44\u307f\u8fbc\u307f\u306e`movie_review`\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u305d\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\uff0e\n\n\n```r\nlibrary(text2vec)\nlibrary(magrittr)\ndata(\"movie_review\")\n# \u8aad\u307f\u8fbc\u307f\u3092\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\u306b\uff0c\u5168\u3066\u306e\u6539\u884c\u3092\u9664\u3044\u3066\u304a\u304f\nmovie_review$review <- gsub(pattern = '\\n', replacement = ' ', \n                            x = movie_review$review, fixed = TRUE)\nN_FILES <- 10\nCHUNK_LEN <- nrow(movie_review) / N_FILES\nfiles <- sapply(1:N_FILES, function(x) tempfile())\nchunks <- split(movie_review, rep(1:N_FILES, each = nrow(movie_review) / N_FILES ))\nfor (i in 1:N_FILES ) {\n  write.table(chunks[[i]], files[[i]], quote = T, row.names = F, col.names = T, sep = '|')\n}\n# \u3069\u3093\u306a\u30c7\u30fc\u30bf\u304b\u898b\u3066\u304a\u304f\nstr(movie_review, strict.width = 'cut')\n```\n\n```\n## 'data.frame':\t5000 obs. of  3 variables:\n##  $ id       : chr  \"5814_8\" \"2381_9\" \"7759_3\" \"3630_4\" ...\n##  $ sentiment: int  1 1 0 0 1 1 0 0 0 1 ...\n##  $ review   : chr  \"With all this stuff going down at the moment with MJ\"..\n```\n\n<!-- `text2vec` provides functions to **easily** work with files. -->\n`text2vec`\u306f\u30d5\u30a1\u30a4\u30eb\u3092**\u5bb9\u6613\u306b**\u6271\u3046\u305f\u3081\u306e\u95a2\u6570\u7fa4\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\n\n<!-- User need to perform only a few things:  -->\n\u30e6\u30fc\u30b6\u306f\u307b\u3093\u306e\u3044\u304f\u3064\u304b\u306e\u4f5c\u696d\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u3060\u3051\u3067\u3059\uff0e\n\n<!-- 1. Construct iterator over the files with `ifiles` function. -->\n1. `ifiles`\u95a2\u6570\u3067\u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u69cb\u7bc9\u3059\u308b\uff0e\n\n<!-- 1. Provide reader function to `ifiles`. `text2vec` doesn't know anything about underlying files. They can be in plain text or some binary format. -->\n2. `ifiles`\u306b\u30d5\u30a1\u30a4\u30eb\u8aad\u8fbc\u95a2\u6570\u3092\u4e0e\u3048\u308b\uff0e`text2vec`\u306f\u80cc\u5f8c\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066\u306f\u4f55\u3082\u77e5\u308a\u307e\u305b\u3093\uff0e\u30d7\u30ec\u30fc\u30f3\u30c6\u30ad\u30b9\u30c8\u3067\u3082\u4f55\u3089\u304b\u306e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u3067\u3082\u69cb\u3044\u307e\u305b\u3093\uff0e\n\n<!-- 1. Construct tokens iterator from files iterator via `itoken` function. -->\n3. `itoken`\u95a2\u6570\u306b\u3088\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u69cb\u7bc9\u3059\u308b\uff0e\n\n<!-- Lets see how it works: -->\n\u3069\u3046\u3084\u308b\u306e\u304b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n\n\n```r\nlibrary(data.table)\nreader <- function(x, ...) {\n  # \u8aad\u307f\u8fbc\u307f\n  chunk <- fread(x, header = T, sep = '|')\n  # review\u5217\u3092\u9078\u629e\n  res <- chunk$review\n  # \u30ec\u30d3\u30e5\u30fc\u306bid\u3092\u632f\u308b\n  names(res) <- chunk$id\n  res\n}\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f5c\u6210\nit_files  <- ifiles(files, reader_function = reader)\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u304b\u3089\u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f5c\u6210\nit_tokens <- itoken(it_files, preprocess_function = tolower, \n                    tokenizer = word_tokenizer, progessbar = FALSE)\n\nvocab <- create_vocabulary(it_tokens)\n```\n\n<!-- Now are able to construct DTM in `lda_c` format (as required by `lda` package): -->\n\u3053\u308c\u3067DTM\u3092\uff08`lda`\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u5fc5\u8981\u306a\uff09`lda_c`\u5f62\u5f0f\u3067\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n\n```r\n# \u30a4\u30c6\u30ec\u30fc\u30bf\u306f\u518d\u521d\u671f\u5316\u304c\u5fc5\u8981\uff01\n# \u30a4\u30c6\u30ec\u30fc\u30bf\u306f\u5909\u66f4\u53ef\u80fd\uff08mutable\uff09\u3067\u65e2\u306b\u7a7a\u306b\u306a\u3063\u3066\u3044\u308b\uff01\n# try(it_files$nextElem())\nit_files  <- ifiles(files, reader_function = reader)\nit_tokens <- itoken(it_files, preprocess_function = tolower, \n                    tokenizer = word_tokenizer, progessbar = FALSE)\n\ndtm <- create_dtm(it_tokens, vectorizer = vocab_vectorizer(vocab), type = 'lda_c')\nstr(dtm, list.len = 5)\n```\n\n```\n## List of 5000\n##  $ 5814_8  : int [1:2, 1:228] 7489 1 7835 1 8174 3 8365 1 8816 4 ...\n##  $ 2381_9  : int [1:2, 1:109] 7835 1 8174 2 8816 2 9560 1 9562 1 ...\n##  $ 7759_3  : int [1:2, 1:253] 7593 1 7790 1 8174 1 8634 1 8708 1 ...\n##  $ 3630_4  : int [1:2, 1:218] 7410 1 7841 1 8174 1 8258 1 8572 1 ...\n##  $ 9495_8  : int [1:2, 1:256] 7489 1 7659 1 7835 1 7878 1 8174 1 ...\n##   [list output truncated]\n```\n\n<!-- Note that DTM has document ids. They are inhereted from document names we assigned in `reader` function. This is a convenient way to assign document ids when working with files. -->\nDTM\u306b\u6587\u66f8ID\u304c\u4ed8\u3044\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\u3053\u308c\u3089\u306eID\u306f`reader`\u95a2\u6570\u3067\u5272\u308a\u5f53\u3066\u305f\u540d\u524d\u304b\u3089\u5f15\u304d\u7d99\u304c\u308c\u3066\u3044\u307e\u3059\uff0e\u30d5\u30a1\u30a4\u30eb\u3092\u6271\u3046\u3068\u304d\u306b\u6587\u66f8ID\u3092\u5272\u308a\u5f53\u3066\u308b\u4fbf\u5229\u306a\u65b9\u6cd5\u3067\u3059\uff0e\n\n<!-- Now we can fit `LDA` model using `lda::lda.collapsed.gibbs.sampler()` function: -->\n\u3053\u308c\u3067`lda::lda.collapsed.gibbs.sampler()`\u95a2\u6570\u3092\u4f7f\u3063\u3066`LDA`\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n\n```r\nlibrary(lda)\n# \u30c8\u30d4\u30c3\u30af\u306e\u4e8b\u524d\u5206\u5e03\nalpha = 0.1\n# \u5358\u8a9e\u306e\u4e8b\u524d\u5206\u5e03\neta = 0.001\n# \u30c8\u30d4\u30c3\u30af30\u500b\u306e\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\uff0eGibbs\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306f30\u56de\nlda_fit <- lda.collapsed.gibbs.sampler(documents = dtm, K = 30, \n                                       vocab = vocab$vocab$terms, \n                                       alpha = alpha, \n                                       eta = eta,\n                                       num.iterations = 30, \n                                       trace = 2L)\n```\n\n<!-- # Parallel mode - using multiple cores -->\n# \u30de\u30eb\u30c1\u30b3\u30a2\u3092\u4f7f\u3063\u305f\u4e26\u5217\u30e2\u30fc\u30c9\n\n<!-- `create_dtm`, `create_tcm`, `create_vocabulary` are albe to take advantage of multicore machines and do it in transparent manner. In contrast to GloVe fitting which uses low-level thread parallelism via `RcppParallel`, these functions use standart R high-level parallelism on top of `foreach` package. They are flexible and can use diffrent parallel backends - `doParallel`, `doRedis`, etc. But user should remember that such high-level parallelism **can** involve significant overhead. -->\n`create_dtm`, `create_tcm`, `create_vocabulary`\u306f\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u30de\u30b7\u30f3\u3092\u900f\u904e\u7684\u306a\u3084\u308a\u65b9\u3067\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0eGloVe\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3067\u306f`RcppParallel`\u306b\u3088\u308b\u4f4e\u30ec\u30d9\u30eb\u30b9\u30ec\u30c3\u30c9\u306e\u4e26\u5217\u51e6\u7406\u3092\u4f7f\u7528\u3059\u308b\u306e\u3068\u306f\u5bfe\u7167\u7684\u306b\uff0c\u3053\u308c\u3089\u306e\u95a2\u6570\u306f`foreach`\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u901a\u3058\u3066\u6a19\u6e96\u7684\u306aR\u306e\u9ad8\u30ec\u30d9\u30eb\u4e26\u5217\u51e6\u7406\u3092\u4f7f\u7528\u3057\u307e\u3059\uff0e\u3053\u308c\u3089\u306e\u95a2\u6570\u306f\u67d4\u8edf\u3067\u3042\u308a\uff0c`doParallel`\u3084`doRedis`\u7b49\uff0c\u3044\u308d\u3044\u308d\u306a\u4e26\u5217\u51e6\u7406\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\u305f\u3060\u3057\uff0c\u3053\u306e\u3088\u3046\u306a\u9ad8\u30ec\u30d9\u30eb\u306e\u4e26\u5217\u51e6\u7406\u306f\u5927\u304d\u306a\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u3092\u3082\u305f\u3089\u3059**\u53ef\u80fd\u6027\u304c\u3042\u308b**\u3068\u3044\u3046\u3053\u3068\u3092\u30e6\u30fc\u30b6\u306f\u899a\u3048\u3066\u304a\u304f\u3079\u304d\u3067\u3059\uff0e\n\n<!-- Only two things user should perform manually to take advantage of multicore machine:  -->\n\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u30de\u30b7\u30f3\u3092\u5229\u7528\u3059\u308b\u306e\u306b\u30e6\u30fc\u30b6\u304c\u624b\u52d5\u3067\u884c\u3046\u3079\u304d\u64cd\u4f5c\u306f2\u3064\u3060\u3051\u3067\u3059\uff0e\n\n<!-- 1. register parallel backend -->\n1. \u4e26\u5217\u51e6\u7406\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u767b\u9332\u3059\u308b\uff0e\n\n<!-- 1. prepare splits of input data in a form of `list` of `itoken` iterators. -->\n2. `itoken`\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u306e\u5f62\u5f0f\u3067\u5206\u5272\u3055\u308c\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3059\u308b\uff0e\n\n<!-- Here is simple example: -->\n\u4ee5\u4e0b\u306f\u7c21\u5358\u306a\u4f8b\u3067\u3059[^win]\uff0e\n\n\n```r\nN_WORKERS <- 4\nlibrary(doParallel)\n```\n\n```\n## Loading required package: foreach\n```\n\n```\n## Loading required package: iterators\n```\n\n```\n## Loading required package: parallel\n```\n\n```r\n# \u4e26\u5217\u51e6\u7406\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u767b\u9332\nregisterDoParallel(N_WORKERS)\n\n# \u5206\u5272\u3092\u7528\u610f\u3059\u308b\n# \"jobs\"\u306fitoken\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\uff01\nN_SPLITS <- 4\n\njobs <- files %>% \n  split_into(N_SPLITS) %>% \n  lapply(ifiles, reader_function = reader) %>% \n  # \u65e2\u306b\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u3042\u308b\u306e\u3067\uff0cchunks_number\u306f1\u306b\u3059\u308b\u306e\u304c\u3088\u3044\n  lapply(itoken, chunks_number = 1, preprocess_function = tolower, \n         tokenizer = word_tokenizer, progessbar = FALSE)\n\n# \u3042\u308b\u3044\u306f\uff0c\u30c7\u30fc\u30bf\u304c\u30e1\u30e2\u30ea\u4e0a\u306b\u3042\u308b\u6642\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u5206\u5272\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\n#\n# review_chunks <- split_into(movie_review$review, N_SPLITS)\n# review_ids <- split_into(movie_review$id, N_SPLITS)\n#\n# jobs <- Map(function(doc, ids) {\n#  itoken(iterable = doc, ids = ids, preprocess_function = tolower, \n#         tokenizer = word_tokenizer, chunks_number = 1, progessbar = FALSE) \n# }, review_chunks, review_ids)\n\n# \u4ee5\u964d\u306e\u95a2\u6570\u547c\u3073\u51fa\u3057\u306f\u5168\u3066\u30de\u30eb\u30c1\u30b3\u30a2\u306e\u6069\u6075\u3092\u53d7\u3051\u308b\n# \u305d\u308c\u305e\u308c\u306ejob\u304c\u5225\u3005\u306e\u30d7\u30ed\u30bb\u30b9\u3067\u8a55\u4fa1\u3055\u308c\u308b\n\n# \u8a9e\u5f59\u306e\u4f5c\u6210\nvocab <- create_vocabulary(jobs)\n\n# \u8a9e\u5f59\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u305fDTM\nv_vectorizer <- vocab_vectorizer(vocab)\nvocab_dtm_parallel <- create_dtm(jobs, vectorizer = v_vectorizer)\n\n# \u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u305fDTM\nh_vectorizer <- hash_vectorizer()\nhash_dtm_parallel <- create_dtm(jobs, vectorizer = h_vectorizer)\n\n# \u5171\u8d77\u56de\u6570\u306e\u7d71\u8a08\ntcm_vectorizer <- vocab_vectorizer(vocab, grow_dtm = FALSE, skip_grams_window = 5)\ntcm_parallel <- create_tcm(jobs, vectorizer = tcm_vectorizer)\n```\n\n\n\n----\n\n[^translater_env]: \u8a33\u8005\u306e\u74b0\u5883\u3092\u793a\u3057\u3066\u304a\u304f\uff0e\n    \n    ```r\n    devtools::session_info()\n    ```\n    \n    ```\n    ## Session info --------------------------------------------------------------\n    ```\n    \n    ```\n    ##  setting  value                       \n    ##  version  R version 3.3.1 (2016-06-21)\n    ##  system   x86_64, linux-gnu           \n    ##  ui       X11                         \n    ##  language (EN)                        \n    ##  collate  ja_JP.UTF-8                 \n    ##  tz       <NA>                        \n    ##  date     2016-09-04\n    ```\n    \n    ```\n    ## Packages ------------------------------------------------------------------\n    ```\n    \n    ```\n    ##  package      * version date       source        \n    ##  Matrix         1.2-7.1 2016-09-01 CRAN (R 3.3.1)\n    ##  Rcpp           0.12.3  2016-01-10 CRAN (R 3.2.3)\n    ##  RcppParallel   4.3.20  2016-08-16 CRAN (R 3.3.1)\n    ##  chron          2.3-47  2015-06-24 CRAN (R 3.2.1)\n    ##  codetools      0.2-14  2015-07-15 CRAN (R 3.3.1)\n    ##  data.table   * 1.9.6   2015-09-19 CRAN (R 3.3.1)\n    ##  devtools       1.12.0  2016-06-24 CRAN (R 3.3.1)\n    ##  digest         0.6.9   2016-01-08 CRAN (R 3.2.3)\n    ##  doParallel   * 1.0.10  2015-10-14 CRAN (R 3.3.1)\n    ##  evaluate       0.9     2016-04-29 CRAN (R 3.3.1)\n    ##  foreach      * 1.4.3   2015-10-13 CRAN (R 3.3.1)\n    ##  formatR        1.2.1   2015-09-18 CRAN (R 3.2.3)\n    ##  htmltools      0.3.5   2016-03-21 CRAN (R 3.3.1)\n    ##  iterators    * 1.0.8   2015-10-13 CRAN (R 3.2.2)\n    ##  knitr          1.14    2016-08-13 CRAN (R 3.3.1)\n    ##  lattice        0.20-33 2015-07-14 CRAN (R 3.2.1)\n    ##  lda          * 1.4.2   2015-11-22 CRAN (R 3.3.1)\n    ##  magrittr     * 1.5     2014-11-22 CRAN (R 3.3.1)\n    ##  memoise        1.0.0   2016-01-29 CRAN (R 3.3.1)\n    ##  rmarkdown      1.0     2016-07-08 CRAN (R 3.3.1)\n    ##  stringi        1.0-1   2015-10-22 CRAN (R 3.2.2)\n    ##  stringr        1.0.0   2015-04-30 CRAN (R 3.2.2)\n    ##  text2vec     * 0.3.0   2016-03-31 CRAN (R 3.3.1)\n    ##  withr          1.0.2   2016-06-20 CRAN (R 3.3.1)\n    ##  yaml           2.1.13  2014-06-12 CRAN (R 3.2.3)\n    ```\n\n[^win]: `doParallel`\u306e\u4ed5\u69d8\u306b\u3088\u308a\uff0c\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306fWindows\u3067\u306f\u52d5\u4f5c\u3057\u306a\u3044\uff0eWindows\u3067\u52d5\u304b\u3059\u306b\u306f\uff0c\u8a33\u8005\u306f\u672a\u691c\u8a3c\u3060\u304c[\u3053\u306e\u3042\u305f\u308a](http://stackoverflow.com/questions/34653567/doparallel-foreach-inconsistently-inherits-objects-from-parent-environment-e)\u304c\u53c2\u8003\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff0e\n", "tags": ["R", "NLP", "text2vec"]}