{"context": "\n\n1. \u306f\u3058\u3081\u306b\nKFC\u5e97\u8217\u60c5\u5831\u304b\u3089\u5e97\u8217\u540d\u79f0\u3001\u7def\u5ea6\u7d4c\u5ea6\u3001\u5c55\u958b\u30b5\u30fc\u30d3\u30b9\u7b49\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002\n\u5e97\u8217\u60c5\u5831\u306fjavascript\u3067\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001splash\u3092\u9593\u306b\u565b\u307e\u305b\u307e\u3059\u3002\nWeb\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u6ce8\u610f\u4e8b\u9805\u4e00\u89a7\n\n2. \u5b9f\u884c\u74b0\u5883\nAWS\nEC2: Amazon Linux (2016.09-release) ,t2.micro\nPython 2.7.12\n\n3. \u74b0\u5883\u69cb\u7bc9\n\npython2.7\nsudo yum groupinstall \"Development tools\" \nsudo yum install python-devel libffi-devel openssl-devel libxml2-devel libxslt-devel\nsudo pip install scrapy\nsudo pip install service_identity  #Amazon Linux\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u306e\u305f\u3081\u4e0d\u8981\n\nsudo yum -y install docker-io\nsudo service docker start\nsudo chkconfig docker on\n\nsudo pip install scrapy-splash\n\ndocker pull scrapinghub/splash\ndocker run -p 8050:8050 scrapinghub/splash\n\n\n\nsplash\u306fdocker\u3067\u8cc4\u3044\u307e\u3059\u3002\u304a\u624b\u8efd\u3002\n\uff08\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\uff09\nPython\u88fd\u30af\u30ed\u30fc\u30e9\u30fc\u300cScrapy\u300d\u306e\u59cb\u3081\u65b9\u30e1\u30e2\nscrapy-splash\u3092\u4f7f\u3063\u3066JavaScript\u5229\u7528\u30da\u30fc\u30b8\u3092\u7c21\u5358\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\ngithub (scrapy-splash)\n\n4. scrapy\n\nproject\u4f5c\u6210\nproject\u3068spider\u306e\u96db\u578b\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\npython2.7\nexport PRJ_NAME=KFCShopSpider\nscrapy startproject ${PRJ_NAME}\ncd ./${PRJ_NAME}/${PRJ_NAME}/spider\nscrapy genspider ${PRJ_NAME} kfc.co.jp\n\n\n\nitem\u5b9a\u7fa9\n\u53d6\u5f97\u3057\u305f\u3044\u9805\u76ee\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306f\u5e97\u8217\u540d\u3001\u4f4f\u6240\u3001map_url(\u7def\u5ea6\u7d4c\u5ea6\uff09\u3001\u5404\u7a2e\u30b5\u30fc\u30d3\u30b9\u306e\u6709\u7121\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\n~/KFCShopSpider/KFCShopSpider/items.py\n# -*- coding: utf-8 -*-\n\nimport scrapy\n\nclass KFCShopspiderItem(scrapy.Item):\n    name = scrapy.Field()\n    address = scrapy.Field()\n    map_url = scrapy.Field()\n    DriveThrough = scrapy.Field()\n    Parking = scrapy.Field()\n    Delivery = scrapy.Field()\n    Wlan = scrapy.Field()\n    pass\n\n\n\n\u521d\u671f\u8a2d\u5b9a\n\u76f8\u624b\u5148\u306b\u8ca0\u8377\u3092\u304b\u3051\u306a\u3044\u3088\u3046\u306b\u3002\nUSER_AGENT\u3001ROBOTSTXT_OBEY\u3001DOWNLOAD_DELAY\u3042\u305f\u308a\u306f\u5fc5\u9808\u3067\u3002\n\n~/KFCShopSpider/KFCShopSpider/settings.py\n# -*- coding: utf-8 -*-\n\nBOT_NAME = 'KFCShopSpider'\nSPIDER_MODULES = ['KFCShopSpider.spiders']\nNEWSPIDER_MODULE = 'KFCShopSpider.spiders'\n\nUSER_AGENT = 'KFCShopSpider (+http://www.yourdomain.com)'\nROBOTSTXT_OBEY = True\nDOWNLOAD_DELAY = 3\n\nSPIDER_MIDDLEWARES = {\n     'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n\nSPLASH_URL = 'http://localhost:8050/'\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\n\n\n\nspider\u30d7\u30ed\u30b0\u30e9\u30e0\n\n~/KFCShopSpider/KFCShopSpider/spider/KFCShop_spider.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy_splash import SplashRequest\nfrom ..items import KFCShopspiderItem\n\nclass KFCShopSpider(CrawlSpider):\n    name = \"KFCShopSpider\"\n    allowed_domains = [\"kfc.co.jp\"]\n\n    start_urls = []\n    shop_url_home ='http://www.kfc.co.jp/search/fuken.html?t=attr_con&kencode='\n    # 47\u90fd\u9053\u5e9c\u770c\u306e\u691c\u7d22\u7d50\u679c\u30da\u30fc\u30b8\u3092\u8d77\u70b9\u306b\u3059\u308b\u3002\n    for i in range(1,48):\n        prfct_id = '{0:02d}'.format(i)\n        url = shop_url_home + prfct_id\n        start_urls.append(url)\n\n    # \u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u6e08\u307f\u306eresponse\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url, self.parse,\n                args={'wait': 0.5},\n        )\n\n    def parse(self, response):\n        # \u5404\u5e97\u8217\u60c5\u5831(=list\u8981\u7d20)\u307e\u3067\u6307\u5b9a\u3059\u308b\u3002\n        stores = response.xpath('//ul[@id=\"outShop\"]/li')\n        for store in stores:\n            item = KFCShopspiderItem()\n            # \u76f8\u5bfe\u30d1\u30b9\u3067\u8a18\u8ff0\u3059\u308b\u3002\n            item['address']     = store.xpath('./span[@class=\"scAddress\"]/text()[1]').extract()\n            item['map_url']     = store.xpath('./ul/li[2]/div/a/@href').extract()\n            item['DriveThrough']= store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check04\")]/@alt').extract()\n            item['Parking']     = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check05\")]/@alt').extract()\n            item['Delivery']    = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check02\")]/@alt').extract()\n            item['Wlan']        = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check03\")]/@alt').extract()\n            yield item\n\n        # \u5404\u691c\u7d22\u7d50\u679c\u306e'\u6b21\u3078'\u306elink\u5148\u3092\u53d6\u5f97\u3057\u3066paese\u30e1\u30bd\u30c3\u30c9\u3092call\u3059\u308b\u3002\n        next_page= response.xpath('//li[@class=\"next\"]/a/@href')\n        if next_page:\n            # '\u6b21\u3078'\u304c\u5e97\u8217\u30ea\u30b9\u30c8\u306e\u4e0a\u4e0b\u306b2\u3064\u3042\u308b\u306e\u3067\u30011\u3064\u76ee\u306e\u8981\u7d20\u3060\u3051\u53d6\u5f97\n            url = response.urljoin(next_page[0].extract())\n            yield SplashRequest(url, self.parse)\n\n\n\n5. \u30c7\u30d0\u30c3\u30b0\nxpath\u306e\u78ba\u8a8d\u306fchrome\u306edeveloper tool(F12 key)\u304c\u4fbf\u5229\u3067\u3059\u3002\nElements view\u304b\u3089\u78ba\u8a8d\u3057\u305f\u3044\u8981\u7d20\u3092\u53f3\u30af\u30ea\u30c3\u30af>Copy>Copy XPath\u3067\u53d6\u5f97\u3067\u304d\u307e\u3059\u3002\n\npython2.7\nscrapy shell \"http://localhost:8050/render.html?url={\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u3057\u305f\u3044url}\"\n\n\n\n6. \u5b9f\u884c\n\u7d0410\u5206\u30671,000\u5e97\u8217\u5f37\u306e\u60c5\u5831\u3092\u53d6\u5f97\u3067\u304d\u307e\u3057\u305f\u3002\n\npython2.7\nscrapy crawl ${PRJ_NAME} -o hoge.csv\n\n\n#1. \u306f\u3058\u3081\u306b\n\n[KFC\u5e97\u8217\u60c5\u5831](http://www.kfc.co.jp/search/)\u304b\u3089\u5e97\u8217\u540d\u79f0\u3001\u7def\u5ea6\u7d4c\u5ea6\u3001\u5c55\u958b\u30b5\u30fc\u30d3\u30b9\u7b49\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002\n\u5e97\u8217\u60c5\u5831\u306fjavascript\u3067\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001splash\u3092\u9593\u306b\u565b\u307e\u305b\u307e\u3059\u3002\n\n[Web\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u6ce8\u610f\u4e8b\u9805\u4e00\u89a7](http://qiita.com/nezuq/items/c5e827e1827e7cb29011)\n\n#2. \u5b9f\u884c\u74b0\u5883\nAWS\nEC2: Amazon Linux (2016.09-release) ,t2.micro\nPython 2.7.12\n\n#3. \u74b0\u5883\u69cb\u7bc9\n\n\n```python2.7\nsudo yum groupinstall \"Development tools\" \nsudo yum install python-devel libffi-devel openssl-devel libxml2-devel libxslt-devel\nsudo pip install scrapy\nsudo pip install service_identity  #Amazon Linux\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u306e\u305f\u3081\u4e0d\u8981\n\nsudo yum -y install docker-io\nsudo service docker start\nsudo chkconfig docker on\n\nsudo pip install scrapy-splash\n\ndocker pull scrapinghub/splash\ndocker run -p 8050:8050 scrapinghub/splash\n\n```\n\nsplash\u306fdocker\u3067\u8cc4\u3044\u307e\u3059\u3002\u304a\u624b\u8efd\u3002\n\n\uff08\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\uff09\n[Python\u88fd\u30af\u30ed\u30fc\u30e9\u30fc\u300cScrapy\u300d\u306e\u59cb\u3081\u65b9\u30e1\u30e2](http://sechiro.hatenablog.com/entry/2016/04/02/Python%E8%A3%BD%E3%82%AF%E3%83%AD%E3%83%BC%E3%83%A9%E3%83%BC%E3%80%8CScrapy%E3%80%8D%E3%81%AE%E5%A7%8B%E3%82%81%E6%96%B9%E3%83%A1%E3%83%A2)\n[scrapy-splash\u3092\u4f7f\u3063\u3066JavaScript\u5229\u7528\u30da\u30fc\u30b8\u3092\u7c21\u5358\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0](http://amacbee.hatenablog.com/entry/2016/12/01/210436)\n[github (scrapy-splash)](https://github.com/scrapy-plugins/scrapy-splash/blob/master/README.rst)\n\n#4. scrapy\n##project\u4f5c\u6210\nproject\u3068spider\u306e\u96db\u578b\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```python2.7\nexport PRJ_NAME=KFCShopSpider\nscrapy startproject ${PRJ_NAME}\ncd ./${PRJ_NAME}/${PRJ_NAME}/spider\nscrapy genspider ${PRJ_NAME} kfc.co.jp\n```\n\n##item\u5b9a\u7fa9\n\u53d6\u5f97\u3057\u305f\u3044\u9805\u76ee\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306f\u5e97\u8217\u540d\u3001\u4f4f\u6240\u3001map_url(\u7def\u5ea6\u7d4c\u5ea6\uff09\u3001\u5404\u7a2e\u30b5\u30fc\u30d3\u30b9\u306e\u6709\u7121\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\n```~/KFCShopSpider/KFCShopSpider/items.py\n# -*- coding: utf-8 -*-\n\nimport scrapy\n\nclass KFCShopspiderItem(scrapy.Item):\n    name = scrapy.Field()\n    address = scrapy.Field()\n    map_url = scrapy.Field()\n    DriveThrough = scrapy.Field()\n    Parking = scrapy.Field()\n    Delivery = scrapy.Field()\n    Wlan = scrapy.Field()\n    pass\n```\n\n##\u521d\u671f\u8a2d\u5b9a\n\u76f8\u624b\u5148\u306b\u8ca0\u8377\u3092\u304b\u3051\u306a\u3044\u3088\u3046\u306b\u3002\nUSER_AGENT\u3001ROBOTSTXT_OBEY\u3001DOWNLOAD_DELAY\u3042\u305f\u308a\u306f\u5fc5\u9808\u3067\u3002\n\n```~/KFCShopSpider/KFCShopSpider/settings.py\n# -*- coding: utf-8 -*-\n\nBOT_NAME = 'KFCShopSpider'\nSPIDER_MODULES = ['KFCShopSpider.spiders']\nNEWSPIDER_MODULE = 'KFCShopSpider.spiders'\n\nUSER_AGENT = 'KFCShopSpider (+http://www.yourdomain.com)'\nROBOTSTXT_OBEY = True\nDOWNLOAD_DELAY = 3\n\nSPIDER_MIDDLEWARES = {\n     'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n\nSPLASH_URL = 'http://localhost:8050/'\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\n```\n\n##spider\u30d7\u30ed\u30b0\u30e9\u30e0\n\n```~/KFCShopSpider/KFCShopSpider/spider/KFCShop_spider.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy_splash import SplashRequest\nfrom ..items import KFCShopspiderItem\n\nclass KFCShopSpider(CrawlSpider):\n    name = \"KFCShopSpider\"\n    allowed_domains = [\"kfc.co.jp\"]\n\n    start_urls = []\n    shop_url_home ='http://www.kfc.co.jp/search/fuken.html?t=attr_con&kencode='\n    # 47\u90fd\u9053\u5e9c\u770c\u306e\u691c\u7d22\u7d50\u679c\u30da\u30fc\u30b8\u3092\u8d77\u70b9\u306b\u3059\u308b\u3002\n    for i in range(1,48):\n        prfct_id = '{0:02d}'.format(i)\n        url = shop_url_home + prfct_id\n        start_urls.append(url)\n\n    # \u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u6e08\u307f\u306eresponse\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url, self.parse,\n                args={'wait': 0.5},\n        )\n\n    def parse(self, response):\n        # \u5404\u5e97\u8217\u60c5\u5831(=list\u8981\u7d20)\u307e\u3067\u6307\u5b9a\u3059\u308b\u3002\n        stores = response.xpath('//ul[@id=\"outShop\"]/li')\n        for store in stores:\n            item = KFCShopspiderItem()\n            # \u76f8\u5bfe\u30d1\u30b9\u3067\u8a18\u8ff0\u3059\u308b\u3002\n            item['address']     = store.xpath('./span[@class=\"scAddress\"]/text()[1]').extract()\n            item['map_url']     = store.xpath('./ul/li[2]/div/a/@href').extract()\n            item['DriveThrough']= store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check04\")]/@alt').extract()\n            item['Parking']     = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check05\")]/@alt').extract()\n            item['Delivery']    = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check02\")]/@alt').extract()\n            item['Wlan']        = store.xpath('./span[@class=\"scIcon\"]/img[contains(./@src,\"check03\")]/@alt').extract()\n            yield item\n\n        # \u5404\u691c\u7d22\u7d50\u679c\u306e'\u6b21\u3078'\u306elink\u5148\u3092\u53d6\u5f97\u3057\u3066paese\u30e1\u30bd\u30c3\u30c9\u3092call\u3059\u308b\u3002\n        next_page= response.xpath('//li[@class=\"next\"]/a/@href')\n        if next_page:\n            # '\u6b21\u3078'\u304c\u5e97\u8217\u30ea\u30b9\u30c8\u306e\u4e0a\u4e0b\u306b2\u3064\u3042\u308b\u306e\u3067\u30011\u3064\u76ee\u306e\u8981\u7d20\u3060\u3051\u53d6\u5f97\n            url = response.urljoin(next_page[0].extract())\n            yield SplashRequest(url, self.parse)\n```\n\n#5. \u30c7\u30d0\u30c3\u30b0\n\nxpath\u306e\u78ba\u8a8d\u306fchrome\u306edeveloper tool(F12 key)\u304c\u4fbf\u5229\u3067\u3059\u3002\nElements view\u304b\u3089\u78ba\u8a8d\u3057\u305f\u3044\u8981\u7d20\u3092\u53f3\u30af\u30ea\u30c3\u30af>Copy>Copy XPath\u3067\u53d6\u5f97\u3067\u304d\u307e\u3059\u3002\n\n```python2.7\nscrapy shell \"http://localhost:8050/render.html?url={\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u3057\u305f\u3044url}\"\n```\n\n#6. \u5b9f\u884c\n\n\u7d0410\u5206\u30671,000\u5e97\u8217\u5f37\u306e\u60c5\u5831\u3092\u53d6\u5f97\u3067\u304d\u307e\u3057\u305f\u3002\n\n```python2.7\nscrapy crawl ${PRJ_NAME} -o hoge.csv\n```\n", "tags": ["Python", "Scrapy", "splash", "Web\u30af\u30ed\u30fc\u30ea\u30f3\u30b0", "Web\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0"]}