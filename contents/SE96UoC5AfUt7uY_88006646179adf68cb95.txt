{"context": "\n\n\u3084\u3063\u305f\u3053\u3068\n\nOne Class SVM \u3092\u4f7f\u3063\u305f\u5916\u308c\u5024\u691c\u77e5\n\u4e71\u6570\u4f7f\u3063\u3066\u751f\u6210\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\uff08\u6b63\u898f\u5206\u5e03\u3001\u6df7\u5408\u6b63\u898f\u5206\u5e03\uff09\u3092\u4f7f\u7528\nScikit-learn\u306e\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306b\u3042\u308b2.7. Novelty and Outlier Detection\u3092\u53c2\u8003\n\n\u306a\u304a\u3001\u4e0a\u8a18\u306e\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306e\u30da\u30fc\u30b8\u3067\u306f\u3001Robust Covariance Estimator (\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3092\u4f7f\u3063\u305f\u5916\u308c\u5024\u691c\u77e5\u624b\u6cd5\u3001\u6b63\u5e38\u30c7\u30fc\u30bf\u304c\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3053\u3068\u304c\u524d\u63d0\uff09\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u4eca\u56de\u306f\u5272\u611b\u3002\n\nOne-Class SVM\nSVM \u3092\u5229\u7528\u3057\u305f\u5916\u308c\u5024\u691c\u77e5\u624b\u6cd5\u3002\u30ab\u30fc\u30cd\u30eb\u3092\u4f7f\u3063\u3066\u7279\u5fb4\u7a7a\u9593\u306b\u5199\u50cf\u3001\u5143\u7a7a\u9593\u4e0a\u3067\u5b64\u7acb\u3057\u305f\u70b9\u306f\u3001\u7279\u5fb4\u7a7a\u9593\u3067\u306f\u539f\u70b9\u4ed8\u8fd1\u306b\u5206\u5e03\u3002Kernel\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306erbf\u3067\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5272\u5408\u3092\u6c7a\u3081\u308bnu(0~1\u306e\u7bc4\u56f2\u3001def.= 0.5)\u3092\u5909\u66f4\u3057\u3066\u307f\u308b\u3002\nScikit-learn One Class SVM\u306eObject\u306e\u30da\u30fc\u30b8\n\n\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\n\u6b63\u5e38\u30c7\u30fc\u30bf\u304c\u6df7\u5408\u6b63\u898f\u5206\u5e03\u3067\u8868\u3055\u308c\u308b\u306e3\u7a2e\u985e\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u610f\u3002\u4e00\u756a\u5de6\u306f\u5358\u72ec\u306e\u6b63\u898f\u5206\u5e03\u3001\u53f3\u306e\uff12\u3064\u306f\uff14\u3064\u306e\u6b63\u898f\u5206\u5e03\u306e\u91cd\u306d\u3042\u308f\u305b\u3002\u7570\u5e38\u30c7\u30fc\u30bf\u306f\u4e00\u69d8\u5206\u5e03\u3068\u3057\u305f\u3002\n\n\n\u7d50\u679c\n\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3068\u9055\u3063\u3066\u3001\u5143\u306e\u7a7a\u9593\u4e0a\u3067\u96e2\u308c\u3066\u3044\u308b\u5206\u5e03\u306e\u305d\u308c\u305e\u308c\u306e\u584a\u306b\u5bfe\u3057\u3066\u3001\u8b58\u5225\u5883\u754c\u304c\u3067\u304d\u3066\u3044\u308b\u3002nu\u304c\u5c0f\u3055\u3044\u307b\u3069\u3001\u4e00\u3064\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u306b\u611f\u5ea6\u304c\u9ad8\u304f\u3066\u8907\u96d1\u306b\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e0.5\u3060\u3068\u975e\u5e38\u306b\u5358\u7d14\u3002\n\n\n\n\n\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\n\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u3092\u53c2\u8003\u306b\u3002\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.font_manager\nfrom scipy import stats\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\n\n# Example settings\nn_samples = 400 # \u6a19\u672c\u6570\noutliers_fraction = 0.05 # \u5168\u6a19\u672c\u6570\u306e\u3046\u3061\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5272\u5408\nclusters_separation = [0, 1, 2]\n# \uff12\u6b21\u5143\u4f5c\u56f3\u7528\u683c\u5b50\u72b6\u30c7\u30fc\u30bf\u306e\u751f\u6210\nxx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))\n\n# \u6b63\u5e38\u30c7\u30fc\u30bf\u3068\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u751f\u6210\nn_inliers = int((1. - outliers_fraction) * n_samples) # \u6b63\u5e38\u30c7\u30fc\u30bf\u306e\u6a19\u672c\u6570\nn_outliers = int(outliers_fraction * n_samples) # \u7570\u5e38\u30c7\u30fc\u30bf\u306e\u6a19\u672c\u6570\nground_truth = np.ones(n_samples, dtype=int) # \u30e9\u30d9\u30eb\u30c7\u30fc\u30bf\nground_truth[-n_outliers:] = 0\n\n# Fit the problem with varying cluster separation\n# [enumerate\u95a2\u6570](http://python.civic-apps.com/zip-enumerate/)\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3068\u3082\u306b\u30eb\u30fc\u30d7\u3059\u308b\nfor i, offset in enumerate(clusters_separation): \n    np.random.seed(42)\n    # \u6b63\u5e38\u30c7\u30fc\u30bf\u751f\u6210\n    X1 = 0.3 * np.random.randn(0.25 * n_inliers, 2) - offset # \u6b63\u898f\u5206\u5e03 N(\u03bc= -offset, \u03c3=0.3)\n    X2 = 0.3 * np.random.randn(0.25 * n_inliers, 2) + offset # \u6b63\u898f\u5206\u5e03 N(\u03bc= +offset, \u03c3=0.3)\n\n    X3 = np.c_[\n            0.3 * np.random.randn(0.25 * n_inliers, 1) - 3*offset, # \u6b63\u898f\u5206\u5e03 N(\u03bc= -3*offset, \u03c3=0.3)\n            0.3 * np.random.randn(0.25 * n_inliers, 1) + 3*offset  # \u6b63\u898f\u5206\u5e03 N(\u03bc= +3*offset, \u03c3=0.3)\n        ]\n\n    X4 = np.c_[\n            0.3 * np.random.randn(0.25 * n_inliers, 1) + 3*offset, # \u6b63\u898f\u5206\u5e03 N(\u03bc= +3*offset, \u03c3=0.3)\n            0.3 * np.random.randn(0.25 * n_inliers, 1) - 3*offset  # \u6b63\u898f\u5206\u5e03 N(\u03bc= -3*offset, \u03c3=0.3)\n        ]\n\n    X = np.r_[X1, X2, X3, X4] # \u884c\u3067\u7d50\u5408\n    # \u5916\u308c\u5024\u30c7\u30fc\u30bf\u751f\u6210\n    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))] # \u4e00\u69d8\u5206\u5e03 -6 <= X <= +6\n\n        # Fit the model with the One-Class SVM\n    plt.figure(figsize=(10, 12))\n    # \u5916\u308c\u5024\u691c\u77e5\u306e\u30c4\u30fc\u30eb\u3001\uff11\u30af\u30e9\u30b9SVM\u3068Robust Covariance Estimator\n#    classifiers = {\n#    \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n#                                     kernel=\"rbf\", gamma=0.1),\n#    \"robust covariance estimator\": EllipticEnvelope(contamination=.1)} # \u5171\u5206\u6563\u63a8\u5b9a\n    nu_l = [0.05, 0.1, 0.5]\n    for j, nu in enumerate(nu_l):\n#    clf = svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05, kernel=\"rbf\", gamma=0.1)\n        clf = svm.OneClassSVM(nu=nu, kernel=\"rbf\", gamma='auto')\n        clf.fit(X)\n        y_pred = clf.decision_function(X).ravel() # \u5404\u30c7\u30fc\u30bf\u306e\u8d85\u5e73\u9762\u3068\u306e\u8ddd\u96e2\u3001ravel()\u3067\u914d\u5217\u30921D\u5316\n        threshold = stats.scoreatpercentile(y_pred, 100 * outliers_fraction) # \u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\u3067\u7570\u5e38\u5224\u5b9a\u306e\u95be\u5024\u8a2d\u5b9a\n        y_pred = y_pred > threshold\n        n_errors = (y_pred != ground_truth).sum() # \u8aa4\u5224\u5b9a\u306e\u6570\n\n        # plot the levels lines and the points\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) # \u683c\u5b50\u72b6\u306b\u8d85\u5e73\u9762\u3068\u306e\u8ddd\u96e2\u3092\u51fa\u529b\n        Z = Z.reshape(xx.shape)\n        subplot = plt.subplot(3, 3,i*3+j+1)\n        subplot.set_title(\"Outlier detection nu=%s\" % nu)\n        # \u4e88\u6e2c\u7d50\u679c\n        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7), cmap=plt.cm.Blues_r)\n        # \u8d85\u5e73\u9762\n        a = subplot.contour(xx, yy, Z, levels=[threshold], linewidths=2, colors='red')\n        # \u6b63\u5e38\u7bc4\u56f2\n        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='orange')\n        # \u6b63\u5e38\u30c7\u30fc\u30bf\n        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n        # \u7570\u5e38\u30c7\u30fc\u30bf\n        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n        subplot.axis('tight')\n        subplot.legend(\n            [a.collections[0], b, c],\n            ['learned decision function', 'true inliers', 'true outliers'],\n            prop=matplotlib.font_manager.FontProperties(size=11))\n    #    subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n        subplot.set_xlabel(\"%d. One class SVM (errors: %d)\" % (i+1, n_errors))\n        subplot.set_xlim((-7, 7))\n        subplot.set_ylim((-7, 7))\n        plt.subplots_adjust(0.04, 0.1, 1.2, 0.84, 0.1, 0.26)\n\nplt.show()\n\n# \u3084\u3063\u305f\u3053\u3068\n- One Class SVM \u3092\u4f7f\u3063\u305f\u5916\u308c\u5024\u691c\u77e5\n- \u4e71\u6570\u4f7f\u3063\u3066\u751f\u6210\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\uff08\u6b63\u898f\u5206\u5e03\u3001\u6df7\u5408\u6b63\u898f\u5206\u5e03\uff09\u3092\u4f7f\u7528\n- Scikit-learn\u306e[\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9](http://scikit-learn.org/stable/user_guide.html)\u306b\u3042\u308b[2.7. Novelty and Outlier Detection](http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html)\u3092\u53c2\u8003\n\n\u306a\u304a\u3001\u4e0a\u8a18\u306e\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306e\u30da\u30fc\u30b8\u3067\u306f\u3001Robust Covariance Estimator (\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3092\u4f7f\u3063\u305f\u5916\u308c\u5024\u691c\u77e5\u624b\u6cd5\u3001\u6b63\u5e38\u30c7\u30fc\u30bf\u304c\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u3053\u3068\u304c\u524d\u63d0\uff09\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u304c\u4eca\u56de\u306f\u5272\u611b\u3002\n\n# One-Class SVM\nSVM \u3092\u5229\u7528\u3057\u305f\u5916\u308c\u5024\u691c\u77e5\u624b\u6cd5\u3002\u30ab\u30fc\u30cd\u30eb\u3092\u4f7f\u3063\u3066\u7279\u5fb4\u7a7a\u9593\u306b\u5199\u50cf\u3001\u5143\u7a7a\u9593\u4e0a\u3067\u5b64\u7acb\u3057\u305f\u70b9\u306f\u3001\u7279\u5fb4\u7a7a\u9593\u3067\u306f\u539f\u70b9\u4ed8\u8fd1\u306b\u5206\u5e03\u3002Kernel\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306erbf\u3067\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5272\u5408\u3092\u6c7a\u3081\u308bnu(0~1\u306e\u7bc4\u56f2\u3001def.= 0.5)\u3092\u5909\u66f4\u3057\u3066\u307f\u308b\u3002\n[Scikit-learn One Class SVM\u306eObject\u306e\u30da\u30fc\u30b8](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM)\n\n# \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\n\u6b63\u5e38\u30c7\u30fc\u30bf\u304c\u6df7\u5408\u6b63\u898f\u5206\u5e03\u3067\u8868\u3055\u308c\u308b\u306e3\u7a2e\u985e\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u610f\u3002\u4e00\u756a\u5de6\u306f\u5358\u72ec\u306e\u6b63\u898f\u5206\u5e03\u3001\u53f3\u306e\uff12\u3064\u306f\uff14\u3064\u306e\u6b63\u898f\u5206\u5e03\u306e\u91cd\u306d\u3042\u308f\u305b\u3002\u7570\u5e38\u30c7\u30fc\u30bf\u306f\u4e00\u69d8\u5206\u5e03\u3068\u3057\u305f\u3002\n![image](https://qiita-image-store.s3.amazonaws.com/0/119826/b06f43d9-28d0-061e-34db-621016af2209.png)\n\n#\u7d50\u679c\n\u30de\u30cf\u30e9\u30ce\u30d3\u30b9\u8ddd\u96e2\u3068\u9055\u3063\u3066\u3001\u5143\u306e\u7a7a\u9593\u4e0a\u3067\u96e2\u308c\u3066\u3044\u308b\u5206\u5e03\u306e\u305d\u308c\u305e\u308c\u306e\u584a\u306b\u5bfe\u3057\u3066\u3001\u8b58\u5225\u5883\u754c\u304c\u3067\u304d\u3066\u3044\u308b\u3002nu\u304c\u5c0f\u3055\u3044\u307b\u3069\u3001\u4e00\u3064\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u306b\u611f\u5ea6\u304c\u9ad8\u304f\u3066\u8907\u96d1\u306b\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e0.5\u3060\u3068\u975e\u5e38\u306b\u5358\u7d14\u3002\n\n![image](https://qiita-image-store.s3.amazonaws.com/0/119826/f0db4598-06d4-d416-838a-55c00204651e.png)\n![image](https://qiita-image-store.s3.amazonaws.com/0/119826/e4914aca-477d-0f4b-c2fb-e3a2d888e149.png)\n![image](https://qiita-image-store.s3.amazonaws.com/0/119826/b5c909a5-66e3-f65b-c456-7f3c9e6ba184.png)\n\n\n# \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\n\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u3092\u53c2\u8003\u306b\u3002\n\n```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.font_manager\nfrom scipy import stats\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\n\n# Example settings\nn_samples = 400 # \u6a19\u672c\u6570\noutliers_fraction = 0.05 # \u5168\u6a19\u672c\u6570\u306e\u3046\u3061\u3001\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5272\u5408\nclusters_separation = [0, 1, 2]\n# \uff12\u6b21\u5143\u4f5c\u56f3\u7528\u683c\u5b50\u72b6\u30c7\u30fc\u30bf\u306e\u751f\u6210\nxx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))\n\n# \u6b63\u5e38\u30c7\u30fc\u30bf\u3068\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u751f\u6210\nn_inliers = int((1. - outliers_fraction) * n_samples) # \u6b63\u5e38\u30c7\u30fc\u30bf\u306e\u6a19\u672c\u6570\nn_outliers = int(outliers_fraction * n_samples) # \u7570\u5e38\u30c7\u30fc\u30bf\u306e\u6a19\u672c\u6570\nground_truth = np.ones(n_samples, dtype=int) # \u30e9\u30d9\u30eb\u30c7\u30fc\u30bf\nground_truth[-n_outliers:] = 0\n\n# Fit the problem with varying cluster separation\n# [enumerate\u95a2\u6570](http://python.civic-apps.com/zip-enumerate/)\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3068\u3082\u306b\u30eb\u30fc\u30d7\u3059\u308b\nfor i, offset in enumerate(clusters_separation): \n    np.random.seed(42)\n    # \u6b63\u5e38\u30c7\u30fc\u30bf\u751f\u6210\n    X1 = 0.3 * np.random.randn(0.25 * n_inliers, 2) - offset # \u6b63\u898f\u5206\u5e03 N(\u03bc= -offset, \u03c3=0.3)\n    X2 = 0.3 * np.random.randn(0.25 * n_inliers, 2) + offset # \u6b63\u898f\u5206\u5e03 N(\u03bc= +offset, \u03c3=0.3)\n    \n    X3 = np.c_[\n            0.3 * np.random.randn(0.25 * n_inliers, 1) - 3*offset, # \u6b63\u898f\u5206\u5e03 N(\u03bc= -3*offset, \u03c3=0.3)\n            0.3 * np.random.randn(0.25 * n_inliers, 1) + 3*offset  # \u6b63\u898f\u5206\u5e03 N(\u03bc= +3*offset, \u03c3=0.3)\n        ]\n\n    X4 = np.c_[\n            0.3 * np.random.randn(0.25 * n_inliers, 1) + 3*offset, # \u6b63\u898f\u5206\u5e03 N(\u03bc= +3*offset, \u03c3=0.3)\n            0.3 * np.random.randn(0.25 * n_inliers, 1) - 3*offset  # \u6b63\u898f\u5206\u5e03 N(\u03bc= -3*offset, \u03c3=0.3)\n        ]\n\n    X = np.r_[X1, X2, X3, X4] # \u884c\u3067\u7d50\u5408\n    # \u5916\u308c\u5024\u30c7\u30fc\u30bf\u751f\u6210\n    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))] # \u4e00\u69d8\u5206\u5e03 -6 <= X <= +6\n\n        # Fit the model with the One-Class SVM\n    plt.figure(figsize=(10, 12))\n    # \u5916\u308c\u5024\u691c\u77e5\u306e\u30c4\u30fc\u30eb\u3001\uff11\u30af\u30e9\u30b9SVM\u3068Robust Covariance Estimator\n#    classifiers = {\n#    \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n#                                     kernel=\"rbf\", gamma=0.1),\n#    \"robust covariance estimator\": EllipticEnvelope(contamination=.1)} # \u5171\u5206\u6563\u63a8\u5b9a\n    nu_l = [0.05, 0.1, 0.5]\n    for j, nu in enumerate(nu_l):\n#    clf = svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05, kernel=\"rbf\", gamma=0.1)\n        clf = svm.OneClassSVM(nu=nu, kernel=\"rbf\", gamma='auto')\n        clf.fit(X)\n        y_pred = clf.decision_function(X).ravel() # \u5404\u30c7\u30fc\u30bf\u306e\u8d85\u5e73\u9762\u3068\u306e\u8ddd\u96e2\u3001ravel()\u3067\u914d\u5217\u30921D\u5316\n        threshold = stats.scoreatpercentile(y_pred, 100 * outliers_fraction) # \u30d1\u30fc\u30bb\u30f3\u30bf\u30a4\u30eb\u3067\u7570\u5e38\u5224\u5b9a\u306e\u95be\u5024\u8a2d\u5b9a\n        y_pred = y_pred > threshold\n        n_errors = (y_pred != ground_truth).sum() # \u8aa4\u5224\u5b9a\u306e\u6570\n\n        # plot the levels lines and the points\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) # \u683c\u5b50\u72b6\u306b\u8d85\u5e73\u9762\u3068\u306e\u8ddd\u96e2\u3092\u51fa\u529b\n        Z = Z.reshape(xx.shape)\n        subplot = plt.subplot(3, 3,i*3+j+1)\n        subplot.set_title(\"Outlier detection nu=%s\" % nu)\n        # \u4e88\u6e2c\u7d50\u679c\n        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7), cmap=plt.cm.Blues_r)\n        # \u8d85\u5e73\u9762\n        a = subplot.contour(xx, yy, Z, levels=[threshold], linewidths=2, colors='red')\n        # \u6b63\u5e38\u7bc4\u56f2\n        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='orange')\n        # \u6b63\u5e38\u30c7\u30fc\u30bf\n        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n        # \u7570\u5e38\u30c7\u30fc\u30bf\n        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n        subplot.axis('tight')\n        subplot.legend(\n            [a.collections[0], b, c],\n            ['learned decision function', 'true inliers', 'true outliers'],\n            prop=matplotlib.font_manager.FontProperties(size=11))\n    #    subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n        subplot.set_xlabel(\"%d. One class SVM (errors: %d)\" % (i+1, n_errors))\n        subplot.set_xlim((-7, 7))\n        subplot.set_ylim((-7, 7))\n        plt.subplots_adjust(0.04, 0.1, 1.2, 0.84, 0.1, 0.26)\n\nplt.show()\n```\n", "tags": ["Python", "\u6a5f\u68b0\u5b66\u7fd2", "svm"]}