{"context": "\n\n\u76ee\u7684\n\n\nChainer 1.11.0 \u306e example\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b \u6587\u7ae0\u751f\u6210Task\uff08\u524d\u306e\u5358\u8a9e\u306e\u5c65\u6b74\u304b\u3089\u3001\u6b21\u306b\u51fa\u73fe\u3059\u308b\u5358\u8a9e\u3092\u4e88\u6e2c\u3059\u308b\u30bf\u30b9\u30af\uff09\u3092\u89e3\u304f\u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb \u306eChainer\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u300cptb\u300d\u306e\u30d5\u30a1\u30a4\u30eb\u914d\u7f6e\uff06\u30b3\u30fc\u30c9\u8a18\u8ff0 \u304c\u3001\u3059\u3067\u306b\u3044\u308d\u3093\u306a\u4eba\u304c\u89e3\u8aac\u3057\u3066\u3044\u308b\u904e\u53bb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306eChainer example\u30b3\u30fc\u30c9\u3068\u3044\u308d\u3044\u308d\u306a\u90e8\u5206\u3067\u5909\u308f\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\u7279\u306b\u3001example\u30b3\u30fc\u30c9\u3092\u3082\u3068\u306b\u3001\u81ea\u5206\u306e\u597d\u304d\u306a\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3044\u5834\u5408\u306b\u3001\u624b\u3092\u52a0\u3048\u308b\u3079\u304d\u3001\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u51e6\u7406\u3092\u8a18\u8ff0\u3057\u305f\u30b3\u30fc\u30c9\u304c\u3069\u3053\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u306e\u304b\u3001\u904e\u53bb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3 \u306e Chainer example\u30b3\u30fc\u30c9 \u3092\u89e3\u8aac\u3057\u305f\u591a\u304f\u306e\u8a18\u4e8b \u304c \u524d\u63d0\u3068\u3057\u3066\u3044\u305f \u304a\u624b\u672c\u30b3\u30fc\u30c9 \u304b\u3089 \u8a18\u8ff0\u5834\u6240 \u304c \u5909\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u7a81\u304d\u6b62\u3081\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u8abf\u3079\u3066\u7a81\u304d\u6b62\u3081\u305f\u3002\n\u3042\u308f\u305b\u3066\u3001example\u306e\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u8aad\u307f\u8fbc\u3093\u3067\u3044\u308b\u6587\u66f8\u30c7\u30fc\u30bf\u306e\u8a18\u8ff0\u66f8\u5f0f\u3092\u76ee\u3067\u898b\u3066\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u304a\u624b\u672c\u30b3\u30fc\u30c9 \u304c \u8aad\u307f\u8fbc\u307f\u53ef\u80fd\u306a\u6587\u66f8\u66f8\u5f0f \u3082\u3001\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n\nChainer example\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u3042\u308b ptb \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u3068\u306f \uff1f\n\nMNIST \u3084 word2vec \u3068\u4e26\u3093\u3067\u3001Preferred Networks\u793e \u306e Chainer\u516c\u5f0f GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u3001Chainer \u3092 git clone \u3059\u308b\u3068\u540c\u68b1\u3055\u308c\u3066\u3044\u308b \u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\uff06\u4e88\u6e2c \u3092 \u884c\u3046 Chainer\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\uff11\u3064\u3002\n\u300c\u524d\u306e\u5358\u8a9e\u306e\u5c65\u6b74\u304c\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067\u3001\u6b21\u306b\u51fa\u73fe\u3059\u308b\u5358\u8a9e\u3092\u4e88\u6e2c\u3059\u308b\u300d\u30bf\u30b9\u30af \u3092\u89e3\u304f\u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb \u3092 Chainer\u5b9f\u88c5\u3057\u305f\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\uff08\u4e00\u5f0f\uff09\u3067\u3059\u3002\n\nTerminal\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/examples/ptb\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ cat README.md \n# Recurrent Net Language Model\n\nThis is an example of a recurrent net for language modeling.\nThe network is trained to predict the word given the preceding word sequence.\n\nThis example is based on the following RNNLM implementation written in Torch7.\nhttps://github.com/tomsercu/lstm\n\nThis example requires the dataset to be downloaded by the script `download.py`.\nIf you want to run this example on the N-th GPU, pass `--gpu=N` to the script.\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \n\n\n\n\nver 1.11.0 \u3088\u308a\u53e4\u3044version \u306e Chainer \u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u305f ptb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d5\u30a1\u30a4\u30eb\u306e\u30b3\u30fc\u30c9\u8a18\u8ff0\n\n\u53e4\u3044version \u306e Preferred Network\u793e \u516c\u5f0fGitHub\u30ec\u30dd\u30b8\u30c8\u30ea\u306e  \u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3067\u306f\u3001ptb.net \u3068\u540c\u3058\u968e\u5c64\uff08\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\uff09\u306b download.py \u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u305f\u3002\n\u3057\u304b\u3057\u3001Chainer ver 1.11.0 \u306e GitHub\u30ec\u30dd\u30b8\u30c8\u30ea\u3067\u306f\u3001ptb.py \u3068\u540c\u968e\u5c64\u306b download.py \u30d5\u30a1\u30a4\u30eb \u306f \u683c\u7d0d\u3055\u308c\u3066\u3044\u306a\u3044\u3002\n\uff08\u53e4\u3044ver \u4f7f\u3063\u3066\u307f\u305f\u7cfb \u53c2\u8003\u30b5\u30a4\u30c8\uff09\nver1.5.0.2 \u3067\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u547c\u3073\u51fa\u3057\u3067\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092\u4ee5\u4e0b\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3067\u3044\u308b\u3002\ndownload.py\n\n\uff08\u53c2\u8003\uff09\n* Hatena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3 \uff082016/02/17\uff09\u300cexample\\ptb\u3092\u8aad\u3080\u300d \n\u4e0a\u8a18\u30b5\u30a4\u30c8\u3067\u89e3\u8aac\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u3001Chainer 1.5.0.2 \u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b example \u306e ptb.py \u3068\u306e\u3053\u3068\u3002\n\n\nTerminal\n# Prepare dataset (preliminary download dataset by ./download.py)\nvocab = {} \n\n\n\u5168\u8a9e\u5f59\u3092\u4fdd\u7ba1\u3059\u308b\u7528\u306e\u8f9e\u66f8\u3092\u6e96\u5099\u3002\u5358\u8a9e\u306b\u5bfe\u3057\u3066ID\u3092\u8fd4\u3059\u305f\u3081\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n\u3057\u304b\u3057\u3001Chainer ver 1.11.0 \u3092 PFN\u793e\u306e\u516c\u5f0fGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089git clone \u3057\u3066\u304d\u3066\u3082\u3001ptb.net \u3068\u540c\u3058\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b download.py \u306f\u5b58\u5728\u3057\u306a\u3044\u3002\n\n\nChainer 1.11.0 \u306b\u3082\u3001chainer/chainer/dataset\u76f4\u4e0b \u306b download.py \u3068\u3044\u3046\u540d\u524d \u306e Python\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u304c \u3042\u308b\n\n\nTerminal\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/chainer/dataset\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ \nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ cat download.py \nfrom __future__ import print_function\nimport hashlib\nimport os\nimport shutil\nimport tempfile\n\nimport filelock\nfrom six.moves.urllib import request\n\n\n_dataset_root = os.environ.get('CHAINER_DATASET_ROOT',\n                               os.path.expanduser('~/.chainer/dataset'))\n\n\ndef get_dataset_root():\n    \"\"\"Gets the path to the root directory to download and cache datasets.\n\n    Returns:\n        str: The path to the dataset root directory.\n\n    \"\"\"\n    return _dataset_root\n\n\ndef set_dataset_root(path):\n    \"\"\"Sets the root directory to download and cache datasets.\n\n    There are two ways to set the dataset root directory. One is by setting the\n    environment variable ``CHAINER_DATASET_ROOT``. The other is by using this\n    function. If both are specified, one specified via this function is used.\n    The default dataset root is ``$HOME/.chainer/dataset``.\n\n    Args:\n        path (str): Path to the new dataset root directory.\n\n    \"\"\"\n    global _dataset_root\n    _dataset_root = path\n\n\ndef get_dataset_directory(dataset_name, create_directory=True):\n    \"\"\"Gets the path to the directory of given dataset.\n\n    The generated path is just a concatenation of the global root directory\n    (see :func:`set_dataset_root` for how to change it) and the dataset name.\n    The dataset name can contain slashes, which are treated as path separators.\n\n    Args:\n        dataset_name (str): Name of the dataset.\n        create_directory (bool): If True (default), this function also creates\n            the directory at the first time. If the directory already exists,\n            then this option is ignored.\n\n    Returns:\n        str: Path to the dataset directory.\n\n    \"\"\"\n    path = os.path.join(_dataset_root, dataset_name)\n    if create_directory:\n        try:\n            os.makedirs(path)\n        except OSError:\n            pass\n    return path\n\n\ndef cached_download(url):\n    \"\"\"Downloads a file and caches it.\n\n    It downloads a file from the URL if there is no corresponding cache. After\n    the download, this function stores a cache to the directory under the\n    dataset root (see :func:`set_dataset_root`). If there is already a cache\n    for the given URL, it just returns the path to the cache without\n    downloading the same file.\n\n    Args:\n        url (str): URL to download from.\n\n    Returns:\n        str: Path to the downloaded file.\n\n    \"\"\"\n    cache_root = os.path.join(_dataset_root, '_dl_cache')\n    try:\n        os.makedirs(cache_root)\n    except OSError:\n        if not os.path.exists(cache_root):\n            raise RuntimeError('cannot create download cache directory')\n\n    lock_path = os.path.join(cache_root, '_dl_lock')\n    urlhash = hashlib.md5(url.encode('utf-8')).hexdigest()\n    cache_path = os.path.join(cache_root, urlhash)\n\n    with filelock.FileLock(lock_path):\n        if os.path.exists(cache_path):\n            return cache_path\n\n    temp_root = tempfile.mkdtemp(dir=cache_root)\n    try:\n        temp_path = os.path.join(temp_root, 'dl')\n        print('Downloading from {}...'.format(url))\n        request.urlretrieve(url, temp_path)\n        with filelock.FileLock(lock_path):\n            shutil.move(temp_path, cache_path)\n    finally:\n        shutil.rmtree(temp_root)\n\n    return cache_path\n\n\ndef cache_or_load_file(path, creator, loader):\n    \"\"\"Caches a file if it does not exist, or loads it otherwise.\n\n    This is a utility function used in dataset loading routines. The\n    ``creator`` creates the file to given path, and returns the content. If the\n    file already exists, the ``loader`` is called instead, and it loads the\n    file and returns the content.\n\n    Note that the path passed to the creator is temporary one, and not same as\n    the path given to this function. This function safely renames the file\n    created by the creator to a given path, even if this function is called\n    simultaneously by multiple threads or processes.\n\n    Args:\n        path (str): Path to save the cached file.\n        creator: Function to create the file and returns the content. It takes\n            a path to temporary place as the argument. Before calling the\n            creator, there is no file at the temporary path.\n        loader: Function to load the cached file and returns the content.\n\n    Returns:\n        It returns the returned values by the creator or the loader.\n\n    \"\"\"\n    if os.path.exists(path):\n        return loader(path)\n\n    file_name = os.path.basename(path)\n    temp_dir = tempfile.mkdtemp()\n    temp_path = os.path.join(temp_dir, file_name)\n\n    lock_path = os.path.join(_dataset_root, '_create_lock')\n\n    try:\n        content = creator(temp_path)\n        with filelock.FileLock(lock_path):\n            if not os.path.exists(path):\n                shutil.move(temp_path, path)\n    finally:\n        shutil.rmtree(temp_dir)\n\n    return content\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$\n\n\n\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u30e1\u30bd\u30c3\u30c9\u4e00\u89a7\n\nget_dataset_root()\nset_dataset_root(path)\nget_dataset_directory(dataset_name, create_directory=True)\ncached_download(url)\ncache_or_load_file(path, creator, loader)\n\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ cat download.py | grep def\ndef get_dataset_root():\ndef set_dataset_root(path):\n    The default dataset root is ``$HOME/.chainer/dataset``.\ndef get_dataset_directory(dataset_name, create_directory=True):\n        create_directory (bool): If True (default), this function also creates\ndef cached_download(url):\ndef cache_or_load_file(path, creator, loader):\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ \n\n\n\nChainer version 1.11.0 \u3067\u306f\u3001\u3053\u3053\u304c\u5909\u308f\u3063\u3066\u3044\u308b\n\n\nChainer version 1.11.0 \u4e2d \u306e \u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30af \u6df1\u5c64\u5b66\u7fd2 example Chainer \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb ptb \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u3067\u306f\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u51e6\u7406\u304c\u3069\u3053\u3067\u30b3\u30fc\u30c9\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u306e\u304b\u3092\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n\n\u3010 \u74b0\u5883 \u3011\n\n\nPython version: 3.5.2\nChainer version: 1.11.0\n\n\nTerminal\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv version\n2.7 (set by /Users/hirofumiyashima/.pyenv/version)\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv global 3.5.2\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv version\n3.5.2 (set by /Users/hirofumiyashima/.pyenv/version)\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import chainer\n>>> \n>>> print(chainer.__version__)\n1.11.0\n>>> \n\n\n\n1. train_ptb.py \u30d5\u30a1\u30a4\u30eb \u3067 get_ptb_words\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u3066\u3001train\u7528\u30c7\u30fc\u30bf\u3001val\u7528\u30c7\u30fc\u30bf\u3001test\u7528\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\n\nTerminal\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ cat train_ptb.py | grep get_ptb_words\n    train, val, test = chainer.datasets.get_ptb_words()\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \n\n\n\n \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u7528\u3001\u30c6\u30b9\u30c8\uff08\u8a55\u4fa1\uff09\u7528\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\u4f5c\u696d\u306fchainer.datasets\u3000\u306b\u3042\u308b .get_ptb_words() \u30e1\u30bd\u30c3\u30c9\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3060\u3002\n\n\n2. Desktop/chainer/chainer/datasets/ \u76f4\u4e0b\u306b\u306f\u3001init.py \u3068\u3000ptb.py\u3000\u304c\u3042\u308a\u3001ptb.py \u30d5\u30a1\u30a4\u30eb\u5185\u3067\u5b9a\u7fa9\u3055\u308c\u305f get_ptb_words \u306b\u5bfe\u3057\u3066\u3001init.py \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001get_ptb_words \u3068\u3044\u3046\u5225\u540d\u304c\u3064\u3051\u3089\u308c\u3066\u3044\u308b\u3002__\n\n\n__init__.py\nget_ptb_words = ptb.get_ptb_words\n\n\n\n\uff08 Desktop/chainer/chainer/datasets/ \u76f4\u4e0b\u306b\u306f\u3001init.py \u3068\u3000ptb.py\u3000\u304c\u3042\u308b \uff09\n\nTerminal\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ ls\n3.1.0.zip   Desktop     Library     Pictures    data        tensorflow\n3.1.0.zip.1 Documents   Movies      Public      log\nApplications    Downloads   Music       chainer     result\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ \nHirofumiYashima-no-MacBook:~ hirofumiyashima$ cd chainer\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ ls\nLICENSE         appveyor        chainer_bibtex.txt  docs            setup.cfg\nMANIFEST.in     appveyor.yml        chainer_setup_build.py  examples        setup.py\nREADME.md       chainer         cupy            install         tests\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ cd chainer\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ ls\n__init__.py     flag.py         gradient_check.py   links           serializers\ncomputational_graph.py  function.py     initializer.py      optimizer.py        testing\ncuda.py         function_hooks      initializers        optimizers      training\ndataset         function_set.py     iterators       reporter.py     utils\ndatasets        functions       link.py         serializer.py       variable.py\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ \nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ cd datasets\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ ls\n__init__.py     dict_dataset.py     mnist.py        sub_dataset.py\ncifar.py        image_dataset.py    ptb.py          tuple_dataset.py\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n\n\n\uff08 get_ptb_words \u306b\u5bfe\u3057\u3066\u3001init.py \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001get_ptb_words \u3068\u3044\u3046\u5225\u540d\u304c\u3064\u3051\u3089\u308c\u3066\u3044\u308b\u3002\uff09\n\n\u30b3\u30e1\u30f3\u30c8\u6587 #examples \u4ee5\u4e0b \u306b\u3001get_ptb_words = ptb.get_ptb_words\u3000\u306e\u8a18\u8ff0 \u304c \u3042\u308b\n\n\nTerminal\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat __init__.py\n\n\n\n__init__.py\nfrom chainer.datasets import cifar\nfrom chainer.datasets import dict_dataset\nfrom chainer.datasets import image_dataset\n\n\uff08\u3000\u4ee5\u4e0b\u7565\u3000\uff09\n\n# examples\n\nget_cifar10 = cifar.get_cifar10\nget_cifar100 = cifar.get_cifar100\nget_mnist = mnist.get_mnist\nget_ptb_words = ptb.get_ptb_words\nget_ptb_words_vocabulary = ptb.get_ptb_words_vocabulary\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n\n\nptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u306b\u3001get_ptb_words\u30e1\u30bd\u30c3\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u305f\u3002\n\u3055\u3089\u306b\u3001\u3053\u306e get_ptb_words\u30e1\u30bd\u30c3\u30c9\u306e\u4e2d\u3067\u3001_load_words(url) \u30e1\u30bd\u30c3\u30c9\u304c\u3001\u5f15\u6570 url \u3092\u6e21\u3055\u308c\u3066\u547c\u3073\u51fa\u3055\u308c\u3066\u3044\u305f\u3002\n\n\u5f15\u6570 url \u306f\u3001\u3053\u306e ptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306bURL\u30a2\u30c9\u30ec\u30b9\u6587\u5b57\u5217\u3000\u304c\u3000\u30d9\u30bf\u6253\u3061 \u3067 \u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3002\n\n\n\nptb.py\n_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\n\n\n\uff08 \u4e0a\u8a18 URL\u5148\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb \u306e \u4e2d\u8eab \uff09\n\n\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001\u30c8\u30fc\u30af\u30f3\uff08token\uff09\u3054\u3068\u306b \u5206\u304b\u3061\u66f8\u304d \u3057\u305f\u6587\u7ae0\u30c6\u30ad\u30b9\u30c8 \u304c\u914d\u7f6e\u3055\u308c\u305fURL\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308c\u3070\u3001ptb\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u306f \u554f\u984c\u306a\u304f\u52d5\u4f5c\u3059\u308b\u4ed5\u69d8 \u306b \u306a\u3063\u3066\u3044\u308b\u3001\u3068 \u898b\u306a\u305b\u308b\u3002\n\n\uff08 _train_url \uff09\n 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\n\n\uff08 _valid_url \uff09\n'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'\n\n\uff08 _test_url \uff09\n 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  \n\n\n\u4e0a\u8a18\u306eURL\u306f\u3001def get_ptb_words\u30e1\u30bd\u30c3\u30c9\u5192\u982d\u306e\u30b3\u30e1\u30f3\u30c8\u6ce8\u8a18\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3042\u308b\u3002\n\n\uff08\u8aad\u307f\u8fbc\u307f\u5148\uff09https://github.com/tomsercu/lstm\n\uff08 \u4e0a\u8a18\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30d1\u30b9\u306e\u6982\u8981 \uff09\n\u3000\u3082\u3068\u3082\u3068\uff08originally\uff09\u3001corpus of English sentences with linguistic structure annotations \u3067\u3042\u3063\u305f\u3000\u300cthe Penn Tree Bank dataset\u300d\uff08Penn Tree Bank https://www.cis.upenn.edu/~treebank/\uff09\u306e\u300ca variant distributed\u300d\u3067\u3042\u308b\u3002a variant \u3067\u3042\u308b\u7406\u7531\u306f\u3001github.com/tomsercu/lstm \u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3001\u300cThe Penn Tree Bank dataset\u300d\u304b\u3089\u3001omits the annotation \u3057\u305f\u4e0a\u3067\u3001and splits the dataset into three parts:\u3000training, validation, and test\u3000\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u304b\u3089\u3002\nThe Penn Treebank Project\n\nThe Penn Treebank Project annotates naturally-occuring text for linguistic structure. \nMost notably, we produce skeletal parses showing rough syntactic and semantic information -- a bank of linguistic trees. \nWe also annotate text with part-of-speech tags, and for the Switchboard corpus of telephone conversations, dysfluency annotation. \nWe are located in the LINC Laboratory of the Computer and Information Science Department at the University of Pennsylvania. \nAll data produced by the Treebank is released through the Linguistic Data Consortium. \n\n\nGitHub tomsercu / lstm\n\nGitHub tomsercu / lstm / data /\n\n\nptb.test.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab\n\n\n\nptb.train.txt\u30d5\u30a1\u30a4\u30eb \u683c\u7d0d\u5834\u6240\n\n\n\nptb.train.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab\n\n\n\nptb.valid.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab\n\n\n\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\uff08 \u4ee5\u4e0b\u3001ptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u306e \u4e2d\u8eab \uff09\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\nTerminal\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat ptb.py\n\n\n\nptb.py\nimport os\n\nimport numpy\n\nfrom chainer.dataset import download\n\n\ndef get_ptb_words():\n    \"\"\"Gets the Penn Tree Bank dataset as long word sequences.\n\n    `Penn Tree Bank <https://www.cis.upenn.edu/~treebank/>`_ is originally a\n    corpus of English sentences with linguistic structure annotations. This\n    function uses a variant distributed at\n    `https://github.com/tomsercu/lstm <https://github.com/tomsercu/lstm>`_,\n    which omits the annotation and splits the dataset into three parts:\n    training, validation, and test.\n\n    This function returns the training, validation, and test sets, each of\n    which is represented as a long array of word IDs. All sentences in the\n    dataset are concatenated by End-of-Sentence mark '<eos>', which is treated\n    as one of the vocabulary.\n\n    Returns:\n        tuple of numpy.ndarray: Int32 vectors of word IDs.\n\n    .. Seealso::\n       Use :func:`get_ptb_words_vocabulary` to get the mapping between the\n       words and word IDs.\n\n    \"\"\"\n    train = _retrieve_ptb_words('train.npz', _train_url)\n    valid = _retrieve_ptb_words('valid.npz', _valid_url)\n    test = _retrieve_ptb_words('test.npz', _test_url)\n    return train, valid, test\n\n\ndef get_ptb_words_vocabulary():\n    \"\"\"Gets the Penn Tree Bank word vocabulary.\n\n    Returns:\n        dict: Dictionary that maps words to corresponding word IDs. The IDs are\n            used in the Penn Tree Bank long sequence datasets.\n\n    .. seealso::\n       See :func:`get_ptb_words` for the actual datasets.\n\n    \"\"\"\n    return _retrieve_word_vocabulary()\n\n\n_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\ndef _retrieve_ptb_words(name, url):\n    def creator(path):\n        vocab = _retrieve_word_vocabulary()\n        words = _load_words(url)\n        x = numpy.empty(len(words), dtype=numpy.int32)\n        for i, word in enumerate(words):\n            x[i] = vocab[word]\n\n        numpy.savez_compressed(path, x=x)\n        return {'x': x}\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, name)\n    loaded = download.cache_or_load_file(path, creator, numpy.load)\n    return loaded['x']\n\n\ndef _retrieve_word_vocabulary():\n    def creator(path):\n        words = _load_words(_train_url)\n        vocab = {}\n        index = 0\n        with open(path, 'w') as f:\n            for word in words:\n                if word not in vocab:\n                    vocab[word] = index\n                    index += 1\n                    f.write(word + '\\n')\n\n        return vocab\n\n    def loader(path):\n        vocab = {}\n        with open(path) as f:\n            for i, word in enumerate(f):\n                vocab[word.strip()] = i\n        return vocab\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, 'vocab.txt')\n    return download.cache_or_load_file(path, creator, loader)\n\n\ndef _load_words(url):\n    path = download.cached_download(url)\n    words = []\n    with open(path) as words_file:\n        for line in words_file:\n            if line:\n                words += line.strip().split()\n                words.append('<eos>')\n    return words\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n\n\n\n\n\u3010 \u6b21\u306b\u3084\u308b\u3053\u3068 \u3011\n\n\u53d6\u5f97\u3057\u3066\u53d6\u308a\u8fbc\u3080 \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u306e\u53d6\u5f97\u5143URL \u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u30b3\u30fc\u30c9\u7b87\u6240\u304c\u308f\u304b\u3063\u305f\u306e\u3067\u3001web\u4e0a\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\uff08URL\u3067download\u3067\u304d\u308b\uff09\u5206\u304b\u3061\u66f8\u304d\u6e08\u307f \u306e \u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf \u3092\u53d6\u5f97\u3057\u3066\u3001\u5358\u8a9e\u4e88\u6e2c\u30bf\u30b9\u30af \u3092 \u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3044\u3002\n\u307e\u305f\u3001\u591a\u304f\u306e\u89e3\u8aac\u30b5\u30a4\u30c8\u3092\u983c\u308a\u306b\u3001\u30ed\u30fc\u30ab\u30eb\u306b\u3042\u308b\u6587\u66f8\u30d5\u30a1\u30a4\u30eb \u3092 Mecab-neoplogd \u306a\u3069 \u3067 \u5206\u304b\u3061\u66f8\u304d\u3057\u3066\u3001\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u3092 \u53d6\u308a\u8fbc\u3080\u3053\u3068\u3082\u3084\u3063\u3066\u307f\u305f\u3044\u3002\n\u305d\u306e\u624b\u59cb\u3081\u306b\u3001\u30b3\u30fc\u30c9\u306b\u4f55\u3082\u624b\u3092\u52a0\u3048\u306a\u3044\u3067\u3001train_ptb.py \u3092 \u305d\u306e\u307e\u307e\u5b9f\u884c\u3057\u3066\u307f\u305f\u3002\nCPU\u30e2\u30fc\u30c9 \u3067 \u8d70\u3089\u305b\u3066\u307f\u305f\u306e\u3067\u3001\u4e88\u60f3\u6240\u8981\u6642\u9593\u306f\u3001\u4f55\u3068\uff13\u65e5\u9593  \n\nTerminal\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/examples/ptb\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ python --version\nPython 3.5.2\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ time python train_ptb.py --gpu=-1\n\n\n\n\uff08 1\u6642\u9593\u307b\u3069\u7d4c\u3063\u305f\u5f8c\u306e\u753b\u9762 \uff09\n\n\n\n\u3010 \u53c2\u8003\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8 \u3011\n\n\nHatena Blog \u7d42\u672b A.I.\uff082016-03-19\uff09\u300cChainer \u306e ptb \u30b5\u30f3\u30d7\u30eb\u3067\u904a\u3093\u3067\u307f\u308b\u300d\nHatena::Diary shi3z\u306e\u9577\u6587\u65e5\u8a18\uff082015-07-14\uff09\u300cChainer\u306eptb\u30b5\u30f3\u30d7\u30eb\u3092\u89e3\u8aac\u3057\u3064\u3064\u3001\u81ea\u5206\u306e\u6587\u7ae0\u3092\u6df1\u5c64\u5b66\u7fd2\u3055\u305b\u3066\u3001\u50d5\u306e\u6587\u7ae0\u3063\u307d\u3044\u6587\u3092\u81ea\u52d5\u751f\u6210\u3055\u305b\u3066\u307f\u308b\u300d\nHatgena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3\uff082016-02-17\uff09\u300cexample/ptb\u3092\u8aad\u3080\u300d\nHatgena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3\uff082016-02-18\uff09\u300cptb\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u6587\u751f\u6210\u300d\n\n\n\nptb \u3092\u6539\u826f\u3057\u3066\u6587\u7ae0\u751f\u6210 LSTM Chainer\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u65b9\u3092\u89e3\u8aac\u3057\u3066\u3044\u308bWeb\u30b5\u30a4\u30c8\n\n\nHatena Blog kivantium\u6d3b\u52d5\u65e5\u8a18\uff082016-01-31\uff09\u300cChainer\u3067\u5b66\u3076LSTM\u300d\nHatena Blog sora_sakaki\u306e\u30d6\u30ed\u30b0\uff082015-07-16\uff09\u300c\u6a5f\u68b0\u5b66\u7fd2\u521d\u5fc3\u8005\u304c1\u304b\u3089Chainer\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u59cb\u3081\u3089\u308c\u308b\u307e\u3067\u306e\u7d4c\u7def\u3092\u66f8\u3044\u305f\u3089\u9577\u304f\u306a\u308a\u3059\u304e\u305f\u300d\nGushiSnow\u3055\u3093 Qiita\u8a18\u4e8b\uff082015/08/05\uff09\u300cChainer \u3092\u7528\u3044\u305f\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u8a00\u8a9e\u30e2\u30c7\u30eb\u4f5c\u6210\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u89e3\u8aac\u306b\u6311\u6226\u3057\u3066\u307f\u305f\u300d\n\n\n\n\u3010 Chainer example\u4ee5\u5916 \u306e \u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30afDeep neural network \u5b66\u7fd2\uff06\u65b0\u898f\u30c7\u30fc\u30bf \u4e88\u6e2cChainer\u30b9\u30af\u30ea\u30d7\u30c8\u4e8b\u4f8b \u3011\n\n\n\nGitHub yusuketomoto/chainer-char-rnn\n> karpathy's char-rnn (https://github.com/karpathy/char-rnn) implementation by Chainer\n\n\u4e0a\u8a18\u306f\u3001Torch\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u4e88\u6e2c\u30bf\u30b9\u30af\uff08\u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30af\uff09\u3092\u884c\u3046deep neural network\u3092\u5b66\u7fd2\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u3092Chainer\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\n\n\nGitHub karpathy/char-rnn\n\nMulti-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch\n\n\nAndrej Karpathy blog (May 21, 2015) The Unreasonable Effectiveness of Recurrent Neural Networks\n\n\u4e0a\u8a18 Chainer\u30b3\u30fc\u30c9\u3092\u6539\u826f\u3057\u3066\u8a66\u3057\u3066\u3044\u308b\u4e8b\u4f8b\n\nS346\u3055\u3093 Qiita\u8a18\u4e8b\uff082015/12/08\uff09\u300c\u3010\u30a8\u30f4\u30a1\u30f3\u30b2\u30ea\u30aa\u30f3\u3011\u30a2\u30b9\u30ab\u3063\u307d\u3044\u30bb\u30ea\u30d5\u3092DeepLearning\u3067\u81ea\u52d5\u751f\u6210\u3057\u3066\u307f\u308b\u300d\n\n\n\n\u3010 \u305d\u306e\u4ed6\u53c2\u8003 \u3011\n\n\nichiroex\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/06\uff09\u300c\u6df1\u5c64\u5b66\u7fd2\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30afChainer\u306e\u52c9\u5f37\u306b\u5f79\u7acb\u3064\u30da\u30fc\u30b8\u306e\u307e\u3068\u3081\u300d\nHatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18\uff082015/12/17\uff09\u300cDeepLearning\u3092\u4f7f\u3063\u305f\u5b9f\u88c5\u3092\u7e8f\u3081\u3066\u307f\u305f\u300d\n\n\n\n\u3010 \u88dc\u8db3 \u3011\n\nchainer.datasets.ptb\u30d5\u30a1\u30a4\u30eb \u306b get_ptb_words() \u30e1\u30bd\u30c3\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u3066\u3001\u3055\u3089\u306b\u3001\n\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u5143 URL \u304c\u30d9\u30bf\u6253\u3061\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u66f8\u304d\u65b9\u306f\u3001Chainer ver.1.12 \u3067\u3082\u5909\u308f\u3089\u306a\u3044\u3088\u3046\u3060\u3002\n\nChainer Docs \u00bb Module code \u00bb chainer \u00bb  Source code for chainer.datasets.ptb\n\n>_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n>_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n>_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\n\n\u3010 \u88dc\u8db3\uff12 \u3011\u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf\u306eLSTM\u5165\u529b\u524d\u306e\u5f62\u5f0f\u5909\u63db\u51e6\u7406\n\n\n\n\u4e0a\u8a18\u306e\u30da\u30f3\u30b7\u30eb\u30d0\u30cb\u30a2\u5927\u5b66\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060\u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf\u306f\u3001\u30b3\u30fc\u30c9\u306e\u4e2d\u3067\u3001\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u3066\u3001LSTM\u30e2\u30c7\u30eb\u3078\u306e\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\nPython\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> sample_sentence = \"I will examine what new sentence this sentence will be converted into by a Python code.\"\n>>> \n>>> words = sample_sentence.replace('/n', '<eos>').strip().split()\n>>> print(words)\n['I', 'will', 'examine', 'what', 'new', 'sentence', 'this', 'sentence', 'will', 'be', 'converted', 'into', 'by', 'a', 'Python', 'code.']\n>>>\n\n\n\nPython\n>>> vocab = {}\n>>> print(vocab)\n{}\n>>> \n>>> 'this' in vocab\nFalse\n>>> \n>>> 'Python' in vocab\nFalse\n>>> \n>>> for i, word in enumerate(words):\n...     if word not in vocab:\n...             vocab[word] = len(vocab)\n...     dataset[i] = vocab[word]\n... \n>>> \n>>> print(dataset)\n[ 0  1  2  3  4  5  6  5  1  7  8  9 10 11 12 13]\n>>> \n>>> print(vocab)\n{'converted': 8, 'this': 6, 'a': 11, 'will': 1, 'new': 4, 'Python': 12, 'sentence': 5, 'be': 7, 'into': 9, 'I': 0, 'examine': 2, 'code.': 13, 'what': 3, 'by': 10}\n>>>\n\n\n\n\u5358\u8a9e\u6587\u5b57\u5217 : \u4e0e\u3048\u305f\u6587\u5b57\u5217\uff08list\u578b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff09\u5185\u3067\u306e \u5f53\u8a72\u5358\u8a9e \u306e \u51fa\u73fe\u756a\u5730\uff08\u5192\u982d\u306f\u30bc\u30ed\uff09 \u304c \u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\n\n\nPython\n>>> print(words)\n['I', 'will', 'examine', 'what', 'new', 'sentence', 'this', 'sentence', 'will', 'be', 'converted', 'into', 'by', 'a', 'Python', 'code.']\n>>> \n>>> import types\n>>> print(type(vocab))\n<class 'dict'>\n>>> \n>>> print(vocab['I'])\n0\n>>> \n>>> print(vocab['will'])\n1\n>>> \n>>> print(vocab['examine'])\n2\n>>> \n>>> print(vocab['what'])\n3\n>>> \n\n\n\n\u4e0a\u8a18\u306e\u5909\u5316\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u308b\u30b3\u30fc\u30c9\u90e8\u5206\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\n\u4ee5\u524d\u306eversion \u306e Chainer example\u30b3\u30fc\u30c9\n\ngushiSnow\u3055\u3093 Qiita\u8a18\u4e8b\u300cChainer\u3092\u7528\u3044\u305f\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u8a00\u8a9e\u30e2\u30c7\u30eb\u4f5c\u6210\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u89e3\u8aac\u306b\u6311\u6226\u3057\u3066\u307f\u308b\u300d\n\n\n\u30c7\u30fc\u30bf\u4fdd\u6301\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\nvalid\u30c7\u30fc\u30bf\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n\u4e0a\u8a18\u3092\u305d\u308c\u305e\u308c\u884c\u5217\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4fdd\u6301\u3057\u307e\u3059\u3002\n\n\nPython\n# Prepare dataset (preliminary download dataset by ./download.py)\nvocab = {}\n\ndef load_data(filename):\n   global vocab, n_vocab\n   words = open(filename).read().replace('\\n', '<eos>').strip().split()\n   dataset = np.ndarray((len(words),), dtype=np.int32)\n   for i, word in enumerate(words):\n       if word not in vocab:\n           vocab[word] = len(vocab)\n       dataset[i] = vocab[word]\n   return dataset\n\ntrain_data = load_data('ptb.train.txt')\nvalid_data = load_data('ptb.valid.txt')\ntest_data = load_data('ptb.test.txt')\nprint('#vocab =', len(vocab))\n\n\n\n\n\n\nChainer ver.1.11.0 \u306e pub example\u30b3\u30fc\u30c9\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5909\u308f\u3063\u3066\u3044\u307e\u3059\u3002\n\n\nChainer ver.1.11.0 \u306e chainer/chainer/datasets/ptb.py \u306e \u30b3\u30fc\u30c9\n\n\nTerminal\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/chainer/datasets\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat ptb.py \nimport os\n\nimport numpy\n\nfrom chainer.dataset import download\n\n\ndef get_ptb_words():\n    \"\"\"Gets the Penn Tree Bank dataset as long word sequences.\n\n    `Penn Tree Bank <https://www.cis.upenn.edu/~treebank/>`_ is originally a\n    corpus of English sentences with linguistic structure annotations. This\n    function uses a variant distributed at\n    `https://github.com/tomsercu/lstm <https://github.com/tomsercu/lstm>`_,\n    which omits the annotation and splits the dataset into three parts:\n    training, validation, and test.\n\n    This function returns the training, validation, and test sets, each of\n    which is represented as a long array of word IDs. All sentences in the\n    dataset are concatenated by End-of-Sentence mark '<eos>', which is treated\n    as one of the vocabulary.\n\n    Returns:\n        tuple of numpy.ndarray: Int32 vectors of word IDs.\n\n    .. Seealso::\n       Use :func:`get_ptb_words_vocabulary` to get the mapping between the\n       words and word IDs.\n\n    \"\"\"\n    train = _retrieve_ptb_words('train.npz', _train_url)\n    valid = _retrieve_ptb_words('valid.npz', _valid_url)\n    test = _retrieve_ptb_words('test.npz', _test_url)\n    return train, valid, test\n\n\ndef get_ptb_words_vocabulary():\n    \"\"\"Gets the Penn Tree Bank word vocabulary.\n\n    Returns:\n        dict: Dictionary that maps words to corresponding word IDs. The IDs are\n            used in the Penn Tree Bank long sequence datasets.\n\n    .. seealso::\n       See :func:`get_ptb_words` for the actual datasets.\n\n    \"\"\"\n    return _retrieve_word_vocabulary()\n\n\n_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\ndef _retrieve_ptb_words(name, url):\n    def creator(path):\n        vocab = _retrieve_word_vocabulary()\n        words = _load_words(url)\n        x = numpy.empty(len(words), dtype=numpy.int32)\n        for i, word in enumerate(words):\n            x[i] = vocab[word]\n\n        numpy.savez_compressed(path, x=x)\n        return {'x': x}\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, name)\n    loaded = download.cache_or_load_file(path, creator, numpy.load)\n    return loaded['x']\n\n\ndef _retrieve_word_vocabulary():\n    def creator(path):\n        words = _load_words(_train_url)\n        vocab = {}\n        index = 0\n        with open(path, 'w') as f:\n            for word in words:\n                if word not in vocab:\n                    vocab[word] = index\n                    index += 1\n                    f.write(word + '\\n')\n\n        return vocab\n\n    def loader(path):\n        vocab = {}\n        with open(path) as f:\n            for i, word in enumerate(f):\n                vocab[word.strip()] = i\n        return vocab\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, 'vocab.txt')\n    return download.cache_or_load_file(path, creator, loader)\n\n\n\n###__\u76ee\u7684__\n\n* Chainer 1.11.0 \u306e example\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b \u6587\u7ae0\u751f\u6210Task\uff08\u524d\u306e\u5358\u8a9e\u306e\u5c65\u6b74\u304b\u3089\u3001\u6b21\u306b\u51fa\u73fe\u3059\u308b\u5358\u8a9e\u3092\u4e88\u6e2c\u3059\u308b\u30bf\u30b9\u30af\uff09\u3092\u89e3\u304f\u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb \u306eChainer\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u300cptb\u300d\u306e\u30d5\u30a1\u30a4\u30eb\u914d\u7f6e\uff06\u30b3\u30fc\u30c9\u8a18\u8ff0 \u304c\u3001\u3059\u3067\u306b\u3044\u308d\u3093\u306a\u4eba\u304c\u89e3\u8aac\u3057\u3066\u3044\u308b\u904e\u53bb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306eChainer example\u30b3\u30fc\u30c9\u3068\u3044\u308d\u3044\u308d\u306a\u90e8\u5206\u3067\u5909\u308f\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n* \u7279\u306b\u3001example\u30b3\u30fc\u30c9\u3092\u3082\u3068\u306b\u3001\u81ea\u5206\u306e\u597d\u304d\u306a\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3044\u5834\u5408\u306b\u3001\u624b\u3092\u52a0\u3048\u308b\u3079\u304d\u3001\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u51e6\u7406\u3092\u8a18\u8ff0\u3057\u305f\u30b3\u30fc\u30c9\u304c\u3069\u3053\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u306e\u304b\u3001\u904e\u53bb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3 \u306e Chainer example\u30b3\u30fc\u30c9 \u3092\u89e3\u8aac\u3057\u305f\u591a\u304f\u306e\u8a18\u4e8b \u304c \u524d\u63d0\u3068\u3057\u3066\u3044\u305f \u304a\u624b\u672c\u30b3\u30fc\u30c9 \u304b\u3089 \u8a18\u8ff0\u5834\u6240 \u304c \u5909\u308f\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u7a81\u304d\u6b62\u3081\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u8abf\u3079\u3066\u7a81\u304d\u6b62\u3081\u305f\u3002\n\n* \u3042\u308f\u305b\u3066\u3001example\u306e\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u8aad\u307f\u8fbc\u3093\u3067\u3044\u308b\u6587\u66f8\u30c7\u30fc\u30bf\u306e\u8a18\u8ff0\u66f8\u5f0f\u3092\u76ee\u3067\u898b\u3066\u78ba\u8a8d\u3059\u308b\u3053\u3068\u3067\u3001\u304a\u624b\u672c\u30b3\u30fc\u30c9 \u304c \u8aad\u307f\u8fbc\u307f\u53ef\u80fd\u306a\u6587\u66f8\u66f8\u5f0f \u3082\u3001\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n___\n\n###__Chainer example\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u3042\u308b ptb \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u3068\u306f \uff1f__\n\n**MNIST** \u3084 **word2vec** \u3068\u4e26\u3093\u3067\u3001Preferred Networks\u793e \u306e Chainer\u516c\u5f0f GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u3001Chainer \u3092 git clone \u3059\u308b\u3068\u540c\u68b1\u3055\u308c\u3066\u3044\u308b \u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\uff06\u4e88\u6e2c \u3092 \u884c\u3046 Chainer\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\uff11\u3064\u3002\n\n\u300c\u524d\u306e\u5358\u8a9e\u306e\u5c65\u6b74\u304c\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067\u3001\u6b21\u306b\u51fa\u73fe\u3059\u308b\u5358\u8a9e\u3092\u4e88\u6e2c\u3059\u308b\u300d\u30bf\u30b9\u30af \u3092\u89e3\u304f\u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb \u3092 Chainer\u5b9f\u88c5\u3057\u305f\u304a\u624b\u672c\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\uff08\u4e00\u5f0f\uff09\u3067\u3059\u3002\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/examples/ptb\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ cat README.md \n# Recurrent Net Language Model\n\nThis is an example of a recurrent net for language modeling.\nThe network is trained to predict the word given the preceding word sequence.\n\nThis example is based on the following RNNLM implementation written in Torch7.\nhttps://github.com/tomsercu/lstm\n\nThis example requires the dataset to be downloaded by the script `download.py`.\nIf you want to run this example on the N-th GPU, pass `--gpu=N` to the script.\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \n```\n\n___\n\n###__ver 1.11.0 \u3088\u308a\u53e4\u3044version \u306e Chainer \u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u305f ptb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d5\u30a1\u30a4\u30eb\u306e\u30b3\u30fc\u30c9\u8a18\u8ff0__ \n\n\u53e4\u3044version \u306e Preferred Network\u793e \u516c\u5f0fGitHub\u30ec\u30dd\u30b8\u30c8\u30ea\u306e  \u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3067\u306f\u3001ptb.net \u3068\u540c\u3058\u968e\u5c64\uff08\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\uff09\u306b download.py \u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u305f\u3002\n\n\u3057\u304b\u3057\u3001Chainer ver 1.11.0 \u306e GitHub\u30ec\u30dd\u30b8\u30c8\u30ea\u3067\u306f\u3001ptb.py \u3068\u540c\u968e\u5c64\u306b download.py \u30d5\u30a1\u30a4\u30eb \u306f \u683c\u7d0d\u3055\u308c\u3066\u3044\u306a\u3044\u3002\n\n\uff08\u53e4\u3044ver \u4f7f\u3063\u3066\u307f\u305f\u7cfb \u53c2\u8003\u30b5\u30a4\u30c8\uff09\n\nver1.5.0.2 \u3067\u306f\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u547c\u3073\u51fa\u3057\u3067\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092\u4ee5\u4e0b\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3067\u3044\u308b\u3002\n\ndownload.py\n\n___\n\n\uff08\u53c2\u8003\uff09\n* [Hatena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3 \uff082016/02/17\uff09\u300cexample\\ptb\u3092\u8aad\u3080\u300d ](http://chainernlpman.hatenablog.com/entry/2016/02/17/023545)\n\n\u4e0a\u8a18\u30b5\u30a4\u30c8\u3067\u89e3\u8aac\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u3001Chainer 1.5.0.2 \u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b example \u306e ptb.py \u3068\u306e\u3053\u3068\u3002\n\n> ```{bash:Terminal}\n># Prepare dataset (preliminary download dataset by ./download.py)\n>vocab = {} \n> ```\n>\n>\u5168\u8a9e\u5f59\u3092\u4fdd\u7ba1\u3059\u308b\u7528\u306e\u8f9e\u66f8\u3092\u6e96\u5099\u3002\u5358\u8a9e\u306b\u5bfe\u3057\u3066ID\u3092\u8fd4\u3059\u305f\u3081\u306b\u5229\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n>\n\n___\n\n\n\u3057\u304b\u3057\u3001Chainer ver 1.11.0 \u3092 PFN\u793e\u306e\u516c\u5f0fGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089git clone \u3057\u3066\u304d\u3066\u3082\u3001ptb.net \u3068\u540c\u3058\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b download.py \u306f\u5b58\u5728\u3057\u306a\u3044\u3002\n\n\n___\n\n####__Chainer 1.11.0 \u306b\u3082\u3001chainer/chainer/dataset\u76f4\u4e0b \u306b download.py \u3068\u3044\u3046\u540d\u524d \u306e Python\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u304c \u3042\u308b__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/chainer/dataset\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ \nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ cat download.py \nfrom __future__ import print_function\nimport hashlib\nimport os\nimport shutil\nimport tempfile\n\nimport filelock\nfrom six.moves.urllib import request\n\n\n_dataset_root = os.environ.get('CHAINER_DATASET_ROOT',\n                               os.path.expanduser('~/.chainer/dataset'))\n\n\ndef get_dataset_root():\n    \"\"\"Gets the path to the root directory to download and cache datasets.\n\n    Returns:\n        str: The path to the dataset root directory.\n\n    \"\"\"\n    return _dataset_root\n\n\ndef set_dataset_root(path):\n    \"\"\"Sets the root directory to download and cache datasets.\n\n    There are two ways to set the dataset root directory. One is by setting the\n    environment variable ``CHAINER_DATASET_ROOT``. The other is by using this\n    function. If both are specified, one specified via this function is used.\n    The default dataset root is ``$HOME/.chainer/dataset``.\n\n    Args:\n        path (str): Path to the new dataset root directory.\n\n    \"\"\"\n    global _dataset_root\n    _dataset_root = path\n\n\ndef get_dataset_directory(dataset_name, create_directory=True):\n    \"\"\"Gets the path to the directory of given dataset.\n\n    The generated path is just a concatenation of the global root directory\n    (see :func:`set_dataset_root` for how to change it) and the dataset name.\n    The dataset name can contain slashes, which are treated as path separators.\n\n    Args:\n        dataset_name (str): Name of the dataset.\n        create_directory (bool): If True (default), this function also creates\n            the directory at the first time. If the directory already exists,\n            then this option is ignored.\n\n    Returns:\n        str: Path to the dataset directory.\n\n    \"\"\"\n    path = os.path.join(_dataset_root, dataset_name)\n    if create_directory:\n        try:\n            os.makedirs(path)\n        except OSError:\n            pass\n    return path\n\n\ndef cached_download(url):\n    \"\"\"Downloads a file and caches it.\n\n    It downloads a file from the URL if there is no corresponding cache. After\n    the download, this function stores a cache to the directory under the\n    dataset root (see :func:`set_dataset_root`). If there is already a cache\n    for the given URL, it just returns the path to the cache without\n    downloading the same file.\n\n    Args:\n        url (str): URL to download from.\n\n    Returns:\n        str: Path to the downloaded file.\n\n    \"\"\"\n    cache_root = os.path.join(_dataset_root, '_dl_cache')\n    try:\n        os.makedirs(cache_root)\n    except OSError:\n        if not os.path.exists(cache_root):\n            raise RuntimeError('cannot create download cache directory')\n\n    lock_path = os.path.join(cache_root, '_dl_lock')\n    urlhash = hashlib.md5(url.encode('utf-8')).hexdigest()\n    cache_path = os.path.join(cache_root, urlhash)\n\n    with filelock.FileLock(lock_path):\n        if os.path.exists(cache_path):\n            return cache_path\n\n    temp_root = tempfile.mkdtemp(dir=cache_root)\n    try:\n        temp_path = os.path.join(temp_root, 'dl')\n        print('Downloading from {}...'.format(url))\n        request.urlretrieve(url, temp_path)\n        with filelock.FileLock(lock_path):\n            shutil.move(temp_path, cache_path)\n    finally:\n        shutil.rmtree(temp_root)\n\n    return cache_path\n\n\ndef cache_or_load_file(path, creator, loader):\n    \"\"\"Caches a file if it does not exist, or loads it otherwise.\n\n    This is a utility function used in dataset loading routines. The\n    ``creator`` creates the file to given path, and returns the content. If the\n    file already exists, the ``loader`` is called instead, and it loads the\n    file and returns the content.\n\n    Note that the path passed to the creator is temporary one, and not same as\n    the path given to this function. This function safely renames the file\n    created by the creator to a given path, even if this function is called\n    simultaneously by multiple threads or processes.\n\n    Args:\n        path (str): Path to save the cached file.\n        creator: Function to create the file and returns the content. It takes\n            a path to temporary place as the argument. Before calling the\n            creator, there is no file at the temporary path.\n        loader: Function to load the cached file and returns the content.\n\n    Returns:\n        It returns the returned values by the creator or the loader.\n\n    \"\"\"\n    if os.path.exists(path):\n        return loader(path)\n\n    file_name = os.path.basename(path)\n    temp_dir = tempfile.mkdtemp()\n    temp_path = os.path.join(temp_dir, file_name)\n\n    lock_path = os.path.join(_dataset_root, '_create_lock')\n\n    try:\n        content = creator(temp_path)\n        with filelock.FileLock(lock_path):\n            if not os.path.exists(path):\n                shutil.move(temp_path, path)\n    finally:\n        shutil.rmtree(temp_dir)\n\n    return content\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$\n```\n\n__\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u30e1\u30bd\u30c3\u30c9\u4e00\u89a7__\n\n* get_dataset_root()\n* set_dataset_root(path)\n* get_dataset_directory(dataset_name, create_directory=True)\n* cached_download(url)\n* cache_or_load_file(path, creator, loader)\n\n\n```{chainer/chainer/dataset} \nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ cat download.py | grep def\ndef get_dataset_root():\ndef set_dataset_root(path):\n    The default dataset root is ``$HOME/.chainer/dataset``.\ndef get_dataset_directory(dataset_name, create_directory=True):\n        create_directory (bool): If True (default), this function also creates\ndef cached_download(url):\ndef cache_or_load_file(path, creator, loader):\nHirofumiYashima-no-MacBook:dataset hirofumiyashima$ \n```\n\n___\n\n\n##__Chainer version 1.11.0 \u3067\u306f\u3001\u3053\u3053\u304c\u5909\u308f\u3063\u3066\u3044\u308b__\n\n####__Chainer version 1.11.0 \u4e2d \u306e \u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30af \u6df1\u5c64\u5b66\u7fd2 example Chainer \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb ptb \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u3067\u306f\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u51e6\u7406\u304c\u3069\u3053\u3067\u30b3\u30fc\u30c9\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u306e\u304b\u3092\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002__\n\n___\n\n###__\u3010 \u74b0\u5883 \u3011__\n\n* __Python version: 3.5.2__\n* __Chainer version: 1.11.0__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv version\n2.7 (set by /Users/hirofumiyashima/.pyenv/version)\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv global 3.5.2\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pyenv version\n3.5.2 (set by /Users/hirofumiyashima/.pyenv/version)\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import chainer\n>>> \n>>> print(chainer.__version__)\n1.11.0\n>>> \n```\n\n####1. train_ptb.py \u30d5\u30a1\u30a4\u30eb \u3067 get_ptb_words\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u3066\u3001train\u7528\u30c7\u30fc\u30bf\u3001val\u7528\u30c7\u30fc\u30bf\u3001test\u7528\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ cat train_ptb.py | grep get_ptb_words\n    train, val, test = chainer.datasets.get_ptb_words()\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \n```\n\n*  \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u7528\u3001\u30c6\u30b9\u30c8\uff08\u8a55\u4fa1\uff09\u7528\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\u4f5c\u696d\u306fchainer.datasets\u3000\u306b\u3042\u308b .get_ptb_words() \u30e1\u30bd\u30c3\u30c9\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3060\u3002\n\n####2. Desktop/chainer/chainer/datasets/ \u76f4\u4e0b\u306b\u306f\u3001__init__.py \u3068\u3000ptb.py\u3000\u304c\u3042\u308a\u3001ptb.py \u30d5\u30a1\u30a4\u30eb\u5185\u3067\u5b9a\u7fa9\u3055\u308c\u305f get_ptb_words \u306b\u5bfe\u3057\u3066\u3001__init__.py \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001get_ptb_words \u3068\u3044\u3046\u5225\u540d\u304c\u3064\u3051\u3089\u308c\u3066\u3044\u308b\u3002__\n\n>```{python:__init__.py}\n>get_ptb_words = ptb.get_ptb_words\n>```\n\n\n\uff08 Desktop/chainer/chainer/datasets/ \u76f4\u4e0b\u306b\u306f\u3001__init__.py \u3068\u3000ptb.py\u3000\u304c\u3042\u308b \uff09\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ ls\n3.1.0.zip\tDesktop\t\tLibrary\t\tPictures\tdata\t\ttensorflow\n3.1.0.zip.1\tDocuments\tMovies\t\tPublic\t\tlog\nApplications\tDownloads\tMusic\t\tchainer\t\tresult\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ \nHirofumiYashima-no-MacBook:~ hirofumiyashima$ cd chainer\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ ls\nLICENSE\t\t\tappveyor\t\tchainer_bibtex.txt\tdocs\t\t\tsetup.cfg\nMANIFEST.in\t\tappveyor.yml\t\tchainer_setup_build.py\texamples\t\tsetup.py\nREADME.md\t\tchainer\t\t\tcupy\t\t\tinstall\t\t\ttests\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ cd chainer\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ ls\n__init__.py\t\tflag.py\t\t\tgradient_check.py\tlinks\t\t\tserializers\ncomputational_graph.py\tfunction.py\t\tinitializer.py\t\toptimizer.py\t\ttesting\ncuda.py\t\t\tfunction_hooks\t\tinitializers\t\toptimizers\t\ttraining\ndataset\t\t\tfunction_set.py\t\titerators\t\treporter.py\t\tutils\ndatasets\t\tfunctions\t\tlink.py\t\t\tserializer.py\t\tvariable.py\nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ \nHirofumiYashima-no-MacBook:chainer hirofumiyashima$ cd datasets\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ ls\n__init__.py\t\tdict_dataset.py\t\tmnist.py\t\tsub_dataset.py\ncifar.py\t\timage_dataset.py\tptb.py\t\t\ttuple_dataset.py\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n```\n\n\uff08 get_ptb_words \u306b\u5bfe\u3057\u3066\u3001__init__.py \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001get_ptb_words \u3068\u3044\u3046\u5225\u540d\u304c\u3064\u3051\u3089\u308c\u3066\u3044\u308b\u3002\uff09\n\n* __\u30b3\u30e1\u30f3\u30c8\u6587 #examples \u4ee5\u4e0b \u306b\u3001get_ptb_words = ptb.get_ptb_words\u3000\u306e\u8a18\u8ff0 \u304c \u3042\u308b__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat __init__.py\n```\n\n```{Python:__init__.py}\nfrom chainer.datasets import cifar\nfrom chainer.datasets import dict_dataset\nfrom chainer.datasets import image_dataset\n\n\uff08\u3000\u4ee5\u4e0b\u7565\u3000\uff09\n\n# examples\n\nget_cifar10 = cifar.get_cifar10\nget_cifar100 = cifar.get_cifar100\nget_mnist = mnist.get_mnist\nget_ptb_words = ptb.get_ptb_words\nget_ptb_words_vocabulary = ptb.get_ptb_words_vocabulary\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n```\n\n\n__ptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u306b\u3001get_ptb_words\u30e1\u30bd\u30c3\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u305f\u3002__\n__\u3055\u3089\u306b\u3001\u3053\u306e get_ptb_words\u30e1\u30bd\u30c3\u30c9\u306e\u4e2d\u3067\u3001_load_words(url) \u30e1\u30bd\u30c3\u30c9\u304c\u3001\u5f15\u6570 url \u3092\u6e21\u3055\u308c\u3066\u547c\u3073\u51fa\u3055\u308c\u3066\u3044\u305f\u3002__\n\n* __\u5f15\u6570 url \u306f\u3001\u3053\u306e ptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306bURL\u30a2\u30c9\u30ec\u30b9\u6587\u5b57\u5217\u3000\u304c\u3000\u30d9\u30bf\u6253\u3061 \u3067 \u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3002__\n\n>```{Python:ptb.py}\n>_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n>_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n>_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n>```\n\n###__\uff08 \u4e0a\u8a18 URL\u5148\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb \u306e \u4e2d\u8eab \uff09__\n\n####__\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001\u30c8\u30fc\u30af\u30f3\uff08token\uff09\u3054\u3068\u306b \u5206\u304b\u3061\u66f8\u304d \u3057\u305f\u6587\u7ae0\u30c6\u30ad\u30b9\u30c8 \u304c\u914d\u7f6e\u3055\u308c\u305fURL\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308c\u3070\u3001ptb\u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u306f \u554f\u984c\u306a\u304f\u52d5\u4f5c\u3059\u308b\u4ed5\u69d8 \u306b \u306a\u3063\u3066\u3044\u308b\u3001\u3068 \u898b\u306a\u305b\u308b\u3002__\n\n\n\uff08 _train_url \uff09\n 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'\n\n<img width=\"1244\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c82 2016-08-04 17.59.37.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/aaa0e977-2a94-a732-0f57-cd5060c71e63.png\">\n\n\n\uff08 _valid_url \uff09\n\n'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'\n\n<img width=\"1247\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 18.07.07.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7112a422-adec-681c-7bca-50c5c04efc95.png\">\n\n\n\uff08 _test_url \uff09\n 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  \n\n<img width=\"1246\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 18.07.22.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/f132a1ae-750c-fdb1-2a0f-702019f13304.png\">\n\n\n####__\u4e0a\u8a18\u306eURL\u306f\u3001def get_ptb_words\u30e1\u30bd\u30c3\u30c9\u5192\u982d\u306e\u30b3\u30e1\u30f3\u30c8\u6ce8\u8a18\u306b\u8a18\u8f09\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3042\u308b\u3002__\n\n\uff08\u8aad\u307f\u8fbc\u307f\u5148\uff09https://github.com/tomsercu/lstm\n\uff08 \u4e0a\u8a18\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u30b3\u30fc\u30d1\u30b9\u306e\u6982\u8981 \uff09\n\n\u3000\u3082\u3068\u3082\u3068\uff08originally\uff09\u3001corpus of English sentences with linguistic structure annotations \u3067\u3042\u3063\u305f\u3000\u300cthe Penn Tree Bank dataset\u300d\uff08Penn Tree Bank <https://www.cis.upenn.edu/~treebank/>\uff09\u306e\u300ca variant distributed\u300d\u3067\u3042\u308b\u3002a variant \u3067\u3042\u308b\u7406\u7531\u306f\u3001github.com/tomsercu/lstm \u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3001\u300cThe Penn Tree Bank dataset\u300d\u304b\u3089\u3001omits the annotation \u3057\u305f\u4e0a\u3067\u3001and splits the dataset into three parts:\u3000training, validation, and test\u3000\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u304b\u3089\u3002\n\n[The Penn Treebank Project](https://www.cis.upenn.edu/~treebank/)\n\n> The Penn Treebank Project annotates naturally-occuring text for linguistic structure. \n>\n> Most notably, we produce skeletal parses showing rough syntactic and semantic information -- a bank of linguistic trees. \n>\n> We also annotate text with part-of-speech tags, and for the Switchboard corpus of telephone conversations, dysfluency annotation. \n>\n> We are located in the LINC Laboratory of the Computer and Information Science Department at the University of Pennsylvania. \n>\n>All data produced by the Treebank is released through the Linguistic Data Consortium. \n\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 15.56.20.png](https://qiita-image-store.s3.amazonaws.com/0/43487/90a2453f-d886-2f86-0db1-4d1d72d71bfa.png)\n\n\n[GitHub tomsercu / lstm](https://github.com/tomsercu/lstm)\n\n<img width=\"1254\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 15.58.31.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/ad10a505-6ec2-9704-a396-568faee482e4.png\">\n\n\n[GitHub tomsercu / lstm / data /](https://github.com/tomsercu/lstm/tree/master/data)\n\n<img width=\"1246\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.00.26.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7fc001df-427d-7e86-b932-ced7f53725db.png\">\n\n* __ptb.test.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab__\n\n<img width=\"1238\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.00.47.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/d2f2472f-88c1-f36b-710c-c6b237e4d42a.png\">\n\n* __ptb.train.txt\u30d5\u30a1\u30a4\u30eb \u683c\u7d0d\u5834\u6240__\n\n<img width=\"1251\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.01.06.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/731c6750-fa7a-b392-3b27-26dd0e0e472b.png\">\n\n* __ptb.train.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab__\n\n<img width=\"1244\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c82 2016-08-04 17.59.37.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/1b954d7f-c510-fa30-fbdf-c7fdfdfce4ee.png\">\n\n* __ptb.valid.txt\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u8eab__\n\n<img width=\"1244\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.01.33.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/87c9f18e-b007-9133-fe7b-4d1a30fd875a.png\">\n\n<img width=\"1222\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.03.05.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/c623712b-03ed-7605-aa12-c3bc47a8013a.png\">\n\n\n<img width=\"1245\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.03.13.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/2affbeb8-37eb-d1fb-779f-92914a4856c9.png\">\n\n<img width=\"1242\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 16.03.25.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/e5ec2969-5216-7e83-d2f5-5bc8aaae8cec.png\">\n\n\n\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\uff08 \u4ee5\u4e0b\u3001ptb.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u306e \u4e2d\u8eab \uff09\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat ptb.py\n```\n\n```{Python:ptb.py}\nimport os\n\nimport numpy\n\nfrom chainer.dataset import download\n\n\ndef get_ptb_words():\n    \"\"\"Gets the Penn Tree Bank dataset as long word sequences.\n\n    `Penn Tree Bank <https://www.cis.upenn.edu/~treebank/>`_ is originally a\n    corpus of English sentences with linguistic structure annotations. This\n    function uses a variant distributed at\n    `https://github.com/tomsercu/lstm <https://github.com/tomsercu/lstm>`_,\n    which omits the annotation and splits the dataset into three parts:\n    training, validation, and test.\n\n    This function returns the training, validation, and test sets, each of\n    which is represented as a long array of word IDs. All sentences in the\n    dataset are concatenated by End-of-Sentence mark '<eos>', which is treated\n    as one of the vocabulary.\n\n    Returns:\n        tuple of numpy.ndarray: Int32 vectors of word IDs.\n\n    .. Seealso::\n       Use :func:`get_ptb_words_vocabulary` to get the mapping between the\n       words and word IDs.\n\n    \"\"\"\n    train = _retrieve_ptb_words('train.npz', _train_url)\n    valid = _retrieve_ptb_words('valid.npz', _valid_url)\n    test = _retrieve_ptb_words('test.npz', _test_url)\n    return train, valid, test\n\n\ndef get_ptb_words_vocabulary():\n    \"\"\"Gets the Penn Tree Bank word vocabulary.\n\n    Returns:\n        dict: Dictionary that maps words to corresponding word IDs. The IDs are\n            used in the Penn Tree Bank long sequence datasets.\n\n    .. seealso::\n       See :func:`get_ptb_words` for the actual datasets.\n\n    \"\"\"\n    return _retrieve_word_vocabulary()\n\n\n_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\ndef _retrieve_ptb_words(name, url):\n    def creator(path):\n        vocab = _retrieve_word_vocabulary()\n        words = _load_words(url)\n        x = numpy.empty(len(words), dtype=numpy.int32)\n        for i, word in enumerate(words):\n            x[i] = vocab[word]\n\n        numpy.savez_compressed(path, x=x)\n        return {'x': x}\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, name)\n    loaded = download.cache_or_load_file(path, creator, numpy.load)\n    return loaded['x']\n\n\ndef _retrieve_word_vocabulary():\n    def creator(path):\n        words = _load_words(_train_url)\n        vocab = {}\n        index = 0\n        with open(path, 'w') as f:\n            for word in words:\n                if word not in vocab:\n                    vocab[word] = index\n                    index += 1\n                    f.write(word + '\\n')\n\n        return vocab\n\n    def loader(path):\n        vocab = {}\n        with open(path) as f:\n            for i, word in enumerate(f):\n                vocab[word.strip()] = i\n        return vocab\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, 'vocab.txt')\n    return download.cache_or_load_file(path, creator, loader)\n\n\ndef _load_words(url):\n    path = download.cached_download(url)\n    words = []\n    with open(path) as words_file:\n        for line in words_file:\n            if line:\n                words += line.strip().split()\n                words.append('<eos>')\n    return words\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \n```\n\n___\n\n##__\u3010 \u6b21\u306b\u3084\u308b\u3053\u3068 \u3011__\n\n\u53d6\u5f97\u3057\u3066\u53d6\u308a\u8fbc\u3080 \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u306e\u53d6\u5f97\u5143URL \u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u30b3\u30fc\u30c9\u7b87\u6240\u304c\u308f\u304b\u3063\u305f\u306e\u3067\u3001web\u4e0a\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\uff08URL\u3067download\u3067\u304d\u308b\uff09\u5206\u304b\u3061\u66f8\u304d\u6e08\u307f \u306e \u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf \u3092\u53d6\u5f97\u3057\u3066\u3001\u5358\u8a9e\u4e88\u6e2c\u30bf\u30b9\u30af \u3092 \u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3044\u3002\n\n\u307e\u305f\u3001\u591a\u304f\u306e\u89e3\u8aac\u30b5\u30a4\u30c8\u3092\u983c\u308a\u306b\u3001\u30ed\u30fc\u30ab\u30eb\u306b\u3042\u308b\u6587\u66f8\u30d5\u30a1\u30a4\u30eb \u3092 Mecab-neoplogd \u306a\u3069 \u3067 \u5206\u304b\u3061\u66f8\u304d\u3057\u3066\u3001\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u3092 \u53d6\u308a\u8fbc\u3080\u3053\u3068\u3082\u3084\u3063\u3066\u307f\u305f\u3044\u3002\n\n\u305d\u306e\u624b\u59cb\u3081\u306b\u3001\u30b3\u30fc\u30c9\u306b\u4f55\u3082\u624b\u3092\u52a0\u3048\u306a\u3044\u3067\u3001train_ptb.py \u3092 \u305d\u306e\u307e\u307e\u5b9f\u884c\u3057\u3066\u307f\u305f\u3002\nCPU\u30e2\u30fc\u30c9 \u3067 \u8d70\u3089\u305b\u3066\u307f\u305f\u306e\u3067\u3001\u4e88\u60f3\u6240\u8981\u6642\u9593\u306f\u3001\u4f55\u3068\uff13\u65e5\u9593 :disappointed_relieved: \n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/examples/ptb\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ python --version\nPython 3.5.2\nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ \nHirofumiYashima-no-MacBook:ptb hirofumiyashima$ time python train_ptb.py --gpu=-1\n```\n\n<img width=\"826\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 19.11.48.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/e88d1a5c-8625-c2f2-8464-e8ec47806d19.png\">\n\n__\uff08 1\u6642\u9593\u307b\u3069\u7d4c\u3063\u305f\u5f8c\u306e\u753b\u9762 \uff09__\n\n<img width=\"824\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-08-04 20.19.36.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/0d1dddec-763f-3500-df06-96a0cc760b36.png\">\n\n\n\n\n___\n\n##__\u3010 \u53c2\u8003\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8 \u3011__\n\n* [Hatena Blog \u7d42\u672b A.I.\uff082016-03-19\uff09\u300cChainer \u306e ptb \u30b5\u30f3\u30d7\u30eb\u3067\u904a\u3093\u3067\u307f\u308b\u300d](http://ksksksks2.hatenadiary.jp/entry/20160319/1458391617)\n* [Hatena::Diary shi3z\u306e\u9577\u6587\u65e5\u8a18\uff082015-07-14\uff09\u300cChainer\u306eptb\u30b5\u30f3\u30d7\u30eb\u3092\u89e3\u8aac\u3057\u3064\u3064\u3001\u81ea\u5206\u306e\u6587\u7ae0\u3092\u6df1\u5c64\u5b66\u7fd2\u3055\u305b\u3066\u3001\u50d5\u306e\u6587\u7ae0\u3063\u307d\u3044\u6587\u3092\u81ea\u52d5\u751f\u6210\u3055\u305b\u3066\u307f\u308b\u300d](http://d.hatena.ne.jp/shi3z/20150714/1436832305)\n* [Hatgena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3\uff082016-02-17\uff09\u300cexample/ptb\u3092\u8aad\u3080\u300d](http://chainernlpman.hatenablog.com/entry/2016/02/17/023545)\n* [Hatgena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3\uff082016-02-18\uff09\u300cptb\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u6587\u751f\u6210\u300d](http://chainernlpman.hatenablog.com/entry/2016/02/18/015515)\n\n\n___\n\n###__ptb \u3092\u6539\u826f\u3057\u3066\u6587\u7ae0\u751f\u6210 LSTM Chainer\u30b3\u30fc\u30c9\u306e\u66f8\u304d\u65b9\u3092\u89e3\u8aac\u3057\u3066\u3044\u308bWeb\u30b5\u30a4\u30c8__\n\n* [Hatena Blog kivantium\u6d3b\u52d5\u65e5\u8a18\uff082016-01-31\uff09\u300cChainer\u3067\u5b66\u3076LSTM\u300d](http://kivantium.hateblo.jp/entry/2016/01/31/222050)\n* [Hatena Blog sora_sakaki\u306e\u30d6\u30ed\u30b0\uff082015-07-16\uff09\u300c\u6a5f\u68b0\u5b66\u7fd2\u521d\u5fc3\u8005\u304c1\u304b\u3089Chainer\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u59cb\u3081\u3089\u308c\u308b\u307e\u3067\u306e\u7d4c\u7def\u3092\u66f8\u3044\u305f\u3089\u9577\u304f\u306a\u308a\u3059\u304e\u305f\u300d](http://sora-sakaki.hatenablog.com/entry/2015/07/16/195301)\n* [GushiSnow\u3055\u3093 Qiita\u8a18\u4e8b\uff082015/08/05\uff09\u300cChainer \u3092\u7528\u3044\u305f\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u8a00\u8a9e\u30e2\u30c7\u30eb\u4f5c\u6210\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u89e3\u8aac\u306b\u6311\u6226\u3057\u3066\u307f\u305f\u300d](http://qiita.com/GushiSnow/items/b34da4962dd930d1487a)\n\n___\n\n###__\u3010 Chainer example\u4ee5\u5916 \u306e \u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30afDeep neural network \u5b66\u7fd2\uff06\u65b0\u898f\u30c7\u30fc\u30bf \u4e88\u6e2cChainer\u30b9\u30af\u30ea\u30d7\u30c8\u4e8b\u4f8b \u3011__\n\n* [GitHub yusuketomoto/chainer-char-rnn](https://github.com/yusuketomoto/chainer-char-rnn)\n> karpathy's char-rnn (https://github.com/karpathy/char-rnn) implementation by Chainer\n\n__\u4e0a\u8a18\u306f\u3001Torch\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u4e88\u6e2c\u30bf\u30b9\u30af\uff08\u6587\u7ae0\u751f\u6210\u30bf\u30b9\u30af\uff09\u3092\u884c\u3046deep neural network\u3092\u5b66\u7fd2\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u3092Chainer\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e__\n\n* [GitHub karpathy/char-rnn](https://github.com/karpathy/char-rnn)\n> Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch\n\n* [Andrej Karpathy blog (May 21, 2015) _The Unreasonable Effectiveness of Recurrent Neural Networks_](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n\n\n__\u4e0a\u8a18 Chainer\u30b3\u30fc\u30c9\u3092\u6539\u826f\u3057\u3066\u8a66\u3057\u3066\u3044\u308b\u4e8b\u4f8b__\n\n* [S346\u3055\u3093 Qiita\u8a18\u4e8b\uff082015/12/08\uff09\u300c\u3010\u30a8\u30f4\u30a1\u30f3\u30b2\u30ea\u30aa\u30f3\u3011\u30a2\u30b9\u30ab\u3063\u307d\u3044\u30bb\u30ea\u30d5\u3092DeepLearning\u3067\u81ea\u52d5\u751f\u6210\u3057\u3066\u307f\u308b\u300d](http://qiita.com/S346/items/24e875e3c5ac58f55810)\n\n___\n\n###__\u3010 \u305d\u306e\u4ed6\u53c2\u8003 \u3011__\n\n* [ichiroex\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/06\uff09\u300c\u6df1\u5c64\u5b66\u7fd2\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30afChainer\u306e\u52c9\u5f37\u306b\u5f79\u7acb\u3064\u30da\u30fc\u30b8\u306e\u307e\u3068\u3081\u300d](http://qiita.com/ichiroex/items/e0486a6dea1f14c2cfc2)\n* [Hatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18\uff082015/12/17\uff09\u300cDeepLearning\u3092\u4f7f\u3063\u305f\u5b9f\u88c5\u3092\u7e8f\u3081\u3066\u307f\u305f\u300d](http://nonbiri-tereka.hatenablog.com/entry/2015/12/17/004410)\n\n___\n\n##__\u3010 \u88dc\u8db3 \u3011__\n\nchainer.datasets.ptb\u30d5\u30a1\u30a4\u30eb \u306b get_ptb_words() \u30e1\u30bd\u30c3\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u3066\u3001\u3055\u3089\u306b\u3001\n\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u5143 URL \u304c\u30d9\u30bf\u6253\u3061\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u66f8\u304d\u65b9\u306f\u3001Chainer ver.1.12 \u3067\u3082\u5909\u308f\u3089\u306a\u3044\u3088\u3046\u3060\u3002\n\n* [Chainer Docs \u00bb Module code \u00bb chainer \u00bb  Source code for chainer.datasets.ptb](http://docs.chainer.org/en/stable/_modules/chainer/datasets/ptb.html)\n\n```\n>_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n>_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n>_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n```\n\n___\n\n\n\n#__\u3010 \u88dc\u8db3\uff12 \u3011\u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf\u306eLSTM\u5165\u529b\u524d\u306e\u5f62\u5f0f\u5909\u63db\u51e6\u7406__\n\n___\n\n###__\u4e0a\u8a18\u306e\u30da\u30f3\u30b7\u30eb\u30d0\u30cb\u30a2\u5927\u5b66\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060\u30b3\u30fc\u30d1\u30b9\u30c7\u30fc\u30bf\u306f\u3001\u30b3\u30fc\u30c9\u306e\u4e2d\u3067\u3001\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u3066\u3001LSTM\u30e2\u30c7\u30eb\u3078\u306e\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002__\n\n```{python:Python}\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> sample_sentence = \"I will examine what new sentence this sentence will be converted into by a Python code.\"\n>>> \n>>> words = sample_sentence.replace('/n', '<eos>').strip().split()\n>>> print(words)\n['I', 'will', 'examine', 'what', 'new', 'sentence', 'this', 'sentence', 'will', 'be', 'converted', 'into', 'by', 'a', 'Python', 'code.']\n>>>\n```\n\n\n```{python:Python}\n>>> vocab = {}\n>>> print(vocab)\n{}\n>>> \n>>> 'this' in vocab\nFalse\n>>> \n>>> 'Python' in vocab\nFalse\n>>> \n>>> for i, word in enumerate(words):\n...     if word not in vocab:\n...             vocab[word] = len(vocab)\n...     dataset[i] = vocab[word]\n... \n>>> \n>>> print(dataset)\n[ 0  1  2  3  4  5  6  5  1  7  8  9 10 11 12 13]\n>>> \n>>> print(vocab)\n{'converted': 8, 'this': 6, 'a': 11, 'will': 1, 'new': 4, 'Python': 12, 'sentence': 5, 'be': 7, 'into': 9, 'I': 0, 'examine': 2, 'code.': 13, 'what': 3, 'by': 10}\n>>>\n```\n\n####__\u5358\u8a9e\u6587\u5b57\u5217 : \u4e0e\u3048\u305f\u6587\u5b57\u5217\uff08list\u578b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff09\u5185\u3067\u306e \u5f53\u8a72\u5358\u8a9e \u306e \u51fa\u73fe\u756a\u5730\uff08\u5192\u982d\u306f\u30bc\u30ed\uff09 \u304c \u683c\u7d0d\u3055\u308c\u3066\u3044\u308b__\n\n\n\n```{python:Python}\n>>> print(words)\n['I', 'will', 'examine', 'what', 'new', 'sentence', 'this', 'sentence', 'will', 'be', 'converted', 'into', 'by', 'a', 'Python', 'code.']\n>>> \n>>> import types\n>>> print(type(vocab))\n<class 'dict'>\n>>> \n>>> print(vocab['I'])\n0\n>>> \n>>> print(vocab['will'])\n1\n>>> \n>>> print(vocab['examine'])\n2\n>>> \n>>> print(vocab['what'])\n3\n>>> \n```\n\n###__\u4e0a\u8a18\u306e\u5909\u5316\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u308b\u30b3\u30fc\u30c9\u90e8\u5206\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059\u3002__\n___\n\n####__\u4ee5\u524d\u306eversion \u306e Chainer example\u30b3\u30fc\u30c9__\n\n[gushiSnow\u3055\u3093 Qiita\u8a18\u4e8b\u300cChainer\u3092\u7528\u3044\u305f\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u8a00\u8a9e\u30e2\u30c7\u30eb\u4f5c\u6210\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u89e3\u8aac\u306b\u6311\u6226\u3057\u3066\u307f\u308b\u300d](http://qiita.com/GushiSnow/items/b34da4962dd930d1487a)\n\n>###__\u30c7\u30fc\u30bf\u4fdd\u6301__\n>\n>\u5b66\u7fd2\u30c7\u30fc\u30bf\n>valid\u30c7\u30fc\u30bf\n>\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n>\n>\u4e0a\u8a18\u3092\u305d\u308c\u305e\u308c\u884c\u5217\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4fdd\u6301\u3057\u307e\u3059\u3002\n>\n>>```{python:Python}\n>># Prepare dataset (preliminary download dataset by ./download.py)\n>>vocab = {}\n>>\n>>def load_data(filename):\n>>    global vocab, n_vocab\n>>    words = open(filename).read().replace('\\n', '<eos>').strip().split()\n>>    dataset = np.ndarray((len(words),), dtype=np.int32)\n>>    for i, word in enumerate(words):\n>>        if word not in vocab:\n>>            vocab[word] = len(vocab)\n>>        dataset[i] = vocab[word]\n>>    return dataset\n>>\n>>train_data = load_data('ptb.train.txt')\n>>valid_data = load_data('ptb.valid.txt')\n>>test_data = load_data('ptb.test.txt')\n>>print('#vocab =', len(vocab))\n>>```\n\n___\n\n###__Chainer ver.1.11.0 \u306e pub example\u30b3\u30fc\u30c9\u3067\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5909\u308f\u3063\u3066\u3044\u307e\u3059\u3002__\n\n* __Chainer ver.1.11.0 \u306e chainer/chainer/datasets/ptb.py \u306e \u30b3\u30fc\u30c9__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ pwd\n/Users/hirofumiyashima/chainer/chainer/datasets\nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ \nHirofumiYashima-no-MacBook:datasets hirofumiyashima$ cat ptb.py \nimport os\n\nimport numpy\n\nfrom chainer.dataset import download\n\n\ndef get_ptb_words():\n    \"\"\"Gets the Penn Tree Bank dataset as long word sequences.\n\n    `Penn Tree Bank <https://www.cis.upenn.edu/~treebank/>`_ is originally a\n    corpus of English sentences with linguistic structure annotations. This\n    function uses a variant distributed at\n    `https://github.com/tomsercu/lstm <https://github.com/tomsercu/lstm>`_,\n    which omits the annotation and splits the dataset into three parts:\n    training, validation, and test.\n\n    This function returns the training, validation, and test sets, each of\n    which is represented as a long array of word IDs. All sentences in the\n    dataset are concatenated by End-of-Sentence mark '<eos>', which is treated\n    as one of the vocabulary.\n\n    Returns:\n        tuple of numpy.ndarray: Int32 vectors of word IDs.\n\n    .. Seealso::\n       Use :func:`get_ptb_words_vocabulary` to get the mapping between the\n       words and word IDs.\n\n    \"\"\"\n    train = _retrieve_ptb_words('train.npz', _train_url)\n    valid = _retrieve_ptb_words('valid.npz', _valid_url)\n    test = _retrieve_ptb_words('test.npz', _test_url)\n    return train, valid, test\n\n\ndef get_ptb_words_vocabulary():\n    \"\"\"Gets the Penn Tree Bank word vocabulary.\n\n    Returns:\n        dict: Dictionary that maps words to corresponding word IDs. The IDs are\n            used in the Penn Tree Bank long sequence datasets.\n\n    .. seealso::\n       See :func:`get_ptb_words` for the actual datasets.\n\n    \"\"\"\n    return _retrieve_word_vocabulary()\n\n\n_train_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt'  # NOQA\n_valid_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt'  # NOQA\n_test_url = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt'  # NOQA\n\n\ndef _retrieve_ptb_words(name, url):\n    def creator(path):\n        vocab = _retrieve_word_vocabulary()\n        words = _load_words(url)\n        x = numpy.empty(len(words), dtype=numpy.int32)\n        for i, word in enumerate(words):\n            x[i] = vocab[word]\n\n        numpy.savez_compressed(path, x=x)\n        return {'x': x}\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, name)\n    loaded = download.cache_or_load_file(path, creator, numpy.load)\n    return loaded['x']\n\n\ndef _retrieve_word_vocabulary():\n    def creator(path):\n        words = _load_words(_train_url)\n        vocab = {}\n        index = 0\n        with open(path, 'w') as f:\n            for word in words:\n                if word not in vocab:\n                    vocab[word] = index\n                    index += 1\n                    f.write(word + '\\n')\n\n        return vocab\n\n    def loader(path):\n        vocab = {}\n        with open(path) as f:\n            for i, word in enumerate(f):\n                vocab[word.strip()] = i\n        return vocab\n\n    root = download.get_dataset_directory('pfnet/chainer/ptb')\n    path = os.path.join(root, 'vocab.txt')\n    return download.cache_or_load_file(path, creator, loader)\n```\n\n\n", "tags": ["DeepLearning", "NLP", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "Chainer", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0"]}