{"context": " More than 1 year has passed since last update.Hadoop HDFS\u3068\u9023\u643a\u3059\u308b\u305f\u3081Apache Spark\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u3002\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u624b\u9806\n\u25a0\u30cf\u30fc\u30c9\u69cb\u6210\u3000\n\u3000\u30fb\u81ea\u4f5c\u7121\u7dda\u30eb\u30fc\u30bf\uff08Raspberry Pi\uff09\u3000\n\u3000\u30fbCubieTruck (Fedora 19) * 4\n\n\u25a0\u30df\u30c9\u30eb\u69cb\u6210\n  \u30fbOracle JDK 1.7 for ARM\n  \u30fbHadoop 2.2.0\n  \u30fbApache Spark 1.0.2\n\nHadoop\u69cb\u7bc9\u6642\u306b\u884c\u3063\u305f\u3001hosts\u8a2d\u5b9a\u3001Java\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3001SSH\u30ce\u30f3\u30d1\u30b9\u51e6\u7406\u306f\u5272\u611b\u3057\u307e\u3059\u3002\n\u3010Hadoop\u69cb\u7bc9\u624b\u9806\u3011\n\u3000http://qiita.com/tsunaki/items/41b9ea36ae99b7702ae3\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n# cd /usr/local/src\n# wget http://ftp.riken.jp/net/apache/spark/spark-1.0.2/spark-1.0.2-bin-hadoop2.tgz \n\n# tar zxf spark-1.0.2-bin-hadoop2.tgz \n# mv spark-1.0.2-bin-hadoop2 /opt/\n\n\n\u3053\u308c\u3067\u65e2\u306bStandalone mode\u3067\u52d5\u304d\u307e\u3059\u3002\n\u3053\u3053\u304b\u3089\u3001cluster\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\n# mv spark-1.0.2-bin-hadoop2 /opt/\n# cd /opt/spark-1.0.2-bin-hadoop2\n# cp conf/spark-env.sh.template conf/spark-env.sh\n\n# vi conf/spark-env.sh.template\n\n\u4e00\u756a\u4e0b\u306bmaster\u306eIP\u3092\u8ffd\u8a18\u3057\u307e\u3059\u3002\n#!/usr/bin/env bash\n\n# This file is sourced when running various Spark programs.\n# Copy it as spark-env.sh and edit that to configure Spark for your site.\n\n# Options read when launching programs locally with\n# ./bin/run-example or ./bin/spark-submit\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program\n# - SPARK_CLASSPATH, default classpath entries to append\n\n# Options read by executors and drivers running inside the cluster\n# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program\n# - SPARK_CLASSPATH, default classpath entries to append\n# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data\n# - MESOS_NATIVE_LIBRARY, to point to your libmesos.so if you use Mesos\n\n# Options read in YARN client mode\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_EXECUTOR_INSTANCES, Number of workers to start (Default: 2)\n# - SPARK_EXECUTOR_CORES, Number of cores for the workers (Default: 1).\n# - SPARK_EXECUTOR_MEMORY, Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n# - SPARK_YARN_APP_NAME, The name of your application (Default: Spark)\n# - SPARK_YARN_QUEUE, The hadoop queue to use for allocation requests (Default: \u2018default\u2019)\n# - SPARK_YARN_DIST_FILES, Comma separated list of files to be distributed with the job.\n# - SPARK_YARN_DIST_ARCHIVES, Comma separated list of archives to be distributed with the job.\n\n# Options for the daemons used in the standalone deploy mode:\n# - SPARK_MASTER_IP, to bind the master to a different IP address or hostname\n# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master\n# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")\n# - SPARK_WORKER_CORES, to set the number of cores to use on this machine\n# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\n# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker\n# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node\n# - SPARK_WORKER_DIR, to set the working directory of worker processes\n# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")\n# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")\n# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")\n# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers\n\n# ADD master00 \nSPARK_MASTER_IP=192.168.100.2\n\n\u6b21\u306bslaves\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n# vi conf/slaves\n\nlocalhost\u306e\u8a18\u8ff0\u3092\u3044\u304b\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\nmaster00\nslave01\nslave02\nslave03\n\n\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5404slave\u306b\u5c55\u958b\u3057\u307e\u3059\u3002\n\u203bmaster\u3068\u540c\u3058\u306b\u3059\u308b\u3053\u3068\u3002\n\u5c55\u958b\u5f8c\u306bmaster00\u3067\u8d77\u52d5\u51e6\u7406\n# su - hduser\n$ /opt/spark-1.0.2-bin-hadoop2/start-all.sh\n\n\u3053\u308c\u3067\u5b8c\u4e86\u3067\u3059\u3002\nWEB\u3067\u8d77\u52d5\u306e\u78ba\u8a8d\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u3044\u3066\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\u78ba\u8a8d\u3067\u304d\u305f\u3089Hadoop\u3068\u9023\u7d50\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\u5148\u305a\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u6e96\u5099\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u90f5\u4fbf\u756a\u53f7\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\uff08\u51e6\u7406\u306f\u3059\u3079\u3066master00\u3067\u884c\u3063\u3066\u307e\u3059\uff09\n# su - hduser\n$ wget http://www.post.japanpost.jp/zipcode/dl/oogaki/zip/ken_all.zip\n$ unzip ken_all.zip\n\n$ nkf -w KEN_ALL.CSV > KEN_ALL.CSV.utf\n### UTF\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n$ mv KEN_ALL.CSV.utf KEN_ALL.CSV \n### HFDS\u306b\u767b\u9332\n$ hdfs dfs -put KEN_ALL.CSV /user/\n$ hdfs dfs -ls /user\n[hduser@master00 ~]$ hdfs dfs -ls /user\n14/08/29 21:55:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 1 items\n-rw-r--r--   4 hduser supergroup   12232566 2014-08-29 21:07 /user/KEN_ALL.CSV\n\nSpark-shell\u3092\u8d77\u52d5\u3055\u305b\u3066\u307e\u3059\u3002\n$ /opt/spark-1.0.2-bin-hadoop2/bin/spark-shell\n\n\nHadoop\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\nscala> val file = sc.textFile(\"hdfs://master00:9000/user/KEN_ALL.CSV\")\n\n\n\u8aad\u307f\u8fbc\u3093\u3060\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\nscala> file.count()\n\n\n\u6b21\u306b\u6a2a\u6d5c\u3068\u3044\u3046\u6587\u5b57\u3092\u691c\u7d22\u3057\u3066\u307f\u307e\u3059\nscala> file.filter(line => line.contains(\"\u6a2a\u6d5c\")).foreach(println)\n\n\n\u691c\u7d22\u3067\u304d\u305f\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\u4ee5\u4e0a\nHadoop\u306eMapReduce\u306b\u6bd4\u3079\u308b\u3068\u3001\u3082\u306e\u3059\u3054\u304f\u65e9\u304f\u306a\u308a\u307e\u3057\u305f\u3002\n\u3053\u308c\u306a\u3089ARM\u306eCPU\u30dc\u30fc\u30c9\u3067\u3082\u5341\u5206\u6d3b\u7528\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n\nHadoop HDFS\u3068\u9023\u643a\u3059\u308b\u305f\u3081Apache Spark\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u3002\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u624b\u9806\n\n```\n\u25a0\u30cf\u30fc\u30c9\u69cb\u6210\u3000\n\u3000\u30fb\u81ea\u4f5c\u7121\u7dda\u30eb\u30fc\u30bf\uff08Raspberry Pi\uff09\u3000\n\u3000\u30fbCubieTruck (Fedora 19) * 4\n```\n\n```\n\u25a0\u30df\u30c9\u30eb\u69cb\u6210\n  \u30fbOracle JDK 1.7 for ARM\n  \u30fbHadoop 2.2.0\n  \u30fbApache Spark 1.0.2\n```\n\nHadoop\u69cb\u7bc9\u6642\u306b\u884c\u3063\u305f\u3001hosts\u8a2d\u5b9a\u3001Java\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3001SSH\u30ce\u30f3\u30d1\u30b9\u51e6\u7406\u306f\u5272\u611b\u3057\u307e\u3059\u3002\n\u3010Hadoop\u69cb\u7bc9\u624b\u9806\u3011\n\u3000http://qiita.com/tsunaki/items/41b9ea36ae99b7702ae3\n\n<h3>Spark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</h3>\n\n```\n# cd /usr/local/src\n# wget http://ftp.riken.jp/net/apache/spark/spark-1.0.2/spark-1.0.2-bin-hadoop2.tgz \n\n# tar zxf spark-1.0.2-bin-hadoop2.tgz \n# mv spark-1.0.2-bin-hadoop2 /opt/\n\n```\n\n\u3053\u308c\u3067\u65e2\u306bStandalone mode\u3067\u52d5\u304d\u307e\u3059\u3002\n\n\u3053\u3053\u304b\u3089\u3001cluster\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\n\n```\n# mv spark-1.0.2-bin-hadoop2 /opt/\n# cd /opt/spark-1.0.2-bin-hadoop2\n# cp conf/spark-env.sh.template conf/spark-env.sh\n\n# vi conf/spark-env.sh.template\n```\n\n\u4e00\u756a\u4e0b\u306bmaster\u306eIP\u3092\u8ffd\u8a18\u3057\u307e\u3059\u3002\n\n```\n#!/usr/bin/env bash\n\n# This file is sourced when running various Spark programs.\n# Copy it as spark-env.sh and edit that to configure Spark for your site.\n\n# Options read when launching programs locally with\n# ./bin/run-example or ./bin/spark-submit\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program\n# - SPARK_CLASSPATH, default classpath entries to append\n\n# Options read by executors and drivers running inside the cluster\n# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program\n# - SPARK_CLASSPATH, default classpath entries to append\n# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data\n# - MESOS_NATIVE_LIBRARY, to point to your libmesos.so if you use Mesos\n\n# Options read in YARN client mode\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_EXECUTOR_INSTANCES, Number of workers to start (Default: 2)\n# - SPARK_EXECUTOR_CORES, Number of cores for the workers (Default: 1).\n# - SPARK_EXECUTOR_MEMORY, Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n# - SPARK_YARN_APP_NAME, The name of your application (Default: Spark)\n# - SPARK_YARN_QUEUE, The hadoop queue to use for allocation requests (Default: \u2018default\u2019)\n# - SPARK_YARN_DIST_FILES, Comma separated list of files to be distributed with the job.\n# - SPARK_YARN_DIST_ARCHIVES, Comma separated list of archives to be distributed with the job.\n\n# Options for the daemons used in the standalone deploy mode:\n# - SPARK_MASTER_IP, to bind the master to a different IP address or hostname\n# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master\n# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")\n# - SPARK_WORKER_CORES, to set the number of cores to use on this machine\n# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\n# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker\n# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node\n# - SPARK_WORKER_DIR, to set the working directory of worker processes\n# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")\n# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")\n# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")\n# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers\n\n# ADD master00 \nSPARK_MASTER_IP=192.168.100.2\n```\n\n\u6b21\u306bslaves\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n```\n# vi conf/slaves\n```\nlocalhost\u306e\u8a18\u8ff0\u3092\u3044\u304b\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n\n```\nmaster00\nslave01\nslave02\nslave03\n```\n\n\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5404slave\u306b\u5c55\u958b\u3057\u307e\u3059\u3002\n\u203bmaster\u3068\u540c\u3058\u306b\u3059\u308b\u3053\u3068\u3002\n\n\n\u5c55\u958b\u5f8c\u306bmaster00\u3067\u8d77\u52d5\u51e6\u7406\n\n```\n# su - hduser\n$ /opt/spark-1.0.2-bin-hadoop2/start-all.sh\n```\n\n\u3053\u308c\u3067\u5b8c\u4e86\u3067\u3059\u3002\n\nWEB\u3067\u8d77\u52d5\u306e\u78ba\u8a8d\n\n![spark.png](https://qiita-image-store.s3.amazonaws.com/0/52867/d7ba7138-9ff5-e079-2e79-41246c792cbd.png)\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u3067\u52d5\u3044\u3066\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\n\u78ba\u8a8d\u3067\u304d\u305f\u3089Hadoop\u3068\u9023\u7d50\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u5148\u305a\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u6e96\u5099\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u90f5\u4fbf\u756a\u53f7\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\uff08\u51e6\u7406\u306f\u3059\u3079\u3066master00\u3067\u884c\u3063\u3066\u307e\u3059\uff09\n\n```\n# su - hduser\n$ wget http://www.post.japanpost.jp/zipcode/dl/oogaki/zip/ken_all.zip\n$ unzip ken_all.zip\n\n$ nkf -w KEN_ALL.CSV > KEN_ALL.CSV.utf\n### UTF\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n$ mv KEN_ALL.CSV.utf KEN_ALL.CSV \n### HFDS\u306b\u767b\u9332\n$ hdfs dfs -put KEN_ALL.CSV /user/\n$ hdfs dfs -ls /user\n[hduser@master00 ~]$ hdfs dfs -ls /user\n14/08/29 21:55:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 1 items\n-rw-r--r--   4 hduser supergroup   12232566 2014-08-29 21:07 /user/KEN_ALL.CSV\n```\n\nSpark-shell\u3092\u8d77\u52d5\u3055\u305b\u3066\u307e\u3059\u3002\n\n```\n$ /opt/spark-1.0.2-bin-hadoop2/bin/spark-shell\n```\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2014-08-29 21.59.27.png](https://qiita-image-store.s3.amazonaws.com/0/52867/e7902fec-2a45-f467-f931-e1abacc20e61.png)\n\nHadoop\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```\nscala> val file = sc.textFile(\"hdfs://master00:9000/user/KEN_ALL.CSV\")\n```\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2014-08-29 22.11.59.png](https://qiita-image-store.s3.amazonaws.com/0/52867/f59da7d5-da7a-3c69-f07d-281aab14d132.png)\n\n\u8aad\u307f\u8fbc\u3093\u3060\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n```\nscala> file.count()\n```\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2014-08-29 22.12.49.png](https://qiita-image-store.s3.amazonaws.com/0/52867/312f021a-cd8d-cd3d-1a3b-143e88d220c8.png)\n\n\u6b21\u306b\u6a2a\u6d5c\u3068\u3044\u3046\u6587\u5b57\u3092\u691c\u7d22\u3057\u3066\u307f\u307e\u3059\n\n```\nscala> file.filter(line => line.contains(\"\u6a2a\u6d5c\")).foreach(println)\n```\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2014-08-29 22.14.41.png](https://qiita-image-store.s3.amazonaws.com/0/52867/5670d8df-5950-e543-aad0-c0e8985372b9.png)\n\n\u691c\u7d22\u3067\u304d\u305f\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\n\u4ee5\u4e0a\n\nHadoop\u306eMapReduce\u306b\u6bd4\u3079\u308b\u3068\u3001\u3082\u306e\u3059\u3054\u304f\u65e9\u304f\u306a\u308a\u307e\u3057\u305f\u3002\n\u3053\u308c\u306a\u3089ARM\u306eCPU\u30dc\u30fc\u30c9\u3067\u3082\u5341\u5206\u6d3b\u7528\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n\n", "tags": ["Spark", "hadoop"]}