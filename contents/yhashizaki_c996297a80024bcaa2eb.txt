{"context": "\u5148\u65e5\u6765\u304b\u3089\u8a13\u7df4\u4e2d\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3067\u3059\u304c\u3001\n\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u306a\u304b\u306a\u304b\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3059\u304c\u3001\n\u51fa\u6765\u305f\u306e\u3067\u8a18\u4e8b\u306b\u3002\n\u30fb\u30c6\u30fc\u30d6\u30eb\u69cb\u9020\u306e\u4e2d\u306b\u5b58\u5728\u3059\u308b\u3001\u30c6\u30ad\u30b9\u30c8\u3068\u3001\u30ea\u30f3\u30af\u5148URL\u3092\u30bb\u30c3\u30c8\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3044\uff08pandas\u306eDataFrame\u3092\u5229\u7528\uff09\n\u30fb\u5f53\u8a72\u30ea\u30f3\u30af\u5148URL\u306f\u3001\u540c\u3058\u30c6\u30fc\u30d6\u30eb\u5185\u3067\u8907\u6570\u306ea href\u304c\u3042\u308a\u3001\u304b\u3064\u8b58\u5225\u53ef\u80fd\u306a\u540d\u524d\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u304a\u3089\u305a\u6b63\u898f\u8868\u73fe\u3067\u3082\u3068\u308a\u306b\u304f\u3044\u3082\u306e\u3060\u3063\u305f\n\u2192\u30c6\u30ad\u30b9\u30c8\u6587\u7ae0\u3092\u6307\u5b9a\u3057\u3066\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u30ea\u30f3\u30af\u5148\u3068\u3057\u3066\u6307\u5b9a\u3092\u3057\u3066\u3001\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3089\u826f\u3055\u305d\u3046\u3060\u3063\u305f\u306e\u3067XPath\u3092\u4f7f\u3046\u3053\u3068\u306b\n\uff08DataFrame\u306f\u884c\u6570\u304c\u63c3\u308f\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u8fd4\u3063\u3066\u304f\u308b\u306e\u3067\u4e0d\u8981\u306a\u30c7\u30fc\u30bf\u3092\u7701\u3044\u3066\u78ba\u5b9f\u306b\u3068\u308a\u305f\u3044\uff09\n\u30fbBeautiful Soup\u306fXPath\u4f7f\u3048\u306a\u3044\u3051\u3069\u3001lxml\u3092\u4f7f\u3048\u3070\u51fa\u6765\u305f\n\u3010\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u30b5\u30a4\u30c8\u3011\nhttp://gci.t.u-tokyo.ac.jp/tutorial/crawling/\nhttp://www.slideshare.net/tushuhei/python-xpath\nhttp://qiita.com/tamonoki/items/a341657a86ff7a945224\n\nscraping.py\n#coding: utf-8\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport pandas as pd\nimport time\nimport lxml.html\n\naaa = []\nbbb = []\n\nfor page in range(1,2):\n    url = \"http://www.\uff5e\uff5e\uff5e\" + str(page)\n    html = urllib2.urlopen(url)\n    html2 = urllib2.urlopen(url)\n    soup = BeautifulSoup(html, \"lxml\")\n    dom = lxml.html.fromstring(html2.read())\n\n    for o1 in soup.findAll(\"td\", class_=\"xx\"):\n        aaa.append(o1.string)\n\n    for o2 in dom.xpath(u\"//a[text()='xxx']/@href\"): #xxx\u306e\u90e8\u5206\u3092\u30c6\u30ad\u30b9\u30c8\u6307\u5b9a\u3067href\u3092\u53d6\u5f97\n        bbb.append(o2)\n\n    time.sleep(2)\n\ndf = pd.DataFrame({\"aaa\":aaa, \"bbb\":bbb})\nprint(df)\ndf.to_csv(\"xxxx.csv\", index=False, encoding='utf-8')\n\n\n\u7c21\u5358\u3067\u3059\u304c\u3001\u4eca\u65e5\u306f\u4ee5\u4e0a\u3067\u3059\u3002\n\u5148\u65e5\u6765\u304b\u3089\u8a13\u7df4\u4e2d\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3067\u3059\u304c\u3001\n\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u306a\u304b\u306a\u304b\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3059\u304c\u3001\n\u51fa\u6765\u305f\u306e\u3067\u8a18\u4e8b\u306b\u3002\n\n\u30fb\u30c6\u30fc\u30d6\u30eb\u69cb\u9020\u306e\u4e2d\u306b\u5b58\u5728\u3059\u308b\u3001\u30c6\u30ad\u30b9\u30c8\u3068\u3001\u30ea\u30f3\u30af\u5148URL\u3092\u30bb\u30c3\u30c8\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3044\uff08pandas\u306eDataFrame\u3092\u5229\u7528\uff09\n\u30fb\u5f53\u8a72\u30ea\u30f3\u30af\u5148URL\u306f\u3001\u540c\u3058\u30c6\u30fc\u30d6\u30eb\u5185\u3067\u8907\u6570\u306ea href\u304c\u3042\u308a\u3001\u304b\u3064\u8b58\u5225\u53ef\u80fd\u306a\u540d\u524d\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u304a\u3089\u305a\u6b63\u898f\u8868\u73fe\u3067\u3082\u3068\u308a\u306b\u304f\u3044\u3082\u306e\u3060\u3063\u305f\n\u2192\u30c6\u30ad\u30b9\u30c8\u6587\u7ae0\u3092\u6307\u5b9a\u3057\u3066\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u30ea\u30f3\u30af\u5148\u3068\u3057\u3066\u6307\u5b9a\u3092\u3057\u3066\u3001\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3089\u826f\u3055\u305d\u3046\u3060\u3063\u305f\u306e\u3067XPath\u3092\u4f7f\u3046\u3053\u3068\u306b\n\uff08DataFrame\u306f\u884c\u6570\u304c\u63c3\u308f\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u8fd4\u3063\u3066\u304f\u308b\u306e\u3067\u4e0d\u8981\u306a\u30c7\u30fc\u30bf\u3092\u7701\u3044\u3066\u78ba\u5b9f\u306b\u3068\u308a\u305f\u3044\uff09\n\u30fbBeautiful Soup\u306fXPath\u4f7f\u3048\u306a\u3044\u3051\u3069\u3001lxml\u3092\u4f7f\u3048\u3070\u51fa\u6765\u305f\n\n\u3010\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u30b5\u30a4\u30c8\u3011\nhttp://gci.t.u-tokyo.ac.jp/tutorial/crawling/\nhttp://www.slideshare.net/tushuhei/python-xpath\nhttp://qiita.com/tamonoki/items/a341657a86ff7a945224\n\n```scraping.py\n#coding: utf-8\nfrom bs4 import BeautifulSoup\nimport urllib2\nimport pandas as pd\nimport time\nimport lxml.html\n\naaa = []\nbbb = []\n\nfor page in range(1,2):\n\turl = \"http://www.\uff5e\uff5e\uff5e\" + str(page)\n\thtml = urllib2.urlopen(url)\n\thtml2 = urllib2.urlopen(url)\n\tsoup = BeautifulSoup(html, \"lxml\")\n\tdom = lxml.html.fromstring(html2.read())\n\n\tfor o1 in soup.findAll(\"td\", class_=\"xx\"):\n\t\taaa.append(o1.string)\n\n\tfor o2 in dom.xpath(u\"//a[text()='xxx']/@href\"): #xxx\u306e\u90e8\u5206\u3092\u30c6\u30ad\u30b9\u30c8\u6307\u5b9a\u3067href\u3092\u53d6\u5f97\n\t\tbbb.append(o2)\n\n\ttime.sleep(2)\n\ndf = pd.DataFrame({\"aaa\":aaa, \"bbb\":bbb})\nprint(df)\ndf.to_csv(\"xxxx.csv\", index=False, encoding='utf-8')\n```\n\n\u7c21\u5358\u3067\u3059\u304c\u3001\u4eca\u65e5\u306f\u4ee5\u4e0a\u3067\u3059\u3002\n", "tags": ["BeautifulSoup", "xpath", "Python", "pandas", "lxml"]}