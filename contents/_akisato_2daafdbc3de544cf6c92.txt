{"context": "\u307f\u306a\u3055\u307e\uff0c\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff0c@_akisato \u3067\u3054\u3056\u3044\u307e\u3059\uff0e\n\u30af\u30ed\u30fc\u30e9/web\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0Advent Calender http://qiita.com/advent-calendar/2015/crawler \u306e6\u65e5\u76ee\u306e\u8a18\u4e8b\u3068\u3057\u3066\u66f8\u3044\u3066\u304a\u308a\u307e\u3059\uff0e\n\u672c\u65e5\u306f\uff0cJavaScript\u3084cookie\u3092\u8a31\u53ef\u3057\u3066\u3044\u306a\u3044\u3068\u8aad\u307f\u8fbc\u3081\u306a\u3044web\u30da\u30fc\u30b8\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306b\u3064\u3044\u3066\uff0c\u7d39\u4ecb\u3057\u307e\u3059\uff0e\n\u672c\u5f53\u306e\u5b9f\u88c5\u306f GitHub https://github.com/akisato-/pyScraper \u306b\u30a2\u30c3\u30d7\u3057\u3066\u3042\u308a\u307e\u3059\uff0e\n\n\u307e\u305a\u306f\u4f55\u306e\u5de5\u592b\u3082\u306a\u3044\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n(1) requests\u3067web\u30da\u30fc\u30b8\u3092\u53d6\u5f97\u3057\uff0c(2) BeautufulSoup4\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3092\u5b9f\u884c\u3057\u307e\u3059\uff0ePython\u6a19\u6e96\u306eHTML\u30d1\u30fc\u30b5\u306f\u3042\u307e\u308a\u512a\u79c0\u3067\u306f\u306a\u3044\u3067\u3059\u306e\u3067\uff0c\u3053\u3053\u3067\u306flxml\u3092\u5229\u7528\u3057\u307e\u3059\uff0e\nBeautifulSoup4\u306e\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u306f http://qiita.com/itkr/items/513318a9b5b92bd56185 \u306a\u3069\u3092\u53c2\u7167\u3059\u308b\u3068\u826f\u3044\u3067\u3057\u3087\u3046\uff0e\n\n\u5fc5\u8981\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip\u3092\u4f7f\u3044\u307e\u3059\uff0e\npip install requests\npip install lxml\npip install beautifulsoup4\n\n\n\u30bd\u30fc\u30b9\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3068\u601d\u3044\u307e\u3059\uff0e\n\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3044\u30da\u30fc\u30b8\u306eURL\u3068\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u3068\uff0c\u30da\u30fc\u30b8\u306e\u30bf\u30a4\u30c8\u30eb\u306a\u3069\u304cJSON\u5f62\u5f0f\u3067\u5e30\u3063\u3066\u304f\u308b\u4ed5\u7d44\u307f\u3067\u3059\uff0e\n\u95a2\u6570scraping\u304c\u672c\u4f53\u3067\u3059\uff0e\n\nscraping.py\nimport sys\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # get a HTML response\n    response = requests.get(url)\n    html = response.text.encode(response.encoding)  # prevent encoding errors\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n\n\n\nJavaScript\u306b\u5bfe\u5fdc\u3059\u308b\nJavaScript\u3092\u6709\u52b9\u306b\u3057\u3066\u3044\u306a\u3044\u3068\u898b\u3089\u308c\u306a\u3044web\u30da\u30fc\u30b8\u304c\u975e\u5e38\u306b\u5897\u52a0\u3057\u3066\u3044\u307e\u3059\uff0e\u5148\u307b\u3069\u307e\u3067\u306e\u30bd\u30fc\u30b9\u3067\u305d\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\uff0c\u300cJavaScript\u3092\u6709\u52b9\u306b\u3057\u3066\u4e0b\u3055\u3044\u300d\u3068\u3044\u3046\u30da\u30fc\u30b8\u3057\u304b\u53d6\u308c\u306a\u304f\u306a\u308a\u307e\u3059\uff0e\n\u3053\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u306b\uff0crequests\u3067\u884c\u3063\u3066\u3044\u305fweb\u30da\u30fc\u30b8\u53d6\u5f97\u3092\uff0cSelenium\u3068PhantomJS\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u3082\u306e\u306b\u7f6e\u304d\u63db\u3048\u307e\u3059\uff0e\nSelenium\u306f\u30d6\u30e9\u30a6\u30b6\u52d5\u4f5c\u3092\u81ea\u52d5\u5316\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\uff0cPhantomJS\u306fQt\u30d9\u30fc\u30b9\u306e\u30d6\u30e9\u30a6\u30b6\u3067\u3059\uff0e1\n\nPhantomJS\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nMac\u3084Linux\u3067\u306f\uff0cbrew\u3084yum\u306a\u3069\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u30de\u30cd\u30fc\u30b8\u30e3\u3067\u5373\u5ea7\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\uff0e\n\nMac\nbrew install phantomjs\n\n\n\nCentOS\nyum install phantomjs\n\n\nWindows\u3067\u306f\uff0chttp://phantomjs.org/download.html \u304b\u3089\u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\uff0c\u9069\u5f53\u306a\u5834\u6240\u306b\u7f6e\u3044\u305f\u5f8c\u306b\uff0c\u30d1\u30b9\u3092\u901a\u3057\u3066\u304a\u304d\u307e\u3059\uff0e\n\nSelenium\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip\u3067\u3059\u3050\u3067\u304d\u307e\u3059\uff0e\npip install selenium\n\n\n\u30bd\u30fc\u30b9\nSelenium\u3068PhantomJS\u3092\u5229\u7528\u3059\u308b\u3068\uff0c\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u30bd\u30fc\u30b9\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3055\u308c\u307e\u3059\uff0eweb\u30da\u30fc\u30b8\u53d6\u5f97\u4ee5\u964d\u306e\u624b\u9806\u306f\u5909\u66f4\u4e0d\u8981\u3067\u3059\uff0e\nSelenium\u3067PhantomJS\u306eweb\u30c9\u30e9\u30a4\u30d0\u3092\u69cb\u6210\u3057\uff0c\u305d\u306e\u30c9\u30e9\u30a4\u30d0\u3092\u901a\u3058\u3066HTML\u3092\u53d6\u5f97\u3057\u307e\u3059\uff0e\u4ee5\u964d\u306f\u540c\u3058\u3067\u3059\uff0e\n\u30c9\u30e9\u30a4\u30d0\u306e\u52d5\u4f5c\u30ed\u30b0\u3092\u8a18\u9332\u3057\u305f\u3044\u5834\u5408\u306b\u306f\uff0cos.path.devnull\u3092\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u5909\u66f4\u3057\u307e\u3059\uff0e\n\nscraping_js.py\nimport sys\nimport json\nimport os\nimport requests\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # Selenium settings\n    driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n    # get a HTML response\n    driver.get(url)\n    html = driver.page_source.encode('utf-8')  # more sophisticated methods may be available\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n\n\n\nProxy\u306b\u5bfe\u5fdc\u3059\u308b\nPhantomJS\u306e\u5f15\u6570\u3068\u3057\u3066proxy\u306e\u8a2d\u5b9a\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\nphantomjs_args = [ '--proxy=proxy.server.no.basho:0000' ]\ndriver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n\n\nCookie\u306b\u5bfe\u5fdc\u3059\u308b\nPhantomJS\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067Cookie\u304c\u6709\u52b9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\u3082\u3057cookie\u30d5\u30a1\u30a4\u30eb\u3092\u624b\u5143\u306b\u7f6e\u304d\u305f\u3044\u5834\u5408\u306b\u306f\uff0cPhantomJS\u306e\u5f15\u6570\u306b\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\nphantomjs_args = [ '--cookie-file={}'.format(\"cookie.txt\") ]\ndriver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n\n\n\u30bd\u30fc\u30b9\u6700\u7d42\u5f62\u614b\n\u5168\u3066\u306e\u6a5f\u80fd\u3092\u30ab\u30d0\u30fc\u3059\u308b\u3068\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n\nscraping_complete.py\nimport sys\nimport json\nimport os\nimport requests\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # Selenium settings\n    phantomjs_args = [ '--proxy=proxy.server.no.basho:0000', '--cookie-file={}'.format(\"cookie.txt\") ]\n    driver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n    # get a HTML response\n    driver.get(url)\n    html = driver.page_source.encode('utf-8')  # more sophisticated methods may be available\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n\n\n\n\n\n\nPhantomJS\u306f\u30d6\u30e9\u30a6\u30b6\u3067\u3059\u306e\u3067\uff0cIE\u30fbFirefox\u30fbChrome\u306a\u3069\uff0c\u4e00\u822c\u7684\u306b\u5229\u7528\u3055\u308c\u308bweb\u30d6\u30e9\u30a6\u30b6\u306b\u7f6e\u304d\u63db\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e\u8a73\u7d30\u306f\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 http://docs.seleniumhq.org/docs/03_webdriver.jsp#selenium-webdriver-s-drivers \u3092\u898b\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\uff0e\u00a0\u21a9\n\n\n\n\u307f\u306a\u3055\u307e\uff0c\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff0c@_akisato \u3067\u3054\u3056\u3044\u307e\u3059\uff0e\n\n\u30af\u30ed\u30fc\u30e9/web\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0Advent Calender http://qiita.com/advent-calendar/2015/crawler \u306e6\u65e5\u76ee\u306e\u8a18\u4e8b\u3068\u3057\u3066\u66f8\u3044\u3066\u304a\u308a\u307e\u3059\uff0e\n\n\u672c\u65e5\u306f\uff0cJavaScript\u3084cookie\u3092\u8a31\u53ef\u3057\u3066\u3044\u306a\u3044\u3068\u8aad\u307f\u8fbc\u3081\u306a\u3044web\u30da\u30fc\u30b8\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306b\u3064\u3044\u3066\uff0c\u7d39\u4ecb\u3057\u307e\u3059\uff0e\n\n\u672c\u5f53\u306e\u5b9f\u88c5\u306f GitHub https://github.com/akisato-/pyScraper \u306b\u30a2\u30c3\u30d7\u3057\u3066\u3042\u308a\u307e\u3059\uff0e\n\n# \u307e\u305a\u306f\u4f55\u306e\u5de5\u592b\u3082\u306a\u3044\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n\n(1) requests\u3067web\u30da\u30fc\u30b8\u3092\u53d6\u5f97\u3057\uff0c(2) BeautufulSoup4\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3092\u5b9f\u884c\u3057\u307e\u3059\uff0ePython\u6a19\u6e96\u306eHTML\u30d1\u30fc\u30b5\u306f\u3042\u307e\u308a\u512a\u79c0\u3067\u306f\u306a\u3044\u3067\u3059\u306e\u3067\uff0c\u3053\u3053\u3067\u306flxml\u3092\u5229\u7528\u3057\u307e\u3059\uff0e\nBeautifulSoup4\u306e\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u306f http://qiita.com/itkr/items/513318a9b5b92bd56185 \u306a\u3069\u3092\u53c2\u7167\u3059\u308b\u3068\u826f\u3044\u3067\u3057\u3087\u3046\uff0e\n\n## \u5fc5\u8981\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\npip\u3092\u4f7f\u3044\u307e\u3059\uff0e\n\n```\npip install requests\npip install lxml\npip install beautifulsoup4\n```\n\n## \u30bd\u30fc\u30b9\n\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3068\u601d\u3044\u307e\u3059\uff0e\n\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3057\u305f\u3044\u30da\u30fc\u30b8\u306eURL\u3068\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u6307\u5b9a\u3059\u308b\u3068\uff0c\u30da\u30fc\u30b8\u306e\u30bf\u30a4\u30c8\u30eb\u306a\u3069\u304cJSON\u5f62\u5f0f\u3067\u5e30\u3063\u3066\u304f\u308b\u4ed5\u7d44\u307f\u3067\u3059\uff0e\n\u95a2\u6570scraping\u304c\u672c\u4f53\u3067\u3059\uff0e\n\n```py:scraping.py\nimport sys\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # get a HTML response\n    response = requests.get(url)\n    html = response.text.encode(response.encoding)  # prevent encoding errors\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n```\n\n# JavaScript\u306b\u5bfe\u5fdc\u3059\u308b\n\nJavaScript\u3092\u6709\u52b9\u306b\u3057\u3066\u3044\u306a\u3044\u3068\u898b\u3089\u308c\u306a\u3044web\u30da\u30fc\u30b8\u304c\u975e\u5e38\u306b\u5897\u52a0\u3057\u3066\u3044\u307e\u3059\uff0e\u5148\u307b\u3069\u307e\u3067\u306e\u30bd\u30fc\u30b9\u3067\u305d\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\uff0c\u300cJavaScript\u3092\u6709\u52b9\u306b\u3057\u3066\u4e0b\u3055\u3044\u300d\u3068\u3044\u3046\u30da\u30fc\u30b8\u3057\u304b\u53d6\u308c\u306a\u304f\u306a\u308a\u307e\u3059\uff0e\n\n\u3053\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u306b\u5bfe\u5fdc\u3059\u308b\u305f\u3081\u306b\uff0crequests\u3067\u884c\u3063\u3066\u3044\u305fweb\u30da\u30fc\u30b8\u53d6\u5f97\u3092\uff0cSelenium\u3068PhantomJS\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u3082\u306e\u306b\u7f6e\u304d\u63db\u3048\u307e\u3059\uff0e\nSelenium\u306f\u30d6\u30e9\u30a6\u30b6\u52d5\u4f5c\u3092\u81ea\u52d5\u5316\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\uff0cPhantomJS\u306fQt\u30d9\u30fc\u30b9\u306e\u30d6\u30e9\u30a6\u30b6\u3067\u3059\uff0e[^browser]\n\n[^browser]: PhantomJS\u306f\u30d6\u30e9\u30a6\u30b6\u3067\u3059\u306e\u3067\uff0cIE\u30fbFirefox\u30fbChrome\u306a\u3069\uff0c\u4e00\u822c\u7684\u306b\u5229\u7528\u3055\u308c\u308bweb\u30d6\u30e9\u30a6\u30b6\u306b\u7f6e\u304d\u63db\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e\u8a73\u7d30\u306f\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 http://docs.seleniumhq.org/docs/03_webdriver.jsp#selenium-webdriver-s-drivers \u3092\u898b\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\uff0e\n\n## PhantomJS\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nMac\u3084Linux\u3067\u306f\uff0cbrew\u3084yum\u306a\u3069\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u30de\u30cd\u30fc\u30b8\u30e3\u3067\u5373\u5ea7\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\uff0e\n\n```Mac:Mac\nbrew install phantomjs\n```\n\n```CentoOS:CentOS\nyum install phantomjs\n```\n\nWindows\u3067\u306f\uff0chttp://phantomjs.org/download.html \u304b\u3089\u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\uff0c\u9069\u5f53\u306a\u5834\u6240\u306b\u7f6e\u3044\u305f\u5f8c\u306b\uff0c\u30d1\u30b9\u3092\u901a\u3057\u3066\u304a\u304d\u307e\u3059\uff0e\n\n## Selenium\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\npip\u3067\u3059\u3050\u3067\u304d\u307e\u3059\uff0e\n\n```\npip install selenium\n```\n\n## \u30bd\u30fc\u30b9\n\nSelenium\u3068PhantomJS\u3092\u5229\u7528\u3059\u308b\u3068\uff0c\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u30bd\u30fc\u30b9\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3055\u308c\u307e\u3059\uff0eweb\u30da\u30fc\u30b8\u53d6\u5f97\u4ee5\u964d\u306e\u624b\u9806\u306f\u5909\u66f4\u4e0d\u8981\u3067\u3059\uff0e\nSelenium\u3067PhantomJS\u306eweb\u30c9\u30e9\u30a4\u30d0\u3092\u69cb\u6210\u3057\uff0c\u305d\u306e\u30c9\u30e9\u30a4\u30d0\u3092\u901a\u3058\u3066HTML\u3092\u53d6\u5f97\u3057\u307e\u3059\uff0e\u4ee5\u964d\u306f\u540c\u3058\u3067\u3059\uff0e\n\u30c9\u30e9\u30a4\u30d0\u306e\u52d5\u4f5c\u30ed\u30b0\u3092\u8a18\u9332\u3057\u305f\u3044\u5834\u5408\u306b\u306f\uff0cos.path.devnull\u3092\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u5909\u66f4\u3057\u307e\u3059\uff0e\n\n```py:scraping_js.py\nimport sys\nimport json\nimport os\nimport requests\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # Selenium settings\n    driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n    # get a HTML response\n    driver.get(url)\n    html = driver.page_source.encode('utf-8')  # more sophisticated methods may be available\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n```\n\n# Proxy\u306b\u5bfe\u5fdc\u3059\u308b\n\nPhantomJS\u306e\u5f15\u6570\u3068\u3057\u3066proxy\u306e\u8a2d\u5b9a\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n```py\nphantomjs_args = [ '--proxy=proxy.server.no.basho:0000' ]\ndriver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n```\n\n# Cookie\u306b\u5bfe\u5fdc\u3059\u308b\n\nPhantomJS\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067Cookie\u304c\u6709\u52b9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\u3082\u3057cookie\u30d5\u30a1\u30a4\u30eb\u3092\u624b\u5143\u306b\u7f6e\u304d\u305f\u3044\u5834\u5408\u306b\u306f\uff0cPhantomJS\u306e\u5f15\u6570\u306b\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n```py\nphantomjs_args = [ '--cookie-file={}'.format(\"cookie.txt\") ]\ndriver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n```\n\n# \u30bd\u30fc\u30b9\u6700\u7d42\u5f62\u614b\n\n\u5168\u3066\u306e\u6a5f\u80fd\u3092\u30ab\u30d0\u30fc\u3059\u308b\u3068\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n\n```py:scraping_complete.py\nimport sys\nimport json\nimport os\nimport requests\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\n\ndef scraping(url, output_name):\n    # Selenium settings\n    phantomjs_args = [ '--proxy=proxy.server.no.basho:0000', '--cookie-file={}'.format(\"cookie.txt\") ]\n    driver = webdriver.PhantomJS(service_args=phantomjs_args, service_log_path=os.path.devnull)\n    # get a HTML response\n    driver.get(url)\n    html = driver.page_source.encode('utf-8')  # more sophisticated methods may be available\n    # parse the response\n    soup = BeautifulSoup(html, \"lxml\")\n    # extract\n    ## title\n    header = soup.find(\"head\")\n    title = header.find(\"title\").text\n    ## description\n    description = header.find(\"meta\", attrs={\"name\": \"description\"})\n    description_content = description.attrs['content'].text\n    # output\n    output = {\"title\": title, \"description\": description_content}\n    # write the output as a json file\n    with open(output_name, \"w\") as fout:\n        json = json.dump(output, fout, indent=4, sort_keys=True)\n\nif __name__ == '__main__':\n    # arguments\n    argvs = sys.argv\n    ## check\n    if len(argvs) != 2:\n        print \"Usage: python scraping.py [url] [output]\"\n        exit()\n    url = argvs[1]\n    output_name = argvs[2]\n\n    scraping(url, output_name)\n```\n", "tags": ["Python", "BeautifulSoup", "PhantomJS", "Selenium"]}