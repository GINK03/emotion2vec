{"context": " More than 1 year has passed since last update.Python\u3067\u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u591a\u5024\u5206\u985e\u3092\u3057\u3066\u307f\u308b.\nPython Machine Learning\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u308b.\n\u307e\u305a, \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u53d6\u5f97\u3059\u308b. \u3053\u3053\u3067\u306f\u6709\u540d\u306airis\u3092\u4f7f\u3046.\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u53d6\u5f97\u3057, \u5206\u985e\u306b\u7528\u3044\u308b\u30ab\u30e9\u30e0\u3092X, \u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\u3092y\u306b\u4ee3\u5165\u3057\u3066\u3044\u308b.\n\u305d\u3057\u3066\u305d\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092scikit-learn\u306ecross_validation\u3092\u4f7f\u3063\u3066\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272,\n\u305d\u3057\u3066\u5e73\u57470\u5206\u65631\u306b\u63c3\u3048\u308b\u6a19\u6e96\u5316\u3092\u884c\u3063\u3066\u304a\u304f.\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\niris = datasets.load_iris()\nprint(iris)\nX = iris.data[:, [2, 3]] # iris\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7b2c3, 4\u30ab\u30e9\u30e0\ny = iris.target # iris\u306e\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272, \u4eca\u306ftest_size=0.3\u306a\u306e\u3067\u8a13\u7df4\u30c7\u30fc\u30bf7\u5272\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf3\u5272\u306b\u3057\u3066\u3044\u308b\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nsc = StandardScaler()\nsc.fit(X_train)\n# \u5e73\u57470, \u5206\u65631\u306b\u6a19\u6e96\u5316\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\u6b21\u306b, \u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u5206\u96e2\u5e73\u9762\u3092\u4f5c\u308b\u305f\u3081\u306e\u8a13\u7df4\u3092\u884c\u3046.\n\u3053\u3053\u3067, scikit-learn\u3067\u306f\u591a\u304f\u306e\u30af\u30e9\u30b9\u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u304a\u3044\u3066\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306fOne-vs.-Rest\u6cd5\u3092\u7528\u3044\u3066\u3044\u308b.\nOne-vs.-Rest\u6cd5\u3067\u306f\u5404\u30af\u30e9\u30b9\u306b\u3064\u3044\u3066\u4e00\u3064\u306e\u5206\u96e2\u5e73\u9762\u3092\u4f5c\u308b.\n\u3064\u307e\u308a, \u305d\u306e\u30af\u30e9\u30b9\u306b\u5c5e\u3059\u308b\u304b\u5426\u304b\u3092\u5224\u5225\u3059\u308b\u5e73\u9762\u3092\u4f5c\u308b\u3053\u3068\u306b\u3088\u308a\u3044\u305a\u308c\u304b\u306e\u30af\u30e9\u30b9\u3078\u5206\u985e\u3059\u308b.\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u5b9f\u969b\u306b\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u8a13\u7df4\u3055\u305b\u3066\u307f\u308b.\nfrom sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1)\nppn.fit(X_train_std, y_train)\n\n\u3053\u3053\u3067 ppn? \u306a\u3069\u3068\u3059\u308b\u3068, Perceptron\u306b\u6e21\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a73\u7d30\u304c\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067,\n\u5fc5\u8981\u306a\u5834\u5408\u306f\u898b\u3066\u304a\u304f\u3068\u3044\u3044\u304b\u3082.\n\u3053\u3053\u3067\u306fn_iter(\u7e70\u308a\u8fd4\u3057\u56de\u6570)\u309240, eta0(\u5b66\u7fd2\u7387)\u30920.1\u3068\u3057\u3066\u521d\u671f\u5316\u3057\u305f.\nfit\u306f\u5b9f\u969b\u306b\u8a13\u7df4\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3067\u3042\u308a, \u3053\u3053\u3067\u306f\u6a19\u6e96\u5316\u3057\u305f\u8a13\u7df4\u30c7\u30fc\u30bfX_train_std\u3068\u305d\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eby_train\u3092\u6e21\u3057\u305f.\n\u3067\u306f\u8a13\u7df4\u3092\u884c\u3063\u305f\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u304c\u3069\u306e\u7a0b\u5ea6\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6b63\u89e3\u3057\u305f\u306e\u304b\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u898b\u3066\u307f\u308b.\ny_pred = ppn.predict(X_test_std)\nprint('Misclassified samples: %d' % (y_test != y_pred).sum())\n\n\u3053\u308c\u3067\u6b63\u89e3\u3068\u4e00\u81f4\u3057\u306a\u304b\u3063\u305f\u30c7\u30fc\u30bf\u6570\u304c\u53d6\u5f97\u3067\u304d\u308b.\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6570\u3092\u78ba\u8a8d\u3059\u308b\u306e\u304c\u9762\u5012\u306a\u306e\u3067, scikit-learn\u306b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u6b63\u89e3\u7387\u3092\u51fa\u3057\u3066\u307f\u308b.\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n\n\u3053\u308c\u3067\u6b63\u7b54\u7387\u3092\u78ba\u8a8d\u3067\u304d\u308b. y_test\u304c\u6b63\u89e3\u306e\u30af\u30e9\u30b9\u3067, y_pred\u304c\u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3059\u308b.\n\u6c7a\u5b9a\u5883\u754c\u3092\u63cf\u753b\u3057\u3066\u307f\u308b. \u4ee5\u4e0b\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b.\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    #setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    # \u6700\u5c0f\u5024, \u6700\u5927\u5024\u304b\u3089\u30a8\u30ea\u30a2\u306e\u9818\u57df\u3092\u5272\u308a\u51fa\u3059\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    # resolution\u306e\u9593\u9694\u3067\u533a\u5207\u3063\u305f\u9818\u57df\u3092\u5b9a\u7fa9\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                            np.arange(x2_min, x2_max, resolution))\n    # print(xx1.shape)\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot all samples\n    X_test, y_test = X[test_idx, :], y[test_idx]\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='',\n            alpha=1.0, linewidth=1, marker='o',\n            s=55, label='test set')\n\n\u305d\u3057\u3066\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u3088\u308a, \u3053\u306e\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059.\nX_combined_std = np.vstack((X_train_std, X_test_std)) # \u7e26\u306b\u9023\u7d50\ny_combined = np.hstack((y_train, y_test)) # \u6a2a\u306b\u9023\u7d50\nplot_decision_regions(X=X_combined_std,\n                        y=y_combined,\n                        classifier=ppn,\n                        test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n\n\u3053\u308c\u3092\u5b9f\u884c\u3059\u308b\u3068, \u6c7a\u5b9a\u5883\u754c\u304c\u8868\u793a\u3055\u308c\u308b. \u975e\u7dda\u5f62\u306e\u305f\u3081, \u5b8c\u74a7\u306b\u5206\u985e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b.\n\n\n\u53c2\u8003\nPython Machine Learning\nPython\u3067\u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u591a\u5024\u5206\u985e\u3092\u3057\u3066\u307f\u308b.\n[Python Machine Learning](https://github.com/rasbt/python-machine-learning-book)\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u308b.\n\n\n\u307e\u305a, \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u53d6\u5f97\u3059\u308b. \u3053\u3053\u3067\u306f\u6709\u540d\u306airis\u3092\u4f7f\u3046.\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u53d6\u5f97\u3057, \u5206\u985e\u306b\u7528\u3044\u308b\u30ab\u30e9\u30e0\u3092X, \u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\u3092y\u306b\u4ee3\u5165\u3057\u3066\u3044\u308b.\n\u305d\u3057\u3066\u305d\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092scikit-learn\u306ecross_validation\u3092\u4f7f\u3063\u3066\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272,\n\u305d\u3057\u3066\u5e73\u57470\u5206\u65631\u306b\u63c3\u3048\u308b\u6a19\u6e96\u5316\u3092\u884c\u3063\u3066\u304a\u304f.\n\n```\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\niris = datasets.load_iris()\nprint(iris)\nX = iris.data[:, [2, 3]] # iris\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7b2c3, 4\u30ab\u30e9\u30e0\ny = iris.target # iris\u306e\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272, \u4eca\u306ftest_size=0.3\u306a\u306e\u3067\u8a13\u7df4\u30c7\u30fc\u30bf7\u5272\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf3\u5272\u306b\u3057\u3066\u3044\u308b\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nsc = StandardScaler()\nsc.fit(X_train)\n# \u5e73\u57470, \u5206\u65631\u306b\u6a19\u6e96\u5316\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n```\n\n\u6b21\u306b, \u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u5206\u96e2\u5e73\u9762\u3092\u4f5c\u308b\u305f\u3081\u306e\u8a13\u7df4\u3092\u884c\u3046.\n\u3053\u3053\u3067, scikit-learn\u3067\u306f\u591a\u304f\u306e\u30af\u30e9\u30b9\u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u304a\u3044\u3066\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306fOne-vs.-Rest\u6cd5\u3092\u7528\u3044\u3066\u3044\u308b.\n\nOne-vs.-Rest\u6cd5\u3067\u306f\u5404\u30af\u30e9\u30b9\u306b\u3064\u3044\u3066\u4e00\u3064\u306e\u5206\u96e2\u5e73\u9762\u3092\u4f5c\u308b.\n\u3064\u307e\u308a, \u305d\u306e\u30af\u30e9\u30b9\u306b\u5c5e\u3059\u308b\u304b\u5426\u304b\u3092\u5224\u5225\u3059\u308b\u5e73\u9762\u3092\u4f5c\u308b\u3053\u3068\u306b\u3088\u308a\u3044\u305a\u308c\u304b\u306e\u30af\u30e9\u30b9\u3078\u5206\u985e\u3059\u308b.\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u5b9f\u969b\u306b\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u8a13\u7df4\u3055\u305b\u3066\u307f\u308b.\n\n```\nfrom sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1)\nppn.fit(X_train_std, y_train)\n```\n\n\u3053\u3053\u3067 ```ppn?``` \u306a\u3069\u3068\u3059\u308b\u3068, Perceptron\u306b\u6e21\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a73\u7d30\u304c\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067,\n\u5fc5\u8981\u306a\u5834\u5408\u306f\u898b\u3066\u304a\u304f\u3068\u3044\u3044\u304b\u3082.\n\u3053\u3053\u3067\u306fn_iter(\u7e70\u308a\u8fd4\u3057\u56de\u6570)\u309240, eta0(\u5b66\u7fd2\u7387)\u30920.1\u3068\u3057\u3066\u521d\u671f\u5316\u3057\u305f.\nfit\u306f\u5b9f\u969b\u306b\u8a13\u7df4\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3067\u3042\u308a, \u3053\u3053\u3067\u306f\u6a19\u6e96\u5316\u3057\u305f\u8a13\u7df4\u30c7\u30fc\u30bfX_train_std\u3068\u305d\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eby_train\u3092\u6e21\u3057\u305f.\n\n\u3067\u306f\u8a13\u7df4\u3092\u884c\u3063\u305f\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u304c\u3069\u306e\u7a0b\u5ea6\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6b63\u89e3\u3057\u305f\u306e\u304b\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u898b\u3066\u307f\u308b.\n\n```\ny_pred = ppn.predict(X_test_std)\nprint('Misclassified samples: %d' % (y_test != y_pred).sum())\n```\n\n\u3053\u308c\u3067\u6b63\u89e3\u3068\u4e00\u81f4\u3057\u306a\u304b\u3063\u305f\u30c7\u30fc\u30bf\u6570\u304c\u53d6\u5f97\u3067\u304d\u308b.\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6570\u3092\u78ba\u8a8d\u3059\u308b\u306e\u304c\u9762\u5012\u306a\u306e\u3067, scikit-learn\u306b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u6b63\u89e3\u7387\u3092\u51fa\u3057\u3066\u307f\u308b.\n\n```\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n```\n\n\u3053\u308c\u3067\u6b63\u7b54\u7387\u3092\u78ba\u8a8d\u3067\u304d\u308b. y_test\u304c\u6b63\u89e3\u306e\u30af\u30e9\u30b9\u3067, y_pred\u304c\u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3059\u308b.\n\n\u6c7a\u5b9a\u5883\u754c\u3092\u63cf\u753b\u3057\u3066\u307f\u308b. \u4ee5\u4e0b\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b.\n\n```\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    #setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    # \u6700\u5c0f\u5024, \u6700\u5927\u5024\u304b\u3089\u30a8\u30ea\u30a2\u306e\u9818\u57df\u3092\u5272\u308a\u51fa\u3059\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    # resolution\u306e\u9593\u9694\u3067\u533a\u5207\u3063\u305f\u9818\u57df\u3092\u5b9a\u7fa9\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                            np.arange(x2_min, x2_max, resolution))\n    # print(xx1.shape)\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot all samples\n    X_test, y_test = X[test_idx, :], y[test_idx]\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='',\n            alpha=1.0, linewidth=1, marker='o',\n            s=55, label='test set')\n```\n\n\u305d\u3057\u3066\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u3088\u308a, \u3053\u306e\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059.\n\n```\nX_combined_std = np.vstack((X_train_std, X_test_std)) # \u7e26\u306b\u9023\u7d50\ny_combined = np.hstack((y_train, y_test)) # \u6a2a\u306b\u9023\u7d50\nplot_decision_regions(X=X_combined_std,\n                        y=y_combined,\n                        classifier=ppn,\n                        test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n\u3053\u308c\u3092\u5b9f\u884c\u3059\u308b\u3068, \u6c7a\u5b9a\u5883\u754c\u304c\u8868\u793a\u3055\u308c\u308b. \u975e\u7dda\u5f62\u306e\u305f\u3081, \u5b8c\u74a7\u306b\u5206\u985e\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b.\n\n![decision_boundaries.png](https://qiita-image-store.s3.amazonaws.com/0/70855/43b6bc5b-1c40-f882-1a11-2371ab370150.png)\n\n\n## \u53c2\u8003\n[Python Machine Learning](https://github.com/rasbt/python-machine-learning-book)\n", "tags": ["Python", "scikit-learn"]}