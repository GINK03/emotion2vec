{"tags": ["Spark", "docker"], "context": "\u524d\u56de\u306e\u6295\u7a3f\u3067\u69cb\u7bc9\u3057\u305fjobserver\u3067\u3057\u305f\u304c\u3001\u3069\u3046\u3084\u3089\u5b9f\u884c\u53ef\u80fd\u306ajar\u306e\u30b5\u30a4\u30ba\u306b\u3088\u3063\u3066\u306f\u3001put\u306b\u5931\u6557\u3059\u308b\u3088\u3046\u3067\u3059\u3002\u3002\n\ngithub#spark-jobserver/issue/294\n\n\u4e0a\u8a18\u3092\u53c2\u7167\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u56f0\u3063\u3066\u3044\u308b\u4eba\u304c\u3044\u305f\u3088\u3046\u3067\u3059\u3002\n\u307b\u3093\u3068\u306b\u3053\u308c\u3067\u52d5\u304f\u306e\u304b\u3088\u3002\u3002\u3068\u3044\u3046\u3068\u3053\u308d\u304c\u305d\u3082\u305d\u3082\u602a\u3057\u304b\u3063\u305f\u306e\u3067Docker\u3092\u5229\u7528\u3057\u3066\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\nFROM velvia/spark-jobserver:0.6.0\n\nADD  jobserver.conf /app/docker.conf\n\njobserver.conf\u3092\u30d3\u30eb\u30c9\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u914d\u7f6e\u3057\u3066docker build\u3057\u307e\u3057\u305f\u3002\n\njobserver.conf\nspark {\n # spark.master will be passed to each job's JobContext\n #master = \"yarn-client\"\njobserver {\n port = 8090\n jar-store-rootdir = /mnt/tmp/spark-jobserver/jars\n jobdao = spark.jobserver.io.JobFileDAO\n filedao {\n   rootdir = /mnt/tmp/spark-jobserver/filedao/data\n }\n}\n# predefined Spark contexts\ncontexts {\n # test {\n #   num-cpu-cores = 1            # Number of cores to allocate.  Required.\n #   memory-per-node = 1g         # Executor memory per node, -Xmx style eg 512m, 1G, etc.\n #   spark.executor.instances = 1\n # }\n # define additional contexts here\n}\n# universal context configuration.  These settings can be overridden, see README.md\ncontext-settings {\n num-cpu-cores = 4          # Number of cores to allocate.  Required.\n memory-per-node = 1g         # Executor memory per node, -Xmx style eg 512m, #1G, etc.\n spark.executor.instances = 2\n # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,\n # such as hadoop connection settings that don't use the \"spark.\" prefix\n passthrough {\n   #es.nodes = \"192.1.1.1\"\n }\n}\n# This needs to match SPARK_HOME for cluster SparkContexts to be created successfully\nhome = \"/usr/lib/spark\"\n}\n\nspray.can.server.parsing.max-content-length=160537630\n\n\n\u4e00\u756a\u6700\u5f8c\u306e\u884c\u306b\u3042\u308b\u3001\nspray.can.server.parsing.max-content-length=160537630\n\n\u306e\u53f3\u8fba\u3067\u30b3\u30f3\u30c6\u30f3\u30c4\u30b5\u30a4\u30ba\u306e\u4e0a\u9650\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u3067\u3059\u3002\n\u3053\u308c\u3092\u3057\u305f\u3042\u3068\u306b\u3001docker build\u3057\u305f\u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3057\u3066\u3001culr --data-binary\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u4e8b\u8c61\u304c\u89e3\u6c7a\u3057\u307e\u3057\u305f\u3002\n\u3082\u3046\u3061\u3087\u3063\u3068\u624b\u8efd\u306aUI\u3068\u304b\u304c\u6b32\u3057\u3044\u3068\u3053\u308d\u3067\u3059\u306d\uff3e\uff3e\uff1b\n\u30bd\u30fc\u30b9\u8aad\u3093\u3067\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n[\u524d\u56de\u306e\u6295\u7a3f](http://qiita.com/letusfly85/items/9fb90d5166116241c431)\u3067\u69cb\u7bc9\u3057\u305fjobserver\u3067\u3057\u305f\u304c\u3001\u3069\u3046\u3084\u3089\u5b9f\u884c\u53ef\u80fd\u306ajar\u306e\u30b5\u30a4\u30ba\u306b\u3088\u3063\u3066\u306f\u3001put\u306b\u5931\u6557\u3059\u308b\u3088\u3046\u3067\u3059\u3002\u3002\n\n* [github#spark-jobserver/issue/294](https://github.com/spark-jobserver/spark-jobserver/issues/294)\n\n\u4e0a\u8a18\u3092\u53c2\u7167\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u56f0\u3063\u3066\u3044\u308b\u4eba\u304c\u3044\u305f\u3088\u3046\u3067\u3059\u3002\n\n\u307b\u3093\u3068\u306b\u3053\u308c\u3067\u52d5\u304f\u306e\u304b\u3088\u3002\u3002\u3068\u3044\u3046\u3068\u3053\u308d\u304c\u305d\u3082\u305d\u3082\u602a\u3057\u304b\u3063\u305f\u306e\u3067Docker\u3092\u5229\u7528\u3057\u3066\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n```Dockerfile\nFROM velvia/spark-jobserver:0.6.0\n\nADD  jobserver.conf /app/docker.conf\n```\n\njobserver.conf\u3092\u30d3\u30eb\u30c9\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u914d\u7f6e\u3057\u3066`docker build`\u3057\u307e\u3057\u305f\u3002\n\n```jobserver.conf\nspark {\n # spark.master will be passed to each job's JobContext\n #master = \"yarn-client\"\njobserver {\n port = 8090\n jar-store-rootdir = /mnt/tmp/spark-jobserver/jars\n jobdao = spark.jobserver.io.JobFileDAO\n filedao {\n   rootdir = /mnt/tmp/spark-jobserver/filedao/data\n }\n}\n# predefined Spark contexts\ncontexts {\n # test {\n #   num-cpu-cores = 1            # Number of cores to allocate.  Required.\n #   memory-per-node = 1g         # Executor memory per node, -Xmx style eg 512m, 1G, etc.\n #   spark.executor.instances = 1\n # }\n # define additional contexts here\n}\n# universal context configuration.  These settings can be overridden, see README.md\ncontext-settings {\n num-cpu-cores = 4          # Number of cores to allocate.  Required.\n memory-per-node = 1g         # Executor memory per node, -Xmx style eg 512m, #1G, etc.\n spark.executor.instances = 2\n # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,\n # such as hadoop connection settings that don't use the \"spark.\" prefix\n passthrough {\n   #es.nodes = \"192.1.1.1\"\n }\n}\n# This needs to match SPARK_HOME for cluster SparkContexts to be created successfully\nhome = \"/usr/lib/spark\"\n}\n\nspray.can.server.parsing.max-content-length=160537630\n```\n\n\u4e00\u756a\u6700\u5f8c\u306e\u884c\u306b\u3042\u308b\u3001\n\n```\nspray.can.server.parsing.max-content-length=160537630\n```\n\n\u306e\u53f3\u8fba\u3067\u30b3\u30f3\u30c6\u30f3\u30c4\u30b5\u30a4\u30ba\u306e\u4e0a\u9650\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u3067\u3059\u3002\n\n\u3053\u308c\u3092\u3057\u305f\u3042\u3068\u306b\u3001docker build\u3057\u305f\u30a4\u30e1\u30fc\u30b8\u3092\u5229\u7528\u3057\u3066\u3001culr --data-binary\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u4e8b\u8c61\u304c\u89e3\u6c7a\u3057\u307e\u3057\u305f\u3002\n\n\u3082\u3046\u3061\u3087\u3063\u3068\u624b\u8efd\u306aUI\u3068\u304b\u304c\u6b32\u3057\u3044\u3068\u3053\u308d\u3067\u3059\u306d\uff3e\uff3e\uff1b\n\n\u30bd\u30fc\u30b9\u8aad\u3093\u3067\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n"}