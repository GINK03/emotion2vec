{"tags": ["R", "NLP", "text2vec"], "context": "\u3053\u306e\u6587\u66f8\u306f Dmitriy Selivanov \u306b\u3088\u308bR\u30d1\u30c3\u30b1\u30fc\u30b8text2vec (version 0.3.0) \u306e\u30d3\u30cd\u30c3\u30c8 \"Analyzing Texts with the text2vec Package\" \u306e\u65e5\u672c\u8a9e\u8a33\u3067\u3059\uff0e\n\u305f\u3060\u3057\u6587\u4e2d\u306e\u6ce8\u306f\u5168\u3066\u8a33\u8005\u306b\u3088\u308b\u3082\u306e\u3067\u30591\uff0e\nLicense: MIT\n\u95a2\u9023\u6587\u66f8\n\ntext2vec vignette: text2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790\ntext2vec vignette: GloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\ntext2vec vignette: \u767a\u5c55\u7684\u306a\u8a71\u984c\n\n\n\n\u7279\u5fb4\ntext2vec \u306fR\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790\u3068\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u305f\u3081\u306e\uff0c\u7c21\u6f54\u306aAPI\u3092\u5099\u3048\u305f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\n\u3053\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306fC++\u306b\u3088\u3063\u3066\u6ce8\u610f\u6df1\u304f\u66f8\u304b\u308c\u3066\u3044\u308b\u305f\u3081\u52b9\u7387\u7684\u3067\uff0c\u30e1\u30e2\u30ea\u306b\u3082\u512a\u3057\u3044\u3067\u3059\uff0e\u7b87\u6240\u306b\u3088\u3063\u3066\u306f\u512a\u308c\u305fRcppParallel\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u5b8c\u5168\u306b\u4e26\u5217\u5316\u3055\u308c\u3066\u304a\u308a\uff0cGloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u8a13\u7df4\u306f\u305d\u306e\u4e00\u4f8b\u3067\u3059\uff0e\u3064\u307e\u308a\uff0cOS X\u3067\u3082Linux\u3067\u3082Windows\u3067\u3082Solaris\uff08x86\uff09\u3067\u3082\uff0c\u4f59\u8a08\u306a\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u3057\u305f\u308a\u6280\u5de7\u3092\u51dd\u3089\u3059\u3053\u3068\u306a\u3057\u306b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u304c\u4e26\u5217\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff0e\u307e\u305f\uff0c\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0API\u304c\u3042\u308b\u306e\u3067\u30e6\u30fc\u30b6\u306f\u5168\u3066\u306e\u30c7\u30fc\u30bf\u3092RAM\u306b\u8aad\u307f\u8fbc\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u305b\u3093\uff0e\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u3067\u306ftext2vec\u3092\u4f7f\u3044\uff0c\u4efb\u610f\u306en\u30b0\u30e9\u30e0\u306e\u8a9e\u5f59\u304b\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3092\u7528\u3044\u3066\u30c6\u30ad\u30b9\u30c8\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\uff0e\u6700\u5148\u7aef\u306eGloVe\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3092\u3053\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u4f7f\u3046\u65b9\u6cd5\u306e\u89e3\u8aac\u306b\u3064\u3044\u3066\u306fglove\u30d3\u30cd\u30c3\u30c8\u3092\u898b\u3066\u304f\u3060\u3055\u3044\uff0e\n\n\u30c6\u30ad\u30b9\u30c8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n\u30c6\u30ad\u30b9\u30c8\u30de\u30a4\u30cb\u30f3\u30b0\u3084\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5927\u62b5\u306e\u30e2\u30c7\u30ea\u30f3\u30b0\u3067\u306fbag-of-words\u304bbag-of-n-grams\u3068\u3044\u3046\u65b9\u6cd5\u304c\u4f7f\u308f\u308c\u307e\u30592\uff0e\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306f\u5358\u7d14\u3067\u3042\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\uff0c\u30c6\u30ad\u30b9\u30c8\u306e\u5206\u985e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u901a\u5e38\u306f\u9ad8\u3044\u6027\u80fd\u3092\u767a\u63ee\u3057\u307e\u3059\uff0e\u3067\u3059\u304c\uff0c\u305d\u306e\u7406\u8ad6\u7684\u306a\u5358\u7d14\u3055\u3068\u5b9f\u7528\u7684\u306a\u52b9\u7387\u6027\u3068\u306f\u5bfe\u7167\u7684\u306b\uff0cbag-of-words\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u306f\u6280\u8853\u7684\u306b\u306f\u96e3\u554f\u3067\u3059\uff0eR\u306e\u5834\u5408\u306f\u30b3\u30d4\u30fc\u6642\u4fee\u6b63\uff08copy-on-modify\uff09\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u304c\u3042\u308b\u306e\u3067\u7279\u306b\u96e3\u3057\u3044\u3067\u3059\uff0e\n\n\u30c6\u30ad\u30b9\u30c8\u5206\u6790\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\n\u5178\u578b\u7684\u306a\u30c6\u30ad\u30b9\u30c8\u5206\u6790\u3068\u5206\u6790\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u7d30\u90e8\u306b\u3064\u3044\u3066\uff0c\u7c21\u5358\u306b\u632f\u308a\u8fd4\u3063\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n\n\u7814\u7a76\u8005\u306f\u901a\u5e38\uff0c\u5165\u529b\u6587\u66f8\u304b\u3089\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\uff08document-term matrix\uff0cDTM\uff09\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u307e\u3059\uff0e\u8a00\u3044\u304b\u3048\u308b\u3068\uff0c\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u5358\u8a9e\u3084n\u30b0\u30e9\u30e0\u3092\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u5199\u50cf\u3057\u3066\u6587\u66f8\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u3053\u3068\u3067\u3059\uff0e\n\n\n\u7814\u7a76\u8005\u306f\u30e2\u30c7\u30eb\u3092DTM\u306b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3057\u307e\u3059\uff0e\u30e2\u30c7\u30eb\u306f\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u5668\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3057\uff0c\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb\u3084\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\u30e2\u30c7\u30eb\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306b\u306f\u30e2\u30c7\u30eb\u306e\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3084\u691c\u8a3c\u3082\u542b\u307f\u307e\u3059\uff0e\n\n\n\u6700\u5f8c\u306b\uff0c\u7814\u7a76\u8005\u306f\u30e2\u30c7\u30eb\u3092\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3057\u307e\u3059\uff0e\n\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u3067\u306f\u4e3b\u306b1\u756a\u76ee\u306e\u6bb5\u968e\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3057\u307e\u3059\uff0e\u30c6\u30ad\u30b9\u30c8\u305d\u308c\u81ea\u4f53\u306f\u5927\u91cf\u306e\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u306f\u758e\u884c\u5217\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u308b\u306e\u3067\uff0c\u666e\u901a\u306f\u3042\u307e\u308a\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3057\u307e\u305b\u3093\uff0eR\u306b\u306f\u30b3\u30d4\u30fc\u6642\u4fee\u6b63\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u304c\u3042\u308b\u306e\u3067\uff0c\u53cd\u5fa9\u7684\u306bDTM\u3092\u5927\u304d\u304f\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u5bb9\u6613\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\u305d\u308c\u3086\u3048\uff0c\u5c0f\u3055\u306a\u6587\u66f8\u96c6\u5408\u3067\u3042\u3063\u3066\u3082DTM\u306e\u69cb\u7bc9\u304c\u5206\u6790\u8005\u3084\u7814\u7a76\u8005\u306b\u3068\u3063\u3066\u306e\u6df1\u523b\u306a\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3068\u306a\u308a\u5f97\u307e\u3059\uff0eR\u3067DTM\u3092\u69cb\u7bc9\u3059\u308b\u306b\u306f\u6587\u66f8\u96c6\u5408\u306e\u5168\u4f53\u3092RAM\u306b\u8aad\u307f\u8fbc\u3093\u3067\u5358\u4e00\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u51e6\u7406\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u305d\u3046\u3059\u308b\u3068\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u304c\u5bb9\u6613\u306b2\uff5e4\u500d\u307e\u3067\u5897\u5927\u3057\u3066\u3057\u307e\u3044\u307e\u3059\uff0etext2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3088\u308a\u512a\u308c\u305f\u65b9\u6cd5\u3067\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3067\u3053\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066\u3044\u307e\u3059\uff0e\n\n\u4f8b\uff1aIMDB\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30bb\u30f3\u30c1\u30e1\u30f3\u30c8\u5206\u6790\ntext2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306fmovie_review\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\u3053\u306e\u30c7\u30fc\u30bf\u306f5000\u4ef6\u306e\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u304b\u3089\u306a\u3063\u3066\u304a\u308a\uff0c\u305d\u308c\u305e\u308c\u306e\u30ec\u30d3\u30e5\u30fc\u304c\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u3082\u306e\u304b\u30cd\u30ac\u30c6\u30a3\u30d6\u306a\u3082\u306e\u304b\u5370\u304c\u4ed8\u3051\u3066\u3042\u308a\u307e\u3059\uff0e\nlibrary(text2vec)\ndata(\"movie_review\")\nset.seed(42L)\n\n\u6587\u66f8\u3092\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u8868\u73fe\u3059\u308b\u306b\u306f\uff0c\u307e\u305a\u30bf\u30fc\u30e0\u306b\u30bf\u30fc\u30e0ID\u3092\u5bfe\u5fdc\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\u5358\u8a9e\u3067\u306f\u306a\u304f\u30bf\u30fc\u30e0\u3068\u547c\u3076\u306e\u306f\uff0c\u30bf\u30fc\u30e0\u306f1\u8a9e\u3068\u306f\u9650\u3089\u305a\u4efb\u610f\u306en\u30b0\u30e9\u30e0\u3067\u3088\u3044\u304b\u3089\u3067\u3059\uff0e\u305d\u308c\u304b\u3089\u6587\u66f8\u96c6\u5408\u3092\u758e\u884c\u5217\u3067\u8868\u73fe\u3057\uff0c\u5404\u884c\u304c1\u3064\u306e\u6587\u66f8\u306b\uff0c\u5404\u5217\u304c1\u3064\u306e\u30bf\u30fc\u30e0\u306b\u5bfe\u5fdc\u3059\u308b\u3082\u306e\u3068\u3057\u307e\u3059\uff0e\u3053\u308c\u306b\u306f\u3084\u308a\u65b9\u304c2\u901a\u308a\u3042\u308a\u307e\u3059\uff0e\u8a9e\u5f59\u305d\u306e\u3082\u306e\u3092\u4f7f\u3046\u304b\uff0c\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3092\u4f7f\u3046\u304b\u3067\u3059\uff0e\n\n\u8a9e\u5f59\u306b\u57fa\u3065\u304f\u30d9\u30af\u30c8\u30eb\u5316\n\u307e\u305a\u8a9e\u5f59\u306b\u57fa\u3065\u304fDTM\u306e\u65b9\u3092\u4f5c\u308a\u307e\u3057\u3087\u3046\uff0e\u5168\u3066\u306e\u6587\u66f8\u304b\u3089\u30bf\u30fc\u30e0\u3092\u91cd\u8907\u306a\u304f\u96c6\u3081\uff0cvocabulary()\u95a2\u6570\u3092\u4f7f\u3063\u3066\u5404\u30bf\u30fc\u30e0\u306b\u4e00\u610f\u306eID\u3092\u5272\u308a\u5f53\u3066\u307e\u3059\uff0e\u8a9e\u5f59\u306e\u4f5c\u6210\u306b\u306f\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\uff0e\nit <- itoken(movie_review$review, \n             preprocess_function = tolower, \n             tokenizer = word_tokenizer, \n             ids = movie_review$id)\n\nsw <- c(\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\")\nvocab <- create_vocabulary(it, stopwords = sw)\n\n\u3042\u308b\u3044\u306f\uff0c\u30c7\u30fc\u30bf\u304cRAM\u306b\u53ce\u307e\u308b\u306e\u3067\u3042\u308c\u3070\uff0c\u30c8\u30fc\u30af\u30f3\u306e\u30ea\u30b9\u30c8\u3092\u4e00\u5ea6\u4f5c\u3063\u3066\u304a\u3044\u3066\u305d\u308c\u3092\u4ee5\u964d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u4f7f\u3044\u307e\u308f\u3059\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e\n# \u30ea\u30b9\u30c8\u306e\u5404\u8981\u7d20\u304c\u6587\u66f8\u3092\u8868\u3059\ntokens <- movie_review$review %>% \n  tolower() %>% \n  word_tokenizer()\nit <- itoken(tokens, ids = movie_review$id)\nvocab <- create_vocabulary(it, stopwords = sw)\n\n\u8a9e\u5f59\u304c\u3067\u304d\u305f\u306e\u3067\uff0c\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff08\u4ee3\u308f\u308a\u306bcreate_corpus()\u3068get_dtm()\u3092\u4f7f\u3046\u3053\u3068\u3082\u3067\u304d\u308b\u3067\u3057\u3087\u3046\uff09\uff0e\nit <- itoken(tokens, ids = movie_review$id)\n# Or\n# it <- itoken(movie_review$review, tolower, word_tokenizer, ids = movie_review$id)\nvectorizer <- vocab_vectorizer(vocab)\ndtm <- create_dtm(it, vectorizer)\n\nDTM\u3092\u4f5c\u3063\u305f\u306e\u3067\uff0c\u884c\u5217\u306e\u6b21\u5143\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff0e\nstr(dtm)\n\n## Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n##   ..@ i       : int [1:706047] 4999 4999 4999 4999 4999 4999 4998 4998 4998 49..\n##   ..@ p       : int [1:42642] 0 1 2 3 4 5 6 7 8 9 ...\n##   ..@ Dim     : int [1:2] 5000 42641\n##   ..@ Dimnames:List of 2\n##   .. ..$ : chr [1:5000] \"5814_8\" \"2381_9\" \"7759_3\" \"3630_4\" ...\n##   .. ..$ : chr [1:42641] \"decent.the\" \"nudity.i\" \"nowadays.i\" \"fantastic.it\" ...\n##   ..@ x       : num [1:706047] 1 1 1 1 1 1 1 1 1 1 ...\n##   ..@ factors : list()\n\nidentical(rownames(dtm), movie_review$id)\n\n## [1] TRUE\n\n\u898b\u3066\u306e\u901a\u308aDTM\u306e\u884c\u6570\u306f5000\u3067\u3042\u308a\uff0c\u6587\u66f8\u306e\u6570\u306b\u7b49\u3057\u3044\u3067\u3059\uff0e\u307e\u305f\u5217\u6570\u306f5000\u3067\uff0c\u30e6\u30cb\u30fc\u30af\u306a\u30bf\u30fc\u30e0\u306e\u6570\u306b\u7b49\u3057\u3044\u3067\u3059\uff0e\n\u3055\u3066\u6700\u521d\u306e\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6e96\u5099\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\u3053\u3053\u3067\u306fglmnet\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u3044\uff0cL1\u7f70\u5247\u4ed8\u304d\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u307e\u3059\uff0e\nlibrary(glmnet)\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 # lasso \u7f70\u5247\n                 alpha = 1,\n                 # \u7cbe\u5ea6\u306fROC\u66f2\u7dda\u4e0b\u306e\u9762\u7a4d\u3067\u8a55\u4fa1\n                 type.measure = \"auc\",\n                 # 5\u91cd\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n                 nfolds = 5,\n                 # \u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u7cbe\u5ea6\u306f\u4e0b\u304c\u308b\u304c\u8a13\u7df4\u304c\u901f\u304f\u306a\u308b\n                 thresh = 1e-3,\n                 # \u3053\u3063\u3061\u3082\u53cd\u5fa9\u56de\u6570\u3092\u6e1b\u3089\u3059\u3068\u8a13\u7df4\u304c\u901f\u304f\u306a\u308b\n                 maxit = 1e3)\nplot(fit)\n\n\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n\n## [1] \"max AUC = 0.9186\"\n\n\u3046\u307e\u304fDTM\u306b\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\n\n\u4f59\u5206\u306a\u8a9e\u5f59\u3092\u53d6\u308a\u9664\u304f\n\u3057\u304b\u3057\uff0c\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u6642\u9593\u304c\u304b\u306a\u308a\u9577\u304b\u3063\u305f\u3053\u3068\u306b\u6c17\u4ed8\u304d\u307e\u3059\uff0e\u4f59\u5206\u306a\u8a9e\u5f59\u3092\u53d6\u308a\u9664\u3051\u3070\uff0c\u7cbe\u5ea6\u3092\u5927\u304d\u304f\u5411\u4e0a\u3055\u305b\u3064\u3064\u8a13\u7df4\u6642\u9593\u3092\u77ed\u304f\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\uff0e\n\u305f\u3068\u3048\u3070\uff0c\"a\"\u3084\"the\"\u3084\"in\"\u3068\u3044\u3063\u305f\u5358\u8a9e\u306f\u307b\u3068\u3093\u3069\u5168\u3066\u306e\u6587\u66f8\u3067\u898b\u3064\u304b\u308a\u307e\u3059\u304c\uff0c\u3053\u308c\u3089\u306e\u5358\u8a9e\u306f\u3042\u307e\u308a\u6709\u7528\u306a\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u3066\u306f\u3044\u307e\u305b\u3093\uff0e\u666e\u901a\uff0c\u3053\u308c\u3089\u306e\u5358\u8a9e\u306f\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\uff08stop words\uff09\u3068\u547c\u3070\u308c\u3066\u3044\u307e\u3059\uff0e\u4ed6\u65b9\uff0c\u30b3\u30fc\u30d1\u30b9\u306b\u306f\u307b\u3093\u306e\u6570\u500b\u306e\u6587\u66f8\u306b\u3057\u304b\u51fa\u73fe\u3057\u306a\u3044\u3088\u3046\u306a\u975e\u5e38\u306b\u7a00\u306a\u30bf\u30fc\u30e0\u3082\u542b\u307e\u308c\u3066\u3044\u307e\u3059\uff0e\u3053\u308c\u3089\u306e\u30bf\u30fc\u30e0\u3082\uff0c\u5341\u5206\u306a\u6570\u306e\u30c7\u30fc\u30bf\u304c\u306a\u3044\u305f\u3081\u306b\u6709\u7528\u6027\u304c\u4f4e\u3044\u3082\u306e\u3067\u3059\uff0e\u3053\u3053\u3067\u306f\u975e\u5e38\u306b\u591a\u304f\u51fa\u73fe\u3059\u308b\u30bf\u30fc\u30e0\u3068\u975e\u5e38\u306b\u7a00\u306a\u30bf\u30fc\u30e0\u306e\u4e21\u65b9\u3092\u53d6\u308a\u9664\u304f\u3053\u3068\u306b\u3057\u307e\u3059\uff0e\npruned_vocab <- prune_vocabulary(vocab, term_count_min = 10,\n doc_proportion_max = 0.5, doc_proportion_min = 0.001)\nit <- itoken(tokens, ids = movie_review$id)\nvectorizer <- vocab_vectorizer(pruned_vocab)\ndtm <- create_dtm(it, vectorizer)\ndim(dtm)\n\n## [1] 5000 7656\n\n\u65b0\u3057\u3044DTM\u306f\u3082\u3068\u306eDTM\u3088\u308a\u5217\u6570\u304c\u305a\u3063\u3068\u5c11\u306a\u304f\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n\nTF-IDF\nDTM\u306b\u5bfe\u3057\u3066TF-IDF\u5909\u63db\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff08\u3057\uff0c\u666e\u901a\u306f\u5229\u7528\u3059\u308b\u3079\u304d\u3067\u3059\uff01\uff09\uff0eTF-IDF\u306f\u5c11\u6570\u306e\u6587\u66f8\u306b\u7279\u6709\u306a\u30bf\u30fc\u30e0\u306e\u91cd\u307f\u3092\u5927\u304d\u304f\u3057\uff0c\u591a\u304f\u306e\u6587\u66f8\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u30bf\u30fc\u30e0\u306e\u91cd\u307f\u306f\u6e1b\u3089\u3057\u307e\u3059\uff0e\ndtm <- dtm %>% transform_tfidf()\n\n## idf scaling matrix not provided, calculating it form input matrix\n\n\u3055\u3066\uff0c\u3082\u3046\u4e00\u5ea6\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\nt1 <- Sys.time()\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\nprint(difftime(Sys.time(), t1, units = 'sec'))\n\n## Time difference of 4.092234 secs\n\nplot(fit)\n\n\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n\n## [1] \"max AUC = 0.9199\"\n\n\u30e2\u30c7\u30eb\u3092\u3088\u308a\u901f\u304f\u8a13\u7df4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\uff0cAUC\u306e\u5024\u3082\u5927\u304d\u304f\u306a\u308a\u307e\u3057\u305f\uff0e\n\n\u3055\u3089\u306b\u30e2\u30c7\u30eb\u3092\u6539\u5584\u3067\u304d\u308b\u304b\uff1f\n\u5358\u8a9e\u3067\u306f\u306a\u304fn\u30b0\u30e9\u30e0\u3092\u4f7f\u3046\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6539\u5584\u3067\u304d\u308b\u304b\u8a66\u3057\u3066\u307f\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e3\u30b0\u30e9\u30e0\u307e\u3067\u4f7f\u3046\u3053\u3068\u306b\u3057\u307e\u3059\uff0e\nit <- itoken(tokens, ids = movie_review$id)\n\nvocab <- create_vocabulary(it, ngram = c(1L, 3L)) %>% \n  prune_vocabulary(term_count_min = 10, \n                   doc_proportion_max = 0.5, \n                   doc_proportion_min = 0.001)\n\nvectorizer <- vocab_vectorizer(vocab)\n\ndtm <- tokens %>% \n  itoken() %>% \n  create_dtm(vectorizer) %>% \n  transform_tfidf()\n\n## idf scaling matrix not provided, calculating it form input matrix\n\ndim(dtm)\n\n## [1]  5000 27226\n\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\n\nplot(fit)\n\n\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n\n## [1] \"max AUC = 0.9193\"\n\nn\u30b0\u30e9\u30e0\u3092\u4f7f\u3046\u3053\u3068\u3067\u30e2\u30c7\u30eb\u304c\u3082\u3046\u5c11\u3057\u6539\u5584\u3057\u307e\u3057\u305f\uff0e\u3055\u3089\u306a\u308b\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306f\u8aad\u8005\u306b\u59d4\u306d\u307e\u3059\uff0e\n\n\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\n\u3082\u3057\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\uff08\u3044\u308f\u3086\u308b\u30cf\u30c3\u30b7\u30f3\u30b0\u30c8\u30ea\u30c3\u30af\uff08hashing trick\uff09\uff09\u306b\u306a\u3058\u307f\u304c\u306a\u3051\u308c\u3070\uff0c\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u306e\u8a18\u4e8b\u3092\u307e\u305a\u8aad\u3093\u3067\u304b\u3089\uff0cYahoo!\u306e\u7814\u7a76\u30c1\u30fc\u30e0\u306b\u3088\u308b\u539f\u8ad6\u6587\u3092\u8aad\u3080\u306e\u304c\u3088\u3044\u3067\u3057\u3087\u3046\uff0e\u3053\u306e\u30c6\u30af\u30cb\u30c3\u30af\u306f\u9023\u60f3\u914d\u5217\u4e0a\u3067\u306e\u691c\u7d22\u3092\u884c\u3046\u5fc5\u8981\u304c\u306a\u3044\u305f\u3081\uff0c\u975e\u5e38\u306b\u9ad8\u901f\u3067\u3059\uff0e\u3082\u3046\u4e00\u3064\u306e\u5229\u70b9\u3068\u3057\u3066\uff0c\u4efb\u610f\u306e\u500b\u6570\u306e\u7d20\u6027\u3092\u305a\u3063\u3068\u30b3\u30f3\u30d1\u30af\u30c8\u306a\u7a7a\u9593\u3078\u3068\u5199\u50cf\u3067\u304d\u308b\u306e\u3067\uff0c\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u91cf\u3082\u975e\u5e38\u306b\u5c11\u306a\u304f\u306a\u308a\u307e\u3059\uff0e\u3053\u306e\u624b\u6cd5\u306fYahoo!\u306e\u304a\u304b\u3052\u3067\u666e\u53ca\u3057\uff0cVowpal Wabbit\u306b\u304a\u3044\u3066\u5e83\u7bc4\u306b\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\ntext2vec\u3067\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3092\u4f7f\u3046\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u307e\u3059\uff0e\nit <- itoken(tokens, ids = movie_review$id)\n\nvectorizer <- hash_vectorizer(hash_size = 2 ^ 16, ngram = c(1L, 3L))\ndtm <- create_dtm(it, vectorizer) %>% \n  transform_tfidf()\n\n## idf scaling matrix not provided, calculating it form input matrix\n\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\n\nplot(fit)\n\n\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n\n## [1] \"max AUC = 0.9027\"\n\n\u898b\u3066\u306e\u901a\u308aAUC\u306f\u5c11\u3057\u60aa\u5316\u3057\u3066\u3044\u307e\u3059\u304c\uff0cDTM\u306e\u69cb\u7bc9\u306b\u304b\u304b\u308b\u6642\u9593\u306f\u304b\u306a\u308a\u77ed\u304f\u306a\u308a\u307e\u3057\u305f\uff0e\u3053\u308c\u306f\u5de8\u5927\u306a\u6587\u66f8\u96c6\u5408\u306b\u5bfe\u3057\u3066\u306f\u5927\u304d\u306a\u30e1\u30ea\u30c3\u30c8\u306b\u306a\u308a\u5f97\u307e\u3059\uff0e\n\n\n\n\n\n\u8a33\u8005\u306e\u74b0\u5883\u3092\u793a\u3057\u3066\u304a\u304f\uff0e\u00a0\u21a9\ndevtools::session_info()\n\n## Session info -------------------------------------------------------------------\n\n##  setting  value                       \n##  version  R version 3.3.1 (2016-06-21)\n##  system   x86_64, mingw32             \n##  ui       RTerm                       \n##  language (EN)                        \n##  collate  Japanese_Japan.932          \n##  tz       Asia/Tokyo                  \n##  date     2016-09-03\n\n## Packages -----------------------------------------------------------------------\n\n##  package       * version date       source        \n##  chron           2.3-47  2015-06-24 CRAN (R 3.3.1)\n##  codetools       0.2-14  2015-07-15 CRAN (R 3.3.1)\n##  data.table      1.9.6   2015-09-19 CRAN (R 3.3.1)\n##  devtools        1.12.0  2016-06-24 CRAN (R 3.3.1)\n##  digest          0.6.10  2016-08-02 CRAN (R 3.3.1)\n##  evaluate        0.9     2016-04-29 CRAN (R 3.3.1)\n##  foreach       * 1.4.3   2015-10-13 CRAN (R 3.2.2)\n##  formatR         1.4     2016-05-09 CRAN (R 3.3.1)\n##  glmnet        * 2.0-5   2016-03-17 CRAN (R 3.3.1)\n##  htmltools       0.3.5   2016-03-21 CRAN (R 3.3.1)\n##  iterators       1.0.8   2015-10-13 CRAN (R 3.2.2)\n##  knitr           1.14    2016-08-13 CRAN (R 3.3.1)\n##  lattice         0.20-33 2015-07-14 CRAN (R 3.3.1)\n##  magrittr        1.5     2014-11-22 CRAN (R 3.3.1)\n##  Matrix        * 1.2-6   2016-05-02 CRAN (R 3.3.1)\n##  memoise         1.0.0   2016-01-29 CRAN (R 3.3.1)\n##  Rcpp            0.12.6  2016-07-19 CRAN (R 3.3.1)\n##  RcppParallel    4.3.20  2016-08-16 CRAN (R 3.3.1)\n##  RevoUtils       10.0.1  2016-08-24 local         \n##  RevoUtilsMath * 8.0.3   2016-04-13 local         \n##  rmarkdown       1.0     2016-07-08 CRAN (R 3.3.1)\n##  stringi         1.1.1   2016-05-27 CRAN (R 3.3.0)\n##  stringr         1.1.0   2016-08-19 CRAN (R 3.3.1)\n##  text2vec      * 0.3.0   2016-03-31 CRAN (R 3.3.1)\n##  withr           1.0.2   2016-06-20 CRAN (R 3.3.1)\n##  yaml            2.1.13  2014-06-12 CRAN (R 3.3.1)\n\n\n\nbag-of-words\u3082n-gram\u3082\u65e5\u672c\u8a9e\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u3067\u306f\u9805\u76ee\u304c\u306a\u3044\u305f\u3081\u82f1\u8a9e\u7248\u3078\u306e\u30ea\u30f3\u30af\u306e\u307e\u307e\u3068\u3057\u305f\uff0e\u4ee5\u4e0b\u3067\u3082\u65e5\u672c\u8a9e\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u306e\u9805\u76ee\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u306f\u305d\u3061\u3089\u3078\u306e\u30ea\u30f3\u30af\uff0c\u306a\u3044\u5834\u5408\u306b\u306f\u82f1\u8a9e\u7248\u3078\u306e\u30ea\u30f3\u30af\u3068\u3057\u3066\u3044\u308b\uff0e\u00a0\u21a9\n\n\n\n\n\u3053\u306e\u6587\u66f8\u306f Dmitriy Selivanov \u306b\u3088\u308bR\u30d1\u30c3\u30b1\u30fc\u30b8[`text2vec`](https://cran.r-project.org/web/packages/text2vec/) (version 0.3.0) \u306e\u30d3\u30cd\u30c3\u30c8 \"[Analyzing Texts with the text2vec Package](https://cran.r-project.org/web/packages/text2vec/)\" \u306e\u65e5\u672c\u8a9e\u8a33\u3067\u3059\uff0e\n\u305f\u3060\u3057\u6587\u4e2d\u306e\u6ce8\u306f\u5168\u3066\u8a33\u8005\u306b\u3088\u308b\u3082\u306e\u3067\u3059[^ss]\uff0e\n\nLicense: [MIT](https://opensource.org/licenses/MIT)\n\n**\u95a2\u9023\u6587\u66f8**\n\n+ text2vec vignette: text2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790\n+ [text2vec vignette: GloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f](http://qiita.com/nakamichi/items/69142b6d19f307ec8d9d)\n+ [text2vec vignette: \u767a\u5c55\u7684\u306a\u8a71\u984c](http://qiita.com/nakamichi/items/9805cd3831f674d81dcd)\n\n----\n\n<!-- # Features -->\n# \u7279\u5fb4\n<!-- **text2vec** provides an efficient framework with a concise API for text analysis and natural language processing (NLP) in R.  -->\n**text2vec** \u306fR\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u5206\u6790\u3068\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u305f\u3081\u306e\uff0c\u7c21\u6f54\u306aAPI\u3092\u5099\u3048\u305f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\n\n<!-- This package is efficient because it is carefully written in C++, which also means that text2vec is memory friendly. Some parts, such as training GloVe word embeddings are fully parallelized using the excellent [RcppParallel](http://rcppcore.github.io/RcppParallel/) package. This means that the word embeddings are computed in parallel on OS X, Linux, Windows, and Solaris (x86) without any additional tuning or tricks. Finally, a streaming API means that  users do not have to load all the data into RAM. -->\n\u3053\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306fC++\u306b\u3088\u3063\u3066\u6ce8\u610f\u6df1\u304f\u66f8\u304b\u308c\u3066\u3044\u308b\u305f\u3081\u52b9\u7387\u7684\u3067\uff0c\u30e1\u30e2\u30ea\u306b\u3082\u512a\u3057\u3044\u3067\u3059\uff0e\u7b87\u6240\u306b\u3088\u3063\u3066\u306f\u512a\u308c\u305f[RcppParallel](http://rcppcore.github.io/RcppParallel/)\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u5b8c\u5168\u306b\u4e26\u5217\u5316\u3055\u308c\u3066\u304a\u308a\uff0cGloVe\u306b\u3088\u308b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u8a13\u7df4\u306f\u305d\u306e\u4e00\u4f8b\u3067\u3059\uff0e\u3064\u307e\u308a\uff0cOS X\u3067\u3082Linux\u3067\u3082Windows\u3067\u3082Solaris\uff08x86\uff09\u3067\u3082\uff0c\u4f59\u8a08\u306a\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u3057\u305f\u308a\u6280\u5de7\u3092\u51dd\u3089\u3059\u3053\u3068\u306a\u3057\u306b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u304c\u4e26\u5217\u306b\u8a08\u7b97\u3055\u308c\u307e\u3059\uff0e\u307e\u305f\uff0c\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0API\u304c\u3042\u308b\u306e\u3067\u30e6\u30fc\u30b6\u306f\u5168\u3066\u306e\u30c7\u30fc\u30bf\u3092RAM\u306b\u8aad\u307f\u8fbc\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u305b\u3093\uff0e\n\n<!-- This vignette explains how to use text2vec to vectorize text on arbitrary n-grams using either a vocabulary or feature hashing. See the `glove` vignette for an explanation of how to use state-of-the art [GloVe](http://www-nlp.stanford.edu/projects/glove/) word embeddings with this package.  -->\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u3067\u306ftext2vec\u3092\u4f7f\u3044\uff0c\u4efb\u610f\u306en\u30b0\u30e9\u30e0\u306e\u8a9e\u5f59\u304b\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3092\u7528\u3044\u3066\u30c6\u30ad\u30b9\u30c8\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\uff0e\u6700\u5148\u7aef\u306e[GloVe](http://www-nlp.stanford.edu/projects/glove/)\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3092\u3053\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u4f7f\u3046\u65b9\u6cd5\u306e\u89e3\u8aac\u306b\u3064\u3044\u3066\u306f`glove`\u30d3\u30cd\u30c3\u30c8\u3092\u898b\u3066\u304f\u3060\u3055\u3044\uff0e\n\n<!-- # Text vectorization -->\n# \u30c6\u30ad\u30b9\u30c8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n\n<!-- Most text mining and NLP modeling use [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) or [bag-of-n-grams](https://en.wikipedia.org/wiki/N-gram) methods. Despite their simplicity, these models usually demonstrate good performance on text categorization and classification tasks. But, in contrast to their theoretical simplicity and practical efficiency, building bag-of-words models involves technical challenges. This is especially the case in R because of its copy-on-modify semantics.  -->\n\u30c6\u30ad\u30b9\u30c8\u30de\u30a4\u30cb\u30f3\u30b0\u3084\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5927\u62b5\u306e\u30e2\u30c7\u30ea\u30f3\u30b0\u3067\u306f[bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)\u304b[bag-of-n-grams](https://en.wikipedia.org/wiki/N-gram)\u3068\u3044\u3046\u65b9\u6cd5\u304c\u4f7f\u308f\u308c\u307e\u3059[^bow]\uff0e\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306f\u5358\u7d14\u3067\u3042\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\uff0c\u30c6\u30ad\u30b9\u30c8\u306e\u5206\u985e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u901a\u5e38\u306f\u9ad8\u3044\u6027\u80fd\u3092\u767a\u63ee\u3057\u307e\u3059\uff0e\u3067\u3059\u304c\uff0c\u305d\u306e\u7406\u8ad6\u7684\u306a\u5358\u7d14\u3055\u3068\u5b9f\u7528\u7684\u306a\u52b9\u7387\u6027\u3068\u306f\u5bfe\u7167\u7684\u306b\uff0cbag-of-words\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u306f\u6280\u8853\u7684\u306b\u306f\u96e3\u554f\u3067\u3059\uff0eR\u306e\u5834\u5408\u306f\u30b3\u30d4\u30fc\u6642\u4fee\u6b63\uff08copy-on-modify\uff09\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u304c\u3042\u308b\u306e\u3067\u7279\u306b\u96e3\u3057\u3044\u3067\u3059\uff0e\n\n<!-- ## Text analysis pipeline  -->\n## \u30c6\u30ad\u30b9\u30c8\u5206\u6790\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\n\n<!-- Let's briefly review some details of a typical text analysis pipeline :   -->\n\u5178\u578b\u7684\u306a\u30c6\u30ad\u30b9\u30c8\u5206\u6790\u3068\u5206\u6790\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u7d30\u90e8\u306b\u3064\u3044\u3066\uff0c\u7c21\u5358\u306b\u632f\u308a\u8fd4\u3063\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n\n<!-- 1. The reseacher usually begins by constructing a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (DTM) from input documents. In other words, the first step is to  *vectorize text* by creating a map from words or n-grams to a [vector space](https://en.wikipedia.org/wiki/Vector_space_model). -->\n1. \u7814\u7a76\u8005\u306f\u901a\u5e38\uff0c\u5165\u529b\u6587\u66f8\u304b\u3089[\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\uff08document-term matrix\uff0cDTM\uff09](https://en.wikipedia.org/wiki/Document-term_matrix)\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u307e\u3059\uff0e\u8a00\u3044\u304b\u3048\u308b\u3068\uff0c\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u5358\u8a9e\u3084n\u30b0\u30e9\u30e0\u3092[\u30d9\u30af\u30c8\u30eb\u7a7a\u9593](https://en.wikipedia.org/wiki/Vector_space_model)\u306b\u5199\u50cf\u3057\u3066*\u6587\u66f8\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b*\u3053\u3068\u3067\u3059\uff0e\n\n<!-- 2. The researcher fits a model to that DTM. These models might include text classification, topic modeling, or word embedding. Fitting the model will include tuning and validating the model. -->\n2. \u7814\u7a76\u8005\u306f\u30e2\u30c7\u30eb\u3092DTM\u306b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3057\u307e\u3059\uff0e\u30e2\u30c7\u30eb\u306f\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u5668\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3057\uff0c\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb\u3084\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\u30e2\u30c7\u30eb\u306e\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306b\u306f\u30e2\u30c7\u30eb\u306e\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3084\u691c\u8a3c\u3082\u542b\u307f\u307e\u3059\uff0e\n\n<!-- 3. Finally the researcher applies the model to new data. -->\n3. \u6700\u5f8c\u306b\uff0c\u7814\u7a76\u8005\u306f\u30e2\u30c7\u30eb\u3092\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3057\u307e\u3059\uff0e\n\n<!-- In this vignette we will primarily discuss first stage. Texts themselves can take up a lot of memory, but vectorized texts usually do not, because they are stored as sparse matrices. Because of R's copy-on-modify semantics, it is not easy to iteratively grow a DTM. So constructing a DTM, even for a small collections of documents, can be a serious bottleneck for analysts and researchers. It involves reading the whole collection of text documents into RAM and processing it as single vector, which can easily increase memory use by a factor of 2 to 4. The text2vec package solves this problem by providing a better way of constructing a document-term matrix. -->\n\u3053\u306e\u30d3\u30cd\u30c3\u30c8\u3067\u306f\u4e3b\u306b1\u756a\u76ee\u306e\u6bb5\u968e\u306b\u3064\u3044\u3066\u8b70\u8ad6\u3057\u307e\u3059\uff0e\u30c6\u30ad\u30b9\u30c8\u305d\u308c\u81ea\u4f53\u306f\u5927\u91cf\u306e\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u306f\u758e\u884c\u5217\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u308b\u306e\u3067\uff0c\u666e\u901a\u306f\u3042\u307e\u308a\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3057\u307e\u305b\u3093\uff0eR\u306b\u306f\u30b3\u30d4\u30fc\u6642\u4fee\u6b63\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u304c\u3042\u308b\u306e\u3067\uff0c\u53cd\u5fa9\u7684\u306bDTM\u3092\u5927\u304d\u304f\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u5bb9\u6613\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\u305d\u308c\u3086\u3048\uff0c\u5c0f\u3055\u306a\u6587\u66f8\u96c6\u5408\u3067\u3042\u3063\u3066\u3082DTM\u306e\u69cb\u7bc9\u304c\u5206\u6790\u8005\u3084\u7814\u7a76\u8005\u306b\u3068\u3063\u3066\u306e\u6df1\u523b\u306a\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3068\u306a\u308a\u5f97\u307e\u3059\uff0eR\u3067DTM\u3092\u69cb\u7bc9\u3059\u308b\u306b\u306f\u6587\u66f8\u96c6\u5408\u306e\u5168\u4f53\u3092RAM\u306b\u8aad\u307f\u8fbc\u3093\u3067\u5358\u4e00\u306e\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u51e6\u7406\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u305d\u3046\u3059\u308b\u3068\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u304c\u5bb9\u6613\u306b2\uff5e4\u500d\u307e\u3067\u5897\u5927\u3057\u3066\u3057\u307e\u3044\u307e\u3059\uff0etext2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3088\u308a\u512a\u308c\u305f\u65b9\u6cd5\u3067\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3067\u3053\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066\u3044\u307e\u3059\uff0e\n\n<!-- ## Example: Sentiment analysis on IMDB movie review dataset -->\n## \u4f8b\uff1aIMDB\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30bb\u30f3\u30c1\u30e1\u30f3\u30c8\u5206\u6790\n\n<!-- This package provides the `movie_review` dataset. It consists of 5000 movie reviews, each of which is marked as positive or negative. -->\ntext2vec\u30d1\u30c3\u30b1\u30fc\u30b8\u306f`movie_review`\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff0e\u3053\u306e\u30c7\u30fc\u30bf\u306f5000\u4ef6\u306e\u6620\u753b\u30ec\u30d3\u30e5\u30fc\u304b\u3089\u306a\u3063\u3066\u304a\u308a\uff0c\u305d\u308c\u305e\u308c\u306e\u30ec\u30d3\u30e5\u30fc\u304c\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u3082\u306e\u304b\u30cd\u30ac\u30c6\u30a3\u30d6\u306a\u3082\u306e\u304b\u5370\u304c\u4ed8\u3051\u3066\u3042\u308a\u307e\u3059\uff0e\n\n\n```r\nlibrary(text2vec)\ndata(\"movie_review\")\nset.seed(42L)\n```\n\n<!-- To represent documents in vector space, we first have to create `term -> term_id` mappings. We call them *terms* instead of *words*, because they can be arbitrary n-grams, not just single words. We represent a set of documents as a sparse matrix, where each row corresponds to a document and each column corresponds to a term. This can be done in 2 ways: using the vocabulary itself or by [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing). -->\n\u6587\u66f8\u3092\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u8868\u73fe\u3059\u308b\u306b\u306f\uff0c\u307e\u305a\u30bf\u30fc\u30e0\u306b\u30bf\u30fc\u30e0ID\u3092\u5bfe\u5fdc\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e*\u5358\u8a9e*\u3067\u306f\u306a\u304f*\u30bf\u30fc\u30e0*\u3068\u547c\u3076\u306e\u306f\uff0c\u30bf\u30fc\u30e0\u306f1\u8a9e\u3068\u306f\u9650\u3089\u305a\u4efb\u610f\u306en\u30b0\u30e9\u30e0\u3067\u3088\u3044\u304b\u3089\u3067\u3059\uff0e\u305d\u308c\u304b\u3089\u6587\u66f8\u96c6\u5408\u3092\u758e\u884c\u5217\u3067\u8868\u73fe\u3057\uff0c\u5404\u884c\u304c1\u3064\u306e\u6587\u66f8\u306b\uff0c\u5404\u5217\u304c1\u3064\u306e\u30bf\u30fc\u30e0\u306b\u5bfe\u5fdc\u3059\u308b\u3082\u306e\u3068\u3057\u307e\u3059\uff0e\u3053\u308c\u306b\u306f\u3084\u308a\u65b9\u304c2\u901a\u308a\u3042\u308a\u307e\u3059\uff0e\u8a9e\u5f59\u305d\u306e\u3082\u306e\u3092\u4f7f\u3046\u304b\uff0c[\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0](https://ja.wikipedia.org/wiki/Feature_Hashing)\u3092\u4f7f\u3046\u304b\u3067\u3059\uff0e\n\n<!-- ### Vocabulary based vectorization -->\n### \u8a9e\u5f59\u306b\u57fa\u3065\u304f\u30d9\u30af\u30c8\u30eb\u5316\n\n<!-- Let's first create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique_id. Using the `vocabulary()` function. We use an iterator to create the vocabulary. -->\n\u307e\u305a\u8a9e\u5f59\u306b\u57fa\u3065\u304fDTM\u306e\u65b9\u3092\u4f5c\u308a\u307e\u3057\u3087\u3046\uff0e\u5168\u3066\u306e\u6587\u66f8\u304b\u3089\u30bf\u30fc\u30e0\u3092\u91cd\u8907\u306a\u304f\u96c6\u3081\uff0c`vocabulary()`\u95a2\u6570\u3092\u4f7f\u3063\u3066\u5404\u30bf\u30fc\u30e0\u306b\u4e00\u610f\u306eID\u3092\u5272\u308a\u5f53\u3066\u307e\u3059\uff0e\u8a9e\u5f59\u306e\u4f5c\u6210\u306b\u306f\u30a4\u30c6\u30ec\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\uff0e\n\n\n```r\nit <- itoken(movie_review$review, \n             preprocess_function = tolower, \n             tokenizer = word_tokenizer, \n             ids = movie_review$id)\n\nsw <- c(\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\")\nvocab <- create_vocabulary(it, stopwords = sw)\n```\n\n<!-- Alternatively, if your data fits in RAM, you can once create list of tokens and the reuse it in further steps: -->\n\u3042\u308b\u3044\u306f\uff0c\u30c7\u30fc\u30bf\u304cRAM\u306b\u53ce\u307e\u308b\u306e\u3067\u3042\u308c\u3070\uff0c\u30c8\u30fc\u30af\u30f3\u306e\u30ea\u30b9\u30c8\u3092\u4e00\u5ea6\u4f5c\u3063\u3066\u304a\u3044\u3066\u305d\u308c\u3092\u4ee5\u964d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u4f7f\u3044\u307e\u308f\u3059\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e\n\n\n```r\n# \u30ea\u30b9\u30c8\u306e\u5404\u8981\u7d20\u304c\u6587\u66f8\u3092\u8868\u3059\ntokens <- movie_review$review %>% \n  tolower() %>% \n  word_tokenizer()\nit <- itoken(tokens, ids = movie_review$id)\nvocab <- create_vocabulary(it, stopwords = sw)\n```\n\n<!-- Now that we have a vocabulary, we can construct a document-term matrix. (We could instead use `create_corpus()` and `get_dtm()`). -->\n\u8a9e\u5f59\u304c\u3067\u304d\u305f\u306e\u3067\uff0c\u6587\u66f8\u30bf\u30fc\u30e0\u884c\u5217\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff08\u4ee3\u308f\u308a\u306b`create_corpus()`\u3068`get_dtm()`\u3092\u4f7f\u3046\u3053\u3068\u3082\u3067\u304d\u308b\u3067\u3057\u3087\u3046\uff09\uff0e\n\n\n\n```r\nit <- itoken(tokens, ids = movie_review$id)\n# Or\n# it <- itoken(movie_review$review, tolower, word_tokenizer, ids = movie_review$id)\nvectorizer <- vocab_vectorizer(vocab)\ndtm <- create_dtm(it, vectorizer)\n```\n\n<!-- Now we have a DTM and can check its dimensions. -->\nDTM\u3092\u4f5c\u3063\u305f\u306e\u3067\uff0c\u884c\u5217\u306e\u6b21\u5143\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\uff0e\n\n\n```r\nstr(dtm)\n```\n\n```\n## Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n##   ..@ i       : int [1:706047] 4999 4999 4999 4999 4999 4999 4998 4998 4998 49..\n##   ..@ p       : int [1:42642] 0 1 2 3 4 5 6 7 8 9 ...\n##   ..@ Dim     : int [1:2] 5000 42641\n##   ..@ Dimnames:List of 2\n##   .. ..$ : chr [1:5000] \"5814_8\" \"2381_9\" \"7759_3\" \"3630_4\" ...\n##   .. ..$ : chr [1:42641] \"decent.the\" \"nudity.i\" \"nowadays.i\" \"fantastic.it\" ...\n##   ..@ x       : num [1:706047] 1 1 1 1 1 1 1 1 1 1 ...\n##   ..@ factors : list()\n```\n\n```r\nidentical(rownames(dtm), movie_review$id)\n```\n\n```\n## [1] TRUE\n```\n\n<!-- As you can see, the DTM has 5000 rows, equal to the number of documents, and 42641 columns, equal to the number of unique terms. -->\n\u898b\u3066\u306e\u901a\u308aDTM\u306e\u884c\u6570\u306f5000\u3067\u3042\u308a\uff0c\u6587\u66f8\u306e\u6570\u306b\u7b49\u3057\u3044\u3067\u3059\uff0e\u307e\u305f\u5217\u6570\u306f5000\u3067\uff0c\u30e6\u30cb\u30fc\u30af\u306a\u30bf\u30fc\u30e0\u306e\u6570\u306b\u7b49\u3057\u3044\u3067\u3059\uff0e\n\n<!-- Now we are ready to fit our first model. Here we will use the `glmnet` package to fit a logistic regression model with an L1 penalty. -->\n\u3055\u3066\u6700\u521d\u306e\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6e96\u5099\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\u3053\u3053\u3067\u306f`glmnet`\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u3044\uff0cL1\u7f70\u5247\u4ed8\u304d\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u307e\u3059\uff0e\n\n\n```r\nlibrary(glmnet)\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 # lasso \u7f70\u5247\n                 alpha = 1,\n                 # \u7cbe\u5ea6\u306fROC\u66f2\u7dda\u4e0b\u306e\u9762\u7a4d\u3067\u8a55\u4fa1\n                 type.measure = \"auc\",\n                 # 5\u91cd\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\n                 nfolds = 5,\n                 # \u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u7cbe\u5ea6\u306f\u4e0b\u304c\u308b\u304c\u8a13\u7df4\u304c\u901f\u304f\u306a\u308b\n                 thresh = 1e-3,\n                 # \u3053\u3063\u3061\u3082\u53cd\u5fa9\u56de\u6570\u3092\u6e1b\u3089\u3059\u3068\u8a13\u7df4\u304c\u901f\u304f\u306a\u308b\n                 maxit = 1e3)\nplot(fit)\n```\n\n![fit_1-1.png](https://qiita-image-store.s3.amazonaws.com/0/58917/cd8224e8-4c70-e785-394b-2427f480858b.png)\n\n\n```r\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n```\n\n```\n## [1] \"max AUC = 0.9186\"\n```\n\n<!-- We have successfully fit a model to our DTM. -->\n\u3046\u307e\u304fDTM\u306b\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\n\n<!-- ### Pruning vocabulary -->\n### \u4f59\u5206\u306a\u8a9e\u5f59\u3092\u53d6\u308a\u9664\u304f\n\n<!-- We can note, however, that the training time for our model was quite high. We can reduce it and also significantly improve accuracy by pruning the vocabulary. -->\n\u3057\u304b\u3057\uff0c\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u6642\u9593\u304c\u304b\u306a\u308a\u9577\u304b\u3063\u305f\u3053\u3068\u306b\u6c17\u4ed8\u304d\u307e\u3059\uff0e\u4f59\u5206\u306a\u8a9e\u5f59\u3092\u53d6\u308a\u9664\u3051\u3070\uff0c\u7cbe\u5ea6\u3092\u5927\u304d\u304f\u5411\u4e0a\u3055\u305b\u3064\u3064\u8a13\u7df4\u6642\u9593\u3092\u77ed\u304f\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\uff0e\n\n<!-- For example we can find words \"a,\" \"the,\" and \"in\" in almost all documents, but they do not provide much useful information. Usually they called [stop words](https://en.wikipedia.org/wiki/Stop_words). On the other hand, the corpus also contains very uncommon terms, which are contained in only a few documents. These terms are also useless, because we don't have sufficient statistics for them. Here we will remove both very common and very unusual terms. -->\n\u305f\u3068\u3048\u3070\uff0c\"a\"\u3084\"the\"\u3084\"in\"\u3068\u3044\u3063\u305f\u5358\u8a9e\u306f\u307b\u3068\u3093\u3069\u5168\u3066\u306e\u6587\u66f8\u3067\u898b\u3064\u304b\u308a\u307e\u3059\u304c\uff0c\u3053\u308c\u3089\u306e\u5358\u8a9e\u306f\u3042\u307e\u308a\u6709\u7528\u306a\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u3066\u306f\u3044\u307e\u305b\u3093\uff0e\u666e\u901a\uff0c\u3053\u308c\u3089\u306e\u5358\u8a9e\u306f[\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\uff08stop words\uff09](https://en.wikipedia.org/wiki/Stop_words)\u3068\u547c\u3070\u308c\u3066\u3044\u307e\u3059\uff0e\u4ed6\u65b9\uff0c\u30b3\u30fc\u30d1\u30b9\u306b\u306f\u307b\u3093\u306e\u6570\u500b\u306e\u6587\u66f8\u306b\u3057\u304b\u51fa\u73fe\u3057\u306a\u3044\u3088\u3046\u306a\u975e\u5e38\u306b\u7a00\u306a\u30bf\u30fc\u30e0\u3082\u542b\u307e\u308c\u3066\u3044\u307e\u3059\uff0e\u3053\u308c\u3089\u306e\u30bf\u30fc\u30e0\u3082\uff0c\u5341\u5206\u306a\u6570\u306e\u30c7\u30fc\u30bf\u304c\u306a\u3044\u305f\u3081\u306b\u6709\u7528\u6027\u304c\u4f4e\u3044\u3082\u306e\u3067\u3059\uff0e\u3053\u3053\u3067\u306f\u975e\u5e38\u306b\u591a\u304f\u51fa\u73fe\u3059\u308b\u30bf\u30fc\u30e0\u3068\u975e\u5e38\u306b\u7a00\u306a\u30bf\u30fc\u30e0\u306e\u4e21\u65b9\u3092\u53d6\u308a\u9664\u304f\u3053\u3068\u306b\u3057\u307e\u3059\uff0e\n\n\n```r\npruned_vocab <- prune_vocabulary(vocab, term_count_min = 10,\n doc_proportion_max = 0.5, doc_proportion_min = 0.001)\nit <- itoken(tokens, ids = movie_review$id)\nvectorizer <- vocab_vectorizer(pruned_vocab)\ndtm <- create_dtm(it, vectorizer)\ndim(dtm)\n```\n\n```\n## [1] 5000 7656\n```\n\n<!-- Note that the new DTM has many fewer columns than the original DTM.  -->\n\u65b0\u3057\u3044DTM\u306f\u3082\u3068\u306eDTM\u3088\u308a\u5217\u6570\u304c\u305a\u3063\u3068\u5c11\u306a\u304f\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u76ee\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n\n<!-- ### TF-IDF -->\n### TF-IDF\n\n<!-- We can (and usually should!) also apply [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) transformation to our DTM** which will increase the weight of terms which are specific to a single document or handful of documents and decrease the weight for terms used in most documents: -->\nDTM\u306b\u5bfe\u3057\u3066[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\u5909\u63db\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff08\u3057\uff0c\u666e\u901a\u306f\u5229\u7528\u3059\u308b\u3079\u304d\u3067\u3059\uff01\uff09\uff0eTF-IDF\u306f\u5c11\u6570\u306e\u6587\u66f8\u306b\u7279\u6709\u306a\u30bf\u30fc\u30e0\u306e\u91cd\u307f\u3092\u5927\u304d\u304f\u3057\uff0c\u591a\u304f\u306e\u6587\u66f8\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u30bf\u30fc\u30e0\u306e\u91cd\u307f\u306f\u6e1b\u3089\u3057\u307e\u3059\uff0e\n\n\n```r\ndtm <- dtm %>% transform_tfidf()\n```\n\n```\n## idf scaling matrix not provided, calculating it form input matrix\n```\n\n<!-- Now, let's fit our model again: -->\n\u3055\u3066\uff0c\u3082\u3046\u4e00\u5ea6\u30e2\u30c7\u30eb\u3092\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n\n\n```r\nt1 <- Sys.time()\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\nprint(difftime(Sys.time(), t1, units = 'sec'))\n```\n\n```\n## Time difference of 4.092234 secs\n```\n\n```r\nplot(fit)\n```\n\n![fit_2-1.png](https://qiita-image-store.s3.amazonaws.com/0/58917/f5860152-643d-90ce-264a-a16e897ef93a.png)\n\n\n```r\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n```\n\n```\n## [1] \"max AUC = 0.9199\"\n```\n\n<!-- We trained this model faster with a larger AUC. -->\n\u30e2\u30c7\u30eb\u3092\u3088\u308a\u901f\u304f\u8a13\u7df4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\uff0cAUC\u306e\u5024\u3082\u5927\u304d\u304f\u306a\u308a\u307e\u3057\u305f\uff0e\n\n<!-- ### Can we improve the model? -->\n### \u3055\u3089\u306b\u30e2\u30c7\u30eb\u3092\u6539\u5584\u3067\u304d\u308b\u304b\uff1f\n\n<!-- We can try to improve our model by using n-grams instead of words. We will use up to 3-grams: -->\n\u5358\u8a9e\u3067\u306f\u306a\u304fn\u30b0\u30e9\u30e0\u3092\u4f7f\u3046\u3053\u3068\u3067\u30e2\u30c7\u30eb\u3092\u6539\u5584\u3067\u304d\u308b\u304b\u8a66\u3057\u3066\u307f\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff0e3\u30b0\u30e9\u30e0\u307e\u3067\u4f7f\u3046\u3053\u3068\u306b\u3057\u307e\u3059\uff0e\n\n\n```r\nit <- itoken(tokens, ids = movie_review$id)\n\nvocab <- create_vocabulary(it, ngram = c(1L, 3L)) %>% \n  prune_vocabulary(term_count_min = 10, \n                   doc_proportion_max = 0.5, \n                   doc_proportion_min = 0.001)\n\nvectorizer <- vocab_vectorizer(vocab)\n\ndtm <- tokens %>% \n  itoken() %>% \n  create_dtm(vectorizer) %>% \n  transform_tfidf()\n```\n\n```\n## idf scaling matrix not provided, calculating it form input matrix\n```\n\n```r\ndim(dtm)\n```\n\n```\n## [1]  5000 27226\n```\n\n```r\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\n\nplot(fit)\n```\n\n![ngram_dtm_1-1.png](https://qiita-image-store.s3.amazonaws.com/0/58917/24ec68b4-c1d5-7420-9970-531c3f9d5193.png)\n\n\n```r\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n```\n\n```\n## [1] \"max AUC = 0.9193\"\n```\n\n<!-- Using n-grams improved our model a little bit more. Further tuning is left up to the reader. -->\nn\u30b0\u30e9\u30e0\u3092\u4f7f\u3046\u3053\u3068\u3067\u30e2\u30c7\u30eb\u304c\u3082\u3046\u5c11\u3057\u6539\u5584\u3057\u307e\u3057\u305f\uff0e\u3055\u3089\u306a\u308b\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306f\u8aad\u8005\u306b\u59d4\u306d\u307e\u3059\uff0e\n\n<!-- ### Feature hashing -->\n### \u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\n\n<!-- If you are not familiar with feature hashing (the so-called \"hashing trick\") I recommend that you start with the [Wikipedia article](https://en.wikipedia.org/wiki/Feature_hashing), then read the [original paper](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf) by a Yahoo! research team. This techique is very fast because we don't have to perform a lookup over an associative array. Another benefit is that it leads to a very low memory footprint, since we can map an arbitrary number of features into much more compact space. This method was popularized by Yahoo! and widely used in [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/).  -->\n\u3082\u3057\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\uff08\u3044\u308f\u3086\u308b\u30cf\u30c3\u30b7\u30f3\u30b0\u30c8\u30ea\u30c3\u30af\uff08hashing trick\uff09\uff09\u306b\u306a\u3058\u307f\u304c\u306a\u3051\u308c\u3070\uff0c[\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u306e\u8a18\u4e8b](https://ja.wikipedia.org/wiki/Feature_Hashing)\u3092\u307e\u305a\u8aad\u3093\u3067\u304b\u3089\uff0cYahoo!\u306e\u7814\u7a76\u30c1\u30fc\u30e0\u306b\u3088\u308b[\u539f\u8ad6\u6587](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf)\u3092\u8aad\u3080\u306e\u304c\u3088\u3044\u3067\u3057\u3087\u3046\uff0e\u3053\u306e\u30c6\u30af\u30cb\u30c3\u30af\u306f\u9023\u60f3\u914d\u5217\u4e0a\u3067\u306e\u691c\u7d22\u3092\u884c\u3046\u5fc5\u8981\u304c\u306a\u3044\u305f\u3081\uff0c\u975e\u5e38\u306b\u9ad8\u901f\u3067\u3059\uff0e\u3082\u3046\u4e00\u3064\u306e\u5229\u70b9\u3068\u3057\u3066\uff0c\u4efb\u610f\u306e\u500b\u6570\u306e\u7d20\u6027\u3092\u305a\u3063\u3068\u30b3\u30f3\u30d1\u30af\u30c8\u306a\u7a7a\u9593\u3078\u3068\u5199\u50cf\u3067\u304d\u308b\u306e\u3067\uff0c\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u91cf\u3082\u975e\u5e38\u306b\u5c11\u306a\u304f\u306a\u308a\u307e\u3059\uff0e\u3053\u306e\u624b\u6cd5\u306fYahoo!\u306e\u304a\u304b\u3052\u3067\u666e\u53ca\u3057\uff0c[Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/)\u306b\u304a\u3044\u3066\u5e83\u7bc4\u306b\u5229\u7528\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\n\n<!-- Here is how to use feature hashing in text2vec. -->\ntext2vec\u3067\u7d20\u6027\u30cf\u30c3\u30b7\u30f3\u30b0\u3092\u4f7f\u3046\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u307e\u3059\uff0e\n\n\n```r\nit <- itoken(tokens, ids = movie_review$id)\n\nvectorizer <- hash_vectorizer(hash_size = 2 ^ 16, ngram = c(1L, 3L))\ndtm <- create_dtm(it, vectorizer) %>% \n  transform_tfidf()\n```\n\n```\n## idf scaling matrix not provided, calculating it form input matrix\n```\n\n```r\nfit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], \n                 family = 'binomial', \n                 alpha = 1,\n                 type.measure = \"auc\",\n                 nfolds = 5,\n                 thresh = 1e-3,\n                 maxit = 1e3)\n\nplot(fit)\n```\n\n\n![hash_dtm-1.png](https://qiita-image-store.s3.amazonaws.com/0/58917/0231b8cd-c22a-91ce-d9a5-d26978a5f192.png)\n\n\n```r\nprint(paste(\"max AUC =\", round(max(fit$cvm), 4)))\n```\n\n```\n## [1] \"max AUC = 0.9027\"\n```\n\n<!-- As you can see, we our AUC is a bit worse, but DTM construction time was considerably lower. On large collections of documents this can be a significant advantage. -->\n\u898b\u3066\u306e\u901a\u308aAUC\u306f\u5c11\u3057\u60aa\u5316\u3057\u3066\u3044\u307e\u3059\u304c\uff0cDTM\u306e\u69cb\u7bc9\u306b\u304b\u304b\u308b\u6642\u9593\u306f\u304b\u306a\u308a\u77ed\u304f\u306a\u308a\u307e\u3057\u305f\uff0e\u3053\u308c\u306f\u5de8\u5927\u306a\u6587\u66f8\u96c6\u5408\u306b\u5bfe\u3057\u3066\u306f\u5927\u304d\u306a\u30e1\u30ea\u30c3\u30c8\u306b\u306a\u308a\u5f97\u307e\u3059\uff0e\n\n\n----\n\n[^bow]: bag-of-words\u3082n-gram\u3082\u65e5\u672c\u8a9e\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u3067\u306f\u9805\u76ee\u304c\u306a\u3044\u305f\u3081\u82f1\u8a9e\u7248\u3078\u306e\u30ea\u30f3\u30af\u306e\u307e\u307e\u3068\u3057\u305f\uff0e\u4ee5\u4e0b\u3067\u3082\u65e5\u672c\u8a9e\u30a6\u30a3\u30ad\u30da\u30c7\u30a3\u30a2\u306e\u9805\u76ee\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u306f\u305d\u3061\u3089\u3078\u306e\u30ea\u30f3\u30af\uff0c\u306a\u3044\u5834\u5408\u306b\u306f\u82f1\u8a9e\u7248\u3078\u306e\u30ea\u30f3\u30af\u3068\u3057\u3066\u3044\u308b\uff0e\n\n[^ss]: \u8a33\u8005\u306e\u74b0\u5883\u3092\u793a\u3057\u3066\u304a\u304f\uff0e\n\n    \n    ```r\n    devtools::session_info()\n    ```\n    \n    ```\n    ## Session info -------------------------------------------------------------------\n    ```\n    \n    ```\n    ##  setting  value                       \n    ##  version  R version 3.3.1 (2016-06-21)\n    ##  system   x86_64, mingw32             \n    ##  ui       RTerm                       \n    ##  language (EN)                        \n    ##  collate  Japanese_Japan.932          \n    ##  tz       Asia/Tokyo                  \n    ##  date     2016-09-03\n    ```\n    \n    ```\n    ## Packages -----------------------------------------------------------------------\n    ```\n    \n    ```\n    ##  package       * version date       source        \n    ##  chron           2.3-47  2015-06-24 CRAN (R 3.3.1)\n    ##  codetools       0.2-14  2015-07-15 CRAN (R 3.3.1)\n    ##  data.table      1.9.6   2015-09-19 CRAN (R 3.3.1)\n    ##  devtools        1.12.0  2016-06-24 CRAN (R 3.3.1)\n    ##  digest          0.6.10  2016-08-02 CRAN (R 3.3.1)\n    ##  evaluate        0.9     2016-04-29 CRAN (R 3.3.1)\n    ##  foreach       * 1.4.3   2015-10-13 CRAN (R 3.2.2)\n    ##  formatR         1.4     2016-05-09 CRAN (R 3.3.1)\n    ##  glmnet        * 2.0-5   2016-03-17 CRAN (R 3.3.1)\n    ##  htmltools       0.3.5   2016-03-21 CRAN (R 3.3.1)\n    ##  iterators       1.0.8   2015-10-13 CRAN (R 3.2.2)\n    ##  knitr           1.14    2016-08-13 CRAN (R 3.3.1)\n    ##  lattice         0.20-33 2015-07-14 CRAN (R 3.3.1)\n    ##  magrittr        1.5     2014-11-22 CRAN (R 3.3.1)\n    ##  Matrix        * 1.2-6   2016-05-02 CRAN (R 3.3.1)\n    ##  memoise         1.0.0   2016-01-29 CRAN (R 3.3.1)\n    ##  Rcpp            0.12.6  2016-07-19 CRAN (R 3.3.1)\n    ##  RcppParallel    4.3.20  2016-08-16 CRAN (R 3.3.1)\n    ##  RevoUtils       10.0.1  2016-08-24 local         \n    ##  RevoUtilsMath * 8.0.3   2016-04-13 local         \n    ##  rmarkdown       1.0     2016-07-08 CRAN (R 3.3.1)\n    ##  stringi         1.1.1   2016-05-27 CRAN (R 3.3.0)\n    ##  stringr         1.1.0   2016-08-19 CRAN (R 3.3.1)\n    ##  text2vec      * 0.3.0   2016-03-31 CRAN (R 3.3.1)\n    ##  withr           1.0.2   2016-06-20 CRAN (R 3.3.1)\n    ##  yaml            2.1.13  2014-06-12 CRAN (R 3.3.1)\n    ```\n"}