{"context": "\n\n\u306f\u3058\u3081\u306b\n\u3000Python3 \u3068\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3042\u308b Scrapy \u3092\u4f7f\u3063\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u7df4\u7fd2\u74b0\u5883\u3092\u4f5c\u308b\u3088(\u30fb\u2200\u30fb)\uff01\n\n\u74b0\u5883\n\nLinux x86-64 (Ubuntu)\nPython3\nDocker1\nBash\n\n\n\u4eee\u60f3\u74b0\u5883\u306e\u69cb\u7bc9\n\u3000Docker \u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u7528\u306e\u4eee\u60f3\u74b0\u5883\u3092\u69cb\u7bc9\u3002\u30a4\u30e1\u30fc\u30b8\u306f Ubuntu \u306e\u6700\u65b0\u7248\u3092\u30d7\u30eb\u3002\u8d70\u3089\u305b\u308b\u30b3\u30f3\u30c6\u30ca\u306b\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30dd\u30fc\u30c8\u3092\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0\u3002\u305d\u308c\u304b\u3089\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea my-scrapy \u3092\u30b3\u30f3\u30c6\u30ca\u5185\u306e /root/my-scrapy \u306b\u30de\u30a6\u30f3\u30c8\u3002\n\ndocker\n$ docker pull ubuntu:latest\n$ docker run -itp 127.0.0.1:8000:80 -v /home/narupo/src/my-scrapy:/root/my-scrapy ubuntu:latest /bin/bash\n\n\n\n\n\u958b\u767a\u30c4\u30fc\u30eb\u306e\u6e96\u5099\n\u3000\u30b3\u30f3\u30c6\u30ca\u5185\u306b\u958b\u767a\u30c4\u30fc\u30eb\u3092\u6e96\u5099\u3059\u308b\u3002\n\napt-get\nroot# apt-get update\nroot# apt-get install vim less git python3 python3-pip\n\n\n\npip \u3067 scrapy \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u3000pip \u3092\u66f4\u65b0\u5f8c\u3001scrapy \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3002\n\npip\nroot# pip3 install --upgrade pip\nroot# pip install scrapy\n\n\n\u3000\u3057\u304b\u3057 scrapy \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306b\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u3002\u30a8\u30e9\u30fc\u3092\u89e3\u6c7a\u3059\u308b\u3002\n\nerror\n...\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n\n\u3000\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u3067\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u8db3\u308a\u3066\u3044\u306a\u3044\u69d8\u306a\u306e\u3067\u5165\u308c\u308b\u3002\u4f9d\u5b58\u95a2\u4fc2\u3068\u6226\u3046\u65e5\u3005(\u00b4\uff65\u03c9\uff65)(\uff65\u03c9\uff65\uff40)\uff88\uff70\u3002\n\napt-get\nroot# apt-get install libxslt1-dev libssl-dev\n\n\n\u3000scrapy \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u52d5\u4f5c\u78ba\u8a8d\u3002\nroot# pip install scrapy\nroot# scrapy --version\n\n\nApache2 \u3068 MySQL \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u3000\u7df4\u7fd2\u7528\u306e\u30b5\u30fc\u30d0\u30fc\u3068\u30c7\u30fc\u30bf\u4fdd\u5b58\u7528\u306eDB\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u8d77\u52d5\u3055\u305b\u3066\u304a\u304f\u3002\n\napt-get\nroot# apt-get install apache2 mysql-server\nroot# service apache2 start\nroot# service mysql start\n\n\n\n\u3000Apache2 \u3092\u8d77\u52d5\u3057\u305f\u306e\u3067\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30d6\u30e9\u30a6\u30b6\u4e0a\u304b\u3089 localhost:8000 \u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3002\n\n\u30c6\u30b9\u30c8\u7528\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u306e\u30c7\u30d7\u30ed\u30a4\n\u3000\u4f5c\u308b\u306e\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067 github \u3067 \"dummy website\" \u3067\u691c\u7d22\u3002\u4eca\u56de\u306f https://github.com/crysxd/Herbboy-Dummy-Website \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3002\n\ndeploy\nroot# cd /var/www/html/\nroot# git clone https://github.com/crysxd/Herbboy-Dummy-Website\nroot# mv Herbboy-Dummy-Website/ test\nroot# chown -R www-data:www-data ./test\n\n\n\n\u3000\u30c7\u30d7\u30ed\u30a4\u304c\u5b8c\u4e86\u3057\u305f\u306e\u3067\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067 http://localhost:8000/test \u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3002\n\n\u958b\u767a\n\u3000\u6e96\u5099\u304c\u5b8c\u4e86\u3057\u305f\u306e\u3067\u958b\u767a\u306b\u5165\u308b\u3002\n\u3000Scrapy \u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb http://doc.scrapy.org/en/latest/intro/tutorial.html \u306b\u5f93\u3046\u3002\n\nshell\n\u3000\u3044\u304f\u3064\u304b\u98db\u3070\u3057\u3066\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u8a66\u3059\u3002scrapy \u3067\u306f\u30b7\u30a7\u30eb\u3067\u5bfe\u8a71\u5f0f\u306b\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u64cd\u4f5c\u304c\u51fa\u6765\u308b\u3002\u4fbf\u5229\u3067\u3059\u306d\u3002\n\nscrapy-shell\nroot# scrapy shell \"http://localhost:80/test\"\n\n\n\u3000\u30b7\u30a7\u30eb\u306b\u30ed\u30b0\u30a4\u30f3\u5f8c\u3001\u5f15\u6570\u306eURL\u306e\u7d50\u679c\u304c\u8868\u793a\u3055\u308c\u308b\u3002\n\nscrapy-shell\n...\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f09aea67048>\n[s]   item       {}\n[s]   request    <GET http://localhost:80/test>\n[s]   response   <301 http://localhost:80/test>\n[s]   settings   <scrapy.settings.Settings object at 0x7f09ad4555f8>\n[s]   spider     <DefaultSpider 'default' at 0x7f09ad267710>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n...\n\n\n\u3000response   <301 http://localhost:80/test> \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u8ee2\u9001\u3055\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002Docker \u306e\u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5f71\u97ff\uff1f https://scholar.live/question/scrapy-301-redirect-in-shell \u3092\u53c2\u8003\u306b fetch \u5f8c\u3001\u7121\u4e8b\u306b\u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\u3067\u304d\u305f\u3002\n\nscrapy-shell\n# \u518d\u8aad\u307f\u8fbc\u307f\n>>> fetch(response.headers['Location'].decode())\n\n# \u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\n>>> response.xpath('//title')\n[<Selector xpath='//title' data='<title>Carousel Template for Bootstrap</'>]\n\n\n\n\u3000shell \u3067\u306f Python3 \u306e\u69cb\u6587\u3082\u6709\u52b9\u3002\n\nscrapy-shell\n>>> for title in response.xpath('//title'):         \n...     txts = title.xpath('text()').extract()\n...     print(txts)\n... \n['Carousel Template for Bootstrap']\n\n\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\n\u3000\u8857\u3092\u99c6\u3051\u307e\u308f\u308b\u306e\u306f\u30b9\u30d1\u30a4\u30c0\u30fc\u30de\u30f3\u3060\u304c\u3001\u30cd\u30c3\u30c8\u3092\u99c6\u3051\u307e\u308f\u308b\u306e\u306f\uff65\uff65\uff65\uff65\uff65\uff65\u305d\u3046\u3001 Spider \u3060(\uff40\u30fb\u03c9\u30fb\u00b4)\uff1c\uff94\uff76\uff8f\uff7c\uff72\uff9c\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u958b\u59cb\u3057\u3066\u3001\u7de8\u96c6\u306e\u305f\u3081\u306b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u6240\u6709\u8005\u3068\u30b0\u30eb\u30fc\u30d7\u3092\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306b\u5408\u308f\u305b\u308b\u3002\n\nproject\nroot# cd ~/my-scrapy\nroot# scrapy startproject tutorial\nroot# chown -R 1000:1000 ./tutorial\n\n\n\u3000\u3053\u3053\u304b\u3089\u306f\u30de\u30a6\u30f3\u30c8\u3057\u305f\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u53ef\u80fd\u3002\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u6cbf\u3063\u3066 tutorial/tutorial \u76f4\u4e0b\u306e items.py \u3092\u7de8\u96c6\u3002\u306a\u3093\u3068\u306a\u304f Django \u306e\u30e2\u30c7\u30eb\u306b\u4f3c\u3066\u307e\u3059\u306d\u3002\n\nitems.py\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass MyItem(scrapy.Item):\n    title = scrapy.Field()\n\n\n\n\u3000spiders/ \u4ee5\u4e0b\u306b Spider \u306e my_spider.py \u3092\u4f5c\u6210\u3002parse \u306b shell \u3067\u64cd\u4f5c\u3057\u305f response \u304c\u6e21\u3055\u308c\u308b\u3002MyItem \u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u3066\u30d1\u30fc\u30b9\u6642\u306e\u30c7\u30fc\u30bf\u4fdd\u5b58\u306b\u5229\u7528\u3059\u308b\u3002\n\nmy_spider.py\nimport scrapy\nfrom tutorial.items import MyItem\n\nclass MySpider(scrapy.Spider):\n    name = 'my'\n    allowed_domains = ['localhost:80']\n    start_urls = [\n        'http://localhost:80/test',\n    ]\n\n    def parse(self, response):\n        for title in response.xpath('//title'):\n            item = MyItem()\n            item['title'] = title.xpath('text()').extract()\n            yield item\n\n\n\n\u3000\u4f5c\u6210\u3057\u305f Spider \u3092\u30af\u30ed\u30fc\u30eb\u3055\u305b\u308b\u3068\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3092\u958b\u59cb\u3002\n\nscrapy-crawl\nroot# scrapy crawl my\n\n\n\u3000\u51fa\u529b\u3092\u4fdd\u5b58\u3057\u305f\u3044\u5834\u5408\u306f -o \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u51fa\u529b\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308b\u3002\n\nscrapy-crawl\nroot# scrapy crawl my -o items.json\n\n\n\u3000\u51fa\u529b\u7d50\u679c\u304c\u30ed\u30b0\u306b\u7d1b\u308c\u3066\u308f\u304b\u308a\u3065\u3089\u3044\u304c\u3001\u7121\u4e8b\u306b shell \u306e\u6642\u3068\u540c\u69d8\u306b\u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\u3067\u304d\u305f\u3002\u5c0e\u5165\u306f\u624b\u9593\u3060\u304c\u3001\u3084\u306f\u308a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3060\u3051\u3042\u3063\u3066\u30b5\u30c3\u30af\u30ea\u3068\u66f8\u3051\u307e\u3059\u3002\u5168\u4f53\u7684\u306b Django \u3068\u4f3c\u3066\u307e\u3059\u306d\u3002\n# \u306f\u3058\u3081\u306b\n\n\u3000Python3 \u3068\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3042\u308b Scrapy \u3092\u4f7f\u3063\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u7df4\u7fd2\u74b0\u5883\u3092\u4f5c\u308b\u3088(\u30fb\u2200\u30fb)\uff01\n\n# \u74b0\u5883\n\n* Linux x86-64 (Ubuntu)\n* Python3\n* Docker1\n* Bash\n\n# \u4eee\u60f3\u74b0\u5883\u306e\u69cb\u7bc9\n\n\u3000Docker \u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u7528\u306e\u4eee\u60f3\u74b0\u5883\u3092\u69cb\u7bc9\u3002\u30a4\u30e1\u30fc\u30b8\u306f Ubuntu \u306e\u6700\u65b0\u7248\u3092\u30d7\u30eb\u3002\u8d70\u3089\u305b\u308b\u30b3\u30f3\u30c6\u30ca\u306b\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30dd\u30fc\u30c8\u3092\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0\u3002\u305d\u308c\u304b\u3089\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea my-scrapy \u3092\u30b3\u30f3\u30c6\u30ca\u5185\u306e /root/my-scrapy \u306b\u30de\u30a6\u30f3\u30c8\u3002\n\n```bash:docker\n$ docker pull ubuntu:latest\n$ docker run -itp 127.0.0.1:8000:80 -v /home/narupo/src/my-scrapy:/root/my-scrapy ubuntu:latest /bin/bash\n\n```\n\n## \u958b\u767a\u30c4\u30fc\u30eb\u306e\u6e96\u5099\n\n\u3000\u30b3\u30f3\u30c6\u30ca\u5185\u306b\u958b\u767a\u30c4\u30fc\u30eb\u3092\u6e96\u5099\u3059\u308b\u3002\n\n```bash:apt-get\nroot# apt-get update\nroot# apt-get install vim less git python3 python3-pip\n```\n\n### pip \u3067 scrapy \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u3000pip \u3092\u66f4\u65b0\u5f8c\u3001scrapy \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3002\n\n```bash:pip\nroot# pip3 install --upgrade pip\nroot# pip install scrapy\n```\n\n\u3000\u3057\u304b\u3057 scrapy \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306b\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u3002\u30a8\u30e9\u30fc\u3092\u89e3\u6c7a\u3059\u308b\u3002\n\n```bash:error\n...\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n```\n\n\u3000\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6642\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u3067\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u8db3\u308a\u3066\u3044\u306a\u3044\u69d8\u306a\u306e\u3067\u5165\u308c\u308b\u3002\u4f9d\u5b58\u95a2\u4fc2\u3068\u6226\u3046\u65e5\u3005(*\u00b4\uff65\u03c9\uff65)(\uff65\u03c9\uff65\uff40*)\uff88\uff70\u3002\n\n```bash:apt-get\nroot# apt-get install libxslt1-dev libssl-dev\n```\n\n\u3000scrapy \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u52d5\u4f5c\u78ba\u8a8d\u3002\n\n```\nroot# pip install scrapy\nroot# scrapy --version\n```\n\n### Apache2 \u3068 MySQL \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u3000\u7df4\u7fd2\u7528\u306e\u30b5\u30fc\u30d0\u30fc\u3068\u30c7\u30fc\u30bf\u4fdd\u5b58\u7528\u306eDB\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u8d77\u52d5\u3055\u305b\u3066\u304a\u304f\u3002\n\n```bash:apt-get\nroot# apt-get install apache2 mysql-server\nroot# service apache2 start\nroot# service mysql start\n\n````\n\n\u3000Apache2 \u3092\u8d77\u52d5\u3057\u305f\u306e\u3067\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30d6\u30e9\u30a6\u30b6\u4e0a\u304b\u3089 localhost:8000 \u3078\u30a2\u30af\u30bb\u30b9\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3002\n\n## \u30c6\u30b9\u30c8\u7528\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u306e\u30c7\u30d7\u30ed\u30a4\n\n\u3000\u4f5c\u308b\u306e\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067 github \u3067 \"dummy website\" \u3067\u691c\u7d22\u3002\u4eca\u56de\u306f https://github.com/crysxd/Herbboy-Dummy-Website \u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u3002\n\n```bash:deploy\nroot# cd /var/www/html/\nroot# git clone https://github.com/crysxd/Herbboy-Dummy-Website\nroot# mv Herbboy-Dummy-Website/ test\nroot# chown -R www-data:www-data ./test\n\n```\n\n\u3000\u30c7\u30d7\u30ed\u30a4\u304c\u5b8c\u4e86\u3057\u305f\u306e\u3067\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30d6\u30e9\u30a6\u30b6\u4e0a\u3067 http://localhost:8000/test \u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\u3002\n\n# \u958b\u767a\n\n\u3000\u6e96\u5099\u304c\u5b8c\u4e86\u3057\u305f\u306e\u3067\u958b\u767a\u306b\u5165\u308b\u3002\n\u3000Scrapy \u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb http://doc.scrapy.org/en/latest/intro/tutorial.html \u306b\u5f93\u3046\u3002\n\n## shell\n\n\u3000\u3044\u304f\u3064\u304b\u98db\u3070\u3057\u3066\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u8a66\u3059\u3002scrapy \u3067\u306f\u30b7\u30a7\u30eb\u3067\u5bfe\u8a71\u5f0f\u306b\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u64cd\u4f5c\u304c\u51fa\u6765\u308b\u3002\u4fbf\u5229\u3067\u3059\u306d\u3002\n\n```bash:scrapy-shell\nroot# scrapy shell \"http://localhost:80/test\"\n```\n\n\u3000\u30b7\u30a7\u30eb\u306b\u30ed\u30b0\u30a4\u30f3\u5f8c\u3001\u5f15\u6570\u306eURL\u306e\u7d50\u679c\u304c\u8868\u793a\u3055\u308c\u308b\u3002\n\n```bash:scrapy-shell\n...\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f09aea67048>\n[s]   item       {}\n[s]   request    <GET http://localhost:80/test>\n[s]   response   <301 http://localhost:80/test>\n[s]   settings   <scrapy.settings.Settings object at 0x7f09ad4555f8>\n[s]   spider     <DefaultSpider 'default' at 0x7f09ad267710>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n...\n```\n\n\u3000`response   <301 http://localhost:80/test>` \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u8ee2\u9001\u3055\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002Docker \u306e\u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5f71\u97ff\uff1f https://scholar.live/question/scrapy-301-redirect-in-shell \u3092\u53c2\u8003\u306b fetch \u5f8c\u3001\u7121\u4e8b\u306b\u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\u3067\u304d\u305f\u3002\n\n```bash:scrapy-shell\n# \u518d\u8aad\u307f\u8fbc\u307f\n>>> fetch(response.headers['Location'].decode())\n\n# \u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\n>>> response.xpath('//title')\n[<Selector xpath='//title' data='<title>Carousel Template for Bootstrap</'>]\n\n```\n\n\u3000shell \u3067\u306f Python3 \u306e\u69cb\u6587\u3082\u6709\u52b9\u3002\n\n```bash:scrapy-shell\n>>> for title in response.xpath('//title'):         \n...     txts = title.xpath('text()').extract()\n...     print(txts)\n... \n['Carousel Template for Bootstrap']\n```\n\n## \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\n\n\u3000\u8857\u3092\u99c6\u3051\u307e\u308f\u308b\u306e\u306f\u30b9\u30d1\u30a4\u30c0\u30fc\u30de\u30f3\u3060\u304c\u3001\u30cd\u30c3\u30c8\u3092\u99c6\u3051\u307e\u308f\u308b\u306e\u306f\uff65\uff65\uff65\uff65\uff65\uff65\u305d\u3046\u3001 Spider \u3060(\uff40\u30fb\u03c9\u30fb\u00b4)\uff1c\uff94\uff76\uff8f\uff7c\uff72\uff9c\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u958b\u59cb\u3057\u3066\u3001\u7de8\u96c6\u306e\u305f\u3081\u306b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u6240\u6709\u8005\u3068\u30b0\u30eb\u30fc\u30d7\u3092\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306b\u5408\u308f\u305b\u308b\u3002\n\n```bash:project\nroot# cd ~/my-scrapy\nroot# scrapy startproject tutorial\nroot# chown -R 1000:1000 ./tutorial\n```\n\n\u3000\u3053\u3053\u304b\u3089\u306f\u30de\u30a6\u30f3\u30c8\u3057\u305f\u30db\u30b9\u30c8\u30de\u30b7\u30f3\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u53ef\u80fd\u3002\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u6cbf\u3063\u3066 tutorial/tutorial \u76f4\u4e0b\u306e items.py \u3092\u7de8\u96c6\u3002\u306a\u3093\u3068\u306a\u304f Django \u306e\u30e2\u30c7\u30eb\u306b\u4f3c\u3066\u307e\u3059\u306d\u3002\n\n```py3:items.py\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass MyItem(scrapy.Item):\n\ttitle = scrapy.Field()\n\n```\n\n\u3000spiders/ \u4ee5\u4e0b\u306b Spider \u306e my_spider.py \u3092\u4f5c\u6210\u3002`parse` \u306b shell \u3067\u64cd\u4f5c\u3057\u305f response \u304c\u6e21\u3055\u308c\u308b\u3002MyItem \u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u3066\u30d1\u30fc\u30b9\u6642\u306e\u30c7\u30fc\u30bf\u4fdd\u5b58\u306b\u5229\u7528\u3059\u308b\u3002\n\n```py3:my_spider.py\nimport scrapy\nfrom tutorial.items import MyItem\n\nclass MySpider(scrapy.Spider):\n    name = 'my'\n    allowed_domains = ['localhost:80']\n    start_urls = [\n        'http://localhost:80/test',\n    ]\n\n    def parse(self, response):\n        for title in response.xpath('//title'):\n            item = MyItem()\n            item['title'] = title.xpath('text()').extract()\n            yield item\n\n```\n\n\u3000\u4f5c\u6210\u3057\u305f Spider \u3092\u30af\u30ed\u30fc\u30eb\u3055\u305b\u308b\u3068\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3092\u958b\u59cb\u3002\n\n```bash:scrapy-crawl\nroot# scrapy crawl my\n```\n\n\u3000\u51fa\u529b\u3092\u4fdd\u5b58\u3057\u305f\u3044\u5834\u5408\u306f `-o` \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u51fa\u529b\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308b\u3002\n\n```bash:scrapy-crawl\nroot# scrapy crawl my -o items.json\n```\n\n\u3000\u51fa\u529b\u7d50\u679c\u304c\u30ed\u30b0\u306b\u7d1b\u308c\u3066\u308f\u304b\u308a\u3065\u3089\u3044\u304c\u3001\u7121\u4e8b\u306b shell \u306e\u6642\u3068\u540c\u69d8\u306b\u30bf\u30a4\u30c8\u30eb\u3092\u53d6\u5f97\u3067\u304d\u305f\u3002\u5c0e\u5165\u306f\u624b\u9593\u3060\u304c\u3001\u3084\u306f\u308a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u3060\u3051\u3042\u3063\u3066\u30b5\u30c3\u30af\u30ea\u3068\u66f8\u3051\u307e\u3059\u3002\u5168\u4f53\u7684\u306b Django \u3068\u4f3c\u3066\u307e\u3059\u306d\u3002\n\n\n", "tags": ["python3", "Scrapy1", "Linux", "Ubuntu", "\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0"]}