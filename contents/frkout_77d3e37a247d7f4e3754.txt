{"context": " More than 1 year has passed since last update.OSX\u3067\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u306eApacheHadoop2.7.0\u3092\u52d5\u304b\u3059 \u3067\u69cb\u7bc9\u3057\u305fHadoop\u4e0a\u306b\u3001YARN+Spark\u74b0\u5883\u3092\u69cb\u7bc9\u3057\u305f\u30e1\u30e2\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 & \u5c55\u958b\n\u30d3\u30eb\u30c9\u3059\u308b\u306e\u3060\u308b\u3044\u306e\u3067 https://spark.apache.org/downloads.html \u304b\u3089 \n1.4.0 / Pre-build for Hadoop 2.6 and later \u3092\u9078\u629e\u3057\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n/usr/local2/\u4ee5\u4e0b\u306b\u5c55\u958b\n$ tar zxvf ~/Download/spark-1.4.0-bin-hadoop2.6.tgz -C /usr/local2/\n\n\n\u8a2d\u5b9a\n/usr/local2/spark-1.4.0-bin-hadoop2.6/conf/spark-env.sh\u3092\u4f5c\u6210\n\u3053\u306ehadoop\u306b\u5408\u308f\u305b\u3066\u3042\u308b\u3002\n\n/usr/local2/spark-1.4.0-bin-hadoop2.6/conf/spark-env.sh\nHADOOP_CONF_DIR=/usr/local2/hadoop-2.7.0/etc/hadoop\n\n\n\n\u52d5\u4f5c\u78ba\u8a8d\nyarn-client\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b\n$ /usr/local2/spark-1.4.0-bin-hadoop2.6/bin/spark-shell --master yarn-client\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n15/06/27 18:49:49 INFO SecurityManager: Changing view acls to: kodai_abe\n15/06/27 18:49:49 INFO SecurityManager: Changing modify acls to: kodai_abe\n15/06/27 18:49:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kodai_abe); users with modify permissions: Set(kodai_abe)\n15/06/27 18:49:49 INFO HttpServer: Starting HTTP Server\n15/06/27 18:49:49 INFO Utils: Successfully started service 'HTTP class server' on port 60019.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.4.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_05)\nType in expressions to have them evaluated.\nType :help for more information.\n15/06/27 18:49:51 INFO SparkContext: Running Spark version 1.4.0\n15/06/27 18:49:51 WARN SparkConf: \nSPARK_CLASSPATH was detected (set to ':/usr/local2/hadoop-2.7.0/lib').\nThis is deprecated in Spark 1.0+.\n\n## \u4ee5\u4e0b\u7701\u7565\n## \u30ba\u30e9\u30ba\u30e9\u6d41\u308c\u3066\u304f\u308b\u4e2d\u306b\u4e0b\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n## ....\n\n15/06/27 18:50:04 INFO SparkILoop: Created spark context..\nSpark context available as sc.\n\n## ....\n15/06/27 18:50:07 INFO SparkILoop: Created sql context (with Hive support)..\nSQL context available as sqlContext.\n\n## scala\u306erepl\u304c\u51fa\u3066\u304f\u308b\nscala>\n\n## \u9069\u5f53\u306a\u30d5\u30a1\u30a4\u30eb\u3067\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\nscala> val textfile = sc.textFile(\"/user/kodai_abe/hadoop-env.sh\")\nscala> val counts = textfile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\nscala> counts.saveAsTextFile(\"/user/kodai_abe/spark_wordcount_out\")\n\n\nHDFS\u306eWeb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u304b\u3089\u78ba\u8a8d\n\n[OSX\u3067\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u306eApacheHadoop2.7.0\u3092\u52d5\u304b\u3059](http://qiita.com/frkout/items/f965e166d0af529b5ce8) \u3067\u69cb\u7bc9\u3057\u305fHadoop\u4e0a\u306b\u3001YARN+Spark\u74b0\u5883\u3092\u69cb\u7bc9\u3057\u305f\u30e1\u30e2\n\n## \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 & \u5c55\u958b\n\u30d3\u30eb\u30c9\u3059\u308b\u306e\u3060\u308b\u3044\u306e\u3067 https://spark.apache.org/downloads.html \u304b\u3089 \n1.4.0 / Pre-build for Hadoop 2.6 and later \u3092\u9078\u629e\u3057\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n![](http://i.gyazo.com/b5f58a0177a3d66700c58faa7eb0f587.png)\n\n\n`/usr/local2/`\u4ee5\u4e0b\u306b\u5c55\u958b\n\n```bash\n$ tar zxvf ~/Download/spark-1.4.0-bin-hadoop2.6.tgz -C /usr/local2/\n```\n\n## \u8a2d\u5b9a\n\n`/usr/local2/spark-1.4.0-bin-hadoop2.6/conf/spark-env.sh`\u3092\u4f5c\u6210\n[\u3053\u306ehadoop](http://qiita.com/frkout/items/f965e166d0af529b5ce8)\u306b\u5408\u308f\u305b\u3066\u3042\u308b\u3002\n\n```/usr/local2/spark-1.4.0-bin-hadoop2.6/conf/spark-env.sh\nHADOOP_CONF_DIR=/usr/local2/hadoop-2.7.0/etc/hadoop\n```\n\n## \u52d5\u4f5c\u78ba\u8a8d\nyarn-client\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b\n\n```\n$ /usr/local2/spark-1.4.0-bin-hadoop2.6/bin/spark-shell --master yarn-client\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n15/06/27 18:49:49 INFO SecurityManager: Changing view acls to: kodai_abe\n15/06/27 18:49:49 INFO SecurityManager: Changing modify acls to: kodai_abe\n15/06/27 18:49:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(kodai_abe); users with modify permissions: Set(kodai_abe)\n15/06/27 18:49:49 INFO HttpServer: Starting HTTP Server\n15/06/27 18:49:49 INFO Utils: Successfully started service 'HTTP class server' on port 60019.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.4.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_05)\nType in expressions to have them evaluated.\nType :help for more information.\n15/06/27 18:49:51 INFO SparkContext: Running Spark version 1.4.0\n15/06/27 18:49:51 WARN SparkConf: \nSPARK_CLASSPATH was detected (set to ':/usr/local2/hadoop-2.7.0/lib').\nThis is deprecated in Spark 1.0+.\n\n## \u4ee5\u4e0b\u7701\u7565\n## \u30ba\u30e9\u30ba\u30e9\u6d41\u308c\u3066\u304f\u308b\u4e2d\u306b\u4e0b\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n## ....\n\n15/06/27 18:50:04 INFO SparkILoop: Created spark context..\nSpark context available as sc.\n\n## ....\n15/06/27 18:50:07 INFO SparkILoop: Created sql context (with Hive support)..\nSQL context available as sqlContext.\n\n## scala\u306erepl\u304c\u51fa\u3066\u304f\u308b\nscala>\n```\n```\n## \u9069\u5f53\u306a\u30d5\u30a1\u30a4\u30eb\u3067\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\nscala> val textfile = sc.textFile(\"/user/kodai_abe/hadoop-env.sh\")\nscala> val counts = textfile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\nscala> counts.saveAsTextFile(\"/user/kodai_abe/spark_wordcount_out\")\n```\n\n## HDFS\u306eWeb\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u304b\u3089\u78ba\u8a8d\n\n![](http://i.gyazo.com/567adb6e5ad409447085a85d13ca2ac1.png)\n\n", "tags": ["hadoop", "YARN", "Spark", "Scala"]}