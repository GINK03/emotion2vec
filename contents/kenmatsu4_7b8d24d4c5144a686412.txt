{"tags": ["Python2.7", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "Chainer", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0"], "context": " More than 1 year has passed since last update.\u4eca\u8a71\u984c\u306eDeep Learning(\u6df1\u5c64\u5b66\u7fd2)\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3001Chainer\u306b\u624b\u66f8\u304d\u6587\u5b57\u306e\u5224\u5225\u3092\u884c\u3046\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3061\u3089\u3092\u4f7f\u3063\u3066\u5185\u5bb9\u3092\u5c11\u3057\u89e3\u8aac\u3059\u308b\u8a18\u4e8b\u3092\u66f8\u3044\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n(\u672c\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u306e\u5168\u6587\u3092GitHub\u306b\u30a2\u30c3\u30d7\u3057\u307e\u3057\u305f\u3002[PC\u63a8\u5968])\n\u3068\u306b\u304b\u304f\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u3059\u3054\u304f\u7c21\u5358\u304b\u3064\u3001Python\u304c\u66f8\u3051\u308c\u3070\u3059\u3050\u306b\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u3066\u304a\u3059\u3059\u3081\u3067\u3059\uff01\nPython\u306b\u9589\u3058\u3066\u30b3\u30fc\u30c9\u304c\u66f8\u3051\u308b\u306e\u3082\u3059\u3054\u304f\u3044\u3044\u3067\u3059\u3088\u306d\u3002\n\u3053\u3093\u306a\u611f\u3058\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u8a66\u3057\u3066\u307f\u308b\u3001\u3068\u3044\u3046\u8a18\u4e8b\u3067\u3059\u3002\n\n\u4e3b\u8981\u306a\u60c5\u5831\u306f\u3053\u3061\u3089\u306b\u3042\u308a\u307e\u3059\u3002\nChainer\u306e\u30e1\u30a4\u30f3\u30b5\u30a4\u30c8\nChainer\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\nChainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3068\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\n\n1. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u307e\u305a\u306f\u4f55\u306f\u3068\u3082\u3042\u308c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u3059\u3002Chainer\u306eGitHub\u306b\u8a18\u8f09\u306e\"Requirements\" ( https://github.com/pfnet/chainer#requirements )\u3092\u53c2\u8003\u306b\u5fc5\u8981\u306a\u30bd\u30d5\u30c8\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u4e0a\u3067\npip install chainer\n\n\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u3053\u308c\u3060\u3051\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u3061\u3083\u3044\u307e\u3059\u3002\u8d85\u7c21\u5358\uff01Caffe\u3092Mac\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3088\u3046\u3068\u3057\u305f\u6642\u306f\u304b\u306a\u308a\u82e6\u6226\u3057\u307e\u3057\u305f\u304c\u3001\u5618\u306e\u3088\u3046\u3067\u3059  \n\u3082\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u8a70\u307e\u3063\u305f\u3089cvl-robot\u3055\u3093\u306e\u300cDeepLearning\u30e9\u30a4\u30d6\u30e9\u30ea\u306eChainer\u304c\u3059\u3054\u3044\u3001\u3089\u3057\u3044\u300d\u3068\u3044\u3046\u8a18\u4e8b\u304c\u8a73\u3057\u304f\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u7b49\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u3064\u3044\u3066\u8a18\u8f09\u3057\u3066\u304f\u308c\u3066\u3044\u3066\u4fbf\u5229\u3067\u3059\u3002\n\n2.\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u306e\u5165\u624b\nGitHub\u306e\u4e0b\u8a18\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u304a\u306a\u3058\u307fMNIST\u306e\u624b\u66f8\u304d\u6587\u5b57\u3092\u5224\u5225\u3059\u308b\u3001\u3068\u3044\u3046\u30b5\u30f3\u30d7\u30eb\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u3053\u308c\u3092\u984c\u6750\u3068\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u308c\u3092Chainer\u306e\u9806\u4f1d\u64ad\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067Classification\u3057\u3066\u307f\u308b\u3001\u3068\u3044\u3046\u8a66\u307f\u3067\u3059\u3002\nhttps://github.com/pfnet/chainer/tree/master/examples/mnist\n\u3000\u3000\u2517 train_mnist.py\n\u3053\u306e\u30b3\u30fc\u30c9\u306b\u30b3\u30e1\u30f3\u30c8\u3092\u52a0\u3048\u305f\u308a\u3001\u4e00\u90e8\u9014\u4e2d\u306e\u30d5\u30ed\u30fc\u3092\u30b0\u30e9\u30d5\u3067\u8868\u793a\u3057\u3066\u30a4\u30e1\u30fc\u30b8\u3092\u3064\u3051\u305f\u308a\u3057\u306a\u304c\u3089\u898b\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n3.\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u898b\u3066\u3044\u304f\n\u4eca\u56de\u3001\u624b\u6301\u3061\u306eMacbook Air(OS X ver10.10.2)\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u3092\u3057\u306a\u304c\u3089\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u74b0\u5883\u306b\u3088\u3063\u3066\u306f\u5dee\u5206\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u305d\u3053\u306f\u3088\u3057\u306a\u306b\u898b\u3066\u3082\u3089\u3048\u308c\u3070\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u3053\u3046\u3044\u3063\u305f\u74b0\u5883\u306e\u305f\u3081GPU\u3067\u306e\u8a08\u7b97\u306f\u884c\u308f\u305aCPU\u306e\u307f\u3068\u306a\u308a\u307e\u3059\u306e\u3067GPU\u95a2\u9023\u306e\u30b3\u30fc\u30c9\u306f\u7701\u7565\u3057\u3066\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n3-1.\u6e96\u5099\n\u307e\u305a\u6700\u521d\u306b\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u7fa4\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u3059\u3002\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_mldata\nfrom chainer import cuda, Variable, FunctionSet, optimizers\nimport chainer.functions  as F\nimport sys\n\nplt.style.use('ggplot')\n\n\u6b21\u306b\u5404\u7a2e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u5b9a\u7fa9\u30fb\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\n# \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306e\uff11\u56de\u5206\u306e\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\nbatchsize = 100\n\n# \u5b66\u7fd2\u306e\u7e70\u308a\u8fd4\u3057\u56de\u6570\nn_epoch   = 20\n\n# \u4e2d\u9593\u5c64\u306e\u6570\nn_units   = 1000\n\nScikit Learn\u3092\u3064\u304b\u3063\u3066MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n# MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n# #HOME/scikit_learn_data/mldata/mnist-original.mat \u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3055\u308c\u308b\nprint 'fetch MNIST dataset'\nmnist = fetch_mldata('MNIST original')\n# mnist.data : 70,000\u4ef6\u306e784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u30c7\u30fc\u30bf\nmnist.data   = mnist.data.astype(np.float32)\nmnist.data  /= 255     # 0-1\u306e\u30c7\u30fc\u30bf\u306b\u5909\u63db\n\n# mnist.target : \u6b63\u89e3\u30c7\u30fc\u30bf\uff08\u6559\u5e2b\u30c7\u30fc\u30bf\uff09\nmnist.target = mnist.target.astype(np.int32)\n\n3\u3064\u304f\u3089\u3044\u53d6\u308a\u51fa\u3057\u3066\u63cf\u753b\u3057\u3066\u307f\u307e\u3059\u3002\n# \u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u3092\u63cf\u753b\u3059\u308b\u95a2\u6570\ndef draw_digit(data):\n    size = 28\n    plt.figure(figsize=(2.5, 3))\n\n    X, Y = np.meshgrid(range(size),range(size))\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(X, Y, Z)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\n    plt.show()\n\ndraw_digit(mnist.data[5])\ndraw_digit(mnist.data[12345])\ndraw_digit(mnist.data[33456])\n\n28x28, 784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u3053\u3093\u306a\u30c7\u30fc\u30bf\u3067\u3059\u306d\u3002\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u307e\u3059\u3002\n# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092 N\u500b\u3001\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u6b8b\u308a\u306e\u500b\u6570\u3068\u8a2d\u5b9a\nN = 60000\nx_train, x_test = np.split(mnist.data,   [N])\ny_train, y_test = np.split(mnist.target, [N])\nN_test = y_test.size\n\n\n3.2 \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u3044\u3088\u3044\u3088\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3067\u3059\u3002\u3053\u3053\u304b\u3089\u304c\u672c\u756a\u3067\u3059\u306d\u3002Chainer\u306e\u30af\u30e9\u30b9\u3084\u95a2\u6570\u3092\u4f7f\u3044\u307e\u3059\u3002\n# Prepare multi-layer perceptron model\n# \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u30e2\u30c7\u30eb\u306e\u8a2d\u5b9a\n# \u5165\u529b 784\u6b21\u5143\u3001\u51fa\u529b 10\u6b21\u5143\nmodel = FunctionSet(l1=F.Linear(784, n_units),\n                    l2=F.Linear(n_units, n_units),\n                    l3=F.Linear(n_units, 10))\n\n\u5165\u529b\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u30c7\u30fc\u30bf\u304c784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306a\u306e\u3067\u3001\u5165\u529b\u7d20\u5b50\u306f784\u500b\u306b\u306a\u308a\u307e\u3059\u3002\u4eca\u56de\u4e2d\u9593\u5c64\u306fn_units\u30671000\u3068\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u51fa\u529b\u306f\u3001\u6570\u5b57\u3092\u8b58\u5225\u3059\u308b\u306e\u306710\u500b\u306b\u306a\u308a\u307e\u3059\u3002\u4e0b\u8a18\u304c\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u3002\n\n\u9806\u4f1d\u64ad\u306e\u69cb\u9020\u304c\u4e0b\u8a18\u306eforward()\u95a2\u6570\u3067\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\n# Neural net architecture\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u69cb\u9020\ndef forward(x_data, y_data, train=True):\n    x, t = Variable(x_data), Variable(y_data)\n    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n    y  = model.l3(h2)\n    # \u591a\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\u8aa4\u5dee\u95a2\u6570\u3068\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u8aa4\u5dee\u3092\u5c0e\u51fa\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n\n\u3053\u3053\u3067\u5404\u95a2\u6570\u7b49\u3092\u8aac\u660e\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nChainer\u306e\u304a\u4f5c\u6cd5\u3067\u3001\u30c7\u30fc\u30bf\u306f\u914d\u5217\u304b\u3089Chainer\u306eVariable\u3068\u3044\u3046\u578b\uff08\u30af\u30e9\u30b9\uff09\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5909\u63db\u3057\u3066\u4f7f\u3044\u307e\u3059\u3002\nx, t = Variable(x_data), Variable(y_data)\n\n\u6d3b\u6027\u5316\u95a2\u6570\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u306f\u306a\u304f\u3001F.relu()\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\nF.relu(model.l1(x))\n\n\u3053\u306eF.relu()\u306f\u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570(Rectified Linear Unit function)\u3067\nf(x) = \\max(0, x)\nf(x)=max{f(x) = \\max(0, x)\n}\n\u3064\u307e\u308a\n\n\u3053\u3093\u306a\u611f\u3058\u3067\u3059\u3002\n\u63cf\u753b\u30b3\u30fc\u30c9\u306f\u3053\u3061\u3089\u3002\n# F.relu\u30c6\u30b9\u30c8\nx_data = np.linspace(-10, 10, 100, dtype=np.float32)\nx = Variable(x_data)\ny = F.relu(x)\n\nplt.figure(figsize=(7,5))\nplt.ylim(-2,10)\nplt.plot(x.data, y.data)\nplt.show()\n\n\u30b7\u30f3\u30d7\u30eb\u306a\u95a2\u6570\u3067\u3059\u306d\u3002\u3053\u306e\u305f\u3081\u3001\u8a08\u7b97\u91cf\u304c\u5c0f\u3055\u304f\u5b66\u7fd2\u30b9\u30d4\u30fc\u30c9\u304c\u901f\u304f\u306a\u308b\u3053\u3068\u304c\u5229\u70b9\u306e\u3088\u3046\u3067\u3059\u3002\n\u6b21\u306b\u3001\u3053\u306erelu()\u95a2\u6570\u306e\u51fa\u529b\u3092\u5165\u529b\u3068\u3057\u3066F.dropout()\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\nF.dropout(F.relu(model.l1(x)),  train=train)\n\n\u3053\u306e\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u95a2\u6570F.dropout()\u306fDropout: A Simple Way to Prevent Neural Networks from Overfitting\u3068\u3044\u3046\u8ad6\u6587\u3067\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u624b\u6cd5\u3067\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u4e2d\u9593\u5c64\u3092\u30c9\u30ed\u30c3\u30d7\uff08\u306a\u3044\u3082\u306e\u3068\u3059\u308b\uff09\u3057\u3001\u305d\u3046\u3059\u308b\u3068\u904e\u5b66\u7fd2\u3092\u9632\u3050\u3053\u3068\u304c\u3067\u304d\u308b\u305d\u3046\u3067\u3059\u3002\n\u3061\u3087\u3063\u3068\u52d5\u304b\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n# dropout(x, ratio=0.5, train=True) \u30c6\u30b9\u30c8\n# x: \u5165\u529b\u5024\n# ratio: 0\u3092\u51fa\u529b\u3059\u308b\u78ba\u7387\n# train: False\u306e\u5834\u5408\u306fx\u3092\u305d\u306e\u307e\u307e\u8fd4\u5374\u3059\u308b\n# return: ratio\u306e\u78ba\u7387\u30670\u3092\u30011\u2212ratio\u306e\u78ba\u7387\u3067,x*(1/(1-ratio))\u306e\u5024\u3092\u8fd4\u3059\n\nn = 50\nv_sum = 0\nfor i in range(n):\n    x_data = np.array([1,2,3,4,5,6], dtype=np.float32)\n    x = Variable(x_data)\n    dr = F.dropout(x, ratio=0.6,train=True)\n\n    for j in range(6):\n        sys.stdout.write( str(dr.data[j]) + ', ' )\n    print(\"\")\n    v_sum += dr.data\n\n# output\u306e\u5e73\u5747\u304cx_data\u3068\u3060\u3044\u305f\u3044\u4e00\u81f4\u3059\u308b \nsys.stdout.write( str((v_sum/float(n))) )\n\n\noutput\n2.5, 5.0, 7.5, 0.0, 0.0, 0.0, \n2.5, 5.0, 7.5, 10.0, 0.0, 15.0, \n0.0, 5.0, 7.5, 10.0, 12.5, 15.0, \n\u3000\u3000\u3000\u3000\u3000\u3000\u30fb\u30fb\u30fb\n0.0, 0.0, 7.5, 10.0, 0.0, 0.0, \n2.5, 0.0, 7.5, 10.0, 0.0, 15.0, \n[ 0.94999999  2.29999995  3.          3.5999999   7.25        5.69999981]\n\n\n[1,2,3,4,5,6]\u3068\u3044\u3046\u914d\u5217\u3092F.dropout()\u95a2\u6570\u306b\u6e21\u3057\u307e\u3059\u3002\u3044\u307e\u3001ratio\u306f\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\u3067\u3042\u308a\u3001ratio=0.6\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u306e\u3067\u300160%\u306e\u78ba\u7387\u3067\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3055\u308c\u30010\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u300240%\u306e\u78ba\u7387\u3067\u5024\u304c\u8fd4\u3055\u308c\u308b\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u969b\u3001\u5024\u3092\u8fd4\u3059\u78ba\u7387\u304c40%\u306b\u6e1b\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u88dc\u3046\u305f\u3081\u306b{1 \\over 0.4}{1 \\over 0.4}\u500d=2.5\u500d\u3055\u308c\u305f\u5024\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\u3064\u307e\u308a\n(0 \\times 0.6 + 2.5 \\times 0.4) = 1\n{(0 \\times 0.6 + 2.5 \\times 0.4) = 1\n}{(0 \\times 0.6 + 2.5 \\times 0.4) = 1\n}\n\u3067\u3001\u5e73\u5747\u3059\u308b\u3068\u5143\u306e\u6570\u5b57\u306b\u306a\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u4e0a\u8a18\u306e\u4f8b\u3060\u3068\u6700\u5f8c\u306e\u884c\u304c\u51fa\u529b\u306e\u5e73\u5747\u3067\u3059\u304c\u300150\u56de\u7e70\u308a\u8fd4\u3057\u3066\u5927\u4f53\u5143\u306e[1,2,3,4,5,6]\u306b\u8fd1\u3044\u5024\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u540c\u3058\u69cb\u9020\u304c\u3082\u3046\uff11\u5c64\u3042\u308a\u3001\u51fa\u529b\u3055\u308c\u51fa\u529b\u5024\u304cyy\u3068\u306a\u308a\u307e\u3059\u3002\n    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n    y  = model.l3(h2)\n\n\u6700\u5f8c\u306e\u51fa\u529b\u3067\u3059\u304c\u3001\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3068\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u8aa4\u5dee\u306e\u51fa\u529b\u3002\u305d\u308c\u3068F.accuracy()\u95a2\u6570\u3067\u7cbe\u5ea6\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n    # \u591a\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\u8aa4\u5dee\u95a2\u6570\u3068\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u8aa4\u5dee\u3092\u5c0e\u51fa\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n\n\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3067\u3059\u304c\u3001\ny_k = z_k = f_{k}({\\bf u})={\\exp(u_{k}) \\over \\sum_j^K \\exp(u_{j})}\n{y_k = z_k = f_{k}({\\bf u})={\\exp(u_{k}) \\over \\sum_j^K \\exp(u_{j})}\n}{y_k = z_k = f_{k}({\\bf u})={\\exp(u_{k}) \\over \\sum_j^K \\exp(u_{j})}\n}\n\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u95a2\u6570\u3067\u3001\u3053\u306e\u95a2\u6570\u3092\u631f\u3080\u3053\u3068\u3067y_1, \\cdots ,y_{10}y_1, \\cdots ,y_{10}\u306e10\u500b\u306e\u51fa\u529b\u306e\u7dcf\u548c\u304c1\u3068\u306a\u308a\u3001\u51fa\u529b\u3092\u78ba\u7387\u3068\u3057\u3066\u89e3\u91c8\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\u306a\u305c\\exp()\\exp()\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u308b\u304b\u3068\u3044\u3046\u3068\u3001\u5024\u304c\u30de\u30a4\u30ca\u30b9\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u3053\u3068\u3068\u81ea\u5206\u306f\u7406\u89e3\u3057\u3066\u3044\u307e\u3059\u3002\n\u304a\u306a\u3058\u307f\\exp()\\exp()\u95a2\u6570\u306f\n\n\u306e\u3088\u3046\u306a\u5f62\u306a\u306e\u3067\u3001\u30de\u30a4\u30ca\u30b9\u306e\u5024\u3092\u53d6\u308a\u307e\u305b\u3093\u3002\u3053\u308c\u306b\u3088\u308a\u5024\u304c\u30de\u30a4\u30ca\u30b9\u306b\u306a\u3089\u305a\u3001\u304b\u3064\u7dcf\u548c\u304c\uff11\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u3001\u78ba\u7387\u3068\u89e3\u91c8\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002\n\u3055\u3063\u304d\u306e\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\u51fa\u529b\u5024y_ky_k\u3092\u7528\u3044\u3066\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u306f\nE({\\bf w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\log y_k ({\\bf x}_n, {\\bf w})\n{E({\\bf w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\log y_k ({\\bf x}_n, {\\bf w})\n}{E({\\bf w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\log y_k ({\\bf x}_n, {\\bf w})\n}\n\u3068\u8868\u73fe\u3055\u308c\u307e\u3059\u3002\nChainer\u306e\u30b3\u30fc\u30c9\u3067\u8a00\u3046\u3068\u3001\nhttps://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py\n\u306b\u3042\u308b\u3001\ndef forward_cpu(self, inputs):\n        x, t = inputs\n        self.y, = Softmax().forward_cpu((x,))\n        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,\n\n\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002\n\u307e\u305f\u3001F.accuracy(y, t)\u306f\u51fa\u529b\u3068\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u7167\u5408\u3057\u3066\u6b63\u7b54\u7387\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n\n3.3 Optimizer\u306e\u8a2d\u5b9a\n\u3055\u3066\u3001\u30e2\u30c7\u30eb\u304c\u6c7a\u307e\u3063\u305f\u306e\u3067\u8a13\u7df4\u306b\u79fb\u308a\u307e\u3059\u3002\n\u3053\u3053\u3067\u306f\u6700\u9069\u5316\u624b\u6cd5\u3068\u3057\u3066Adam\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model.collect_parameters())\n\nAdam\u306b\u3064\u3044\u3066\u306f30\u5206\u3067\u308f\u304b\u308bAdam\u3067echizen_tm\u3055\u3093\u304c\u89e3\u8aac\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\n\n4.\u8a13\u7df4\u306e\u5b9f\u65bd\u3068\u7d50\u679c\n\u4ee5\u4e0a\u306e\u6e96\u5099\u304b\u3089\u3001\u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u3067\u624b\u66f8\u304d\u6570\u5b57\u306e\u5224\u5225\u3092\u5b9f\u65bd\u3057\u3001\u305d\u306e\u7cbe\u5ea6\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002\ntrain_loss = []\ntrain_acc  = []\ntest_loss = []\ntest_acc  = []\n\nl1_W = []\nl2_W = []\nl3_W = []\n\n# Learning loop\nfor epoch in xrange(1, n_epoch+1):\n    print 'epoch', epoch\n\n    # training\n    # N\u500b\u306e\u9806\u756a\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3073\u66ff\u3048\u308b\n    perm = np.random.permutation(N)\n    sum_accuracy = 0\n    sum_loss = 0\n    # 0\u301cN\u307e\u3067\u306e\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3054\u3068\u306b\u4f7f\u3063\u3066\u5b66\u7fd2\n    for i in xrange(0, N, batchsize):\n        x_batch = x_train[perm[i:i+batchsize]]\n        y_batch = y_train[perm[i:i+batchsize]]\n\n        # \u52fe\u914d\u3092\u521d\u671f\u5316\n        optimizer.zero_grads()\n        # \u9806\u4f1d\u64ad\u3055\u305b\u3066\u8aa4\u5dee\u3068\u7cbe\u5ea6\u3092\u7b97\u51fa\n        loss, acc = forward(x_batch, y_batch)\n        # \u8aa4\u5dee\u9006\u4f1d\u64ad\u3067\u52fe\u914d\u3092\u8a08\u7b97\n        loss.backward()\n        optimizer.update()\n\n        train_loss.append(loss.data)\n        train_acc.append(acc.data)\n        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n\n    # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u8868\u793a\n    print 'train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N)\n\n    # evaluation\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u7b97\u51fa\u3057\u6c4e\u5316\u6027\u80fd\u3092\u78ba\u8a8d\n    sum_accuracy = 0\n    sum_loss     = 0\n    for i in xrange(0, N_test, batchsize):\n        x_batch = x_test[i:i+batchsize]\n        y_batch = y_test[i:i+batchsize]\n\n        # \u9806\u4f1d\u64ad\u3055\u305b\u3066\u8aa4\u5dee\u3068\u7cbe\u5ea6\u3092\u7b97\u51fa\n        loss, acc = forward(x_batch, y_batch, train=False)\n\n        test_loss.append(loss.data)\n        test_acc.append(acc.data)\n        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u8868\u793a\n    print 'test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test)\n\n    # \u5b66\u7fd2\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u4fdd\u5b58\n    l1_W.append(model.l1.W)\n    l2_W.append(model.l2.W)\n    l3_W.append(model.l3.W)\n\n# \u7cbe\u5ea6\u3068\u8aa4\u5dee\u3092\u30b0\u30e9\u30d5\u63cf\u753b\nplt.figure(figsize=(8,6))\nplt.plot(range(len(train_acc)), train_acc)\nplt.plot(range(len(test_acc)), test_acc)\nplt.legend([\"train_acc\",\"test_acc\"],loc=4)\nplt.title(\"Accuracy of digit recognition.\")\nplt.plot()\n\nepoch\u6bce\u306e\u30b5\u30de\u30ea\u7d50\u679c\u306f\u3053\u3061\u3089\u3067\u3059\u300220\u56de\u3057\u3057\u306698.5%\u304f\u3089\u3044\u306e\u9ad8\u7cbe\u5ea6\u3067\u5224\u5225\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\noutput\nepoch 1\ntrain mean loss=0.278375425202, accuracy=0.914966667456\ntest  mean loss=0.11533634907, accuracy=0.964300005436\nepoch 2\ntrain mean loss=0.137060894324, accuracy=0.958216670454\ntest  mean loss=0.0765812527167, accuracy=0.976100009084\nepoch 3\ntrain mean loss=0.107826075749, accuracy=0.966816672881\ntest  mean loss=0.0749603212342, accuracy=0.97770000577\nepoch 4\ntrain mean loss=0.0939164237926, accuracy=0.970616674324\ntest  mean loss=0.0672153823725, accuracy=0.980000005364\nepoch 5\ntrain mean loss=0.0831089563683, accuracy=0.973950009048\ntest  mean loss=0.0705943618687, accuracy=0.980100004673\nepoch 6\ntrain mean loss=0.0752325405277, accuracy=0.976883343955\ntest  mean loss=0.0732760328815, accuracy=0.977900006771\nepoch 7\ntrain mean loss=0.0719517664274, accuracy=0.977383343875\ntest  mean loss=0.063611669606, accuracy=0.981900005937\nepoch 8\ntrain mean loss=0.0683009948514, accuracy=0.978566677173\ntest  mean loss=0.0604036964733, accuracy=0.981400005221\nepoch 9\ntrain mean loss=0.0621755663728, accuracy=0.980550010701\ntest  mean loss=0.0591542539285, accuracy=0.982400006652\nepoch 10\ntrain mean loss=0.0618313539471, accuracy=0.981183344225\ntest  mean loss=0.0693172766063, accuracy=0.982900006175\nepoch 11\ntrain mean loss=0.0583098273944, accuracy=0.982000010014\ntest  mean loss=0.0668152360269, accuracy=0.981600006819\nepoch 12\ntrain mean loss=0.054178619228, accuracy=0.983533344865\ntest  mean loss=0.0614466062452, accuracy=0.982900005579\nepoch 13\ntrain mean loss=0.0532431817259, accuracy=0.98390001148\ntest  mean loss=0.060112986485, accuracy=0.98400000751\nepoch 14\ntrain mean loss=0.0538122716064, accuracy=0.983266676267\ntest  mean loss=0.0624165921964, accuracy=0.983300005198\nepoch 15\ntrain mean loss=0.0501562882114, accuracy=0.983833344777\ntest  mean loss=0.0688113694015, accuracy=0.98310000658\nepoch 16\ntrain mean loss=0.0513108611095, accuracy=0.984533343514\ntest  mean loss=0.0724038232205, accuracy=0.982200007439\nepoch 17\ntrain mean loss=0.0471463404785, accuracy=0.985666677058\ntest  mean loss=0.0612579581685, accuracy=0.983600008488\nepoch 18\ntrain mean loss=0.0460166006556, accuracy=0.986050010125\ntest  mean loss=0.0654888718335, accuracy=0.984400007725\nepoch 19\ntrain mean loss=0.0458772557077, accuracy=0.986433342795\ntest  mean loss=0.0602016936944, accuracy=0.984400007129\nepoch 20\ntrain mean loss=0.046333729005, accuracy=0.986433343093\ntest  mean loss=0.0621869922416, accuracy=0.985100006461\n\n\n\u5404\u30d0\u30c3\u30c1\u6bce\u306e\u5224\u5225\u7cbe\u5ea6\u3068\u3001\u8aa4\u5dee\u306e\u30b0\u30e9\u30d5\u304c\u3053\u3061\u3089\u3067\u3059\u3002\u8d64\u3044\u307b\u3046\u304c\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3001\u9752\u3044\u307b\u3046\u304c\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u4ee5\u524d\u3001\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011k-nearest neighbor method(k\u6700\u8fd1\u508d\u6cd5)\u3092\u81ea\u529b\u3067python\u3067\u66f8\u3044\u3066\u3001\u624b\u66f8\u304d\u6570\u5b57\u306e\u8a8d\u8b58\u3092\u3059\u308b\u3068\u3044\u3046\u8a18\u4e8b\u3067\u3001\u540c\u69d8\u306b\u624b\u66f8\u304d\u6570\u5b57\u306e\u5224\u5225\u3092\u3084\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u6642\u306e\u7cbe\u5ea6\u304c97%\u304f\u3089\u3044\u3060\u3063\u305f\u306e\u3067\u3001\u66f4\u306b\u5c11\u3057\u4e0a\u304c\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u3053\u306eChiner\u306f\u5168\u3066Python\u30b3\u30fc\u30c9\u3067\u64cd\u4f5c\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u975e\u5e38\u306bPythonista\u3068\u3057\u3066\u306f\u5b09\u3057\u3044\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\u307e\u3060\u3001\u300c\u30c7\u30a3\u30fc\u30d7\u300d\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u304d\u3066\u304a\u3089\u305a\u3001\u305f\u3060\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306a\u306e\u3067\u3001\u8fd1\u3005\u300c\u30c7\u30a3\u30fc\u30d7\u300d\u306a\u3084\u3064\u306e\u8a18\u4e8b\u3082\u66f8\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n5.\u7b54\u3048\u5408\u308f\u305b\n\u8b58\u5225\u3057\u305f100\u500b\u306e\u6570\u5b57\u3092\u8868\u793a\u3057\u3066\u307f\u307e\u3059\u3002\u30e9\u30f3\u30c0\u30e0\u306b100\u500b\u62bd\u51fa\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u307b\u3068\u3093\u3069\u6b63\u89e3\u3067\u3059\u3002\u4f55\u56de\u304b100\u500b\u8868\u793a\u3092\u884c\u3063\u3066\u3084\u3063\u3068\u9593\u9055\u3063\u3066\u3044\u308b\u3068\u3053\u308d\u3092\uff11\u3064\u8868\u793a\u3067\u304d\u305f\u306e\u3067\u3001\u305d\u306e\u4f8b\u3092\u4e0b\u8a18\u306b\u8cbc\u3063\u3066\u3044\u307e\u3059\u3002\u306a\u3093\u3060\u304b\u4eba\u9593\u306e\u65b9\u304c\u8a66\u3055\u308c\u3066\u3044\u308b\u6c17\u5206\u3067\u3059\uff08\u7b11\uff09\n\n\uff08\u203b 2\u884c3\u5217\u306e4\u30929\u3068\u8aa4\u8b58\u5225\u3057\u3066\u3044\u307e\u3059\uff09\nplt.style.use('fivethirtyeight')\ndef draw_digit3(data, n, ans, recog):\n    size = 28\n    plt.subplot(10, 10, n)\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(Z)\n    plt.title(\"ans=%d, recog=%d\"%(ans,recog), size=8)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\n\nplt.figure(figsize=(15,15))\n\ncnt = 0\nfor idx in np.random.permutation(N)[:100]:\n\n    xxx = x_train[idx].astype(np.float32)\n    h1 = F.dropout(F.relu(model.l1(Variable(xxx.reshape(1,784)))),  train=False)\n    h2 = F.dropout(F.relu(model.l2(h1)), train=False)\n    y  = model.l3(h2)\n    cnt+=1\n    draw_digit3(x_train[idx], cnt, y_train[idx], np.argmax(y.data))\nplt.show\n\n\n6.\u7b2c\uff11\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bfw\u306e\u53ef\u8996\u5316\n\u5165\u529b\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bfw^{(1)}w^{(1)}784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u309228x28\u30d4\u30af\u30bb\u30eb\u3068\u3057\u3066\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u8868\u793a\u3057\u3066\u307f\u307e\u3057\u305f\u30021000\u500b\u306e\u3046\u3061\u30e9\u30f3\u30c0\u30e0\u306b100\u500b\u9078\u3093\u3067\u3044\u307e\u3059\u3002\u3088\u304f\u307f\u308b\u3068\"2\"\u3068\u304b\"5\"\u3068\u304b\"0\"\u306b\u898b\u3048\u308b\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u306d\u30021\u5c64\u76ee\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u7279\u5fb4\u62bd\u51fa\u304c\u3067\u304d\u3066\u3044\u305d\u3046\u306a\u96f0\u56f2\u6c17\u304c\u4f3a\u3048\u307e\u3059\u3002\n\ndef draw_digit2(data, n, i):\n    size = 28\n    plt.subplot(10, 10, n)\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(Z)\n    plt.title(\"%d\"%i, size=9)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\nplt.figure(figsize=(10,10))\ncnt = 1\nfor i in np.random.permutation(1000)[:100]:\n    draw_digit2(l1_W[len(l1_W)-1][i], cnt, i)\n    cnt += 1\n\nplt.show()\n\n\n7.\u51fa\u529b\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bfw\u306e\u53ef\u8996\u5316\n\u51fa\u529b\u5c64\u306f1000\u500b\u306e\u30a4\u30f3\u30d7\u30c3\u30c8\u3092\u53d7\u3051\u3066\u300110\u500b\u306e\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3092\u884c\u3046\u5c64\u3067\u3059\u304c\u3001\u3053\u3053\u3082\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3057\u305f\u3002\"0\"\u3068\u66f8\u3044\u3066\u3042\u308b\u3068\u3053\u308d\u304c\u624b\u66f8\u304d\u6570\u5b57\u3092\"0\"\u3068\u5224\u5225\u3059\u308b\u305f\u3081\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u3059\u3002\n1000\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306a\u306e\u3067\u30010\u309224\u500b\u5f8c\u308d\u306b\u3064\u3051\u306632x32\u306e\u753b\u50cf\u306b\u843d\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u30ec\u30a4\u30e4\u30fc3\ndef draw_digit2(data, n, i):\n    size = 32\n    plt.subplot(4, 4, n)\n    data = np.r_[data,np.zeros(24)]\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,size-1)\n    plt.ylim(0,size-1)\n    plt.pcolor(Z)\n    plt.title(\"%d\"%i, size=9)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\nplt.figure(figsize=(10,10))\ncnt = 1\nfor i in range(10):\n    draw_digit2(l3_W[len(l3_W)-1][i], cnt, i)\n    cnt += 1\n\nplt.show()\n\n\n8.\u304a\u307e\u3051\n\u4e2d\u9593\u5c64\u306e\u7d20\u5b50\u6570\u3092[100, 500, 800, 900, 1000, 1100, 1200, 1500, 2000]\u306b\u3057\u3066\u305d\u308c\u305e\u308c\u5224\u5225\u3057\u3066\u307f\u305f\u3002\u7d50\u679c\u306e\u30b0\u30e9\u30d5\u304c\u4e0b\u8a18\u3067\u3059\u3002\u7d20\u5b50\u6570500\u4ee5\u4e0a\u3067\u6982\u306d98%\u3092\u9054\u6210\u3057\u3066\u304a\u308a\u3001\u305d\u308c\u4ee5\u4e0a\u306e\u7d20\u5b50\u6570\u306f\u3042\u3093\u307e\u308a\u5909\u308f\u3089\u306a\u3044\u307f\u305f\u3044\u3067\u3059\u306d\u3002\n\n\n9.\u304a\u307e\u30512 : \u6d3b\u6027\u5316\u95a2\u6570\nChainer\u306b\u30d7\u30ea\u30a4\u30f3\u3055\u308c\u3066\u3044\u308b\u4e3b\u306a\u6d3b\u6027\u5316\u95a2\u6570\u306b\n\nReLu function\ntanh function\nsigmoid function\n\n\u304c\u3042\u308a\u307e\u3059\u3002\u56f3\u793a\u3059\u308b\u3068\u4e0b\u8a18\u306e\u3088\u3046\u306a\u5f62\u3067\u3059\u3002\n\u7d20\u5b50\u306e\u5165\u529b\u3068\u51fa\u529b\u306e\u9593\u306b\u5165\u308b\u95a2\u6570\u3067\u3001\u5165\u51fa\u529b\u306b\u95a2\u3059\u308b\u95be\u5024\u3092\u8a2d\u5b9a\u3059\u308b\u3088\u3046\u306a\u5f79\u5272\u3092\u6301\u3061\u307e\u3059\u3002\n\n# \u6d3b\u6027\u5316\u95a2\u6570\u30c6\u30b9\u30c8\nx_data = np.linspace(-10, 10, 100, dtype=np.float32)\nx = Variable(x_data)\n\ny = F.relu(x)\nplt.figure(figsize=(8,15))\nplt.subplot(311)\nplt.title(\"ReLu function.\")\nplt.ylim(-2,10)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\n\ny = F.tanh(x)\nplt.subplot(312)\nplt.title(\"tanh function.\")\nplt.ylim(-1.5,1.5)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\n\ny = F.sigmoid(x)\nplt.subplot(313)\nplt.title(\"sigmoid function.\")\nplt.ylim(-.2,1.2)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\nplt.show()\n\n\u6b21\u306e\u8a18\u4e8b\n\u300c\u3010\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3011Chainer\u3067Autoencoder\u3092\u8a66\u3057\u3066\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u308b\u3002\u300d\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u7279\u5fb4\u62bd\u51fa\u3092\u81ea\u52d5\u5316\u3059\u308b\u6280\u8853\u306eAutoencoder\u3092\u5b9f\u88c5\u3057\u3066\u307f\u305f\u8a18\u4e8b\u3067\u3059\u3002\n\u3010\u53c2\u8003\u66f8\u7c4d\u3011\n\u3000\u6df1\u5c64\u5b66\u7fd2\uff08\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba\uff09 \u5ca1\u8c37\u8cb4\u4e4b\n\u3000\n\u3010\u53c2\u8003web\u30b5\u30a4\u30c8\u3011\n\u3000Chainer\u306e\u30e1\u30a4\u30f3\u30b5\u30a4\u30c8\n\u3000\u3000\u3000http://chainer.org/\n\u3000Chainer\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\n\u3000\u3000\u3000https://github.com/pfnet/chainer\n\u3000Chainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3068\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\n\u3000\u3000\u3000http://docs.chainer.org/en/latest/\n\u3000\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"\n\u3000 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov \n\u3000\u3000\u3000http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n\u4eca\u8a71\u984c\u306eDeep Learning(\u6df1\u5c64\u5b66\u7fd2)\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3001[Chainer](http://chainer.org/)\u306b\u624b\u66f8\u304d\u6587\u5b57\u306e\u5224\u5225\u3092\u884c\u3046\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3061\u3089\u3092\u4f7f\u3063\u3066\u5185\u5bb9\u3092\u5c11\u3057\u89e3\u8aac\u3059\u308b\u8a18\u4e8b\u3092\u66f8\u3044\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n**(\u672c\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u306e\u5168\u6587\u3092[GitHub](https://github.com/matsuken92/Qiita_Contents/blob/master/chainer-MNIST/chainer-MNIST_forPubs.ipynb)\u306b\u30a2\u30c3\u30d7\u3057\u307e\u3057\u305f\u3002[PC\u63a8\u5968])**\n\n\u3068\u306b\u304b\u304f\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u3059\u3054\u304f\u7c21\u5358\u304b\u3064\u3001Python\u304c\u66f8\u3051\u308c\u3070\u3059\u3050\u306b\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u3066\u304a\u3059\u3059\u3081\u3067\u3059\uff01\nPython\u306b\u9589\u3058\u3066\u30b3\u30fc\u30c9\u304c\u66f8\u3051\u308b\u306e\u3082\u3059\u3054\u304f\u3044\u3044\u3067\u3059\u3088\u306d\u3002\n\n\u3053\u3093\u306a\u611f\u3058\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u8a66\u3057\u3066\u307f\u308b\u3001\u3068\u3044\u3046\u8a18\u4e8b\u3067\u3059\u3002\n![nn_structure6.png](https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png)\n\n\n\u4e3b\u8981\u306a\u60c5\u5831\u306f\u3053\u3061\u3089\u306b\u3042\u308a\u307e\u3059\u3002\n[Chainer\u306e\u30e1\u30a4\u30f3\u30b5\u30a4\u30c8](http://chainer.org/)\n[Chainer\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/pfnet/chainer)\n[Chainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3068\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9](http://docs.chainer.org/en/latest/)\n\n\n#1. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb#\n\n\u307e\u305a\u306f\u4f55\u306f\u3068\u3082\u3042\u308c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u3059\u3002Chainer\u306eGitHub\u306b\u8a18\u8f09\u306e\"Requirements\" ( https://github.com/pfnet/chainer#requirements )\u3092\u53c2\u8003\u306b\u5fc5\u8981\u306a\u30bd\u30d5\u30c8\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u4e0a\u3067\n\n```\npip install chainer\n```\n\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u3053\u308c\u3060\u3051\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u3061\u3083\u3044\u307e\u3059\u3002\u8d85\u7c21\u5358\uff01Caffe\u3092Mac\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3088\u3046\u3068\u3057\u305f\u6642\u306f\u304b\u306a\u308a\u82e6\u6226\u3057\u307e\u3057\u305f\u304c\u3001\u5618\u306e\u3088\u3046\u3067\u3059 :smile: \n\n\u3082\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u8a70\u307e\u3063\u305f\u3089cvl-robot\u3055\u3093\u306e\u300c[DeepLearning\u30e9\u30a4\u30d6\u30e9\u30ea\u306eChainer\u304c\u3059\u3054\u3044\u3001\u3089\u3057\u3044](http://cvl-robot.hateblo.jp/entry/2015/06/11/223928)\u300d\u3068\u3044\u3046\u8a18\u4e8b\u304c\u8a73\u3057\u304f\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u7b49\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u3064\u3044\u3066\u8a18\u8f09\u3057\u3066\u304f\u308c\u3066\u3044\u3066\u4fbf\u5229\u3067\u3059\u3002\n\n#2.\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u306e\u5165\u624b#\nGitHub\u306e\u4e0b\u8a18\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u304a\u306a\u3058\u307fMNIST\u306e\u624b\u66f8\u304d\u6587\u5b57\u3092\u5224\u5225\u3059\u308b\u3001\u3068\u3044\u3046\u30b5\u30f3\u30d7\u30eb\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u3053\u308c\u3092\u984c\u6750\u3068\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u308c\u3092Chainer\u306e\u9806\u4f1d\u64ad\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067Classification\u3057\u3066\u307f\u308b\u3001\u3068\u3044\u3046\u8a66\u307f\u3067\u3059\u3002\nhttps://github.com/pfnet/chainer/tree/master/examples/mnist\n\u3000\u3000\u2517 train_mnist.py\n\n\u3053\u306e\u30b3\u30fc\u30c9\u306b\u30b3\u30e1\u30f3\u30c8\u3092\u52a0\u3048\u305f\u308a\u3001\u4e00\u90e8\u9014\u4e2d\u306e\u30d5\u30ed\u30fc\u3092\u30b0\u30e9\u30d5\u3067\u8868\u793a\u3057\u3066\u30a4\u30e1\u30fc\u30b8\u3092\u3064\u3051\u305f\u308a\u3057\u306a\u304c\u3089\u898b\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n#3.\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u898b\u3066\u3044\u304f#\n\u4eca\u56de\u3001\u624b\u6301\u3061\u306eMacbook Air(OS X ver10.10.2)\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u3092\u3057\u306a\u304c\u3089\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u74b0\u5883\u306b\u3088\u3063\u3066\u306f\u5dee\u5206\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u305d\u3053\u306f\u3088\u3057\u306a\u306b\u898b\u3066\u3082\u3089\u3048\u308c\u3070\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u3053\u3046\u3044\u3063\u305f\u74b0\u5883\u306e\u305f\u3081GPU\u3067\u306e\u8a08\u7b97\u306f\u884c\u308f\u305aCPU\u306e\u307f\u3068\u306a\u308a\u307e\u3059\u306e\u3067GPU\u95a2\u9023\u306e\u30b3\u30fc\u30c9\u306f\u7701\u7565\u3057\u3066\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n##3-1.\u6e96\u5099##\n\n\u307e\u305a\u6700\u521d\u306b\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u7fa4\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u3059\u3002\n\n```py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_mldata\nfrom chainer import cuda, Variable, FunctionSet, optimizers\nimport chainer.functions  as F\nimport sys\n\nplt.style.use('ggplot')\n```\n\n\u6b21\u306b\u5404\u7a2e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u5b9a\u7fa9\u30fb\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\n\n```py\n# \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306e\uff11\u56de\u5206\u306e\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\nbatchsize = 100\n\n# \u5b66\u7fd2\u306e\u7e70\u308a\u8fd4\u3057\u56de\u6570\nn_epoch   = 20\n\n# \u4e2d\u9593\u5c64\u306e\u6570\nn_units   = 1000\n```\n\nScikit Learn\u3092\u3064\u304b\u3063\u3066MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\n```py\n# MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n# #HOME/scikit_learn_data/mldata/mnist-original.mat \u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3055\u308c\u308b\nprint 'fetch MNIST dataset'\nmnist = fetch_mldata('MNIST original')\n# mnist.data : 70,000\u4ef6\u306e784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u30c7\u30fc\u30bf\nmnist.data   = mnist.data.astype(np.float32)\nmnist.data  /= 255     # 0-1\u306e\u30c7\u30fc\u30bf\u306b\u5909\u63db\n\n# mnist.target : \u6b63\u89e3\u30c7\u30fc\u30bf\uff08\u6559\u5e2b\u30c7\u30fc\u30bf\uff09\nmnist.target = mnist.target.astype(np.int32)\n```\n\n3\u3064\u304f\u3089\u3044\u53d6\u308a\u51fa\u3057\u3066\u63cf\u753b\u3057\u3066\u307f\u307e\u3059\u3002\n\n```py\n# \u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\u3092\u63cf\u753b\u3059\u308b\u95a2\u6570\ndef draw_digit(data):\n    size = 28\n    plt.figure(figsize=(2.5, 3))\n\n    X, Y = np.meshgrid(range(size),range(size))\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(X, Y, Z)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\n    plt.show()\n\ndraw_digit(mnist.data[5])\ndraw_digit(mnist.data[12345])\ndraw_digit(mnist.data[33456])\n```\n\n28x28, 784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u3053\u3093\u306a\u30c7\u30fc\u30bf\u3067\u3059\u306d\u3002\n\n![digits-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/0e36e752-3fea-716a-c331-2bf8a15b47de.png)\n\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u307e\u3059\u3002\n\n```py\n# \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3092 N\u500b\u3001\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u6b8b\u308a\u306e\u500b\u6570\u3068\u8a2d\u5b9a\nN = 60000\nx_train, x_test = np.split(mnist.data,   [N])\ny_train, y_test = np.split(mnist.target, [N])\nN_test = y_test.size\n```\n\n##3.2 \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9##\n\n\u3044\u3088\u3044\u3088\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3067\u3059\u3002\u3053\u3053\u304b\u3089\u304c\u672c\u756a\u3067\u3059\u306d\u3002Chainer\u306e\u30af\u30e9\u30b9\u3084\u95a2\u6570\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n```py\n# Prepare multi-layer perceptron model\n# \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u30e2\u30c7\u30eb\u306e\u8a2d\u5b9a\n# \u5165\u529b 784\u6b21\u5143\u3001\u51fa\u529b 10\u6b21\u5143\nmodel = FunctionSet(l1=F.Linear(784, n_units),\n                    l2=F.Linear(n_units, n_units),\n                    l3=F.Linear(n_units, 10))\n```\n\n\u5165\u529b\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u30c7\u30fc\u30bf\u304c784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306a\u306e\u3067\u3001\u5165\u529b\u7d20\u5b50\u306f784\u500b\u306b\u306a\u308a\u307e\u3059\u3002\u4eca\u56de\u4e2d\u9593\u5c64\u306fn_units\u30671000\u3068\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u51fa\u529b\u306f\u3001\u6570\u5b57\u3092\u8b58\u5225\u3059\u308b\u306e\u306710\u500b\u306b\u306a\u308a\u307e\u3059\u3002\u4e0b\u8a18\u304c\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u3002\n\n\n![nn_structure6.png](https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png)\n\n\u9806\u4f1d\u64ad\u306e\u69cb\u9020\u304c\u4e0b\u8a18\u306eforward()\u95a2\u6570\u3067\u5b9a\u7fa9\u3055\u308c\u307e\u3059\u3002\n\n```py\n# Neural net architecture\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u69cb\u9020\ndef forward(x_data, y_data, train=True):\n    x, t = Variable(x_data), Variable(y_data)\n    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n    y  = model.l3(h2)\n    # \u591a\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\u8aa4\u5dee\u95a2\u6570\u3068\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u8aa4\u5dee\u3092\u5c0e\u51fa\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n```\n\n\u3053\u3053\u3067\u5404\u95a2\u6570\u7b49\u3092\u8aac\u660e\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nChainer\u306e\u304a\u4f5c\u6cd5\u3067\u3001\u30c7\u30fc\u30bf\u306f\u914d\u5217\u304b\u3089Chainer\u306eVariable\u3068\u3044\u3046\u578b\uff08\u30af\u30e9\u30b9\uff09\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5909\u63db\u3057\u3066\u4f7f\u3044\u307e\u3059\u3002\n\n```py\nx, t = Variable(x_data), Variable(y_data)\n```\n\n\u6d3b\u6027\u5316\u95a2\u6570\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u306f\u306a\u304f\u3001F.relu()\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n\n```py\nF.relu(model.l1(x))\n```\n\u3053\u306eF.relu()\u306f\u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570(Rectified Linear Unit function)\u3067\n\n```math\nf(x) = \\max(0, x)\n```\n\u3064\u307e\u308a\n\n![relu-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/e1cc4c94-e4ae-0010-82e5-c27956b5986c.png)\n\n\u3053\u3093\u306a\u611f\u3058\u3067\u3059\u3002\n\u63cf\u753b\u30b3\u30fc\u30c9\u306f\u3053\u3061\u3089\u3002\n\n```py\n# F.relu\u30c6\u30b9\u30c8\nx_data = np.linspace(-10, 10, 100, dtype=np.float32)\nx = Variable(x_data)\ny = F.relu(x)\n\nplt.figure(figsize=(7,5))\nplt.ylim(-2,10)\nplt.plot(x.data, y.data)\nplt.show()\n```\n\n\u30b7\u30f3\u30d7\u30eb\u306a\u95a2\u6570\u3067\u3059\u306d\u3002\u3053\u306e\u305f\u3081\u3001\u8a08\u7b97\u91cf\u304c\u5c0f\u3055\u304f\u5b66\u7fd2\u30b9\u30d4\u30fc\u30c9\u304c\u901f\u304f\u306a\u308b\u3053\u3068\u304c\u5229\u70b9\u306e\u3088\u3046\u3067\u3059\u3002\n\n\u6b21\u306b\u3001\u3053\u306erelu()\u95a2\u6570\u306e\u51fa\u529b\u3092\u5165\u529b\u3068\u3057\u3066F.dropout()\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n\n```py\nF.dropout(F.relu(model.l1(x)),  train=train)\n```\n\n\u3053\u306e\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u95a2\u6570F.dropout()\u306f[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\u3068\u3044\u3046\u8ad6\u6587\u3067\u63d0\u5531\u3055\u308c\u3066\u3044\u308b\u624b\u6cd5\u3067\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u4e2d\u9593\u5c64\u3092\u30c9\u30ed\u30c3\u30d7\uff08\u306a\u3044\u3082\u306e\u3068\u3059\u308b\uff09\u3057\u3001\u305d\u3046\u3059\u308b\u3068\u904e\u5b66\u7fd2\u3092\u9632\u3050\u3053\u3068\u304c\u3067\u304d\u308b\u305d\u3046\u3067\u3059\u3002\n\n\u3061\u3087\u3063\u3068\u52d5\u304b\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n```py\n# dropout(x, ratio=0.5, train=True) \u30c6\u30b9\u30c8\n# x: \u5165\u529b\u5024\n# ratio: 0\u3092\u51fa\u529b\u3059\u308b\u78ba\u7387\n# train: False\u306e\u5834\u5408\u306fx\u3092\u305d\u306e\u307e\u307e\u8fd4\u5374\u3059\u308b\n# return: ratio\u306e\u78ba\u7387\u30670\u3092\u30011\u2212ratio\u306e\u78ba\u7387\u3067,x*(1/(1-ratio))\u306e\u5024\u3092\u8fd4\u3059\n\nn = 50\nv_sum = 0\nfor i in range(n):\n    x_data = np.array([1,2,3,4,5,6], dtype=np.float32)\n    x = Variable(x_data)\n    dr = F.dropout(x, ratio=0.6,train=True)\n    \n    for j in range(6):\n        sys.stdout.write( str(dr.data[j]) + ', ' )\n    print(\"\")\n    v_sum += dr.data\n\n# output\u306e\u5e73\u5747\u304cx_data\u3068\u3060\u3044\u305f\u3044\u4e00\u81f4\u3059\u308b \nsys.stdout.write( str((v_sum/float(n))) )\n```\n\n```bash:output\n2.5, 5.0, 7.5, 0.0, 0.0, 0.0, \n2.5, 5.0, 7.5, 10.0, 0.0, 15.0, \n0.0, 5.0, 7.5, 10.0, 12.5, 15.0, \n\u3000\u3000\u3000\u3000\u3000\u3000\u30fb\u30fb\u30fb\n0.0, 0.0, 7.5, 10.0, 0.0, 0.0, \n2.5, 0.0, 7.5, 10.0, 0.0, 15.0, \n[ 0.94999999  2.29999995  3.          3.5999999   7.25        5.69999981]\n```\n\n[1,2,3,4,5,6]\u3068\u3044\u3046\u914d\u5217\u3092F.dropout()\u95a2\u6570\u306b\u6e21\u3057\u307e\u3059\u3002\u3044\u307e\u3001ratio\u306f\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\u3067\u3042\u308a\u3001ratio=0.6\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u306e\u3067\u300160%\u306e\u78ba\u7387\u3067\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3055\u308c\u30010\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u300240%\u306e\u78ba\u7387\u3067\u5024\u304c\u8fd4\u3055\u308c\u308b\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u969b\u3001\u5024\u3092\u8fd4\u3059\u78ba\u7387\u304c40%\u306b\u6e1b\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u88dc\u3046\u305f\u3081\u306b${1 \\over 0.4}$\u500d=2.5\u500d\u3055\u308c\u305f\u5024\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\u3064\u307e\u308a\n\n```math\n(0 \\times 0.6 + 2.5 \\times 0.4) = 1\n```\n\u3067\u3001\u5e73\u5747\u3059\u308b\u3068\u5143\u306e\u6570\u5b57\u306b\u306a\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u4e0a\u8a18\u306e\u4f8b\u3060\u3068\u6700\u5f8c\u306e\u884c\u304c\u51fa\u529b\u306e\u5e73\u5747\u3067\u3059\u304c\u300150\u56de\u7e70\u308a\u8fd4\u3057\u3066\u5927\u4f53\u5143\u306e[1,2,3,4,5,6]\u306b\u8fd1\u3044\u5024\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u540c\u3058\u69cb\u9020\u304c\u3082\u3046\uff11\u5c64\u3042\u308a\u3001\u51fa\u529b\u3055\u308c\u51fa\u529b\u5024\u304c$y$\u3068\u306a\u308a\u307e\u3059\u3002\n\n```py\n    h2 = F.dropout(F.relu(model.l2(h1)), train=train)\n    y  = model.l3(h2)\n```\n\n\u6700\u5f8c\u306e\u51fa\u529b\u3067\u3059\u304c\u3001\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3068\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u8aa4\u5dee\u306e\u51fa\u529b\u3002\u305d\u308c\u3068F.accuracy()\u95a2\u6570\u3067\u7cbe\u5ea6\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n    # \u591a\u30af\u30e9\u30b9\u5206\u985e\u306a\u306e\u3067\u8aa4\u5dee\u95a2\u6570\u3068\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u3092\u7528\u3044\u3066\u3001\u8aa4\u5dee\u3092\u5c0e\u51fa\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n```\n\n\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3067\u3059\u304c\u3001\n\n```math\ny_k = z_k = f_{k}({\\bf u})={\\exp(u_{k}) \\over \\sum_j^K \\exp(u_{j})}\n```\n\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u95a2\u6570\u3067\u3001\u3053\u306e\u95a2\u6570\u3092\u631f\u3080\u3053\u3068\u3067$y_1, \\cdots ,y_{10}$\u306e10\u500b\u306e\u51fa\u529b\u306e\u7dcf\u548c\u304c1\u3068\u306a\u308a\u3001\u51fa\u529b\u3092\u78ba\u7387\u3068\u3057\u3066\u89e3\u91c8\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\u306a\u305c$\\exp()$\u95a2\u6570\u304c\u4f7f\u308f\u308c\u3066\u3044\u308b\u304b\u3068\u3044\u3046\u3068\u3001\u5024\u304c\u30de\u30a4\u30ca\u30b9\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u3053\u3068\u3068\u81ea\u5206\u306f\u7406\u89e3\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u304a\u306a\u3058\u307f$\\exp()$\u95a2\u6570\u306f\n![exp-compressor (1).png](https://qiita-image-store.s3.amazonaws.com/0/50670/50ff3c5d-cab5-65c7-f19a-48a383e0f957.png)\n\u306e\u3088\u3046\u306a\u5f62\u306a\u306e\u3067\u3001\u30de\u30a4\u30ca\u30b9\u306e\u5024\u3092\u53d6\u308a\u307e\u305b\u3093\u3002\u3053\u308c\u306b\u3088\u308a\u5024\u304c\u30de\u30a4\u30ca\u30b9\u306b\u306a\u3089\u305a\u3001\u304b\u3064\u7dcf\u548c\u304c\uff11\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u3001\u78ba\u7387\u3068\u89e3\u91c8\u3067\u304d\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002\n\u3055\u3063\u304d\u306e\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306e\u51fa\u529b\u5024$y_k$\u3092\u7528\u3044\u3066\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u95a2\u6570\u306f\n\n```math\nE({\\bf w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\log y_k ({\\bf x}_n, {\\bf w})\n```\n\u3068\u8868\u73fe\u3055\u308c\u307e\u3059\u3002\n\nChainer\u306e\u30b3\u30fc\u30c9\u3067\u8a00\u3046\u3068\u3001\nhttps://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py\n\u306b\u3042\u308b\u3001\n\n```py\ndef forward_cpu(self, inputs):\n        x, t = inputs\n        self.y, = Softmax().forward_cpu((x,))\n        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,\n```\n\u306b\u76f8\u5f53\u3057\u307e\u3059\u3002\n\n\u307e\u305f\u3001`F.accuracy(y, t)`\u306f\u51fa\u529b\u3068\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u7167\u5408\u3057\u3066\u6b63\u7b54\u7387\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n\n##3.3 Optimizer\u306e\u8a2d\u5b9a##\n\n\u3055\u3066\u3001\u30e2\u30c7\u30eb\u304c\u6c7a\u307e\u3063\u305f\u306e\u3067\u8a13\u7df4\u306b\u79fb\u308a\u307e\u3059\u3002\n\u3053\u3053\u3067\u306f\u6700\u9069\u5316\u624b\u6cd5\u3068\u3057\u3066Adam\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\n\n```py\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model.collect_parameters())\n```\nAdam\u306b\u3064\u3044\u3066\u306f[30\u5206\u3067\u308f\u304b\u308bAdam](http://ja.scribd.com/doc/260859670/30minutes-Adam)\u3067[echizen_tm](https://twitter.com/echizen_tm)\u3055\u3093\u304c\u89e3\u8aac\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\n\n#4.\u8a13\u7df4\u306e\u5b9f\u65bd\u3068\u7d50\u679c#\n\n\u4ee5\u4e0a\u306e\u6e96\u5099\u304b\u3089\u3001\u30df\u30cb\u30d0\u30c3\u30c1\u5b66\u7fd2\u3067\u624b\u66f8\u304d\u6570\u5b57\u306e\u5224\u5225\u3092\u5b9f\u65bd\u3057\u3001\u305d\u306e\u7cbe\u5ea6\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\n\n```py\ntrain_loss = []\ntrain_acc  = []\ntest_loss = []\ntest_acc  = []\n\nl1_W = []\nl2_W = []\nl3_W = []\n\n# Learning loop\nfor epoch in xrange(1, n_epoch+1):\n    print 'epoch', epoch\n\n    # training\n    # N\u500b\u306e\u9806\u756a\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3073\u66ff\u3048\u308b\n    perm = np.random.permutation(N)\n    sum_accuracy = 0\n    sum_loss = 0\n    # 0\u301cN\u307e\u3067\u306e\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3054\u3068\u306b\u4f7f\u3063\u3066\u5b66\u7fd2\n    for i in xrange(0, N, batchsize):\n        x_batch = x_train[perm[i:i+batchsize]]\n        y_batch = y_train[perm[i:i+batchsize]]\n\n        # \u52fe\u914d\u3092\u521d\u671f\u5316\n        optimizer.zero_grads()\n        # \u9806\u4f1d\u64ad\u3055\u305b\u3066\u8aa4\u5dee\u3068\u7cbe\u5ea6\u3092\u7b97\u51fa\n        loss, acc = forward(x_batch, y_batch)\n        # \u8aa4\u5dee\u9006\u4f1d\u64ad\u3067\u52fe\u914d\u3092\u8a08\u7b97\n        loss.backward()\n        optimizer.update()\n\n        train_loss.append(loss.data)\n        train_acc.append(acc.data)\n        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n\n    # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u8868\u793a\n    print 'train mean loss={}, accuracy={}'.format(sum_loss / N, sum_accuracy / N)\n\n    # evaluation\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u7b97\u51fa\u3057\u6c4e\u5316\u6027\u80fd\u3092\u78ba\u8a8d\n    sum_accuracy = 0\n    sum_loss     = 0\n    for i in xrange(0, N_test, batchsize):\n        x_batch = x_test[i:i+batchsize]\n        y_batch = y_test[i:i+batchsize]\n\n        # \u9806\u4f1d\u64ad\u3055\u305b\u3066\u8aa4\u5dee\u3068\u7cbe\u5ea6\u3092\u7b97\u51fa\n        loss, acc = forward(x_batch, y_batch, train=False)\n\n        test_loss.append(loss.data)\n        test_acc.append(acc.data)\n        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize\n        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize\n\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u8aa4\u5dee\u3068\u3001\u6b63\u89e3\u7cbe\u5ea6\u3092\u8868\u793a\n    print 'test  mean loss={}, accuracy={}'.format(sum_loss / N_test, sum_accuracy / N_test)\n\n    # \u5b66\u7fd2\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u4fdd\u5b58\n    l1_W.append(model.l1.W)\n    l2_W.append(model.l2.W)\n    l3_W.append(model.l3.W)\n\n# \u7cbe\u5ea6\u3068\u8aa4\u5dee\u3092\u30b0\u30e9\u30d5\u63cf\u753b\nplt.figure(figsize=(8,6))\nplt.plot(range(len(train_acc)), train_acc)\nplt.plot(range(len(test_acc)), test_acc)\nplt.legend([\"train_acc\",\"test_acc\"],loc=4)\nplt.title(\"Accuracy of digit recognition.\")\nplt.plot()\n```\n\nepoch\u6bce\u306e\u30b5\u30de\u30ea\u7d50\u679c\u306f\u3053\u3061\u3089\u3067\u3059\u300220\u56de\u3057\u3057\u306698.5%\u304f\u3089\u3044\u306e\u9ad8\u7cbe\u5ea6\u3067\u5224\u5225\u3067\u304d\u3066\u3044\u307e\u3059\u3002\n\n\n```bash:output\nepoch 1\ntrain mean loss=0.278375425202, accuracy=0.914966667456\ntest  mean loss=0.11533634907, accuracy=0.964300005436\nepoch 2\ntrain mean loss=0.137060894324, accuracy=0.958216670454\ntest  mean loss=0.0765812527167, accuracy=0.976100009084\nepoch 3\ntrain mean loss=0.107826075749, accuracy=0.966816672881\ntest  mean loss=0.0749603212342, accuracy=0.97770000577\nepoch 4\ntrain mean loss=0.0939164237926, accuracy=0.970616674324\ntest  mean loss=0.0672153823725, accuracy=0.980000005364\nepoch 5\ntrain mean loss=0.0831089563683, accuracy=0.973950009048\ntest  mean loss=0.0705943618687, accuracy=0.980100004673\nepoch 6\ntrain mean loss=0.0752325405277, accuracy=0.976883343955\ntest  mean loss=0.0732760328815, accuracy=0.977900006771\nepoch 7\ntrain mean loss=0.0719517664274, accuracy=0.977383343875\ntest  mean loss=0.063611669606, accuracy=0.981900005937\nepoch 8\ntrain mean loss=0.0683009948514, accuracy=0.978566677173\ntest  mean loss=0.0604036964733, accuracy=0.981400005221\nepoch 9\ntrain mean loss=0.0621755663728, accuracy=0.980550010701\ntest  mean loss=0.0591542539285, accuracy=0.982400006652\nepoch 10\ntrain mean loss=0.0618313539471, accuracy=0.981183344225\ntest  mean loss=0.0693172766063, accuracy=0.982900006175\nepoch 11\ntrain mean loss=0.0583098273944, accuracy=0.982000010014\ntest  mean loss=0.0668152360269, accuracy=0.981600006819\nepoch 12\ntrain mean loss=0.054178619228, accuracy=0.983533344865\ntest  mean loss=0.0614466062452, accuracy=0.982900005579\nepoch 13\ntrain mean loss=0.0532431817259, accuracy=0.98390001148\ntest  mean loss=0.060112986485, accuracy=0.98400000751\nepoch 14\ntrain mean loss=0.0538122716064, accuracy=0.983266676267\ntest  mean loss=0.0624165921964, accuracy=0.983300005198\nepoch 15\ntrain mean loss=0.0501562882114, accuracy=0.983833344777\ntest  mean loss=0.0688113694015, accuracy=0.98310000658\nepoch 16\ntrain mean loss=0.0513108611095, accuracy=0.984533343514\ntest  mean loss=0.0724038232205, accuracy=0.982200007439\nepoch 17\ntrain mean loss=0.0471463404785, accuracy=0.985666677058\ntest  mean loss=0.0612579581685, accuracy=0.983600008488\nepoch 18\ntrain mean loss=0.0460166006556, accuracy=0.986050010125\ntest  mean loss=0.0654888718335, accuracy=0.984400007725\nepoch 19\ntrain mean loss=0.0458772557077, accuracy=0.986433342795\ntest  mean loss=0.0602016936944, accuracy=0.984400007129\nepoch 20\ntrain mean loss=0.046333729005, accuracy=0.986433343093\ntest  mean loss=0.0621869922416, accuracy=0.985100006461\n```\n\n\u5404\u30d0\u30c3\u30c1\u6bce\u306e\u5224\u5225\u7cbe\u5ea6\u3068\u3001\u8aa4\u5dee\u306e\u30b0\u30e9\u30d5\u304c\u3053\u3061\u3089\u3067\u3059\u3002\u8d64\u3044\u307b\u3046\u304c\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3001\u9752\u3044\u307b\u3046\u304c\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u306a\u308a\u307e\u3059\u3002\n\n![nn_result-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/fc2a9774-27e7-1155-0891-69ba9a4e7510.png)\n\n\u4ee5\u524d\u3001[\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011k-nearest neighbor method(k\u6700\u8fd1\u508d\u6cd5)\u3092\u81ea\u529b\u3067python\u3067\u66f8\u3044\u3066\u3001\u624b\u66f8\u304d\u6570\u5b57\u306e\u8a8d\u8b58\u3092\u3059\u308b](http://qiita.com/kenmatsu4/items/c91f5740808022decaae)\u3068\u3044\u3046\u8a18\u4e8b\u3067\u3001\u540c\u69d8\u306b\u624b\u66f8\u304d\u6570\u5b57\u306e\u5224\u5225\u3092\u3084\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u6642\u306e\u7cbe\u5ea6\u304c97%\u304f\u3089\u3044\u3060\u3063\u305f\u306e\u3067\u3001\u66f4\u306b\u5c11\u3057\u4e0a\u304c\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\u3053\u306eChiner\u306f\u5168\u3066Python\u30b3\u30fc\u30c9\u3067\u64cd\u4f5c\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u975e\u5e38\u306bPythonista\u3068\u3057\u3066\u306f\u5b09\u3057\u3044\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\u307e\u3060\u3001\u300c\u30c7\u30a3\u30fc\u30d7\u300d\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u304d\u3066\u304a\u3089\u305a\u3001\u305f\u3060\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306a\u306e\u3067\u3001\u8fd1\u3005\u300c\u30c7\u30a3\u30fc\u30d7\u300d\u306a\u3084\u3064\u306e\u8a18\u4e8b\u3082\u66f8\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n#5.\u7b54\u3048\u5408\u308f\u305b#\n\n\u8b58\u5225\u3057\u305f100\u500b\u306e\u6570\u5b57\u3092\u8868\u793a\u3057\u3066\u307f\u307e\u3059\u3002\u30e9\u30f3\u30c0\u30e0\u306b100\u500b\u62bd\u51fa\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u307b\u3068\u3093\u3069\u6b63\u89e3\u3067\u3059\u3002\u4f55\u56de\u304b100\u500b\u8868\u793a\u3092\u884c\u3063\u3066\u3084\u3063\u3068\u9593\u9055\u3063\u3066\u3044\u308b\u3068\u3053\u308d\u3092\uff11\u3064\u8868\u793a\u3067\u304d\u305f\u306e\u3067\u3001\u305d\u306e\u4f8b\u3092\u4e0b\u8a18\u306b\u8cbc\u3063\u3066\u3044\u307e\u3059\u3002\u306a\u3093\u3060\u304b\u4eba\u9593\u306e\u65b9\u304c\u8a66\u3055\u308c\u3066\u3044\u308b\u6c17\u5206\u3067\u3059\uff08\u7b11\uff09\n\n![mnist_ans2-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/cc9626ad-2314-2c7f-3e67-2f78fdd0ed5d.png)\n\uff08\u203b 2\u884c3\u5217\u306e4\u30929\u3068\u8aa4\u8b58\u5225\u3057\u3066\u3044\u307e\u3059\uff09\n\n```py \nplt.style.use('fivethirtyeight')\ndef draw_digit3(data, n, ans, recog):\n    size = 28\n    plt.subplot(10, 10, n)\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(Z)\n    plt.title(\"ans=%d, recog=%d\"%(ans,recog), size=8)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n    \n\nplt.figure(figsize=(15,15))\n\ncnt = 0\nfor idx in np.random.permutation(N)[:100]:\n    \n    xxx = x_train[idx].astype(np.float32)\n    h1 = F.dropout(F.relu(model.l1(Variable(xxx.reshape(1,784)))),  train=False)\n    h2 = F.dropout(F.relu(model.l2(h1)), train=False)\n    y  = model.l3(h2)\n    cnt+=1\n    draw_digit3(x_train[idx], cnt, y_train[idx], np.argmax(y.data))\nplt.show\n```\n\n\n#6.\u7b2c\uff11\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bfw\u306e\u53ef\u8996\u5316#\n\n\u5165\u529b\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf$w^{(1)}$784\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u309228x28\u30d4\u30af\u30bb\u30eb\u3068\u3057\u3066\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u8868\u793a\u3057\u3066\u307f\u307e\u3057\u305f\u30021000\u500b\u306e\u3046\u3061\u30e9\u30f3\u30c0\u30e0\u306b100\u500b\u9078\u3093\u3067\u3044\u307e\u3059\u3002\u3088\u304f\u307f\u308b\u3068\"2\"\u3068\u304b\"5\"\u3068\u304b\"0\"\u306b\u898b\u3048\u308b\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u306d\u30021\u5c64\u76ee\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u7279\u5fb4\u62bd\u51fa\u304c\u3067\u304d\u3066\u3044\u305d\u3046\u306a\u96f0\u56f2\u6c17\u304c\u4f3a\u3048\u307e\u3059\u3002\n\n![param_images-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/02fda82c-851c-b0f4-e257-e287e6ce0741.png)\n\n```py \ndef draw_digit2(data, n, i):\n    size = 28\n    plt.subplot(10, 10, n)\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,27)\n    plt.ylim(0,27)\n    plt.pcolor(Z)\n    plt.title(\"%d\"%i, size=9)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\nplt.figure(figsize=(10,10))\ncnt = 1\nfor i in np.random.permutation(1000)[:100]:\n    draw_digit2(l1_W[len(l1_W)-1][i], cnt, i)\n    cnt += 1\n    \nplt.show()\n```\n\n#7.\u51fa\u529b\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bfw\u306e\u53ef\u8996\u5316#\n\n\u51fa\u529b\u5c64\u306f1000\u500b\u306e\u30a4\u30f3\u30d7\u30c3\u30c8\u3092\u53d7\u3051\u3066\u300110\u500b\u306e\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3092\u884c\u3046\u5c64\u3067\u3059\u304c\u3001\u3053\u3053\u3082\u53ef\u8996\u5316\u3057\u3066\u307f\u307e\u3057\u305f\u3002\"0\"\u3068\u66f8\u3044\u3066\u3042\u308b\u3068\u3053\u308d\u304c\u624b\u66f8\u304d\u6570\u5b57\u3092\"0\"\u3068\u5224\u5225\u3059\u308b\u305f\u3081\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3067\u3059\u3002\n\n1000\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306a\u306e\u3067\u30010\u309224\u500b\u5f8c\u308d\u306b\u3064\u3051\u306632x32\u306e\u753b\u50cf\u306b\u843d\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n![output_layer-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/03dfe1ac-d4b2-883a-cc78-a336e5061cbe.png)\n\n```py \n# \u30ec\u30a4\u30e4\u30fc3\ndef draw_digit2(data, n, i):\n    size = 32\n    plt.subplot(4, 4, n)\n    data = np.r_[data,np.zeros(24)]\n    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix\n    Z = Z[::-1,:]             # flip vertical\n    plt.xlim(0,size-1)\n    plt.ylim(0,size-1)\n    plt.pcolor(Z)\n    plt.title(\"%d\"%i, size=9)\n    plt.gray()\n    plt.tick_params(labelbottom=\"off\")\n    plt.tick_params(labelleft=\"off\")\n\nplt.figure(figsize=(10,10))\ncnt = 1\nfor i in range(10):\n    draw_digit2(l3_W[len(l3_W)-1][i], cnt, i)\n    cnt += 1\n    \nplt.show()\n```\n\n#8.\u304a\u307e\u3051#\n\u4e2d\u9593\u5c64\u306e\u7d20\u5b50\u6570\u3092`[100, 500, 800, 900, 1000, 1100, 1200, 1500, 2000]`\u306b\u3057\u3066\u305d\u308c\u305e\u308c\u5224\u5225\u3057\u3066\u307f\u305f\u3002\u7d50\u679c\u306e\u30b0\u30e9\u30d5\u304c\u4e0b\u8a18\u3067\u3059\u3002\u7d20\u5b50\u6570500\u4ee5\u4e0a\u3067\u6982\u306d98%\u3092\u9054\u6210\u3057\u3066\u304a\u308a\u3001\u305d\u308c\u4ee5\u4e0a\u306e\u7d20\u5b50\u6570\u306f\u3042\u3093\u307e\u308a\u5909\u308f\u3089\u306a\u3044\u307f\u305f\u3044\u3067\u3059\u306d\u3002\n\n![total_result.png](https://qiita-image-store.s3.amazonaws.com/0/50670/c8e2fadf-362b-bb50-237d-875a57a955e0.png)\n\n\n#9.\u304a\u307e\u30512 : \u6d3b\u6027\u5316\u95a2\u6570#\n\nChainer\u306b\u30d7\u30ea\u30a4\u30f3\u3055\u308c\u3066\u3044\u308b\u4e3b\u306a\u6d3b\u6027\u5316\u95a2\u6570\u306b\n\n* ReLu function\n* tanh function\n* sigmoid function\n\n\u304c\u3042\u308a\u307e\u3059\u3002\u56f3\u793a\u3059\u308b\u3068\u4e0b\u8a18\u306e\u3088\u3046\u306a\u5f62\u3067\u3059\u3002\n\u7d20\u5b50\u306e\u5165\u529b\u3068\u51fa\u529b\u306e\u9593\u306b\u5165\u308b\u95a2\u6570\u3067\u3001\u5165\u51fa\u529b\u306b\u95a2\u3059\u308b\u95be\u5024\u3092\u8a2d\u5b9a\u3059\u308b\u3088\u3046\u306a\u5f79\u5272\u3092\u6301\u3061\u307e\u3059\u3002\n\n![activation_func.png](https://qiita-image-store.s3.amazonaws.com/0/50670/ced9f1d5-5ec2-6086-a142-9833614e3ee6.png)\n\n```py \n# \u6d3b\u6027\u5316\u95a2\u6570\u30c6\u30b9\u30c8\nx_data = np.linspace(-10, 10, 100, dtype=np.float32)\nx = Variable(x_data)\n\ny = F.relu(x)\nplt.figure(figsize=(8,15))\nplt.subplot(311)\nplt.title(\"ReLu function.\")\nplt.ylim(-2,10)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\n\ny = F.tanh(x)\nplt.subplot(312)\nplt.title(\"tanh function.\")\nplt.ylim(-1.5,1.5)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\n\ny = F.sigmoid(x)\nplt.subplot(313)\nplt.title(\"sigmoid function.\")\nplt.ylim(-.2,1.2)\nplt.xlim(-6,6)\nplt.plot(x.data, y.data)\nplt.show()\n```\n\n\u6b21\u306e\u8a18\u4e8b\n\u300c[\u3010\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3011Chainer\u3067Autoencoder\u3092\u8a66\u3057\u3066\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u308b\u3002](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)\u300d\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u7279\u5fb4\u62bd\u51fa\u3092\u81ea\u52d5\u5316\u3059\u308b\u6280\u8853\u306eAutoencoder\u3092\u5b9f\u88c5\u3057\u3066\u307f\u305f\u8a18\u4e8b\u3067\u3059\u3002\n\n\u3010\u53c2\u8003\u66f8\u7c4d\u3011\n\u3000\u6df1\u5c64\u5b66\u7fd2\uff08\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba\uff09 \u5ca1\u8c37\u8cb4\u4e4b\n\u3000\n\n\u3010\u53c2\u8003web\u30b5\u30a4\u30c8\u3011\n\u3000Chainer\u306e\u30e1\u30a4\u30f3\u30b5\u30a4\u30c8\n\u3000\u3000\u3000http://chainer.org/\n\u3000Chainer\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\n\u3000\u3000\u3000https://github.com/pfnet/chainer\n\u3000Chainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3068\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\n\u3000\u3000\u3000http://docs.chainer.org/en/latest/\n\u3000\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"\n\u3000 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov \n\u3000\u3000\u3000http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n"}