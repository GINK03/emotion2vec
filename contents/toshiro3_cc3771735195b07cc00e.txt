{"context": "\n\n\u306f\u3058\u3081\u306b\nCDH(Cloudera's Distribution Including Apache Hadoop)4\u3092\u4f7f\u7528\u3057\u3066hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\n\u74b0\u5883\n\nCentOS 6.5\nCDH 4.7.0\njdk 1.7.0_55\n\n\n\u69cb\u6210\n\n\n\n\u5f79\u5272\n\u30db\u30b9\u30c8\u540d\nIP\u30a2\u30c9\u30ec\u30b9\n\n\n\n\nmaster\nhadoop-master\n192.168.122.11\n\n\nslave\nhadoop-slave\n192.168.122.21\n\n\nslave\nhadoop-slave2\n192.168.122.22\n\n\nclient\nhadoop-client\n192.168.122.101\n\n\n\n\njdk\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nCDH4\u3067\u306f\u3001jdk1.7.0_55\u3067\u306e\u52d5\u4f5c\u3092\u4fdd\u8a3c\u3057\u3066\u3044\u308b\u3088\u3046\u306a\u306e\u3067\u3001\u5f53\u8a72\u30d0\u30fc\u30b8\u30e7\u30f3\u306ejdk\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n$ sudo yum localinstall jdk-7u55-linux-x64.rpm\njava version \"1.7.0_55\"\nJava(TM) SE Runtime Environment (build 1.7.0_55-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)\n\n\nCDH4\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nyum\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u8ffd\u52a0\n$ wget http://archive.cloudera.com/cdh4/one-click-install/redhat/6/x86_64/cloudera-cdh-4-0.x86_64.rpm\n$ sudo yum localinstall cloudera-cdh-4-0.x86_64.rpm\n$ sudo yum clean all\n$ yum repolist\nrepo id              repo name                                            status\nbase                 CentOS-6 - Base                                      6,367\ncloudera-cdh4        Cloudera's Distribution for Hadoop, Version 4          110\nextras               CentOS-6 - Extras                                       15\nupdates              CentOS-6 - Updates                                   1,487\nrepolist: 7,979\n\n\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a2d\u5b9a\n\n\u540d\u524d\u89e3\u6c7a\u304c\u884c\u3048\u308b\u3088\u3046\u306b/etc/hosts\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30db\u30b9\u30c8\u540d\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n\n/etc/hosts\n192.168.122.11  hadoop-master\n192.168.122.21  hadoop-slave\n192.168.122.22  hadoop-slave2\n192.168.122.101 hadoop-client\n\n\n\nHDFS\u306e\u8a2d\u5b9a\n\n\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u30de\u30b9\u30bf\u306e\u5834\u5408\n$ sudo yum install hadoop hadoop-hdfs hadoop-hdfs-namenode\n$ sudo yum install hadoop-yarn hadoop-yarn-resourcemanager\n$ sudo yum install hadoop-mapreduce hadoop-mapreduce-historyserver\n\n\u30b9\u30ec\u30fc\u30d6\u306e\u5834\u5408\n$ sudo yum install hadoop hadoop-hdfs hadoop-hdfs-datanode\n$ sudo yum install hadoop-yarn hadoop-yarn-nodemanager\n$ sudo yum install hadoop-mapreduce\n\n\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u5834\u5408\n$ sudo yum install hadoop hadoop-hdfs hadoop-mapreduce hadoop-yarn hadoop-client\n\n\n\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u3072\u306a\u578b\u3092\u30b3\u30d4\u30fc\u3057\u307e\u3059\n(\u30af\u30e9\u30b9\u30bf\u5168\u4f53)\n\n$ alternatives --display hadoop-conf\nhadoop-conf - status is auto.\n link currently points to /etc/hadoop/conf.empty\n/etc/hadoop/conf.empty - priority 10\nCurrent `best' version is /etc/hadoop/conf.empty.\n\n$ sudo cp -rp /etc/hadoop/conf.empty /etc/hadoop/conf.cluster\n$ sudo alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.cluster 50\n$ sudo alternatives --set hadoop-conf /etc/hadoop/conf.cluster\n$ alternatives --display hadoop-conf\nhadoop-conf - status is manual.\n link currently points to /etc/hadoop/conf.cluster\n/etc/hadoop/conf.empty - priority 10\n/etc/hadoop/conf.cluster - priority 50\nCurrent `best' version is /etc/hadoop/conf.cluster.\n\n\ncore-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n\n/etc/hadoop/conf.cluster/core-site.xml\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://hadoop-master:8020</value>\n  </property>\n</configuration>\n\n\n\nhdfs-site.xml\u3092\u8a2d\u5b9a\u3059\u308b\n\n\n/etc/hadoop/conf.cluster/hdfs-site.xml\n<configuration>\n  <property>\n    <name>dfs.permissions.superusergroup</name>\n    <value>hadoop</value>\n  </property>\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>/var/lib/hadoop-hdfs/nn</value>\n  </property>\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>/var/lib/hadoop-hdfs/dn</value>\n  </property>\n</configuration>\n\n\n\n\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n$ sudo mkdir -p /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n$ sudo chown -R hdfs:hdfs /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n$ sudo chmod 700 /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n\n\n\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3057\u307e\u3059\u3002\n\n$ sudo -u hdfs hdfs namenode -format\n14/09/12 06:48:11 INFO namenode.NameNode: STARTUP_MSG:\n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-master/192.168.122.11\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 2.0.0-cdh4.7.0\nSTARTUP_MSG:   classpath = /etc/hadoop/conf:..(\u7701\u7565)..\nSTARTUP_MSG:   java = 1.7.0_55\n************************************************************/\n14/09/12 06:48:11 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n14/09/12 06:48:12 WARN common.Util: Path /var/lib/hadoop-hdfs/nn should be specified as a URI in configuration files. Please update hdfs configuration.\n14/09/12 06:48:12 WARN common.Util: Path /var/lib/hadoop-hdfs/nn should be specified as a URI in configuration files. Please update hdfs configuration.\nFormatting using clusterid: CID-12509087-4dbc-4977-94ae-134068d9a02f\n14/09/12 06:48:12 INFO namenode.FSNamesystem: fsLock is fair:true\n14/09/12 06:48:12 INFO blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to 30000 since dfs.namenode.stale.datanode.interval is less than dfs.namenode.heartbeat.recheck-interval\n14/09/12 06:48:12 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n14/09/12 06:48:12 INFO util.GSet: Computing capacity for map BlocksMap\n14/09/12 06:48:12 INFO util.GSet: VM type       = 64-bit\n14/09/12 06:48:12 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB\n14/09/12 06:48:12 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: defaultReplication         = 3\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxReplication             = 512\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: minReplication             = 1\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n14/09/12 06:48:13 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)\n14/09/12 06:48:13 INFO namenode.FSNamesystem: supergroup          = hadoop\n14/09/12 06:48:13 INFO namenode.FSNamesystem: isPermissionEnabled = true\n14/09/12 06:48:13 INFO namenode.FSNamesystem: HA Enabled: false\n14/09/12 06:48:13 INFO namenode.FSNamesystem: Append Enabled: true\n14/09/12 06:48:13 INFO namenode.NameNode: Caching file names occuring more than 10 times\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n14/09/12 06:48:13 INFO namenode.NNStorage: Storage directory /var/lib/hadoop-hdfs/nn has been successfully formatted.\n14/09/12 06:48:13 INFO namenode.FSImage: Saving image file /var/lib/hadoop-hdfs/nn/current/fsimage.ckpt_0000000000000000000 using no compression\n14/09/12 06:48:13 INFO namenode.FSImage: Image file of size 115 saved in 0 seconds.\n14/09/12 06:48:13 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n14/09/12 06:48:13 INFO util.ExitUtil: Exiting with status 0\n14/09/12 06:48:13 INFO namenode.NameNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at hadoop-master/192.168.122.11\n************************************************************/\n\n\nMapReduce2 (YARN)\u306e\u8a2d\u5b9a\n\nmapred-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n\n/etc/hadoop/conf.cluster/mapred-site.xml\n<configuration>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>hadoop-master:10020</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>hadoop-master:19888</value>\n  </property>\n</configuration>\n\n\n\nyarn-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n\n/etc/hadoop/conf.cluster/yarn-site.xml\n<configuration>\n  <property>\n    <name>yarn.nodemanager.aux-services</name>\n    <value>mapreduce.shuffle</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.address</name>\n    <value>hadoop-master:8032</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.scheduler.address</name>\n    <value>hadoop-master:8030</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.resource-tracker.address</name>\n    <value>hadoop-master:8031</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.admin.address</name>\n    <value>hadoop-master:8033</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.webapp.address</name>\n    <value>hadoop-master:8089</value>\n  </property>\n  <property>\n    <name>yarn.log-aggregation-enable</name>\n    <value>true</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.local-dirs</name>\n    <value>/var/lib/hadoop-yarn/nm/local</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.log-dirs</name>\n    <value>/var/log/hadoop-yarn/nm</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.remote-app-log-dir</name>\n    <value>hdfs://hadoop-master:8020/var/log/hadoop-yarn/apps</value>\n  </property>\n  <property>\n    <name>yarn.app.mapreduce.am.staging-dir</name>\n    <value>/user</value>\n  </property>\n  <property>\n    <name>yarn.application.classpath</name>\n    <value>\n      $HADOOP_CONF_DIR,\n      $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\n      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\n      $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\n      $YARN_HOME/*, $YARN_HOME/lib/*\n    </value>\n  </property>\n</configuration>\n\n\n\n\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n$ sudo mkdir -p /var/lib/hadoop-yarn/nm/local /var/log/hadoop-yarn/nm\n$ sudo chown -R yarn:yarn /var/lib/hadoop-yarn/nm/local /var/log/hadoop-yarn/nm\n\n\n\u30e6\u30fc\u30b6\u306e\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-hdfs\n$ sudo chown hdfs:hdfs /var/lib/hadoop-hdfs/.bash*\n\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-mapreduce\n\n$ sudo chown mapred:mapred /var/lib/hadoop-mapreduce/.bash*\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-yarn\n$ sudo chown yarn:yarn /var/lib/hadoop-yarn/.bash*\n\n\nhadoop-env.sh\u3092\u8a2d\u5b9a\u3059\u308b\n\n$ sudo sh -c \"echo 'export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce' >> /etc/hadoop/conf.cluster/hadoop-env.sh\"\n\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master\u306e\u307f)\n\n$ sudo service hadoop-hdfs-namenode start\n\n\n\u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2)\n\n$ sudo service hadoop-hdfs-datanode start\n\n\n\u30b7\u30b9\u30c6\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n$ sudo su - hdfs\n$ hadoop fs -mkdir /tmp\n$ hadoop fs -chmod -R 1777 /tmp\n$ hadoop fs -mkdir /user/history\n$ hadoop fs -chmod -R 1777 /user/history\n$ hadoop fs -chown mapred:hadoop /user/history\n$ hadoop fs -mkdir /var/log/hadoop-yarn\n$ hadoop fs -chown yarn:mapred /var/log/hadoop-yarn\n\n$ hadoop fs -ls -R /\ndrwxrwxrwt   - hdfs hadoop          0 2014-09-13 14:30 /tmp\ndrwxr-xr-x   - hdfs hadoop          0 2014-09-13 14:33 /user\ndrwxrwxrwt   - mapred hadoop          0 2014-09-13 14:33 /user/history\ndrwxr-xr-x   - hdfs   hadoop          0 2014-09-13 14:33 /var\ndrwxr-xr-x   - hdfs   hadoop          0 2014-09-13 14:33 /var/log\ndrwxr-xr-x   - yarn   mapred          0 2014-09-13 14:33 /var/log/hadoop-yarn\n\n\n\u30ea\u30bd\u30fc\u30b9\u30de\u30cd\u30fc\u30b8\u30e3\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master)\n\n$ sudo service hadoop-yarn-resourcemanager start\n\n\n\u30ce\u30fc\u30c9\u30de\u30cd\u30fc\u30b8\u30e3\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave, hadoop-slave2)\n\n$ sudo service hadoop-yarn-nodemanager start\n\n\n\u30b8\u30e7\u30d6\u30d2\u30b9\u30c8\u30ea\u30fc\u30b5\u30fc\u30d0\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master)\n\n$ sudo service hadoop-mapreduce-historyserver start\n\n\n\u52d5\u4f5c\u78ba\u8a8d\n\nHDFS\u3092\u64cd\u4f5c\u3059\u308b\n\nhdfs\u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n$ sudo su - hdfs\n$ hadoop fs -mkdir /user/hdfs\n$ hadoop fs -ls /user/hdfs\nFound 1 items\ndrwxr-xr-x   - hdfs hadoop          0 2014-09-13 18:59 /user/hdfs/input\n\n$ hadoop fs -mkdir input\n$ hostname > hostname.txt\n$ hadoop fs -put hostname.txt input\n$ hadoop fs -ls input\nFound 1 items\n-rw-r--r--   3 hdfs hadoop         16 2014-09-13 18:59 input/hostname.txt\n\n\n\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\nmapred\u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n$ sudo su - mapred\n$ hadoop -jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-example.jar pi 5 300\nNumber of Maps  = 5\nSamples per Map = 300\nWrote input for Map #0\nWrote input for Map #1\nWrote input for Map #2\nWrote input for Map #3\nWrote input for Map #4\nStarting Job\n14/09/13 19:31:43 INFO service.AbstractService: Service:org.apache.hadoop.yarn.client.YarnClientImpl is inited.\n14/09/13 19:31:43 INFO service.AbstractService: Service:org.apache.hadoop.yarn.client.YarnClientImpl is started.\n14/09/13 19:31:44 INFO input.FileInputFormat: Total input paths to process : 5\n14/09/13 19:31:44 INFO mapreduce.JobSubmitter: number of splits:5\n14/09/13 19:31:44 WARN conf.Configuration: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n14/09/13 19:31:44 WARN conf.Configuration: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n14/09/13 19:31:44 WARN conf.Configuration: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.inputformat.class is deprecated. Instead, use mapreduce.job.inputformat.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir\n14/09/13 19:31:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1410586655016_0005\n14/09/13 19:31:45 INFO client.YarnClientImpl: Submitted application application_1410586655016_0005 to ResourceManager at hadoop-master/192.168.122.11:8032\n14/09/13 19:31:45 INFO mapreduce.Job: The url to track the job: http://hadoop-master:8089/proxy/application_1410586655016_0005/\n14/09/13 19:31:45 INFO mapreduce.Job: Running job: job_1410586655016_0005\n14/09/13 19:32:03 INFO mapreduce.Job: Job job_1410586655016_0005 running in uber mode : false\n14/09/13 19:32:03 INFO mapreduce.Job:  map 0% reduce 0%\n14/09/13 19:33:15 INFO mapreduce.Job:  map 20% reduce 0%\n14/09/13 19:33:16 INFO mapreduce.Job:  map 40% reduce 0%\n14/09/13 19:33:17 INFO mapreduce.Job:  map 100% reduce 0%\n14/09/13 19:33:28 INFO mapreduce.Job:  map 100% reduce 100%\n14/09/13 19:33:28 INFO mapreduce.Job: Job job_1410586655016_0005 completed successfully\n14/09/13 19:33:28 INFO mapreduce.Job: Counters: 43\n        File System Counters\n                FILE: Number of bytes read=116\n                FILE: Number of bytes written=440147\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=1315\n                HDFS: Number of bytes written=215\n                HDFS: Number of read operations=23\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=3\n        Job Counters\n                Launched map tasks=5\n                Launched reduce tasks=1\n                Data-local map tasks=5\n                Total time spent by all maps in occupied slots (ms)=363619\n                Total time spent by all reduces in occupied slots (ms)=11396\n        Map-Reduce Framework\n                Map input records=5\n                Map output records=10\n                Map output bytes=90\n                Map output materialized bytes=140\n                Input split bytes=725\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=2\n                Reduce shuffle bytes=140\n                Reduce input records=10\n                Reduce output records=0\n                Spilled Records=20\n                Shuffled Maps =5\n                Failed Shuffles=0\n                Merged Map outputs=5\n                GC time elapsed (ms)=6962\n                CPU time spent (ms)=19280\n                Physical memory (bytes) snapshot=761933824\n                Virtual memory (bytes) snapshot=3770851328\n                Total committed heap usage (bytes)=619794432\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters\n                Bytes Read=590\n        File Output Format Counters\n                Bytes Written=97\nJob Finished in 105.013 seconds\nEstimated value of Pi is 3.15200000000000000000\n\n\n\u53c2\u8003\n\nCDH4 Installation Guide\nSupported JDK Versions\nJava SE 7 Downloads\nInstalling CDH4\nPorts Used by Components of CDH4\nDeploying HDFS on a Cluster\nDeploying MapReduce v2 (YARN) on a Cluster\n\n\n## \u306f\u3058\u3081\u306b\n\nCDH(Cloudera's Distribution Including Apache Hadoop)4\u3092\u4f7f\u7528\u3057\u3066hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\n## \u74b0\u5883\n\n* CentOS 6.5\n* CDH 4.7.0\n* jdk 1.7.0_55\n\n## \u69cb\u6210\n\n|\u5f79\u5272  |\u30db\u30b9\u30c8\u540d     |IP\u30a2\u30c9\u30ec\u30b9     |\n|:-----|:------------|:--------------|\n|master|hadoop-master|192.168.122.11 |\n|slave |hadoop-slave |192.168.122.21 |\n|slave |hadoop-slave2|192.168.122.22 |\n|client|hadoop-client|192.168.122.101|\n\n## jdk\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nCDH4\u3067\u306f\u3001jdk1.7.0_55\u3067\u306e\u52d5\u4f5c\u3092\u4fdd\u8a3c\u3057\u3066\u3044\u308b\u3088\u3046\u306a\u306e\u3067\u3001\u5f53\u8a72\u30d0\u30fc\u30b8\u30e7\u30f3\u306ejdk\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo yum localinstall jdk-7u55-linux-x64.rpm\njava version \"1.7.0_55\"\nJava(TM) SE Runtime Environment (build 1.7.0_55-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)\n```\n\n## CDH4\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n### yum\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u8ffd\u52a0\n\n```shell\n$ wget http://archive.cloudera.com/cdh4/one-click-install/redhat/6/x86_64/cloudera-cdh-4-0.x86_64.rpm\n$ sudo yum localinstall cloudera-cdh-4-0.x86_64.rpm\n$ sudo yum clean all\n$ yum repolist\nrepo id              repo name                                            status\nbase                 CentOS-6 - Base                                      6,367\ncloudera-cdh4        Cloudera's Distribution for Hadoop, Version 4          110\nextras               CentOS-6 - Extras                                       15\nupdates              CentOS-6 - Updates                                   1,487\nrepolist: 7,979\n```\n\n### \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a2d\u5b9a\n\n* \u540d\u524d\u89e3\u6c7a\u304c\u884c\u3048\u308b\u3088\u3046\u306b/etc/hosts\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30db\u30b9\u30c8\u540d\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n```shell:/etc/hosts\n192.168.122.11  hadoop-master\n192.168.122.21  hadoop-slave\n192.168.122.22  hadoop-slave2\n192.168.122.101 hadoop-client\n```\n\n### HDFS\u306e\u8a2d\u5b9a\n\n* \u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u30de\u30b9\u30bf\u306e\u5834\u5408\n\n```shell\n$ sudo yum install hadoop hadoop-hdfs hadoop-hdfs-namenode\n$ sudo yum install hadoop-yarn hadoop-yarn-resourcemanager\n$ sudo yum install hadoop-mapreduce hadoop-mapreduce-historyserver\n```\n\n\u30b9\u30ec\u30fc\u30d6\u306e\u5834\u5408\n\n```shell\n$ sudo yum install hadoop hadoop-hdfs hadoop-hdfs-datanode\n$ sudo yum install hadoop-yarn hadoop-yarn-nodemanager\n$ sudo yum install hadoop-mapreduce\n```\n\n\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u5834\u5408\n\n```shell\n$ sudo yum install hadoop hadoop-hdfs hadoop-mapreduce hadoop-yarn hadoop-client\n```\n\n* \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u3072\u306a\u578b\u3092\u30b3\u30d4\u30fc\u3057\u307e\u3059\n(\u30af\u30e9\u30b9\u30bf\u5168\u4f53)\n\n```shell\n$ alternatives --display hadoop-conf\nhadoop-conf - status is auto.\n link currently points to /etc/hadoop/conf.empty\n/etc/hadoop/conf.empty - priority 10\nCurrent `best' version is /etc/hadoop/conf.empty.\n\n$ sudo cp -rp /etc/hadoop/conf.empty /etc/hadoop/conf.cluster\n$ sudo alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.cluster 50\n$ sudo alternatives --set hadoop-conf /etc/hadoop/conf.cluster\n$ alternatives --display hadoop-conf\nhadoop-conf - status is manual.\n link currently points to /etc/hadoop/conf.cluster\n/etc/hadoop/conf.empty - priority 10\n/etc/hadoop/conf.cluster - priority 50\nCurrent `best' version is /etc/hadoop/conf.cluster.\n```\n* core-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n```xml:/etc/hadoop/conf.cluster/core-site.xml\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://hadoop-master:8020</value>\n  </property>\n</configuration>\n```\n* hdfs-site.xml\u3092\u8a2d\u5b9a\u3059\u308b\n\n```xml:/etc/hadoop/conf.cluster/hdfs-site.xml\n<configuration>\n  <property>\n    <name>dfs.permissions.superusergroup</name>\n    <value>hadoop</value>\n  </property>\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>/var/lib/hadoop-hdfs/nn</value>\n  </property>\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>/var/lib/hadoop-hdfs/dn</value>\n  </property>\n</configuration>\n```\n\n* \u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo mkdir -p /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n$ sudo chown -R hdfs:hdfs /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n$ sudo chmod 700 /var/lib/hadoop-hdfs/nn /var/lib/hadoop-hdfs/dn\n```\n\n* \u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo -u hdfs hdfs namenode -format\n14/09/12 06:48:11 INFO namenode.NameNode: STARTUP_MSG:\n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-master/192.168.122.11\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 2.0.0-cdh4.7.0\nSTARTUP_MSG:   classpath = /etc/hadoop/conf:..(\u7701\u7565)..\nSTARTUP_MSG:   java = 1.7.0_55\n************************************************************/\n14/09/12 06:48:11 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n14/09/12 06:48:12 WARN common.Util: Path /var/lib/hadoop-hdfs/nn should be specified as a URI in configuration files. Please update hdfs configuration.\n14/09/12 06:48:12 WARN common.Util: Path /var/lib/hadoop-hdfs/nn should be specified as a URI in configuration files. Please update hdfs configuration.\nFormatting using clusterid: CID-12509087-4dbc-4977-94ae-134068d9a02f\n14/09/12 06:48:12 INFO namenode.FSNamesystem: fsLock is fair:true\n14/09/12 06:48:12 INFO blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to 30000 since dfs.namenode.stale.datanode.interval is less than dfs.namenode.heartbeat.recheck-interval\n14/09/12 06:48:12 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n14/09/12 06:48:12 INFO util.GSet: Computing capacity for map BlocksMap\n14/09/12 06:48:12 INFO util.GSet: VM type       = 64-bit\n14/09/12 06:48:12 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB\n14/09/12 06:48:12 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: defaultReplication         = 3\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxReplication             = 512\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: minReplication             = 1\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n14/09/12 06:48:12 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n14/09/12 06:48:13 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)\n14/09/12 06:48:13 INFO namenode.FSNamesystem: supergroup          = hadoop\n14/09/12 06:48:13 INFO namenode.FSNamesystem: isPermissionEnabled = true\n14/09/12 06:48:13 INFO namenode.FSNamesystem: HA Enabled: false\n14/09/12 06:48:13 INFO namenode.FSNamesystem: Append Enabled: true\n14/09/12 06:48:13 INFO namenode.NameNode: Caching file names occuring more than 10 times\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n14/09/12 06:48:13 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n14/09/12 06:48:13 INFO namenode.NNStorage: Storage directory /var/lib/hadoop-hdfs/nn has been successfully formatted.\n14/09/12 06:48:13 INFO namenode.FSImage: Saving image file /var/lib/hadoop-hdfs/nn/current/fsimage.ckpt_0000000000000000000 using no compression\n14/09/12 06:48:13 INFO namenode.FSImage: Image file of size 115 saved in 0 seconds.\n14/09/12 06:48:13 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n14/09/12 06:48:13 INFO util.ExitUtil: Exiting with status 0\n14/09/12 06:48:13 INFO namenode.NameNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at hadoop-master/192.168.122.11\n************************************************************/\n```\n\n### MapReduce2 (YARN)\u306e\u8a2d\u5b9a\n\n* mapred-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n```xml:/etc/hadoop/conf.cluster/mapred-site.xml\n<configuration>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>hadoop-master:10020</value>\n  </property>\n  <property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>hadoop-master:19888</value>\n  </property>\n</configuration>\n```\n\n* yarn-site.xml\u3092\u8a2d\u5b9a\u3057\u307e\u3059\n\n```xml:/etc/hadoop/conf.cluster/yarn-site.xml\n<configuration>\n  <property>\n    <name>yarn.nodemanager.aux-services</name>\n    <value>mapreduce.shuffle</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.address</name>\n    <value>hadoop-master:8032</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.scheduler.address</name>\n    <value>hadoop-master:8030</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.resource-tracker.address</name>\n    <value>hadoop-master:8031</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.admin.address</name>\n    <value>hadoop-master:8033</value>\n  </property>\n  <property>\n    <name>yarn.resourcemanager.webapp.address</name>\n    <value>hadoop-master:8089</value>\n  </property>\n  <property>\n    <name>yarn.log-aggregation-enable</name>\n    <value>true</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.local-dirs</name>\n    <value>/var/lib/hadoop-yarn/nm/local</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.log-dirs</name>\n    <value>/var/log/hadoop-yarn/nm</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.remote-app-log-dir</name>\n    <value>hdfs://hadoop-master:8020/var/log/hadoop-yarn/apps</value>\n  </property>\n  <property>\n    <name>yarn.app.mapreduce.am.staging-dir</name>\n    <value>/user</value>\n  </property>\n  <property>\n    <name>yarn.application.classpath</name>\n    <value>\n      $HADOOP_CONF_DIR,\n      $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\n      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\n      $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\n      $YARN_HOME/*, $YARN_HOME/lib/*\n    </value>\n  </property>\n</configuration>\n```\n\n* \u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo mkdir -p /var/lib/hadoop-yarn/nm/local /var/log/hadoop-yarn/nm\n$ sudo chown -R yarn:yarn /var/lib/hadoop-yarn/nm/local /var/log/hadoop-yarn/nm\n```\n\n* \u30e6\u30fc\u30b6\u306e\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-hdfs\n$ sudo chown hdfs:hdfs /var/lib/hadoop-hdfs/.bash*\n\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-mapreduce\n\n$ sudo chown mapred:mapred /var/lib/hadoop-mapreduce/.bash*\n$ sudo cp -p /etc/skel/.bash* /var/lib/hadoop-yarn\n$ sudo chown yarn:yarn /var/lib/hadoop-yarn/.bash*\n```\n* hadoop-env.sh\u3092\u8a2d\u5b9a\u3059\u308b\n\n```shell\n$ sudo sh -c \"echo 'export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce' >> /etc/hadoop/conf.cluster/hadoop-env.sh\"\n```\n\n* \u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master\u306e\u307f)\n\n```shell\n$ sudo service hadoop-hdfs-namenode start\n```\n\n* \u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2)\n\n```shell\n$ sudo service hadoop-hdfs-datanode start\n```\n\n* \u30b7\u30b9\u30c6\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - hdfs\n$ hadoop fs -mkdir /tmp\n$ hadoop fs -chmod -R 1777 /tmp\n$ hadoop fs -mkdir /user/history\n$ hadoop fs -chmod -R 1777 /user/history\n$ hadoop fs -chown mapred:hadoop /user/history\n$ hadoop fs -mkdir /var/log/hadoop-yarn\n$ hadoop fs -chown yarn:mapred /var/log/hadoop-yarn\n\n$ hadoop fs -ls -R /\ndrwxrwxrwt   - hdfs hadoop          0 2014-09-13 14:30 /tmp\ndrwxr-xr-x   - hdfs hadoop          0 2014-09-13 14:33 /user\ndrwxrwxrwt   - mapred hadoop          0 2014-09-13 14:33 /user/history\ndrwxr-xr-x   - hdfs   hadoop          0 2014-09-13 14:33 /var\ndrwxr-xr-x   - hdfs   hadoop          0 2014-09-13 14:33 /var/log\ndrwxr-xr-x   - yarn   mapred          0 2014-09-13 14:33 /var/log/hadoop-yarn\n```\n\n* \u30ea\u30bd\u30fc\u30b9\u30de\u30cd\u30fc\u30b8\u30e3\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master)\n\n```shell\n$ sudo service hadoop-yarn-resourcemanager start\n```\n\n* \u30ce\u30fc\u30c9\u30de\u30cd\u30fc\u30b8\u30e3\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave, hadoop-slave2)\n\n```shell\n$ sudo service hadoop-yarn-nodemanager start\n```\n\n* \u30b8\u30e7\u30d6\u30d2\u30b9\u30c8\u30ea\u30fc\u30b5\u30fc\u30d0\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master)\n\n```shell\n$ sudo service hadoop-mapreduce-historyserver start\n```\n## \u52d5\u4f5c\u78ba\u8a8d\n\n* HDFS\u3092\u64cd\u4f5c\u3059\u308b\n\nhdfs\u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - hdfs\n$ hadoop fs -mkdir /user/hdfs\n$ hadoop fs -ls /user/hdfs\nFound 1 items\ndrwxr-xr-x   - hdfs hadoop          0 2014-09-13 18:59 /user/hdfs/input\n\n$ hadoop fs -mkdir input\n$ hostname > hostname.txt\n$ hadoop fs -put hostname.txt input\n$ hadoop fs -ls input\nFound 1 items\n-rw-r--r--   3 hdfs hadoop         16 2014-09-13 18:59 input/hostname.txt\n```\n\n* \u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\nmapred\u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - mapred\n$ hadoop -jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-example.jar pi 5 300\nNumber of Maps  = 5\nSamples per Map = 300\nWrote input for Map #0\nWrote input for Map #1\nWrote input for Map #2\nWrote input for Map #3\nWrote input for Map #4\nStarting Job\n14/09/13 19:31:43 INFO service.AbstractService: Service:org.apache.hadoop.yarn.client.YarnClientImpl is inited.\n14/09/13 19:31:43 INFO service.AbstractService: Service:org.apache.hadoop.yarn.client.YarnClientImpl is started.\n14/09/13 19:31:44 INFO input.FileInputFormat: Total input paths to process : 5\n14/09/13 19:31:44 INFO mapreduce.JobSubmitter: number of splits:5\n14/09/13 19:31:44 WARN conf.Configuration: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n14/09/13 19:31:44 WARN conf.Configuration: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative\n14/09/13 19:31:44 WARN conf.Configuration: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.inputformat.class is deprecated. Instead, use mapreduce.job.inputformat.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n14/09/13 19:31:44 WARN conf.Configuration: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n14/09/13 19:31:44 WARN conf.Configuration: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n14/09/13 19:31:44 WARN conf.Configuration: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir\n14/09/13 19:31:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1410586655016_0005\n14/09/13 19:31:45 INFO client.YarnClientImpl: Submitted application application_1410586655016_0005 to ResourceManager at hadoop-master/192.168.122.11:8032\n14/09/13 19:31:45 INFO mapreduce.Job: The url to track the job: http://hadoop-master:8089/proxy/application_1410586655016_0005/\n14/09/13 19:31:45 INFO mapreduce.Job: Running job: job_1410586655016_0005\n14/09/13 19:32:03 INFO mapreduce.Job: Job job_1410586655016_0005 running in uber mode : false\n14/09/13 19:32:03 INFO mapreduce.Job:  map 0% reduce 0%\n14/09/13 19:33:15 INFO mapreduce.Job:  map 20% reduce 0%\n14/09/13 19:33:16 INFO mapreduce.Job:  map 40% reduce 0%\n14/09/13 19:33:17 INFO mapreduce.Job:  map 100% reduce 0%\n14/09/13 19:33:28 INFO mapreduce.Job:  map 100% reduce 100%\n14/09/13 19:33:28 INFO mapreduce.Job: Job job_1410586655016_0005 completed successfully\n14/09/13 19:33:28 INFO mapreduce.Job: Counters: 43\n        File System Counters\n                FILE: Number of bytes read=116\n                FILE: Number of bytes written=440147\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=1315\n                HDFS: Number of bytes written=215\n                HDFS: Number of read operations=23\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=3\n        Job Counters\n                Launched map tasks=5\n                Launched reduce tasks=1\n                Data-local map tasks=5\n                Total time spent by all maps in occupied slots (ms)=363619\n                Total time spent by all reduces in occupied slots (ms)=11396\n        Map-Reduce Framework\n                Map input records=5\n                Map output records=10\n                Map output bytes=90\n                Map output materialized bytes=140\n                Input split bytes=725\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=2\n                Reduce shuffle bytes=140\n                Reduce input records=10\n                Reduce output records=0\n                Spilled Records=20\n                Shuffled Maps =5\n                Failed Shuffles=0\n                Merged Map outputs=5\n                GC time elapsed (ms)=6962\n                CPU time spent (ms)=19280\n                Physical memory (bytes) snapshot=761933824\n                Virtual memory (bytes) snapshot=3770851328\n                Total committed heap usage (bytes)=619794432\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters\n                Bytes Read=590\n        File Output Format Counters\n                Bytes Written=97\nJob Finished in 105.013 seconds\nEstimated value of Pi is 3.15200000000000000000\n```\n\n## \u53c2\u8003\n\n* [CDH4 Installation Guide](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/CDH4-Installation-Guide.html)\n\n* [Supported JDK Versions](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Requirements-and-Supported-Versions/cdhrsv_topic_3.html)\n\n* [Java SE 7 Downloads](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jdk-7u55-oth-JPR)\n\n* [Installing CDH4](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_4_4.html)\n\n* [Ports Used by Components of CDH4](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_9_1.html)\n\n* [Deploying HDFS on a Cluster](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_11_2.html)\n\n* [Deploying MapReduce v2 (YARN) on a Cluster](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_11_4.html)\n", "tags": ["JDK1.7", "CentOS6.5", "CDH4.7.0"]}