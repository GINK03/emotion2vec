{"context": "Mooc(Udacity, Coursera)\u3067\u6a5f\u68b0\u5b66\u7fd2(Scikit-learn, TensorFlow)\u3092\u3084\u3063\u3066\u307e\u3057\u305f\u3002\n\nMooc\n\nNeural Networks for Machine Learning | Coursera\nDeep Learning - Udacity\n\nAndrew Ng\u5148\u751f\u306e\u304c\u6709\u540d\u3060\u3051\u3069\u3001Google\u306e\u63d0\u4f9b\u3057\u3066\u3044\u308bDeep Learning - Udacity\u306fScikit-learn\u3068\u304bTensorFlow\u3068\u304b\u6271\u3063\u3066\u3044\u308b\u306e\u304c\u3044\u3044\u611f\u3058\n\nNeural Networks for Machine Learning\n\n\nHinton\u5148\u751f\n\u5b9f\u884c\u74b0\u5883\u304cOctave\u3060\u3063\u305f\n\u8ae6\u3081\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3051\u308c\u3069\u4eca\u56de\u306f\u305d\u308c\u3067\u7d42\u308f\u308a\n\n\nDeep Learning - Udacity\n\n\n\u8ab2\u984c\u304cJupyter Notebook\u3067\u4e0e\u3048\u3089\u308c\u308b\u306e\u304c\u3059\u3054\u304f\u3044\u3044(\u52d5\u304f\uff01)\n\u7406\u8ad6\u9762\u306f\u7269\u8db3\u308a\u306a\u3044\u304b\u3082\n\u300c\u3042\u3001\u82e6\u52b4\u3057\u3066\u308b\u3093\u3060\u306a\u300d\u3063\u3066\u8a00\u3046\u30b7\u30fc\u30f3\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u306e\u3067\u30aa\u30b9\u30b9\u30e1\n\n\nDeep Learning - Udacity\n\n\ntensorflow/tensorflow/examples/udacity at master \u00b7 tensorflow/tensorflow\u3092\u3084\u308b\n\n\nSetup\n\n\u74b0\u5883\u3092\u7528\u610f\u3059\u308b\u306e\u304c\u9762\u5012\u3060\u3063\u305f\u306e\u3067Docker\u5229\u7528\nTensorFlow v1.0\u304c\u51fa\u3066\u3044\u305f\u306e\u3067\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\n\u4e00\u62ec\u3057\u3066Docker\u30a4\u30e1\u30fc\u30b8\u3092\u6700\u65b0\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\u3044 - Qiita\n\n\nScikit-learn\n\n\u4e00\u3064\u76ee\u306e\u8ab2\u984c\u304c \"Scikit-learn\u3092\u4f7f\u3063\u3066Logistic\u56de\u5e30\u3057\u3088\u3046\" \u3060\u3063\u305f\ntensorflow/1_notmnist.ipynb at master \u00b7 tensorflow/tensorflow\nMachine Learning, etc: notMNIST dataset\n\n\nScikit-learn\n\nscikit-learn/scikit-learn: scikit-learn: machine learning in Python\nsklearn.linear_model.LogisticRegression \u2014 scikit-learn 0.18.1 documentation\nL1 Penalty and Sparsity in Logistic Regression \u2014 scikit-learn 0.18.1 documentation\n\n\nPython : \u5171\u901a\u3059\u308b\u8981\u7d20\u306e\u53d6\u308a\u51fa\u3057\n\n\u30ea\u30b9\u30c8\u3068\u30ea\u30b9\u30c8\u3092\u6bd4\u8f03\u3057\u3066\u3001\u5171\u901a\u3059\u308b\u8981\u7d20\u3092\u30ea\u30b9\u30c8\u3067\u53d6\u308a\u51fa\u3059\u65b9\u6cd5 - Qiita\n\nv = [1,2,3]\nw = [2,3,4]\n[x for x in v if x in w] # [2,3]\n\n\npython\u306e\u5185\u5305\u8868\u8a18\u3092\u5c11\u3057\u8a73\u3057\u304f - Qiita\n\n\nTensorFlow \nTensorflow\u3067\u540c\u3058\u30bf\u30b9\u30af\u3092\u5b9f\u884c\u3059\u308b\u5185\u5bb9\u3060\u3063\u305f\u3002\n\ntensorflow/2_fullyconnected.ipynb at master \u00b7 tensorflow/tensorflow\nAssignment 2 : building a 3-hidden layer network? - Courses / Deep Learning - Udacity Discussion Forum\nAssignment 2: 2 hidden layers and NaN loss - Courses / Deep Learning - Udacity Discussion Forum\nDeep MNIST for Experts \u00a0|\u00a0 TensorFlow\n\n\n\u3053\u3093\u306a\u611f\u3058\u30673\u5c64NN\u3092\u4f5c\u3063\u3066accuracy 94.2%\u3092\u9054\u6210\u3057\u305f\u306e\u3067\u6e80\u8db3\u3057\u305f\u3002\n\u30b3\u30fc\u30c9\u306e\u4e00\u90e8\u3092\u629c\u304d\u51fa\u3059\u3068\u3053\u3093\u306a\u611f\u3058\u3002\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=1.732/sum(shape))\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\nbatch_size = 128\nhidden_layer_size = 1024\ninput_layer_size = 28*28\noutput_layer_size = 10\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Input data. For the training data, we use a placeholder that will be fed\n    # at run time with a training minibatch.\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n\n    # Variables\n    weight1 = weight_variable( (input_layer_size, hidden_layer_size) )\n    bias1 = bias_variable( [hidden_layer_size] )\n\n    # Hidden Layer\n    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weight1) + bias1)\n\n    # Variables\n    weight2 = weight_variable( (hidden_layer_size, output_layer_size) )\n    bias2 = bias_variable( [output_layer_size] )\n\n    logits = tf.matmul(hidden_layer, weight2) + bias2\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n    # optimizer\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n    # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n    # prediction\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weight1) + bias1), weight2) + bias2)\n    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weight1) + bias1), weight2) + bias2)\n\n\n\nMooc(Udacity, Coursera)\u3067\u6a5f\u68b0\u5b66\u7fd2(Scikit-learn, TensorFlow)\u3092\u3084\u3063\u3066\u307e\u3057\u305f\u3002\n\nMooc\n===========================================================\n\n* [Neural Networks for Machine Learning | Coursera](https://www.coursera.org/learn/\nneural-networks/home/welcome)\n\n* [Deep Learning - Udacity](https://classroom.udacity.com/courses/ud730/)\n\nAndrew Ng\u5148\u751f\u306e\u304c\u6709\u540d\u3060\u3051\u3069\u3001Google\u306e\u63d0\u4f9b\u3057\u3066\u3044\u308b[Deep Learning - Udacity](https://classroom.udacity.com/courses/ud730/)\u306fScikit-learn\u3068\u304bTensorFlow\u3068\u304b\u6271\u3063\u3066\u3044\u308b\u306e\u304c\u3044\u3044\u611f\u3058\n\n[Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks/home/welcome)\n-----------------------------------------------------------\n\n* Hinton\u5148\u751f\n* \u5b9f\u884c\u74b0\u5883\u304cOctave\u3060\u3063\u305f\n* \u8ae6\u3081\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3051\u308c\u3069\u4eca\u56de\u306f\u305d\u308c\u3067\u7d42\u308f\u308a\n\n[Deep Learning - Udacity](https://classroom.udacity.com/courses/ud730/)\n-----------------------------------------------------------\n\n* \u8ab2\u984c\u304cJupyter Notebook\u3067\u4e0e\u3048\u3089\u308c\u308b\u306e\u304c\u3059\u3054\u304f\u3044\u3044(\u52d5\u304f\uff01)\n* \u7406\u8ad6\u9762\u306f\u7269\u8db3\u308a\u306a\u3044\u304b\u3082\n* \u300c\u3042\u3001\u82e6\u52b4\u3057\u3066\u308b\u3093\u3060\u306a\u300d\u3063\u3066\u8a00\u3046\u30b7\u30fc\u30f3\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u306e\u3067\u30aa\u30b9\u30b9\u30e1\n\nDeep Learning - Udacity\n===========================================================\n\n* [tensorflow/tensorflow/examples/udacity at master \u00b7 tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity)\u3092\u3084\u308b\n\nSetup\n-----------------------------------------------------------\n\n* \u74b0\u5883\u3092\u7528\u610f\u3059\u308b\u306e\u304c\u9762\u5012\u3060\u3063\u305f\u306e\u3067Docker\u5229\u7528\n* TensorFlow v1.0\u304c\u51fa\u3066\u3044\u305f\u306e\u3067\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\n\n* [\u4e00\u62ec\u3057\u3066Docker\u30a4\u30e1\u30fc\u30b8\u3092\u6700\u65b0\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\u3044 - Qiita](http://qiita.com/suin/items/5d65320ee9fb9596249f)\n\n\nScikit-learn\n-----------------------------------------------------------\n\n* \u4e00\u3064\u76ee\u306e\u8ab2\u984c\u304c \"Scikit-learn\u3092\u4f7f\u3063\u3066Logistic\u56de\u5e30\u3057\u3088\u3046\" \u3060\u3063\u305f\n\n* [tensorflow/1_notmnist.ipynb at master \u00b7 tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)\n\n* [Machine Learning, etc: notMNIST dataset](http://yaroslavvb.blogspot.jp/2011/09/notmnist-dataset.html)\n\n### Scikit-learn\n\n* [scikit-learn/scikit-learn: scikit-learn: machine learning in Python](https://github.com/scikit-learn/scikit-learn)\n\n* [sklearn.linear_model.LogisticRegression \u2014 scikit-learn 0.18.1 documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\n* [L1 Penalty and Sparsity in Logistic Regression \u2014 scikit-learn 0.18.1 documentation](http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py)\n\n### Python : \u5171\u901a\u3059\u308b\u8981\u7d20\u306e\u53d6\u308a\u51fa\u3057\n\n* [\u30ea\u30b9\u30c8\u3068\u30ea\u30b9\u30c8\u3092\u6bd4\u8f03\u3057\u3066\u3001\u5171\u901a\u3059\u308b\u8981\u7d20\u3092\u30ea\u30b9\u30c8\u3067\u53d6\u308a\u51fa\u3059\u65b9\u6cd5 - Qiita](http://qiita.com/yamao2253/items/309fdaa74b8f9d38ac46)\n\n```Python\nv = [1,2,3]\nw = [2,3,4]\n[x for x in v if x in w] # [2,3]\n```\n\n* [python\u306e\u5185\u5305\u8868\u8a18\u3092\u5c11\u3057\u8a73\u3057\u304f - Qiita](http://qiita.com/y__sama/items/a2c458de97c4aa5a98e7)\n\n\nTensorFlow \n-----------------------------------------------------------\n\nTensorflow\u3067\u540c\u3058\u30bf\u30b9\u30af\u3092\u5b9f\u884c\u3059\u308b\u5185\u5bb9\u3060\u3063\u305f\u3002\n\n* [tensorflow/2_fullyconnected.ipynb at master \u00b7 tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb)\n\n* [Assignment 2 : building a 3-hidden layer network? - Courses / Deep Learning - Udacity Discussion Forum](https://discussions.udacity.com/t/assignment-2-building-a-3-hidden-layer-network/215392)\n\n* [Assignment 2: 2 hidden layers and NaN loss - Courses / Deep Learning - Udacity Discussion Forum](https://discussions.udacity.com/t/assignment-2-2-hidden-layers-and-nan-loss/177715)\n\n* [Deep MNIST for Experts \u00a0|\u00a0 TensorFlow](https://www.tensorflow.org/get_started/mnist/pros)\n\n![UNADJUSTEDNONRAW_thumb_1c0e.jpg](https://qiita-image-store.s3.amazonaws.com/0/41266/1f11a2fd-fb8e-58f7-36e1-627d9a1ceeb0.jpeg \"UNADJUSTEDNONRAW_thumb_1c0e.jpg\")\n\n\u3053\u3093\u306a\u611f\u3058\u30673\u5c64NN\u3092\u4f5c\u3063\u3066accuracy 94.2%\u3092\u9054\u6210\u3057\u305f\u306e\u3067\u6e80\u8db3\u3057\u305f\u3002\n\u30b3\u30fc\u30c9\u306e\u4e00\u90e8\u3092\u629c\u304d\u51fa\u3059\u3068\u3053\u3093\u306a\u611f\u3058\u3002\n\n```Python\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=1.732/sum(shape))\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n  \nbatch_size = 128\nhidden_layer_size = 1024\ninput_layer_size = 28*28\noutput_layer_size = 10\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Input data. For the training data, we use a placeholder that will be fed\n    # at run time with a training minibatch.\n    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n    tf_valid_dataset = tf.constant(valid_dataset)\n    tf_test_dataset = tf.constant(test_dataset)\n    \n    # Variables\n    weight1 = weight_variable( (input_layer_size, hidden_layer_size) )\n    bias1 = bias_variable( [hidden_layer_size] )\n    \n    # Hidden Layer\n    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weight1) + bias1)\n    \n    # Variables\n    weight2 = weight_variable( (hidden_layer_size, output_layer_size) )\n    bias2 = bias_variable( [output_layer_size] )\n    \n    logits = tf.matmul(hidden_layer, weight2) + bias2\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n    \n    # optimizer\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n    # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n    \n    # prediction\n    train_prediction = tf.nn.softmax(logits)\n    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weight1) + bias1), weight2) + bias2)\n    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weight1) + bias1), weight2) + bias2)\n    \n```\n\n\n", "tags": ["scikit-learn", "TensorFlow", "Python"]}