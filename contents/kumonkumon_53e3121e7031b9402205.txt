{"context": "(\u76ee\u6b21\u306f\u3053\u3061\u3089)\n\n\u306f\u3058\u3081\u306b\n\u6700\u8fd1\u3088\u304f\u8033\u306b\u3059\u308b\u3001Deep Learning / \u6df1\u5c64\u5b66\u7fd2\u306b\u3064\u3044\u3066\u3061\u3087\u3063\u3068\u52c9\u5f37\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3046\u3002\u3067\u3082\u3001\u81ea\u5206\u3067\u5b9f\u88c5\u3059\u308b\u307b\u3069\u3084\u308b\u6c17\uff08\u5b9f\u529b\u3082\uff09\u306a\u3044\u306e\u3067\u3001TensorFlow\u4f7f\u3046\u3002\n\u672c\u6765\u3067\u3042\u308c\u3070\u3001\u6570\u5f0f\u3084\u3089\u3092\u4ea4\u3048\u305f\u65b9\u304c\u3044\u3044\u304c\u3001\u6570\u5f0f\u30a2\u30ec\u30eb\u30ae\u30fc\u306e\u4eba\u306b\u3068\u3063\u3066\u306f\u82e6\u75db\u306a\u306e\u3067\u3001\u305d\u3046\u3044\u3063\u305f\u8aac\u660e\u306f\u4ed6\u306e\u5c02\u9580\u5bb6\u306e\u65b9\u3005\u306b\u4efb\u305b\u3066\u30b7\u30f3\u30d7\u30eb\u306b\u8aac\u660e\u3059\u308b\u3002\n\nDeep Learning / \u6df1\u5c64\u5b66\u7fd2\u3068\u306f\uff1f\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e00\u7a2e\u3067\u3001\u8ab0\u304b\u306ebreakthrough\u3067\u6700\u8fd1\u306b\u306a\u3063\u3066\u7a81\u7136\u51fa\u3066\u304d\u305f\u3082\u306e\u3067\u306f\u306a\u3044\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u3046\u3061\u3001\u4e2d\u9593\u5c64 / \u96a0\u308c\u5c64\u304c2\u5c64\u4ee5\u4e0a\u3042\u308b\u3082\u306e\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u30012\u5ea6\u306e\u51ac\u306e\u6642\u4ee3\u3092\u7d4c\u3066\u3001Deep Learning\u3068\u3044\u3046\u5f62\u3067\u518d\u3073\u30db\u30c3\u30c8\u306b\u306a\u3063\u305f\u3002\n\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\uff08logistic regression\uff09\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u306e\u9055\u3044\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3082\u3001\u307e\u305a\u306f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u304b\u3089\u3084\u3063\u3066\u307f\u308b\u3002\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306f\u3001\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306e\u62e1\u5f35\u3067\u3001\u5165\u529b\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066\u6d3b\u6027\u5316\u95a2\u6570\u3092\u304b\u307e\u305b\u3066\u51fa\u529b\u3092\u5f97\u308b\u3068\u3044\u3046\u3082\u306e\u306b\u306f\u9055\u3044\u306a\u3044\u3002\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3067\u306f\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306b\u30b9\u30c6\u30c3\u30d7\u95a2\u6570\u3092\u5229\u7528\u3059\u308b\u304c\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3046\u3002\n\n\u30b3\u30fc\u30c9\n\u3068\u308a\u3042\u3048\u305a\u3001\u30b3\u30fc\u30c9\u3092\u3002\nmnist_logistic.py\n\nmnist_logistic.py\nfrom helper import *\n\nIMAGE_SIZE = 28 * 28\nCATEGORY_NUM = 1\nLEARNING_RATE = 0.1\nTRAINING_LOOP = 20000\nBATCH_SIZE = 100\nSUMMARY_DIR = 'log_logistic'\nSUMMARY_INTERVAL = 100\n\nmnist = input_data.read_data_sets('data', one_hot=True)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n\n    with tf.name_scope('readout'):\n        W = weight_variable([IMAGE_SIZE, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.sigmoid(tf.matmul(x, W) + b)\n\n    with tf.name_scope('optimize'):\n        log_likelihood = tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(-log_likelihood)\n        log_likelihood_summary = tf.scalar_summary('log likelihood', log_likelihood)\n\n    with tf.Session() as sess:\n        train_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/train', sess.graph)\n        test_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/test', sess.graph)\n\n        correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(y_ - 0.5))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            images, labels = mnist.train.next_batch(BATCH_SIZE)\n            labels = [[1] if l[0] == 1 else [0] for l in labels]\n            sess.run(train_step, {x: images, y_: labels})\n\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([log_likelihood_summary, train_accuracy_summary]), {x: images, y_: labels})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: [[1] if l[0] == 1 else [0] for l in mnist.test.labels]})\n                test_writer.add_summary(summary, i)\n\n\n\u5f8c\u307b\u3069\u4f7f\u3046\u5206\u3082\u542b\u307e\u308c\u3066\u3044\u308b\u3051\u3069\u3001\nhelper.py\n\nhelper.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport numpy as np\nimport tensorflow as tf\n\ndef weight_variable(shape, name=None):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial, name=name)\n\ndef bias_variable(shape, name=''):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name=name)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\ndef prewitt_filter():\n    v = np.array([[ 1, 0, -1]] * 3)\n    h = v.swapaxes(0, 1)\n    f = np.zeros(3 * 3 * 1 * 2).reshape(3, 3, 1, 2)\n    f[:, :, 0, 0] = v\n    f[:, :, 0, 1] = h\n    return tf.constant(f, dtype = tf.float32)\n\n\n\n\u30b3\u30fc\u30c9\u306e\u8aac\u660e\n\n\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nMNIST\u3063\u3066\u306e\u306f\u300128x28\u30d4\u30af\u30bb\u30eb\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3002\nmnist = input_data.read_data_sets('data', one_hot=True)\n\n\n\u5165\u529b\u5c64\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n\n\n\u51fa\u529b\u5c64\n\u5165\u529bx\u3092W\u3067\u7dda\u5f62\u5909\u63db\u3057\u3066\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u6e21\u3057\u3066\u7d50\u679cy\u304c\u5f97\u3089\u308c\u308b\u3002W\u3068b\u304c\u6c42\u3081\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3002\n\u3061\u306a\u307f\u306b\u3001\u3053\u306e\u6642\u70b9\u3067\u306f\u3001\u30e2\u30c7\u30eb(Tensorflow\u3067\u306fgraph\u3068\u3044\u3046)\u3092\u4f5c\u3063\u3066\u3044\u308b\u3060\u3051\u306a\u306e\u3067\u306a\u306b\u3082\u52d5\u3044\u3066\u3044\u306a\u3044\u3002\n    with tf.name_scope('readout'):\n        W = weight_variable([IMAGE_SIZE, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.sigmoid(tf.matmul(x, W) + b)\n\n\n\u6700\u9069\u5316\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u63a8\u5b9a\uff09\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u6700\u9069\u5316\u3067\u306f\u5bfe\u6570\u5c24\u5ea6\u3063\u3066\u306e\u3092\u6700\u5927\u5316\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u308b\u3002\u6700\u9069\u5316\u624b\u6cd5\u306f\u3001Stochastic Gradient Descent / \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3002\n        log_likelihood = tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(-log_likelihood)\n\n\n\u5b9f\u884c\n\u5909\u6570\u306e\u521d\u671f\u5316\u5f8c\u3001sess.run()\u3067\u3001\u30c7\u30fc\u30bf\u3092\u98df\u308f\u305b\u3066\u5b66\u7fd2\u3092\u5b9f\u884c\u3059\u308b\u3002\nMNIST\u306f0\u301c9\u306e10\u30ab\u30c6\u30b4\u30ea\u306e\u5206\u985e\u554f\u984c\u3060\u3051\u3069\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u305f\u3081\u306b\u3001\u7121\u7406\u77e2\u7406\u30011\u304b\u305d\u308c\u4ee5\u5916\u304b\u306e2\u30ab\u30c6\u30b4\u30ea\u306e\u5206\u985e\u554f\u984c\u306b\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u30c7\u30fc\u30bf\u306e\u6559\u5e2b\u30e9\u30d9\u30eb\u3092\u4ed8\u3051\u66ff\u3048\u3066\u3044\u308b(1\u4ee5\u5916\u306e\u6642\u306b\u30e9\u30d9\u30eb\u30920)\u3002\n\u672c\u6765\u3067\u3042\u308c\u3070\u3001\u30ab\u30c6\u30b4\u30ea\u9593\u3067\u30c7\u30fc\u30bf\u306e\u504f\u308a\u304c\u3042\u308b\u306e\u306f\u671b\u307e\u3057\u304f\u306a\u3044\u304c\u3001\u4eca\u56de\u306f\u8b58\u5225\u7387\u306f\u91cd\u8981\u3058\u3083\u306a\u3044\u306e\u3067\u6c17\u306b\u3057\u306a\u3044\u3002\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            images, labels = mnist.train.next_batch(BATCH_SIZE)\n            labels = [[1] if l[0] == 1 else [0] for l in labels]\n            sess.run(train_step, {x: images, y_: labels})\n\n\n\u8a55\u4fa1\n\u51fa\u529b\u304c\u30010.5\u4ee5\u4e0a\u306a\u30891\u3067\u3042\u308b\u3068\u3057\u3066\u8a55\u4fa1\u3002\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u63a8\u5b9a\u5024\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u3092\u96c6\u8a08\u3002\n        correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(y_ - 0.5))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n\u5b66\u7fd2\u306e\u904e\u7a0b\u3092\u53ef\u8996\u5316\nTensorFlow\u306fTensorBoard\u3068\u3044\u3046\u53ef\u8996\u5316\u30c4\u30fc\u30eb\u3092\u6301\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u5229\u7528\u3057\u3066\u3001\u5b66\u7fd2\u904e\u7a0b\u3067\u306e\u3001\u5bfe\u6570\u5c24\u5ea6 / \u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u306e\u6b63\u89e3\u7387 / \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u6b63\u89e3\u7387\u3092\u5b9a\u671f\u7684\u306b\u51fa\u529b\u3057\u3066\u53ef\u8996\u5316\u3002\n        log_likelihood_summary = tf.scalar_summary('log likelihood', log_likelihood)\n\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([log_likelihood_summary, train_accuracy_summary]), {x: images, y_: labels})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: [[1] if l[0] == 1 else [0] for l in mnist.test.labels]})\n                test_writer.add_summary(summary, i)\n\n\n\u7d50\u679c\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff08\u9752\u7dda\uff09\u3067\u306e\u8b58\u5225\u7387\u306f\u300199.2%\u7a0b\u5ea6\u3002\n\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u306f\u5358\u7d14\u306a\u30e2\u30c7\u30eb\u306a\u306e\u3067\u3042\u307e\u308a\u610f\u5473\u306f\u7121\u3044\u304c\u3001\u30e2\u30c7\u30eb\uff08\u30b0\u30e9\u30d5\uff09\u3082\u53ef\u8996\u5316\u3067\u304d\u308b\u3002\n\n\n\u3042\u3068\u304c\u304d\n\u6b21\u56de\u306e\u8a18\u4e8b\u3067\u306f\u3001\u4eca\u56de\u306e2\u30af\u30e9\u30b9\u306e\u5206\u985e\u304b\u3089\u3001\u591a\u30af\u30e9\u30b9\u306e\u5206\u985e\u306b\u62e1\u5f35\u3057\u3066\u307f\u307e\u3059\u3002\n([\u76ee\u6b21\u306f\u3053\u3061\u3089](http://qiita.com/kumonkumon/items/6fd05963df92e9eec8c0))\n\n#\u306f\u3058\u3081\u306b\n\u6700\u8fd1\u3088\u304f\u8033\u306b\u3059\u308b\u3001Deep Learning / \u6df1\u5c64\u5b66\u7fd2\u306b\u3064\u3044\u3066\u3061\u3087\u3063\u3068\u52c9\u5f37\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3046\u3002\u3067\u3082\u3001\u81ea\u5206\u3067\u5b9f\u88c5\u3059\u308b\u307b\u3069\u3084\u308b\u6c17\uff08\u5b9f\u529b\u3082\uff09\u306a\u3044\u306e\u3067\u3001TensorFlow\u4f7f\u3046\u3002\n\u672c\u6765\u3067\u3042\u308c\u3070\u3001\u6570\u5f0f\u3084\u3089\u3092\u4ea4\u3048\u305f\u65b9\u304c\u3044\u3044\u304c\u3001\u6570\u5f0f\u30a2\u30ec\u30eb\u30ae\u30fc\u306e\u4eba\u306b\u3068\u3063\u3066\u306f\u82e6\u75db\u306a\u306e\u3067\u3001\u305d\u3046\u3044\u3063\u305f\u8aac\u660e\u306f\u4ed6\u306e\u5c02\u9580\u5bb6\u306e\u65b9\u3005\u306b\u4efb\u305b\u3066\u30b7\u30f3\u30d7\u30eb\u306b\u8aac\u660e\u3059\u308b\u3002\n\n#Deep Learning / \u6df1\u5c64\u5b66\u7fd2\u3068\u306f\uff1f\n* \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e00\u7a2e\u3067\u3001\u8ab0\u304b\u306ebreakthrough\u3067\u6700\u8fd1\u306b\u306a\u3063\u3066\u7a81\u7136\u51fa\u3066\u304d\u305f\u3082\u306e\u3067\u306f\u306a\u3044\u3002\n* \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u3046\u3061\u3001\u4e2d\u9593\u5c64 / \u96a0\u308c\u5c64\u304c2\u5c64\u4ee5\u4e0a\u3042\u308b\u3082\u306e\n* \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u30012\u5ea6\u306e\u51ac\u306e\u6642\u4ee3\u3092\u7d4c\u3066\u3001Deep Learning\u3068\u3044\u3046\u5f62\u3067\u518d\u3073\u30db\u30c3\u30c8\u306b\u306a\u3063\u305f\u3002\n\n#\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\uff08logistic regression\uff09\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u306e\u9055\u3044\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3082\u3001\u307e\u305a\u306f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u304b\u3089\u3084\u3063\u3066\u307f\u308b\u3002\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306f\u3001\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306e\u62e1\u5f35\u3067\u3001\u5165\u529b\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066\u6d3b\u6027\u5316\u95a2\u6570\u3092\u304b\u307e\u305b\u3066\u51fa\u529b\u3092\u5f97\u308b\u3068\u3044\u3046\u3082\u306e\u306b\u306f\u9055\u3044\u306a\u3044\u3002\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3067\u306f\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306b\u30b9\u30c6\u30c3\u30d7\u95a2\u6570\u3092\u5229\u7528\u3059\u308b\u304c\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3046\u3002\n\n##\u30b3\u30fc\u30c9\n\u3068\u308a\u3042\u3048\u305a\u3001\u30b3\u30fc\u30c9\u3092\u3002\n[mnist_logistic.py](https://github.com/kumon/DeepLearningExercise/blob/master/src/tensorflow/mnist_logistic.py)\n\n```py:mnist_logistic.py\nfrom helper import *\n\nIMAGE_SIZE = 28 * 28\nCATEGORY_NUM = 1\nLEARNING_RATE = 0.1\nTRAINING_LOOP = 20000\nBATCH_SIZE = 100\nSUMMARY_DIR = 'log_logistic'\nSUMMARY_INTERVAL = 100\n\nmnist = input_data.read_data_sets('data', one_hot=True)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n\n    with tf.name_scope('readout'):\n        W = weight_variable([IMAGE_SIZE, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.sigmoid(tf.matmul(x, W) + b)\n\n    with tf.name_scope('optimize'):\n        log_likelihood = tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(-log_likelihood)\n        log_likelihood_summary = tf.scalar_summary('log likelihood', log_likelihood)\n\n    with tf.Session() as sess:\n        train_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/train', sess.graph)\n        test_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/test', sess.graph)\n\n        correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(y_ - 0.5))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            images, labels = mnist.train.next_batch(BATCH_SIZE)\n            labels = [[1] if l[0] == 1 else [0] for l in labels]\n            sess.run(train_step, {x: images, y_: labels})\n\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([log_likelihood_summary, train_accuracy_summary]), {x: images, y_: labels})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: [[1] if l[0] == 1 else [0] for l in mnist.test.labels]})\n                test_writer.add_summary(summary, i)\n```\n\n\u5f8c\u307b\u3069\u4f7f\u3046\u5206\u3082\u542b\u307e\u308c\u3066\u3044\u308b\u3051\u3069\u3001\n[helper.py](https://github.com/kumon/DeepLearningExercise/blob/master/src/tensorflow/helper.py)\n\n```py:helper.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport numpy as np\nimport tensorflow as tf\n\ndef weight_variable(shape, name=None):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial, name=name)\n\ndef bias_variable(shape, name=''):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name=name)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\ndef prewitt_filter():\n    v = np.array([[ 1, 0, -1]] * 3)\n    h = v.swapaxes(0, 1)\n    f = np.zeros(3 * 3 * 1 * 2).reshape(3, 3, 1, 2)\n    f[:, :, 0, 0] = v\n    f[:, :, 0, 1] = h\n    return tf.constant(f, dtype = tf.float32)\n```\n\n##\u30b3\u30fc\u30c9\u306e\u8aac\u660e\n\n###\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\nMNIST\u3063\u3066\u306e\u306f\u300128x28\u30d4\u30af\u30bb\u30eb\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3002\n\n```py\nmnist = input_data.read_data_sets('data', one_hot=True)\n```\n\n###\u5165\u529b\u5c64\n```py\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n```\n\n###\u51fa\u529b\u5c64\n\u5165\u529b`x`\u3092`W`\u3067\u7dda\u5f62\u5909\u63db\u3057\u3066\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u6e21\u3057\u3066\u7d50\u679c`y`\u304c\u5f97\u3089\u308c\u308b\u3002`W`\u3068`b`\u304c\u6c42\u3081\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3002\n\u3061\u306a\u307f\u306b\u3001\u3053\u306e\u6642\u70b9\u3067\u306f\u3001\u30e2\u30c7\u30eb(Tensorflow\u3067\u306fgraph\u3068\u3044\u3046)\u3092\u4f5c\u3063\u3066\u3044\u308b\u3060\u3051\u306a\u306e\u3067\u306a\u306b\u3082\u52d5\u3044\u3066\u3044\u306a\u3044\u3002\n\n```py\n    with tf.name_scope('readout'):\n        W = weight_variable([IMAGE_SIZE, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.sigmoid(tf.matmul(x, W) + b)\n```\n\n###\u6700\u9069\u5316\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u63a8\u5b9a\uff09\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u6700\u9069\u5316\u3067\u306f\u5bfe\u6570\u5c24\u5ea6\u3063\u3066\u306e\u3092\u6700\u5927\u5316\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u308b\u3002\u6700\u9069\u5316\u624b\u6cd5\u306f\u3001Stochastic Gradient Descent / \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3002\n\n```py\n        log_likelihood = tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(-log_likelihood)\n```\n\n###\u5b9f\u884c\n\u5909\u6570\u306e\u521d\u671f\u5316\u5f8c\u3001`sess.run()`\u3067\u3001\u30c7\u30fc\u30bf\u3092\u98df\u308f\u305b\u3066\u5b66\u7fd2\u3092\u5b9f\u884c\u3059\u308b\u3002\nMNIST\u306f0\u301c9\u306e10\u30ab\u30c6\u30b4\u30ea\u306e\u5206\u985e\u554f\u984c\u3060\u3051\u3069\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u305f\u3081\u306b\u3001\u7121\u7406\u77e2\u7406\u30011\u304b\u305d\u308c\u4ee5\u5916\u304b\u306e2\u30ab\u30c6\u30b4\u30ea\u306e\u5206\u985e\u554f\u984c\u306b\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u30c7\u30fc\u30bf\u306e\u6559\u5e2b\u30e9\u30d9\u30eb\u3092\u4ed8\u3051\u66ff\u3048\u3066\u3044\u308b(1\u4ee5\u5916\u306e\u6642\u306b\u30e9\u30d9\u30eb\u30920)\u3002\n\u672c\u6765\u3067\u3042\u308c\u3070\u3001\u30ab\u30c6\u30b4\u30ea\u9593\u3067\u30c7\u30fc\u30bf\u306e\u504f\u308a\u304c\u3042\u308b\u306e\u306f\u671b\u307e\u3057\u304f\u306a\u3044\u304c\u3001\u4eca\u56de\u306f\u8b58\u5225\u7387\u306f\u91cd\u8981\u3058\u3083\u306a\u3044\u306e\u3067\u6c17\u306b\u3057\u306a\u3044\u3002\n\n```py\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            images, labels = mnist.train.next_batch(BATCH_SIZE)\n            labels = [[1] if l[0] == 1 else [0] for l in labels]\n            sess.run(train_step, {x: images, y_: labels})\n```\n\n###\u8a55\u4fa1\n\u51fa\u529b\u304c\u30010.5\u4ee5\u4e0a\u306a\u30891\u3067\u3042\u308b\u3068\u3057\u3066\u8a55\u4fa1\u3002\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u63a8\u5b9a\u5024\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u3092\u96c6\u8a08\u3002\n\n```py\n        correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(y_ - 0.5))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n```\n\n###\u5b66\u7fd2\u306e\u904e\u7a0b\u3092\u53ef\u8996\u5316\nTensorFlow\u306fTensorBoard\u3068\u3044\u3046\u53ef\u8996\u5316\u30c4\u30fc\u30eb\u3092\u6301\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u5229\u7528\u3057\u3066\u3001\u5b66\u7fd2\u904e\u7a0b\u3067\u306e\u3001\u5bfe\u6570\u5c24\u5ea6 / \u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u306e\u6b63\u89e3\u7387 / \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u6b63\u89e3\u7387\u3092\u5b9a\u671f\u7684\u306b\u51fa\u529b\u3057\u3066\u53ef\u8996\u5316\u3002\n\n```py\n        log_likelihood_summary = tf.scalar_summary('log likelihood', log_likelihood)\n```\n\n```py\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n```\n\n```py\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([log_likelihood_summary, train_accuracy_summary]), {x: images, y_: labels})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: [[1] if l[0] == 1 else [0] for l in mnist.test.labels]})\n                test_writer.add_summary(summary, i)\n```\n\n##\u7d50\u679c\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff08\u9752\u7dda\uff09\u3067\u306e\u8b58\u5225\u7387\u306f\u300199.2%\u7a0b\u5ea6\u3002\n\n![Screen Shot 2016-05-25 at 22.51.47.png](https://qiita-image-store.s3.amazonaws.com/0/127038/a5e43d5a-dc9c-ade6-b86d-ccc4a5e237a4.png)\n\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u306f\u5358\u7d14\u306a\u30e2\u30c7\u30eb\u306a\u306e\u3067\u3042\u307e\u308a\u610f\u5473\u306f\u7121\u3044\u304c\u3001\u30e2\u30c7\u30eb\uff08\u30b0\u30e9\u30d5\uff09\u3082\u53ef\u8996\u5316\u3067\u304d\u308b\u3002\n![png.png](https://qiita-image-store.s3.amazonaws.com/0/127038/93cec64b-df29-ec1a-291c-3425a0278d3f.png)\n\n#\u3042\u3068\u304c\u304d\n[\u6b21\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/2a659075d55b7b49df5a)\u3067\u306f\u3001\u4eca\u56de\u306e2\u30af\u30e9\u30b9\u306e\u5206\u985e\u304b\u3089\u3001\u591a\u30af\u30e9\u30b9\u306e\u5206\u985e\u306b\u62e1\u5f35\u3057\u3066\u307f\u307e\u3059\u3002\n", "tags": ["TensorFlow", "DeepLearning", "CNN", "ConvolutionalNeuralNetworks", "\u6df1\u5c64\u5b66\u7fd2"]}