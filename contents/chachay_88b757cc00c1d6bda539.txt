{"context": "AI\u306b\u8eca\u3092\u904b\u8ee2\u3055\u305b\u305f\u3044\u3002\u305d\u308c\u306f\u6280\u8853\u8005\u306e\u5922\u3067\u3042\u308b\u3002\n\u6d41\u884c\u308a\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u904b\u8ee2\u3055\u305b\u305f\u3044\u3002\u305d\u308c\u306f\u30b5\u30e9\u30ea\u30fc\u30de\u30f3\u306e\u5922\u3067\u3042\u308b\u3002\n\n\u5236\u5fa1\u306b\u51fa\u3066\u304f\u308b\u95a2\u6570\nNN\u3067\u306e\u95a2\u6570\u8fd1\u4f3c\u3067\u3088\u304f\u898b\u304b\u3051\u308b\u306e\u304csin\u95a2\u6570\u306a\u3069\u95a2\u6570\u306e\u5373\u51fa\u529b\u304c\u6c7a\u307e\u308a\u3001\u51fa\u529b\u5c65\u6b74\u304c\u6b8b\u3089\u306a\u3044\u5185\u5bb9\u3002\n\u3053\u308c\u306b\u5bfe\u3057\u3066\u8eca\u4e21\u7b49\u3001\u904b\u52d5\u3092\u4f34\u3046\u73fe\u8c61\u306b\u306f\u5c65\u6b74\u304c\u4ed8\u304d\u307e\u3068\u3046\u3002\n\u30b9\u30c6\u30a2\u30ea\u30f3\u30b0\u64cd\u4f5c\u306b\u5bfe\u3059\u308b\u8eca\u4e21\u306e\u5fdc\u7b54\u3067\u3059\u3089\u3001\u3086\u3063\u304f\u308a\u5f8c\u304b\u3089\u3057\u304b\u3082\u5c65\u6b74\u3092\u4f34\u3063\u3066\u3084\u3063\u3066\u304f\u308b\u3002\n\n\u753b\u50cf\u5f15\u7528\u5143\uff1a2012-076477\u53f7 \u8eca\u4e21\u7528\u8235\u89d2\u5236\u5fa1\u88c5\u7f6e - astamuse\n\u3082\u3057\u3001\u3053\u3046\u3044\u3063\u305f\u5fdc\u7b54\u6027\u304c\u920d\u304f\u5c65\u6b74\u304c\u6b8b\u308b\u95a2\u6570\u3092\u8fd1\u4f3c\u8868\u73fe\u3059\u308bNN\u3092\u5b9a\u5f0f\u5316\u3067\u304d\u308c\u3070\u3001\nDeep Q learning\u306a\u3069\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u8a08\u306b\u5f79\u7acb\u3064\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3046\u3002\n\n1\u6b21\u9045\u308c\u7cfb\u306e\u96e2\u6563\u5316\n\u5148\u306e\u4f8b\u306f2\u6b21\u9045\u308c\u7cfb\u3067\u8eca\u4e21\u306e\u904b\u52d5\u3092\u8868\u73fe\u3057\u3066\u3044\u305f\u304c\u3001\u3082\u3063\u3068\u7c21\u7d20\u306a\u4f8b\u3068\u3057\u30661\u6b21\u9045\u308c\u7cfb\u3092\u53d6\u308a\u4e0a\u3052\u308b\u3002\nG(s) = \\frac{1}{\\tau s + 1}\nG(s)=1\u03c4s+1G(s)=1\u03c4s+1{G(s) = \\frac{1}{\\tau s + 1}\n}\n\u3053\u308c\u3092\u0394T[sec]\u306e\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30fc\u30c8\u3067\u3082\u3063\u3066\u96e2\u6563\u5316\u3059\u308b\u30681\nG(z) = \\frac{(1-e^{-dT/\\tau})z^{-1}}{1-e^{-dT/\\tau}z^{-1}} \nG(z)=(1\u2212e\u2212dT/\u03c4)z\u221211\u2212e\u2212dT/\u03c4z\u22121G(z)=(1\u2212e\u2212dT/\u03c4)z\u221211\u2212e\u2212dT/\u03c4z\u22121{G(z) = \\frac{(1-e^{-dT/\\tau})z^{-1}}{1-e^{-dT/\\tau}z^{-1}} \n}\n\u3067\u3042\u3063\u3066\u3001x[n]\u3092\u5165\u529b\u3001y[n]\u3092\u51fa\u529b\u3068\u3057\u305f\u6f38\u5316\u5f0f\u3067\u8868\u73fe\u3059\u308b\u3068\ny[n] = (1-e^{-dT/\\tau}) \\cdot x[n-1] + e^{-dT/\\tau} \\cdot y[n-1]\ny[n]=(1\u2212e\u2212dT/\u03c4)\u22c5x[n\u22121]+e\u2212dT/\u03c4\u22c5y[n\u22121]y[n]=(1\u2212e\u2212dT/\u03c4)\u22c5x[n\u22121]+e\u2212dT/\u03c4\u22c5y[n\u22121]{y[n] = (1-e^{-dT/\\tau}) \\cdot x[n-1] + e^{-dT/\\tau} \\cdot y[n-1]\n}\n\u3068\u306a\u308b\u3002\n\u5165\u529b\u3068\u51fa\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3068\u3001\u3055\u3089\u306b\u305d\u306e\u6027\u8cea\u304c1\u6b21\u9045\u308c\u3060\u3068\u77e5\u3063\u3066\u3044\u308c\u3070\u3001\n\u3053\u306e\u03c4\u3092\u6c42\u3081\u308b\u306e\u306f\u3001\u3055\u307b\u3069\u9aa8\u3067\u306f\u306a\u3044\u304c\u3001\n\u3053\u3046\u3044\u3063\u305f\u524d\u63d0\u77e5\u8b58\u3092\u30bc\u30ed\u3067\u904b\u8ee2\u3092\u5b66\u7fd2\u3057\u3066\u307b\u3057\u3044\u3068\u3044\u3046\u306e\u304cDQN\u3068\u3044\u3046\u9b54\u6cd5\u3078\u306e\u671f\u5f85\u3067\u3042\u308b\u3002\n\nNN\u6559\u5e2b\u4fe1\u53f7\u3068\u5352\u696d\u8a66\u9a13\u306e\u5185\u5bb9\n\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30fc\u30c8\u306f10[msec], \u4f1d\u9054\u95a2\u6570\u306f1[sec]\u306e1\u6b21\u9045\u308c\u7cfb\u3068\u3057\u3066\u3057\u307e\u3063\u3066\u8a08\u7b97\u3002\n\n\u6559\u5e2b\u4fe1\u53f7\n\u5165\u529b(\u9752\u8272)\u3092\u30c1\u30e3\u30fc\u30d7\u4fe1\u53f7\u3068\u3057\u305f\u03c4=1[sec]\u306e1\u6b21\u9045\u308c\u4f1d\u9054\u95a2\u6570\u306e\u51fa\u529b(\u7dd1\u8272)\n\nfrom scipy.signal import chirp, lfilter\ndT = 0.01\ndef Plant1order(dT, x):\n    Tau = 1.0\n    a = [1, -np.exp(-dT/Tau)]\n    b = [0.0, (1.0 -np.exp(-dT/Tau))]\n    return lfilter(b, a, x)\n\ndef Data_Set(N):\n    Nyquist_F   = (1/dT)/20 \n    t = np.linspace(0, N*dT, N)\n    x = chirp(t, f0 = 0.01, f1 = Nyquist_F, t1 = N*dT, method = 'linear')\n    y = Plant1order(dT, x)\n    return x.astype(np.float32), y.astype(np.float32), t\n\n\nNN\u3078\u306e\u5165\u529b\n\u7b54\u3048\u3092\u77e5\u3063\u3066\u3044\u308b\u304b\u3089\u3068\u3044\u3046\u7406\u7531\u3082\u3042\u308b\u306e\u3060\u304c\u3001\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u308c\u3070\u3001\ny[n] = A \\cdot x[n-1] + B \\cdot y[n-1]\ny[n]=A\u22c5x[n\u22121]+B\u22c5y[n\u22121]{y[n] = A \\cdot x[n-1] + B \\cdot y[n-1]\n}\n\u3068\u3044\u3046\u7d50\u679c\u306b\u306a\u308b\u306f\u305a(A, B\u306f\u5b9a\u6570)\n\u3057\u305f\u304c\u3063\u3066\u5165\u529b\u306b\u73fe\u5728\u306e\u5165\u529bx, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e(x, y)\u3092\u5165\u308c\u305f3\u5165\u529b\u3092\u5165\u308c\u308b\u3053\u3068\u306b\u3059\u308b\u3002\n\u2192 (\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u306e\u6b63\u89e3)\u3092\u5165\u529b\u3068\u3059\u308b\n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_train)   \n    x_train = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_train,-1)\n    y_train = np.matrix(tmp_DataSet_Y).reshape(N_train,1)\n\n\n\u5352\u696d\u8a66\u9a13\n\u30b9\u30c6\u30c3\u30d7\u5fdc\u7b54(\u30a4\u30f3\u30c7\u30f3\u30b7\u30e3\u30eb\u5fdc\u7b54)\u3055\u305b\u3066\u307f\u308b(\u5165\u529b\uff1a\u9752\u8272\u3001\u51fa\u529b\uff1a\u7dd1\u8272)\n\n\nNN\u3078\u306e\u5165\u529b\nNN\u304c\u81ea\u5206\u3067\u4e88\u6e2c\u3057\u305f\u51fa\u529b\u3092\u4f7f\u3063\u3066\u8a08\u7b97\u3059\u308b\u3068\u8aa4\u5dee\u304c\u7d2f\u7a4d\u3057\u3066\u53b3\u3057\u3044\u3053\u3068\u306b\u306a\u308a\u305d\u3046\u306a\u306e\u3067\u3001\u307e\u305a\u306f\u7518\u3084\u304b\u3057\u8def\u7dda\u3067\u6559\u5e2b\u4fe1\u53f7\u3068\u540c\u3058\u3088\u3046\u306b\u3001(\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u306e\u6b63\u89e3)\u3092\u5165\u529b\u3068\u3059\u308b\n\nNN\u306e\u69cb\u6210\n\u5168\u7d50\u5408\u3067\n(\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b) --> 60\u30e6\u30cb\u30c3\u30c8 --> 60\u30e6\u30cb\u30c3\u30c8 --> \u51fa\u529b\nclass MyChain(Chain):\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\n    n_input  = 3\n    n_output = 1\n    n_units  = 60\n\n    def __init__(self):\n        super(MyChain, self).__init__(\n            l1 = F.Linear(self.n_input, self.n_units),\n            l2 = F.Linear(self.n_units, self.n_units),\n            l3 = F.Linear(self.n_units, self.n_units),\n            l4 = F.Linear(self.n_units, self.n_output)\n        )\n    def __call__(self, x, t):\n        return F.mean_squared_error(self.predict(Variable(x)), Variable(t))\n\n    def predict(self, x):\n        h1 = F.leaky_relu(model.l1(x))\n        h2 = F.leaky_relu(model.l2(h1))\n        h3 = F.leaky_relu(model.l3(h2))\n        return model.l4(h3)\n\n    def get(self, x):\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(len(x),3))).data\n\n\n\u6559\u80b2\u7d50\u679c\n\u3046\u30fc\u3093\nEpoch: 1000\n        Loss: 0.191947328299 CV: 0.202642564848\n\n\n\u305d\u3057\u3066...\n\n\u3046\u30fc\u3093\u2026\n\u6559\u5e2b\u4fe1\u53f7\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u3084\u6559\u80b2\u6570\u304c\u3044\u307e\u3072\u3068\u3064\u3060\u3063\u305f\u304b\u306a\u3041\u2026\n\n\u53c2\u8003\n\u30c7\u30b8\u30bf\u30eb\u5236\u5fa1\u306e\u304a\u52c9\u5f37: \u30d7\u30e9\u30f3\u30c8\u30e2\u30c7\u30eb\u306e\u96e2\u6563\u5316\uff1aZ\u5909\u63db\u8868\n\nAppendix:\u30bd\u30fc\u30b9\n# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom scipy.signal import chirp, lfilter\nfrom scipy.ndimage.interpolation import shift\n\n# Chainer\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u7fa4\nimport chainer                  # \u672c\u4f53\nimport chainer.functions as F   # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b7\u30ca\u30d7\u30b9\u95a2\u6570\u7fa4\nfrom chainer import optimizers  # \u6a5f\u68b0\u5b66\u7fd2\u7528\u95a2\u6570\nfrom chainer import Chain, Variable\n\nimport matplotlib.pyplot as plt\n\nimport argparse\n\n# \u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u9055\u3044\u306b\u3088\u308b\u5dee\u3092\u8a55\u4fa1\u3057\u3084\u3059\u304f\u3059\u308b\nnp.random.seed(0)\n\n\nparser = argparse.ArgumentParser(description='TF approximation')\nparser.add_argument('--dT', '-t', default=0.1, type=float,\n                    help='Sampling Time Step')\nparser.add_argument('--second', '-s', default=False, action='store_true',\n                    help='Use 2nd Lang TF')\nargs = parser.parse_args()\n\ndT  = args.dT\n\ndef random_range(a, b, N):\n    return (b - a)*np.random.rand(N) + a\n\ndef Plant1order(dT, x):\n    Tau = 1.0\n\n    a = [1, -np.exp(-dT/Tau)]\n    b = [0.0, (1.0 -np.exp(-dT/Tau))]\n\n    return lfilter(b, a, x)\n\ndef Plant2order(dT, x):\n    Omega0 = 1.0 * 2 * np.pi\n    Zeta  = 0.7\n\n    OZ = Omega0 * Zeta\n    Omega = Omega0 * np.sqrt(1 - Zeta**2)\n\n    Phi = np.arccos(Zeta)\n\n    a = [1., \n         -2.*np.exp(-OZ*dT)*np.cos(Omega*dT),\n         np.exp(-2.*OZ*dT)]\n    b = [0.,\n         1. - np.exp(-OZ*dT)/np.sqrt(1.-Zeta**2)*np.sin(Omega*dT + Phi),\n         np.exp(-2.*OZ*dT)+np.exp(-OZ*dT)/np.sqrt(1.-Zeta**2)*np.sin(Omega*dT - Phi)]\n\n\n    return lfilter(b, a, x)\n\ndef Data_Set(N):\n\n    Nyquist_F   = (1/dT)/20\n\n    t = np.linspace(0, N*dT, N)\n\n    x = chirp(t, f0 = 0.01, f1 = Nyquist_F, t1 = N*dT, method = 'linear')\n\n    # random_range(-0.5, 0.5, N).astype(np.float32)\n\n    if not args.second:\n        y = Plant1order(dT, x)\n    else:\n        y = Plant2order(dT, x)\n\n    return x.astype(np.float32), y.astype(np.float32), t\n\nclass MyChain(Chain):\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\n    n_input  = 3\n    n_output = 1\n    n_units  = 60\n\n    def __init__(self):\n        super(MyChain, self).__init__(\n            l1 = F.Linear(self.n_input, self.n_units),\n            l2 = F.Linear(self.n_units, self.n_units),\n            l3 = F.Linear(self.n_units, self.n_units),\n            l4 = F.Linear(self.n_units, self.n_output)\n        )\n    def __call__(self, x, t):\n        return F.mean_squared_error(self.predict(Variable(x)), Variable(t))\n\n    def predict(self, x):\n        h1 = F.leaky_relu(model.l1(x))\n        h2 = F.leaky_relu(model.l2(h1))\n        h3 = F.leaky_relu(model.l3(h2))\n        return model.l4(h3)\n\n    def get(self, x):\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(len(x),3))).data\n\nif __name__ == \"__main__\":\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    N_train = 4000\n    N_CV    = 1000\n\n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_train)   \n    x_train = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_train,-1)\n    y_train = np.matrix(tmp_DataSet_Y).reshape(N_train,1)\n\n    print x_train.shape\n\n    fig = plt.figure(2)\n    plt.plot(ttt, tmp_DataSet_X)\n    plt.plot(ttt, tmp_DataSet_Y)\n    fig.patch.set_facecolor('white')\n\n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_CV)   \n    x_CV    = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_CV, -1)\n    y_CV    = np.matrix(tmp_DataSet_Y).reshape(N_CV,1)\n\n\n    # \u5b66\u7fd2\u30e2\u30c7\u30eb\u4f5c\u6210\u304b\u3089\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306b\u30bb\u30c3\u30c8\n    model = MyChain()\n    optimizer = optimizers.Adam()\n    optimizer.setup(model)\n\n    # \u5b66\u7fd2\u306e\u9032\u307f\u306b\u3064\u3044\u3066\u30ed\u30b0\u3092\u53d6\u308b\u305f\u3081\u306e\u5909\u6570\n    train_loss_means = []\n    CV_loss_means    = []\n\n    # \u5b66\u7fd2\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    n_epoch = 1000\n    batch_size = 20\n\n    for epoch in xrange(1, n_epoch + 1):\n        print 'Epoch:', epoch\n\n        # Training\n        perm = np.random.permutation(N_train)\n        sum_loss     = 0\n        for i in xrange(0, N_train, batch_size):\n            x_batch = np.asarray(x_train[perm[i: i + batch_size]])\n            y_batch = np.asarray(y_train[perm[i: i + batch_size]])\n\n            model.zerograds()\n            loss = model(x_batch, y_batch)\n            loss.backward()\n            optimizer.update()\n\n            sum_loss += loss.data * len(y_batch)\n\n        train_loss_means.append(sum_loss/N_train)\n\n        # Cross Validation\n        sum_loss     = 0\n        for i in xrange(0, N_CV, batch_size):\n            x_batch = np.asarray(x_CV[i:i+batch_size])\n            y_batch = np.asarray(y_CV[i:i+batch_size])\n\n            loss = model(x_batch, y_batch)\n\n            sum_loss += loss.data * len(y_batch)\n        CV_loss_means.append(sum_loss/N_CV)\n\n        print \"\\tLoss:\",train_loss_means[-1],\"CV:\",CV_loss_means[-1]\n\n    # Loss\u95a2\u6570\u306e\u30d7\u30ed\u30c3\u30c8\n    plt.figure(1)\n    epolist = range(0, n_epoch)\n    plt.plot(epolist,train_loss_means)\n    plt.plot(epolist,CV_loss_means)\n    plt.yscale('log')\n\n    fig = plt.figure(3)\n    fig.patch.set_facecolor('white')\n    # \u4fee\u696d\u3057\u305f\u6210\u679c\u3092\u898b\u305b\u308b\n    N_test = 200\n\n    # tmp_DataSet_X, tmp_DataSet_Y, t = Data_Set(N_test)  \n    tmp_DataSet_X = np.ones(N_test).astype(np.float32)\n    tmp_DataSet_Y = Plant1order(dT, tmp_DataSet_X).astype(np.float32) if not args.second \\\n                        else Plant2order(dT, tmp_DataSet_X).astype(np.float32)\n    t = np.linspace(0, N_test*dT, N_test)\n\n    x = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_test,-1)\n\n    y1 = model.get(x)\n    t  *= dT\n\n    plt.plot(t, tmp_DataSet_X)\n    plt.plot(t, tmp_DataSet_Y)\n    plt.ylim(0.0,1.1)\n    plt.plot(t, y1)\n\n    plt.show()\n\n\n\n\n\n\n\n\nAI\u306b\u8eca\u3092\u904b\u8ee2\u3055\u305b\u305f\u3044\u3002\u305d\u308c\u306f\u6280\u8853\u8005\u306e\u5922\u3067\u3042\u308b\u3002\n\u6d41\u884c\u308a\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u904b\u8ee2\u3055\u305b\u305f\u3044\u3002\u305d\u308c\u306f\u30b5\u30e9\u30ea\u30fc\u30de\u30f3\u306e\u5922\u3067\u3042\u308b\u3002\n\n\n# \u5236\u5fa1\u306b\u51fa\u3066\u304f\u308b\u95a2\u6570\nNN\u3067\u306e\u95a2\u6570\u8fd1\u4f3c\u3067\u3088\u304f\u898b\u304b\u3051\u308b\u306e\u304csin\u95a2\u6570\u306a\u3069\u95a2\u6570\u306e\u5373\u51fa\u529b\u304c\u6c7a\u307e\u308a\u3001\u51fa\u529b\u5c65\u6b74\u304c\u6b8b\u3089\u306a\u3044\u5185\u5bb9\u3002\n\u3053\u308c\u306b\u5bfe\u3057\u3066\u8eca\u4e21\u7b49\u3001\u904b\u52d5\u3092\u4f34\u3046\u73fe\u8c61\u306b\u306f\u5c65\u6b74\u304c\u4ed8\u304d\u307e\u3068\u3046\u3002\n\n\u30b9\u30c6\u30a2\u30ea\u30f3\u30b0\u64cd\u4f5c\u306b\u5bfe\u3059\u308b\u8eca\u4e21\u306e\u5fdc\u7b54\u3067\u3059\u3089\u3001\u3086\u3063\u304f\u308a\u5f8c\u304b\u3089\u3057\u304b\u3082\u5c65\u6b74\u3092\u4f34\u3063\u3066\u3084\u3063\u3066\u304f\u308b\u3002\n![000015.png](https://qiita-image-store.s3.amazonaws.com/0/117379/686d3348-3ab5-4cbe-e20e-e6a0dae305fe.png)\n\u753b\u50cf\u5f15\u7528\u5143\uff1a[2012-076477\u53f7 \u8eca\u4e21\u7528\u8235\u89d2\u5236\u5fa1\u88c5\u7f6e - astamuse](http://astamuse.com/ja/published/JP/No/2012076477)\n\n\u3082\u3057\u3001\u3053\u3046\u3044\u3063\u305f\u5fdc\u7b54\u6027\u304c\u920d\u304f\u5c65\u6b74\u304c\u6b8b\u308b\u95a2\u6570\u3092\u8fd1\u4f3c\u8868\u73fe\u3059\u308bNN\u3092\u5b9a\u5f0f\u5316\u3067\u304d\u308c\u3070\u3001\nDeep Q learning\u306a\u3069\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u8a08\u306b\u5f79\u7acb\u3064\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3046\u3002\n\n# 1\u6b21\u9045\u308c\u7cfb\u306e\u96e2\u6563\u5316\n\u5148\u306e\u4f8b\u306f2\u6b21\u9045\u308c\u7cfb\u3067\u8eca\u4e21\u306e\u904b\u52d5\u3092\u8868\u73fe\u3057\u3066\u3044\u305f\u304c\u3001\u3082\u3063\u3068\u7c21\u7d20\u306a\u4f8b\u3068\u3057\u30661\u6b21\u9045\u308c\u7cfb\u3092\u53d6\u308a\u4e0a\u3052\u308b\u3002\n\n```math\nG(s) = \\frac{1}{\\tau s + 1}\n```\n\n\u3053\u308c\u3092\u0394T[sec]\u306e\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30fc\u30c8\u3067\u3082\u3063\u3066\u96e2\u6563\u5316\u3059\u308b\u3068[^1]\n\n```math\nG(z) = \\frac{(1-e^{-dT/\\tau})z^{-1}}{1-e^{-dT/\\tau}z^{-1}} \n```\n\n\u3067\u3042\u3063\u3066\u3001x[n]\u3092\u5165\u529b\u3001y[n]\u3092\u51fa\u529b\u3068\u3057\u305f\u6f38\u5316\u5f0f\u3067\u8868\u73fe\u3059\u308b\u3068\n\n```math\ny[n] = (1-e^{-dT/\\tau}) \\cdot x[n-1] + e^{-dT/\\tau} \\cdot y[n-1]\n```\n\n\u3068\u306a\u308b\u3002\n\n\u5165\u529b\u3068\u51fa\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3068\u3001\u3055\u3089\u306b\u305d\u306e\u6027\u8cea\u304c1\u6b21\u9045\u308c\u3060\u3068**\u77e5\u3063\u3066\u3044\u308c\u3070**\u3001\n\u3053\u306e\u03c4\u3092\u6c42\u3081\u308b\u306e\u306f\u3001\u3055\u307b\u3069\u9aa8\u3067\u306f\u306a\u3044\u304c\u3001\n\u3053\u3046\u3044\u3063\u305f\u524d\u63d0\u77e5\u8b58\u3092\u30bc\u30ed\u3067\u904b\u8ee2\u3092\u5b66\u7fd2\u3057\u3066\u307b\u3057\u3044\u3068\u3044\u3046\u306e\u304cDQN\u3068\u3044\u3046\u9b54\u6cd5\u3078\u306e\u671f\u5f85\u3067\u3042\u308b\u3002\n\n# NN\u6559\u5e2b\u4fe1\u53f7\u3068\u5352\u696d\u8a66\u9a13\u306e\u5185\u5bb9\n\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30fc\u30c8\u306f10[msec], \u4f1d\u9054\u95a2\u6570\u306f1[sec]\u306e1\u6b21\u9045\u308c\u7cfb\u3068\u3057\u3066\u3057\u307e\u3063\u3066\u8a08\u7b97\u3002\n\n## \u6559\u5e2b\u4fe1\u53f7\n\u5165\u529b(\u9752\u8272)\u3092[\u30c1\u30e3\u30fc\u30d7\u4fe1\u53f7](https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A3%E3%83%BC%E3%83%97%E4%BF%A1%E5%8F%B7)\u3068\u3057\u305f\u03c4=1[sec]\u306e1\u6b21\u9045\u308c\u4f1d\u9054\u95a2\u6570\u306e\u51fa\u529b(\u7dd1\u8272)\n![figure_2.png](https://qiita-image-store.s3.amazonaws.com/0/117379/f657a852-3ece-0364-2be2-f8461a594538.png)\n\n```py\nfrom scipy.signal import chirp, lfilter\ndT = 0.01\ndef Plant1order(dT, x):\n    Tau = 1.0\n    a = [1, -np.exp(-dT/Tau)]\n    b = [0.0, (1.0 -np.exp(-dT/Tau))]\n    return lfilter(b, a, x)\n\ndef Data_Set(N):\n    Nyquist_F   = (1/dT)/20 \n    t = np.linspace(0, N*dT, N)\n    x = chirp(t, f0 = 0.01, f1 = Nyquist_F, t1 = N*dT, method = 'linear')\n    y = Plant1order(dT, x)\n    return x.astype(np.float32), y.astype(np.float32), t\n```\n\n### NN\u3078\u306e\u5165\u529b\n\u7b54\u3048\u3092\u77e5\u3063\u3066\u3044\u308b\u304b\u3089\u3068\u3044\u3046\u7406\u7531\u3082\u3042\u308b\u306e\u3060\u304c\u3001\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u308c\u3070\u3001\n\n```math\ny[n] = A \\cdot x[n-1] + B \\cdot y[n-1]\n```\n\n\u3068\u3044\u3046\u7d50\u679c\u306b\u306a\u308b\u306f\u305a(A, B\u306f\u5b9a\u6570)\n\u3057\u305f\u304c\u3063\u3066\u5165\u529b\u306b\u73fe\u5728\u306e\u5165\u529bx, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e(x, y)\u3092\u5165\u308c\u305f3\u5165\u529b\u3092\u5165\u308c\u308b\u3053\u3068\u306b\u3059\u308b\u3002\n\u2192 (\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u306e\u6b63\u89e3)\u3092\u5165\u529b\u3068\u3059\u308b\n\n```py\n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_train)   \n    x_train = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_train,-1)\n    y_train = np.matrix(tmp_DataSet_Y).reshape(N_train,1)\n```\n\n## \u5352\u696d\u8a66\u9a13\n\u30b9\u30c6\u30c3\u30d7\u5fdc\u7b54(\u30a4\u30f3\u30c7\u30f3\u30b7\u30e3\u30eb\u5fdc\u7b54)\u3055\u305b\u3066\u307f\u308b(\u5165\u529b\uff1a\u9752\u8272\u3001\u51fa\u529b\uff1a\u7dd1\u8272)\n![figure_3.png](https://qiita-image-store.s3.amazonaws.com/0/117379/0d225859-e228-460f-4f48-a702bd95d4eb.png)\n\n### NN\u3078\u306e\u5165\u529b\nNN\u304c**\u81ea\u5206\u3067\u4e88\u6e2c\u3057\u305f\u51fa\u529b**\u3092\u4f7f\u3063\u3066\u8a08\u7b97\u3059\u308b\u3068\u8aa4\u5dee\u304c\u7d2f\u7a4d\u3057\u3066\u53b3\u3057\u3044\u3053\u3068\u306b\u306a\u308a\u305d\u3046\u306a\u306e\u3067\u3001\u307e\u305a\u306f\u7518\u3084\u304b\u3057\u8def\u7dda\u3067\u6559\u5e2b\u4fe1\u53f7\u3068\u540c\u3058\u3088\u3046\u306b\u3001(\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u306e\u6b63\u89e3)\u3092\u5165\u529b\u3068\u3059\u308b\n\n# NN\u306e\u69cb\u6210\n\u5168\u7d50\u5408\u3067\n(\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5165\u529b, 1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b) --> 60\u30e6\u30cb\u30c3\u30c8 --> 60\u30e6\u30cb\u30c3\u30c8 --> \u51fa\u529b\n\n```py\nclass MyChain(Chain):\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\n    n_input  = 3\n    n_output = 1\n    n_units  = 60\n\n    def __init__(self):\n        super(MyChain, self).__init__(\n            l1 = F.Linear(self.n_input, self.n_units),\n            l2 = F.Linear(self.n_units, self.n_units),\n            l3 = F.Linear(self.n_units, self.n_units),\n            l4 = F.Linear(self.n_units, self.n_output)\n        )\n    def __call__(self, x, t):\n        return F.mean_squared_error(self.predict(Variable(x)), Variable(t))\n        \n    def predict(self, x):\n        h1 = F.leaky_relu(model.l1(x))\n        h2 = F.leaky_relu(model.l2(h1))\n        h3 = F.leaky_relu(model.l3(h2))\n        return model.l4(h3)\n        \n    def get(self, x):\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(len(x),3))).data\n```\n\n# \u6559\u80b2\u7d50\u679c\n\u3046\u30fc\u3093\n\n```\nEpoch: 1000\n        Loss: 0.191947328299 CV: 0.202642564848\n```\n![20160528.png](https://qiita-image-store.s3.amazonaws.com/0/117379/640ebcb9-ee0c-8cca-a09a-f6da2f25741c.png)\n\n\u305d\u3057\u3066...\n![figure_4.png](https://qiita-image-store.s3.amazonaws.com/0/117379/6d00f3d9-2c1a-7aba-c389-d0a17a36ebdb.png)\n\n\u3046\u30fc\u3093\u2026\n\n\u6559\u5e2b\u4fe1\u53f7\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u3084\u6559\u80b2\u6570\u304c\u3044\u307e\u3072\u3068\u3064\u3060\u3063\u305f\u304b\u306a\u3041\u2026\n\n# \u53c2\u8003\n[^1]:[\u30c7\u30b8\u30bf\u30eb\u5236\u5fa1\u306e\u304a\u52c9\u5f37: \u30d7\u30e9\u30f3\u30c8\u30e2\u30c7\u30eb\u306e\u96e2\u6563\u5316\uff1aZ\u5909\u63db\u8868](http://pid123.blogspot.jp/2016/03/z.html)\n\n\n# Appendix:\u30bd\u30fc\u30b9\n\n```py\n# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom scipy.signal import chirp, lfilter\nfrom scipy.ndimage.interpolation import shift\n\n# Chainer\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u7fa4\nimport chainer                  # \u672c\u4f53\nimport chainer.functions as F   # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b7\u30ca\u30d7\u30b9\u95a2\u6570\u7fa4\nfrom chainer import optimizers  # \u6a5f\u68b0\u5b66\u7fd2\u7528\u95a2\u6570\nfrom chainer import Chain, Variable\n\nimport matplotlib.pyplot as plt\n\nimport argparse\n\n# \u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u9055\u3044\u306b\u3088\u308b\u5dee\u3092\u8a55\u4fa1\u3057\u3084\u3059\u304f\u3059\u308b\nnp.random.seed(0)\n\n\nparser = argparse.ArgumentParser(description='TF approximation')\nparser.add_argument('--dT', '-t', default=0.1, type=float,\n                    help='Sampling Time Step')\nparser.add_argument('--second', '-s', default=False, action='store_true',\n                    help='Use 2nd Lang TF')\nargs = parser.parse_args()\n\ndT  = args.dT\n\ndef random_range(a, b, N):\n    return (b - a)*np.random.rand(N) + a\n\ndef Plant1order(dT, x):\n    Tau = 1.0\n\n    a = [1, -np.exp(-dT/Tau)]\n    b = [0.0, (1.0 -np.exp(-dT/Tau))]\n    \n    return lfilter(b, a, x)\n    \ndef Plant2order(dT, x):\n    Omega0 = 1.0 * 2 * np.pi\n    Zeta  = 0.7\n    \n    OZ = Omega0 * Zeta\n    Omega = Omega0 * np.sqrt(1 - Zeta**2)\n    \n    Phi = np.arccos(Zeta)\n\n    a = [1., \n         -2.*np.exp(-OZ*dT)*np.cos(Omega*dT),\n         np.exp(-2.*OZ*dT)]\n    b = [0.,\n         1. - np.exp(-OZ*dT)/np.sqrt(1.-Zeta**2)*np.sin(Omega*dT + Phi),\n         np.exp(-2.*OZ*dT)+np.exp(-OZ*dT)/np.sqrt(1.-Zeta**2)*np.sin(Omega*dT - Phi)]\n\n    \n    return lfilter(b, a, x)\n    \ndef Data_Set(N):\n\n    Nyquist_F   = (1/dT)/20\n    \n    t = np.linspace(0, N*dT, N)\n\n    x = chirp(t, f0 = 0.01, f1 = Nyquist_F, t1 = N*dT, method = 'linear')\n\n    # random_range(-0.5, 0.5, N).astype(np.float32)\n    \n    if not args.second:\n        y = Plant1order(dT, x)\n    else:\n        y = Plant2order(dT, x)\n\n    return x.astype(np.float32), y.astype(np.float32), t\n\nclass MyChain(Chain):\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\n    n_input  = 3\n    n_output = 1\n    n_units  = 60\n\n    def __init__(self):\n        super(MyChain, self).__init__(\n            l1 = F.Linear(self.n_input, self.n_units),\n            l2 = F.Linear(self.n_units, self.n_units),\n            l3 = F.Linear(self.n_units, self.n_units),\n            l4 = F.Linear(self.n_units, self.n_output)\n        )\n    def __call__(self, x, t):\n        return F.mean_squared_error(self.predict(Variable(x)), Variable(t))\n        \n    def predict(self, x):\n        h1 = F.leaky_relu(model.l1(x))\n        h2 = F.leaky_relu(model.l2(h1))\n        h3 = F.leaky_relu(model.l3(h2))\n        return model.l4(h3)\n        \n    def get(self, x):\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(len(x),3))).data\n\nif __name__ == \"__main__\":\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    N_train = 4000\n    N_CV    = 1000\n\n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_train)   \n    x_train = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_train,-1)\n    y_train = np.matrix(tmp_DataSet_Y).reshape(N_train,1)\n    \n    print x_train.shape\n    \n    fig = plt.figure(2)\n    plt.plot(ttt, tmp_DataSet_X)\n    plt.plot(ttt, tmp_DataSet_Y)\n    fig.patch.set_facecolor('white')\n    \n    tmp_DataSet_X, tmp_DataSet_Y, ttt = Data_Set(N_CV)   \n    x_CV    = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_CV, -1)\n    y_CV    = np.matrix(tmp_DataSet_Y).reshape(N_CV,1)\n\n\n    # \u5b66\u7fd2\u30e2\u30c7\u30eb\u4f5c\u6210\u304b\u3089\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306b\u30bb\u30c3\u30c8\n    model = MyChain()\n    optimizer = optimizers.Adam()\n    optimizer.setup(model)\n\n    # \u5b66\u7fd2\u306e\u9032\u307f\u306b\u3064\u3044\u3066\u30ed\u30b0\u3092\u53d6\u308b\u305f\u3081\u306e\u5909\u6570\n    train_loss_means = []\n    CV_loss_means    = []\n    \n    # \u5b66\u7fd2\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    n_epoch = 1000\n    batch_size = 20\n    \n    for epoch in xrange(1, n_epoch + 1):\n        print 'Epoch:', epoch\n        \n        # Training\n        perm = np.random.permutation(N_train)\n        sum_loss     = 0\n        for i in xrange(0, N_train, batch_size):\n            x_batch = np.asarray(x_train[perm[i: i + batch_size]])\n            y_batch = np.asarray(y_train[perm[i: i + batch_size]])\n            \n            model.zerograds()\n            loss = model(x_batch, y_batch)\n            loss.backward()\n            optimizer.update()\n            \n            sum_loss += loss.data * len(y_batch)\n        \n        train_loss_means.append(sum_loss/N_train)\n        \n        # Cross Validation\n        sum_loss     = 0\n        for i in xrange(0, N_CV, batch_size):\n            x_batch = np.asarray(x_CV[i:i+batch_size])\n            y_batch = np.asarray(y_CV[i:i+batch_size])\n            \n            loss = model(x_batch, y_batch)\n            \n            sum_loss += loss.data * len(y_batch)\n        CV_loss_means.append(sum_loss/N_CV)\n        \n        print \"\\tLoss:\",train_loss_means[-1],\"CV:\",CV_loss_means[-1]\n    \n    # Loss\u95a2\u6570\u306e\u30d7\u30ed\u30c3\u30c8\n    plt.figure(1)\n    epolist = range(0, n_epoch)\n    plt.plot(epolist,train_loss_means)\n    plt.plot(epolist,CV_loss_means)\n    plt.yscale('log')\n    \n    fig = plt.figure(3)\n    fig.patch.set_facecolor('white')\n    # \u4fee\u696d\u3057\u305f\u6210\u679c\u3092\u898b\u305b\u308b\n    N_test = 200\n    \n    # tmp_DataSet_X, tmp_DataSet_Y, t = Data_Set(N_test)  \n    tmp_DataSet_X = np.ones(N_test).astype(np.float32)\n    tmp_DataSet_Y = Plant1order(dT, tmp_DataSet_X).astype(np.float32) if not args.second \\\n                        else Plant2order(dT, tmp_DataSet_X).astype(np.float32)\n    t = np.linspace(0, N_test*dT, N_test)\n\n    x = np.vstack((tmp_DataSet_X, shift(tmp_DataSet_X, 1, cval=0.0), shift(tmp_DataSet_Y, 1, cval=0.0))).reshape(N_test,-1)\n\n    y1 = model.get(x)\n    t  *= dT\n\n    plt.plot(t, tmp_DataSet_X)\n    plt.plot(t, tmp_DataSet_Y)\n    plt.ylim(0.0,1.1)\n    plt.plot(t, y1)\n  \n    plt.show()\n```\n", "tags": ["Chainer", "DeepLearning"]}