{"context": "\n\n\uff08 \u4f5c\u696d\u74b0\u5883 \uff09\n\n\nJupyter notebook (Python 3)\n\n\n\npython3 jupyter notebook\n%matplotlib inline\n\n\n\n\u30c7\u30fc\u30bf\u4f5c\u6210\n\n\n\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2460 \u3011\n\n\n\u30ce\u30a4\u30ba\u306a\u3057 \u6b63\u5f26\u66f2\u7dda\n\u632f\u5e45: 1\n\u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d 2\u03c02\u03c02\u03c0 \uff09\n\n\npython3 jupyter notebook\nimport numpy as np\n\nxs = np.arange(-2000*np.pi, 2000*np.pi, 0.1)\nys = np.sin(xs)\n# len(xs) # 125664\n\n\n\npython3 jupyter notebook\n# sin\u66f2\u7dda\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n# \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u6570\u309263\u3068\u3059\u308b\nlen(xs) / 63\n# 63*1994 # 125622 \n# 125623\u4ee5\u964d\u306e\u30c7\u30fc\u30bf\u3092\u5207\u308a\u6368\u3066\u308b\nys = ys[0:125622]\nlen(ys)\n\n\n125622\n\n\n\uff11\u5468\u671f\u5206\uff08 2\u03c0 \uff09\uff1d 63\u6642\u70b9 \u3067 \u30c7\u30fc\u30bf\u3092\u751f\u6210\n\n\npython3 jupyter notebook\nfrom matplotlib import pyplot as plt \n\nplt.plot(xs[0:63], ys[0:63])\nplt.ylim(-2, 2)\nplt.show()\n\n\n\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\n\n\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2461 \u3011\n\n\n\u30ce\u30a4\u30ba\u3042\u308a \u6b63\u5f26\u66f2\u7dda\n\u632f\u5e45: 1.5 \n\u30ce\u30a4\u30ba\uff1a \u6a19\u6e96\u6b63\u898f\u5206\u5e03 \uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee:1\uff09\n\u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d 2\u03c02\u03c0 \uff09\n\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n\uff08 \u53c2\u8003 \uff09\n\n\u300c\u6ce2\u3068\u3057\u3066\u306e\u4e09\u89d2\u95a2\u6570\u300d\n\u4e09\u89d2\u95a2\u6570 \u300c\u89d2\u5ea6\u3068\u30e9\u30b8\u30a2\u30f3\u300d\n\nfrom numpy.random import *\n\nys_2 = 1.5*np.sin(xs*0.3) + randn(len(xs))\n\n\npython3 jupyter notebook\nys_2 = ys_2[0:125622]\nlen(ys_2)\n\n\n125622\n\n\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\nplt.plot(xs[0:63], ys_2[0:63])\nplt.ylim(-5, 5)\nplt.show()\n\n\n\n\n\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2462 \u3011\n\n\n\u30ce\u30a4\u30ba\u3042\u308a \u6b63\u5f26\u66f2\u7dda\n\u632f\u5e45: 1.5 \n\u89d2\u901f\u5ea6: 3.5\n\u30ce\u30a4\u30ba\uff1a \u6a19\u6e96\u6b63\u898f\u5206\u5e03 \uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee:1\uff09\n\u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d 2\u03c02\u03c0 \uff09\n\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n\npython3 jupyter notebook\nfrom numpy.random import *\n\nys_3 = 1.5*np.sin(xs*3.5) + randn(len(xs))\nys_3 = ys_3[0:125622]\n\n\n\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\nplt.plot(xs[0:63], ys_3[0:63])\nplt.ylim(-5, 5)\nplt.show()\n\n\n\n\uff08 \u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\nplt.plot(ys_3)\n\n\n[<matplotlib.lines.Line2D at 0x120f315f8>]\n\n\n\npython3 jupyter notebook\nimport pandas as pd\nys_3_df = pd.Series(ys_3)\nys_3_df.plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12054ef60>\n\n\n\n\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2463 \u3011\n\n\n\u6a19\u6e96\u6b63\u898f\u4e71\u6570 \u306e \u7d2f\u7a4d\u548c\u7cfb\u5217\n\u6642\u70b9\u6570: 125,622 \u6642\u70b9 \uff08=63*1994\uff09\n\u671f\u9593\u6570: 1,994 \u671f\u9593\n\nsin\u66f2\u7dda \u3068 \u5408\u308f\u305b\u3066\u7528\u3044\u308b\u305f\u3081\u306b\u3001\nsin\u66f2\u7dda\u306e\uff11\u5468\u671f\uff08\uff1d0.1\u523b\u307f\u3067\u300163\u6642\u70b9\uff09\u306b\u5408\u308f\u305b\u3066\u300163\u6642\u70b9\u3092\uff11\u30c7\u30fc\u30bf\u533a\u9593\u3068\u3059\u308b\u3002\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8 \u3067\u3001\u56db\u6368\u4e94\u5165\u3057\u3066\u300163\u6642\u70b9\u5206\u3002\n\n\uff08 \u53c2\u8003 \uff09\n\u6a19\u6e96\u6b63\u898f\u4e71\u6570\u3000\u306e\u3000\u7d2f\u7a4d\u548c\u7cfb\u5217\n\nBeingWizard.com (2016\u5e7411\u67089\u65e5) \u300cPython 2\u6b21\u5143\u30d6\u30e9\u30a6\u30f3\u904b\u52d5 \u30b7\u30e5\u30df\u30ec\u30fc\u30b7\u30e7\u30f3\u300d\nJulia vs Python: \u30d3\u30c3\u30c8\u30b3\u30a4\u30f3\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\nMy Life as a Mock Quant \uff082012/08/21\uff09 \u300c\u5e7e\u4f55\u30d6\u30e9\u30a6\u30f3\u904b\u52d5\u306b\u5f93\u3046\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u306e\u30d1\u30b9\u3092\u4f5c\u308b\u300d\n\uff08SlideShare\uff09 \u5bff\u4e4b \u4e0b\u91ce\u300c\u30d6\u30e9\u30a6\u30f3\u904b\u52d5\u3068\u305d\u306e\u6642\u9593\u7a4d\u5206\u306e\u304f\u308a\u8fd4\u3057\u53ca\u3073\u5f62\u72b6\u306e\u81ea\u7136\u30fb\u751f\u7269\u73fe\u8c61\u3078\u306e\u7c21\u5358\u306a\u8003\u5bdf\u300d\n\n\n\npython3 jupyter notebook\nrandn_cumsum_series = np.random.randn(len(xs)).cumsum()\n\n\n\npython3 jupyter notebook\nrandn_cumsum_series = randn_cumsum_series[0:125622]\nlen(randn_cumsum_series)\n\n\n125622\n\n\n\u5024 \u306e \u56db\u5206\u4f4d\u6570 \u307b\u304b \u3092 \u78ba\u8a8d\n\n\npython3 jupyter notebook\nimport pandas as pd\ntmp_df = pd.Series(randn_cumsum_series)\ntmp_df.describe()\n\n\ncount    125622.000000\nmean       -108.356590\nstd          59.930377\nmin        -267.971590\n25%        -149.494232\n50%        -107.989932\n75%         -70.402787\nmax          53.531884\ndtype: float64\n\n\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\nplt.plot(xs[0:63], randn_cumsum_series[0:63])\nplt.show()\n\n\n\n\n\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2464 \u3011\n\n\n\u6b63\u898f\u4e71\u6570\uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee: 20\uff09\u306e \u7d2f\u7a4d\u548c\u7cfb\u5217\n\u6642\u70b9\u6570: 125,622 \u6642\u70b9 \uff08=63*1994\uff09\n\u671f\u9593\u6570: 1,994 \u671f\u9593\n\n\uff08 \u53c2\u8003 \uff09\n\nWikipedia\u300c\u4e71\u6570\u5217\u300d\n\n\npython3 jupyter notebook\nnormal_distribution_cumsum_series = np.array([normal(0, 20) for i in range(len(xs))]).cumsum()\nnormal_distribution_cumsum_series = normal_distribution_cumsum_series[0:125622]\n\n\n\n\u5024 \u306e \u56db\u5206\u4f4d\u6570 \u307b\u304b \u3092 \u78ba\u8a8d\n\n\npython3 jupyter notebook\ntmp_df = pd.Series(normal_distribution_cumsum_series)\ntmp_df.describe()\n\n\ncount    125622.000000\nmean      -1956.171591\nstd        4047.473179\nmin      -10240.019296\n25%       -5467.874522\n50%       -2246.121624\n75%         901.980908\nmax        6004.684593\ndtype: float64\n\n\uff08 \u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\ntmp_df.plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12aa879b0>\n\n\n\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09\n\npython3 jupyter notebook\nplt.plot(normal_distribution_cumsum_series[0:63])\nplt.show()\n\n\n\n\n\uff08 \u53c2\u8003 \uff09\n\n\u6b63\u898f\u4e71\u6570\uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee: 20\uff09\n\n\u203b \u4e71\u6570\u306e\u7a2e\uff08seed\uff09 \u3092 \u56fa\u5b9a\u3057\u306a\u3044\u3068\u3001\u5b9f\u884c\u3059\u308b \u305f\u3073 \u306b \u7570\u306a\u308b\u6570\u5024 \u304c \u751f\u6210\u3055\u308c\u308b\u3002\n\npython3 jupyter notebook\nprint([normal(0, 20) for i in range(20)])\n\n\n[-0.1044916996982958, 19.21901497729581, -10.583793299858185, -34.27589412966404, -8.538314451442394, -12.603024803995908, 32.56951045105264, -16.20113597295369, 37.7772779255066, 16.01546722711461, 18.443395273470987, 14.708712759168765, -23.741788551069313, -5.443674522698121, -4.146401782328453, -12.493122576250633, -26.939969194056097, 5.41340756356258, -1.5770183027819948, -25.248265241894465]\n\n\n\n\u3053\u3053\u307e\u3067\u3067 \u4f5c\u6210 \u3057\u305f \u5168\u30c7\u30fc\u30bf \u306e \u8981\u7d20\u6570 \u304c \u540c\u3058\u3053\u3068 \u3092 \u78ba\u8a8d\n\n\npython3 jupyter notebook\nprint(len(ys),\n      len(ys_2),\n      len(ys_3),\n      len(randn_cumsum_series),\n      len(normal_distribution_cumsum_series))\n\n\n125622 125622 125622 125622 125622\n\n\n\u30c7\u30fc\u30bf\u306e\u6b21\u5143\uff08shape\uff09 \u3092 \u5909\u63db\n\nKeras AutoEncoder\u30e2\u30c7\u30eb \u306b \u6e21\u305b\u308b\u3088\u3046 \u306b\u3001\u4ee5\u4e0b \u306b \u5909\u63db\n\uff12\u6b21\u5143 \u306e Numpy.array\u578b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\n\uff08 \u6b21\u5143 \uff09\n\n\u300c\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570\u300d\uff08\uff1d\u300c\u30c7\u30fc\u30bf\u671f\u9593\u300d\u306e\u6570\uff09\u00d7 \uff11\u671f\u9593\u3042\u305f\u308a\u306e\u30c7\u30fc\u30bf\u81ea\u8ee2\u6570\n\n\u203b shape \u306f\u3001(1,994, 63) \u306b \u306a\u308b\u3002\n\npython3 jupyter notebook\n# \uff11\u671f\u9593 63\u6642\u70b9 \u3001\u5408\u8a08 1994\u671f\u9593 \u306e \u30c7\u30fc\u30bf\u306b\u5909\u63db\u3059\u308b \n125622 / 63\n\n\n1994.0\n\n\npython3 jupyter notebook\nys_array = ys.reshape(1994, 63)\n\n\n\npython3 jupyter notebook\nys_array.shape\n\n\n(1994, 63)\n\n\npython3 jupyter notebook\nlen(ys_array)\n\n\n1994\n\n\npython3 jupyter notebook\nys_array[0]\n\n\narray([  6.42833292e-13,   9.98334166e-02,   1.98669331e-01,\n         2.95520207e-01,   3.89418342e-01,   4.79425539e-01,\n         5.64642473e-01,   6.44217687e-01,   7.17356091e-01,\n         7.83326910e-01,   8.41470985e-01,   8.91207360e-01,\n         9.32039086e-01,   9.63558185e-01,   9.85449730e-01,\n         9.97494987e-01,   9.99573603e-01,   9.91664810e-01,\n         9.73847631e-01,   9.46300088e-01,   9.09297427e-01,\n         8.63209367e-01,   8.08496404e-01,   7.45705212e-01,\n         6.75463181e-01,   5.98472144e-01,   5.15501372e-01,\n         4.27379880e-01,   3.34988150e-01,   2.39249329e-01,\n         1.41120008e-01,   4.15806624e-02,  -5.83741434e-02,\n        -1.57745694e-01,  -2.55541102e-01,  -3.50783228e-01,\n        -4.42520443e-01,  -5.29836141e-01,  -6.11857891e-01,\n        -6.87766159e-01,  -7.56802495e-01,  -8.18277111e-01,\n        -8.71575772e-01,  -9.16165937e-01,  -9.51602074e-01,\n        -9.77530118e-01,  -9.93691004e-01,  -9.99923258e-01,\n        -9.96164609e-01,  -9.82452613e-01,  -9.58924275e-01,\n        -9.25814682e-01,  -8.83454656e-01,  -8.32267442e-01,\n        -7.72764488e-01,  -7.05540326e-01,  -6.31266638e-01,\n        -5.50685543e-01,  -4.64602179e-01,  -3.73876665e-01,\n        -2.79415498e-01,  -1.82162504e-01,  -8.30894028e-02])\n\n\n\uff08 \u30c7\u30fc\u30bf\u4f5c\u6210 \u306f\u3001\u4ee5\u4e0a \u3067 \u5b8c\u4e86 \uff09\n\n\n\nsin\u66f2\u7dda\uff08\u4e71\u6570\u52a0\u7b97\u306a\u3057\uff09 \u3092 AE\u30e2\u30c7\u30eb \u3067 \u518d\u73fe\u3067\u304d\u308b\u304b \u691c\u8a3c\u5b9f\u884c\n\n\npython3 jupyter notebook\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nimport numpy as np\n\n\n\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \u3092 \u8a2d\u8a08\n\n\npython3 jupyter notebook\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n\n\n\npython3 jupyter notebook\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n\n\n\n\u30e2\u30c7\u30eb\u5b66\u7fd2 \u5b9f\u65bd\n\n\npython3 jupyter notebook\nys_ae_model_hist = autoencoder.fit(ys_array, ys_array,\n                                        nb_epoch=50,\n                                        batch_size=20,\n                                        shuffle=True,\n                                        validation_split=0.1)\n\n\nTrain on 1794 samples, validate on 200 samples\nEpoch 1/50\n1794/1794 [==============================] - 0s - loss: 0.7720 - val_loss: 0.7603\nEpoch 2/50\n1794/1794 [==============================] - 0s - loss: 0.7619 - val_loss: 0.7528\nEpoch 3/50\n1794/1794 [==============================] - 0s - loss: 0.7524 - val_loss: 0.7457\nEpoch 4/50\n1794/1794 [==============================] - 0s - loss: 0.7433 - val_loss: 0.7389\nEpoch 5/50\n1794/1794 [==============================] - 0s - loss: 0.7344 - val_loss: 0.7321\nEpoch 6/50\n1794/1794 [==============================] - 0s - loss: 0.7256 - val_loss: 0.7252\nEpoch 7/50\n1794/1794 [==============================] - 0s - loss: 0.7168 - val_loss: 0.7181\nEpoch 8/50\n1794/1794 [==============================] - 0s - loss: 0.7078 - val_loss: 0.7107\nEpoch 9/50\n1794/1794 [==============================] - 0s - loss: 0.6985 - val_loss: 0.7030\nEpoch 10/50\n1794/1794 [==============================] - 0s - loss: 0.6889 - val_loss: 0.6949\nEpoch 11/50\n1794/1794 [==============================] - 0s - loss: 0.6789 - val_loss: 0.6864\nEpoch 12/50\n1794/1794 [==============================] - 0s - loss: 0.6685 - val_loss: 0.6776\nEpoch 13/50\n1794/1794 [==============================] - 0s - loss: 0.6575 - val_loss: 0.6682\nEpoch 14/50\n1794/1794 [==============================] - 0s - loss: 0.6461 - val_loss: 0.6584\nEpoch 15/50\n1794/1794 [==============================] - 0s - loss: 0.6342 - val_loss: 0.6481\nEpoch 16/50\n1794/1794 [==============================] - 0s - loss: 0.6218 - val_loss: 0.6374\nEpoch 17/50\n1794/1794 [==============================] - 0s - loss: 0.6090 - val_loss: 0.6263\nEpoch 18/50\n1794/1794 [==============================] - 0s - loss: 0.5959 - val_loss: 0.6148\nEpoch 19/50\n1794/1794 [==============================] - 0s - loss: 0.5825 - val_loss: 0.6030\nEpoch 20/50\n1794/1794 [==============================] - 0s - loss: 0.5690 - val_loss: 0.5909\nEpoch 21/50\n1794/1794 [==============================] - 0s - loss: 0.5555 - val_loss: 0.5786\nEpoch 22/50\n1794/1794 [==============================] - 0s - loss: 0.5420 - val_loss: 0.5662\nEpoch 23/50\n1794/1794 [==============================] - 0s - loss: 0.5287 - val_loss: 0.5538\nEpoch 24/50\n1794/1794 [==============================] - 0s - loss: 0.5157 - val_loss: 0.5415\nEpoch 25/50\n1794/1794 [==============================] - 0s - loss: 0.5030 - val_loss: 0.5293\nEpoch 26/50\n1794/1794 [==============================] - 0s - loss: 0.4908 - val_loss: 0.5173\nEpoch 27/50\n1794/1794 [==============================] - 0s - loss: 0.4790 - val_loss: 0.5055\nEpoch 28/50\n1794/1794 [==============================] - 0s - loss: 0.4678 - val_loss: 0.4941\nEpoch 29/50\n1794/1794 [==============================] - 0s - loss: 0.4571 - val_loss: 0.4831\nEpoch 30/50\n1794/1794 [==============================] - 0s - loss: 0.4469 - val_loss: 0.4724\nEpoch 31/50\n1794/1794 [==============================] - 0s - loss: 0.4372 - val_loss: 0.4622\nEpoch 32/50\n1794/1794 [==============================] - 0s - loss: 0.4281 - val_loss: 0.4524\nEpoch 33/50\n1794/1794 [==============================] - 0s - loss: 0.4195 - val_loss: 0.4430\nEpoch 34/50\n1794/1794 [==============================] - 0s - loss: 0.4114 - val_loss: 0.4341\nEpoch 35/50\n1794/1794 [==============================] - 0s - loss: 0.4037 - val_loss: 0.4256\nEpoch 36/50\n1794/1794 [==============================] - 0s - loss: 0.3965 - val_loss: 0.4175\nEpoch 37/50\n1794/1794 [==============================] - 0s - loss: 0.3897 - val_loss: 0.4099\nEpoch 38/50\n1794/1794 [==============================] - 0s - loss: 0.3833 - val_loss: 0.4027\nEpoch 39/50\n1794/1794 [==============================] - 0s - loss: 0.3773 - val_loss: 0.3958\nEpoch 40/50\n1794/1794 [==============================] - 0s - loss: 0.3716 - val_loss: 0.3893\nEpoch 41/50\n1794/1794 [==============================] - 0s - loss: 0.3662 - val_loss: 0.3832\nEpoch 42/50\n1794/1794 [==============================] - 0s - loss: 0.3611 - val_loss: 0.3774\nEpoch 43/50\n1794/1794 [==============================] - 0s - loss: 0.3563 - val_loss: 0.3720\nEpoch 44/50\n1794/1794 [==============================] - 0s - loss: 0.3518 - val_loss: 0.3668\nEpoch 45/50\n1794/1794 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3619\nEpoch 46/50\n1794/1794 [==============================] - 0s - loss: 0.3435 - val_loss: 0.3573\nEpoch 47/50\n1794/1794 [==============================] - 0s - loss: 0.3397 - val_loss: 0.3529\nEpoch 48/50\n1794/1794 [==============================] - 0s - loss: 0.3360 - val_loss: 0.3488\nEpoch 49/50\n1794/1794 [==============================] - 0s - loss: 0.3325 - val_loss: 0.3448\nEpoch 50/50\n1794/1794 [==============================] - 0s - loss: 0.3293 - val_loss: 0.3411\n\n\n\u8fd4\u308a\u5024\uff08\u5b66\u7fd2\u904e\u7a0b\u30ed\u30b0\uff09\n\n\npython3 jupyter notebook\nys_ae_model_hist.history.keys()\n\n\ndict_keys(['val_loss', 'loss'])\n\n\npython3 jupyter notebook\nys_ae_model_hist.history['loss']\n\n\n[0.77204897245568704,\n 0.76187914377337984,\n 0.75235942148976231,\n 0.74326126970176309,\n 0.73441166702321536,\n 0.72563687601482851,\n 0.71679974100661514,\n 0.70780590897552675,\n 0.69854387963230125,\n 0.688945037133186,\n 0.67894317573528229,\n 0.66847907745984347,\n 0.65753565756905175,\n 0.64609300605163655,\n 0.63416777061113683,\n 0.62179123285489202,\n 0.60900205885420411,\n 0.59588177247988339,\n 0.58251192258751916,\n 0.56899962669763815,\n 0.55545178378040305,\n 0.54198058366509716,\n 0.52869019374135084,\n 0.51567526744759606,\n 0.50301985351271183,\n 0.49079081406162733,\n 0.47903453469808022,\n 0.46778750924628182,\n 0.45706960170165351,\n 0.44688783737196436,\n 0.43724177626595984,\n 0.42811654483195533,\n 0.41950143260700645,\n 0.41137394737639155,\n 0.40371354680130445,\n 0.39649425943692523,\n 0.3896950109547197,\n 0.38328832982641664,\n 0.37725168217384697,\n 0.37156175955747944,\n 0.36619435784011384,\n 0.36112665060238963,\n 0.35634202971107587,\n 0.35181737876390268,\n 0.34754009573090172,\n 0.34348928851959032,\n 0.3396503280437112,\n 0.33600748638511369,\n 0.33254830813328157,\n 0.32926098203313525]\n\n\npython3 jupyter notebook\nys_ae_model_hist.history['val_loss']\n\n\n[0.7603069543838501,\n 0.75278639793395996,\n 0.74574304223060606,\n 0.73890292644500732,\n 0.73209909796714778,\n 0.72517460584640503,\n 0.7180627644062042,\n 0.71067696809768677,\n 0.70297463536262517,\n 0.69490842819213872,\n 0.68644600510597231,\n 0.67755247354507442,\n 0.66821113824844358,\n 0.65841101408004765,\n 0.6481481730937958,\n 0.63743817806243896,\n 0.62632244825363159,\n 0.6148103713989258,\n 0.60298307538032536,\n 0.59089984297752385,\n 0.5786198645830154,\n 0.56623282432556155,\n 0.55382977724075322,\n 0.54148540198802952,\n 0.52927066981792448,\n 0.51727201938629153,\n 0.5055488228797913,\n 0.49413410127162932,\n 0.483080592751503,\n 0.47242816984653474,\n 0.46217039227485657,\n 0.45236197412014006,\n 0.44299422204494476,\n 0.43406962752342226,\n 0.42557652294635773,\n 0.4175167351961136,\n 0.40988496840000155,\n 0.40265323519706725,\n 0.39581511616706849,\n 0.38933811187744138,\n 0.38321558535099032,\n 0.37743270695209502,\n 0.37196222543716428,\n 0.36679600477218627,\n 0.36190806627273558,\n 0.35728473365306856,\n 0.35290284156799318,\n 0.34875249564647676,\n 0.34481717348098756,\n 0.34108110964298249]\n\n\n\u5b66\u7fd2\u4e2d \u306e loss\u306e\u63a8\u79fb \u3092 \u63cf\u753b\n\n\npython3 jupyter notebook\nloss = ys_ae_model_hist.history['loss']\nval_loss = ys_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\u672b\u5c3e10\u4ef6\u306e\u30c7\u30fc\u30bf \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u5024\u3068\u306e\u76f8\u95a2\u4fc2\u6570 \u3092 \u7b97\u51fa\n\n\u672c\u6765\u306f\u3001\u5b66\u7fd2\u6642\u306b\u4e0e\u3048\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u518d\u73fe\u7cbe\u5ea6 \u3092 \u691c\u8a3c\u3059\u308b\u5fc5\u8981 \u304c \u3042\u308b\u3002\n\uff08 \u691c\u8a3c\u30c7\u30fc\u30bf\u306f\u5168\u30c7\u30fc\u30bf\u306e\uff11\u5272\u3001\u3068\u8a2d\u5b9a\u3057\u3066\u306f\u3044\u305f\u304c \uff09\n\npython3 jupyter notebook\npredicted_value_list = autoencoder.predict(ys_array[-10:])\n\n\n\npython3 jupyter notebook\nfrom pprint import pprint\npprint(predicted_value_list)\n\n\narray([[ 0.73594105,  0.72225511,  0.7410332 ,  0.54851592,  0.67742962,\n         0.57801753,  0.53651255,  0.27468202,  0.37381202,  0.34580681,\n         0.12015434,  0.22161724,  0.11171708,  0.11844756,  0.07758933,\n         0.01668905,  0.06900305,  0.06583834,  0.08793348,  0.04965068,\n         0.02456675,  0.0236507 ,  0.02923935,  0.03159072,  0.04385547,\n         0.01638181,  0.02988848,  0.01914812,  0.02372373,  0.01619347,\n         0.01408893,  0.03196427,  0.03373871,  0.04592645,  0.08455544,\n         0.02531998,  0.01292785,  0.08512255,  0.06376205,  0.08022432,\n         0.11108302,  0.06954875,  0.10636622,  0.2902841 ,  0.07567588,\n         0.27879727,  0.27983958,  0.28210771,  0.50637126,  0.30316794,\n         0.62549382,  0.05440421,  0.61653072,  0.71850741,  0.83345568,\n         0.14307067,  0.55008024,  0.80726302,  0.90825933,  0.73716128,\n         0.82216775,  0.7813161 ,  0.7313447 ],\n       [ 0.733729  ,  0.71809208,  0.73514068,  0.5435183 ,  0.66907358,\n         0.56645191,  0.52723998,  0.26938403,  0.36436084,  0.34249806,\n         0.11959784,  0.21422067,  0.11017971,  0.11774467,  0.07451925,\n         0.01654441,  0.06893405,  0.06502064,  0.08485675,  0.04765516,\n         0.02469465,  0.02301026,  0.02847793,  0.03115889,  0.04317827,\n         0.01666928,  0.02952299,  0.01887808,  0.02336007,  0.01593957,\n         0.01405849,  0.03133192,  0.03430411,  0.0461445 ,  0.08579167,\n         0.02623304,  0.01295731,  0.08695532,  0.0656364 ,  0.08246984,\n         0.11445702,  0.0712442 ,  0.11212448,  0.29577145,  0.07584085,\n         0.28411964,  0.28272381,  0.28900096,  0.50751227,  0.3108463 ,\n         0.63533592,  0.05616919,  0.61515594,  0.72062379,  0.83252782,\n         0.14252836,  0.55875736,  0.80938905,  0.9084363 ,  0.73968405,\n         0.82038754,  0.77843523,  0.72832775],\n       [ 0.73144782,  0.71383685,  0.72910118,  0.53849864,  0.66056025,\n         0.55478871,  0.51793283,  0.26419768,  0.35504192,  0.33923796,\n         0.11909787,  0.20705871,  0.1087138 ,  0.11709987,  0.07160537,\n         0.01641853,  0.06890847,  0.06425421,  0.08192531,  0.04576982,\n         0.0248466 ,  0.02240865,  0.02776112,  0.03075993,  0.04254434,\n         0.01697996,  0.02918763,  0.01863089,  0.0230243 ,  0.01570642,\n         0.01404369,  0.03073879,  0.03490801,  0.04639864,  0.08709227,\n         0.02720362,  0.01300177,  0.08887239,  0.06760614,  0.0848212 ,\n         0.11797456,  0.07302306,  0.11820778,  0.30136323,  0.07605208,\n         0.28955019,  0.28567237,  0.29603648,  0.50864416,  0.3186703 ,\n         0.64502329,  0.0580289 ,  0.61374146,  0.7226721 ,  0.83153045,\n         0.1420446 ,  0.56737715,  0.81143236,  0.90855801,  0.74213326,\n         0.81852782,  0.7754637 ,  0.72523248],\n       [ 0.72979778,  0.71024942,  0.72308356,  0.53334963,  0.65208673,\n         0.54426682,  0.50862783,  0.25842863,  0.34649891,  0.33693671,\n         0.11846694,  0.20022689,  0.10746393,  0.11644603,  0.06889965,\n         0.01633854,  0.06884535,  0.06350876,  0.07890771,  0.0439423 ,\n         0.02493025,  0.02178143,  0.02712904,  0.03050009,  0.0419331 ,\n         0.01726541,  0.02875838,  0.01835509,  0.02264947,  0.01545038,\n         0.0140192 ,  0.03028108,  0.03560189,  0.04681312,  0.08831328,\n         0.02814098,  0.0131101 ,  0.09046544,  0.06941704,  0.08747324,\n         0.12114646,  0.0745055 ,  0.12493989,  0.30690733,  0.07632991,\n         0.294471  ,  0.2885122 ,  0.3027913 ,  0.50923568,  0.32582393,\n         0.65443295,  0.05977015,  0.61267525,  0.72526211,  0.8302238 ,\n         0.14149216,  0.57578909,  0.81288671,  0.90868682,  0.74354064,\n         0.81626701,  0.77169472,  0.72155625],\n       [ 0.72838098,  0.70690531,  0.71699625,  0.52813667,  0.64355201,\n         0.53420484,  0.49931821,  0.25250176,  0.33835238,  0.33506089,\n         0.11781451,  0.19366014,  0.10634047,  0.11582182,  0.06635568,\n         0.01628798,  0.06879146,  0.06280066,  0.07594606,  0.04219799,\n         0.02499847,  0.02116678,  0.02655305,  0.03031433,  0.04135505,\n         0.01755309,  0.0283094 ,  0.01808092,  0.02227516,  0.01519708,\n         0.01399964,  0.0298974 ,  0.03636079,  0.04731946,  0.08953599,\n         0.02909533,  0.01325531,  0.09195744,  0.07120647,  0.09033319,\n         0.12423398,  0.0758974 ,  0.13219135,  0.31248564,  0.07666326,\n         0.29922265,  0.29134056,  0.30949306,  0.50959349,  0.33274963,\n         0.66362816,  0.06150809,  0.61173451,  0.72803396,  0.82874185,\n         0.14094491,  0.58407104,  0.81405413,  0.90878683,  0.74447626,\n         0.81378192,  0.7675159 ,  0.7175799 ],\n       [ 0.7269029 ,  0.70348597,  0.71076947,  0.52290851,  0.63488168,\n         0.52409875,  0.49000126,  0.24671496,  0.3303355 ,  0.33322498,\n         0.11721929,  0.18731335,  0.10527967,  0.11525478,  0.06394097,\n         0.01625498,  0.06878101,  0.06214106,  0.07313178,  0.04055157,\n         0.02509043,  0.02058977,  0.02601322,  0.03015648,  0.04081708,\n         0.01786439,  0.02789226,  0.01782932,  0.02192859,  0.01496413,\n         0.01399564,  0.02954487,  0.03716575,  0.0478667 ,  0.09082279,\n         0.03010837,  0.01341738,  0.09352115,  0.07308438,  0.09332775,\n         0.12744448,  0.07735961,  0.13985269,  0.31816021,  0.07704419,\n         0.3040626 ,  0.29422975,  0.31631491,  0.50994205,  0.33978379,\n         0.67265207,  0.06333612,  0.61075592,  0.73072875,  0.82718396,\n         0.14045629,  0.59227753,  0.81515121,  0.90883183,  0.74535125,\n         0.81120551,  0.7632218 ,  0.71351373],\n       [ 0.72536319,  0.6999917 ,  0.704404  ,  0.51766777,  0.62608141,\n         0.51395935,  0.48068604,  0.2410683 ,  0.32245255,  0.33142927,\n         0.11668076,  0.18118317,  0.10428045,  0.11474442,  0.06164943,\n         0.01623947,  0.06881401,  0.06152886,  0.07045811,  0.03899764,\n         0.02520637,  0.02004845,  0.02550804,  0.03002618,  0.04031809,\n         0.01820029,  0.02750594,  0.01759955,  0.02160882,  0.01475078,\n         0.01400718,  0.02922259,  0.0380188 ,  0.04845591,  0.09217536,\n         0.03118355,  0.01359682,  0.09515864,  0.07505444,  0.09646242,\n         0.13078159,  0.07889453,  0.14793916,  0.3239294 ,  0.07747317,\n         0.30899   ,  0.29717973,  0.32325432,  0.51028126,  0.34692311,\n         0.68149918,  0.06525838,  0.60973954,  0.73334688,  0.82554907,\n         0.14002593,  0.60040247,  0.81617844,  0.90882182,  0.74616593,\n         0.80853623,  0.75881124,  0.70935792],\n       [ 0.7237618 ,  0.69642258,  0.69790101,  0.51241696,  0.61715752,\n         0.50379795,  0.47138187,  0.2355614 ,  0.31470788,  0.32967392,\n         0.1161984 ,  0.17526604,  0.10334171,  0.11429019,  0.05947509,\n         0.01624138,  0.06889052,  0.06096307,  0.06791853,  0.03753109,\n         0.02534656,  0.01954101,  0.02503605,  0.02992313,  0.03985702,\n         0.01856187,  0.02714945,  0.01739092,  0.02131493,  0.01455638,\n         0.01403431,  0.02892984,  0.03892203,  0.04908822,  0.09359549,\n         0.03232447,  0.01379424,  0.09687206,  0.07712036,  0.0997428 ,\n         0.1342489 ,  0.08050464,  0.15646495,  0.32979146,  0.07795085,\n         0.31400385,  0.30019006,  0.33030838,  0.51061106,  0.35416403,\n         0.69016457,  0.06727909,  0.60868567,  0.73588848,  0.82383615,\n         0.13965358,  0.6084401 ,  0.81713647,  0.90875673,  0.74692029,\n         0.80577284,  0.75428361,  0.70511264],\n       [ 0.72209871,  0.69277883,  0.69126171,  0.50715888,  0.60811681,\n         0.4936257 ,  0.46209767,  0.2301939 ,  0.30710483,  0.3279593 ,\n         0.11577175,  0.16955787,  0.1024624 ,  0.11389167,  0.05741234,\n         0.01626073,  0.06901069,  0.06044275,  0.06550682,  0.03614709,\n         0.02551136,  0.01906575,  0.02459596,  0.02984708,  0.03943282,\n         0.01895024,  0.02682191,  0.01720282,  0.02104606,  0.01438031,\n         0.0140771 ,  0.02866583,  0.03987772,  0.04976487,  0.09508491,\n         0.03353506,  0.01401024,  0.0986636 ,  0.07928607,  0.10317458,\n         0.13785011,  0.08219247,  0.16544363,  0.33574444,  0.07847781,\n         0.31910303,  0.30326062,  0.33747393,  0.51093125,  0.36150271,\n         0.69864458,  0.06940277,  0.60759455,  0.73835415,  0.82204425,\n         0.13933884,  0.6163848 ,  0.81802583,  0.90863657,  0.7476145 ,\n         0.80291378,  0.74963796,  0.70077825],\n       [ 0.72037357,  0.68906099,  0.68448782,  0.50189602,  0.59896636,\n         0.4834539 ,  0.45284268,  0.2249649 ,  0.29964685,  0.32628542,\n         0.11540043,  0.1640545 ,  0.10164149,  0.11354861,  0.05545575,\n         0.01629759,  0.06917458,  0.05996701,  0.06321692,  0.03484115,\n         0.02570114,  0.01862111,  0.02418652,  0.02979784,  0.03904467,\n         0.01936666,  0.02652246,  0.01703467,  0.02080148,  0.014222  ,\n         0.01413569,  0.02842995,  0.04088825,  0.05048718,  0.09664558,\n         0.03481942,  0.01424557,  0.1005355 ,  0.08155561,  0.10676365,\n         0.1415889 ,  0.08396073,  0.17488745,  0.34178618,  0.07905488,\n         0.32428643,  0.30639085,  0.34474742,  0.51124179,  0.36893475,\n         0.70693517,  0.07163405,  0.60646623,  0.74074394,  0.82017219,\n         0.13908161,  0.62423152,  0.81884694,  0.90846097,  0.7482487 ,\n         0.79995787,  0.74487394,  0.6963551 ]], dtype=float32)\n\n\n\u5165\u529b\u30c7\u30fc\u30bf\u4ef6\u6570 \u3068 \u540c\u3058\u300110\u4ef6 \u306e \u51fa\u529b\u5024\uff08\u5165\u529b\u5024\u306e\u518d\u73fe\u5024\uff09 \u304c \u5f97\u3089\u308c\u305f\n\n\npython3 jupyter notebook\nprint(len(predicted_value_list))\n\n\n10\n\n\n\uff08 \u4eca\u56de\u3001\u6271\u3063\u305f\u306e \u306f\u3001\u4ee5\u4e0b\u306e\u6319\u52d5 \u3092 \u793a\u3059\u6642\u7cfb\u5217\u30c7\u30fc\u30bf \uff09\n\n\npython3 jupyter notebook\nprint(len(predicted_value_list[0]))\n\n\n63\n\n\n\uff11\u3064\u76ee\u306e\u5165\u529b\u30c7\u30fc\u30bf \u306f\u3001\u51fa\u529b\u5074\u3067\u3001\u518d\u73fe\u6027\u306e\u3042\u308b\u30c7\u30fc\u30bf \u3092 \u751f\u6210\u3067\u304d\u3066\u3044\u308b\u3002\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: 0.8730.873\n\npython3 jupyter notebook\nnp.corrcoef(ys_array[-10], predicted_value_list[0])\n\n\narray([[ 1.        ,  0.87392303],\n       [ 0.87392303,  1.        ]])\n\n\npython3 jupyter notebook\nnp.corrcoef(ys_array[-10], predicted_value_list[0])[0][1]\n\n\n0.87392302591375859\n\n\npython3 jupyter notebook\nplt.plot(ys_array[-10], predicted_value_list[0])\n\n\n[<matplotlib.lines.Line2D at 0x120fad080>]\n\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: 0.8740.874\n\npython3 jupyter notebook\nnp.corrcoef(ys_array[-9], predicted_value_list[1])[0][1]\n\n\n0.87405657153157468\n\n\npython3 jupyter notebook\nplt.plot(ys_array[-9], predicted_value_list[1])\n\n\n[<matplotlib.lines.Line2D at 0x121f7f898>]\n\n\n\npython3 jupyter notebook\nnp.corrcoef(ys_array[-8], predicted_value_list[2])[0][1]\n\n\n0.87415094764373547\n\n\npython3 jupyter notebook\nplt.plot(ys_array[-8], predicted_value_list[2])\n\n\n[<matplotlib.lines.Line2D at 0x11f4a4be0>]\n\n\n\npython3 jupyter notebook\n# \u5165\u529b\u5024\u3068\u51fa\u529b\u5024\u306f\u76f8\u95a2\u4fc2\u6570\u304c\u9ad8\u3044\u3002\uff1d\uff1e\u51fa\u529b\u5024\u306f\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u3002\n\n\n\npython3 jupyter notebook\n# \u5b66\u7fd2\u3055\u305b\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u3092\u5165\u529b\u3057\u3066\u307f\u308b\u3002\nrandon_cumsum_array = np.array(randn_cumsum_series).reshape(1994, 63)\n\n\n\npython3 jupyter notebook\npd.DataFrame(randon_cumsum_array).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x121000470>\n\n\n\npython3 jupyter notebook\npd.DataFrame(ys_array).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x1209e8ef0>\n\n\n\n\u4eca\u5ea6\u306f\u3001\u5b66\u7fd2\u6642 \u306b \u4e0e\u3048\u3066\u3044\u306a\u3044\u65b0\u898f\u30c7\u30fc\u30bf \u304c \u518d\u73fe\u3055\u308c\u300e\u306a\u3044\u300f\u3053\u3068 \u3092 \u78ba\u8a8d\u3059\u308b\u3002\n\n\npython3 jupyter notebook\npredicted_value_list = autoencoder.predict(randon_cumsum_array[-10:])\n\n\n\npython3 jupyter notebook\nlen(predicted_value_list)\n\n\n10\n\n\npython3 jupyter notebook\nprint(predicted_value_list[0])\n\n\n[  1.72828807e-21   2.12488513e-18   8.13269881e-32   0.00000000e+00\n   0.00000000e+00   0.00000000e+00   8.53706776e-21   7.88663642e-16\n   1.00000000e+00   0.00000000e+00   1.00000000e+00   8.83193148e-18\n   5.17360605e-30   4.57608916e-20   1.00000000e+00   0.00000000e+00\n   1.00000000e+00   2.03638096e-11   1.20015812e-11   1.00000000e+00\n   1.00000000e+00   5.81137497e-26   6.16388948e-27   0.00000000e+00\n   1.00000000e+00   2.78305106e-38   1.01486821e-23   0.00000000e+00\n   1.00000000e+00   0.00000000e+00   9.99998450e-01   1.95422539e-31\n   9.52638745e-01   1.00000000e+00   5.48459778e-35   0.00000000e+00\n   3.53405721e-06   1.00000000e+00   1.00000000e+00   3.29031007e-17\n   9.95175838e-01   1.00000000e+00   3.15579701e-10   1.00000000e+00\n   1.40501963e-27   1.00000000e+00   4.27256243e-29   1.00000000e+00\n   1.00000000e+00   9.71367717e-01   1.29174707e-32   1.00000000e+00\n   3.22033428e-02   4.78385709e-14   7.97672728e-27   4.99435331e-28\n   7.83956060e-12   1.00000000e+00   1.32813274e-10   0.00000000e+00\n   1.00000000e+00   1.00000000e+00   9.56023505e-05]\n\n\n\u60f3\u5b9a\u901a\u308a\u3001\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2 \u306f \u4f4e\u3044\u3002\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: \u22120.204-0.204\n\npython3 jupyter notebook\n# \u60f3\u5b9a\u901a\u308a\u3001\u4f4e\u3044\nnp.corrcoef(randon_cumsum_array[-10], predicted_value_list[0])[0][1]\n\n\n-0.20473221757684804\n\n\n\n\u5b66\u7fd2\u3055\u305b\u305f\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3 \u304c\u3001\u901a\u5e38\uff08\u6b63\u5e38\u6642\uff09\u306e\u6ce2\u5f62\u30d1\u30bf\u30fc\u30f3 \u3067 \u3042\u308b \u5834\u5408\u3001\u30e2\u30c7\u30eb\u304c \u518d\u73fe\u3067\u304d\u306a\u3044 \u65b0\u898f\u5165\u529b\u5024 \n\n\u306f\u3001\n\n\u5b66\u7fd2\u6642 \u306b \u7d4c\u9a13\u3057\u305f\u300c\u6b63\u5e38\u30d1\u30bf\u30fc\u30f3\u300d\u3068\u306f\u7570\u306a\u308b\uff08\u7570\u5e38\u72b6\u614b\u306e\uff09\u30d1\u30bf\u30fc\u30f3\u7cfb\u5217\u30c7\u30fc\u30bf \u3067\u3042\u308b \u53ef\u80fd\u6027 \u3092 \u793a\u5506\u3057\u3066\u3044\u308b\u3002\n\n\n\npython3 jupyter notebook\nplt.plot(randon_cumsum_array[-10], predicted_value_list[0])\n\n\n[<matplotlib.lines.Line2D at 0x120476da0>]\n\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: 0.1420.142\n\npython3 jupyter notebook\n# \u60f3\u5b9a\u901a\u308a\u3001\u4f4e\u3044\nnp.corrcoef(randon_cumsum_array[-9], predicted_value_list[1])[0][1]\n\n\n0.14285102123552723\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: 0.0650.065\n\npython3 jupyter notebook\n# \u5c11\u3057\u4e0a\u304c\u3063\u305f\u304c\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u3068\u306f\u8a00\u3048\u306a\u3044\u3002\nnp.corrcoef(randon_cumsum_array[-8], predicted_value_list[2])[0][1]\n\n\n0.065785686223646611\n\n\n\n\uff13\u3064\u306e\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3 \u3092 \u518d\u73fe\u3067\u304d\u308b AE\u30e2\u30c7\u30eb \u3092 \u69cb\u7bc9\u3059\u308b\u3002\n\n\u69cb\u7bc9\u3057\u305f\u30e2\u30c7\u30eb \u306f\u3001\n\nmodel.predict()\u30e1\u30bd\u30c3\u30c9 \u3067\u3001\uff11\u5ea6\u306b\u3000\uff11\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff0863\u6642\u70b9\u306e\u584a\u30c7\u30fc\u30bf\uff09 \u3092 \u53d7\u3051\u53d6\u308a\u3001\n\u53d7\u3051\u53d6\u3063\u305f\u5165\u529b\u30c7\u30fc\u30bf \u306e \u518d\u73fe\u63a8\u5b9a\u5024 \u3092 1\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5206\uff0863\u6642\u70b9\u306e\u584a\u30c7\u30fc\u30bf\uff09 \u3092 \u51fa\u529b\u3059\u308b\u3002\n\n\npython3 jupyter notebook\n# \u5148\u307b\u3069\u306f\u3001sin\u66f2\u7dda\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u5b66\u7fd2\u3055\u305b\u308bAutoEncoder\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u305f\u3002\n# \u4eca\u5ea6\u306f\u30013\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u30013\u30d1\u30bf\u30fc\u30f3\u306e\u5165\u529b\u5024\u306f\u3061\u3083\u3093\u3068\u518d\u73fe\u3067\u304d\u308bAE\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u3002\n# \u65b0\u898f\u306b\u6295\u5165\u3059\u308b\u5165\u529b\u5024\u3068\u3001\u305d\u306e\u51fa\u529b\u5024\u306e\u76f8\u95a2\u4fc2\u6570\u304c\u4f4e\u3051\u308c\u3070\u3001\u65b0\u898f\u5165\u529b\u3057\u305f\u30d1\u30bf\u30fc\u30f3\u306f\u3001\n# \u5b66\u7fd2\u3057\u305f3\u30d1\u30bf\u30fc\u30f3\u306e\u3044\u305a\u308c\u3068\u3082\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3067\u3042\u308b\u3068\u63a8\u6e2c\u3067\u304d\u308b\u3002\n\n\n\npython3 jupyter notebook\nimport types\ntype(normal_distribution_cumsum_series)\n\n\nnumpy.ndarray\n\n\npython3 jupyter notebook\nnorm_dist_cumsum_array = normal_distribution_cumsum_series.reshape(1994, 63)\n\n\n\npython3 jupyter notebook\nrandom_cumsum_array = randon_cumsum_array.reshape(1994, 63)\n\n\n\npython3 jupyter notebook\nprint(norm_dist_cumsum_array.shape,\n      random_cumsum_array.shape)\n\n\n(1994, 63) (1994, 63)\n\n\npython3 jupyter notebook\nthree_pattern_time_series_joined_array = np.vstack((ys_array, norm_dist_cumsum_array))\n\n\n\npython3 jupyter notebook\nthree_pattern_time_series_joined_array.shape\n\n\n(3988, 63)\n\n\npython3 jupyter notebook\nthree_pattern_time_series_joined_array = np.vstack((three_pattern_time_series_joined_array, random_cumsum_array))\n\n\n\npython3 jupyter notebook\nthree_pattern_time_series_joined_array.shape\n\n\n(5982, 63)\n\n\npython3 jupyter notebook\n1994*3 == 5982\n\n\nTrue\n\n\npython3 jupyter notebook\nthree_patterns_data = np.random.permutation(three_pattern_time_series_joined_array)\n\n\n\npython3 jupyter notebook\nthree_patterns_data.shape\n\n\n(5982, 63)\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[0])\nplt.show()\n\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[2])\n\n\n[<matplotlib.lines.Line2D at 0x12078c710>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[3])\n\n\n[<matplotlib.lines.Line2D at 0x120a946a0>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[4])\n\n\n[<matplotlib.lines.Line2D at 0x121815710>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[5])\n\n\n[<matplotlib.lines.Line2D at 0x1219fe518>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[6])\n\n\n[<matplotlib.lines.Line2D at 0x120688a20>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[7])\n\n\n[<matplotlib.lines.Line2D at 0x120940da0>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[8])\n\n\n[<matplotlib.lines.Line2D at 0x1226c7d30>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[9])\n\n\n[<matplotlib.lines.Line2D at 0x1227e1c18>]\n\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[10])\n\n\n[<matplotlib.lines.Line2D at 0x12e55ca20>]\n\n\n\n\u518d\u5ea6\u3001\u30e2\u30c7\u30eb \u3092 \u69cb\u7bc9\uff08\u5b66\u7fd2\uff09\u3055\u305b\u3066\u307f\u308b\n\n\npython3 jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=300,\n                                               batch_size=40,\n                                               shuffle=True,\n                                               validation_split=0.2)\n\n\nTrain on 4785 samples, validate on 1197 samples\nEpoch 1/300\n4785/4785 [==============================] - 0s - loss: 6721223.6275 - val_loss: 6822094.8446\nEpoch 2/300\n4785/4785 [==============================] - 0s - loss: 6721165.0637 - val_loss: 6822094.8446\nEpoch 3/300\n4785/4785 [==============================] - 0s - loss: 6721165.1165 - val_loss: 6822094.8446\nEpoch 4/300\n4785/4785 [==============================] - 0s - loss: 6721145.5157 - val_loss: 6822067.5079\nEpoch 5/300\n4785/4785 [==============================] - 0s - loss: 6721134.8430 - val_loss: 6822067.5079\nEpoch 6/300\n4785/4785 [==============================] - 0s - loss: 6721134.9310 - val_loss: 6822067.5079\nEpoch 7/300\n4785/4785 [==============================] - 0s - loss: \n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 194/300\n4785/4785 [==============================] - 0s - loss: 6720885.7926 - val_loss: 6821814.5998\nEpoch 195/300\n4785/4785 [==============================] - 0s - loss: 6720885.8621 - val_loss: 6821814.5998\nEpoch 196/300\n4785/4785 [==============================] - 0s - loss: 6720885.8568 - val_loss: 6821814.5998\nEpoch 197/300\n4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\nEpoch 198/300\n4785/4785 [==============================] - 0s - loss: 6720885.8809 - val_loss: 6821814.5998\nEpoch 199/300\n4785/4785 [==============================] - 0s - loss: 6720885.8250 - val_loss: 6821814.5998\nEpoch 200/300\n4785/4785 [==============================] - 0s - loss: 6720885.8427 - val_loss: 6821814.5998\nEpoch 201/300\n4785/4785 [==============================] - 0s - loss: 6720885.8203 - val_loss: 6821814.5998\nEpoch 202/300\n4785/4785 [==============================] - 0s - loss: 6720885.8777 - val_loss: 6821814.5998\nEpoch 203/300\n4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\nEpoch 204/300\n4785/4785 [==============================] - 0s - loss: 6720885.8234 - val_loss: 6821814.5998\nEpoch 205/300\n4785/4785 [==============================] - 0s - loss: 6720885.8302 - val_loss: 6821814.5998\nEpoch 206/300\n4785/4785 [==============================] - 0s - loss: 6720885.8579 - val_loss: 6821814.5998\nEpoch 207/300\n4785/4785 [==============================] - 0s - loss: 6720885.8297 - val_loss: 6821814.5998\nEpoch 208/300\n4785/4785 [==============================] - 0s - loss: 6720885.8673 - val_loss: 6821814.5998\nEpoch 209/300\n4785/4785 [==============================] - 0s - loss: 6720885.8490 - val_loss: 6821814.5998\nEpoch 210/300\n4785/4785 [==============================] - 0s - loss: 6720885.8036 - val_loss: 6821814.5998\nEpoch 211/300\n4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\nEpoch 212/300\n4785/4785 [==============================] - 0s - loss: 6720885.8848 - val_loss: 6821814.5998\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 270/300\n4785/4785 [==============================] - 0s - loss: 6720885.8464 - val_loss: 6821814.5581\nEpoch 271/300\n4785/4785 [==============================] - 0s - loss: 6720885.8626 - val_loss: 6821814.5581\nEpoch 272/300\n4785/4785 [==============================] - 0s - loss: 6720885.8501 - val_loss: 6821814.5581\nEpoch 273/300\n4785/4785 [==============================] - 0s - loss: 6720885.8589 - val_loss: 6821814.5581\nEpoch 274/300\n4785/4785 [==============================] - 0s - loss: 6720885.8490 - val_loss: 6821814.5581\nEpoch 275/300\n4785/4785 [==============================] - 0s - loss: 6720885.8357 - val_loss: 6821814.5581\nEpoch 276/300\n4785/4785 [==============================] - 0s - loss: 6720885.8757 - val_loss: 6821814.5581\nEpoch 277/300\n4785/4785 [==============================] - 0s - loss: 6720885.8140 - val_loss: 6821814.5581\nEpoch 278/300\n4785/4785 [==============================] - 0s - loss: 6720885.8474 - val_loss: 6821814.5581\nEpoch 279/300\n4785/4785 [==============================] - 0s - loss: 6720885.8354 - val_loss: 6821814.5581\nEpoch 280/300\n4785/4785 [==============================] - 0s - loss: 6720885.8997 - val_loss: 6821814.5581\nEpoch 281/300\n4785/4785 [==============================] - 0s - loss: 6720885.8171 - val_loss: 6821814.5581\nEpoch 282/300\n4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5581\nEpoch 283/300\n4785/4785 [==============================] - 0s - loss: 6720885.8370 - val_loss: 6821814.5581\nEpoch 284/300\n4785/4785 [==============================] - 0s - loss: 6720885.8375 - val_loss: 6821814.5581\nEpoch 285/300\n4785/4785 [==============================] - 0s - loss: 6720885.8299 - val_loss: 6821814.5581\nEpoch 286/300\n4785/4785 [==============================] - 0s - loss: 6720885.1541 - val_loss: 6821805.3400\nEpoch 287/300\n4785/4785 [==============================] - 0s - loss: 6720875.6938 - val_loss: 6821805.3400\nEpoch 288/300\n4785/4785 [==============================] - 0s - loss: 6720875.7273 - val_loss: 6821805.3400\nEpoch 289/300\n4785/4785 [==============================] - 0s - loss: 6720875.7299 - val_loss: 6821805.3400\nEpoch 290/300\n4785/4785 [==============================] - 0s - loss: 6720875.7011 - val_loss: 6821805.3400\nEpoch 291/300\n4785/4785 [==============================] - 0s - loss: 6720875.7670 - val_loss: 6821805.3400\nEpoch 292/300\n4785/4785 [==============================] - 0s - loss: 6720875.7560 - val_loss: 6821805.3400\nEpoch 293/300\n4785/4785 [==============================] - 0s - loss: 6720875.8323 - val_loss: 6821805.3400\nEpoch 294/300\n4785/4785 [==============================] - 0s - loss: 6720875.7858 - val_loss: 6821805.3400\nEpoch 295/300\n4785/4785 [==============================] - 0s - loss: 6720875.7879 - val_loss: 6821805.3400\nEpoch 296/300\n4785/4785 [==============================] - 0s - loss: 6720875.7168 - val_loss: 6821805.3400\nEpoch 297/300\n4785/4785 [==============================] - 0s - loss: 6720875.6907 - val_loss: 6821805.3400\nEpoch 298/300\n4785/4785 [==============================] - 0s - loss: 6720875.7429 - val_loss: 6821805.3400\nEpoch 299/300\n4785/4785 [==============================] - 0s - loss: 6720875.7894 - val_loss: 6821805.3400\nEpoch 300/300\n4785/4785 [==============================] - 0s - loss: 6720875.7764 - val_loss: 6821805.3400\n\n\npython3 jupyter notebook\nloss = three_patterns_ae_model_hist.history['loss']\nval_loss = three_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306eloss \u304c \u5168\u304f \u6e1b\u5c11 \u3057\u306a\u3044\u3002\n\n\npython3 jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067\u306eloss\u304c\u5168\u304f\u6e1b\u3089\u306a\u3044\u3002\n\n\n\n\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e2d\u9593\u5c64 \u3092 \uff11\u5c64 \u8ffd\u52a0\u3057\u3066\u3001\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\n\n\npython3 jupyter notebook\ninput = Input(shape=(63,))\nencoded_1 = Dense(40, activation='relu')(input)\nencoded_2 = Dense(20, activation='relu')(encoded_1)\nencoded_3 = Dense(40, activation='relu')(encoded_2)\ndecoded = Dense(63, activation='sigmoid')(encoded_3)\nautoencoder = Model(input=input, output=decoded)\n\n\n\uff08 \u53c2\u8003 \uff09\n\nTypeNULL\u3055\u3093 Qiitam\u8a18\u4e8b\u300cKeras\u3092\u4f7f\u3063\u305fAutoEncodder\u306e\u30e1\u30e2\u300d\n\n\npython3 jupyter notebook\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\n# \uff08\u53c2\u8003\uff09http://qiita.com/TypeNULL/items/4e4d7de11ab4361d6085\n# \uff08\u4e0a\u8a18\u8a18\u4e8b\uff09MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u3067\u7279\u5fb4\u62bd\u51fa\u3092\u3084\u308b\u4f8b\u306f\u591a\u3044\u304c\uff0c\u672c\u8a18\u4e8b\u3067\u306f\u753b\u50cf\u4ee5\u5916\u306e\u4fe1\u53f7\u3092\u6271\u3046\u3053\u3068\u306b\u3059\u308b\uff0e\nautoencoder.compile(optimizer='adadelta', loss='mse')\n#autoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n\n\n\npython3 jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\n#3\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u6df7\u3058\u3063\u3066\u3044\u308b\u304b\u3089\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306f\u3057\u306a\u3044\u65b9\u304c\u826f\u3044\u304b\u3082\u3057\u308c\u306a\u3044\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=1000,\n                                               shuffle=True,\n                                               validation_split=0.1)\n\n\nTrain on 5383 samples, validate on 599 samples\nEpoch 1/1000\n5383/5383 [==============================] - 1s - loss: 6763711.8862 - val_loss: 6545538.1327\nEpoch 2/1000\n5383/5383 [==============================] - 0s - loss: 6763476.6957 - val_loss: 6545385.9658\nEpoch 3/1000\n5383/5383 [==============================] - 0s - loss: 6763339.6385 - val_loss: 6545251.5868\nEpoch 4/1000\n5383/5383 [==============================] - 0s - loss: 6763156.2270 - val_loss: 6545023.5601\nEpoch 5/1000\n5383/5383 [==============================] - 0s - loss: 6763010.0146 - val_loss: 6544992.0684\nEpoch 6/1000\n5383/5383 [==============================] - 0s - loss: 6762980.2384 - val_loss: 6544951.2805\nEpoch 7/1000\n5383/5383 [==============================] - 0s - loss: \n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 152/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3166 - val_loss: 6544495.6895\nEpoch 153/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3432 - val_loss: 6544495.7162\nEpoch 154/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3362 - val_loss: 6544495.7162\nEpoch 155/1000\n5383/5383 [==============================] - 1s - loss: 6762488.2878 - val_loss: 6544495.7162\nEpoch 156/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3259 - val_loss: 6544495.7162\nEpoch 157/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3532 - val_loss: 6544495.7162\nEpoch 158/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3567 - val_loss: 6544495.7162\nEpoch 159/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3218 - val_loss: 6544495.7162\nEpoch 160/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3059 - val_loss: 6544495.7162\nEpoch 161/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3103 - val_loss: 6544495.7162\nEpoch 162/1000\n5383/5383 [==============================] - 1s - loss: 6762488.2997 - val_loss: 6544495.7162\nEpoch 163/1000\n5383/5383 [==============================] - 1s - loss: 6762488.2843 - val_loss: 6544495.7162\nEpoch 164/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3346 - val_loss: 6544495.7162\nEpoch 165/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3615 - val_loss: 6544495.6895\nEpoch 166/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3164 - val_loss: 6544495.6895\nEpoch 167/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3279 - val_loss: 6544495.7162\nEpoch 168/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3340 - val_loss: 6544495.6895\nEpoch 169/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3275 - val_loss: 6544495.6895\nEpoch 170/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3415 - val_loss: 6544495.7162\nEpoch 171/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3443 - val_loss: 6544495.6895\nEpoch 172/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3103 - val_loss: 6544495.7162\nEpoch 173/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3602 - val_loss: 6544495.6895\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 668/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3229 - val_loss: 6544495.6761\nEpoch 669/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3171 - val_loss: 6544495.6761\nEpoch 670/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3127 - val_loss: 6544495.6761\nEpoch 671/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3294 - val_loss: 6544495.6761\nEpoch 672/1000\n5383/5383 [==============================] - 1s - loss: 6762488.2898 - val_loss: 6544495.6761\nEpoch 673/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3645 - val_loss: 6544495.6761\nEpoch 674/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3316 - val_loss: 6544495.6761\nEpoch 675/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3396 - val_loss: 6544495.6761\nEpoch 676/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3383 - val_loss: 6544495.6761\nEpoch 677/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3346 - val_loss: 6544495.6761\nEpoch 678/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3231 - val_loss: 6544495.6761\nEpoch 679/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3384 - val_loss: 6544495.6761\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 996/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3163 - val_loss: 6544495.6761\nEpoch 997/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3091 - val_loss: 6544495.6761\nEpoch 998/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3337 - val_loss: 6544495.6761\nEpoch 999/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3074 - val_loss: 6544495.6761\nEpoch 1000/1000\n5383/5383 [==============================] - 1s - loss: 6762488.3032 - val_loss: 6544495.6761\n\n\n\uff11epoch\u76ee\u306eloss\u6841\u6570 \u304c \u5927\u304d\u304f\u30012poch\u4ee5\u964d\u3001\u4e00\u5411\u306b\u6e1b\u3089\u306a\u3044\n\n\npython3 jupyter notebook\nloss = three_patterns_ae_model_hist.history['loss']\nval_loss = three_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\npython3 jupyter notebook\npredicted_value_list = autoencoder.predict(three_patterns_data[-10:])\n\n\n\npython3 jupyter notebook\nlen(predicted_value_list)\n\n\n10\n\n\npython3 jupyter notebook\nplt.plot(three_patterns_data[-10], predicted_value_list[0])\n\n\n[<matplotlib.lines.Line2D at 0x12eff9ef0>]\n\n\n\npython3 jupyter notebook\nnp.corrcoef(three_patterns_data[-10], predicted_value_list[0])[0][1]\n\n\nnan\n\n\npython3 jupyter notebook\nprint(three_patterns_data[-10])\n\n\n[-7017.66048331 -7046.42995364 -7043.87903589 -7033.23990255 -7009.52368735\n -6996.97377304 -6985.96200355 -6960.01459759 -6963.82866094 -6950.98907808\n -6964.30881299 -6983.79644211 -6960.09471708 -6958.23402409 -6966.17581173\n -6968.1895467  -6959.86238021 -6993.08741239 -6993.31397102 -6980.00869287\n -6968.38319723 -6944.49216612 -6910.21318362 -6861.2121015  -6877.91296188\n -6896.34493134 -6894.97624443 -6871.04568337 -6866.45038733 -6860.04245111\n -6843.32247391 -6827.58851595 -6816.65713122 -6774.79431096 -6766.78811715\n -6799.50464858 -6809.11836503 -6824.48740609 -6818.03744908 -6825.63135168\n -6807.72369516 -6801.09578247 -6801.60608541 -6808.32067788 -6820.98796303\n -6827.79351801 -6852.98165785 -6841.154643   -6851.66623672 -6866.94962678\n -6849.72897592 -6823.86860429 -6821.80858664 -6822.50489885 -6833.57137963\n -6838.93244414 -6849.26406901 -6832.61767028 -6854.64057913 -6832.13625882\n -6848.82611789 -6808.18966364 -6810.72885219]\n\n\npython3 jupyter notebook\nlen(three_patterns_data[-10])\n\n\n63\n\n\npython3 jupyter notebook\nprint(predicted_value_list[0])\n\n\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\npython3 jupyter notebook\nlen(predicted_value_list[0])\n\n\n63\n\n\npython3 jupyter notebook\nnp.corrcoef(three_patterns_data[-9], predicted_value_list[1])[0][1]\n\n\nnan\n\n\npython3 jupyter notebook\nprint(predicted_value_list[1])\n\n\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\npython3 jupyter notebook\nprint(predicted_value_list[2])\n\n\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\npython3 jupyter notebook\nprint(predicted_value_list[3])\n\n\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\n\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\n\n\npython3 jupyter notebook\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n\n\n\npython3 jupyter notebook\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n\n\n\npython3 jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\n#3\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u6df7\u3058\u3063\u3066\u3044\u308b\u304b\u3089\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306f\u3057\u306a\u3044\u65b9\u304c\u826f\u3044\u304b\u3082\u3057\u308c\u306a\u3044\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=400,\n                                               shuffle=True,\n                                               validation_split=0.1)\n\n\nTrain on 5383 samples, validate on 599 samples\nEpoch 1/400\n5383/5383 [==============================] - 0s - loss: 6763562.1664 - val_loss: 6545522.4015\nEpoch 2/400\n5383/5383 [==============================] - 0s - loss: 6763541.9822 - val_loss: 6545522.3481\nEpoch 3/400\n5383/5383 [==============================] - 0s - loss: 6763546.5973 - val_loss: 6545522.3080\nEpoch 4/400\n5383/5383 [==============================] - 0s - loss: 6763548.4360 - val_loss: 6545522.3080\nEpoch 5/400\n5383/5383 [==============================] - 0s - loss: 6763548.4405 - val_loss: 6545522.3080\nEpoch 6/400\n5383/5383 [==============================] - 0s - loss: 6763548.4593 - val_loss: 6545522.2813\nEpoch 7/400\n5383/5383 [==============================] - 0s - loss: 6763548.4467 - val_loss: 6545522.2813\nEpoch 8/400\n5383/5383 [==============================] - 0s - loss: 6763548.4722 - val_loss: 6545522.2813\nEpoch 9/400\n5383/5383 [==============================] - 0s - loss: 6763548.4360 - val_loss: 6545522.2813\nEpoch 10/400\n5383/5383 [==============================] - 0s - loss: 6763548.4119 - val_loss: 6545522.2546\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 126/400\n5383/5383 [==============================] - 0s - loss: 6762790.8154 - val_loss: 6544782.6194\nEpoch 127/400\n5383/5383 [==============================] - 0s - loss: 6762800.1371 - val_loss: 6544782.6194\nEpoch 128/400\n5383/5383 [==============================] - 0s - loss: 6762790.8087 - val_loss: 6544782.6194\nEpoch 129/400\n5383/5383 [==============================] - 0s - loss: 6762790.7923 - val_loss: 6544782.6194\nEpoch 130/400\n5383/5383 [==============================] - 0s - loss: 6762790.8203 - val_loss: 6544782.6194\nEpoch 131/400\n5383/5383 [==============================] - 0s - loss: 6762790.7871 - val_loss: 6544782.6194\nEpoch 132/400\n5383/5383 [==============================] - 0s - loss: 6762790.7642 - val_loss: 6544782.6194\nEpoch 133/400\n5383/5383 [==============================] - 0s - loss: 6762790.8106 - val_loss: 6544782.6194\nEpoch 134/400\n5383/5383 [==============================] - 0s - loss: 6762790.8873 - val_loss: 6544782.6194\nEpoch 135/400\n5383/5383 [==============================] - 0s - loss: 6762790.8144 - val_loss: 6544782.6194\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 345/400\n5383/5383 [==============================] - 0s - loss: 6762758.9956 - val_loss: 6544751.5058\nEpoch 346/400\n5383/5383 [==============================] - 0s - loss: 6762759.0084 - val_loss: 6544751.5058\nEpoch 347/400\n5383/5383 [==============================] - 0s - loss: 6762759.0507 - val_loss: 6544751.5058\nEpoch 348/400\n5383/5383 [==============================] - 0s - loss: 6762758.9770 - val_loss: 6544751.5058\nEpoch 349/400\n5383/5383 [==============================] - 0s - loss: 6762759.0153 - val_loss: 6544751.5058\nEpoch 350/400\n5383/5383 [==============================] - 0s - loss: 6762758.9812 - val_loss: 6544751.5058\nEpoch 351/400\n5383/5383 [==============================] - 0s - loss: 6762759.0110 - val_loss: 6544751.5058\nEpoch 352/400\n5383/5383 [==============================] - 0s - loss: 6762758.9616 - val_loss: 6544751.5058\nEpoch 353/400\n5383/5383 [==============================] - 0s - loss: 6762759.0275 - val_loss: 6544751.5058\nEpoch 354/400\n5383/5383 [==============================] - 0s - loss: 6762759.0027 - val_loss: 6544751.5058\nEpoch 355/400\n5383/5383 [==============================] - 0s - loss: 6762759.0140 - val_loss: 6544751.5058\nEpoch 356/400\n5383/5383 [==============================] - 0s - loss: 6762758.9615 - val_loss: 6544751.5058\nEpoch 357/400\n5383/5383 [==============================] - 0s - loss: 6762758.9666 - val_loss: 6544751.5058\nEpoch 358/400\n5383/5383 [==============================] - 0s - loss: 6762759.0127 - val_loss: 6544751.5058\nEpoch 359/400\n5383/5383 [==============================] - 0s - loss: 6762758.9551 - val_loss: 6544751.5058\nEpoch 360/400\n5383/5383 [==============================] - 0s - loss: 6762759.0318 - val_loss: 6544751.5058\nEpoch 361/400\n5383/5383 [==============================] - 0s - loss: 6762758.9853 - val_loss: 6544751.5058\nEpoch 362/400\n5383/5383 [==============================] - 0s - loss: 6762759.0294 - val_loss: 6544751.5058\nEpoch 363/400\n5383/5383 [==============================] - 0s - loss: 6762759.0149 - val_loss: 6544751.5058\nEpoch 364/400\n5383/5383 [==============================] - 0s - loss: 6762759.0251 - val_loss: 6544751.5058\nEpoch 365/400\n5383/5383 [==============================] - 0s - loss: 6762758.9747 - val_loss: 6544751.5058\nEpoch 366/400\n5383/5383 [==============================] - 0s - loss: 6762759.0271 - val_loss: 6544751.5058\nEpoch 367/400\n5383/5383 [==============================] - 0s - loss: 6762759.0308 - val_loss: 6544751.5058\nEpoch 368/400\n5383/5383 [==============================] - 0s - loss: 6762759.0166 - val_loss: 6544751.5058\nEpoch 369/400\n5383/5383 [==============================] - 0s - loss: 6762759.0049 - val_loss: 6544751.5058\nEpoch 370/400\n5383/5383 [==============================] - 0s - loss: 6762758.9755 - val_loss: 6544751.5058\nEpoch 371/400\n5383/5383 [==============================] - 0s - loss: 6762759.0246 - val_loss: 6544751.5058\nEpoch 372/400\n5383/5383 [==============================] - 0s - loss: 6762758.9926 - val_loss: 6544751.5058\nEpoch 373/400\n5383/5383 [==============================] - 0s - loss: 6762758.9992 - val_loss: 6544751.5058\nEpoch 374/400\n5383/5383 [==============================] - 0s - loss: 6762759.0204 - val_loss: 6544751.5058\nEpoch 375/400\n5383/5383 [==============================] - 0s - loss: 6762759.0173 - val_loss: 6544751.5058\nEpoch 376/400\n5383/5383 [==============================] - 0s - loss: 6762759.0221 - val_loss: 6544751.5058\nEpoch 377/400\n5383/5383 [==============================] - 0s - loss: 6762758.9886 - val_loss: 6544751.5058\nEpoch 378/400\n5383/5383 [==============================] - 0s - loss: 6762758.9834 - val_loss: 6544751.5058\nEpoch 379/400\n5383/5383 [==============================] - 0s - loss: 6762758.9874 - val_loss: 6544751.5058\nEpoch 380/400\n5383/5383 [==============================] - 0s - loss: 6762759.0348 - val_loss: 6544751.5058\nEpoch 381/400\n5383/5383 [==============================] - 0s - loss: 6762759.0156 - val_loss: 6544751.5058\nEpoch 382/400\n5383/5383 [==============================] - 0s - loss: 6762758.9957 - val_loss: 6544751.5058\nEpoch 383/400\n5383/5383 [==============================] - 0s - loss: 6762759.0182 - val_loss: 6544751.5058\nEpoch 384/400\n5383/5383 [==============================] - 0s - loss: 6762758.9866 - val_loss: 6544751.5058\nEpoch 385/400\n5383/5383 [==============================] - 0s - loss: 6762758.9694 - val_loss: 6544751.5058\nEpoch 386/400\n5383/5383 [==============================] - 0s - loss: 6762758.9553 - val_loss: 6544751.5058\nEpoch 387/400\n5383/5383 [==============================] - 0s - loss: 6762759.0147 - val_loss: 6544751.5058\nEpoch 388/400\n5383/5383 [==============================] - 0s - loss: 6762759.0150 - val_loss: 6544751.5058\nEpoch 389/400\n5383/5383 [==============================] - 0s - loss: 6762758.9939 - val_loss: 6544751.5058\nEpoch 390/400\n5383/5383 [==============================] - 0s - loss: 6762759.0224 - val_loss: 6544751.5058\nEpoch 391/400\n5383/5383 [==============================] - 0s - loss: 6762758.9539 - val_loss: 6544751.5058\nEpoch 392/400\n5383/5383 [==============================] - 0s - loss: 6762758.9824 - val_loss: 6544751.5058\nEpoch 393/400\n5383/5383 [==============================] - 0s - loss: 6762758.9837 - val_loss: 6544751.5058\nEpoch 394/400\n5383/5383 [==============================] - 0s - loss: 6762759.0260 - val_loss: 6544751.5058\nEpoch 395/400\n5383/5383 [==============================] - 0s - loss: 6762759.0066 - val_loss: 6544751.5058\nEpoch 396/400\n5383/5383 [==============================] - 0s - loss: 6762758.9615 - val_loss: 6544751.5058\nEpoch 397/400\n5383/5383 [==============================] - 0s - loss: 6762758.9870 - val_loss: 6544751.5058\nEpoch 398/400\n5383/5383 [==============================] - 0s - loss: 6762759.0247 - val_loss: 6544751.5058\nEpoch 399/400\n5383/5383 [==============================] - 0s - loss: 6762759.0080 - val_loss: 6544751.5058\nEpoch 400/400\n5383/5383 [==============================] - 1s - loss: 6762758.9872 - val_loss: 6544751.5058\n\n\nnorm_dist_cumsum_array \uff11\u30d1\u30bf\u30fc\u30f3 \u3092 \u5b66\u3070\u305b\u305f\u5834\u5408 \u306e \u518d\u73fe\u6210\u529f\u5ea6\u5408\u3044 \u3092 \u78ba\u8a8d\u3059\u308b\n\n\npython3 jupyter notebook\n# norm_dist_cumsum_array \uff11\u30d1\u30bf\u30fc\u30f3\u3060\u3051\u3092\u5b66\u3070\u305b\u3066\u3001\n# \u51fa\u529b\u5024\u304c\u3001\u5165\u529b\u5024\u3092\u3069\u308c\u3060\u3051\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u3002\nprint(norm_dist_cumsum_array.shape)\n\n\n(1994, 63)\n\n\npython3 jupyter notebook\ntmp_df = pd.DataFrame(norm_dist_cumsum_array)\ntmp_df.plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12f7682b0>\n\n\n\npython3 jupyter notebook\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\n\n\npython3 jupyter notebook\nnorm_dist_cumsum_pattern_ae_model_hist = autoencoder.fit(norm_dist_cumsum_array, norm_dist_cumsum_array,\n                                                         nb_epoch=200,\n                                                         batch_size=20,\n                                                         shuffle=True,\n                                                         validation_split=0.1)\n\n\nTrain on 1794 samples, validate on 200 samples\nEpoch 1/200\n1794/1794 [==============================] - 1s - loss: 21329475.6901 - val_loss: 10176569.8219\nEpoch 2/200\n1794/1794 [==============================] - 0s - loss: 21329452.4192 - val_loss: 10176569.8219\nEpoch 3/200\n1794/1794 [==============================] - 0s - loss: 21329452.7269 - val_loss: 10176569.8219\nEpoch 4/200\n1794/1794 [==============================] - 0s - loss: 21329452.5708 - val_loss: 10176569.8219\nEpoch 5/200\n1794/1794 [==============================] - 0s - loss: 21329452.7804 - val_loss: 10176569.8219\nEpoch 6/200\n1794/1794 [==============================] - 0s - loss: 21329452.5139 - val_loss: 10176569.8219\nEpoch 7/200\n1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\nEpoch 8/200\n1794/1794 [==============================] - 0s - loss: 21329452.4682 - val_loss: 10176569.8219\nEpoch 9/200\n1794/1794 [==============================] - 0s - loss: 21329452.5463 - val_loss: 10176569.8219\nEpoch 10/200\n1794/1794 [==============================] - 0s - loss: 21329452.5641 - val_loss: 10176569.8219\nEpoch 11/200\n1794/1794 [==============================] - 0s - loss: 21329452.7135 - val_loss: 10176569.8219\nEpoch 12/200\n1794/1794 [==============================] - 0s - loss: 21329452.3356 - val_loss: 10176569.8219\nEpoch 13/200\n1794/1794 [==============================] - 0s - loss: 21329452.6711 - val_loss: 10176569.8219\nEpoch 14/200\n1794/1794 [==============================] - 0s - loss: 21329452.9097 - val_loss: 10176569.8219\nEpoch 15/200\n1794/1794 [==============================] - 0s - loss: 21329452.7625 - val_loss: 10176569.8219\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 63/200\n1794/1794 [==============================] - 0s - loss: 21329452.5686 - val_loss: 10176569.8219\nEpoch 64/200\n1794/1794 [==============================] - 0s - loss: 21329452.4682 - val_loss: 10176569.8219\nEpoch 65/200\n1794/1794 [==============================] - 0s - loss: 21329452.6533 - val_loss: 10176569.8219\nEpoch 66/200\n1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\nEpoch 67/200\n1794/1794 [==============================] - 0s - loss: 21329452.5195 - val_loss: 10176569.8219\nEpoch 68/200\n1794/1794 [==============================] - 0s - loss: 21329452.9119 - val_loss: 10176569.8219\nEpoch 69/200\n1794/1794 [==============================] - 0s - loss: 21329452.5775 - val_loss: 10176569.8219\nEpoch 70/200\n1794/1794 [==============================] - 0s - loss: 21329452.5329 - val_loss: 10176569.8219\nEpoch 71/200\n1794/1794 [==============================] - 0s - loss: 21329452.7525 - val_loss: 10176569.8219\nEpoch 72/200\n1794/1794 [==============================] - 0s - loss: 21329452.7625 - val_loss: 10176569.8219\n\n\uff08 \u4e2d\u7565 \uff09\nEpoch 183/200\n1794/1794 [==============================] - 0s - loss: 21329452.7781 - val_loss: 10176569.8219\nEpoch 184/200\n1794/1794 [==============================] - 0s - loss: 21329452.5095 - val_loss: 10176569.8219\nEpoch 185/200\n1794/1794 [==============================] - 0s - loss: 21329452.6823 - val_loss: 10176569.8219\nEpoch 186/200\n1794/1794 [==============================] - 0s - loss: 21329452.6109 - val_loss: 10176569.8219\nEpoch 187/200\n1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\nEpoch 188/200\n1794/1794 [==============================] - 0s - loss: 21329452.4526 - val_loss: 10176569.8219\nEpoch 189/200\n1794/1794 [==============================] - 0s - loss: 21329452.7536 - val_loss: 10176569.8219\nEpoch 190/200\n1794/1794 [==============================] - 0s - loss: 21329452.6288 - val_loss: 10176569.8219\nEpoch 191/200\n1794/1794 [==============================] - 0s - loss: 21329452.7269 - val_loss: 10176569.8219\nEpoch 192/200\n1794/1794 [==============================] - 0s - loss: 21329452.6488 - val_loss: 10176569.8219\nEpoch 193/200\n1794/1794 [==============================] - 0s - loss: 21329452.6912 - val_loss: 10176569.8219\nEpoch 194/200\n1794/1794 [==============================] - 0s - loss: 21329452.4437 - val_loss: 10176569.8219\nEpoch 195/200\n1794/1794 [==============================] - 0s - loss: 21329452.8829 - val_loss: 10176569.8219\nEpoch 196/200\n1794/1794 [==============================] - 0s - loss: 21329452.4225 - val_loss: 10176569.8219\nEpoch 197/200\n1794/1794 [==============================] - 0s - loss: 21329452.7915 - val_loss: 10176569.8219\nEpoch 198/200\n1794/1794 [==============================] - 0s - loss: 21329452.6488 - val_loss: 10176569.8219\nEpoch 199/200\n1794/1794 [==============================] - 0s - loss: 21329452.6722 - val_loss: 10176569.8219\nEpoch 200/200\n1794/1794 [==============================] - 0s - loss: 21329452.8305 - val_loss: 10176569.8219\n\n\npython3 jupyter notebook\nloss = norm_dist_cumsum_pattern_ae_model_hist.history['loss']\nval_loss = norm_dist_cumsum_pattern_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\u30c0\u30e1\n\n\n\u4e71\u6570 \u3092 \u7d2f\u7a4d\u548c\u84c4\u7a4d \u3057\u3066\u3044\u304f randon.cumsum()\u7cfb\u5217 \u306f\u3001\n\u30d1\u30bf\u30fc\u30f3\u305d\u306e\u3082\u306e \u304c \u8907\u96d1\uff08\u5468\u671f\u6027\u304c\u306a\u3044\uff1f\uff09\u305f\u3081\u3001\n\u518d\u73fe \u306f \u56f0\u96e3 \u3068 \u8003\u3048\u3089\u308c\u308b\u3002\n\nsin\u66f2\u7dda \u306b \u4e71\u6570\u3092\u52a0\u7b97 \u3057\u3066 \u63fa\u52d5\u3055\u305b\u305f ys_2 \u3068 ys_3 \u3092 \u8a66\u3057\u3066\u307f\u308b\u3002\n\n\npython3 jupyter notebook\n# \u3084\u306f\u308a\u3001\u4e71\u6570\u751f\u6210\u5668\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u4e71\u6570\u3092\u7d2f\u7a4d\u548c\u84c4\u7a4d\u3057\u3066\u3044\u304frandon.cumsum()\u7cfb\u5217\u306f\u3001\n# \u30d1\u30bf\u30fc\u30f3\u305d\u306e\u3082\u306e\u304c\u8907\u96d1\uff08\u5468\u671f\u6027\u304c\u306a\u3044\uff1f\uff09\u3067\u3001\u518d\u73fe\u306f\u56f0\u96e3\u3068\u8003\u3048\u3089\u308c\u308b\u3002\n\n# sin\u66f2\u7dda\u306b\u4e71\u6570\u3092\u52a0\u7b97\u3057\u3066\u63fa\u52d5\u3055\u305b\u305f ys_2 \u3068 ys_3 \u3092 \u8a66\u3057\u3066\u307f\u308b\u3002\n\ntype(ys_2)\n\n\nnumpy.ndarray\n\n\npython3 jupyter notebook\nprint(ys_2.shape, ys_3.shape)\n\n\n(125622,) (125622,)\n\n\npython3 jupyter notebook\nys_2 = ys_2.reshape(1994, 63)\nys_3 = ys_3.reshape(1994, 63)\n\n\n\npython3 jupyter notebook\nprint(ys_2.shape, ys_3.shape)\n\n\n(1994, 63) (1994, 63)\n\n\n2\u30d1\u30bf\u30fc\u30f3 \u307e\u3068\u3081\u3066\u5b66\u7fd2\u3055\u305b\u308b\n\n\npython3 jupyter notebook\n# 2\u30d1\u30bf\u30fc\u30f3\u3001\u307e\u3068\u3081\u3066\u5b66\u7fd2\u3055\u305b\u308b\nsin_with_random_2_patterns = np.vstack((ys_2, ys_3))\nsin_with_random_2_patterns = np.random.permutation(sin_with_random_2_patterns)\n\nprint(sin_with_random_2_patterns.shape)\n\n\n(3988, 63)\n\n\npython3 jupyter notebook\nplt.plot(sin_with_random_2_patterns[0])\nplt.show()\n\n\n\n\npython3 jupyter notebook\nplt.plot(sin_with_random_2_patterns[1])\nplt.show()\n\n\n\n\npython3 jupyter notebook\nsin_with_random_2_patterns_ae_model_hist = autoencoder.fit(sin_with_random_2_patterns, sin_with_random_2_patterns,\n                                                           nb_epoch=100,\n                                                           batch_size=20,\n                                                           shuffle=True,\n                                                           validation_split=0.1)\n\n\nTrain on 3589 samples, validate on 399 samples\nEpoch 1/100\n3589/3589 [==============================] - 0s - loss: 2.5481 - val_loss: 2.5165\nEpoch 2/100\n3589/3589 [==============================] - 0s - loss: 2.5175 - val_loss: 2.5161\nEpoch 3/100\n3589/3589 [==============================] - 0s - loss: 2.5151 - val_loss: 2.5164\nEpoch 4/100\n3589/3589 [==============================] - 0s - loss: 2.5095 - val_loss: 2.5018\nEpoch 5/100\n3589/3589 [==============================] - 0s - loss: 2.4943 - val_loss: 2.4905\nEpoch 6/100\n3589/3589 [==============================] - 0s - loss: 2.4814 - val_loss: 2.4517\nEpoch 7/100\n3589/3589 [==============================] - 0s - loss: 2.4565 - val_loss: 2.4501\nEpoch 8/100\n3589/3589 [==============================] - 0s - loss: 2.4539 - val_loss: 2.4522\nEpoch 9/100\n3589/3589 [==============================] - 0s - loss: 2.4532 - val_loss: 2.4526\nEpoch 10/100\n3589/3589 [==============================] - 0s - loss: 2.4485 - val_loss: 2.4349\nEpoch 11/100\n3589/3589 [==============================] - 0s - loss: 2.4187 - val_loss: 2.3953\nEpoch 12/100\n3589/3589 [==============================] - 0s - loss: 2.3865 - val_loss: 2.3856\nEpoch 13/100\n3589/3589 [==============================] - 0s - loss: 2.3800 - val_loss: 2.3751\nEpoch 14/100\n3589/3589 [==============================] - 0s - loss: 2.3802 - val_loss: 2.3817\nEpoch 15/100\n3589/3589 [==============================] - 0s - loss: 2.3595 - val_loss: 2.3608\nEpoch 16/100\n3589/3589 [==============================] - 0s - loss: 2.3580 - val_loss: 2.3501\nEpoch 17/100\n3589/3589 [==============================] - 0s - loss: 2.3382 - val_loss: 2.3307\nEpoch 18/100\n3589/3589 [==============================] - 0s - loss: 2.3124 - val_loss: 2.2932\nEpoch 19/100\n3589/3589 [==============================] - 0s - loss: 2.2824 - val_loss: 2.2791\nEpoch 20/100\n3589/3589 [==============================] - 0s - loss: 2.2771 - val_loss: 2.2796\nEpoch 21/100\n3589/3589 [==============================] - 0s - loss: 2.2774 - val_loss: 2.2696\nEpoch 22/100\n3589/3589 [==============================] - 0s - loss: 2.2633 - val_loss: 2.2671\nEpoch 23/100\n3589/3589 [==============================] - 0s - loss: 2.2619 - val_loss: 2.2631\nEpoch 24/100\n3589/3589 [==============================] - 0s - loss: 2.2496 - val_loss: 2.2400\nEpoch 25/100\n3589/3589 [==============================] - 0s - loss: 2.2389 - val_loss: 2.2419\nEpoch 26/100\n3589/3589 [==============================] - 0s - loss: 2.2317 - val_loss: 2.2344\nEpoch 27/100\n3589/3589 [==============================] - 0s - loss: 2.2244 - val_loss: 2.2221\nEpoch 28/100\n3589/3589 [==============================] - 0s - loss: 2.2246 - val_loss: 2.2215\nEpoch 29/100\n3589/3589 [==============================] - 0s - loss: 2.2224 - val_loss: 2.2188\nEpoch 30/100\n3589/3589 [==============================] - 0s - loss: 2.2205 - val_loss: 2.2189\nEpoch 31/100\n3589/3589 [==============================] - 0s - loss: 2.2187 - val_loss: 2.2243\nEpoch 32/100\n3589/3589 [==============================] - 0s - loss: 2.2181 - val_loss: 2.2170\nEpoch 33/100\n3589/3589 [==============================] - 0s - loss: 2.2157 - val_loss: 2.2154\nEpoch 34/100\n3589/3589 [==============================] - 0s - loss: 2.2112 - val_loss: 2.2152\nEpoch 35/100\n3589/3589 [==============================] - 0s - loss: 2.2083 - val_loss: 2.2103\nEpoch 36/100\n3589/3589 [==============================] - 0s - loss: 2.2078 - val_loss: 2.2164\nEpoch 37/100\n3589/3589 [==============================] - 0s - loss: 2.1939 - val_loss: 2.1926\nEpoch 38/100\n3589/3589 [==============================] - 0s - loss: 2.1859 - val_loss: 2.1970\nEpoch 39/100\n3589/3589 [==============================] - 0s - loss: 2.1686 - val_loss: 2.1764\nEpoch 40/100\n3589/3589 [==============================] - 0s - loss: 2.1648 - val_loss: 2.1701\nEpoch 41/100\n3589/3589 [==============================] - 0s - loss: 2.1626 - val_loss: 2.1783\nEpoch 42/100\n3589/3589 [==============================] - 0s - loss: 2.1650 - val_loss: 2.1689\nEpoch 43/100\n3589/3589 [==============================] - 0s - loss: 2.1595 - val_loss: 2.1591\nEpoch 44/100\n3589/3589 [==============================] - 0s - loss: 2.1561 - val_loss: 2.1557\nEpoch 45/100\n3589/3589 [==============================] - 0s - loss: 2.1545 - val_loss: 2.1630\nEpoch 46/100\n3589/3589 [==============================] - 0s - loss: 2.1547 - val_loss: 2.1609\nEpoch 47/100\n3589/3589 [==============================] - 0s - loss: 2.1507 - val_loss: 2.1448\nEpoch 48/100\n3589/3589 [==============================] - 0s - loss: 2.1462 - val_loss: 2.1548\nEpoch 49/100\n3589/3589 [==============================] - 0s - loss: 2.1486 - val_loss: 2.1653\nEpoch 50/100\n3589/3589 [==============================] - 0s - loss: 2.1482 - val_loss: 2.1490\nEpoch 51/100\n3589/3589 [==============================] - 0s - loss: 2.1448 - val_loss: 2.1523\nEpoch 52/100\n3589/3589 [==============================] - 0s - loss: 2.1352 - val_loss: 2.1405\nEpoch 53/100\n3589/3589 [==============================] - 0s - loss: 2.1234 - val_loss: 2.1424\nEpoch 54/100\n3589/3589 [==============================] - 0s - loss: 2.1211 - val_loss: 2.1334\nEpoch 55/100\n3589/3589 [==============================] - 0s - loss: 2.1219 - val_loss: 2.1287\nEpoch 56/100\n3589/3589 [==============================] - 0s - loss: 2.1172 - val_loss: 2.1276\nEpoch 57/100\n3589/3589 [==============================] - 0s - loss: 2.1177 - val_loss: 2.1273\nEpoch 58/100\n3589/3589 [==============================] - 0s - loss: 2.1154 - val_loss: 2.1308\nEpoch 59/100\n3589/3589 [==============================] - 0s - loss: 2.1147 - val_loss: 2.1303\nEpoch 60/100\n3589/3589 [==============================] - 0s - loss: 2.1118 - val_loss: 2.1310\nEpoch 61/100\n3589/3589 [==============================] - 0s - loss: 2.1080 - val_loss: 2.1213\nEpoch 62/100\n3589/3589 [==============================] - 0s - loss: 2.1099 - val_loss: 2.1149\nEpoch 63/100\n3589/3589 [==============================] - 0s - loss: 2.1087 - val_loss: 2.1073\nEpoch 64/100\n3589/3589 [==============================] - 0s - loss: 2.1045 - val_loss: 2.1092\nEpoch 65/100\n3589/3589 [==============================] - 0s - loss: 2.0916 - val_loss: 2.1049\nEpoch 66/100\n3589/3589 [==============================] - 0s - loss: 2.0863 - val_loss: 2.0918\nEpoch 67/100\n3589/3589 [==============================] - 0s - loss: 2.0820 - val_loss: 2.0898\nEpoch 68/100\n3589/3589 [==============================] - 0s - loss: 2.0832 - val_loss: 2.1033\nEpoch 69/100\n3589/3589 [==============================] - 0s - loss: 2.0837 - val_loss: 2.0878\nEpoch 70/100\n3589/3589 [==============================] - 0s - loss: 2.0779 - val_loss: 2.0896\nEpoch 71/100\n3589/3589 [==============================] - 0s - loss: 2.0773 - val_loss: 2.0833\nEpoch 72/100\n3589/3589 [==============================] - 0s - loss: 2.0762 - val_loss: 2.0847\nEpoch 73/100\n3589/3589 [==============================] - 0s - loss: 2.0750 - val_loss: 2.0855\nEpoch 74/100\n3589/3589 [==============================] - 0s - loss: 2.0741 - val_loss: 2.0878\nEpoch 75/100\n3589/3589 [==============================] - 0s - loss: 2.0733 - val_loss: 2.0861\nEpoch 76/100\n3589/3589 [==============================] - 0s - loss: 2.0593 - val_loss: 2.0759\nEpoch 77/100\n3589/3589 [==============================] - 0s - loss: 2.0559 - val_loss: 2.0714\nEpoch 78/100\n3589/3589 [==============================] - 0s - loss: 2.0566 - val_loss: 2.0640\nEpoch 79/100\n3589/3589 [==============================] - 0s - loss: 2.0493 - val_loss: 2.0599\nEpoch 80/100\n3589/3589 [==============================] - 0s - loss: 2.0493 - val_loss: 2.0617\nEpoch 81/100\n3589/3589 [==============================] - 0s - loss: 2.0455 - val_loss: 2.0571\nEpoch 82/100\n3589/3589 [==============================] - 0s - loss: 2.0442 - val_loss: 2.0643\nEpoch 83/100\n3589/3589 [==============================] - 0s - loss: 2.0460 - val_loss: 2.0616\nEpoch 84/100\n3589/3589 [==============================] - 0s - loss: 2.0443 - val_loss: 2.0526\nEpoch 85/100\n3589/3589 [==============================] - 0s - loss: 2.0402 - val_loss: 2.0643\nEpoch 86/100\n3589/3589 [==============================] - 0s - loss: 2.0428 - val_loss: 2.0521\nEpoch 87/100\n3589/3589 [==============================] - 0s - loss: 2.0391 - val_loss: 2.0514\nEpoch 88/100\n3589/3589 [==============================] - 0s - loss: 2.0355 - val_loss: 2.0407\nEpoch 89/100\n3589/3589 [==============================] - 0s - loss: 2.0348 - val_loss: 2.0430\nEpoch 90/100\n3589/3589 [==============================] - 0s - loss: 2.0340 - val_loss: 2.0431\nEpoch 91/100\n3589/3589 [==============================] - 0s - loss: 2.0332 - val_loss: 2.0532\nEpoch 92/100\n3589/3589 [==============================] - 0s - loss: 2.0308 - val_loss: 2.0444\nEpoch 93/100\n3589/3589 [==============================] - 0s - loss: 2.0281 - val_loss: 2.0433\nEpoch 94/100\n3589/3589 [==============================] - 0s - loss: 2.0299 - val_loss: 2.0478\nEpoch 95/100\n3589/3589 [==============================] - 0s - loss: 2.0314 - val_loss: 2.0382\nEpoch 96/100\n3589/3589 [==============================] - 0s - loss: 2.0297 - val_loss: 2.0387\nEpoch 97/100\n3589/3589 [==============================] - 0s - loss: 2.0077 - val_loss: 2.0191\nEpoch 98/100\n3589/3589 [==============================] - 0s - loss: 2.0088 - val_loss: 2.0373\nEpoch 99/100\n3589/3589 [==============================] - 0s - loss: 2.0068 - val_loss: 2.0170\nEpoch 100/100\n3589/3589 [==============================] - 0s - loss: 2.0052 - val_loss: 2.0212\n\n\nloss \u306f 1 epoch\u76ee \u304b\u3089\u30012 \u306b \u6e1b\u3063\u305f\u3002\n\n\n\u3057\u304b\u3057\u3001\u4e71\u6570\u304c\u52a0\u7b97\u3055\u308c\u3066\u3044\u306a\u3044\u3001\u300csin\u66f2\u7dda\u3060\u3051\u300d \u306e \u6642 \u306b \u6bd4\u3079\u308b \u3068 loss \u306f \u306a\u304a \u5927\u304d\u3044\n\n\npython3 jupyter notebook\n# loss\u306f\u6700\u521d\u304b\u3089\u30012\u306b\u307e\u3067\u6e1b\u3063\u305f\u3002\n# \u3057\u304b\u3057\u3001\u4e71\u6570\u304c\u52a0\u7b97\u3055\u308c\u3066\u3044\u306a\u3044\u3001\u300csin\u66f2\u7dda\u3060\u3051\u300d \u306e \u6642 \u306b \u6bd4\u3079\u308b \u3068 loss \u306f \u306a\u304a\u5927\u304d\u3044\n\nloss = sin_with_random_2_patterns_ae_model_hist.history['loss']\nval_loss = sin_with_random_2_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\n\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570 \u306f \u4f4e\u3044\uff08\u518d\u73fe\u3067\u304d\u3066\u3044\u306a\u3044\uff09\n\n\npython3 jupyter notebook\npredicted_value_list = autoencoder.predict(sin_with_random_2_patterns[-10:])\n\n\n\npython3 jupyter notebook\nlen(predicted_value_list)\n\n\n10\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-10], predicted_value_list[0])[0][1]\n#np.corrcoef(three_patterns_data[-10], predicted_value_list[0])[0][1]\n\n\n0.32999374633450923\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-9], predicted_value_list[1])[0][1]\n\n\n-0.14401764907318854\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-8], predicted_value_list[2])[0][1]\n\n\n0.37402418180795188\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-7], predicted_value_list[3])[0][1]\n\n\n-0.056611938444256202\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-6], predicted_value_list[4])[0][1]\n\n\n-0.10400593295651984\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-5], predicted_value_list[5])[0][1]\n\n\n0.45525679191488855\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-4], predicted_value_list[6])[0][1]\n\n\n0.35503679442701519\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-3], predicted_value_list[7])[0][1]\n\n\n0.085707911982792429\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-2], predicted_value_list[8])[0][1]\n\n\n0.38548401081843031\n\n\npython3 jupyter notebook\nnp.corrcoef(sin_with_random_2_patterns[-1], predicted_value_list[9])[0][1]\n\n\n-0.0098453563954306513\n\n\npython3 jupyter notebook\n# \u51fa\u529b\u5024\u306f\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u306a\u3044\n\n\n###__\uff08 \u4f5c\u696d\u74b0\u5883 \uff09__\n\n* Jupyter notebook (Python 3)\n\n___\n\n\n\n```{python:python3 jupyter notebook}\n%matplotlib inline\n```\n\n\n##__\u30c7\u30fc\u30bf\u4f5c\u6210__\n\n####__\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2460 \u3011__\n\n* __\u30ce\u30a4\u30ba\u306a\u3057 \u6b63\u5f26\u66f2\u7dda__\n* \u632f\u5e45: 1\n* \u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d $2\u03c0$ \uff09\n\n```{python:python3 jupyter notebook}\nimport numpy as np\n\nxs = np.arange(-2000*np.pi, 2000*np.pi, 0.1)\nys = np.sin(xs)\n# len(xs) # 125664\n```\n\n\n```{python:python3 jupyter notebook}\n# sin\u66f2\u7dda\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n# \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u6570\u309263\u3068\u3059\u308b\nlen(xs) / 63\n# 63*1994 # 125622 \n# 125623\u4ee5\u964d\u306e\u30c7\u30fc\u30bf\u3092\u5207\u308a\u6368\u3066\u308b\nys = ys[0:125622]\nlen(ys)\n```\n\n    125622\n\n####__\uff11\u5468\u671f\u5206\uff08 2\u03c0 \uff09\uff1d 63\u6642\u70b9 \u3067 \u30c7\u30fc\u30bf\u3092\u751f\u6210__\n\n```{python:python3 jupyter notebook}\nfrom matplotlib import pyplot as plt \n\nplt.plot(xs[0:63], ys[0:63])\nplt.ylim(-2, 2)\nplt.show()\n```\n\n__\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n![output\uff11__3_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/fbb5355c-b516-0e32-e6c8-b4b396312831.png)\n\n\n####__\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2461 \u3011__\n\n* __\u30ce\u30a4\u30ba\u3042\u308a \u6b63\u5f26\u66f2\u7dda__\n* \u632f\u5e45: 1.5 \n* \u30ce\u30a4\u30ba\uff1a \u6a19\u6e96\u6b63\u898f\u5206\u5e03 \uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee:1\uff09\n* \u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d $2\u03c0$ \uff09\n\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n\n\uff08 \u53c2\u8003 \uff09\n\n* [\u300c\u6ce2\u3068\u3057\u3066\u306e\u4e09\u89d2\u95a2\u6570\u300d](http://kouyama.sci.u-toyama.ac.jp/main/education/2006/infomath/pdf/text/text02.pdf)\n* [\u4e09\u89d2\u95a2\u6570 \u300c\u89d2\u5ea6\u3068\u30e9\u30b8\u30a2\u30f3\u300d](http://www8.plala.or.jp/ap2/suugaku/sankakukansuu.html)\n\n```python\nfrom numpy.random import *\n\nys_2 = 1.5*np.sin(xs*0.3) + randn(len(xs))\n```\n\n\n```{python:python3 jupyter notebook}\nys_2 = ys_2[0:125622]\nlen(ys_2)\n```\n\n\n    125622\n\n\n__\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n```{python:python3 jupyter notebook}\nplt.plot(xs[0:63], ys_2[0:63])\nplt.ylim(-5, 5)\nplt.show()\n```\n\n\n![output_6_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/7aa3cef3-5c37-d86e-ff4e-22bfe4932f75.png)\n\n####__\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2462 \u3011__\n\n* __\u30ce\u30a4\u30ba\u3042\u308a \u6b63\u5f26\u66f2\u7dda__\n* \u632f\u5e45: 1.5 \n* \u89d2\u901f\u5ea6: 3.5\n* \u30ce\u30a4\u30ba\uff1a \u6a19\u6e96\u6b63\u898f\u5206\u5e03 \uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee:1\uff09\n* \u5468\u671f\u306e\u6570\uff1a 2,000\uff08 \uff11\u5468\u671f \uff1d $2\u03c0$ \uff09\n\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8\n\n\n```{python:python3 jupyter notebook}\nfrom numpy.random import *\n\nys_3 = 1.5*np.sin(xs*3.5) + randn(len(xs))\nys_3 = ys_3[0:125622]\n```\n\n__\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n```{python:python3 jupyter notebook}\nplt.plot(xs[0:63], ys_3[0:63])\nplt.ylim(-5, 5)\nplt.show()\n```\n\n\n![output_7_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/0bdbd32c-5f5d-e9bb-40d6-4af49683ae86.png)\n\n\n__\uff08 \u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n```{python:python3 jupyter notebook}\nplt.plot(ys_3)\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120f315f8>]\n\n\n\n\n![output_8_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/251a00ed-5d90-ab55-d5f8-677cc0cd95ba.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nimport pandas as pd\nys_3_df = pd.Series(ys_3)\nys_3_df.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12054ef60>\n\n\n\n\n![output_9_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/f9ac922a-d518-a237-53e4-bdc5f9dad43f.png)\n\n####__\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2463 \u3011__\n\n* \u6a19\u6e96\u6b63\u898f\u4e71\u6570 \u306e \u7d2f\u7a4d\u548c\u7cfb\u5217\n* \u6642\u70b9\u6570: 125,622 \u6642\u70b9 \uff08=63*1994\uff09\n* \u671f\u9593\u6570: 1,994 \u671f\u9593\n\n\nsin\u66f2\u7dda \u3068 \u5408\u308f\u305b\u3066\u7528\u3044\u308b\u305f\u3081\u306b\u3001\nsin\u66f2\u7dda\u306e\uff11\u5468\u671f\uff08\uff1d0.1\u523b\u307f\u3067\u300163\u6642\u70b9\uff09\u306b\u5408\u308f\u305b\u3066\u300163\u6642\u70b9\u3092\uff11\u30c7\u30fc\u30bf\u533a\u9593\u3068\u3059\u308b\u3002\n\nsin\u66f2\u7dda\u3000\u306f\u3001\uff11\u5468\u671f \u304c 2*np.sin = 6.28... \u3053\u308c\u3092 0.1 \u523b\u307f\u3057\u305f\u304b\u3089\u30011\u5468\u671f\u306e\u30c7\u30fc\u30bf\u6570\u306f\u300162.8 \u3067\u3001\u56db\u6368\u4e94\u5165\u3057\u3066\u300163\u6642\u70b9\u5206\u3002\n\n___\n\n__\uff08 \u53c2\u8003 \uff09__\n\n__\u6a19\u6e96\u6b63\u898f\u4e71\u6570\u3000\u306e\u3000\u7d2f\u7a4d\u548c\u7cfb\u5217__\n\n* [BeingWizard.com (2016\u5e7411\u67089\u65e5) \u300cPython 2\u6b21\u5143\u30d6\u30e9\u30a6\u30f3\u904b\u52d5 \u30b7\u30e5\u30df\u30ec\u30fc\u30b7\u30e7\u30f3\u300d](http://www.beingwizard.com/wordpress/2016/11/09/pythonbrownianmotion/)\n* [Julia vs Python: \u30d3\u30c3\u30c8\u30b3\u30a4\u30f3\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3](https://chezou.wordpress.com/2014/05/07/julia-vs-python-bitcoin-option/)\n* [My Life as a Mock Quant \uff082012/08/21\uff09 \u300c\u5e7e\u4f55\u30d6\u30e9\u30a6\u30f3\u904b\u52d5\u306b\u5f93\u3046\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u306e\u30d1\u30b9\u3092\u4f5c\u308b\u300d](http://d.hatena.ne.jp/teramonagi/20120821/1345551158)\n* [\uff08SlideShare\uff09 \u5bff\u4e4b \u4e0b\u91ce\u300c\u30d6\u30e9\u30a6\u30f3\u904b\u52d5\u3068\u305d\u306e\u6642\u9593\u7a4d\u5206\u306e\u304f\u308a\u8fd4\u3057\u53ca\u3073\u5f62\u72b6\u306e\u81ea\u7136\u30fb\u751f\u7269\u73fe\u8c61\u3078\u306e\u7c21\u5358\u306a\u8003\u5bdf\u300d](https://www.slideshare.net/shimonotoshiyuki/ss-38792901)\n\n___\n\n\n```{python:python3 jupyter notebook}\nrandn_cumsum_series = np.random.randn(len(xs)).cumsum()\n```\n\n\n```{python:python3 jupyter notebook}\nrandn_cumsum_series = randn_cumsum_series[0:125622]\nlen(randn_cumsum_series)\n```\n\n    125622\n\n\n####__\u5024 \u306e \u56db\u5206\u4f4d\u6570 \u307b\u304b \u3092 \u78ba\u8a8d__\n\n```{python:python3 jupyter notebook}\nimport pandas as pd\ntmp_df = pd.Series(randn_cumsum_series)\ntmp_df.describe()\n```\n\n    count    125622.000000\n    mean       -108.356590\n    std          59.930377\n    min        -267.971590\n    25%        -149.494232\n    50%        -107.989932\n    75%         -70.402787\n    max          53.531884\n    dtype: float64\n\n\n__\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n\n```{python:python3 jupyter notebook}\nplt.plot(xs[0:63], randn_cumsum_series[0:63])\nplt.show()\n```\n\n\n![output_14_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/745b6c37-0972-e258-119b-38db9f698cb7.png)\n\n\n####__\u3010 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u2464 \u3011__\n\n* \u6b63\u898f\u4e71\u6570\uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee: 20\uff09\u306e \u7d2f\u7a4d\u548c\u7cfb\u5217\n* \u6642\u70b9\u6570: 125,622 \u6642\u70b9 \uff08=63*1994\uff09\n* \u671f\u9593\u6570: 1,994 \u671f\u9593\n\n\uff08 \u53c2\u8003 \uff09\n\n* [Wikipedia\u300c\u4e71\u6570\u5217\u300d](https://ja.wikipedia.org/wiki/\u4e71\u6570\u5217)\n\n\n```{python:python3 jupyter notebook}\nnormal_distribution_cumsum_series = np.array([normal(0, 20) for i in range(len(xs))]).cumsum()\nnormal_distribution_cumsum_series = normal_distribution_cumsum_series[0:125622]\n```\n\n####__\u5024 \u306e \u56db\u5206\u4f4d\u6570 \u307b\u304b \u3092 \u78ba\u8a8d__\n\n```{python:python3 jupyter notebook}\ntmp_df = pd.Series(normal_distribution_cumsum_series)\ntmp_df.describe()\n```\n\n    count    125622.000000\n    mean      -1956.171591\n    std        4047.473179\n    min      -10240.019296\n    25%       -5467.874522\n    50%       -2246.121624\n    75%         901.980908\n    max        6004.684593\n    dtype: float64\n\n\n__\uff08 \u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n```{python:python3 jupyter notebook}\ntmp_df.plot()\n```\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12aa879b0>\n\n\n![output_17_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/308344d2-14df-20e0-4723-798e97bcbd00.png)\n\n\n__\uff08 \uff11\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u306e\u5f62 \uff09__\n\n```{python:python3 jupyter notebook}\nplt.plot(normal_distribution_cumsum_series[0:63])\nplt.show()\n```\n\n\n![output_18_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/57e79ae5-db27-fced-c9ed-9b92649bbcad.png)\n\n\n___\n\n\uff08 \u53c2\u8003 \uff09\n\n* \u6b63\u898f\u4e71\u6570\uff08\u5e73\u5747:0, \u6a19\u6e96\u504f\u5dee: 20\uff09\n\n\u203b \u4e71\u6570\u306e\u7a2e\uff08*seed*\uff09 \u3092 \u56fa\u5b9a\u3057\u306a\u3044\u3068\u3001\u5b9f\u884c\u3059\u308b \u305f\u3073 \u306b \u7570\u306a\u308b\u6570\u5024 \u304c \u751f\u6210\u3055\u308c\u308b\u3002\n\n\n```{python:python3 jupyter notebook}\nprint([normal(0, 20) for i in range(20)])\n```\n\n    [-0.1044916996982958, 19.21901497729581, -10.583793299858185, -34.27589412966404, -8.538314451442394, -12.603024803995908, 32.56951045105264, -16.20113597295369, 37.7772779255066, 16.01546722711461, 18.443395273470987, 14.708712759168765, -23.741788551069313, -5.443674522698121, -4.146401782328453, -12.493122576250633, -26.939969194056097, 5.41340756356258, -1.5770183027819948, -25.248265241894465]\n\n___\n\n###__\u3053\u3053\u307e\u3067\u3067 \u4f5c\u6210 \u3057\u305f \u5168\u30c7\u30fc\u30bf \u306e \u8981\u7d20\u6570 \u304c \u540c\u3058\u3053\u3068 \u3092 \u78ba\u8a8d__\n\n\n```{python:python3 jupyter notebook}\nprint(len(ys),\n      len(ys_2),\n      len(ys_3),\n      len(randn_cumsum_series),\n      len(normal_distribution_cumsum_series))\n```\n\n    125622 125622 125622 125622 125622\n\n###__\u30c7\u30fc\u30bf\u306e\u6b21\u5143\uff08*shape*\uff09 \u3092 \u5909\u63db__\n\n__Keras AutoEncoder\u30e2\u30c7\u30eb \u306b \u6e21\u305b\u308b\u3088\u3046 \u306b\u3001\u4ee5\u4e0b \u306b \u5909\u63db__\n\n__\uff12\u6b21\u5143 \u306e *Numpy.array*\u578b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8__\n\n__\uff08 \u6b21\u5143 \uff09__\n\n* \u300c\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570\u300d\uff08\uff1d\u300c\u30c7\u30fc\u30bf\u671f\u9593\u300d\u306e\u6570\uff09\u00d7 \uff11\u671f\u9593\u3042\u305f\u308a\u306e\u30c7\u30fc\u30bf\u81ea\u8ee2\u6570\n\n\u203b shape \u306f\u3001(1,994, 63) \u306b \u306a\u308b\u3002\n\n```{python:python3 jupyter notebook}\n# \uff11\u671f\u9593 63\u6642\u70b9 \u3001\u5408\u8a08 1994\u671f\u9593 \u306e \u30c7\u30fc\u30bf\u306b\u5909\u63db\u3059\u308b \n125622 / 63\n```\n\n    1994.0\n\n```{python:python3 jupyter notebook}\nys_array = ys.reshape(1994, 63)\n```\n\n\n```{python:python3 jupyter notebook}\nys_array.shape\n```\n\n    (1994, 63)\n\n\n```{python:python3 jupyter notebook}\nlen(ys_array)\n```\n\n    1994\n\n\n```{python:python3 jupyter notebook}\nys_array[0]\n```\n\n    array([  6.42833292e-13,   9.98334166e-02,   1.98669331e-01,\n             2.95520207e-01,   3.89418342e-01,   4.79425539e-01,\n             5.64642473e-01,   6.44217687e-01,   7.17356091e-01,\n             7.83326910e-01,   8.41470985e-01,   8.91207360e-01,\n             9.32039086e-01,   9.63558185e-01,   9.85449730e-01,\n             9.97494987e-01,   9.99573603e-01,   9.91664810e-01,\n             9.73847631e-01,   9.46300088e-01,   9.09297427e-01,\n             8.63209367e-01,   8.08496404e-01,   7.45705212e-01,\n             6.75463181e-01,   5.98472144e-01,   5.15501372e-01,\n             4.27379880e-01,   3.34988150e-01,   2.39249329e-01,\n             1.41120008e-01,   4.15806624e-02,  -5.83741434e-02,\n            -1.57745694e-01,  -2.55541102e-01,  -3.50783228e-01,\n            -4.42520443e-01,  -5.29836141e-01,  -6.11857891e-01,\n            -6.87766159e-01,  -7.56802495e-01,  -8.18277111e-01,\n            -8.71575772e-01,  -9.16165937e-01,  -9.51602074e-01,\n            -9.77530118e-01,  -9.93691004e-01,  -9.99923258e-01,\n            -9.96164609e-01,  -9.82452613e-01,  -9.58924275e-01,\n            -9.25814682e-01,  -8.83454656e-01,  -8.32267442e-01,\n            -7.72764488e-01,  -7.05540326e-01,  -6.31266638e-01,\n            -5.50685543e-01,  -4.64602179e-01,  -3.73876665e-01,\n            -2.79415498e-01,  -1.82162504e-01,  -8.30894028e-02])\n\n###__\uff08 \u30c7\u30fc\u30bf\u4f5c\u6210 \u306f\u3001\u4ee5\u4e0a \u3067 \u5b8c\u4e86 \uff09__\n\n___\n\n\n##__sin\u66f2\u7dda\uff08\u4e71\u6570\u52a0\u7b97\u306a\u3057\uff09 \u3092 AE\u30e2\u30c7\u30eb \u3067 \u518d\u73fe\u3067\u304d\u308b\u304b \u691c\u8a3c\u5b9f\u884c__\n\n\n```{python:python3 jupyter notebook}\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nimport numpy as np\n```\n\n\n###__\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \u3092 \u8a2d\u8a08__\n\n```{python:python3 jupyter notebook}\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n```\n\n\n```{python:python3 jupyter notebook}\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n```\n\n###__\u30e2\u30c7\u30eb\u5b66\u7fd2 \u5b9f\u65bd__\n\n```{python:python3 jupyter notebook}\nys_ae_model_hist = autoencoder.fit(ys_array, ys_array,\n                                        nb_epoch=50,\n                                        batch_size=20,\n                                        shuffle=True,\n                                        validation_split=0.1)\n```\n\n    Train on 1794 samples, validate on 200 samples\n    Epoch 1/50\n    1794/1794 [==============================] - 0s - loss: 0.7720 - val_loss: 0.7603\n    Epoch 2/50\n    1794/1794 [==============================] - 0s - loss: 0.7619 - val_loss: 0.7528\n    Epoch 3/50\n    1794/1794 [==============================] - 0s - loss: 0.7524 - val_loss: 0.7457\n    Epoch 4/50\n    1794/1794 [==============================] - 0s - loss: 0.7433 - val_loss: 0.7389\n    Epoch 5/50\n    1794/1794 [==============================] - 0s - loss: 0.7344 - val_loss: 0.7321\n    Epoch 6/50\n    1794/1794 [==============================] - 0s - loss: 0.7256 - val_loss: 0.7252\n    Epoch 7/50\n    1794/1794 [==============================] - 0s - loss: 0.7168 - val_loss: 0.7181\n    Epoch 8/50\n    1794/1794 [==============================] - 0s - loss: 0.7078 - val_loss: 0.7107\n    Epoch 9/50\n    1794/1794 [==============================] - 0s - loss: 0.6985 - val_loss: 0.7030\n    Epoch 10/50\n    1794/1794 [==============================] - 0s - loss: 0.6889 - val_loss: 0.6949\n    Epoch 11/50\n    1794/1794 [==============================] - 0s - loss: 0.6789 - val_loss: 0.6864\n    Epoch 12/50\n    1794/1794 [==============================] - 0s - loss: 0.6685 - val_loss: 0.6776\n    Epoch 13/50\n    1794/1794 [==============================] - 0s - loss: 0.6575 - val_loss: 0.6682\n    Epoch 14/50\n    1794/1794 [==============================] - 0s - loss: 0.6461 - val_loss: 0.6584\n    Epoch 15/50\n    1794/1794 [==============================] - 0s - loss: 0.6342 - val_loss: 0.6481\n    Epoch 16/50\n    1794/1794 [==============================] - 0s - loss: 0.6218 - val_loss: 0.6374\n    Epoch 17/50\n    1794/1794 [==============================] - 0s - loss: 0.6090 - val_loss: 0.6263\n    Epoch 18/50\n    1794/1794 [==============================] - 0s - loss: 0.5959 - val_loss: 0.6148\n    Epoch 19/50\n    1794/1794 [==============================] - 0s - loss: 0.5825 - val_loss: 0.6030\n    Epoch 20/50\n    1794/1794 [==============================] - 0s - loss: 0.5690 - val_loss: 0.5909\n    Epoch 21/50\n    1794/1794 [==============================] - 0s - loss: 0.5555 - val_loss: 0.5786\n    Epoch 22/50\n    1794/1794 [==============================] - 0s - loss: 0.5420 - val_loss: 0.5662\n    Epoch 23/50\n    1794/1794 [==============================] - 0s - loss: 0.5287 - val_loss: 0.5538\n    Epoch 24/50\n    1794/1794 [==============================] - 0s - loss: 0.5157 - val_loss: 0.5415\n    Epoch 25/50\n    1794/1794 [==============================] - 0s - loss: 0.5030 - val_loss: 0.5293\n    Epoch 26/50\n    1794/1794 [==============================] - 0s - loss: 0.4908 - val_loss: 0.5173\n    Epoch 27/50\n    1794/1794 [==============================] - 0s - loss: 0.4790 - val_loss: 0.5055\n    Epoch 28/50\n    1794/1794 [==============================] - 0s - loss: 0.4678 - val_loss: 0.4941\n    Epoch 29/50\n    1794/1794 [==============================] - 0s - loss: 0.4571 - val_loss: 0.4831\n    Epoch 30/50\n    1794/1794 [==============================] - 0s - loss: 0.4469 - val_loss: 0.4724\n    Epoch 31/50\n    1794/1794 [==============================] - 0s - loss: 0.4372 - val_loss: 0.4622\n    Epoch 32/50\n    1794/1794 [==============================] - 0s - loss: 0.4281 - val_loss: 0.4524\n    Epoch 33/50\n    1794/1794 [==============================] - 0s - loss: 0.4195 - val_loss: 0.4430\n    Epoch 34/50\n    1794/1794 [==============================] - 0s - loss: 0.4114 - val_loss: 0.4341\n    Epoch 35/50\n    1794/1794 [==============================] - 0s - loss: 0.4037 - val_loss: 0.4256\n    Epoch 36/50\n    1794/1794 [==============================] - 0s - loss: 0.3965 - val_loss: 0.4175\n    Epoch 37/50\n    1794/1794 [==============================] - 0s - loss: 0.3897 - val_loss: 0.4099\n    Epoch 38/50\n    1794/1794 [==============================] - 0s - loss: 0.3833 - val_loss: 0.4027\n    Epoch 39/50\n    1794/1794 [==============================] - 0s - loss: 0.3773 - val_loss: 0.3958\n    Epoch 40/50\n    1794/1794 [==============================] - 0s - loss: 0.3716 - val_loss: 0.3893\n    Epoch 41/50\n    1794/1794 [==============================] - 0s - loss: 0.3662 - val_loss: 0.3832\n    Epoch 42/50\n    1794/1794 [==============================] - 0s - loss: 0.3611 - val_loss: 0.3774\n    Epoch 43/50\n    1794/1794 [==============================] - 0s - loss: 0.3563 - val_loss: 0.3720\n    Epoch 44/50\n    1794/1794 [==============================] - 0s - loss: 0.3518 - val_loss: 0.3668\n    Epoch 45/50\n    1794/1794 [==============================] - 0s - loss: 0.3475 - val_loss: 0.3619\n    Epoch 46/50\n    1794/1794 [==============================] - 0s - loss: 0.3435 - val_loss: 0.3573\n    Epoch 47/50\n    1794/1794 [==============================] - 0s - loss: 0.3397 - val_loss: 0.3529\n    Epoch 48/50\n    1794/1794 [==============================] - 0s - loss: 0.3360 - val_loss: 0.3488\n    Epoch 49/50\n    1794/1794 [==============================] - 0s - loss: 0.3325 - val_loss: 0.3448\n    Epoch 50/50\n    1794/1794 [==============================] - 0s - loss: 0.3293 - val_loss: 0.3411\n\n\n###__\u8fd4\u308a\u5024\uff08\u5b66\u7fd2\u904e\u7a0b\u30ed\u30b0\uff09__\n\n```{python:python3 jupyter notebook}\nys_ae_model_hist.history.keys()\n```\n\n\n\n\n    dict_keys(['val_loss', 'loss'])\n\n\n\n\n```{python:python3 jupyter notebook}\nys_ae_model_hist.history['loss']\n```\n\n\n\n\n    [0.77204897245568704,\n     0.76187914377337984,\n     0.75235942148976231,\n     0.74326126970176309,\n     0.73441166702321536,\n     0.72563687601482851,\n     0.71679974100661514,\n     0.70780590897552675,\n     0.69854387963230125,\n     0.688945037133186,\n     0.67894317573528229,\n     0.66847907745984347,\n     0.65753565756905175,\n     0.64609300605163655,\n     0.63416777061113683,\n     0.62179123285489202,\n     0.60900205885420411,\n     0.59588177247988339,\n     0.58251192258751916,\n     0.56899962669763815,\n     0.55545178378040305,\n     0.54198058366509716,\n     0.52869019374135084,\n     0.51567526744759606,\n     0.50301985351271183,\n     0.49079081406162733,\n     0.47903453469808022,\n     0.46778750924628182,\n     0.45706960170165351,\n     0.44688783737196436,\n     0.43724177626595984,\n     0.42811654483195533,\n     0.41950143260700645,\n     0.41137394737639155,\n     0.40371354680130445,\n     0.39649425943692523,\n     0.3896950109547197,\n     0.38328832982641664,\n     0.37725168217384697,\n     0.37156175955747944,\n     0.36619435784011384,\n     0.36112665060238963,\n     0.35634202971107587,\n     0.35181737876390268,\n     0.34754009573090172,\n     0.34348928851959032,\n     0.3396503280437112,\n     0.33600748638511369,\n     0.33254830813328157,\n     0.32926098203313525]\n\n\n\n\n```{python:python3 jupyter notebook}\nys_ae_model_hist.history['val_loss']\n```\n\n\n\n\n    [0.7603069543838501,\n     0.75278639793395996,\n     0.74574304223060606,\n     0.73890292644500732,\n     0.73209909796714778,\n     0.72517460584640503,\n     0.7180627644062042,\n     0.71067696809768677,\n     0.70297463536262517,\n     0.69490842819213872,\n     0.68644600510597231,\n     0.67755247354507442,\n     0.66821113824844358,\n     0.65841101408004765,\n     0.6481481730937958,\n     0.63743817806243896,\n     0.62632244825363159,\n     0.6148103713989258,\n     0.60298307538032536,\n     0.59089984297752385,\n     0.5786198645830154,\n     0.56623282432556155,\n     0.55382977724075322,\n     0.54148540198802952,\n     0.52927066981792448,\n     0.51727201938629153,\n     0.5055488228797913,\n     0.49413410127162932,\n     0.483080592751503,\n     0.47242816984653474,\n     0.46217039227485657,\n     0.45236197412014006,\n     0.44299422204494476,\n     0.43406962752342226,\n     0.42557652294635773,\n     0.4175167351961136,\n     0.40988496840000155,\n     0.40265323519706725,\n     0.39581511616706849,\n     0.38933811187744138,\n     0.38321558535099032,\n     0.37743270695209502,\n     0.37196222543716428,\n     0.36679600477218627,\n     0.36190806627273558,\n     0.35728473365306856,\n     0.35290284156799318,\n     0.34875249564647676,\n     0.34481717348098756,\n     0.34108110964298249]\n\n\n###__\u5b66\u7fd2\u4e2d \u306e loss\u306e\u63a8\u79fb \u3092 \u63cf\u753b__\n\n```{python:python3 jupyter notebook}\nloss = ys_ae_model_hist.history['loss']\nval_loss = ys_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n![output_34_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/b2c20b99-2aed-9535-e766-f02a2fb9a6d8.png)\n\n###__\u672b\u5c3e10\u4ef6\u306e\u30c7\u30fc\u30bf \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u5024\u3068\u306e\u76f8\u95a2\u4fc2\u6570 \u3092 \u7b97\u51fa__\n\n__\u672c\u6765\u306f\u3001\u5b66\u7fd2\u6642\u306b\u4e0e\u3048\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u518d\u73fe\u7cbe\u5ea6 \u3092 \u691c\u8a3c\u3059\u308b\u5fc5\u8981 \u304c \u3042\u308b\u3002__\n\n__\uff08 \u691c\u8a3c\u30c7\u30fc\u30bf\u306f\u5168\u30c7\u30fc\u30bf\u306e\uff11\u5272\u3001\u3068\u8a2d\u5b9a\u3057\u3066\u306f\u3044\u305f\u304c \uff09__\n\n\n```{python:python3 jupyter notebook}\npredicted_value_list = autoencoder.predict(ys_array[-10:])\n```\n\n\n```{python:python3 jupyter notebook}\nfrom pprint import pprint\npprint(predicted_value_list)\n```\n\n    array([[ 0.73594105,  0.72225511,  0.7410332 ,  0.54851592,  0.67742962,\n             0.57801753,  0.53651255,  0.27468202,  0.37381202,  0.34580681,\n             0.12015434,  0.22161724,  0.11171708,  0.11844756,  0.07758933,\n             0.01668905,  0.06900305,  0.06583834,  0.08793348,  0.04965068,\n             0.02456675,  0.0236507 ,  0.02923935,  0.03159072,  0.04385547,\n             0.01638181,  0.02988848,  0.01914812,  0.02372373,  0.01619347,\n             0.01408893,  0.03196427,  0.03373871,  0.04592645,  0.08455544,\n             0.02531998,  0.01292785,  0.08512255,  0.06376205,  0.08022432,\n             0.11108302,  0.06954875,  0.10636622,  0.2902841 ,  0.07567588,\n             0.27879727,  0.27983958,  0.28210771,  0.50637126,  0.30316794,\n             0.62549382,  0.05440421,  0.61653072,  0.71850741,  0.83345568,\n             0.14307067,  0.55008024,  0.80726302,  0.90825933,  0.73716128,\n             0.82216775,  0.7813161 ,  0.7313447 ],\n           [ 0.733729  ,  0.71809208,  0.73514068,  0.5435183 ,  0.66907358,\n             0.56645191,  0.52723998,  0.26938403,  0.36436084,  0.34249806,\n             0.11959784,  0.21422067,  0.11017971,  0.11774467,  0.07451925,\n             0.01654441,  0.06893405,  0.06502064,  0.08485675,  0.04765516,\n             0.02469465,  0.02301026,  0.02847793,  0.03115889,  0.04317827,\n             0.01666928,  0.02952299,  0.01887808,  0.02336007,  0.01593957,\n             0.01405849,  0.03133192,  0.03430411,  0.0461445 ,  0.08579167,\n             0.02623304,  0.01295731,  0.08695532,  0.0656364 ,  0.08246984,\n             0.11445702,  0.0712442 ,  0.11212448,  0.29577145,  0.07584085,\n             0.28411964,  0.28272381,  0.28900096,  0.50751227,  0.3108463 ,\n             0.63533592,  0.05616919,  0.61515594,  0.72062379,  0.83252782,\n             0.14252836,  0.55875736,  0.80938905,  0.9084363 ,  0.73968405,\n             0.82038754,  0.77843523,  0.72832775],\n           [ 0.73144782,  0.71383685,  0.72910118,  0.53849864,  0.66056025,\n             0.55478871,  0.51793283,  0.26419768,  0.35504192,  0.33923796,\n             0.11909787,  0.20705871,  0.1087138 ,  0.11709987,  0.07160537,\n             0.01641853,  0.06890847,  0.06425421,  0.08192531,  0.04576982,\n             0.0248466 ,  0.02240865,  0.02776112,  0.03075993,  0.04254434,\n             0.01697996,  0.02918763,  0.01863089,  0.0230243 ,  0.01570642,\n             0.01404369,  0.03073879,  0.03490801,  0.04639864,  0.08709227,\n             0.02720362,  0.01300177,  0.08887239,  0.06760614,  0.0848212 ,\n             0.11797456,  0.07302306,  0.11820778,  0.30136323,  0.07605208,\n             0.28955019,  0.28567237,  0.29603648,  0.50864416,  0.3186703 ,\n             0.64502329,  0.0580289 ,  0.61374146,  0.7226721 ,  0.83153045,\n             0.1420446 ,  0.56737715,  0.81143236,  0.90855801,  0.74213326,\n             0.81852782,  0.7754637 ,  0.72523248],\n           [ 0.72979778,  0.71024942,  0.72308356,  0.53334963,  0.65208673,\n             0.54426682,  0.50862783,  0.25842863,  0.34649891,  0.33693671,\n             0.11846694,  0.20022689,  0.10746393,  0.11644603,  0.06889965,\n             0.01633854,  0.06884535,  0.06350876,  0.07890771,  0.0439423 ,\n             0.02493025,  0.02178143,  0.02712904,  0.03050009,  0.0419331 ,\n             0.01726541,  0.02875838,  0.01835509,  0.02264947,  0.01545038,\n             0.0140192 ,  0.03028108,  0.03560189,  0.04681312,  0.08831328,\n             0.02814098,  0.0131101 ,  0.09046544,  0.06941704,  0.08747324,\n             0.12114646,  0.0745055 ,  0.12493989,  0.30690733,  0.07632991,\n             0.294471  ,  0.2885122 ,  0.3027913 ,  0.50923568,  0.32582393,\n             0.65443295,  0.05977015,  0.61267525,  0.72526211,  0.8302238 ,\n             0.14149216,  0.57578909,  0.81288671,  0.90868682,  0.74354064,\n             0.81626701,  0.77169472,  0.72155625],\n           [ 0.72838098,  0.70690531,  0.71699625,  0.52813667,  0.64355201,\n             0.53420484,  0.49931821,  0.25250176,  0.33835238,  0.33506089,\n             0.11781451,  0.19366014,  0.10634047,  0.11582182,  0.06635568,\n             0.01628798,  0.06879146,  0.06280066,  0.07594606,  0.04219799,\n             0.02499847,  0.02116678,  0.02655305,  0.03031433,  0.04135505,\n             0.01755309,  0.0283094 ,  0.01808092,  0.02227516,  0.01519708,\n             0.01399964,  0.0298974 ,  0.03636079,  0.04731946,  0.08953599,\n             0.02909533,  0.01325531,  0.09195744,  0.07120647,  0.09033319,\n             0.12423398,  0.0758974 ,  0.13219135,  0.31248564,  0.07666326,\n             0.29922265,  0.29134056,  0.30949306,  0.50959349,  0.33274963,\n             0.66362816,  0.06150809,  0.61173451,  0.72803396,  0.82874185,\n             0.14094491,  0.58407104,  0.81405413,  0.90878683,  0.74447626,\n             0.81378192,  0.7675159 ,  0.7175799 ],\n           [ 0.7269029 ,  0.70348597,  0.71076947,  0.52290851,  0.63488168,\n             0.52409875,  0.49000126,  0.24671496,  0.3303355 ,  0.33322498,\n             0.11721929,  0.18731335,  0.10527967,  0.11525478,  0.06394097,\n             0.01625498,  0.06878101,  0.06214106,  0.07313178,  0.04055157,\n             0.02509043,  0.02058977,  0.02601322,  0.03015648,  0.04081708,\n             0.01786439,  0.02789226,  0.01782932,  0.02192859,  0.01496413,\n             0.01399564,  0.02954487,  0.03716575,  0.0478667 ,  0.09082279,\n             0.03010837,  0.01341738,  0.09352115,  0.07308438,  0.09332775,\n             0.12744448,  0.07735961,  0.13985269,  0.31816021,  0.07704419,\n             0.3040626 ,  0.29422975,  0.31631491,  0.50994205,  0.33978379,\n             0.67265207,  0.06333612,  0.61075592,  0.73072875,  0.82718396,\n             0.14045629,  0.59227753,  0.81515121,  0.90883183,  0.74535125,\n             0.81120551,  0.7632218 ,  0.71351373],\n           [ 0.72536319,  0.6999917 ,  0.704404  ,  0.51766777,  0.62608141,\n             0.51395935,  0.48068604,  0.2410683 ,  0.32245255,  0.33142927,\n             0.11668076,  0.18118317,  0.10428045,  0.11474442,  0.06164943,\n             0.01623947,  0.06881401,  0.06152886,  0.07045811,  0.03899764,\n             0.02520637,  0.02004845,  0.02550804,  0.03002618,  0.04031809,\n             0.01820029,  0.02750594,  0.01759955,  0.02160882,  0.01475078,\n             0.01400718,  0.02922259,  0.0380188 ,  0.04845591,  0.09217536,\n             0.03118355,  0.01359682,  0.09515864,  0.07505444,  0.09646242,\n             0.13078159,  0.07889453,  0.14793916,  0.3239294 ,  0.07747317,\n             0.30899   ,  0.29717973,  0.32325432,  0.51028126,  0.34692311,\n             0.68149918,  0.06525838,  0.60973954,  0.73334688,  0.82554907,\n             0.14002593,  0.60040247,  0.81617844,  0.90882182,  0.74616593,\n             0.80853623,  0.75881124,  0.70935792],\n           [ 0.7237618 ,  0.69642258,  0.69790101,  0.51241696,  0.61715752,\n             0.50379795,  0.47138187,  0.2355614 ,  0.31470788,  0.32967392,\n             0.1161984 ,  0.17526604,  0.10334171,  0.11429019,  0.05947509,\n             0.01624138,  0.06889052,  0.06096307,  0.06791853,  0.03753109,\n             0.02534656,  0.01954101,  0.02503605,  0.02992313,  0.03985702,\n             0.01856187,  0.02714945,  0.01739092,  0.02131493,  0.01455638,\n             0.01403431,  0.02892984,  0.03892203,  0.04908822,  0.09359549,\n             0.03232447,  0.01379424,  0.09687206,  0.07712036,  0.0997428 ,\n             0.1342489 ,  0.08050464,  0.15646495,  0.32979146,  0.07795085,\n             0.31400385,  0.30019006,  0.33030838,  0.51061106,  0.35416403,\n             0.69016457,  0.06727909,  0.60868567,  0.73588848,  0.82383615,\n             0.13965358,  0.6084401 ,  0.81713647,  0.90875673,  0.74692029,\n             0.80577284,  0.75428361,  0.70511264],\n           [ 0.72209871,  0.69277883,  0.69126171,  0.50715888,  0.60811681,\n             0.4936257 ,  0.46209767,  0.2301939 ,  0.30710483,  0.3279593 ,\n             0.11577175,  0.16955787,  0.1024624 ,  0.11389167,  0.05741234,\n             0.01626073,  0.06901069,  0.06044275,  0.06550682,  0.03614709,\n             0.02551136,  0.01906575,  0.02459596,  0.02984708,  0.03943282,\n             0.01895024,  0.02682191,  0.01720282,  0.02104606,  0.01438031,\n             0.0140771 ,  0.02866583,  0.03987772,  0.04976487,  0.09508491,\n             0.03353506,  0.01401024,  0.0986636 ,  0.07928607,  0.10317458,\n             0.13785011,  0.08219247,  0.16544363,  0.33574444,  0.07847781,\n             0.31910303,  0.30326062,  0.33747393,  0.51093125,  0.36150271,\n             0.69864458,  0.06940277,  0.60759455,  0.73835415,  0.82204425,\n             0.13933884,  0.6163848 ,  0.81802583,  0.90863657,  0.7476145 ,\n             0.80291378,  0.74963796,  0.70077825],\n           [ 0.72037357,  0.68906099,  0.68448782,  0.50189602,  0.59896636,\n             0.4834539 ,  0.45284268,  0.2249649 ,  0.29964685,  0.32628542,\n             0.11540043,  0.1640545 ,  0.10164149,  0.11354861,  0.05545575,\n             0.01629759,  0.06917458,  0.05996701,  0.06321692,  0.03484115,\n             0.02570114,  0.01862111,  0.02418652,  0.02979784,  0.03904467,\n             0.01936666,  0.02652246,  0.01703467,  0.02080148,  0.014222  ,\n             0.01413569,  0.02842995,  0.04088825,  0.05048718,  0.09664558,\n             0.03481942,  0.01424557,  0.1005355 ,  0.08155561,  0.10676365,\n             0.1415889 ,  0.08396073,  0.17488745,  0.34178618,  0.07905488,\n             0.32428643,  0.30639085,  0.34474742,  0.51124179,  0.36893475,\n             0.70693517,  0.07163405,  0.60646623,  0.74074394,  0.82017219,\n             0.13908161,  0.62423152,  0.81884694,  0.90846097,  0.7482487 ,\n             0.79995787,  0.74487394,  0.6963551 ]], dtype=float32)\n\n####__\u5165\u529b\u30c7\u30fc\u30bf\u4ef6\u6570 \u3068 \u540c\u3058\u300110\u4ef6 \u306e \u51fa\u529b\u5024\uff08\u5165\u529b\u5024\u306e\u518d\u73fe\u5024\uff09 \u304c \u5f97\u3089\u308c\u305f__\n\n```{python:python3 jupyter notebook}\nprint(len(predicted_value_list))\n```\n\n    10\n\n####__\uff08 \u4eca\u56de\u3001\u6271\u3063\u305f\u306e \u306f\u3001\u4ee5\u4e0b\u306e\u6319\u52d5 \u3092 \u793a\u3059\u6642\u7cfb\u5217\u30c7\u30fc\u30bf \uff09__\n\n```{python:python3 jupyter notebook}\nprint(len(predicted_value_list[0]))\n```\n\n    63\n\n####__\uff11\u3064\u76ee\u306e\u5165\u529b\u30c7\u30fc\u30bf \u306f\u3001\u51fa\u529b\u5074\u3067\u3001\u518d\u73fe\u6027\u306e\u3042\u308b\u30c7\u30fc\u30bf \u3092 \u751f\u6210\u3067\u304d\u3066\u3044\u308b\u3002__\n\n__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: $0.873$__\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(ys_array[-10], predicted_value_list[0])\n```\n\n\n\n\n    array([[ 1.        ,  0.87392303],\n           [ 0.87392303,  1.        ]])\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(ys_array[-10], predicted_value_list[0])[0][1]\n```\n\n\n\n\n    0.87392302591375859\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(ys_array[-10], predicted_value_list[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120fad080>]\n\n\n\n\n![output_41_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/32f1774b-3214-4990-6c64-ae32af5f4155.png)\n\n\n__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: $0.874$__\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(ys_array[-9], predicted_value_list[1])[0][1]\n```\n\n\n\n\n    0.87405657153157468\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(ys_array[-9], predicted_value_list[1])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x121f7f898>]\n\n\n\n\n![output_43_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/b5f242da-183f-8975-378d-f0c5b6cbdb39.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(ys_array[-8], predicted_value_list[2])[0][1]\n```\n\n\n\n\n    0.87415094764373547\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(ys_array[-8], predicted_value_list[2])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x11f4a4be0>]\n\n\n\n\n![output_45_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/f5dfd01b-5085-1770-d917-fd9b9c6f9f74.png)\n\n\n```{python:python3 jupyter notebook}\n# \u5165\u529b\u5024\u3068\u51fa\u529b\u5024\u306f\u76f8\u95a2\u4fc2\u6570\u304c\u9ad8\u3044\u3002\uff1d\uff1e\u51fa\u529b\u5024\u306f\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u3002\n```\n\n\n```{python:python3 jupyter notebook}\n# \u5b66\u7fd2\u3055\u305b\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u3092\u5165\u529b\u3057\u3066\u307f\u308b\u3002\nrandon_cumsum_array = np.array(randn_cumsum_series).reshape(1994, 63)\n```\n\n\n```{python:python3 jupyter notebook}\npd.DataFrame(randon_cumsum_array).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x121000470>\n\n\n\n\n![output_48_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/ec4b93cb-1e8a-bf7d-f8ff-9c40c0c77649.png)\n\n\n```{python:python3 jupyter notebook}\npd.DataFrame(ys_array).plot()\n```\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x1209e8ef0>\n\n![output_49_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/3d349a3e-396e-d3d7-2dad-7f48f2649369.png)\n\n\n###__\u4eca\u5ea6\u306f\u3001\u5b66\u7fd2\u6642 \u306b \u4e0e\u3048\u3066\u3044\u306a\u3044\u65b0\u898f\u30c7\u30fc\u30bf \u304c \u518d\u73fe\u3055\u308c\u300e\u306a\u3044\u300f\u3053\u3068 \u3092 \u78ba\u8a8d\u3059\u308b\u3002__\n\n\n```{python:python3 jupyter notebook}\npredicted_value_list = autoencoder.predict(randon_cumsum_array[-10:])\n```\n\n\n```{python:python3 jupyter notebook}\nlen(predicted_value_list)\n```\n\n\n\n\n    10\n\n\n\n\n```{python:python3 jupyter notebook}\nprint(predicted_value_list[0])\n```\n\n    [  1.72828807e-21   2.12488513e-18   8.13269881e-32   0.00000000e+00\n       0.00000000e+00   0.00000000e+00   8.53706776e-21   7.88663642e-16\n       1.00000000e+00   0.00000000e+00   1.00000000e+00   8.83193148e-18\n       5.17360605e-30   4.57608916e-20   1.00000000e+00   0.00000000e+00\n       1.00000000e+00   2.03638096e-11   1.20015812e-11   1.00000000e+00\n       1.00000000e+00   5.81137497e-26   6.16388948e-27   0.00000000e+00\n       1.00000000e+00   2.78305106e-38   1.01486821e-23   0.00000000e+00\n       1.00000000e+00   0.00000000e+00   9.99998450e-01   1.95422539e-31\n       9.52638745e-01   1.00000000e+00   5.48459778e-35   0.00000000e+00\n       3.53405721e-06   1.00000000e+00   1.00000000e+00   3.29031007e-17\n       9.95175838e-01   1.00000000e+00   3.15579701e-10   1.00000000e+00\n       1.40501963e-27   1.00000000e+00   4.27256243e-29   1.00000000e+00\n       1.00000000e+00   9.71367717e-01   1.29174707e-32   1.00000000e+00\n       3.22033428e-02   4.78385709e-14   7.97672728e-27   4.99435331e-28\n       7.83956060e-12   1.00000000e+00   1.32813274e-10   0.00000000e+00\n       1.00000000e+00   1.00000000e+00   9.56023505e-05]\n\n\n####__\u60f3\u5b9a\u901a\u308a\u3001\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2 \u306f \u4f4e\u3044\u3002__\n\n__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: $-0.204$__\n\n```{python:python3 jupyter notebook}\n# \u60f3\u5b9a\u901a\u308a\u3001\u4f4e\u3044\nnp.corrcoef(randon_cumsum_array[-10], predicted_value_list[0])[0][1]\n```\n\n\n    -0.20473221757684804\n\n___\n\n* \u5b66\u7fd2\u3055\u305b\u305f\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3 \u304c\u3001\u901a\u5e38\uff08\u6b63\u5e38\u6642\uff09\u306e\u6ce2\u5f62\u30d1\u30bf\u30fc\u30f3 \u3067 \u3042\u308b \u5834\u5408\u3001\u30e2\u30c7\u30eb\u304c \u518d\u73fe\u3067\u304d\u306a\u3044 \u65b0\u898f\u5165\u529b\u5024 \n\n\u306f\u3001\n\n* \u5b66\u7fd2\u6642 \u306b \u7d4c\u9a13\u3057\u305f\u300c\u6b63\u5e38\u30d1\u30bf\u30fc\u30f3\u300d\u3068\u306f\u7570\u306a\u308b\uff08\u7570\u5e38\u72b6\u614b\u306e\uff09\u30d1\u30bf\u30fc\u30f3\u7cfb\u5217\u30c7\u30fc\u30bf \u3067\u3042\u308b \u53ef\u80fd\u6027 \u3092 \u793a\u5506\u3057\u3066\u3044\u308b\u3002\n\n___\n\n\n```{python:python3 jupyter notebook}\nplt.plot(randon_cumsum_array[-10], predicted_value_list[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120476da0>]\n\n\n\n![output_54_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/5a375c69-259e-6fbc-34d7-c9656295af2f.png)\n\n\n__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: $0.142$__\n\n```{python:python3 jupyter notebook}\n# \u60f3\u5b9a\u901a\u308a\u3001\u4f4e\u3044\nnp.corrcoef(randon_cumsum_array[-9], predicted_value_list[1])[0][1]\n```\n\n\n\n\n    0.14285102123552723\n\n\n__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570: $0.065$__\n\n```{python:python3 jupyter notebook}\n# \u5c11\u3057\u4e0a\u304c\u3063\u305f\u304c\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u3068\u306f\u8a00\u3048\u306a\u3044\u3002\nnp.corrcoef(randon_cumsum_array[-8], predicted_value_list[2])[0][1]\n```\n\n\n\n\n    0.065785686223646611\n\n___\n\n##__\uff13\u3064\u306e\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3 \u3092 \u518d\u73fe\u3067\u304d\u308b AE\u30e2\u30c7\u30eb \u3092 \u69cb\u7bc9\u3059\u308b\u3002__\n\n\u69cb\u7bc9\u3057\u305f\u30e2\u30c7\u30eb \u306f\u3001\n\n* model.predict()\u30e1\u30bd\u30c3\u30c9 \u3067\u3001\uff11\u5ea6\u306b\u3000\uff11\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff0863\u6642\u70b9\u306e\u584a\u30c7\u30fc\u30bf\uff09 \u3092 \u53d7\u3051\u53d6\u308a\u3001\n* \u53d7\u3051\u53d6\u3063\u305f\u5165\u529b\u30c7\u30fc\u30bf \u306e \u518d\u73fe\u63a8\u5b9a\u5024 \u3092 1\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5206\uff0863\u6642\u70b9\u306e\u584a\u30c7\u30fc\u30bf\uff09 \u3092 \u51fa\u529b\u3059\u308b\u3002\n\n\n```{python:python3 jupyter notebook}\n# \u5148\u307b\u3069\u306f\u3001sin\u66f2\u7dda\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u306e\u307f\u3092\u5b66\u7fd2\u3055\u305b\u308bAutoEncoder\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u305f\u3002\n# \u4eca\u5ea6\u306f\u30013\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u30013\u30d1\u30bf\u30fc\u30f3\u306e\u5165\u529b\u5024\u306f\u3061\u3083\u3093\u3068\u518d\u73fe\u3067\u304d\u308bAE\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3059\u308b\u3002\n# \u65b0\u898f\u306b\u6295\u5165\u3059\u308b\u5165\u529b\u5024\u3068\u3001\u305d\u306e\u51fa\u529b\u5024\u306e\u76f8\u95a2\u4fc2\u6570\u304c\u4f4e\u3051\u308c\u3070\u3001\u65b0\u898f\u5165\u529b\u3057\u305f\u30d1\u30bf\u30fc\u30f3\u306f\u3001\n# \u5b66\u7fd2\u3057\u305f3\u30d1\u30bf\u30fc\u30f3\u306e\u3044\u305a\u308c\u3068\u3082\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3067\u3042\u308b\u3068\u63a8\u6e2c\u3067\u304d\u308b\u3002\n```\n\n\n```{python:python3 jupyter notebook}\nimport types\ntype(normal_distribution_cumsum_series)\n```\n\n\n\n\n    numpy.ndarray\n\n\n\n\n```{python:python3 jupyter notebook}\nnorm_dist_cumsum_array = normal_distribution_cumsum_series.reshape(1994, 63)\n```\n\n\n```{python:python3 jupyter notebook}\nrandom_cumsum_array = randon_cumsum_array.reshape(1994, 63)\n```\n\n\n```{python:python3 jupyter notebook}\nprint(norm_dist_cumsum_array.shape,\n      random_cumsum_array.shape)\n```\n\n    (1994, 63) (1994, 63)\n\n\n\n```{python:python3 jupyter notebook}\nthree_pattern_time_series_joined_array = np.vstack((ys_array, norm_dist_cumsum_array))\n```\n\n\n```{python:python3 jupyter notebook}\nthree_pattern_time_series_joined_array.shape\n```\n\n\n\n\n    (3988, 63)\n\n\n\n\n```{python:python3 jupyter notebook}\nthree_pattern_time_series_joined_array = np.vstack((three_pattern_time_series_joined_array, random_cumsum_array))\n```\n\n\n```{python:python3 jupyter notebook}\nthree_pattern_time_series_joined_array.shape\n```\n\n\n\n\n    (5982, 63)\n\n\n\n\n```{python:python3 jupyter notebook}\n1994*3 == 5982\n```\n\n\n\n\n    True\n\n\n\n\n```{python:python3 jupyter notebook}\nthree_patterns_data = np.random.permutation(three_pattern_time_series_joined_array)\n```\n\n\n```{python:python3 jupyter notebook}\nthree_patterns_data.shape\n```\n\n\n\n\n    (5982, 63)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[0])\nplt.show()\n```\n\n\n![output_69_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a86deb98-197c-db91-a7f8-9b4371c70170.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[2])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x12078c710>]\n\n\n\n![output_70_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/594cdba3-aa1f-26a7-8170-dd24876050d1.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[3])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120a946a0>]\n\n\n![output_71_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a7031891-c73b-992a-216e-5227407b6c28.png)\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[4])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x121815710>]\n\n\n\n![output_72_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/49176831-be51-faf1-4e9d-d49142a7cfce.png)\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[5])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x1219fe518>]\n\n\n\n![output_73_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/b605aa83-6fbe-088f-5485-84b7e044fed5.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[6])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120688a20>]\n\n\n\n![output_74_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/0348118b-e0da-457c-c052-38824c3dc0ad.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[7])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x120940da0>]\n\n\n![output_75_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/06062d0f-b650-16b9-65ab-b3f65a34b184.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[8])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x1226c7d30>]\n\n\n![output_76_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/5b6f7767-1af1-7c95-774b-e92a31daf9f8.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[9])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x1227e1c18>]\n\n\n![output_77_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/23a86320-8ed5-0970-15a4-5367c570186d.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[10])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x12e55ca20>]\n\n\n![output_78_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/fd970ca1-9724-2fb7-0677-c1df6623a758.png)\n\n###__\u518d\u5ea6\u3001\u30e2\u30c7\u30eb \u3092 \u69cb\u7bc9\uff08\u5b66\u7fd2\uff09\u3055\u305b\u3066\u307f\u308b__\n\n\n```{python:python3 jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=300,\n                                               batch_size=40,\n                                               shuffle=True,\n                                               validation_split=0.2)\n```\n\n    Train on 4785 samples, validate on 1197 samples\n    Epoch 1/300\n    4785/4785 [==============================] - 0s - loss: 6721223.6275 - val_loss: 6822094.8446\n    Epoch 2/300\n    4785/4785 [==============================] - 0s - loss: 6721165.0637 - val_loss: 6822094.8446\n    Epoch 3/300\n    4785/4785 [==============================] - 0s - loss: 6721165.1165 - val_loss: 6822094.8446\n    Epoch 4/300\n    4785/4785 [==============================] - 0s - loss: 6721145.5157 - val_loss: 6822067.5079\n    Epoch 5/300\n    4785/4785 [==============================] - 0s - loss: 6721134.8430 - val_loss: 6822067.5079\n    Epoch 6/300\n    4785/4785 [==============================] - 0s - loss: 6721134.9310 - val_loss: 6822067.5079\n    Epoch 7/300\n    4785/4785 [==============================] - 0s - loss: \n\n\n\uff08 \u4e2d\u7565 \uff09\n\n\n    Epoch 194/300\n    4785/4785 [==============================] - 0s - loss: 6720885.7926 - val_loss: 6821814.5998\n    Epoch 195/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8621 - val_loss: 6821814.5998\n    Epoch 196/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8568 - val_loss: 6821814.5998\n    Epoch 197/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\n    Epoch 198/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8809 - val_loss: 6821814.5998\n    Epoch 199/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8250 - val_loss: 6821814.5998\n    Epoch 200/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8427 - val_loss: 6821814.5998\n    Epoch 201/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8203 - val_loss: 6821814.5998\n    Epoch 202/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8777 - val_loss: 6821814.5998\n    Epoch 203/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\n    Epoch 204/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8234 - val_loss: 6821814.5998\n    Epoch 205/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8302 - val_loss: 6821814.5998\n    Epoch 206/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8579 - val_loss: 6821814.5998\n    Epoch 207/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8297 - val_loss: 6821814.5998\n    Epoch 208/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8673 - val_loss: 6821814.5998\n    Epoch 209/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8490 - val_loss: 6821814.5998\n    Epoch 210/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8036 - val_loss: 6821814.5998\n    Epoch 211/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5998\n    Epoch 212/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8848 - val_loss: 6821814.5998\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 270/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8464 - val_loss: 6821814.5581\n    Epoch 271/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8626 - val_loss: 6821814.5581\n    Epoch 272/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8501 - val_loss: 6821814.5581\n    Epoch 273/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8589 - val_loss: 6821814.5581\n    Epoch 274/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8490 - val_loss: 6821814.5581\n    Epoch 275/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8357 - val_loss: 6821814.5581\n    Epoch 276/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8757 - val_loss: 6821814.5581\n    Epoch 277/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8140 - val_loss: 6821814.5581\n    Epoch 278/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8474 - val_loss: 6821814.5581\n    Epoch 279/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8354 - val_loss: 6821814.5581\n    Epoch 280/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8997 - val_loss: 6821814.5581\n    Epoch 281/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8171 - val_loss: 6821814.5581\n    Epoch 282/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8422 - val_loss: 6821814.5581\n    Epoch 283/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8370 - val_loss: 6821814.5581\n    Epoch 284/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8375 - val_loss: 6821814.5581\n    Epoch 285/300\n    4785/4785 [==============================] - 0s - loss: 6720885.8299 - val_loss: 6821814.5581\n    Epoch 286/300\n    4785/4785 [==============================] - 0s - loss: 6720885.1541 - val_loss: 6821805.3400\n    Epoch 287/300\n    4785/4785 [==============================] - 0s - loss: 6720875.6938 - val_loss: 6821805.3400\n    Epoch 288/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7273 - val_loss: 6821805.3400\n    Epoch 289/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7299 - val_loss: 6821805.3400\n    Epoch 290/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7011 - val_loss: 6821805.3400\n    Epoch 291/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7670 - val_loss: 6821805.3400\n    Epoch 292/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7560 - val_loss: 6821805.3400\n    Epoch 293/300\n    4785/4785 [==============================] - 0s - loss: 6720875.8323 - val_loss: 6821805.3400\n    Epoch 294/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7858 - val_loss: 6821805.3400\n    Epoch 295/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7879 - val_loss: 6821805.3400\n    Epoch 296/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7168 - val_loss: 6821805.3400\n    Epoch 297/300\n    4785/4785 [==============================] - 0s - loss: 6720875.6907 - val_loss: 6821805.3400\n    Epoch 298/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7429 - val_loss: 6821805.3400\n    Epoch 299/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7894 - val_loss: 6821805.3400\n    Epoch 300/300\n    4785/4785 [==============================] - 0s - loss: 6720875.7764 - val_loss: 6821805.3400\n\n\n\n```{python:python3 jupyter notebook}\nloss = three_patterns_ae_model_hist.history['loss']\nval_loss = three_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_80_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/6d37f274-7b94-d5bb-711c-931ceeaf2e67.png)\n\n\n###__\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306eloss \u304c \u5168\u304f \u6e1b\u5c11 \u3057\u306a\u3044\u3002__\n\n```{python:python3 jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067\u306eloss\u304c\u5168\u304f\u6e1b\u3089\u306a\u3044\u3002\n\n```\n\n###__\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e2d\u9593\u5c64 \u3092 \uff11\u5c64 \u8ffd\u52a0\u3057\u3066\u3001\u518d\u30c1\u30e3\u30ec\u30f3\u30b8__\n\n```{python:python3 jupyter notebook}\ninput = Input(shape=(63,))\nencoded_1 = Dense(40, activation='relu')(input)\nencoded_2 = Dense(20, activation='relu')(encoded_1)\nencoded_3 = Dense(40, activation='relu')(encoded_2)\ndecoded = Dense(63, activation='sigmoid')(encoded_3)\nautoencoder = Model(input=input, output=decoded)\n```\n\n__\uff08 \u53c2\u8003 \uff09__\n\n* [TypeNULL\u3055\u3093 Qiitam\u8a18\u4e8b\u300cKeras\u3092\u4f7f\u3063\u305fAutoEncodder\u306e\u30e1\u30e2\u300d](http://qiita.com/TypeNULL/items/4e4d7de11ab4361d6085)\n\n\n```{python:python3 jupyter notebook}\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\n# \uff08\u53c2\u8003\uff09http://qiita.com/TypeNULL/items/4e4d7de11ab4361d6085\n# \uff08\u4e0a\u8a18\u8a18\u4e8b\uff09MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u3067\u7279\u5fb4\u62bd\u51fa\u3092\u3084\u308b\u4f8b\u306f\u591a\u3044\u304c\uff0c\u672c\u8a18\u4e8b\u3067\u306f\u753b\u50cf\u4ee5\u5916\u306e\u4fe1\u53f7\u3092\u6271\u3046\u3053\u3068\u306b\u3059\u308b\uff0e\nautoencoder.compile(optimizer='adadelta', loss='mse')\n#autoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n```\n\n\n```{python:python3 jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\n#3\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u6df7\u3058\u3063\u3066\u3044\u308b\u304b\u3089\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306f\u3057\u306a\u3044\u65b9\u304c\u826f\u3044\u304b\u3082\u3057\u308c\u306a\u3044\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=1000,\n                                               shuffle=True,\n                                               validation_split=0.1)\n```\n\n    Train on 5383 samples, validate on 599 samples\n    Epoch 1/1000\n    5383/5383 [==============================] - 1s - loss: 6763711.8862 - val_loss: 6545538.1327\n    Epoch 2/1000\n    5383/5383 [==============================] - 0s - loss: 6763476.6957 - val_loss: 6545385.9658\n    Epoch 3/1000\n    5383/5383 [==============================] - 0s - loss: 6763339.6385 - val_loss: 6545251.5868\n    Epoch 4/1000\n    5383/5383 [==============================] - 0s - loss: 6763156.2270 - val_loss: 6545023.5601\n    Epoch 5/1000\n    5383/5383 [==============================] - 0s - loss: 6763010.0146 - val_loss: 6544992.0684\n    Epoch 6/1000\n    5383/5383 [==============================] - 0s - loss: 6762980.2384 - val_loss: 6544951.2805\n    Epoch 7/1000\n    5383/5383 [==============================] - 0s - loss: \n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 152/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3166 - val_loss: 6544495.6895\n    Epoch 153/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3432 - val_loss: 6544495.7162\n    Epoch 154/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3362 - val_loss: 6544495.7162\n    Epoch 155/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.2878 - val_loss: 6544495.7162\n    Epoch 156/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3259 - val_loss: 6544495.7162\n    Epoch 157/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3532 - val_loss: 6544495.7162\n    Epoch 158/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3567 - val_loss: 6544495.7162\n    Epoch 159/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3218 - val_loss: 6544495.7162\n    Epoch 160/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3059 - val_loss: 6544495.7162\n    Epoch 161/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3103 - val_loss: 6544495.7162\n    Epoch 162/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.2997 - val_loss: 6544495.7162\n    Epoch 163/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.2843 - val_loss: 6544495.7162\n    Epoch 164/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3346 - val_loss: 6544495.7162\n    Epoch 165/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3615 - val_loss: 6544495.6895\n    Epoch 166/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3164 - val_loss: 6544495.6895\n    Epoch 167/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3279 - val_loss: 6544495.7162\n    Epoch 168/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3340 - val_loss: 6544495.6895\n    Epoch 169/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3275 - val_loss: 6544495.6895\n    Epoch 170/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3415 - val_loss: 6544495.7162\n    Epoch 171/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3443 - val_loss: 6544495.6895\n    Epoch 172/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3103 - val_loss: 6544495.7162\n    Epoch 173/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3602 - val_loss: 6544495.6895\n\n\uff08 \u4e2d\u7565 \uff09\n\n\n    Epoch 668/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3229 - val_loss: 6544495.6761\n    Epoch 669/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3171 - val_loss: 6544495.6761\n    Epoch 670/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3127 - val_loss: 6544495.6761\n    Epoch 671/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3294 - val_loss: 6544495.6761\n    Epoch 672/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.2898 - val_loss: 6544495.6761\n    Epoch 673/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3645 - val_loss: 6544495.6761\n    Epoch 674/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3316 - val_loss: 6544495.6761\n    Epoch 675/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3396 - val_loss: 6544495.6761\n    Epoch 676/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3383 - val_loss: 6544495.6761\n    Epoch 677/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3346 - val_loss: 6544495.6761\n    Epoch 678/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3231 - val_loss: 6544495.6761\n    Epoch 679/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3384 - val_loss: 6544495.6761\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 996/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3163 - val_loss: 6544495.6761\n    Epoch 997/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3091 - val_loss: 6544495.6761\n    Epoch 998/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3337 - val_loss: 6544495.6761\n    Epoch 999/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3074 - val_loss: 6544495.6761\n    Epoch 1000/1000\n    5383/5383 [==============================] - 1s - loss: 6762488.3032 - val_loss: 6544495.6761\n\n###__\uff11epoch\u76ee\u306eloss\u6841\u6570 \u304c \u5927\u304d\u304f\u30012poch\u4ee5\u964d\u3001\u4e00\u5411\u306b\u6e1b\u3089\u306a\u3044__\n\n```{python:python3 jupyter notebook}\nloss = three_patterns_ae_model_hist.history['loss']\nval_loss = three_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_85_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/2cf3fe99-8dbf-34f9-4026-aede19e50c61.png)\n\n\n\n\n```{python:python3 jupyter notebook}\npredicted_value_list = autoencoder.predict(three_patterns_data[-10:])\n```\n\n\n```{python:python3 jupyter notebook}\nlen(predicted_value_list)\n```\n\n\n\n\n    10\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(three_patterns_data[-10], predicted_value_list[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x12eff9ef0>]\n\n\n\n\n![output_88_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/e64993c6-6771-53b3-bc02-556e6ac956ea.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(three_patterns_data[-10], predicted_value_list[0])[0][1]\n```\n\n\n\n\n    nan\n\n\n\n\n```{python:python3 jupyter notebook}\nprint(three_patterns_data[-10])\n```\n\n    [-7017.66048331 -7046.42995364 -7043.87903589 -7033.23990255 -7009.52368735\n     -6996.97377304 -6985.96200355 -6960.01459759 -6963.82866094 -6950.98907808\n     -6964.30881299 -6983.79644211 -6960.09471708 -6958.23402409 -6966.17581173\n     -6968.1895467  -6959.86238021 -6993.08741239 -6993.31397102 -6980.00869287\n     -6968.38319723 -6944.49216612 -6910.21318362 -6861.2121015  -6877.91296188\n     -6896.34493134 -6894.97624443 -6871.04568337 -6866.45038733 -6860.04245111\n     -6843.32247391 -6827.58851595 -6816.65713122 -6774.79431096 -6766.78811715\n     -6799.50464858 -6809.11836503 -6824.48740609 -6818.03744908 -6825.63135168\n     -6807.72369516 -6801.09578247 -6801.60608541 -6808.32067788 -6820.98796303\n     -6827.79351801 -6852.98165785 -6841.154643   -6851.66623672 -6866.94962678\n     -6849.72897592 -6823.86860429 -6821.80858664 -6822.50489885 -6833.57137963\n     -6838.93244414 -6849.26406901 -6832.61767028 -6854.64057913 -6832.13625882\n     -6848.82611789 -6808.18966364 -6810.72885219]\n\n\n\n```{python:python3 jupyter notebook}\nlen(three_patterns_data[-10])\n```\n\n\n\n\n    63\n\n\n\n\n```{python:python3 jupyter notebook}\nprint(predicted_value_list[0])\n```\n\n    [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\n\n```{python:python3 jupyter notebook}\nlen(predicted_value_list[0])\n```\n\n\n\n\n    63\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(three_patterns_data[-9], predicted_value_list[1])[0][1]\n```\n\n\n\n\n    nan\n\n\n\n\n```{python:python3 jupyter notebook}\nprint(predicted_value_list[1])\n```\n\n    [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\n\n```{python:python3 jupyter notebook}\nprint(predicted_value_list[2])\n```\n\n    [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n\n\n```{python:python3 jupyter notebook}\nprint(predicted_value_list[3])\n```\n\n    [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n      0.  0.  0.  0.  0.  0.  0.  0.  0.]\n\n###__\u518d\u30c1\u30e3\u30ec\u30f3\u30b8__\n\n\n```{python:python3 jupyter notebook}\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n```\n\n\n```{python:python3 jupyter notebook}\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n```\n\n\n```{python:python3 jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\n#3\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u6df7\u3058\u3063\u3066\u3044\u308b\u304b\u3089\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306f\u3057\u306a\u3044\u65b9\u304c\u826f\u3044\u304b\u3082\u3057\u308c\u306a\u3044\nthree_patterns_ae_model_hist = autoencoder.fit(three_patterns_data, three_patterns_data,\n                                               nb_epoch=400,\n                                               shuffle=True,\n                                               validation_split=0.1)\n```\n\n    Train on 5383 samples, validate on 599 samples\n    Epoch 1/400\n    5383/5383 [==============================] - 0s - loss: 6763562.1664 - val_loss: 6545522.4015\n    Epoch 2/400\n    5383/5383 [==============================] - 0s - loss: 6763541.9822 - val_loss: 6545522.3481\n    Epoch 3/400\n    5383/5383 [==============================] - 0s - loss: 6763546.5973 - val_loss: 6545522.3080\n    Epoch 4/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4360 - val_loss: 6545522.3080\n    Epoch 5/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4405 - val_loss: 6545522.3080\n    Epoch 6/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4593 - val_loss: 6545522.2813\n    Epoch 7/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4467 - val_loss: 6545522.2813\n    Epoch 8/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4722 - val_loss: 6545522.2813\n    Epoch 9/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4360 - val_loss: 6545522.2813\n    Epoch 10/400\n    5383/5383 [==============================] - 0s - loss: 6763548.4119 - val_loss: 6545522.2546\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 126/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8154 - val_loss: 6544782.6194\n    Epoch 127/400\n    5383/5383 [==============================] - 0s - loss: 6762800.1371 - val_loss: 6544782.6194\n    Epoch 128/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8087 - val_loss: 6544782.6194\n    Epoch 129/400\n    5383/5383 [==============================] - 0s - loss: 6762790.7923 - val_loss: 6544782.6194\n    Epoch 130/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8203 - val_loss: 6544782.6194\n    Epoch 131/400\n    5383/5383 [==============================] - 0s - loss: 6762790.7871 - val_loss: 6544782.6194\n    Epoch 132/400\n    5383/5383 [==============================] - 0s - loss: 6762790.7642 - val_loss: 6544782.6194\n    Epoch 133/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8106 - val_loss: 6544782.6194\n    Epoch 134/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8873 - val_loss: 6544782.6194\n    Epoch 135/400\n    5383/5383 [==============================] - 0s - loss: 6762790.8144 - val_loss: 6544782.6194\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 345/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9956 - val_loss: 6544751.5058\n    Epoch 346/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0084 - val_loss: 6544751.5058\n    Epoch 347/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0507 - val_loss: 6544751.5058\n    Epoch 348/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9770 - val_loss: 6544751.5058\n    Epoch 349/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0153 - val_loss: 6544751.5058\n    Epoch 350/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9812 - val_loss: 6544751.5058\n    Epoch 351/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0110 - val_loss: 6544751.5058\n    Epoch 352/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9616 - val_loss: 6544751.5058\n    Epoch 353/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0275 - val_loss: 6544751.5058\n    Epoch 354/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0027 - val_loss: 6544751.5058\n    Epoch 355/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0140 - val_loss: 6544751.5058\n    Epoch 356/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9615 - val_loss: 6544751.5058\n    Epoch 357/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9666 - val_loss: 6544751.5058\n    Epoch 358/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0127 - val_loss: 6544751.5058\n    Epoch 359/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9551 - val_loss: 6544751.5058\n    Epoch 360/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0318 - val_loss: 6544751.5058\n    Epoch 361/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9853 - val_loss: 6544751.5058\n    Epoch 362/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0294 - val_loss: 6544751.5058\n    Epoch 363/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0149 - val_loss: 6544751.5058\n    Epoch 364/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0251 - val_loss: 6544751.5058\n    Epoch 365/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9747 - val_loss: 6544751.5058\n    Epoch 366/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0271 - val_loss: 6544751.5058\n    Epoch 367/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0308 - val_loss: 6544751.5058\n    Epoch 368/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0166 - val_loss: 6544751.5058\n    Epoch 369/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0049 - val_loss: 6544751.5058\n    Epoch 370/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9755 - val_loss: 6544751.5058\n    Epoch 371/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0246 - val_loss: 6544751.5058\n    Epoch 372/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9926 - val_loss: 6544751.5058\n    Epoch 373/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9992 - val_loss: 6544751.5058\n    Epoch 374/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0204 - val_loss: 6544751.5058\n    Epoch 375/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0173 - val_loss: 6544751.5058\n    Epoch 376/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0221 - val_loss: 6544751.5058\n    Epoch 377/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9886 - val_loss: 6544751.5058\n    Epoch 378/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9834 - val_loss: 6544751.5058\n    Epoch 379/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9874 - val_loss: 6544751.5058\n    Epoch 380/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0348 - val_loss: 6544751.5058\n    Epoch 381/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0156 - val_loss: 6544751.5058\n    Epoch 382/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9957 - val_loss: 6544751.5058\n    Epoch 383/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0182 - val_loss: 6544751.5058\n    Epoch 384/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9866 - val_loss: 6544751.5058\n    Epoch 385/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9694 - val_loss: 6544751.5058\n    Epoch 386/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9553 - val_loss: 6544751.5058\n    Epoch 387/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0147 - val_loss: 6544751.5058\n    Epoch 388/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0150 - val_loss: 6544751.5058\n    Epoch 389/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9939 - val_loss: 6544751.5058\n    Epoch 390/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0224 - val_loss: 6544751.5058\n    Epoch 391/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9539 - val_loss: 6544751.5058\n    Epoch 392/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9824 - val_loss: 6544751.5058\n    Epoch 393/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9837 - val_loss: 6544751.5058\n    Epoch 394/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0260 - val_loss: 6544751.5058\n    Epoch 395/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0066 - val_loss: 6544751.5058\n    Epoch 396/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9615 - val_loss: 6544751.5058\n    Epoch 397/400\n    5383/5383 [==============================] - 0s - loss: 6762758.9870 - val_loss: 6544751.5058\n    Epoch 398/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0247 - val_loss: 6544751.5058\n    Epoch 399/400\n    5383/5383 [==============================] - 0s - loss: 6762759.0080 - val_loss: 6544751.5058\n    Epoch 400/400\n    5383/5383 [==============================] - 1s - loss: 6762758.9872 - val_loss: 6544751.5058\n\n\n###__norm_dist_cumsum_array \uff11\u30d1\u30bf\u30fc\u30f3 \u3092 \u5b66\u3070\u305b\u305f\u5834\u5408 \u306e \u518d\u73fe\u6210\u529f\u5ea6\u5408\u3044 \u3092 \u78ba\u8a8d\u3059\u308b__\n\n```{python:python3 jupyter notebook}\n# norm_dist_cumsum_array \uff11\u30d1\u30bf\u30fc\u30f3\u3060\u3051\u3092\u5b66\u3070\u305b\u3066\u3001\n# \u51fa\u529b\u5024\u304c\u3001\u5165\u529b\u5024\u3092\u3069\u308c\u3060\u3051\u518d\u73fe\u3067\u304d\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u3002\nprint(norm_dist_cumsum_array.shape)\n```\n\n    (1994, 63)\n\n\n\n```{python:python3 jupyter notebook}\ntmp_df = pd.DataFrame(norm_dist_cumsum_array)\ntmp_df.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12f7682b0>\n\n\n\n\n![output_104_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/d3159dd3-314e-0308-20c6-986c6e1374ec.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nencoding_dim = 20\ninput = Input(shape=(63,))\nencoded = Dense(encoding_dim, activation='relu')(input)\ndecoded = Dense(63, activation='sigmoid')(encoded)\nautoencoder = Model(input=input, output=decoded)\n\nfrom keras.layers import Activation, Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nautoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n```\n\n\n```{python:python3 jupyter notebook}\nnorm_dist_cumsum_pattern_ae_model_hist = autoencoder.fit(norm_dist_cumsum_array, norm_dist_cumsum_array,\n                                                         nb_epoch=200,\n                                                         batch_size=20,\n                                                         shuffle=True,\n                                                         validation_split=0.1)\n```\n\n    Train on 1794 samples, validate on 200 samples\n    Epoch 1/200\n    1794/1794 [==============================] - 1s - loss: 21329475.6901 - val_loss: 10176569.8219\n    Epoch 2/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4192 - val_loss: 10176569.8219\n    Epoch 3/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7269 - val_loss: 10176569.8219\n    Epoch 4/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5708 - val_loss: 10176569.8219\n    Epoch 5/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7804 - val_loss: 10176569.8219\n    Epoch 6/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5139 - val_loss: 10176569.8219\n    Epoch 7/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\n    Epoch 8/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4682 - val_loss: 10176569.8219\n    Epoch 9/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5463 - val_loss: 10176569.8219\n    Epoch 10/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5641 - val_loss: 10176569.8219\n    Epoch 11/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7135 - val_loss: 10176569.8219\n    Epoch 12/200\n    1794/1794 [==============================] - 0s - loss: 21329452.3356 - val_loss: 10176569.8219\n    Epoch 13/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6711 - val_loss: 10176569.8219\n    Epoch 14/200\n    1794/1794 [==============================] - 0s - loss: 21329452.9097 - val_loss: 10176569.8219\n    Epoch 15/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7625 - val_loss: 10176569.8219\n\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 63/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5686 - val_loss: 10176569.8219\n    Epoch 64/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4682 - val_loss: 10176569.8219\n    Epoch 65/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6533 - val_loss: 10176569.8219\n    Epoch 66/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\n    Epoch 67/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5195 - val_loss: 10176569.8219\n    Epoch 68/200\n    1794/1794 [==============================] - 0s - loss: 21329452.9119 - val_loss: 10176569.8219\n    Epoch 69/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5775 - val_loss: 10176569.8219\n    Epoch 70/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5329 - val_loss: 10176569.8219\n    Epoch 71/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7525 - val_loss: 10176569.8219\n    Epoch 72/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7625 - val_loss: 10176569.8219\n\n\uff08 \u4e2d\u7565 \uff09\n\n    Epoch 183/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7781 - val_loss: 10176569.8219\n    Epoch 184/200\n    1794/1794 [==============================] - 0s - loss: 21329452.5095 - val_loss: 10176569.8219\n    Epoch 185/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6823 - val_loss: 10176569.8219\n    Epoch 186/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6109 - val_loss: 10176569.8219\n    Epoch 187/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7068 - val_loss: 10176569.8219\n    Epoch 188/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4526 - val_loss: 10176569.8219\n    Epoch 189/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7536 - val_loss: 10176569.8219\n    Epoch 190/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6288 - val_loss: 10176569.8219\n    Epoch 191/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7269 - val_loss: 10176569.8219\n    Epoch 192/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6488 - val_loss: 10176569.8219\n    Epoch 193/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6912 - val_loss: 10176569.8219\n    Epoch 194/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4437 - val_loss: 10176569.8219\n    Epoch 195/200\n    1794/1794 [==============================] - 0s - loss: 21329452.8829 - val_loss: 10176569.8219\n    Epoch 196/200\n    1794/1794 [==============================] - 0s - loss: 21329452.4225 - val_loss: 10176569.8219\n    Epoch 197/200\n    1794/1794 [==============================] - 0s - loss: 21329452.7915 - val_loss: 10176569.8219\n    Epoch 198/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6488 - val_loss: 10176569.8219\n    Epoch 199/200\n    1794/1794 [==============================] - 0s - loss: 21329452.6722 - val_loss: 10176569.8219\n    Epoch 200/200\n    1794/1794 [==============================] - 0s - loss: 21329452.8305 - val_loss: 10176569.8219\n\n\n\n```{python:python3 jupyter notebook}\nloss = norm_dist_cumsum_pattern_ae_model_hist.history['loss']\nval_loss = norm_dist_cumsum_pattern_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n###__\u30c0\u30e1__\n\n![output_107_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/8fbf5204-e730-9c5c-79cf-bfcbaa20b062.png)\n\n\n\u4e71\u6570 \u3092 \u7d2f\u7a4d\u548c\u84c4\u7a4d \u3057\u3066\u3044\u304f randon.cumsum()\u7cfb\u5217 \u306f\u3001\n\u30d1\u30bf\u30fc\u30f3\u305d\u306e\u3082\u306e \u304c \u8907\u96d1\uff08\u5468\u671f\u6027\u304c\u306a\u3044\uff1f\uff09\u305f\u3081\u3001\n\u518d\u73fe \u306f \u56f0\u96e3 \u3068 \u8003\u3048\u3089\u308c\u308b\u3002\n\n###__sin\u66f2\u7dda \u306b \u4e71\u6570\u3092\u52a0\u7b97 \u3057\u3066 \u63fa\u52d5\u3055\u305b\u305f ys_2 \u3068 ys_3 \u3092 \u8a66\u3057\u3066\u307f\u308b\u3002__\n\n\n```{python:python3 jupyter notebook}\n# \u3084\u306f\u308a\u3001\u4e71\u6570\u751f\u6210\u5668\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u4e71\u6570\u3092\u7d2f\u7a4d\u548c\u84c4\u7a4d\u3057\u3066\u3044\u304frandon.cumsum()\u7cfb\u5217\u306f\u3001\n# \u30d1\u30bf\u30fc\u30f3\u305d\u306e\u3082\u306e\u304c\u8907\u96d1\uff08\u5468\u671f\u6027\u304c\u306a\u3044\uff1f\uff09\u3067\u3001\u518d\u73fe\u306f\u56f0\u96e3\u3068\u8003\u3048\u3089\u308c\u308b\u3002\n\n# sin\u66f2\u7dda\u306b\u4e71\u6570\u3092\u52a0\u7b97\u3057\u3066\u63fa\u52d5\u3055\u305b\u305f ys_2 \u3068 ys_3 \u3092 \u8a66\u3057\u3066\u307f\u308b\u3002\n\ntype(ys_2)\n```\n\n\n\n\n    numpy.ndarray\n\n\n\n\n```{python:python3 jupyter notebook}\nprint(ys_2.shape, ys_3.shape)\n```\n\n    (125622,) (125622,)\n\n\n\n```{python:python3 jupyter notebook}\nys_2 = ys_2.reshape(1994, 63)\nys_3 = ys_3.reshape(1994, 63)\n```\n\n\n```{python:python3 jupyter notebook}\nprint(ys_2.shape, ys_3.shape)\n```\n\n    (1994, 63) (1994, 63)\n\n\n###__2\u30d1\u30bf\u30fc\u30f3 \u307e\u3068\u3081\u3066\u5b66\u7fd2\u3055\u305b\u308b__\n\n```{python:python3 jupyter notebook}\n# 2\u30d1\u30bf\u30fc\u30f3\u3001\u307e\u3068\u3081\u3066\u5b66\u7fd2\u3055\u305b\u308b\nsin_with_random_2_patterns = np.vstack((ys_2, ys_3))\nsin_with_random_2_patterns = np.random.permutation(sin_with_random_2_patterns)\n\nprint(sin_with_random_2_patterns.shape)\n```\n\n    (3988, 63)\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(sin_with_random_2_patterns[0])\nplt.show()\n```\n\n\n![output_113_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/28b959f3-7935-fbfb-0c9c-0fa5f7ed21a6.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nplt.plot(sin_with_random_2_patterns[1])\nplt.show()\n```\n\n\n![output_114_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/704b81f5-585c-bfef-948d-054d6fc86a1c.png)\n\n\n\n\n```{python:python3 jupyter notebook}\nsin_with_random_2_patterns_ae_model_hist = autoencoder.fit(sin_with_random_2_patterns, sin_with_random_2_patterns,\n                                                           nb_epoch=100,\n                                                           batch_size=20,\n                                                           shuffle=True,\n                                                           validation_split=0.1)\n```\n\n    Train on 3589 samples, validate on 399 samples\n    Epoch 1/100\n    3589/3589 [==============================] - 0s - loss: 2.5481 - val_loss: 2.5165\n    Epoch 2/100\n    3589/3589 [==============================] - 0s - loss: 2.5175 - val_loss: 2.5161\n    Epoch 3/100\n    3589/3589 [==============================] - 0s - loss: 2.5151 - val_loss: 2.5164\n    Epoch 4/100\n    3589/3589 [==============================] - 0s - loss: 2.5095 - val_loss: 2.5018\n    Epoch 5/100\n    3589/3589 [==============================] - 0s - loss: 2.4943 - val_loss: 2.4905\n    Epoch 6/100\n    3589/3589 [==============================] - 0s - loss: 2.4814 - val_loss: 2.4517\n    Epoch 7/100\n    3589/3589 [==============================] - 0s - loss: 2.4565 - val_loss: 2.4501\n    Epoch 8/100\n    3589/3589 [==============================] - 0s - loss: 2.4539 - val_loss: 2.4522\n    Epoch 9/100\n    3589/3589 [==============================] - 0s - loss: 2.4532 - val_loss: 2.4526\n    Epoch 10/100\n    3589/3589 [==============================] - 0s - loss: 2.4485 - val_loss: 2.4349\n    Epoch 11/100\n    3589/3589 [==============================] - 0s - loss: 2.4187 - val_loss: 2.3953\n    Epoch 12/100\n    3589/3589 [==============================] - 0s - loss: 2.3865 - val_loss: 2.3856\n    Epoch 13/100\n    3589/3589 [==============================] - 0s - loss: 2.3800 - val_loss: 2.3751\n    Epoch 14/100\n    3589/3589 [==============================] - 0s - loss: 2.3802 - val_loss: 2.3817\n    Epoch 15/100\n    3589/3589 [==============================] - 0s - loss: 2.3595 - val_loss: 2.3608\n    Epoch 16/100\n    3589/3589 [==============================] - 0s - loss: 2.3580 - val_loss: 2.3501\n    Epoch 17/100\n    3589/3589 [==============================] - 0s - loss: 2.3382 - val_loss: 2.3307\n    Epoch 18/100\n    3589/3589 [==============================] - 0s - loss: 2.3124 - val_loss: 2.2932\n    Epoch 19/100\n    3589/3589 [==============================] - 0s - loss: 2.2824 - val_loss: 2.2791\n    Epoch 20/100\n    3589/3589 [==============================] - 0s - loss: 2.2771 - val_loss: 2.2796\n    Epoch 21/100\n    3589/3589 [==============================] - 0s - loss: 2.2774 - val_loss: 2.2696\n    Epoch 22/100\n    3589/3589 [==============================] - 0s - loss: 2.2633 - val_loss: 2.2671\n    Epoch 23/100\n    3589/3589 [==============================] - 0s - loss: 2.2619 - val_loss: 2.2631\n    Epoch 24/100\n    3589/3589 [==============================] - 0s - loss: 2.2496 - val_loss: 2.2400\n    Epoch 25/100\n    3589/3589 [==============================] - 0s - loss: 2.2389 - val_loss: 2.2419\n    Epoch 26/100\n    3589/3589 [==============================] - 0s - loss: 2.2317 - val_loss: 2.2344\n    Epoch 27/100\n    3589/3589 [==============================] - 0s - loss: 2.2244 - val_loss: 2.2221\n    Epoch 28/100\n    3589/3589 [==============================] - 0s - loss: 2.2246 - val_loss: 2.2215\n    Epoch 29/100\n    3589/3589 [==============================] - 0s - loss: 2.2224 - val_loss: 2.2188\n    Epoch 30/100\n    3589/3589 [==============================] - 0s - loss: 2.2205 - val_loss: 2.2189\n    Epoch 31/100\n    3589/3589 [==============================] - 0s - loss: 2.2187 - val_loss: 2.2243\n    Epoch 32/100\n    3589/3589 [==============================] - 0s - loss: 2.2181 - val_loss: 2.2170\n    Epoch 33/100\n    3589/3589 [==============================] - 0s - loss: 2.2157 - val_loss: 2.2154\n    Epoch 34/100\n    3589/3589 [==============================] - 0s - loss: 2.2112 - val_loss: 2.2152\n    Epoch 35/100\n    3589/3589 [==============================] - 0s - loss: 2.2083 - val_loss: 2.2103\n    Epoch 36/100\n    3589/3589 [==============================] - 0s - loss: 2.2078 - val_loss: 2.2164\n    Epoch 37/100\n    3589/3589 [==============================] - 0s - loss: 2.1939 - val_loss: 2.1926\n    Epoch 38/100\n    3589/3589 [==============================] - 0s - loss: 2.1859 - val_loss: 2.1970\n    Epoch 39/100\n    3589/3589 [==============================] - 0s - loss: 2.1686 - val_loss: 2.1764\n    Epoch 40/100\n    3589/3589 [==============================] - 0s - loss: 2.1648 - val_loss: 2.1701\n    Epoch 41/100\n    3589/3589 [==============================] - 0s - loss: 2.1626 - val_loss: 2.1783\n    Epoch 42/100\n    3589/3589 [==============================] - 0s - loss: 2.1650 - val_loss: 2.1689\n    Epoch 43/100\n    3589/3589 [==============================] - 0s - loss: 2.1595 - val_loss: 2.1591\n    Epoch 44/100\n    3589/3589 [==============================] - 0s - loss: 2.1561 - val_loss: 2.1557\n    Epoch 45/100\n    3589/3589 [==============================] - 0s - loss: 2.1545 - val_loss: 2.1630\n    Epoch 46/100\n    3589/3589 [==============================] - 0s - loss: 2.1547 - val_loss: 2.1609\n    Epoch 47/100\n    3589/3589 [==============================] - 0s - loss: 2.1507 - val_loss: 2.1448\n    Epoch 48/100\n    3589/3589 [==============================] - 0s - loss: 2.1462 - val_loss: 2.1548\n    Epoch 49/100\n    3589/3589 [==============================] - 0s - loss: 2.1486 - val_loss: 2.1653\n    Epoch 50/100\n    3589/3589 [==============================] - 0s - loss: 2.1482 - val_loss: 2.1490\n    Epoch 51/100\n    3589/3589 [==============================] - 0s - loss: 2.1448 - val_loss: 2.1523\n    Epoch 52/100\n    3589/3589 [==============================] - 0s - loss: 2.1352 - val_loss: 2.1405\n    Epoch 53/100\n    3589/3589 [==============================] - 0s - loss: 2.1234 - val_loss: 2.1424\n    Epoch 54/100\n    3589/3589 [==============================] - 0s - loss: 2.1211 - val_loss: 2.1334\n    Epoch 55/100\n    3589/3589 [==============================] - 0s - loss: 2.1219 - val_loss: 2.1287\n    Epoch 56/100\n    3589/3589 [==============================] - 0s - loss: 2.1172 - val_loss: 2.1276\n    Epoch 57/100\n    3589/3589 [==============================] - 0s - loss: 2.1177 - val_loss: 2.1273\n    Epoch 58/100\n    3589/3589 [==============================] - 0s - loss: 2.1154 - val_loss: 2.1308\n    Epoch 59/100\n    3589/3589 [==============================] - 0s - loss: 2.1147 - val_loss: 2.1303\n    Epoch 60/100\n    3589/3589 [==============================] - 0s - loss: 2.1118 - val_loss: 2.1310\n    Epoch 61/100\n    3589/3589 [==============================] - 0s - loss: 2.1080 - val_loss: 2.1213\n    Epoch 62/100\n    3589/3589 [==============================] - 0s - loss: 2.1099 - val_loss: 2.1149\n    Epoch 63/100\n    3589/3589 [==============================] - 0s - loss: 2.1087 - val_loss: 2.1073\n    Epoch 64/100\n    3589/3589 [==============================] - 0s - loss: 2.1045 - val_loss: 2.1092\n    Epoch 65/100\n    3589/3589 [==============================] - 0s - loss: 2.0916 - val_loss: 2.1049\n    Epoch 66/100\n    3589/3589 [==============================] - 0s - loss: 2.0863 - val_loss: 2.0918\n    Epoch 67/100\n    3589/3589 [==============================] - 0s - loss: 2.0820 - val_loss: 2.0898\n    Epoch 68/100\n    3589/3589 [==============================] - 0s - loss: 2.0832 - val_loss: 2.1033\n    Epoch 69/100\n    3589/3589 [==============================] - 0s - loss: 2.0837 - val_loss: 2.0878\n    Epoch 70/100\n    3589/3589 [==============================] - 0s - loss: 2.0779 - val_loss: 2.0896\n    Epoch 71/100\n    3589/3589 [==============================] - 0s - loss: 2.0773 - val_loss: 2.0833\n    Epoch 72/100\n    3589/3589 [==============================] - 0s - loss: 2.0762 - val_loss: 2.0847\n    Epoch 73/100\n    3589/3589 [==============================] - 0s - loss: 2.0750 - val_loss: 2.0855\n    Epoch 74/100\n    3589/3589 [==============================] - 0s - loss: 2.0741 - val_loss: 2.0878\n    Epoch 75/100\n    3589/3589 [==============================] - 0s - loss: 2.0733 - val_loss: 2.0861\n    Epoch 76/100\n    3589/3589 [==============================] - 0s - loss: 2.0593 - val_loss: 2.0759\n    Epoch 77/100\n    3589/3589 [==============================] - 0s - loss: 2.0559 - val_loss: 2.0714\n    Epoch 78/100\n    3589/3589 [==============================] - 0s - loss: 2.0566 - val_loss: 2.0640\n    Epoch 79/100\n    3589/3589 [==============================] - 0s - loss: 2.0493 - val_loss: 2.0599\n    Epoch 80/100\n    3589/3589 [==============================] - 0s - loss: 2.0493 - val_loss: 2.0617\n    Epoch 81/100\n    3589/3589 [==============================] - 0s - loss: 2.0455 - val_loss: 2.0571\n    Epoch 82/100\n    3589/3589 [==============================] - 0s - loss: 2.0442 - val_loss: 2.0643\n    Epoch 83/100\n    3589/3589 [==============================] - 0s - loss: 2.0460 - val_loss: 2.0616\n    Epoch 84/100\n    3589/3589 [==============================] - 0s - loss: 2.0443 - val_loss: 2.0526\n    Epoch 85/100\n    3589/3589 [==============================] - 0s - loss: 2.0402 - val_loss: 2.0643\n    Epoch 86/100\n    3589/3589 [==============================] - 0s - loss: 2.0428 - val_loss: 2.0521\n    Epoch 87/100\n    3589/3589 [==============================] - 0s - loss: 2.0391 - val_loss: 2.0514\n    Epoch 88/100\n    3589/3589 [==============================] - 0s - loss: 2.0355 - val_loss: 2.0407\n    Epoch 89/100\n    3589/3589 [==============================] - 0s - loss: 2.0348 - val_loss: 2.0430\n    Epoch 90/100\n    3589/3589 [==============================] - 0s - loss: 2.0340 - val_loss: 2.0431\n    Epoch 91/100\n    3589/3589 [==============================] - 0s - loss: 2.0332 - val_loss: 2.0532\n    Epoch 92/100\n    3589/3589 [==============================] - 0s - loss: 2.0308 - val_loss: 2.0444\n    Epoch 93/100\n    3589/3589 [==============================] - 0s - loss: 2.0281 - val_loss: 2.0433\n    Epoch 94/100\n    3589/3589 [==============================] - 0s - loss: 2.0299 - val_loss: 2.0478\n    Epoch 95/100\n    3589/3589 [==============================] - 0s - loss: 2.0314 - val_loss: 2.0382\n    Epoch 96/100\n    3589/3589 [==============================] - 0s - loss: 2.0297 - val_loss: 2.0387\n    Epoch 97/100\n    3589/3589 [==============================] - 0s - loss: 2.0077 - val_loss: 2.0191\n    Epoch 98/100\n    3589/3589 [==============================] - 0s - loss: 2.0088 - val_loss: 2.0373\n    Epoch 99/100\n    3589/3589 [==============================] - 0s - loss: 2.0068 - val_loss: 2.0170\n    Epoch 100/100\n    3589/3589 [==============================] - 0s - loss: 2.0052 - val_loss: 2.0212\n\n\n###__loss \u306f 1 epoch\u76ee \u304b\u3089\u30012 \u306b \u6e1b\u3063\u305f\u3002__\n\n####__\u3057\u304b\u3057\u3001\u4e71\u6570\u304c\u52a0\u7b97\u3055\u308c\u3066\u3044\u306a\u3044\u3001\u300csin\u66f2\u7dda\u3060\u3051\u300d \u306e \u6642 \u306b \u6bd4\u3079\u308b \u3068 loss \u306f \u306a\u304a \u5927\u304d\u3044__\n\n```{python:python3 jupyter notebook}\n# loss\u306f\u6700\u521d\u304b\u3089\u30012\u306b\u307e\u3067\u6e1b\u3063\u305f\u3002\n# \u3057\u304b\u3057\u3001\u4e71\u6570\u304c\u52a0\u7b97\u3055\u308c\u3066\u3044\u306a\u3044\u3001\u300csin\u66f2\u7dda\u3060\u3051\u300d \u306e \u6642 \u306b \u6bd4\u3079\u308b \u3068 loss \u306f \u306a\u304a\u5927\u304d\u3044\n\nloss = sin_with_random_2_patterns_ae_model_hist.history['loss']\nval_loss = sin_with_random_2_patterns_ae_model_hist.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_116_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/d0ba641d-f5f0-3e9d-735d-b902779156ae.png)\n\n####__\u5165\u529b\u5024 \u3068 \u51fa\u529b\u5024 \u306e \u76f8\u95a2\u4fc2\u6570 \u306f \u4f4e\u3044\uff08\u518d\u73fe\u3067\u304d\u3066\u3044\u306a\u3044\uff09__\n\n```{python:python3 jupyter notebook}\npredicted_value_list = autoencoder.predict(sin_with_random_2_patterns[-10:])\n```\n\n\n```{python:python3 jupyter notebook}\nlen(predicted_value_list)\n```\n\n\n\n\n    10\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-10], predicted_value_list[0])[0][1]\n#np.corrcoef(three_patterns_data[-10], predicted_value_list[0])[0][1]\n```\n\n\n\n\n    0.32999374633450923\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-9], predicted_value_list[1])[0][1]\n```\n\n\n\n\n    -0.14401764907318854\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-8], predicted_value_list[2])[0][1]\n```\n\n\n\n\n    0.37402418180795188\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-7], predicted_value_list[3])[0][1]\n```\n\n\n\n\n    -0.056611938444256202\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-6], predicted_value_list[4])[0][1]\n```\n\n\n\n\n    -0.10400593295651984\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-5], predicted_value_list[5])[0][1]\n```\n\n\n\n\n    0.45525679191488855\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-4], predicted_value_list[6])[0][1]\n```\n\n\n\n\n    0.35503679442701519\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-3], predicted_value_list[7])[0][1]\n```\n\n\n\n\n    0.085707911982792429\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-2], predicted_value_list[8])[0][1]\n```\n\n\n\n\n    0.38548401081843031\n\n\n\n\n```{python:python3 jupyter notebook}\nnp.corrcoef(sin_with_random_2_patterns[-1], predicted_value_list[9])[0][1]\n```\n\n\n\n\n    -0.0098453563954306513\n\n\n\n\n```{python:python3 jupyter notebook}\n# \u51fa\u529b\u5024\u306f\u3001\u5165\u529b\u5024\u3092\u518d\u73fe\u3067\u304d\u3066\u3044\u306a\u3044\n```\n", "tags": ["Keras", "Autoencoder", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "\u4eba\u5de5\u77e5\u80fd"]}