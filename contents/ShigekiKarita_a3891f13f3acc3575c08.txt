{"tags": ["CUDA8.0", "C++11"], "context": "\n\n\u306f\u3058\u3081\u306b\nCUDA8 \u306b\u306a\u308a\u6f38\u304f C++11 \u5bfe\u5fdc\u304c\u5165\u3063\u305f\u306e\u3067\u3001C\u8a00\u8a9e\u3068\u5171\u7528\u306epthread\u3084cudaStream\u3067\u306f\u306a\u304f\u76f8\u5f53\u3059\u308bC++11\u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3044\u305f\u3044\u3068\u3044\u3046\u306e\u304c\u3001\u3053\u306e\u8a18\u4e8b\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3067\u3059\u3002\u4eca\u56de\u306fCUDA7RC\u306e\u3068\u304d\u306e\u516c\u5f0f\u30d6\u30ed\u30b0\u8a18\u4e8b\u3092\u30d9\u30fc\u30b9\u306b\u3084\u3063\u3066\u307f\u307e\u3059\u3002\n$ nvcc ./stream_test.cu -o ./stream_legacy\n$ nvvp ./stream_legacy\n\n\u4ee5\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3067\u3001nvcc\u3067\u5b9f\u884c\u30d5\u30a1\u30a4\u30eb\u306b\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u3066\u3001Eclipse\u3063\u307d\u3044UI\u306eNVVP\u3067\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3067\u304d\u307e\u3059\u3002\u521d\u3081\u3066NVVP\u3092\u4f7f\u3063\u305f\u306e\u3067\u3001\u6700\u521d\u306e\u8a2d\u5b9a\u3067Arguments: Profile all processes \u3068\u306a\u3063\u3066\u3044\u308c\u3070 Profile child processes \u306b\u5909\u66f4\u3057\u306a\u3044\u3068\u3001\u3044\u3064\u307e\u3067\u3082\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u7d42\u308f\u3089\u306a\u3044\u306e\u3067\u30cf\u30de\u308a\u307e\u3057\u305f\u3002\u4e0b\u306e settings \u304b\u3089\u8a2d\u5b9a\u76f4\u3057\u3066\u4e0a\u306e Run > Generate Timeline \u3067\u52d5\u304b\u3057\u307e\u3059\u3002\n\u8a73\u7d30\u306f\u98db\u3070\u3057\u3066\u3001\u96d1\u306b\u516c\u5f0f\u30d6\u30ed\u30b0\u306e\u8a71\u3092\u307e\u3068\u3081\u308b\u3068\n\ncudaStream_t \u3084 pthread_t \u3092\u4f7f\u3063\u3066\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u3001\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\u3092\u3059\u308b\u3068\u304d\u306fnvcc \u306e\u30aa\u30d7\u30b7\u30e7\u30f3 --default-stream per-thread \u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\npthread_t\u3092\u4f7f\u3046\u3068\u304d\u306f\u30b9\u30ec\u30c3\u30c9\u6bce\u306b\u30e1\u30e2\u30ea\u78ba\u4fdd\u3084\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u3066\u3001cudaStreamSynchronize(cudaStream_t stream)\u3059\u308b\u95a2\u6570\u3092pthread_create\u306b\u6e21\u3059\n\n\u4ee5\u4e0a\u304c\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u30fb\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\u306e\u304a\u7d04\u675f\u306e\u3088\u3046\u3067\u3059\u3002\u305d\u308c\u3067\u306f\u3001C++11\u306e <future> \u3084 <thread> \u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3063\u3066\u66f8\u304d\u63db\u3048\u3066\u307f\u307e\u3059\n\nC++11 future \u3067\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u51e6\u7406\n\u52c9\u5f37\u4e0d\u8db3\u306a\u306e\u3067\u3001\u5143\u306ecudaStream\u3092\u4f7f\u3063\u305f\u4f8b\u3088\u308a\u66f8\u304d\u3084\u3059\u304f\u306a\u3063\u305f\u3068\u3044\u3046\u3053\u3068\u3082\u7121\u3044\u6c17\u304c\u3057\u307e\u3059\u3002\u3082\u3057\u304b\u3059\u308b\u3068std::promise \u306b\u4f8b\u5916\u304b\u8fd4\u308a\u5024\u3092\u5165\u308c\u308b\u65b9\u91dd\u3067\u66f8\u304f\u3068\u7dba\u9e97\u306b\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u4eca\u56de\u306eC++11 future \u3092\u4f7f\u3063\u305f\u30b3\u30fc\u30c9\u4f8b\u306f\u4ee5\u4e0b\u3067\u3059\u3002\n#include <array>\n#include <future>\n#include \"cuda_check.hpp\"\n\n\nconst int N = 1 << 20;\n\n__global__ void kernel(float *x, int n) {\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (int i = tid; i < n; i += blockDim.x * gridDim.x) {\n        x[i] = sqrt(pow(3.14159,i));\n    }\n}\n\nint main() {\n    const int num_streams = 8;\n    std::future<cudaError_t> fs[num_streams];\n    float* data[num_streams];\n\n    for (int i = 0; i < num_streams; i++) {\n        fs[i] = std::async(\n            std::launch::async, [=]() mutable\n            {\n                CUDA_CHECK(cudaMalloc(&data[i], N * sizeof(float)));\n\n                // launch one worker kernel per stream\n                kernel<<<1, 64, 0>>>(data[i], N);\n                CUDA_CHECK(cudaPeekAtLastError());\n                return cudaSuccess;\n            });\n        // launch a dummy kernel on the default stream\n        kernel<<<1, 1>>>(0, 0);\n        CUDA_CHECK(cudaPeekAtLastError());\n    }\n\n    for (auto&f: fs) {\n        CUDA_CHECK(f.get());\n    }\n\n    cudaDeviceReset();\n}\n\nNVVP\u3067\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3059\u308b\u3068\u3001cudaStream\u3092\u4f7f\u3063\u3066\u3044\u305f\u3068\u304d\u306f\u51fa\u3066\u3053\u306a\u304b\u3063\u305fcudaMalloc\u3068\u3001num_stream\u500b\u306ethread\u304c\u3067\u304d\u3066\u3044\u3066\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u305f\u3060\u6b21\u306e\u3088\u3046\u306b std::launch::deferred\u3068cudaStream_t\u3092\u4f7f\u3046\u3068 (\u3055\u3089\u306b\u7169\u96d1\u3060\u3051\u3069)\u3001thread\u306f\u4f5c\u3089\u308c\u306a\u3044\u4e0a\u306b\u9045\u5ef6\u5b9f\u884c\u3082\u3067\u304d\u308b\u306e\u3067\u3001\u3088\u308astream\u611f\u306e\u5f37\u3044\u4f7f\u3044\u65b9\u304c\u3067\u304d\u308b??\nint main() {\n    const int num_streams = 8;\n    cudaStream_t streams[num_streams];\n    std::future<cudaError_t> fs[num_streams];\n    float* data[num_streams];\n\n    for (int i = 0; i < num_streams; i++) {\n        // \u3053\u3061\u3089\u306f\u3053\u306efor\u30eb\u30fc\u30d7\u5185\u3067\u306f\u5b9f\u884c\u3055\u308c\u306a\u3044\n        fs[i] = std::async(\n            std::launch::deferred, [=]() mutable\n            {\n                CUDA_CHECK(cudaStreamCreate(&streams[i]));\n                CUDA_CHECK(cudaMalloc(&data[i], N * sizeof(float)));\n                kernel<<<1, 64, 0, streams[i]>>>(data[i], N);\n                CUDA_CHECK(cudaPeekAtLastError());\n                return cudaSuccess;\n            });\n        // \u3053\u3061\u3089\u306f\u4e0a\u306e\u30b3\u30fc\u30c9\u3088\u308a\u5148\u306b\u3053\u306efor\u30eb\u30fc\u30d7\u5185\u3067\u5b9f\u884c\u3055\u308c\u308b\n        kernel<<<1, 1>>>(0, 0);\n        CUDA_CHECK(cudaPeekAtLastError());\n    }\n\n    for (auto&f: fs) {\n        // \u3053\u3053\u3067\u306f\u3058\u3081\u3066 deferred \u3055\u308c\u305f\u30e9\u30e0\u30c0\u5f0f\u3092\u5b9f\u884c\n        CUDA_CHECK(f.get());\n    }\n\n    cudaDeviceReset();\n}\n\n\u306a\u304a\u3001\u4e0a\u306e\u30b3\u30fc\u30c9\u3068\u9055\u3063\u3066\u5143\u306e\u30b3\u30fc\u30c9\u3067\u306ffor\u30eb\u30fc\u30d7\u5185\u306e\uff12\u3064\u306e\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u306f\u4ea4\u4e92\u306b\u5b9f\u884c\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u78ba\u8a8d\u306fNVVP\u306e\u4e0b\u5074GPU Details\u3092\u898b\u308b\u3068\u308f\u304b\u308a\u307e\u3059\u3002\n\nC++11 thread \u3067\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\n\u5358\u7d14\u306b pthread \u3092\u4f7f\u3063\u3066\u3044\u305f\u90e8\u5206\u3092\u7f6e\u304d\u63db\u3048\u307e\u3057\u305f\u3002\u30b3\u30fc\u30c9\u304c pthread \u975e\u4f9d\u5b58\u306b\u306a\u3063\u305f\u306e\u3068\u623b\u308a\u5024\u3092\u53d6\u5f97\u3057\u3084\u3059\u304f\u306a\u3063\u305f\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\u3002\n#include <future>\n#include <thread>\n#include <iostream>\n#include \"cuda_check.hpp\"\n\nconst int N = 1 << 20;\n\n__global__ void kernel(float *x, int n) {\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (int i = tid; i < n; i += blockDim.x * gridDim.x) {\n        x[i] = sqrt(pow(3.14159,i));\n    }\n}\n\nstd::string launch_kernel() {\n    float *data;\n    CUDA_CHECK(cudaMalloc(&data, N * sizeof(float)));\n    kernel<<<1, 64>>>(data, N);\n    CUDA_CHECK(cudaPeekAtLastError());\n    CUDA_CHECK(cudaStreamSynchronize(0));\n    return \"ok\";\n}\n\n\nint main() {\n    const int num_threads = 8;\n    std::future<std::string> fs[num_threads];\n\n    for (auto& f: fs) {\n        try {\n            std::packaged_task<std::string()> task(launch_kernel);\n            f = task.get_future();\n            std::thread(std::move(task)).detach();\n        } catch(std::exception& e) {\n            std::cerr << \"Error creating thread: \" << e.what() << std::endl;\n        }\n    }\n\n    for (auto& f: fs) {\n        try {\n            std::cout << f.get() << std::endl;\n        } catch(std::exception& e) {\n            std::cerr << \"Error joining thread: \" << e.what() << std::endl;\n        }\n    }\n\n    cudaDeviceReset();\n}\n\n\u3056\u3063\u3068\u898b\u305f\u3068\u3053\u308d\u3001pthread\u3092\u4f7f\u3063\u305f\u5143\u306e\u30b3\u30fc\u30c9\u4f8b\u3068\u6bd4\u3079\u3066\u3082\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u304c\u767a\u751f\u3057\u305f\u3088\u3046\u306b\u306f\u898b\u3048\u307e\u305b\u3093\u3002\u3053\u306e\u4f7f\u3044\u65b9\u306f\u7a4d\u6975\u7684\u306b\u63a1\u7528\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n\n\n\u5168\u30b3\u30fc\u30c9\nhttps://github.com/ShigekiKarita/cuda8-async\n\u3053\u3053\u306f\u3053\u3046\u3059\u308b\u3079\u304d\u3068\u3044\u3046\u6307\u6458\u304c\u3042\u308c\u3070PR\u3084\u30b3\u30e1\u30f3\u30c8\u3067\u3082\u3089\u3048\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n## \u306f\u3058\u3081\u306b\n\nCUDA8 \u306b\u306a\u308a\u6f38\u304f C++11 \u5bfe\u5fdc\u304c\u5165\u3063\u305f\u306e\u3067\u3001C\u8a00\u8a9e\u3068\u5171\u7528\u306epthread\u3084cudaStream\u3067\u306f\u306a\u304f\u76f8\u5f53\u3059\u308bC++11\u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3044\u305f\u3044\u3068\u3044\u3046\u306e\u304c\u3001\u3053\u306e\u8a18\u4e8b\u306e\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u3067\u3059\u3002\u4eca\u56de\u306fCUDA7RC\u306e\u3068\u304d\u306e[\u516c\u5f0f\u30d6\u30ed\u30b0\u8a18\u4e8b](https://devblogs.nvidia.com/parallelforall/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)\u3092\u30d9\u30fc\u30b9\u306b\u3084\u3063\u3066\u307f\u307e\u3059\u3002\n\n```\n$ nvcc ./stream_test.cu -o ./stream_legacy\n$ nvvp ./stream_legacy\n```\n\n\u4ee5\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3067\u3001nvcc\u3067\u5b9f\u884c\u30d5\u30a1\u30a4\u30eb\u306b\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u3066\u3001Eclipse\u3063\u307d\u3044UI\u306eNVVP\u3067\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3067\u304d\u307e\u3059\u3002\u521d\u3081\u3066NVVP\u3092\u4f7f\u3063\u305f\u306e\u3067\u3001\u6700\u521d\u306e\u8a2d\u5b9a\u3067Arguments: `Profile all processes` \u3068\u306a\u3063\u3066\u3044\u308c\u3070 `Profile child processes` \u306b\u5909\u66f4\u3057\u306a\u3044\u3068\u3001\u3044\u3064\u307e\u3067\u3082\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u7d42\u308f\u3089\u306a\u3044\u306e\u3067\u30cf\u30de\u308a\u307e\u3057\u305f\u3002\u4e0b\u306e settings \u304b\u3089\u8a2d\u5b9a\u76f4\u3057\u3066\u4e0a\u306e Run > Generate Timeline \u3067\u52d5\u304b\u3057\u307e\u3059\u3002\n\n\u8a73\u7d30\u306f\u98db\u3070\u3057\u3066\u3001\u96d1\u306b\u516c\u5f0f\u30d6\u30ed\u30b0\u306e\u8a71\u3092\u307e\u3068\u3081\u308b\u3068\n\n+ cudaStream_t \u3084 pthread_t \u3092\u4f7f\u3063\u3066\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u3001\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\u3092\u3059\u308b\u3068\u304d\u306fnvcc \u306e\u30aa\u30d7\u30b7\u30e7\u30f3 `--default-stream per-thread` \u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\n+ pthread_t\u3092\u4f7f\u3046\u3068\u304d\u306f\u30b9\u30ec\u30c3\u30c9\u6bce\u306b\u30e1\u30e2\u30ea\u78ba\u4fdd\u3084\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u3092\u547c\u3073\u51fa\u3057\u3066\u3001`cudaStreamSynchronize(cudaStream_t stream)`\u3059\u308b\u95a2\u6570\u3092pthread_create\u306b\u6e21\u3059\n\n\u4ee5\u4e0a\u304c\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u30fb\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\u306e\u304a\u7d04\u675f\u306e\u3088\u3046\u3067\u3059\u3002\u305d\u308c\u3067\u306f\u3001C++11\u306e `<future>` \u3084 `<thread>` \u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u3063\u3066\u66f8\u304d\u63db\u3048\u3066\u307f\u307e\u3059\n\n## C++11 future \u3067\u30de\u30eb\u30c1\u30b9\u30c8\u30ea\u30fc\u30e0\u51e6\u7406\n\n\u52c9\u5f37\u4e0d\u8db3\u306a\u306e\u3067\u3001\u5143\u306ecudaStream\u3092\u4f7f\u3063\u305f\u4f8b\u3088\u308a\u66f8\u304d\u3084\u3059\u304f\u306a\u3063\u305f\u3068\u3044\u3046\u3053\u3068\u3082\u7121\u3044\u6c17\u304c\u3057\u307e\u3059\u3002\u3082\u3057\u304b\u3059\u308b\u3068`std::promise` \u306b\u4f8b\u5916\u304b\u8fd4\u308a\u5024\u3092\u5165\u308c\u308b\u65b9\u91dd\u3067\u66f8\u304f\u3068\u7dba\u9e97\u306b\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u4eca\u56de\u306eC++11 future \u3092\u4f7f\u3063\u305f\u30b3\u30fc\u30c9\u4f8b\u306f\u4ee5\u4e0b\u3067\u3059\u3002\n\n``` cuda\n#include <array>\n#include <future>\n#include \"cuda_check.hpp\"\n\n\nconst int N = 1 << 20;\n\n__global__ void kernel(float *x, int n) {\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (int i = tid; i < n; i += blockDim.x * gridDim.x) {\n        x[i] = sqrt(pow(3.14159,i));\n    }\n}\n\nint main() {\n    const int num_streams = 8;\n    std::future<cudaError_t> fs[num_streams];\n    float* data[num_streams];\n\n    for (int i = 0; i < num_streams; i++) {\n        fs[i] = std::async(\n            std::launch::async, [=]() mutable\n            {\n                CUDA_CHECK(cudaMalloc(&data[i], N * sizeof(float)));\n\n                // launch one worker kernel per stream\n                kernel<<<1, 64, 0>>>(data[i], N);\n                CUDA_CHECK(cudaPeekAtLastError());\n                return cudaSuccess;\n            });\n        // launch a dummy kernel on the default stream\n        kernel<<<1, 1>>>(0, 0);\n        CUDA_CHECK(cudaPeekAtLastError());\n    }\n\n    for (auto&f: fs) {\n        CUDA_CHECK(f.get());\n    }\n\n    cudaDeviceReset();\n}\n```\nNVVP\u3067\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u3059\u308b\u3068\u3001cudaStream\u3092\u4f7f\u3063\u3066\u3044\u305f\u3068\u304d\u306f\u51fa\u3066\u3053\u306a\u304b\u3063\u305fcudaMalloc\u3068\u3001num_stream\u500b\u306ethread\u304c\u3067\u304d\u3066\u3044\u3066\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n![](https://github.com/ShigekiKarita/cuda8-async/raw/master/res/cpp11async.png)\n\n\u305f\u3060\u6b21\u306e\u3088\u3046\u306b `std::launch::deferred`\u3068`cudaStream_t`\u3092\u4f7f\u3046\u3068 (\u3055\u3089\u306b\u7169\u96d1\u3060\u3051\u3069)\u3001thread\u306f\u4f5c\u3089\u308c\u306a\u3044\u4e0a\u306b\u9045\u5ef6\u5b9f\u884c\u3082\u3067\u304d\u308b\u306e\u3067\u3001\u3088\u308astream\u611f\u306e\u5f37\u3044\u4f7f\u3044\u65b9\u304c\u3067\u304d\u308b??\n\n``` cuda\nint main() {\n    const int num_streams = 8;\n    cudaStream_t streams[num_streams];\n    std::future<cudaError_t> fs[num_streams];\n    float* data[num_streams];\n\n    for (int i = 0; i < num_streams; i++) {\n        // \u3053\u3061\u3089\u306f\u3053\u306efor\u30eb\u30fc\u30d7\u5185\u3067\u306f\u5b9f\u884c\u3055\u308c\u306a\u3044\n        fs[i] = std::async(\n            std::launch::deferred, [=]() mutable\n            {\n                CUDA_CHECK(cudaStreamCreate(&streams[i]));\n                CUDA_CHECK(cudaMalloc(&data[i], N * sizeof(float)));\n                kernel<<<1, 64, 0, streams[i]>>>(data[i], N);\n                CUDA_CHECK(cudaPeekAtLastError());\n                return cudaSuccess;\n            });\n        // \u3053\u3061\u3089\u306f\u4e0a\u306e\u30b3\u30fc\u30c9\u3088\u308a\u5148\u306b\u3053\u306efor\u30eb\u30fc\u30d7\u5185\u3067\u5b9f\u884c\u3055\u308c\u308b\n        kernel<<<1, 1>>>(0, 0);\n        CUDA_CHECK(cudaPeekAtLastError());\n    }\n\n    for (auto&f: fs) {\n        // \u3053\u3053\u3067\u306f\u3058\u3081\u3066 deferred \u3055\u308c\u305f\u30e9\u30e0\u30c0\u5f0f\u3092\u5b9f\u884c\n        CUDA_CHECK(f.get());\n    }\n\n    cudaDeviceReset();\n}\n```\n\u306a\u304a\u3001\u4e0a\u306e\u30b3\u30fc\u30c9\u3068\u9055\u3063\u3066\u5143\u306e\u30b3\u30fc\u30c9\u3067\u306ffor\u30eb\u30fc\u30d7\u5185\u306e\uff12\u3064\u306e\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u306f\u4ea4\u4e92\u306b\u5b9f\u884c\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u78ba\u8a8d\u306fNVVP\u306e\u4e0b\u5074`GPU Details`\u3092\u898b\u308b\u3068\u308f\u304b\u308a\u307e\u3059\u3002\n\n## C++11 thread \u3067\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u51e6\u7406\n\n\u5358\u7d14\u306b pthread \u3092\u4f7f\u3063\u3066\u3044\u305f\u90e8\u5206\u3092\u7f6e\u304d\u63db\u3048\u307e\u3057\u305f\u3002\u30b3\u30fc\u30c9\u304c pthread \u975e\u4f9d\u5b58\u306b\u306a\u3063\u305f\u306e\u3068\u623b\u308a\u5024\u3092\u53d6\u5f97\u3057\u3084\u3059\u304f\u306a\u3063\u305f\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\u3002\n\n``` cuda\n#include <future>\n#include <thread>\n#include <iostream>\n#include \"cuda_check.hpp\"\n\nconst int N = 1 << 20;\n\n__global__ void kernel(float *x, int n) {\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (int i = tid; i < n; i += blockDim.x * gridDim.x) {\n        x[i] = sqrt(pow(3.14159,i));\n    }\n}\n\nstd::string launch_kernel() {\n    float *data;\n    CUDA_CHECK(cudaMalloc(&data, N * sizeof(float)));\n    kernel<<<1, 64>>>(data, N);\n    CUDA_CHECK(cudaPeekAtLastError());\n    CUDA_CHECK(cudaStreamSynchronize(0));\n    return \"ok\";\n}\n\n\nint main() {\n    const int num_threads = 8;\n    std::future<std::string> fs[num_threads];\n\n    for (auto& f: fs) {\n        try {\n            std::packaged_task<std::string()> task(launch_kernel);\n            f = task.get_future();\n            std::thread(std::move(task)).detach();\n        } catch(std::exception& e) {\n            std::cerr << \"Error creating thread: \" << e.what() << std::endl;\n        }\n    }\n\n    for (auto& f: fs) {\n        try {\n            std::cout << f.get() << std::endl;\n        } catch(std::exception& e) {\n            std::cerr << \"Error joining thread: \" << e.what() << std::endl;\n        }\n    }\n\n    cudaDeviceReset();\n}\n```\n\n\u3056\u3063\u3068\u898b\u305f\u3068\u3053\u308d\u3001pthread\u3092\u4f7f\u3063\u305f\u5143\u306e\u30b3\u30fc\u30c9\u4f8b\u3068\u6bd4\u3079\u3066\u3082\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\u304c\u767a\u751f\u3057\u305f\u3088\u3046\u306b\u306f\u898b\u3048\u307e\u305b\u3093\u3002\u3053\u306e\u4f7f\u3044\u65b9\u306f\u7a4d\u6975\u7684\u306b\u63a1\u7528\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n![](https://github.com/ShigekiKarita/cuda8-async/raw/master/res/cpp11thread.png)\n\n\n## \u5168\u30b3\u30fc\u30c9\n\nhttps://github.com/ShigekiKarita/cuda8-async\n\n\u3053\u3053\u306f\u3053\u3046\u3059\u308b\u3079\u304d\u3068\u3044\u3046\u6307\u6458\u304c\u3042\u308c\u3070PR\u3084\u30b3\u30e1\u30f3\u30c8\u3067\u3082\u3089\u3048\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n"}