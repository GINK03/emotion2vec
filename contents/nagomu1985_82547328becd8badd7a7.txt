{"tags": ["Scala", "Spark", "CentOS6.x", "Java"], "context": " More than 1 year has passed since last update.\u5fd8\u5099\u9332\n\u3010OS\u3011\n\u4eca\u56de\u306fCentOS6.6_x86_64\u7248\u3092\u4f7f\u7528\u3002\u8a73\u7d30\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://centos.server-manual.com/\n\u4e8b\u524d\u6e96\u5099\n\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u306b\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4e8b\u524d\u306b\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002\u4ee5\u4e0b\u3092\u5168\u3066\u8a2d\u5b9a\u3059\u308b\u3002\n\u30b7\u30b9\u30c6\u30e0\u5909\u66f4\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u7ba1\u7406\u8005\u6a29\u9650\u304c\u5fc5\u9808\u3002root\u306bsu\u3057\u3066\u304a\u304f\u4e8b\u3002\n\u3010YUM\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u3011\nyum -y install yum-plugin-fastestmirror\nyum -y update\nyum -y groupinstall \"Base\" \"Development tools\" \"Japanese Support\"\n[RPMforge\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://apt.sw.be/RPM-GPG-KEY.dag.txt\nrpm -ivh http://apt.sw.be/redhat/el6/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm\n[EPEL\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-6\nrpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\n[ELRepo\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm\n[Remi\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://rpms.famillecollet.com/RPM-GPG-KEY-remi\nrpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm\n\u3010SELinux\u7121\u52b9\u5316\u3011\ngetenforce\nEnforcing \u2190SELinux\u6709\u52b9\nsetenforce 0\ngetenforce\nPermissive \u2190SELinux\u7121\u52b9\nvi /etc/sysconfig/selinux\nSELINUX=enforcing \nSELINUX=disabled \u2190\u5909\u66f4(\u8d77\u52d5\u6642\u306b\u7121\u52b9\u306b\u3059\u308b)\n\u3010iptables\u3067HTTP\u3092\u8a31\u53ef\u3011\nvi /etc/sysconfig/iptables\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT \u2190\u8ffd\u52a0\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\nCOMMIT\nIptables\u518d\u8d77\u52d5\nservice iptables restart\n\u3010JAVA\u3011\nCentOS\u69cb\u7bc9\u6642\u306b\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3002\nyum erase java*\n\u6700\u65b0\u7248\u3092\u30cd\u30c3\u30c8\u3088\u308a\u5165\u624b(rpm\u7248)\u3057\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nrpm \u2013ivh jdk-8u45-linux-x64.rpm\n\u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d\njava \u2013version\njava version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\n\u25a0JAVA_HOME\u8a2d\u5b9a\nvi /etc/profile\nexport JAVA_HOME=/usr/java/default\nexport PATH=$PATH:$JAVA_HOME/bin\nexport CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar\n\n\u3010\u524d\u63d0\u6761\u4ef6\u3011\nStandalone mode\u3067\u7a3c\u50cd\u3055\u305b\u308b\u305f\u3081\u4eca\u56de\u306f\u30d3\u30eb\u30c9\u306f\u884c\u308f\u306a\u3044\u3082\u306e\u3068\u3057\u307e\u3059\n\u3010Scala\u3011\ncd /usr/local/src\nwget http://www.scala-lang.org/files/archive/scala-2.11.7.tgz\ntar -zxvf scala-2.11.7.tgz\nchown -R root:root scala-2.11.7\nmv scala-2.11.7 ../scala\n\u3010Spark\u3011\nwget http://ftp.riken.jp/net/apache/spark/spark-1.4.0/spark-1.4.0-bin-cdh4.tgz\ntar -zxvf spark-1.4.0-bin-cdh4.tgz\nchown -R root:root spark-1.4.0-bin-cdh4\nmv spark-1.4.0-bin-cdh4 ../spark\n\u74b0\u5883\u5909\u6570\u3092\u8ffd\u8a18\nvi /etc/profile\nexport SCALA_HOME=/usr/local/scala\nexport SPARK_HOME=/usr/local/spark\nexport PATH=$SCALA_HOME/bin:$PATH\n\nsource /etc/profile\n\n\u78ba\u8a8d\n\ncd $SPARK_HOME\n./bin/spark-shell\n    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n    14/10/01 05:53:08 INFO SecurityManager: Changing view acls to: hdspark,\n    14/10/01 05:53:08 INFO SecurityManager: Changing modify acls to: hdspark,\n    14/10/01 05:53:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdspark, ); users with modify permissions: Set(hdspark, )\n    14/10/01 05:53:08 INFO HttpServer: Starting HTTP Server\n    14/10/01 05:53:09 INFO Utils: Successfully started service 'HTTP class server' on port 33066.\n    Welcome to\n          ____              __\n         / /  ___ ____/ /_\n        \\ \\/ _ \\/ _ `/ _/  '/\n       // ./_,// //_\\   version 1.4.0\n          //\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)\nType in expressions to have them evaluated.\nType :help for more information.\n14/10/01 05:53:22 INFO SecurityManager: Changing view acls to: hdspark,\n14/10/01 05:53:22 INFO SecurityManager: Changing modify acls to: hdspark,\n14/10/01 05:53:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdspark, ); users with modify permissions: Set(hdspark, )\n14/10/01 05:53:24 INFO Slf4jLogger: Slf4jLogger started\n14/10/01 05:53:24 INFO Remoting: Starting remoting\n14/10/01 05:53:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@localhost:36288]\n14/10/01 05:53:25 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@localhost:36288]\n14/10/01 05:53:25 INFO Utils: Successfully started service 'sparkDriver' on port 36288.\n14/10/01 05:53:25 INFO SparkEnv: Registering MapOutputTracker\n14/10/01 05:53:25 INFO SparkEnv: Registering BlockManagerMaster\n14/10/01 05:53:25 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20141001055325-22ac\n14/10/01 05:53:26 INFO Utils: Successfully started service 'Connection manager for block manager' on port 56196.\n14/10/01 05:53:26 INFO ConnectionManager: Bound socket to port 56196 with id = ConnectionManagerId(localhost,56196)\n14/10/01 05:53:26 INFO MemoryStore: MemoryStore started with capacity 267.3 MB\n14/10/01 05:53:26 INFO BlockManagerMaster: Trying to register BlockManager\n14/10/01 05:53:26 INFO BlockManagerMasterActor: Registering block manager localhost:56196 with 267.3 MB RAM\n14/10/01 05:53:26 INFO BlockManagerMaster: Registered BlockManager\n14/10/01 05:53:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a33f43d9-37da-4c9e-a0b8-71b117b37012\n14/10/01 05:53:26 INFO HttpServer: Starting HTTP Server\n14/10/01 05:53:26 INFO Utils: Successfully started service 'HTTP file server' on port 54714.\n14/10/01 05:53:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n14/10/01 05:53:27 INFO SparkUI: Started SparkUI at http://localhost:4040\n14/10/01 05:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n14/10/01 05:53:29 INFO Executor: Using REPL class URI: http://localhost:33066\n14/10/01 05:53:29 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@localhost:36288/user/HeartbeatReceiver\n14/10/01 05:53:30 INFO SparkILoop: Created spark context..\nSpark context available as sc.\n\n\nscala>\n\n\n//\u7c21\u5358\u306a\u884c\u6570\u30ab\u30a6\u30f3\u30c8\u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3059\nscala> val txtFile = sc.textFile(\"README.md\")\n    14/10/01 05:56:17 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n    14/10/01 05:56:17 INFO MemoryStore: ensureFreeSpace(156973) called with curMem=0, maxMem=280248975\n    14/10/01 05:56:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 153.3 KB, free 267.1 MB)\n    txtFile: org.apache.spark.rdd.RDD[String] = ../README.md MappedRDD[1] at textFile at <console>:12\n\n\nscala> txtFile.count()\n    14/10/01 05:56:29 INFO FileInputFormat: Total input paths to process : 1\n    14/10/01 05:56:29 INFO SparkContext: Starting job: count at <console>:15\n    14/10/01 05:56:29 INFO DAGScheduler: Got job 0 (count at <console>:15) with 1 output partitions (allowLocal=false)\n    14/10/01 05:56:29 INFO DAGScheduler: Final stage: Stage 0(count at <console>:15)\n    14/10/01 05:56:29 INFO DAGScheduler: Parents of final stage: List()\n    14/10/01 05:56:29 INFO DAGScheduler: Missing parents: List()\n    14/10/01 05:56:29 INFO DAGScheduler: Submitting Stage 0 (../README.md MappedRDD[1] at textFile at <console>:12), which has no missing parents\n    14/10/01 05:56:29 INFO MemoryStore: ensureFreeSpace(2384) called with curMem=156973, maxMem=280248975\n    14/10/01 05:56:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.3 KB, free 267.1 MB)\n    14/10/01 05:56:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (../README.md MappedRDD[1] at textFile at <console>:12)\n    14/10/01 05:56:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n    14/10/01 05:56:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1207 bytes)\n    14/10/01 05:56:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n    14/10/01 05:56:29 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+4811\n    14/10/01 05:56:29 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n    14/10/01 05:56:29 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n    14/10/01 05:56:29 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n    14/10/01 05:56:29 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n    14/10/01 05:56:29 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n    14/10/01 05:56:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1731 bytes result sent to driver\n    14/10/01 05:56:30 INFO DAGScheduler: Stage 0 (count at <console>:15) finished in 0.462 s\n    14/10/01 05:56:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 423 ms on localhost (1/1)\n    14/10/01 05:56:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\n    14/10/01 05:56:30 INFO SparkContext: Job finished: count at <console>:15, took 0.828128221 s\n    res0: Long = 141\n\n//\u6210\u529f\uff01\n\u5fd8\u5099\u9332\n\n\u3010OS\u3011\n\u4eca\u56de\u306fCentOS6.6_x86_64\u7248\u3092\u4f7f\u7528\u3002\u8a73\u7d30\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://centos.server-manual.com/\n\u4e8b\u524d\u6e96\u5099\n\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u306b\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4e8b\u524d\u306b\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002\u4ee5\u4e0b\u3092\u5168\u3066\u8a2d\u5b9a\u3059\u308b\u3002\n\u30b7\u30b9\u30c6\u30e0\u5909\u66f4\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u7ba1\u7406\u8005\u6a29\u9650\u304c\u5fc5\u9808\u3002root\u306bsu\u3057\u3066\u304a\u304f\u4e8b\u3002\n\n\u3010YUM\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u3011\nyum -y install yum-plugin-fastestmirror\nyum -y update\nyum -y groupinstall \"Base\" \"Development tools\" \"Japanese Support\"\n[RPMforge\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://apt.sw.be/RPM-GPG-KEY.dag.txt\nrpm -ivh http://apt.sw.be/redhat/el6/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm\n[EPEL\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-6\nrpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\n[ELRepo\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm\n[Remi\u30ea\u30dd\u30b8\u30c8\u30ea\u8ffd\u52a0]\nrpm --import http://rpms.famillecollet.com/RPM-GPG-KEY-remi\nrpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm\n\n\u3010SELinux\u7121\u52b9\u5316\u3011\ngetenforce\nEnforcing \u2190SELinux\u6709\u52b9\nsetenforce 0\ngetenforce\nPermissive \u2190SELinux\u7121\u52b9\nvi /etc/sysconfig/selinux\nSELINUX=enforcing \nSELINUX=disabled \u2190\u5909\u66f4(\u8d77\u52d5\u6642\u306b\u7121\u52b9\u306b\u3059\u308b)\n\n\u3010iptables\u3067HTTP\u3092\u8a31\u53ef\u3011\nvi /etc/sysconfig/iptables\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT \u2190\u8ffd\u52a0\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\nCOMMIT\nIptables\u518d\u8d77\u52d5\nservice iptables restart\n\n\u3010JAVA\u3011\nCentOS\u69cb\u7bc9\u6642\u306b\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3002\nyum erase java*\n\u6700\u65b0\u7248\u3092\u30cd\u30c3\u30c8\u3088\u308a\u5165\u624b(rpm\u7248)\u3057\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nrpm \u2013ivh jdk-8u45-linux-x64.rpm\n\u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d\njava \u2013version\njava version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\n\u25a0JAVA_HOME\u8a2d\u5b9a\nvi /etc/profile\n<pre>\nexport JAVA_HOME=/usr/java/default\nexport PATH=$PATH:$JAVA_HOME/bin\nexport CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar</pre>\n\n\u3010\u524d\u63d0\u6761\u4ef6\u3011\nStandalone mode\u3067\u7a3c\u50cd\u3055\u305b\u308b\u305f\u3081\u4eca\u56de\u306f\u30d3\u30eb\u30c9\u306f\u884c\u308f\u306a\u3044\u3082\u306e\u3068\u3057\u307e\u3059\n\n\u3010Scala\u3011\ncd /usr/local/src\nwget http://www.scala-lang.org/files/archive/scala-2.11.7.tgz\ntar -zxvf scala-2.11.7.tgz\nchown -R root:root scala-2.11.7\nmv scala-2.11.7 ../scala\n\n\u3010Spark\u3011\nwget http://ftp.riken.jp/net/apache/spark/spark-1.4.0/spark-1.4.0-bin-cdh4.tgz\ntar -zxvf spark-1.4.0-bin-cdh4.tgz\nchown -R root:root spark-1.4.0-bin-cdh4\nmv spark-1.4.0-bin-cdh4 ../spark\n\n\u74b0\u5883\u5909\u6570\u3092\u8ffd\u8a18\nvi /etc/profile\n<pre>\nexport SCALA_HOME=/usr/local/scala\nexport SPARK_HOME=/usr/local/spark\nexport PATH=$SCALA_HOME/bin:$PATH\n</pre>\nsource /etc/profile\n\n\u78ba\u8a8d\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\ncd $SPARK_HOME\n./bin/spark-shell\n    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n    14/10/01 05:53:08 INFO SecurityManager: Changing view acls to: hdspark,\n    14/10/01 05:53:08 INFO SecurityManager: Changing modify acls to: hdspark,\n    14/10/01 05:53:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdspark, ); users with modify permissions: Set(hdspark, )\n    14/10/01 05:53:08 INFO HttpServer: Starting HTTP Server\n    14/10/01 05:53:09 INFO Utils: Successfully started service 'HTTP class server' on port 33066.\n\tWelcome to\n\t      ____              __\n\t     / __/__  ___ _____/ /__\n\t    _\\ \\/ _ \\/ _ `/ __/  '_/\n\t   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.4.0\n\t      /_/\n\n    Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)\n    Type in expressions to have them evaluated.\n    Type :help for more information.\n    14/10/01 05:53:22 INFO SecurityManager: Changing view acls to: hdspark,\n    14/10/01 05:53:22 INFO SecurityManager: Changing modify acls to: hdspark,\n    14/10/01 05:53:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdspark, ); users with modify permissions: Set(hdspark, )\n    14/10/01 05:53:24 INFO Slf4jLogger: Slf4jLogger started\n    14/10/01 05:53:24 INFO Remoting: Starting remoting\n    14/10/01 05:53:25 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@localhost:36288]\n    14/10/01 05:53:25 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@localhost:36288]\n    14/10/01 05:53:25 INFO Utils: Successfully started service 'sparkDriver' on port 36288.\n    14/10/01 05:53:25 INFO SparkEnv: Registering MapOutputTracker\n    14/10/01 05:53:25 INFO SparkEnv: Registering BlockManagerMaster\n    14/10/01 05:53:25 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20141001055325-22ac\n    14/10/01 05:53:26 INFO Utils: Successfully started service 'Connection manager for block manager' on port 56196.\n    14/10/01 05:53:26 INFO ConnectionManager: Bound socket to port 56196 with id = ConnectionManagerId(localhost,56196)\n    14/10/01 05:53:26 INFO MemoryStore: MemoryStore started with capacity 267.3 MB\n    14/10/01 05:53:26 INFO BlockManagerMaster: Trying to register BlockManager\n    14/10/01 05:53:26 INFO BlockManagerMasterActor: Registering block manager localhost:56196 with 267.3 MB RAM\n    14/10/01 05:53:26 INFO BlockManagerMaster: Registered BlockManager\n    14/10/01 05:53:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a33f43d9-37da-4c9e-a0b8-71b117b37012\n    14/10/01 05:53:26 INFO HttpServer: Starting HTTP Server\n    14/10/01 05:53:26 INFO Utils: Successfully started service 'HTTP file server' on port 54714.\n    14/10/01 05:53:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n    14/10/01 05:53:27 INFO SparkUI: Started SparkUI at http://localhost:4040\n    14/10/01 05:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    14/10/01 05:53:29 INFO Executor: Using REPL class URI: http://localhost:33066\n    14/10/01 05:53:29 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@localhost:36288/user/HeartbeatReceiver\n    14/10/01 05:53:30 INFO SparkILoop: Created spark context..\n    Spark context available as sc.\nscala>\n</pre>\n<br>\n//\u7c21\u5358\u306a\u884c\u6570\u30ab\u30a6\u30f3\u30c8\u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3059\n<pre>\nscala&gt; val txtFile = sc.textFile(&quot;README.md&quot;)\n\t14/10/01 05:56:17 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n\t14/10/01 05:56:17 INFO MemoryStore: ensureFreeSpace(156973) called with curMem=0, maxMem=280248975\n\t14/10/01 05:56:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 153.3 KB, free 267.1 MB)\n\ttxtFile: org.apache.spark.rdd.RDD[String] = ../README.md MappedRDD[1] at textFile at &lt;console&gt;:12\n</pre>\n<br>\n<pre>\nscala&gt; txtFile.count()\n\t14/10/01 05:56:29 INFO FileInputFormat: Total input paths to process : 1\n\t14/10/01 05:56:29 INFO SparkContext: Starting job: count at &lt;console&gt;:15\n\t14/10/01 05:56:29 INFO DAGScheduler: Got job 0 (count at &lt;console&gt;:15) with 1 output partitions (allowLocal=false)\n\t14/10/01 05:56:29 INFO DAGScheduler: Final stage: Stage 0(count at &lt;console&gt;:15)\n\t14/10/01 05:56:29 INFO DAGScheduler: Parents of final stage: List()\n\t14/10/01 05:56:29 INFO DAGScheduler: Missing parents: List()\n\t14/10/01 05:56:29 INFO DAGScheduler: Submitting Stage 0 (../README.md MappedRDD[1] at textFile at &lt;console&gt;:12), which has no missing parents\n\t14/10/01 05:56:29 INFO MemoryStore: ensureFreeSpace(2384) called with curMem=156973, maxMem=280248975\n\t14/10/01 05:56:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.3 KB, free 267.1 MB)\n\t14/10/01 05:56:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (../README.md MappedRDD[1] at textFile at &lt;console&gt;:12)\n\t14/10/01 05:56:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n\t14/10/01 05:56:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1207 bytes)\n\t14/10/01 05:56:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n\t14/10/01 05:56:29 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+4811\n\t14/10/01 05:56:29 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n\t14/10/01 05:56:29 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n\t14/10/01 05:56:29 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n\t14/10/01 05:56:29 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n\t14/10/01 05:56:29 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n\t14/10/01 05:56:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1731 bytes result sent to driver\n\t14/10/01 05:56:30 INFO DAGScheduler: Stage 0 (count at &lt;console&gt;:15) finished in 0.462 s\n\t14/10/01 05:56:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 423 ms on localhost (1/1)\n\t14/10/01 05:56:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\n\t14/10/01 05:56:30 INFO SparkContext: Job finished: count at &lt;console&gt;:15, took 0.828128221 s\n\tres0: Long = 141</pre>\n//\u6210\u529f\uff01\n"}