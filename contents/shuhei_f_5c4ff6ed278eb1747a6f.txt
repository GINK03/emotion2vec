{"context": " More than 1 year has passed since last update.Python + NumPy \u3060\u3051\u3067\uff0c\u672c\u6c17\u3067\u901f\u5ea6\u3092\u8ffd\u6c42\u3057\u305f SVM \u3092\u66f8\u3044\u3066\u307f\u305f\uff0e\n\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f LIVSVM \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u95a2\u9023\u8ad6\u6587\u306b\u5023\u3063\u3066\u8272\u3005\u3068\u5de5\u592b\u3092\u53d6\u308a\u5165\u308c\u305f SMO\uff0e\nWorking set \u306e\u9078\u629e (\u5404\u53cd\u5fa9\u3067\u90e8\u5206\u554f\u984c\u306b\u4f7f\u3046 2 \u5909\u6570\u3092\u9078\u3076\u30a2\u30ec) \u306f\u5b9f\u88c5\u306e\u3057\u6613\u3055\u3092\u512a\u5148\u3057\u3066\u5c11\u3057\u53e4\u3044\u624b\u6cd5 (LIBSVM ver. 2.8 \u304f\u3089\u3044\u307e\u3067\u4f7f\u308f\u308c\u3066\u3044\u305f\u3084\u3064) \u3092\u63a1\u7528\u3057\u305f\u304c\uff0c\u65b0\u3057\u3044\u3084\u3064\u304c\u5fc5\u305a\u3057\u3082\u901f\u3044\u4fdd\u8a3c\u306f\u306a\u3044\u3057\u307e\u3042\u826f\u3044\u304b\u306a\u3068\u3044\u3046\u611f\u3058\uff0e\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\u672c\u5f53\u306f\u4e00\u4eba\u3067\u697d\u3057\u3080\u4e88\u5b9a\u3060\u3063\u305f\u3051\u308c\u3069\uff0c\u6848\u5916\u5b9f\u7528\u30ec\u30d9\u30eb\u306e\u901f\u5ea6\u304c\u51fa\u305f\u306e\u3067\u7686\u3068\u559c\u3073\u3092\u5206\u304b\u3061\u5408\u3046\u3053\u3068\u306b\u3057\u305f\uff0e\n\u3067\u3082\uff0c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8aac\u660e\u3092\u66f8\u304f\u3068\u3081\u3063\u3061\u3083\u9577\u304f\u306a\u308a\u305d\u3046\u306a\u306e\u3067\uff0c\u3072\u3068\u307e\u305a\u59a5\u5354\u3057\u3066\u30b3\u30fc\u30c9\u3060\u3051\u8cbc\u308b\u3053\u3068\u306b\u3057\u305f\uff0e\n\u307e\u3060\u5b9f\u88c5\u3067\u304d\u3066\u3044\u306a\u3044\u6a5f\u80fd\u304c\u6ca2\u5c71\u3042\u308b\u3051\u3069\uff0c\u57fa\u672c\u7684\u306b\u4f7f\u3044\u65b9\u306f scikit-learn \u3068\u307b\u307c\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u306f\u305a\uff0e\n# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\ndef linear_kernel(x1, x2):\n    x1 = np.atleast_2d(x1)\n    x2 = np.atleast_2d(x2)\n    return np.dot(x1, x2.T)\n\n\n# k(x, y) = exp(- gamma || x1 - x2 ||^2)\ndef get_rbf_kernel(gamma):\n    def rbf_kernel(x1, x2):\n        x1 = np.atleast_2d(x1)\n        x2 = np.atleast_2d(x2)\n        m1, _ = x1.shape\n        m2, _ = x2.shape\n        norm1 = np.dot(np.ones([m2, 1]), np.atleast_2d(np.sum(x1 ** 2, axis=1))).T\n        norm2 = np.dot(np.ones([m1, 1]), np.atleast_2d(np.sum(x2 ** 2, axis=1)))\n        return np.exp(- gamma * (norm1 + norm2 - 2 * np.dot(x1, x2.T)))\n    return rbf_kernel\n\n\n# k(x1, x2) = (<x1, x2> + coef0)^degree\ndef get_polynomial_kernel(degree, coef0):\n    def polynomial_kernel(x1, x2):\n        x1 = np.atleast_2d(x1)\n        x2 = np.atleast_2d(x2)\n        return (np.dot(x1, x2.T) + coef0)**degree\n    return polynomial_kernel\n\n\nclass CSvc:\n\n    def __init__(self, C=1e0, kernel=linear_kernel, tol=1e-3, max_iter=1000,\n                 gamma=1e0, degree=3, coef0=0):\n        self._EPS = 1e-5\n        self._TAU = 1e-12\n        self._cache = {}\n        self.tol = tol\n        self.max_iter = max_iter\n        self.C = C\n        self.gamma = gamma\n        self.degree, self.coef0 = degree, coef0\n        self.kernel = None\n        self._alpha = None\n        self.intercept_ = None\n        self._grad = None\n        self.itr = None\n        self._ind_y_p, self._ind_y_n = None, None\n        self._i_low, self._i_up = None, None\n        self.set_kernel_function(kernel)\n\n    def _init_solution(self, y):\n        num = len(y)\n        self._alpha = np.zeros(num)\n        self._i_low = y < 0\n        self._i_up = y > 0\n\n    def set_kernel_function(self, kernel):\n        if callable(kernel):\n            self.kernel = kernel\n        elif kernel == 'linear':\n            self.kernel = linear_kernel\n        elif kernel == 'rbf' or kernel == 'gaussian':\n            self.kernel = get_rbf_kernel(self.gamma)\n        elif kernel == 'polynomial' or kernel == 'poly':\n            self.kernel = get_polynomial_kernel(self.degree, self.coef0)\n        else:\n            raise ValueError('{} is undefined name as kernel function'.format(kernel))\n\n    def _select_working_set1(self, y):\n        minus_y_times_grad = - y * self._grad\n        # Convert boolean mask to index\n        i_up = self._i_up.nonzero()[0]\n        i_low = self._i_low.nonzero()[0]\n        ind_ws1 = i_up[np.argmax(minus_y_times_grad[i_up])]\n        ind_ws2 = i_low[np.argmin(minus_y_times_grad[i_low])]\n        return ind_ws1, ind_ws2\n\n    def fit(self, x, y):\n        self._init_solution(y)\n        self._cache = {}\n        num, _ = x.shape\n        # Initialize the dual coefficients and gradient\n        self._grad = - np.ones(num)\n        # Start the iterations of SMO algorithm\n        for itr in xrange(self.max_iter):\n            # Select two indices of variables as working set\n            ind_ws1, ind_ws2 = self._select_working_set1(y)\n            # Check stopping criteria: m(a_k) <= M(a_k) + tolerance\n            m_lb = - y[ind_ws1] * self._grad[ind_ws1]\n            m_ub = - y[ind_ws2] * self._grad[ind_ws2]\n            kkt_violation = m_lb - m_ub\n            # print 'KKT Violation:', kkt_violation\n            if kkt_violation <= self.tol:\n                print 'Converged!', 'Iter:', itr, 'KKT Violation:', kkt_violation\n                break\n            # Compute (or get from cache) two columns of gram matrix\n            if ind_ws1 in self._cache:\n                qi = self._cache[ind_ws1]\n            else:\n                qi = self.kernel(x, x[ind_ws1]).ravel() * y * y[ind_ws1]\n                self._cache[ind_ws1] = qi\n            if ind_ws2 in self._cache:\n                qj = self._cache[ind_ws2]\n            else:\n                qj = self.kernel(x, x[ind_ws2]).ravel() * y * y[ind_ws2]\n                self._cache[ind_ws2] = qj\n            # Construct sub-problem\n            qii, qjj, qij = qi[ind_ws1], qj[ind_ws2], qi[ind_ws2]\n            # Solve sub-problem\n            if y[ind_ws1] * y[ind_ws2] > 0:  # The case where y_i equals y_j\n                v1, v2 = 1., -1.\n                d_max = min(self.C - self._alpha[ind_ws1], self._alpha[ind_ws2])\n                d_min = max(-self._alpha[ind_ws1], self._alpha[ind_ws2] - self.C)\n            else:  # The case where y_i equals y_j\n                v1, v2 = 1., 1.\n                d_max = min(self.C - self._alpha[ind_ws1], self.C - self._alpha[ind_ws2])\n                d_min = max(-self._alpha[ind_ws1], -self._alpha[ind_ws2])\n            quad_coef = v1**2 * qii + v2**2 * qjj + 2 * v1 * v2 * qij\n            quad_coef = max(quad_coef, self._TAU)\n            d = - (self._grad[ind_ws1] * v1 + self._grad[ind_ws2] * v2) / quad_coef\n            d = max(min(d, d_max), d_min)\n            # Update dual coefficients\n            self._alpha[ind_ws1] += d * v1\n            self._alpha[ind_ws2] += d * v2\n            # Update the gradient\n            self._grad += d * v1 * qi + d * v2 * qj\n            # Update I_up with respect to ind_ws1 and ind_ws2\n            self._update_iup_and_ilow(y, ind_ws1)\n            self._update_iup_and_ilow(y, ind_ws2)\n        else:\n            print 'Exceed maximum iteration'\n            print 'KKT Violation:', kkt_violation\n        # Set results after optimization procedure\n        self._set_result(x, y)\n        self.intercept_ = (m_lb + m_ub) / 2.\n        self.itr = itr + 1\n\n    def _update_iup_and_ilow(self, y, ind):\n        # Update I_up with respect to ind\n        if (y[ind] > 0) and (self._alpha[ind] / self.C <= 1 - self._EPS):\n            self._i_up[ind] = True\n        elif (y[ind] < 0) and (self._EPS <= self._alpha[ind] / self.C):\n            self._i_up[ind] = True\n        else:\n            self._i_up[ind] = False\n        # Update I_low with respect to ind\n        if (y[ind] > 0) and (self._EPS <= self._alpha[ind] / self.C):\n            self._i_low[ind] = True\n        elif (y[ind] < 0) and (self._alpha[ind] / self.C <= 1 - self._EPS):\n            self._i_low[ind] = True\n        else:\n            self._i_low[ind] = False\n\n    def _set_result(self, x, y):\n        self.support_ = np.where(self._EPS < (self._alpha / self.C))[0]\n        self.support_vectors_ = x[self.support_]\n        self.dual_coef_ = self._alpha[self.support_] * y[self.support_]\n        # Compute w when using linear kernel\n        if self.kernel == linear_kernel:\n            self.coef_ = np.sum(self.dual_coef_ * x[self.support_].T, axis=1)\n\n    def decision_function(self, x):\n        return np.sum(self.kernel(x, self.support_vectors_) * self.dual_coef_, axis=1) + self.intercept_\n\n    def predict(self, x):\n        return np.sign(self.decision_function(x))\n\n    def score(self, x, y):\n        return sum(self.decision_function(x) * y > 0) / float(len(y))\n\n\nif __name__ == '__main__':\n    # Create toy problem\n    np.random.seed(0)\n    num_p = 15\n    num_n = 15\n    dim = 2\n    x_p = np.random.multivariate_normal(np.ones(dim) * 1, np.eye(dim), num_p)\n    x_n = np.random.multivariate_normal(np.ones(dim) * 2, np.eye(dim), num_n)\n    x = np.vstack([x_p, x_n])\n    y = np.array([1.] * num_p + [-1.] * num_n)\n\n    # Set parameters\n    max_iter = 500000\n    C = 1e0\n    gamma = 0.005\n    tol = 1e-3\n\n    # Set kernel function\n    # kernel = get_rbf_kernel(gamma)\n    # kernel = linear_kernel\n    # kernel = 'rbf'\n    # kernel = 'polynomial'\n    kernel = 'linear'\n\n    # Create object\n    csvc = CSvc(C=C, kernel=kernel, max_iter=max_iter, tol=tol)\n\n    # Run SMO algorithm\n    csvc.fit(x, y)\n\n\nLIBSVM \u3068\u901f\u5ea6\u6bd4\u8f03\u3057\u3066\u307f\u308b\n\u672c\u5bb6 LIBSVM\uff0cscikit-learn\uff0c\u81ea\u524d\u306e\u5b9f\u88c5\uff0c\u306e 3 \u3064\u3092 1 \u56de\u52dd\u8ca0\u3067\u96d1\u306b\u6bd4\u3079\u3066\u307f\u305f\uff0e\n\u4f7f\u7528\u3057\u305f\u306e\u306f\u3053\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff0e\nscikit-learn \u306f\u4e2d\u8eab\u306f LIBSVM \u3060\u3051\u3069\uff0cPython \u3068\u306e\u7e4b\u304e\u304c\u672c\u5bb6\u306f ctypes \u306a\u306e\u306b\u5bfe\u3057\u3066 scikit-learn \u306f Cython \u3092\u4f7f\u3063\u3066\u3044\u308b\u3068\u304b\u4f55\u3068\u304b?\n\u8a73\u3057\u3044\u3053\u3068\u306f\u3042\u3093\u307e\u308a\u77e5\u3089\u306a\u3044\uff0e\n\u4f7f\u3063\u305f\u306e\u306f linear kernel \u3067\uff0c\u8868\u306e\u6570\u5024\u306e\u5358\u4f4d\u306f sec\uff0e\n\n\n\n\n\u81ea\u524d\u5b9f\u88c5\nscikit-learn\nLIBSVM\n\n\n\n\nmushrooms\n0.374\n0.320\n0.907\n\n\na1a\n0.729\n0.332\n0.663\n\n\ndiabetes\n0.0795\n0.015\n0.0451\n\n\nliver\n0.0243\n0.0031\n0.0109\n\n\nsplice\n2.28\n0.49\n1.10\n\n\nsvmguide3\n0.175\n0.483\n0.148\n\n\n\n\u672c\u5bb6 LIBSVM \u306b\u306f\u304b\u306a\u308a\u8feb\u3063\u3066\u3044\u308b\u304c\uff0cscikit-learn \u306e\u8b0e\u306e\u901f\u3055\u3088\uff0e\n\u305d\u308c\u3067\u3082 Python + NumPy \u3060\u3051\u3067\u30ac\u30c1\u74b0\u5883 (?) \u306b\u8010\u3048\u5f97\u308b\u901f\u5ea6\u304c\u51fa\u305b\u3066\u3044\u308b\u611f\u3058\u304c\u3059\u308b\u306e\u3067\u7d50\u69cb\u9811\u5f35\u3063\u305f\u3068\u601d\u3046\uff0e\n\n\u307e\u3068\u3081\n\n\u3053\u308c\u3092\u30d9\u30fc\u30b9\u306b SVM \u3092\u62e1\u5f35\u3057\u305f\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u8272\u3005\u66f8\u304d\u305f\u3044\n\u307e\u3060\u901f\u304f\u3057\u305f\u3044\nPython \u3067\u3082\u7d50\u69cb\u9811\u5f35\u308c\u308b\u3063\u307d\u3044\u306e\u3067\u7686\u3067\u9811\u5f35\u308d\u3046\n\nPython + NumPy \u3060\u3051\u3067\uff0c\u672c\u6c17\u3067\u901f\u5ea6\u3092\u8ffd\u6c42\u3057\u305f SVM \u3092\u66f8\u3044\u3066\u307f\u305f\uff0e\n\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f [LIVSVM \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)\u3068[\u95a2\u9023\u8ad6\u6587](https://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf)\u306b\u5023\u3063\u3066\u8272\u3005\u3068\u5de5\u592b\u3092\u53d6\u308a\u5165\u308c\u305f SMO\uff0e\n\nWorking set \u306e\u9078\u629e (\u5404\u53cd\u5fa9\u3067\u90e8\u5206\u554f\u984c\u306b\u4f7f\u3046 2 \u5909\u6570\u3092\u9078\u3076\u30a2\u30ec) \u306f\u5b9f\u88c5\u306e\u3057\u6613\u3055\u3092\u512a\u5148\u3057\u3066\u5c11\u3057\u53e4\u3044\u624b\u6cd5 (LIBSVM ver. 2.8 \u304f\u3089\u3044\u307e\u3067\u4f7f\u308f\u308c\u3066\u3044\u305f\u3084\u3064) \u3092\u63a1\u7528\u3057\u305f\u304c\uff0c\u65b0\u3057\u3044\u3084\u3064\u304c\u5fc5\u305a\u3057\u3082\u901f\u3044\u4fdd\u8a3c\u306f\u306a\u3044\u3057\u307e\u3042\u826f\u3044\u304b\u306a\u3068\u3044\u3046\u611f\u3058\uff0e\n\n# \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\u672c\u5f53\u306f\u4e00\u4eba\u3067\u697d\u3057\u3080\u4e88\u5b9a\u3060\u3063\u305f\u3051\u308c\u3069\uff0c\u6848\u5916\u5b9f\u7528\u30ec\u30d9\u30eb\u306e\u901f\u5ea6\u304c\u51fa\u305f\u306e\u3067\u7686\u3068\u559c\u3073\u3092\u5206\u304b\u3061\u5408\u3046\u3053\u3068\u306b\u3057\u305f\uff0e\n\n\u3067\u3082\uff0c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8aac\u660e\u3092\u66f8\u304f\u3068\u3081\u3063\u3061\u3083\u9577\u304f\u306a\u308a\u305d\u3046\u306a\u306e\u3067\uff0c\u3072\u3068\u307e\u305a\u59a5\u5354\u3057\u3066\u30b3\u30fc\u30c9\u3060\u3051\u8cbc\u308b\u3053\u3068\u306b\u3057\u305f\uff0e\n\u307e\u3060\u5b9f\u88c5\u3067\u304d\u3066\u3044\u306a\u3044\u6a5f\u80fd\u304c\u6ca2\u5c71\u3042\u308b\u3051\u3069\uff0c\u57fa\u672c\u7684\u306b\u4f7f\u3044\u65b9\u306f scikit-learn \u3068\u307b\u307c\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u306f\u305a\uff0e\n\n```py\n# -*- coding: utf-8 -*-\n\nimport numpy as np\n\n\ndef linear_kernel(x1, x2):\n    x1 = np.atleast_2d(x1)\n    x2 = np.atleast_2d(x2)\n    return np.dot(x1, x2.T)\n\n\n# k(x, y) = exp(- gamma || x1 - x2 ||^2)\ndef get_rbf_kernel(gamma):\n    def rbf_kernel(x1, x2):\n        x1 = np.atleast_2d(x1)\n        x2 = np.atleast_2d(x2)\n        m1, _ = x1.shape\n        m2, _ = x2.shape\n        norm1 = np.dot(np.ones([m2, 1]), np.atleast_2d(np.sum(x1 ** 2, axis=1))).T\n        norm2 = np.dot(np.ones([m1, 1]), np.atleast_2d(np.sum(x2 ** 2, axis=1)))\n        return np.exp(- gamma * (norm1 + norm2 - 2 * np.dot(x1, x2.T)))\n    return rbf_kernel\n\n\n# k(x1, x2) = (<x1, x2> + coef0)^degree\ndef get_polynomial_kernel(degree, coef0):\n    def polynomial_kernel(x1, x2):\n        x1 = np.atleast_2d(x1)\n        x2 = np.atleast_2d(x2)\n        return (np.dot(x1, x2.T) + coef0)**degree\n    return polynomial_kernel\n\n\nclass CSvc:\n\n    def __init__(self, C=1e0, kernel=linear_kernel, tol=1e-3, max_iter=1000,\n                 gamma=1e0, degree=3, coef0=0):\n        self._EPS = 1e-5\n        self._TAU = 1e-12\n        self._cache = {}\n        self.tol = tol\n        self.max_iter = max_iter\n        self.C = C\n        self.gamma = gamma\n        self.degree, self.coef0 = degree, coef0\n        self.kernel = None\n        self._alpha = None\n        self.intercept_ = None\n        self._grad = None\n        self.itr = None\n        self._ind_y_p, self._ind_y_n = None, None\n        self._i_low, self._i_up = None, None\n        self.set_kernel_function(kernel)\n\n    def _init_solution(self, y):\n        num = len(y)\n        self._alpha = np.zeros(num)\n        self._i_low = y < 0\n        self._i_up = y > 0\n\n    def set_kernel_function(self, kernel):\n        if callable(kernel):\n            self.kernel = kernel\n        elif kernel == 'linear':\n            self.kernel = linear_kernel\n        elif kernel == 'rbf' or kernel == 'gaussian':\n            self.kernel = get_rbf_kernel(self.gamma)\n        elif kernel == 'polynomial' or kernel == 'poly':\n            self.kernel = get_polynomial_kernel(self.degree, self.coef0)\n        else:\n            raise ValueError('{} is undefined name as kernel function'.format(kernel))\n\n    def _select_working_set1(self, y):\n        minus_y_times_grad = - y * self._grad\n        # Convert boolean mask to index\n        i_up = self._i_up.nonzero()[0]\n        i_low = self._i_low.nonzero()[0]\n        ind_ws1 = i_up[np.argmax(minus_y_times_grad[i_up])]\n        ind_ws2 = i_low[np.argmin(minus_y_times_grad[i_low])]\n        return ind_ws1, ind_ws2\n\n    def fit(self, x, y):\n        self._init_solution(y)\n        self._cache = {}\n        num, _ = x.shape\n        # Initialize the dual coefficients and gradient\n        self._grad = - np.ones(num)\n        # Start the iterations of SMO algorithm\n        for itr in xrange(self.max_iter):\n            # Select two indices of variables as working set\n            ind_ws1, ind_ws2 = self._select_working_set1(y)\n            # Check stopping criteria: m(a_k) <= M(a_k) + tolerance\n            m_lb = - y[ind_ws1] * self._grad[ind_ws1]\n            m_ub = - y[ind_ws2] * self._grad[ind_ws2]\n            kkt_violation = m_lb - m_ub\n            # print 'KKT Violation:', kkt_violation\n            if kkt_violation <= self.tol:\n                print 'Converged!', 'Iter:', itr, 'KKT Violation:', kkt_violation\n                break\n            # Compute (or get from cache) two columns of gram matrix\n            if ind_ws1 in self._cache:\n                qi = self._cache[ind_ws1]\n            else:\n                qi = self.kernel(x, x[ind_ws1]).ravel() * y * y[ind_ws1]\n                self._cache[ind_ws1] = qi\n            if ind_ws2 in self._cache:\n                qj = self._cache[ind_ws2]\n            else:\n                qj = self.kernel(x, x[ind_ws2]).ravel() * y * y[ind_ws2]\n                self._cache[ind_ws2] = qj\n            # Construct sub-problem\n            qii, qjj, qij = qi[ind_ws1], qj[ind_ws2], qi[ind_ws2]\n            # Solve sub-problem\n            if y[ind_ws1] * y[ind_ws2] > 0:  # The case where y_i equals y_j\n                v1, v2 = 1., -1.\n                d_max = min(self.C - self._alpha[ind_ws1], self._alpha[ind_ws2])\n                d_min = max(-self._alpha[ind_ws1], self._alpha[ind_ws2] - self.C)\n            else:  # The case where y_i equals y_j\n                v1, v2 = 1., 1.\n                d_max = min(self.C - self._alpha[ind_ws1], self.C - self._alpha[ind_ws2])\n                d_min = max(-self._alpha[ind_ws1], -self._alpha[ind_ws2])\n            quad_coef = v1**2 * qii + v2**2 * qjj + 2 * v1 * v2 * qij\n            quad_coef = max(quad_coef, self._TAU)\n            d = - (self._grad[ind_ws1] * v1 + self._grad[ind_ws2] * v2) / quad_coef\n            d = max(min(d, d_max), d_min)\n            # Update dual coefficients\n            self._alpha[ind_ws1] += d * v1\n            self._alpha[ind_ws2] += d * v2\n            # Update the gradient\n            self._grad += d * v1 * qi + d * v2 * qj\n            # Update I_up with respect to ind_ws1 and ind_ws2\n            self._update_iup_and_ilow(y, ind_ws1)\n            self._update_iup_and_ilow(y, ind_ws2)\n        else:\n            print 'Exceed maximum iteration'\n            print 'KKT Violation:', kkt_violation\n        # Set results after optimization procedure\n        self._set_result(x, y)\n        self.intercept_ = (m_lb + m_ub) / 2.\n        self.itr = itr + 1\n\n    def _update_iup_and_ilow(self, y, ind):\n        # Update I_up with respect to ind\n        if (y[ind] > 0) and (self._alpha[ind] / self.C <= 1 - self._EPS):\n            self._i_up[ind] = True\n        elif (y[ind] < 0) and (self._EPS <= self._alpha[ind] / self.C):\n            self._i_up[ind] = True\n        else:\n            self._i_up[ind] = False\n        # Update I_low with respect to ind\n        if (y[ind] > 0) and (self._EPS <= self._alpha[ind] / self.C):\n            self._i_low[ind] = True\n        elif (y[ind] < 0) and (self._alpha[ind] / self.C <= 1 - self._EPS):\n            self._i_low[ind] = True\n        else:\n            self._i_low[ind] = False\n\n    def _set_result(self, x, y):\n        self.support_ = np.where(self._EPS < (self._alpha / self.C))[0]\n        self.support_vectors_ = x[self.support_]\n        self.dual_coef_ = self._alpha[self.support_] * y[self.support_]\n        # Compute w when using linear kernel\n        if self.kernel == linear_kernel:\n            self.coef_ = np.sum(self.dual_coef_ * x[self.support_].T, axis=1)\n\n    def decision_function(self, x):\n        return np.sum(self.kernel(x, self.support_vectors_) * self.dual_coef_, axis=1) + self.intercept_\n\n    def predict(self, x):\n        return np.sign(self.decision_function(x))\n\n    def score(self, x, y):\n        return sum(self.decision_function(x) * y > 0) / float(len(y))\n\n\nif __name__ == '__main__':\n    # Create toy problem\n    np.random.seed(0)\n    num_p = 15\n    num_n = 15\n    dim = 2\n    x_p = np.random.multivariate_normal(np.ones(dim) * 1, np.eye(dim), num_p)\n    x_n = np.random.multivariate_normal(np.ones(dim) * 2, np.eye(dim), num_n)\n    x = np.vstack([x_p, x_n])\n    y = np.array([1.] * num_p + [-1.] * num_n)\n\n    # Set parameters\n    max_iter = 500000\n    C = 1e0\n    gamma = 0.005\n    tol = 1e-3\n\n    # Set kernel function\n    # kernel = get_rbf_kernel(gamma)\n    # kernel = linear_kernel\n    # kernel = 'rbf'\n    # kernel = 'polynomial'\n    kernel = 'linear'\n\n    # Create object\n    csvc = CSvc(C=C, kernel=kernel, max_iter=max_iter, tol=tol)\n\n    # Run SMO algorithm\n    csvc.fit(x, y)\n```\n\n# LIBSVM \u3068\u901f\u5ea6\u6bd4\u8f03\u3057\u3066\u307f\u308b\n\u672c\u5bb6 LIBSVM\uff0cscikit-learn\uff0c\u81ea\u524d\u306e\u5b9f\u88c5\uff0c\u306e 3 \u3064\u3092 1 \u56de\u52dd\u8ca0\u3067\u96d1\u306b\u6bd4\u3079\u3066\u307f\u305f\uff0e\n\u4f7f\u7528\u3057\u305f\u306e\u306f[\u3053\u3053](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff0e\n\nscikit-learn \u306f\u4e2d\u8eab\u306f LIBSVM \u3060\u3051\u3069\uff0cPython \u3068\u306e\u7e4b\u304e\u304c\u672c\u5bb6\u306f ctypes \u306a\u306e\u306b\u5bfe\u3057\u3066 scikit-learn \u306f Cython \u3092\u4f7f\u3063\u3066\u3044\u308b\u3068\u304b\u4f55\u3068\u304b?\n\u8a73\u3057\u3044\u3053\u3068\u306f\u3042\u3093\u307e\u308a\u77e5\u3089\u306a\u3044\uff0e\n\n\u4f7f\u3063\u305f\u306e\u306f linear kernel \u3067\uff0c\u8868\u306e\u6570\u5024\u306e\u5358\u4f4d\u306f sec\uff0e\n\n|  | \u81ea\u524d\u5b9f\u88c5 | scikit-learn | LIBSVM |\n|:-:|:----------|:------------|:------------|\n| mushrooms | 0.374  | 0.320  | 0.907  |\n| a1a | 0.729  | 0.332  | 0.663  |\n| diabetes | 0.0795  | 0.015  | 0.0451  |\n| liver | 0.0243  | 0.0031  | 0.0109  |\n| splice | 2.28  | 0.49  | 1.10 |\n| svmguide3 | 0.175 | 0.483 | 0.148 |\n\n\u672c\u5bb6 LIBSVM \u306b\u306f\u304b\u306a\u308a\u8feb\u3063\u3066\u3044\u308b\u304c\uff0cscikit-learn \u306e\u8b0e\u306e\u901f\u3055\u3088\uff0e\n\u305d\u308c\u3067\u3082 Python + NumPy \u3060\u3051\u3067\u30ac\u30c1\u74b0\u5883 (?) \u306b\u8010\u3048\u5f97\u308b\u901f\u5ea6\u304c\u51fa\u305b\u3066\u3044\u308b\u611f\u3058\u304c\u3059\u308b\u306e\u3067\u7d50\u69cb\u9811\u5f35\u3063\u305f\u3068\u601d\u3046\uff0e\n\n# \u307e\u3068\u3081\n* \u3053\u308c\u3092\u30d9\u30fc\u30b9\u306b SVM \u3092\u62e1\u5f35\u3057\u305f\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u8272\u3005\u66f8\u304d\u305f\u3044\n* \u307e\u3060\u901f\u304f\u3057\u305f\u3044\n* Python \u3067\u3082\u7d50\u69cb\u9811\u5f35\u308c\u308b\u3063\u307d\u3044\u306e\u3067\u7686\u3067\u9811\u5f35\u308d\u3046\n", "tags": ["svm", "\u6a5f\u68b0\u5b66\u7fd2", "\u6570\u5b66", "Python", "numpy"]}