{"context": " More than 1 year has passed since last update.NTT\u30c7\u30fc\u30bf\u3055\u3093\u304c\u66f8\u3044\u305fSpark\u5165\u9580\u306e5.5\u7ae0\u306e\u554f\u984c\u3092Scala\u3067\u306f\u306a\u304f\u3001Python3\u3067\u66f8\u304d\u76f4\u3057\u305f\u969b\u306e\u30e1\u30e2\u3002\u4ee5\u4e0b\u306e\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30f3\u30b1\u30fc\u30c8\u96c6\u8a08\u306eSpark\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8ff0\u3002\n\u30fbRDD\u306e\u6c38\u7d9a\u5316(persist\u3001cache)\n\u30fb\u30ab\u30a6\u30f3\u30c8\uff08count)\n\u30fb\u5185\u90e8\u8981\u7d20\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\uff08sum\uff09\n\u30fbRDD\u306b\u542b\u307e\u308c\u308b\u8981\u7d20\u3059\u3079\u3066\u306b\u5bfe\u3059\u308b\u96c6\u7d04\u51e6\u7406\uff08reduce\uff09\n\u30fb\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\uff08accumulator\uff09\n\n0. \u6e96\u5099\n\u6271\u3046\u30c7\u30fc\u30bf\u5f62\u5f0f\u3002\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092HDFS\u4e0a\u306b\u683c\u7d0d\u3002\n\nquestionaire.csv\n\u5e74\u9f62\u3001\u6027\u5225\u3001\u8a55\u4fa1\n\n\n\n1. \u30a2\u30f3\u30b1\u30fc\u30c8\u96c6\u8a08\u30d7\u30ed\u30b0\u30e9\u30e0\u5168\u4f53\n\u8a55\u4fa1\u3092\u5168\u4f53\u306e\u5e73\u5747\u3001\u5e74\u4ee3\u5225\u306e\u5e73\u5747\u3001\u6027\u5225\u3054\u3068\u306e\u5e73\u5747\u306e\u8ef8\u3067\u96c6\u8a08\u3059\u308b\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\u3002\n\nchap5-2.py\n# -*- coding:utf-8 -*-\nfrom __future__ import print_function\nimport sys\nimport io\nfrom operator import add\nfrom pyspark import SparkContext\nif __name__ == \"__main__\":\n    if len(sys.argv) != 1:\n        print(\"Usage: examplent\")\n        exit(-1)\n    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n\n    fileQ = \"hdfs:///user/y_tadayasu/data/questionnaire.csv\"\n    sc = SparkContext(appName=\"chap5-2\")\n    questionRDD = sc.textFile(fileQ).map(lambda x: x.split(\",\"))\n    # RDD Persistence\n    #questionRDD.persist()\n    questionRDD.cache() # Persist this RDD with the default storage level (MEMORY_ONLY_SER).\n    #######################################\n    # draft-1: use count and sum method\n    #######################################\n    count = questionRDD.count()\n\n    #totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n    totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n\n    print(\"AVZ=\",totalPoints/count)\n\n    #######################################\n    # draft-2: use reduce\n    #######################################\n    (totalPoints,numQuestionare) = questionRDD.map(lambda x:(x[2],1)).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n    print(\"AVZ=%f\" %(totalPoints/numQuestionare))\n\n    ageRDD = questionRDD.map(lambda x:(int(x[0])//10*10,(x[2],1))).reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n    for i in ageRDD.collect():\n       (age,(agePoint,ageCount)) = i\n       print(\"AVG Age Range(%s):%.2f\" %(age,agePoint/ageCount))\n\n    #######################################\n    # calculate average about male and female\n    #######################################\n    numMAcc = sc.accumulator(0)\n    totalPointMAcc = sc.accumulator(0)\n    numFAcc = sc.accumulator(0)\n    totalPointFAcc = sc.accumulator(0)\n\n    mfRDD = questionRDD.map(lambda x:(x[1],x[2])).foreach(lambda x: (numMAcc.add(1),totalPointMAcc.add(int(x[1]))) if x[0] == 'M' else (numFAcc.add(1),totalPointFAcc.add(int(x[1]))))\n    print(\"AVG Male is %f\" %(totalPointMAcc.value/numMAcc.value))\n    print(\"AVG Female is %f\" %(totalPointFAcc.value/numFAcc.value))\n    sc.stop()\n\n\n\u30b8\u30e7\u30d6\u306e\u5b9f\u884c\u3002\n$ spark-submit --master yarn-client chap5-2.py \nAVZ= 3.3684210526315788                                                         \nAVZ=3.368421\nAVG Age Range(40):2.50\nAVG Age Range(10):4.00\nAVG Age Range(20):3.71\nAVG Age Range(50):1.50\nAVG Age Range(30):3.50\nAVG Male is 3.500000\nAVG Female is 3.272727\n\n\n2. \u30d7\u30ed\u30b0\u30e9\u30e0\u8aac\u660e\n\n2.1.RDD\u306e\u6c38\u7d9a\u5316\n\u6bce\u56decsv\u304b\u3089RDD\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3059\u308b\u306e\u306f\u52b9\u7387\u304c\u60aa\u3044\u306e\u3067\u3001\u6c38\u7d9a\u5316\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u305d\u306e\u51e6\u7406\u3092\u52b9\u7387\u5316\u3059\u308b\uff08\u6700\u521d\u306e\u30b8\u30e7\u30d6\u304c\u5b9f\u884c\u3055\u308c\u305f\u3068\u304d\u3001\u5404\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u304c\u30a8\u30b0\u30bc\u30ad\u30e5\u30fc\u30bf\u4e0a\u306b\u6c38\u7d9a\u5316\u3059\u308b\uff09\u3002\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u306b\u3088\u308a\u3001\u6c38\u7d9a\u5316\u5148\u3092\u30e1\u30e2\u30ea\u4e0a\u3084\u30c7\u30a3\u30b9\u30af\u306a\u3069\u9078\u629e\u3067\u304d\u308b\u3002\u30e1\u30e2\u30ea\u3060\u3051\u306b\u6c38\u7d9a\u5316\u3059\u308b\u5834\u5408\u306fpersist\u30e1\u30bd\u30c3\u30c9\u3067\u306f\u306a\u304fcache\u30e1\u30bd\u30c3\u30c9\u304c\u4f7f\u3048\u308b\u3002\n\u4f7f\u7528\u53ef\u80fd\u306a\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://spark.apache.org/docs/latest/programming-guide.html\n  fileQ = \"hdfs:///user/y_tadayasu/data/questionnaire.csv\"\n    sc = SparkContext(appName=\"chap5-2\")\n    questionRDD = sc.textFile(fileQ).map(lambda x: x.split(\",\"))\n    # RDD Persistence\n    # persist\u30e1\u30bd\u30c3\u30c9\u3067\u6c38\u7d9a\u5316\u3067\u304d\u308b\u304c\u3001\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u4e0a\u3060\u3051\u306b\u6c38\u7d9a\u5316\u3059\u308b\u306e\u3067cache\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u7528\n    #questionRDD.persist()\n    questionRDD.cache() \n\n\n2.2. \u8981\u7d20\u6570\u306e\u30ab\u30a6\u30f3\u30c8\u3068\u96c6\u8a08\u3000count\u30e1\u30bd\u30c3\u30c9\u3001sum\u30e1\u30bd\u30c3\u30c9\ncount\u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308aRDD\u306b\u542b\u307e\u308c\u308b\u8981\u7d20\u6570\u306e\u6570\u3092\u53d6\u5f97\u3059\u308b\u30a2\u30af\u30b7\u30e7\u30f3\u3002\nsum\u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308a\u6570\u5024\u578b\u306e\u8981\u7d20\u3092\u6301\u3064RDD\u3092\u5bfe\u8c61\u306b\u3001\u8981\u7d20\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\u3059\u308b\u30a2\u30af\u30b7\u30e7\u30f3\u3002\n    count = questionRDD.count()\n\n    #totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n    totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n\n    print(\"AVZ=\",totalPoints/count)\n\n\n2.3. \u8981\u7d20\u6570\u306e\u30ab\u30a6\u30f3\u30c8\u3068\u96c6\u8a08\u3000reduce\u30e1\u30bd\u30c3\u30c9\u3001foreach\u30e1\u30bd\u30c3\u30c9\nmap\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066RDD\u306e\u5168\u3066\u306e\u8981\u7d20\u3092\u3001(\u8a55\u4fa1,1)\u306e\u30bf\u30d7\u30eb\u306b\u5909\u63db\u3057\u3001reduce\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066\u8a55\u4fa1\u306e\u5408\u8a08\u5024\u3068\u8a55\u4fa1\u6570\u306e\u5408\u8a08\u5024\u3092\u96c6\u8a08\u3059\u308b\u3002\n(totalPoints,numQuestionare) = questionRDD.map(lambda x:(x[2],1)).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\nprint(\"AVZ=%f\" %(totalPoints/numQuestionare))\n\n\u5404\u5e74\u4ee3\u5225\u306e\u8a55\u4fa1\u306b\u95a2\u3057\u3066\u306f\u3001(\u5e74\u4ee3\u3001(\u8a55\u4fa1,1))\u306e\u30bf\u30d7\u30eb\u306b\u5909\u63db\u3057\u3001reduceByKey\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066\u5e74\u4ee3\u6bce\u306e\u8a55\u4fa1\u306e\u5408\u8a08\u5024\u3068\u8a55\u4fa1\u6570\u306e\u5408\u8a08\u5024\u3092\u96c6\u8a08\u3059\u308b\u3002\nageRDD = questionRDD.map(lambda x:(int(x[0])//10*10,(x[2],1))).reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\nfor i in ageRDD.collect():\n    (age,(agePoint,ageCount)) = i\n    print(\"AVG Age Range(%s):%.2f\" %(age,agePoint/ageCount))\n\nreduce\u30e1\u30bd\u30c3\u30c9\u3068foreach\u30e1\u30bd\u30c3\u30c9\u306b\u3064\u3044\u3066\u306fslideshare\u306b\u307e\u3068\u3081\u308b\u4e88\u5b9a\u3002\nreduce\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3046\u969b\u306e\u30c7\u30fc\u30bf\u51e6\u7406\u306b\u95a2\u3057\u3066\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u78ba\u8a8d\u3002\n\u30bf\u30d7\u30eb\u306a\u306e\u3067\u5358\u306b\u52a0\u7b97\u3059\u308b\u3068\u6587\u5b57\u306e\u9023\u7d50\u306b\u306a\u308b\u306e\u3067\u3001int\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u3042\u308a\u3002\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\")\n>>> txtRDD.collect()\n['1,2', '3,4', '5,6', '7,8', '9,10']\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\"))\n>>> txtRDD.collect()\n[['1', '2'], ['3', '4'], ['5', '6'], ['7', '8'], ['9', '10']]\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:x)\n>>> print(txtRDD)\n['1', '2']\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:y)\n>>> print(txtRDD)\n['9', '10']\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n>>> print(txtRDD)\n('13579', '246810')\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n>>> print(txtRDD)\n(25, 30)\n\n\n2.4. \u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30ab\u30a6\u30f3\u30c8\n\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u306f\u30c9\u30e9\u30a4\u30d0\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u306f\u5024\u306e\u8a2d\u5b9a\u3068\u53c2\u7167\u3092\u884c\u3044\u3001\u30a8\u30b0\u30bc\u30ad\u30e5\u30fc\u30bf\u4e0a\u3067\u5b9f\u884c\u3055\u308c\u308b\u30bf\u30b9\u30af\u304b\u3089\u306f\u5024\u306e\u52a0\u7b97\u306e\u307f\u3092\u884c\u3046\u3053\u3068\u3092\u60f3\u5b9a\u3057\u305f\u5171\u6709\u5909\u6570\u3002\u4f55\u304b\u306e\u5024\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081\u3060\u3051\u306b\u5bb9\u6613\u3055\u308c\u305f\u4ed5\u7d44\u307f\u3060\u3068\u601d\u308f\u308c\u308b\u3002\naccumulator\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u7528\u3057\u3001\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u3092\u4f7f\u7528\u30021\u3064\u76ee\u306e\u5f15\u6570\u306f\u521d\u671f\u5024\u3002\n\u6027\u5225\u304cM\u306a\u3089numMAcc(\u7537\u6027\u306e\u6570)\u306b1\u3092\u52a0\u7b97\u3057\u3001totalPointMAcc(\u7537\u6027\u306e\u7dcf\u30dd\u30a4\u30f3\u30c8)\u306b\u8a55\u4fa1\u3092\u3059\u308b\u3002M\u4ee5\u5916\u306e\u5834\u5408\u306f\u5973\u6027\u306e\u65b9\u306b\u540c\u3058\u51e6\u7406\u3092\u884c\u3046\u3002\nnumMAcc = sc.accumulator(0)\ntotalPointMAcc = sc.accumulator(0)\nnumFAcc = sc.accumulator(0)\ntotalPointFAcc = sc.accumulator(0)\n\nmfRDD = questionRDD.map(lambda x:(x[1],x[2])).foreach(lambda x: (numMAcc.add(1),totalPointMAcc.add(int(x[1]))) if x[0] == 'M' else (numFAcc.add(1),totalPointFAcc.add(int(x[1]))))\n\nprint(\"AVG Male is %f\" %(totalPointMAcc.value/numMAcc.value))\nprint(\"AVG Female is %f\" %(totalPointFAcc.value/numFAcc.value))\n\n\n3. \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\n\u30c7\u30d0\u30c3\u30b0\u3082\u517c\u306d\u3066\u4f55\u56de\u304b\u30b8\u30e7\u30d6\u3092\u5b9f\u884c\u3057\u3088\u3046\u3068\u3057\u305f\u3089\u3001\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u529b\u3055\u308c\u30b8\u30e7\u30d6\u304c\u5931\u6557\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n5/12/01 00:42:56 ERROR SparkContext: Error initializing SparkContext.\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/y_tadayasu/.sparkStaging/application_1448\n929618763_0004/spark-assembly-1.5.2-hadoop2.6.0.jar could only be replicated to 0 nodes instead of minReplicatio\nn (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.\n\n\u8abf\u3079\u3066\u307f\u308b\u3068HDFS\u3092\u69cb\u6210\u3057\u3066\u3044\u308b\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u304c100%\u8fd1\u304f\u306b\u306a\u3063\u3066\u3044\u305f\u3002\u3069\u306e\u9818\u57df\u304c\u30cd\u30c3\u30af\u306b\u306a\u3063\u3066\u3044\u308b\u304b\u8abf\u3079\u3066\u3044\u304f\u3068DataNode\u306e\u30d5\u30a1\u30a4\u30eb\u30ad\u30e3\u30c3\u30b7\u30e5\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u5bb9\u91cf\u3092\u903c\u8feb\u3055\u305b\u3066\u3044\u305f\u306e\u3067\u3001\u3068\u308a\u3042\u3048\u305a\u524a\u9664\u3002HDFS\u3092\u3061\u3083\u3093\u3068\u904b\u7528\u3059\u308b\u305f\u3081\u306b\u6700\u5927\u5024\u3092\u8a2d\u5b9a\u3059\u308b\u306a\u3069\u3057\u305f\u65b9\u304c\u826f\u3055\u305d\u3046\u3002\n/hadoop/yarn/node-manager/local/usercache/y_tadayasu/filecache\nNTT\u30c7\u30fc\u30bf\u3055\u3093\u304c\u66f8\u3044\u305fSpark\u5165\u9580\u306e5.5\u7ae0\u306e\u554f\u984c\u3092Scala\u3067\u306f\u306a\u304f\u3001Python3\u3067\u66f8\u304d\u76f4\u3057\u305f\u969b\u306e\u30e1\u30e2\u3002\u4ee5\u4e0b\u306e\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30f3\u30b1\u30fc\u30c8\u96c6\u8a08\u306eSpark\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8ff0\u3002\n\u30fbRDD\u306e\u6c38\u7d9a\u5316(persist\u3001cache)\n\u30fb\u30ab\u30a6\u30f3\u30c8\uff08count)\n\u30fb\u5185\u90e8\u8981\u7d20\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\uff08sum\uff09\n\u30fbRDD\u306b\u542b\u307e\u308c\u308b\u8981\u7d20\u3059\u3079\u3066\u306b\u5bfe\u3059\u308b\u96c6\u7d04\u51e6\u7406\uff08reduce\uff09\n\u30fb\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\uff08accumulator\uff09\n\n# 0. \u6e96\u5099\n\u6271\u3046\u30c7\u30fc\u30bf\u5f62\u5f0f\u3002\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092HDFS\u4e0a\u306b\u683c\u7d0d\u3002\n\n```questionaire.csv\n\u5e74\u9f62\u3001\u6027\u5225\u3001\u8a55\u4fa1\n```\n\n# 1. \u30a2\u30f3\u30b1\u30fc\u30c8\u96c6\u8a08\u30d7\u30ed\u30b0\u30e9\u30e0\u5168\u4f53\n\u8a55\u4fa1\u3092\u5168\u4f53\u306e\u5e73\u5747\u3001\u5e74\u4ee3\u5225\u306e\u5e73\u5747\u3001\u6027\u5225\u3054\u3068\u306e\u5e73\u5747\u306e\u8ef8\u3067\u96c6\u8a08\u3059\u308b\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\u3002\n\n```chap5-2.py\n# -*- coding:utf-8 -*-\nfrom __future__ import print_function\nimport sys\nimport io\nfrom operator import add\nfrom pyspark import SparkContext\nif __name__ == \"__main__\":\n    if len(sys.argv) != 1:\n        print(\"Usage: examplent\")\n        exit(-1)\n    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n\n    fileQ = \"hdfs:///user/y_tadayasu/data/questionnaire.csv\"\n    sc = SparkContext(appName=\"chap5-2\")\n    questionRDD = sc.textFile(fileQ).map(lambda x: x.split(\",\"))\n    # RDD Persistence\n    #questionRDD.persist()\n    questionRDD.cache() # Persist this RDD with the default storage level (MEMORY_ONLY_SER).\n    #######################################\n    # draft-1: use count and sum method\n    #######################################\n    count = questionRDD.count()\n\n    #totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n    totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n\n    print(\"AVZ=\",totalPoints/count)\n\n    #######################################\n    # draft-2: use reduce\n    #######################################\n    (totalPoints,numQuestionare) = questionRDD.map(lambda x:(x[2],1)).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n    print(\"AVZ=%f\" %(totalPoints/numQuestionare))\n\n    ageRDD = questionRDD.map(lambda x:(int(x[0])//10*10,(x[2],1))).reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n    for i in ageRDD.collect():\n       (age,(agePoint,ageCount)) = i\n       print(\"AVG Age Range(%s):%.2f\" %(age,agePoint/ageCount))\n\n    #######################################\n    # calculate average about male and female\n    #######################################\n    numMAcc = sc.accumulator(0)\n    totalPointMAcc = sc.accumulator(0)\n    numFAcc = sc.accumulator(0)\n    totalPointFAcc = sc.accumulator(0)\n    \n    mfRDD = questionRDD.map(lambda x:(x[1],x[2])).foreach(lambda x: (numMAcc.add(1),totalPointMAcc.add(int(x[1]))) if x[0] == 'M' else (numFAcc.add(1),totalPointFAcc.add(int(x[1]))))\n    print(\"AVG Male is %f\" %(totalPointMAcc.value/numMAcc.value))\n    print(\"AVG Female is %f\" %(totalPointFAcc.value/numFAcc.value))\n    sc.stop()\n```\n\n\u30b8\u30e7\u30d6\u306e\u5b9f\u884c\u3002\n\n```\n$ spark-submit --master yarn-client chap5-2.py \nAVZ= 3.3684210526315788                                                         \nAVZ=3.368421\nAVG Age Range(40):2.50\nAVG Age Range(10):4.00\nAVG Age Range(20):3.71\nAVG Age Range(50):1.50\nAVG Age Range(30):3.50\nAVG Male is 3.500000\nAVG Female is 3.272727\n```\n\n# 2. \u30d7\u30ed\u30b0\u30e9\u30e0\u8aac\u660e\n## 2.1.RDD\u306e\u6c38\u7d9a\u5316\n\u6bce\u56decsv\u304b\u3089RDD\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3059\u308b\u306e\u306f\u52b9\u7387\u304c\u60aa\u3044\u306e\u3067\u3001\u6c38\u7d9a\u5316\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u3001\b\u305d\u306e\u51e6\u7406\u3092\u52b9\u7387\u5316\u3059\u308b\uff08\b\u6700\u521d\u306e\u30b8\u30e7\u30d6\u304c\u5b9f\u884c\u3055\u308c\u305f\u3068\u304d\u3001\u5404\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u304c\u30a8\u30b0\u30bc\u30ad\u30e5\u30fc\u30bf\u4e0a\u306b\u6c38\u7d9a\u5316\u3059\u308b\uff09\u3002\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u306b\u3088\u308a\u3001\u6c38\u7d9a\u5316\u5148\u3092\u30e1\u30e2\u30ea\u4e0a\u3084\u30c7\u30a3\u30b9\u30af\u306a\u3069\u9078\u629e\u3067\u304d\u308b\u3002\u30e1\u30e2\u30ea\u3060\u3051\u306b\u6c38\u7d9a\u5316\u3059\u308b\u5834\u5408\u306fpersist\u30e1\u30bd\u30c3\u30c9\u3067\u306f\u306a\u304fcache\u30e1\u30bd\u30c3\u30c9\u304c\u4f7f\u3048\u308b\u3002\n\u4f7f\u7528\u53ef\u80fd\u306a\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://spark.apache.org/docs/latest/programming-guide.html\n\n```\n  fileQ = \"hdfs:///user/y_tadayasu/data/questionnaire.csv\"\n    sc = SparkContext(appName=\"chap5-2\")\n    questionRDD = sc.textFile(fileQ).map(lambda x: x.split(\",\"))\n    # RDD Persistence\n    # persist\u30e1\u30bd\u30c3\u30c9\u3067\u6c38\u7d9a\u5316\u3067\u304d\u308b\u304c\u3001\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u4e0a\u3060\u3051\u306b\u6c38\u7d9a\u5316\u3059\u308b\u306e\u3067cache\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u7528\n    #questionRDD.persist()\n    questionRDD.cache() \n```\n\n## 2.2. \u8981\u7d20\u6570\u306e\u30ab\u30a6\u30f3\u30c8\u3068\u96c6\u8a08\u3000count\u30e1\u30bd\u30c3\u30c9\u3001sum\u30e1\u30bd\u30c3\u30c9\ncount\u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308aRDD\u306b\u542b\u307e\u308c\u308b\u8981\u7d20\u6570\u306e\u6570\u3092\u53d6\u5f97\u3059\u308b\u30a2\u30af\u30b7\u30e7\u30f3\u3002\n\nsum\u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308a\u6570\u5024\u578b\u306e\u8981\u7d20\u3092\u6301\u3064RDD\u3092\u5bfe\u8c61\u306b\u3001\u8981\u7d20\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\u3059\u308b\u30a2\u30af\u30b7\u30e7\u30f3\u3002\n\n```\n    count = questionRDD.count()\n\n    #totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n    totalPoints = questionRDD.map(lambda x: int(x[2])).sum()\n\n    print(\"AVZ=\",totalPoints/count)\n```\n\n## 2.3. \u8981\u7d20\u6570\u306e\u30ab\u30a6\u30f3\u30c8\u3068\u96c6\u8a08\u3000reduce\u30e1\u30bd\u30c3\u30c9\u3001foreach\u30e1\u30bd\u30c3\u30c9\nmap\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066RDD\u306e\u5168\u3066\u306e\u8981\u7d20\u3092\u3001(\u8a55\u4fa1,1)\u306e\u30bf\u30d7\u30eb\u306b\u5909\u63db\u3057\u3001reduce\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066\u8a55\u4fa1\u306e\u5408\u8a08\u5024\u3068\u8a55\u4fa1\u6570\u306e\u5408\u8a08\u5024\u3092\u96c6\u8a08\u3059\u308b\u3002\n\n```\n(totalPoints,numQuestionare) = questionRDD.map(lambda x:(x[2],1)).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\nprint(\"AVZ=%f\" %(totalPoints/numQuestionare))\n```\n\n\u5404\u5e74\u4ee3\u5225\u306e\u8a55\u4fa1\u306b\u95a2\u3057\u3066\u306f\u3001(\u5e74\u4ee3\u3001(\u8a55\u4fa1,1))\u306e\u30bf\u30d7\u30eb\u306b\u5909\u63db\u3057\u3001reduceByKey\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066\u5e74\u4ee3\u6bce\u306e\u8a55\u4fa1\u306e\u5408\u8a08\u5024\u3068\u8a55\u4fa1\u6570\u306e\u5408\u8a08\u5024\u3092\u96c6\u8a08\u3059\u308b\u3002\n\n```\nageRDD = questionRDD.map(lambda x:(int(x[0])//10*10,(x[2],1))).reduceByKey(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\nfor i in ageRDD.collect():\n    (age,(agePoint,ageCount)) = i\n    print(\"AVG Age Range(%s):%.2f\" %(age,agePoint/ageCount))\n```\n\nreduce\u30e1\u30bd\u30c3\u30c9\u3068foreach\u30e1\u30bd\u30c3\u30c9\u306b\u3064\u3044\u3066\u306fslideshare\u306b\u307e\u3068\u3081\u308b\u4e88\u5b9a\u3002\nreduce\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3046\u969b\u306e\u30c7\u30fc\u30bf\u51e6\u7406\u306b\u95a2\u3057\u3066\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3057\u3066\u78ba\u8a8d\u3002\n\u30bf\u30d7\u30eb\u306a\u306e\u3067\u5358\u306b\u52a0\u7b97\u3059\u308b\u3068\u6587\u5b57\u306e\u9023\u7d50\u306b\u306a\u308b\u306e\u3067\u3001int\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u3042\u308a\u3002\n\n```\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\")\n>>> txtRDD.collect()\n['1,2', '3,4', '5,6', '7,8', '9,10']\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\"))\n>>> txtRDD.collect()\n[['1', '2'], ['3', '4'], ['5', '6'], ['7', '8'], ['9', '10']]\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:x)\n>>> print(txtRDD)\n['1', '2']\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:y)\n>>> print(txtRDD)\n['9', '10']\n\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n>>> print(txtRDD)\n('13579', '246810')\n>>> txtRDD = sc.textFile(\"hdfs:///user/y_tadayasu/data/aaa.csv\").map(lambda x: x.split(\",\")).reduce(lambda x,y:(int(x[0])+int(y[0]),int(x[1])+int(y[1])))\n>>> print(txtRDD)\n(25, 30)\n```\n\n## 2.4. \u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30ab\u30a6\u30f3\u30c8\n\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u306f\u30c9\u30e9\u30a4\u30d0\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u306f\u5024\u306e\u8a2d\u5b9a\u3068\u53c2\u7167\u3092\u884c\u3044\u3001\u30a8\u30b0\u30bc\u30ad\u30e5\u30fc\u30bf\u4e0a\u3067\u5b9f\u884c\u3055\u308c\u308b\u30bf\u30b9\u30af\u304b\u3089\u306f\u5024\u306e\u52a0\u7b97\u306e\u307f\u3092\u884c\u3046\u3053\u3068\u3092\u60f3\u5b9a\u3057\u305f\u5171\u6709\u5909\u6570\u3002\u4f55\u304b\u306e\u5024\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081\u3060\u3051\u306b\u5bb9\u6613\u3055\u308c\u305f\u4ed5\u7d44\u307f\u3060\u3068\u601d\u308f\u308c\u308b\u3002\naccumulator\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u7528\u3057\u3001\u30a2\u30ad\u30e5\u30e0\u30ec\u30fc\u30bf\u3092\u4f7f\u7528\u30021\u3064\u76ee\u306e\u5f15\u6570\u306f\u521d\u671f\u5024\u3002\n\n\u6027\u5225\u304cM\u306a\u3089numMAcc(\u7537\u6027\u306e\u6570)\u306b1\u3092\u52a0\u7b97\u3057\u3001totalPointMAcc(\u7537\u6027\u306e\u7dcf\u30dd\u30a4\u30f3\u30c8)\u306b\u8a55\u4fa1\u3092\u3059\u308b\u3002M\u4ee5\u5916\u306e\u5834\u5408\u306f\u5973\u6027\u306e\u65b9\u306b\u540c\u3058\u51e6\u7406\u3092\u884c\u3046\u3002\n\n```\nnumMAcc = sc.accumulator(0)\ntotalPointMAcc = sc.accumulator(0)\nnumFAcc = sc.accumulator(0)\ntotalPointFAcc = sc.accumulator(0)\n    \nmfRDD = questionRDD.map(lambda x:(x[1],x[2])).foreach(lambda x: (numMAcc.add(1),totalPointMAcc.add(int(x[1]))) if x[0] == 'M' else (numFAcc.add(1),totalPointFAcc.add(int(x[1]))))\n\nprint(\"AVG Male is %f\" %(totalPointMAcc.value/numMAcc.value))\nprint(\"AVG Female is %f\" %(totalPointFAcc.value/numFAcc.value))\n```\n\n#3. \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\n\u30c7\u30d0\u30c3\u30b0\u3082\u517c\u306d\u3066\u4f55\u56de\u304b\u30b8\u30e7\u30d6\u3092\u5b9f\u884c\u3057\u3088\u3046\u3068\u3057\u305f\u3089\u3001\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u529b\u3055\u308c\u30b8\u30e7\u30d6\u304c\u5931\u6557\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n\n```\n5/12/01 00:42:56 ERROR SparkContext: Error initializing SparkContext.\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/y_tadayasu/.sparkStaging/application_1448\n929618763_0004/spark-assembly-1.5.2-hadoop2.6.0.jar could only be replicated to 0 nodes instead of minReplicatio\nn (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.\n```\n\n\u8abf\u3079\u3066\u307f\u308b\u3068HDFS\u3092\u69cb\u6210\u3057\u3066\u3044\u308b\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u304c100%\u8fd1\u304f\u306b\u306a\u3063\u3066\u3044\u305f\u3002\u3069\u306e\u9818\u57df\u304c\u30cd\u30c3\u30af\u306b\u306a\u3063\u3066\u3044\u308b\u304b\u8abf\u3079\u3066\u3044\u304f\u3068DataNode\u306e\u30d5\u30a1\u30a4\u30eb\u30ad\u30e3\u30c3\u30b7\u30e5\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u5bb9\u91cf\u3092\u903c\u8feb\u3055\u305b\u3066\u3044\u305f\u306e\u3067\u3001\u3068\u308a\u3042\u3048\u305a\u524a\u9664\u3002HDFS\u3092\u3061\u3083\u3093\u3068\u904b\u7528\u3059\u308b\u305f\u3081\u306b\u6700\u5927\u5024\u3092\u8a2d\u5b9a\u3059\u308b\u306a\u3069\u3057\u305f\u65b9\u304c\u826f\u3055\u305d\u3046\u3002\n/hadoop/yarn/node-manager/local/usercache/y_tadayasu/filecache\n", "tags": ["Spark", "python3"]}