{"context": "(\u672c\u30da\u30fc\u30b8\u306fTensorFlow\u3092\u7406\u89e3\u3057\u3066\u3044\u306a\u3044\u3001python\u3082\u4f7f\u3063\u305f\u3053\u3068\u306a\u3044\u3001OpenCV\u3082\u5148\u65e5\u4f7f\u3044\u59cb\u3081\u305f\u3070\u304b\u308a\u306a\u8005\u304c\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u8aa4\u308a\u3092\u542b\u3080\u304b\u3082\u3057\u308c\u307e\u305b\u3093)\n\n2016/08/23\u8ffd\u8a18\ntensorflow\u3068OpenCV\u306fdocker\u4e0a\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u65b9\u304c\u697d\u305d\u3046\u3067\u3059\u3002\n\u4ee5\u4e0b\u3082\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\nhttp://qiita.com/summer4an/items/5a10fe8cc13476f09963\n\n2016/02/10\u8ffd\u8a18\n\u30b5\u30fc\u30d0\u4e0a\u3067\u8a66\u3057\u306b\u5224\u5225\u3057\u3066\u307f\u3066\u3044\u305f\u3060\u3051\u308b\u74b0\u5883\u3092\u7528\u610f\u3057\u307e\u3057\u305f\u3002\n\u3088\u3051\u308c\u3070\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\nhttp://www.nakajimadevnakajima.info/va/kyohin/form.html\n\n2016/03/14\u8ffd\u8a18\n\u30b3\u30fc\u30c9\u3092\u5c11\u3057\u6574\u7406\u3057\u3001github\u3067\u516c\u958b\u3057\u307e\u3057\u305f\u3002\nhttps://github.com/summer4an/kyonyuu_hinnyuu_hanbetu\n\n\u524d\u63d0\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3084\u753b\u50cf\u51e6\u7406\u306e\u52c9\u5f37\u3092\u3057\u305f\u3044\uff0b\u305d\u3046\u3044\u3048\u3070\u5de8\u4e73\u9854\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308b\u3089\u3057\u3044\n\u3000\u2192\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u9854\u5199\u771f\u304b\u3089\u5de8\u4e73\u304b\u3069\u3046\u304b\u3092\u5224\u5225\u3057\u3066\u307f\u3088\u3046\uff01\n\u4e00\u5fdc\u307e\u3058\u3081\u306b\u8003\u3048\u3066\u3001\u9854\u304c\u3075\u3063\u304f\u3089\u3057\u3066\u3044\u308c\u3070\u80f8\u3082\u5927\u304d\u3044\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3060\u308d\u3046\u304b\u3089\u3001\u9854\u306e\u3064\u304f\u308a\u3068\u80f8\u306e\u5927\u304d\u3055\u306b\u306f\u76f8\u95a2\u304c\u3042\u308b\u3060\u308d\u3046\u3002\n\u9854\u306e\u4e38\u3055\u3084\u307e\u3076\u305f\u306e\u8089\u306e\u4ed8\u304d\u65b9\u7b49\u306e\u9854\u306e\u7279\u5fb4\u3092\u6570\u5024\u5316\u3057\u3066\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3055\u305b\u308c\u3070\u826f\u3055\u305d\u3046\u3002\n\u3068\u3044\u3046\u3053\u3068\u3067\u3084\u3063\u3066\u307f\u305f\u3002\n\n\u5de8\u4e73\u306e\u65b9\u3068\u8ca7\u4e73\u306e\u65b9\u3092\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\n\u5de8\u4e73\u306a\u65b9\u306f\u3053\u306e\u8fba\u308a\u304b\u3089\u3002\n\nhttp://matome.naver.jp/topic/1Luy1\nhttp://matome.naver.jp/odai/2138314089511622601\nhttp://matome.naver.jp/odai/2144212944181454201?&page=1\nhttp://blog.ikebukuroh.com/?eid=688\n\n\u8ca7\u4e73\u306a\u65b9\u306f\u3053\u306e\u8fba\u308a\u304b\u3089\u3002\n\nhttp://blog.livedoor.jp/aoba_f/archives/28219941.html\nhttp://matome.naver.jp/odai/2144290113141288701\nhttp://matome.naver.jp/odai/2140352985012641801\nhttp://matome.naver.jp/odai/2134329973691150901\n\n\u305f\u3060\u3057\u5916\u56fd\u306e\u65b9\u306f\u9854\u306e\u3064\u304f\u308a\u304c\u7570\u306a\u308a\u3059\u304e\u3066\u5b66\u7fd2\u304c\u3046\u307e\u304f\u3044\u304f\u6c17\u304c\u3057\u306a\u3044\u306e\u3067\u65e5\u672c\u306e\u65b9\u306b\u9650\u5b9a\u3002\n\u307e\u305f\u3001AV\u5973\u512a\u306e\u65b9\u306f\u6574\u5f62\u3084\u8c4a\u80f8\u306e\u53ef\u80fd\u6027\u304c\u9ad8\u305d\u3046\u306a\u306e\u3067\u9664\u3044\u305f(\u3068\u601d\u3046)\u3002\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d\u7a0b\u5ea6\u3002\u30ea\u30b9\u30c8\u306f\u81ea\u7c9b\u3002\u6b32\u3057\u3051\u308c\u3070\u3054\u9023\u7d61\u304f\u3060\u3055\u3044\u3002\n\n\u5199\u771f\u3092\u7247\u3063\u7aef\u304b\u3089\u96c6\u3081\u308b\n\u9069\u5f53\u306a\u753b\u50cf\u691c\u7d22\u30b5\u30a4\u30c8\u304b\u3089\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3002\u4eca\u56de\u306fbing\u753b\u50cf\u691c\u7d22\u3092\u4f7f\u3063\u305f\u3002\n\njpggetter.sh\n#!/bin/bash\n\n#\u753b\u50cf\u30b2\u30c3\u30bf\u30fc\n\n#\u5f15\u6570\u3067\u4eba\u540d\u3092\u4e0e\u3048\u308b\u3068\u3001temp_\u307b\u3052\u307b\u3052_gazou/gazou_00001.jpg\u7b49\u3068\u3044\u3046\u9023\u756a\u3067\u753b\u50cf\u3092\u4fdd\u5b58\u3057\u3066\u304f\u308c\u308b\u3002\n#  getter \"\u307b\u3052\u307b\u3052\"\nfunction getter() {\n  for name in \"$@\"; do\n    echo -e '\\n\\n\\n\\n'\n\n    targettext_encoded=`echo \"$name\" | perl -MURI::Escape -lne 'print uri_escape($_)'`\n    url='https://www.bing.com/images/search?FORM=HDRSC2&q='${targettext_encoded}\n    echo target is \"${name}\" \"${url}\"\n\n    if [ -d temp_${name}_gazou ]; then\n      echo already downloaded temp_${name}_gazou\n      continue\n    fi\n\n    mkdir temp_${name}_gazou\n    pushd temp_${name}_gazou\n\n    wget -r -l 1 -H \"${url}\"\n    rm -rf tse1.mm.bing.net/ tse2.mm.bing.net/ tse3.mm.bing.net/ tse4.mm.bing.net/ www.bing.com/ www.bingfudosan.jp/\n    find ./ -not -name '*.jpg' -a -not -name '*.jpeg' -a -not -name '*.JPG' -type f -print0 | xargs -0 rm\n\n    find ./ -type f | awk '{printf(\"mv '\"'\"'%s'\"'\"' gazou_%05d.jpg\\n\", $0, NR);}' > ../temp_${name}_gazou_filename_henkan.sh\n    bash ../temp_${name}_gazou_filename_henkan.sh\n    find ./ -maxdepth 1 -type d -print0 | xargs -0 rm -rf\n\n    popd\n\n    #\u9023\u7d9a\u3067wget\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\n    sleep 5\n  done\n}\n\ngetter \"\u307b\u3052\u307b\u3052\u5b50\" \"\u3082\u3052\u3082\u3052\u7f8e\" \u30fb\u30fb\u30fb\n\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n  $ ./jpggetter.sh\n\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d5600\u679a\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d3500\u679a\u7a0b\u5ea6\u96c6\u307e\u3063\u305f\u3002\n\n\u5199\u771f\u304b\u3089\u9854\u90e8\u5206\u3060\u3051\u3092\u5207\u308a\u629c\u304d\n\u3053\u306e\u8fba\u3092\u53c2\u8003\u306b\u3002\n\n\u9854\u8a8d\u8b58 http://nonbiri-tereka.hatenablog.com/entry/2015/05/13/063713\n\n\u753b\u50cf\u306e\u5207\u308a\u629c\u304d http://reiji1020.hatenablog.com/entry/2014/10/28/225829\n\n\n\u305f\u3060\u3057\u8907\u6570\u4eba\u306e\u9854\u304c\u691c\u51fa\u3055\u308c\u305f\u5834\u5408\u306f\u305d\u308c\u305e\u308c\u306e\u540d\u524d\u304c\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u6368\u3066\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\u90fd\u5408\u306b\u3088\u308a\u30011\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3059\u308b\u9854\u306e\u5207\u308a\u629c\u304d\u306fwindows\u4e0a\u306eOpenCV\u3092\u4f7f\u3063\u305fexe\u3067\u3001\u305d\u308c\u3092\u5404\u9854\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3059\u308b\u306e\u306fcygwin\u4e0a\u3067\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u884c\u3063\u305f\u3002\nwindows\u4e0a\u3067OpenCV\u3059\u308b\u305f\u3081\u306e\u30e1\u30e2\u306f\u3053\u3053\u3002\n\nface_kirinuki.exe\n//\u5b9f\u884c\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3002\n//  hoge.exe \u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30d5\u30a1\u30a4\u30eb\u540d \u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u540d cascade\u30d5\u30a1\u30a4\u30eb\u540d\n//\u623b\u308a\u5024\u306f\u30010\u304c\u6b63\u5e38\u7d42\u4e86\u3001100\u3088\u308a\u4e0b\u306f\u5f15\u6570\u4e0d\u8db3\u7b49\u306e\u5b9f\u884c\u6642\u306e\u7570\u5e38\u3001100\u4ee5\u4e0a\u306fOpenCV\u306f\u8d70\u3063\u305f\u304c\u691c\u51fa\u51fa\u6765\u306a\u304b\u3063\u305f\u7b49\n\n#include \"stdafx.h\"\n#include <opencv2/opencv.hpp>\n#include <iostream>\n#include <windows.h>\n\nusing namespace std;\nusing namespace cv;\n\n#define FACE_KAKUDAI_WARIAI 1.2 //\u4f8b\u3048\u3070\u691c\u51fa\u3055\u308c\u305f\u9854\u304c100px\u56db\u65b9\u3060\u3068\u3057\u305f\u3089\u3001100\u00d7FACE_KAKUDAI_WARIAIpx\u306e\u30b5\u30a4\u30ba\u3067\u5207\u308a\u629c\u304f\n\nint main(int argc, char const *argv[]) {\n    if(argc!=4){\n        printf(\"error. too few argument.\\n\");\n        exit(1);\n    }\n    printf(\"target file is %s\\n\", argv[1]);\n\n    Mat image = imread(argv[1]);\n    if(image.data==NULL){\n        printf(\"error. can not read source picture file.\\n\");\n        exit(2);\n    }\n    printf(\"image size width=%d height=%d\\n\", image.size().width, image.size().height);\n\n    string cascade_filename = argv[3];\n    CascadeClassifier cascade;\n    if(cascade.load(cascade_filename)!=true){\n        printf(\"error. can not read cascade file.\\n\");\n        exit(3);\n    }\n\n    vector<Rect> faces;\n    cascade.detectMultiScale(image, faces, 1.1, 3, 0);\n    printf(\"detect %zu faces.\\n\", faces.size());\n\n    if(faces.size()!=1){\n        printf(\"too many faces or no faces detected. exit.\\n\");\n        exit(100);\n    }\n\n    for (int i = 0; i < faces.size(); i++) {\n        printf(\"face[%d] x=%d y=%d width=%d height=%d\\n\", i, faces[i].x, faces[i].y, faces[i].width, faces[i].height);\n\n        //\u5207\u308a\u629c\u304f\u30b5\u30a4\u30ba\u3092\u5c11\u3057\u5927\u304d\u304f\u3057\u305f\u6642\u306b\u753b\u50cf\u3092\u306f\u307f\u51fa\u308b\u304b\u3069\u3046\u304b\u78ba\u8a8d\u3002\n        int kakudai_size = (int)(faces[i].width * (FACE_KAKUDAI_WARIAI-1) / 2); //\u5de6\u53f3\u3067\u62e1\u5927\u3059\u308b\u306e\u30672\u3067\u5272\u3063\u3066\u3044\u308b\u3002\n        int kirinuki_x = faces[i].x-kakudai_size;\n        int kirinuki_y = faces[i].y-kakudai_size;\n        int kirinuki_width  = faces[i].width + kakudai_size*2;\n        int kirinuki_height = faces[i].height+ kakudai_size*2;\n        if( kirinuki_x<0 ||\n            kirinuki_y<0 ||\n            image.size().width  <= kirinuki_x+kirinuki_width ||\n            image.size().height <= kirinuki_y+kirinuki_height ){\n            printf(\"face[%d] located on corner. skip.\\n\", i);\n            exit(101);\n        }\n\n        Mat cut_img(image, Rect(kirinuki_x, kirinuki_y, kirinuki_width, kirinuki_height));\n        imwrite(argv[2], cut_img);\n        printf(\"face[%d] outputed as \\\"%s\\\"\\n\", i, argv[2]);\n\n        rectangle(image, Point(faces[i].x, faces[i].y), Point(faces[i].x + faces[i].width, faces[i].y + faces[i].height), Scalar(0, 200, 0), 1, CV_AA);\n        rectangle(image, Point(kirinuki_x, kirinuki_y), Point(kirinuki_x + kirinuki_width, kirinuki_y + kirinuki_height), Scalar(200, 0, 0), 1, CV_AA);\n    }\n\n    //imshow(\"detect face\", image);\n    //waitKey(0);\n\n    return 0;\n}\n\n\n\nface_kirinuki.sh\n#!/bin/bash\n\n#\u9854\u90e8\u5206\u3060\u3051\u5207\u308a\u629c\u304f\u3002\n#\u300c./faces/temp_*_gazou/\u300d\u4ee5\u4e0b\u306e\u5404gazou_*.jpg\u306b\u5bfe\u3057\u3066\u51e6\u7406\u3092\u5b9f\u884c\u3002\n#\u300c./faces/temp_*_gazou/face_gazou_00001.jpg\u300d\u7b49\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u308b\u3002\n\nCASCADE_FILENAME='(haarcascade_frontalface_alt2.xml\u3078\u306e\u30d1\u30b9)'\n\nfor i in ./faces/temp_*_gazou; do\n  echo -e '\\n\\n\\n\\n'\n  echo target dir $i\n  pushd $i\n\n  for j in gazou_*.jpg; do\n    echo -e '\\n'\n    echo -e 'target dir is' \"$i\" ' target file is' \"$j\"\n\n    if [ -f face_$j ]; then\n      echo 'already detected face. skip'\n      continue\n    fi\n\n    echo target file $j\n\n    ./face_kirinuki.exe $j face_$j ${CASCADE_FILENAME}\n\n    #\u3082\u3057\u5224\u5225\u306b\u5931\u6557\u3057\u3066face_*.jpg\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u7a7a\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b\u3002\n    if [ $? -ne 0 ]; then\n      touch face_$j\n      continue\n    fi\n\n    #\u30b5\u30a4\u30ba\u3092\u540c\u3058\u306b\u3059\u308b\n    mogrify -geometry 256x256 face_$j\n  done\n\n  popd\ndone\n\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n  $ ./face_kirinuki.sh\n\n\u7d42\u308f\u3063\u305f\u3089\u76ee\u3067\u898b\u3066\u8aa4\u691c\u77e5\u3057\u3066\u3044\u308b\u3082\u306e\u3001\u9854\u306e\u4e00\u90e8\u3057\u304b\u5199\u3063\u3066\u3044\u306a\u3044\u3082\u306e\u3001\u6b63\u9762\u5411\u304d\u3067\u306a\u3044\u3082\u306e\u3001\u305d\u3082\u305d\u3082\u5168\u7136\u9055\u3046\u4eba\u306e\u5199\u771f\u306f\u9664\u53bb\u3057\u3066\u304a\u304f\u3002\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d2500\u679a\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d1700\u679a\u7a0b\u5ea6\u306b\u306a\u3063\u305f\u3002\n\n\n\u9854\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u5b66\u7fd2\n\u30ed\u30fc\u30ab\u30eb\u3067\u52d5\u304f\u9854\u306e\u4e38\u3055\u7b49\u306e\u7279\u5fb4\u62bd\u51fa\u7528\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u306a\u3044\u304b\u63a2\u3057\u3066\u307f\u305f\u304c\u898b\u3064\u3051\u3089\u308c\u305a\u3002\u81ea\u524d\u3067\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u306e\u306f\u8ae6\u3081\u3002\nweb\u30b5\u30fc\u30d3\u30b9\u306a\u3089\u3053\u306e\u8fba\u308a\u304c\u3042\u308b\u6a21\u69d8\u3002\n\ndetectface http://konisimple.net/lab/detectface_api/\n\nReKognition https://www.rekognition.com/\n\n\n\u7279\u5fb4\u91cf\u3092\u6c42\u3081\u3066\u304b\u3089\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306eChainer\u3084TensorFlow\u306b\u98df\u308f\u305b\u3088\u3046\u3068\u8003\u3048\u3066\u3044\u305f\u304c\u3001\u305d\u3082\u305d\u3082\u4f55\u3082\u8003\u3048\u305a\u306b\u3076\u3063\u3053\u3080\u3060\u3051\u3067\u753b\u50cf\u5206\u985e\u3057\u3066\u304f\u308c\u308b\u3063\u307d\u3044\u306e\u3067\u3084\u3063\u3066\u307f\u308b\u3002\n\nTensorFlow http://kivantium.hateblo.jp/entry/2015/11/18/233834\n\nChainer http://hi-king.hatenablog.com/entry/2015/06/11/021144\n\nChainer http://d.hatena.ne.jp/shi3z/20150709/1436397615\n\n\nChainer\u306f\u306a\u305c\u304b\u3046\u307e\u304f\u52d5\u304b\u306a\u304b\u3063\u305f\u306e\u3067TensorFlow\u3067\u3002\n\nTensorFlow\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\u4ee5\u4e0b\u516c\u5f0f\u30b5\u30a4\u30c8\u306e\u300cPip Installation\u300d\u3092\u53c2\u7167\u3002python\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306fpip\u3068\u3044\u3046\u3082\u306e\u3092\u4f7f\u3063\u3066\u5165\u308c\u308b\u3089\u3057\u3044\u3002\n\nhttps://www.tensorflow.org/versions/master/get_started/os_setup.html\n\n$ sudo apt-get install python-pip python-dev\n$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n\nvirtual box\u4e0aUbuntu14.04 64bit\u3092\u4f7f\u7528\u3002\n\u308d\u304f\u306aGPU\u304c\u8f09\u3063\u3066\u3044\u306a\u3044\u306e\u3067CPU\u7248\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u3002\u7279\u306b\u96e3\u3057\u304f\u306a\u3044\u3002\n\u300cRun TensorFlow from the Command Line\u300d\u306e\u7b87\u6240\u3092\u53c2\u7167\u3057\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u304a\u304f\u3002\n\n\u5b66\u7fd2\n\u96c6\u3081\u305f\u9854\u5199\u771f\u30d5\u30a1\u30a4\u30eb\u7fa4\u304b\u3089\u3001\u5b66\u7fd2\u7528\u306e\u65b9\u3068\u3001\u30c6\u30b9\u30c8\u7528\u306e\u65b9\u3068\u3001\u6700\u5f8c\u306b\u5de8\u4e73\u304b\u5224\u5225\u3059\u308b\u304a\u8a66\u3057\u7528\u306e\u65b9\u3092\u5206\u3051\u3066\u304a\u304f\u3002\n\u5b66\u7fd2\u7528\u306fface_filelist_train.txt\u3001\u30c6\u30b9\u30c8\u7528\u306fface_filelist_test.txt\u306b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\u3002\n  (\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9) (\u5206\u985e \u5de8\u4e73\u306a\u30891\u3001\u8ca7\u4e73\u306a\u30890)\n  ./faces/face_hogeko_00001.jpg 1\n  ./faces/face_mogemi_00001.jpg 0\n  \u30fb\u30fb\u30fb\n\n\u3082\u3057\u304b\u3057\u305f\u3089\u30ea\u30b9\u30c8\u306f\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3066\u304a\u3044\u305f\u65b9\u304c\u826f\u3044\u304b\u3082\u3002\u524d\u534a\u304c\u5de8\u4e73\u3001\u5f8c\u534a\u304c\u8ca7\u4e73\u7b49\u4e26\u3093\u3067\u3044\u308b\u3068\u5b66\u7fd2\u304c\u9032\u307e\u306a\u3044\u304b\u3082(\u8a66\u884c\u932f\u8aa4\u3057\u3066\u3044\u305f\u6642\u3060\u3063\u305f\u306e\u3067\u4ed6\u306e\u539f\u56e0\u304b\u3082\u3057\u308c\u306a\u3044)\u3002sort\u306e-R\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u4f7f\u3048\u3070OK\u3002\n\u4ee5\u4e0b\u306b\u3088\u308b\u3068\u5b66\u7fd2\u7528\u306b\u306f60\uff5e80%\u5272\u308a\u5f53\u3066\u308b\u3068\u826f\u3044\u3068\u306e\u3053\u3068\u3002\n\nhttps://www.researchgate.net/post/What_is_the_best_way_to_divide_our_dataset_into_training_and_test_sets\n\n\u5b66\u7fd2\u7528\u306b3000\u679a\u3001\u30c6\u30b9\u30c8\u7528\u306b1200\u679a\u3092\u5272\u308a\u5f53\u3066\u305f\u3002\n\n\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u300c\u30b3\u30fc\u30c9\u5168\u4f53\u300d\u304b\u3089\u62dd\u501f\u3002\n\nhttp://kivantium.hateblo.jp/entry/2015/11/18/233834\n\n\u305f\u3060\u7a00\u306b\u300cReluGrad input is not finite. : Tensor had NaN values\u300d\u3068\u3044\u3046\u30a8\u30e9\u30fc\u3067\u7570\u5e38\u7d42\u4e86\u3059\u308b\u3053\u3068\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u4ee5\u4e0b\u53c2\u8003\u306b\u7570\u5e38\u51e6\u7406\u3092\u8ffd\u52a0\u3002\n\nhttp://qiita.com/ikki8412/items/3846697668fc37e3b7e0\n\n\ngakusyuu.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.python.platform\n\nNUM_CLASSES = 2\nIMAGE_SIZE = 28\nIMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE*3\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('train', 'face_filelist_train.txt', 'File name of train data')\nflags.DEFINE_string('test', 'face_filelist_test.txt', 'File name of train data')\nflags.DEFINE_string('train_dir', './logdata', 'Directory to put the training data.')\nflags.DEFINE_integer('max_steps', 100, 'Number of steps to run trainer.')\nflags.DEFINE_integer('batch_size', 10, 'Batch size'\n                     'Must divide evenly into the dataset sizes.')\nflags.DEFINE_float('learning_rate', 1e-4, 'Initial learning rate.')\n\ndef inference(images_placeholder, keep_prob):\n    \"\"\" \u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      images_placeholder: \u753b\u50cf\u306eplaceholder\n      keep_prob: dropout\u7387\u306eplace_holder\n\n    \u8fd4\u308a\u5024:\n      y_conv: \u5404\u30af\u30e9\u30b9\u306e\u78ba\u7387(\u306e\u3088\u3046\u306a\u3082\u306e)\n    \"\"\"\n    # \u91cd\u307f\u3092\u6a19\u6e96\u504f\u5dee0.1\u306e\u6b63\u898f\u5206\u5e03\u3067\u521d\u671f\u5316\n    def weight_variable(shape):\n      initial = tf.truncated_normal(shape, stddev=0.1)\n      return tf.Variable(initial)\n\n    # \u30d0\u30a4\u30a2\u30b9\u3092\u6a19\u6e96\u504f\u5dee0.1\u306e\u6b63\u898f\u5206\u5e03\u3067\u521d\u671f\u5316\n    def bias_variable(shape):\n      initial = tf.constant(0.1, shape=shape)\n      return tf.Variable(initial)\n\n    # \u7573\u307f\u8fbc\u307f\u5c64\u306e\u4f5c\u6210\n    def conv2d(x, W):\n      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u4f5c\u6210\n    def max_pool_2x2(x):\n      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1], padding='SAME')\n\n    # \u5165\u529b\u309228x28x3\u306b\u5909\u5f62\n    x_image = tf.reshape(images_placeholder, [-1, 28, 28, 3])\n\n    # \u7573\u307f\u8fbc\u307f\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('conv1') as scope:\n        W_conv1 = weight_variable([5, 5, 3, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('pool1') as scope:\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # \u7573\u307f\u8fbc\u307f\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('conv2') as scope:\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('pool2') as scope:\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # \u5168\u7d50\u5408\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('fc1') as scope:\n        W_fc1 = weight_variable([7*7*64, 1024])\n        b_fc1 = bias_variable([1024])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        # dropout\u306e\u8a2d\u5b9a\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # \u5168\u7d50\u5408\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('fc2') as scope:\n        W_fc2 = weight_variable([1024, NUM_CLASSES])\n        b_fc2 = bias_variable([NUM_CLASSES])\n\n    # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306b\u3088\u308b\u6b63\u898f\u5316\n    with tf.name_scope('softmax') as scope:\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    # \u5404\u30e9\u30d9\u30eb\u306e\u78ba\u7387\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u8fd4\u3059\n    return y_conv\n\ndef loss(logits, labels):\n    \"\"\" loss\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      logits: \u30ed\u30b8\u30c3\u30c8\u306etensor, float - [batch_size, NUM_CLASSES]\n      labels: \u30e9\u30d9\u30eb\u306etensor, int32 - [batch_size, NUM_CLASSES]\n\n    \u8fd4\u308a\u5024:\n      cross_entropy: \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306etensor, float\n\n    \"\"\"\n\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u8a08\u7b97\n    #cross_entropy = -tf.reduce_sum(labels*tf.log(logits)) \u7570\u5e38\u51e6\u7406 http://qiita.com/ikki8412/items/3846697668fc37e3b7e0\n    cross_entropy = -tf.reduce_sum(labels*tf.log(tf.clip_by_value(logits,1e-10,1.0)))\n    # TensorBoard\u3067\u8868\u793a\u3059\u308b\u3088\u3046\u6307\u5b9a\n    tf.scalar_summary(\"cross_entropy\", cross_entropy)\n    return cross_entropy\n\ndef training(loss, learning_rate):\n    \"\"\" \u8a13\u7df4\u306eOp\u3092\u5b9a\u7fa9\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      loss: \u640d\u5931\u306etensor, loss()\u306e\u7d50\u679c\n      learning_rate: \u5b66\u7fd2\u4fc2\u6570\n\n    \u8fd4\u308a\u5024:\n      train_step: \u8a13\u7df4\u306eOp\n\n    \"\"\"\n\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    return train_step\n\ndef accuracy(logits, labels):\n    \"\"\" \u6b63\u89e3\u7387(accuracy)\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      logits: inference()\u306e\u7d50\u679c\n      labels: \u30e9\u30d9\u30eb\u306etensor, int32 - [batch_size, NUM_CLASSES]\n\n    \u8fd4\u308a\u5024:\n      accuracy: \u6b63\u89e3\u7387(float)\n\n    \"\"\"\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    tf.scalar_summary(\"accuracy\", accuracy)\n    return accuracy\n\nif __name__ == '__main__':\n    # \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f\n    f = open(FLAGS.train, 'r')\n    # \u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u914d\u5217\n    train_image = []\n    train_label = []\n    for line in f:\n        # \u6539\u884c\u3092\u9664\u3044\u3066\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u306b\u3059\u308b\n        line = line.rstrip()\n        l = line.split()\n        # \u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u306728x28\u306b\u7e2e\u5c0f\n\n        print \"aaaaa line[%s] %s %s\"%(line,l[0],l[1])\n\n        img = cv2.imread(l[0])\n        img = cv2.resize(img, (28, 28))\n        # \u4e00\u5217\u306b\u3057\u305f\u5f8c\u30010-1\u306efloat\u5024\u306b\u3059\u308b\n        train_image.append(img.flatten().astype(np.float32)/255.0)\n        # \u30e9\u30d9\u30eb\u30921-of-k\u65b9\u5f0f\u3067\u7528\u610f\u3059\u308b\n        tmp = np.zeros(NUM_CLASSES)\n        tmp[int(l[1])] = 1\n        train_label.append(tmp)\n    # numpy\u5f62\u5f0f\u306b\u5909\u63db\n    train_image = np.asarray(train_image)\n    train_label = np.asarray(train_label)\n    f.close()\n\n    f = open(FLAGS.test, 'r')\n    test_image = []\n    test_label = []\n    for line in f:\n        line = line.rstrip()\n        l = line.split()\n        img = cv2.imread(l[0])\n        img = cv2.resize(img, (28, 28))\n\n        print \"aaaaa2 line[%s] %s %s\"%(line,l[0],l[1])\n\n        test_image.append(img.flatten().astype(np.float32)/255.0)\n        tmp = np.zeros(NUM_CLASSES)\n        tmp[int(l[1])] = 1\n        test_label.append(tmp)\n    test_image = np.asarray(test_image)\n    test_label = np.asarray(test_label)\n    f.close()\n\n    with tf.Graph().as_default():\n        # \u753b\u50cf\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        images_placeholder = tf.placeholder(\"float\", shape=(None, IMAGE_PIXELS))\n        # \u30e9\u30d9\u30eb\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        labels_placeholder = tf.placeholder(\"float\", shape=(None, NUM_CLASSES))\n        # dropout\u7387\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        keep_prob = tf.placeholder(\"float\")\n\n        # inference()\u3092\u547c\u3073\u51fa\u3057\u3066\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\n        logits = inference(images_placeholder, keep_prob)\n        # loss()\u3092\u547c\u3073\u51fa\u3057\u3066\u640d\u5931\u3092\u8a08\u7b97\n        loss_value = loss(logits, labels_placeholder)\n        # training()\u3092\u547c\u3073\u51fa\u3057\u3066\u8a13\u7df4\n        train_op = training(loss_value, FLAGS.learning_rate)\n        # \u7cbe\u5ea6\u306e\u8a08\u7b97\n        acc = accuracy(logits, labels_placeholder)\n\n        # \u4fdd\u5b58\u306e\u6e96\u5099\n        saver = tf.train.Saver()\n        # Session\u306e\u4f5c\u6210\n        sess = tf.Session()\n        # \u5909\u6570\u306e\u521d\u671f\u5316\n        sess.run(tf.initialize_all_variables())\n        # TensorBoard\u3067\u8868\u793a\u3059\u308b\u5024\u306e\u8a2d\u5b9a\n        summary_op = tf.merge_all_summaries()\n        summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph_def)\n\n        # \u8a13\u7df4\u306e\u5b9f\u884c\n        for step in range(FLAGS.max_steps):\n            for i in range(len(train_image)/FLAGS.batch_size):\n                # batch_size\u5206\u306e\u753b\u50cf\u306b\u5bfe\u3057\u3066\u8a13\u7df4\u306e\u5b9f\u884c\n                batch = FLAGS.batch_size*i\n                # feed_dict\u3067placeholder\u306b\u5165\u308c\u308b\u30c7\u30fc\u30bf\u3092\u6307\u5b9a\u3059\u308b\n                sess.run(train_op, feed_dict={\n                  images_placeholder: train_image[batch:batch+FLAGS.batch_size],\n                  labels_placeholder: train_label[batch:batch+FLAGS.batch_size],\n                  keep_prob: 0.5})\n\n            # 1 step\u7d42\u308f\u308b\u305f\u3073\u306b\u7cbe\u5ea6\u3092\u8a08\u7b97\u3059\u308b\n            train_accuracy = sess.run(acc, feed_dict={\n                images_placeholder: train_image,\n                labels_placeholder: train_label,\n                keep_prob: 1.0})\n            print \"step %d, training accuracy %g\"%(step, train_accuracy)\n\n            # 1 step\u7d42\u308f\u308b\u305f\u3073\u306bTensorBoard\u306b\u8868\u793a\u3059\u308b\u5024\u3092\u8ffd\u52a0\u3059\u308b\n            summary_str = sess.run(summary_op, feed_dict={\n                images_placeholder: train_image,\n                labels_placeholder: train_label,\n                keep_prob: 1.0})\n            summary_writer.add_summary(summary_str, step)\n\n    # \u8a13\u7df4\u304c\u7d42\u4e86\u3057\u305f\u3089\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u3092\u8868\u793a\n    print \"test accuracy %g\"%sess.run(acc, feed_dict={\n        images_placeholder: test_image,\n        labels_placeholder: test_label,\n        keep_prob: 1.0})\n\n    # \u6700\u7d42\u7684\u306a\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n    save_path = saver.save(sess, \"model.ckpt\")\n\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n  $ python gakusyuu.py\n\nCore i7 2.1GHz win7\u306e\u30ce\u30fc\u30c8PC\u4e0a\u3067\u3001virtual box\u3067ubuntu14.04\u306b4CPU 4GB\u30e1\u30e2\u30ea\u5272\u308a\u5f53\u3066\u3066\u52d5\u304b\u3057\u305f\u4e0a\u3067\u5b9f\u884c\u3002100\u56de\u306e\u5b66\u7fd2\u306730\u5206\u7a0b\u5ea6\u3067\u7d42\u308f\u308b\u3002\n\u5b66\u7fd2\u306e\u9014\u4e2d\u7d4c\u904e\u306f\u4ee5\u4e0b\u5b9f\u884c\u3057\u305f\u5f8c\u3001\u30d6\u30e9\u30a6\u30b6\u3067\u300c http://localhost:6006 \u300d\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308c\u3070\u898b\u308c\u308b\u3002\n  $ tensorboard --logdir=(\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u3042\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u7d76\u5bfe\u30d1\u30b9)\n  $ tensorboard --logdir=/home/hoge/kyonyuu_hanbetu/logdata\n\n\n\u6700\u7d42\u7684\u306b\u5b66\u7fd2\u7528\u306e\u9854\u306b\u5bfe\u3057\u3066\u306f\u300cstep 99, training accuracy 0.980135\u300d\u306b\u306a\u3063\u305f\u3002\n\u305f\u3060\u30c6\u30b9\u30c8\u7528\u306e\u9854\u306b\u5bfe\u3057\u3066\u306e\u5224\u5b9a\u7d50\u679c\u304c\u300ctest accuracy 0.534127\u300d\u3068\u306e\u3053\u3068\u3002\u307b\u307c\u5224\u5225\u3067\u304d\u3066\u3044\u306a\u3044\u2026\n\n\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u5224\u5b9a\n\u3068\u308a\u3042\u3048\u305a\u5b66\u7fd2\u7528\u306b\u3082\u30c6\u30b9\u30c8\u7528\u306b\u3082\u4f7f\u3063\u3066\u3044\u306a\u3044\u65b9\u306e\u9854\u5199\u771f\u3067\u5224\u5b9a\u3057\u3066\u307f\u308b\u3002\n\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u300c\u753b\u50cf\u306b\u5bfe\u3057\u3066\u4e88\u60f3\u30e9\u30d9\u30eb\u3092\u8868\u793a\u3059\u308b\u300d\u306e\u4e0b\u304b\u3089\u62dd\u501f\u3002\n\nhttp://kivantium.hateblo.jp/entry/2015/11/18/233834\n\n\u30e1\u30a4\u30f3\u306fpython\u30b3\u30fc\u30c9\u3067\u3001\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u306f\u30ed\u30b0\u3092\u8a18\u9332\u3057\u3066\u3044\u308b\u3060\u3051\u3002\n\nhanbetu__all_file.py\n#!/usr/bin/env python\n#! -*- coding: utf-8 -*-\n\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n\nNUM_CLASSES = 2\nIMAGE_SIZE = 28\nIMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE*3\n\ndef inference(images_placeholder, keep_prob):\n    \"\"\" \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      images_placeholder: inputs()\u3067\u4f5c\u6210\u3057\u305f\u753b\u50cf\u306eplaceholder\n      keep_prob: dropout\u7387\u306eplace_holder\n\n    \u8fd4\u308a\u5024:\n      cross_entropy: \u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u7d50\u679c\n    \"\"\"\n    def weight_variable(shape):\n      initial = tf.truncated_normal(shape, stddev=0.1)\n      return tf.Variable(initial)\n\n    def bias_variable(shape):\n      initial = tf.constant(0.1, shape=shape)\n      return tf.Variable(initial)\n\n    def conv2d(x, W):\n      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n    def max_pool_2x2(x):\n      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1], padding='SAME')\n\n    x_image = tf.reshape(images_placeholder, [-1, 28, 28, 3])\n\n    with tf.name_scope('conv1') as scope:\n        W_conv1 = weight_variable([5, 5, 3, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    with tf.name_scope('pool1') as scope:\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    with tf.name_scope('conv2') as scope:\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    with tf.name_scope('pool2') as scope:\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    with tf.name_scope('fc1') as scope:\n        W_fc1 = weight_variable([7*7*64, 1024])\n        b_fc1 = bias_variable([1024])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    with tf.name_scope('fc2') as scope:\n        W_fc2 = weight_variable([1024, NUM_CLASSES])\n        b_fc2 = bias_variable([NUM_CLASSES])\n\n    with tf.name_scope('softmax') as scope:\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    return y_conv\n\nif __name__ == '__main__':\n    test_image = []\n    filenames = []\n    for i in range(1, len(sys.argv)):\n        img = cv2.imread(sys.argv[i])\n        img = cv2.resize(img, (28, 28))\n        test_image.append(img.flatten().astype(np.float32)/255.0)\n        filenames.append(sys.argv[i])\n    test_image = np.asarray(test_image)\n\n    images_placeholder = tf.placeholder(\"float\", shape=(None, IMAGE_PIXELS))\n    labels_placeholder = tf.placeholder(\"float\", shape=(None, NUM_CLASSES))\n    keep_prob = tf.placeholder(\"float\")\n\n    logits = inference(images_placeholder, keep_prob)\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    saver.restore(sess, \"model.ckpt\")\n\n    for i in range(len(test_image)):\n        pred = np.argmax(logits.eval(feed_dict={\n            images_placeholder: [test_image[i]],\n            keep_prob: 1.0 })[0])\n        pred2 = logits.eval(feed_dict={\n            images_placeholder: [test_image[i]],\n            keep_prob: 1.0 })[0]\n        print filenames[i],pred,\"{0:10.8f}\".format(pred2[0]),\"{0:10.8f}\".format(pred2[1])\n\n\n\nhanbetu_all.sh\n#!/bin/bash\npython ./hanbetu__all_file.py \"$@\" >> log_hanbetu_`date +%Y%m%d_%H%M%S`.txt\n\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n  $ ./hanbetu_all.sh (\u5224\u5225\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb \u8907\u6570)\n\nlog_hanbetu_XXX.txt\u306b\u7d50\u679c\u304c\u66f8\u304d\u8fbc\u307e\u308c\u3001\u5de8\u4e73\u3068\u5224\u5225\u3057\u305f\u3082\u306e\u306b\u306f1\u304c\u3001\u8ca7\u4e73\u3068\u5224\u5225\u3057\u305f\u3082\u306e\u306b\u306f0\u304c\u3064\u304f\u3002\u307e\u305f\u3001\u5de8\u4e73\u306e\u53ef\u80fd\u6027\u3068\u8ca7\u4e73\u306e\u53ef\u80fd\u6027\u3082\u6570\u5024\u3067\u8868\u793a\u3055\u308c\u308b\u3002\n\u5de8\u4e73\u306e\u65b9\u51688\u540d84\u679a\u3001\u8ca7\u4e73\u306e\u65b9\u51688\u540d81\u679a\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3002\n\u7d50\u679c\u306f\u4ee5\u4e0b\u3002\n\n\u5de8\u4e73\u306e\u65b9\u3092\u5de8\u4e73\u3068\u6b63\u3057\u304f\u5224\u5225\u51fa\u6765\u305f\u306e\u306f82%(69/84)\n\u8ca7\u4e73\u306e\u65b9\u3092\u8ca7\u4e73\u3068\u6b63\u3057\u304f\u5224\u5225\u51fa\u6765\u305f\u306e\u306f37%(30/81)\n\n\n\u5de8\u4e73\u306e\u65b9\u3060\u3051\u898b\u308b\u3068\u3088\u3055\u6c17\u3060\u304c\u3001\u5358\u306b\u591a\u304f\u3092\u5de8\u4e73\u3068\u5224\u5225\u3057\u3066\u3044\u308b\u3060\u3051\u304b\u2026\uff1f\n\u305f\u3060\u5de8\u4e73\u306e\u65b9\u3067\u9ad8\u3044\u78ba\u7387\u3067\u5f53\u3066\u3066\u3044\u308b\u65b9\u3082\u3044\u308b\u306e\u3067\u3001\u5f53\u3066\u3084\u3059\u3044\u9854\u306e\u65b9\u306a\u306e\u304b\u3082\u3002\n\n\u307e\u3068\u3081\n\u3044\u307e\u3044\u3061\u3046\u307e\u304f\u3044\u3063\u305f\u304b\u5fae\u5999\u3002\n\u6557\u56e0\u3068\u3057\u3066\u601d\u3044\u3064\u304f\u306e\u306f\u4ee5\u4e0b\u3002\n\nTensorFlow\u306e\u4f7f\u3044\u65b9\u3092\u9593\u9055\u3063\u3066\u3044\u308b\uff1f\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\uff1f\u5de8\u4e73\u9854\u306b\u3082\u7a2e\u985e\u304c\u3042\u308a\u305d\u3046\u3060\u3057\u3001\u5de8\u4e73\u306a\u65b9\u306e\u4e2d\u306b\u3082\u5de8\u4e73\u9854\u306e\u4eba\u3068\u666e\u901a\u9854\u306e\u4eba\u304c\u3044\u305d\u3046\u306a\u306e\u3067\u3001\u5b66\u7fd2\u6642\u306b1\u3064\u306eCLASS\u306b\u307e\u3068\u3081\u3066\u3057\u307e\u3046\u306e\u304c\u307e\u305a\u3044\uff1f\n\u5de8\u4e73\u3084\u8ca7\u4e73\u3092\u5224\u65ad\u3059\u308b\u6642\u306e\u57fa\u6e96\u304c\u4eba\u305d\u308c\u305e\u308c\u3067\u66d6\u6627\u3002\u672c\u5f53\u306b\u305d\u3046\u304b\u3069\u3046\u304b\u306f\u81ea\u5206\u3067\u306f\u672a\u78ba\u8a8d\u3002\n\u6574\u5f62\u3084\u8c4a\u80f8\u3084\u507d\u4e73\u3084\u30d1\u30c3\u30c9\u7b49\u3060\u3068\u56f0\u308b\u304c\u3001\u5224\u5225\u3067\u304d\u306a\u3044\u3002\n\u305d\u3082\u305d\u3082\u5de8\u4e73\u9854\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\uff1f\n\n\u3082\u3057\u3046\u307e\u304f\u5224\u5225\u51fa\u6765\u3066\u3044\u305f\u3089\u3053\u3093\u306a\u5fdc\u7528\u304c\u3067\u304d\u308b\u304b\u3082\uff1f\n\nSNS\u7b49\u3067\u9854\u5199\u771f\u3060\u3051\u3082\u3089\u3063\u305f\u5834\u5408\u306b\u4e73\u4e88\u6e2c\n\u5199\u771f\u304b\u3089\u507d\u4e73\u5224\u5b9a\n\u30a4\u30e9\u30b9\u30c8\u306b\u3082\u9069\u7528\u3067\u304d\u308b\u306a\u3089\u3001\u59a5\u5f53\u306a\u4e73\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u3089\u308c\u308b\n\n\u6c17\u3065\u3044\u305f\u3053\u3068\u3002\n\n\u5224\u5225\u7d50\u679c\u304c\u81ea\u5206\u306e\u601d\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u7570\u306a\u308b\u3068\u3001\u8a00\u308f\u308c\u3066\u307f\u308b\u3068\u305d\u3046\u304b\u3082\u3001\u3068\u3044\u3046\u6c17\u306b\u306a\u308b\u3002\n\u5927\u91cf\u306e\u9854\u753b\u50cf\u304b\u3089\u3075\u3055\u308f\u3057\u304f\u306a\u3044\u3082\u306e\u3092\u5f3e\u304f\u305f\u3081\u306b\u9ad8\u901f\u3067\u30b9\u30af\u30ed\u30fc\u30eb\u3057\u307e\u304f\u308a\u306a\u304c\u3089\u5224\u5225\u3057\u3066\u3044\u308b\u3068\u932f\u8996\u306eFlashed Face Distortion Effect\u3068\u3084\u3089\u304c\u8d77\u304d\u3066\u6016\u3044\u3002\u591c\u306b\u3084\u3063\u3066\u306f\u3044\u3051\u306a\u3044\u3002\n\u753b\u50cf\u3092\u96c6\u3081\u305f\u65b9\u3005\u306e\u4e2d\u306b\u3001\u5de6\u5411\u304d\u3070\u3063\u304b\u308a\u306e\u4eba\u3001\u53f3\u5411\u304d\u3070\u3063\u304b\u308a\u306e\u4eba\u304c\u3044\u308b\u3002\u305d\u3061\u3089\u304c\u305d\u306e\u4eba\u306e\u4e00\u62bc\u3057\u306e\u5411\u304d\u306a\u306e\u304b\u3082\u3002\n\n(\u672c\u30da\u30fc\u30b8\u306fTensorFlow\u3092\u7406\u89e3\u3057\u3066\u3044\u306a\u3044\u3001python\u3082\u4f7f\u3063\u305f\u3053\u3068\u306a\u3044\u3001OpenCV\u3082\u5148\u65e5\u4f7f\u3044\u59cb\u3081\u305f\u3070\u304b\u308a\u306a\u8005\u304c\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u8aa4\u308a\u3092\u542b\u3080\u304b\u3082\u3057\u308c\u307e\u305b\u3093)\n\n\n# 2016/08/23\u8ffd\u8a18\ntensorflow\u3068OpenCV\u306fdocker\u4e0a\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u65b9\u304c\u697d\u305d\u3046\u3067\u3059\u3002\n\u4ee5\u4e0b\u3082\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\nhttp://qiita.com/summer4an/items/5a10fe8cc13476f09963\n\n# 2016/02/10\u8ffd\u8a18\n\u30b5\u30fc\u30d0\u4e0a\u3067\u8a66\u3057\u306b\u5224\u5225\u3057\u3066\u307f\u3066\u3044\u305f\u3060\u3051\u308b\u74b0\u5883\u3092\u7528\u610f\u3057\u307e\u3057\u305f\u3002\n\u3088\u3051\u308c\u3070\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\nhttp://www.nakajimadevnakajima.info/va/kyohin/form.html\n\n\n# 2016/03/14\u8ffd\u8a18\n\u30b3\u30fc\u30c9\u3092\u5c11\u3057\u6574\u7406\u3057\u3001github\u3067\u516c\u958b\u3057\u307e\u3057\u305f\u3002\nhttps://github.com/summer4an/kyonyuu_hinnyuu_hanbetu\n\n\n# \u524d\u63d0\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3084\u753b\u50cf\u51e6\u7406\u306e\u52c9\u5f37\u3092\u3057\u305f\u3044\uff0b\u305d\u3046\u3044\u3048\u3070\u5de8\u4e73\u9854\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308b\u3089\u3057\u3044\n\u3000\u2192\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u9854\u5199\u771f\u304b\u3089\u5de8\u4e73\u304b\u3069\u3046\u304b\u3092\u5224\u5225\u3057\u3066\u307f\u3088\u3046\uff01\n\n\u4e00\u5fdc\u307e\u3058\u3081\u306b\u8003\u3048\u3066\u3001\u9854\u304c\u3075\u3063\u304f\u3089\u3057\u3066\u3044\u308c\u3070\u80f8\u3082\u5927\u304d\u3044\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3060\u308d\u3046\u304b\u3089\u3001\u9854\u306e\u3064\u304f\u308a\u3068\u80f8\u306e\u5927\u304d\u3055\u306b\u306f\u76f8\u95a2\u304c\u3042\u308b\u3060\u308d\u3046\u3002\n\u9854\u306e\u4e38\u3055\u3084\u307e\u3076\u305f\u306e\u8089\u306e\u4ed8\u304d\u65b9\u7b49\u306e\u9854\u306e\u7279\u5fb4\u3092\u6570\u5024\u5316\u3057\u3066\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3055\u305b\u308c\u3070\u826f\u3055\u305d\u3046\u3002\n\u3068\u3044\u3046\u3053\u3068\u3067\u3084\u3063\u3066\u307f\u305f\u3002\n\n\n# \u5de8\u4e73\u306e\u65b9\u3068\u8ca7\u4e73\u306e\u65b9\u3092\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\n\u5de8\u4e73\u306a\u65b9\u306f\u3053\u306e\u8fba\u308a\u304b\u3089\u3002\n\n- http://matome.naver.jp/topic/1Luy1\n- http://matome.naver.jp/odai/2138314089511622601\n- http://matome.naver.jp/odai/2144212944181454201?&page=1\n- http://blog.ikebukuroh.com/?eid=688\n\n\u8ca7\u4e73\u306a\u65b9\u306f\u3053\u306e\u8fba\u308a\u304b\u3089\u3002\n\n- http://blog.livedoor.jp/aoba_f/archives/28219941.html\n- http://matome.naver.jp/odai/2144290113141288701\n- http://matome.naver.jp/odai/2140352985012641801\n- http://matome.naver.jp/odai/2134329973691150901\n\n\u305f\u3060\u3057\u5916\u56fd\u306e\u65b9\u306f\u9854\u306e\u3064\u304f\u308a\u304c\u7570\u306a\u308a\u3059\u304e\u3066\u5b66\u7fd2\u304c\u3046\u307e\u304f\u3044\u304f\u6c17\u304c\u3057\u306a\u3044\u306e\u3067\u65e5\u672c\u306e\u65b9\u306b\u9650\u5b9a\u3002\n\u307e\u305f\u3001AV\u5973\u512a\u306e\u65b9\u306f\u6574\u5f62\u3084\u8c4a\u80f8\u306e\u53ef\u80fd\u6027\u304c\u9ad8\u305d\u3046\u306a\u306e\u3067\u9664\u3044\u305f(\u3068\u601d\u3046)\u3002\n\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d\u7a0b\u5ea6\u3002\u30ea\u30b9\u30c8\u306f\u81ea\u7c9b\u3002\u6b32\u3057\u3051\u308c\u3070\u3054\u9023\u7d61\u304f\u3060\u3055\u3044\u3002\n\n\n# \u5199\u771f\u3092\u7247\u3063\u7aef\u304b\u3089\u96c6\u3081\u308b\n\u9069\u5f53\u306a\u753b\u50cf\u691c\u7d22\u30b5\u30a4\u30c8\u304b\u3089\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3002\u4eca\u56de\u306fbing\u753b\u50cf\u691c\u7d22\u3092\u4f7f\u3063\u305f\u3002\n\n```bash:jpggetter.sh\n#!/bin/bash\n\n#\u753b\u50cf\u30b2\u30c3\u30bf\u30fc\n\n#\u5f15\u6570\u3067\u4eba\u540d\u3092\u4e0e\u3048\u308b\u3068\u3001temp_\u307b\u3052\u307b\u3052_gazou/gazou_00001.jpg\u7b49\u3068\u3044\u3046\u9023\u756a\u3067\u753b\u50cf\u3092\u4fdd\u5b58\u3057\u3066\u304f\u308c\u308b\u3002\n#  getter \"\u307b\u3052\u307b\u3052\"\nfunction getter() {\n  for name in \"$@\"; do\n    echo -e '\\n\\n\\n\\n'\n\n    targettext_encoded=`echo \"$name\" | perl -MURI::Escape -lne 'print uri_escape($_)'`\n    url='https://www.bing.com/images/search?FORM=HDRSC2&q='${targettext_encoded}\n    echo target is \"${name}\" \"${url}\"\n\n    if [ -d temp_${name}_gazou ]; then\n      echo already downloaded temp_${name}_gazou\n      continue\n    fi\n\n    mkdir temp_${name}_gazou\n    pushd temp_${name}_gazou\n\n    wget -r -l 1 -H \"${url}\"\n    rm -rf tse1.mm.bing.net/ tse2.mm.bing.net/ tse3.mm.bing.net/ tse4.mm.bing.net/ www.bing.com/ www.bingfudosan.jp/\n    find ./ -not -name '*.jpg' -a -not -name '*.jpeg' -a -not -name '*.JPG' -type f -print0 | xargs -0 rm\n\n    find ./ -type f | awk '{printf(\"mv '\"'\"'%s'\"'\"' gazou_%05d.jpg\\n\", $0, NR);}' > ../temp_${name}_gazou_filename_henkan.sh\n    bash ../temp_${name}_gazou_filename_henkan.sh\n    find ./ -maxdepth 1 -type d -print0 | xargs -0 rm -rf\n\n    popd\n\n    #\u9023\u7d9a\u3067wget\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\n    sleep 5\n  done\n}\n\ngetter \"\u307b\u3052\u307b\u3052\u5b50\" \"\u3082\u3052\u3082\u3052\u7f8e\" \u30fb\u30fb\u30fb\n```\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n\n```bash\n  $ ./jpggetter.sh\n```\n\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d5600\u679a\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d3500\u679a\u7a0b\u5ea6\u96c6\u307e\u3063\u305f\u3002\n\n\n\n# \u5199\u771f\u304b\u3089\u9854\u90e8\u5206\u3060\u3051\u3092\u5207\u308a\u629c\u304d\n\u3053\u306e\u8fba\u3092\u53c2\u8003\u306b\u3002\n\n- \u9854\u8a8d\u8b58 http://nonbiri-tereka.hatenablog.com/entry/2015/05/13/063713\n- \u753b\u50cf\u306e\u5207\u308a\u629c\u304d http://reiji1020.hatenablog.com/entry/2014/10/28/225829\n\n\u305f\u3060\u3057\u8907\u6570\u4eba\u306e\u9854\u304c\u691c\u51fa\u3055\u308c\u305f\u5834\u5408\u306f\u305d\u308c\u305e\u308c\u306e\u540d\u524d\u304c\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u6368\u3066\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\n\u90fd\u5408\u306b\u3088\u308a\u30011\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3059\u308b\u9854\u306e\u5207\u308a\u629c\u304d\u306fwindows\u4e0a\u306eOpenCV\u3092\u4f7f\u3063\u305fexe\u3067\u3001\u305d\u308c\u3092\u5404\u9854\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3059\u308b\u306e\u306fcygwin\u4e0a\u3067\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u884c\u3063\u305f\u3002\nwindows\u4e0a\u3067OpenCV\u3059\u308b\u305f\u3081\u306e\u30e1\u30e2\u306f[\u3053\u3053](http://qiita.com/summer4an/items/a1ce5fdf1ed2f3544562)\u3002\n\n```C++:face_kirinuki.exe\n//\u5b9f\u884c\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3002\n//  hoge.exe \u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30d5\u30a1\u30a4\u30eb\u540d \u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u540d cascade\u30d5\u30a1\u30a4\u30eb\u540d\n//\u623b\u308a\u5024\u306f\u30010\u304c\u6b63\u5e38\u7d42\u4e86\u3001100\u3088\u308a\u4e0b\u306f\u5f15\u6570\u4e0d\u8db3\u7b49\u306e\u5b9f\u884c\u6642\u306e\u7570\u5e38\u3001100\u4ee5\u4e0a\u306fOpenCV\u306f\u8d70\u3063\u305f\u304c\u691c\u51fa\u51fa\u6765\u306a\u304b\u3063\u305f\u7b49\n\n#include \"stdafx.h\"\n#include <opencv2/opencv.hpp>\n#include <iostream>\n#include <windows.h>\n\nusing namespace std;\nusing namespace cv;\n\n#define FACE_KAKUDAI_WARIAI 1.2 //\u4f8b\u3048\u3070\u691c\u51fa\u3055\u308c\u305f\u9854\u304c100px\u56db\u65b9\u3060\u3068\u3057\u305f\u3089\u3001100\u00d7FACE_KAKUDAI_WARIAIpx\u306e\u30b5\u30a4\u30ba\u3067\u5207\u308a\u629c\u304f\n\nint main(int argc, char const *argv[]) {\n\tif(argc!=4){\n\t\tprintf(\"error. too few argument.\\n\");\n\t\texit(1);\n\t}\n\tprintf(\"target file is %s\\n\", argv[1]);\n\n\tMat image = imread(argv[1]);\n\tif(image.data==NULL){\n\t\tprintf(\"error. can not read source picture file.\\n\");\n\t\texit(2);\n\t}\n\tprintf(\"image size width=%d height=%d\\n\", image.size().width, image.size().height);\n\n\tstring cascade_filename = argv[3];\n\tCascadeClassifier cascade;\n\tif(cascade.load(cascade_filename)!=true){\n\t\tprintf(\"error. can not read cascade file.\\n\");\n\t\texit(3);\n\t}\n\n\tvector<Rect> faces;\n\tcascade.detectMultiScale(image, faces, 1.1, 3, 0);\n\tprintf(\"detect %zu faces.\\n\", faces.size());\n\n\tif(faces.size()!=1){\n\t\tprintf(\"too many faces or no faces detected. exit.\\n\");\n\t\texit(100);\n\t}\n\n\tfor (int i = 0; i < faces.size(); i++) {\n\t\tprintf(\"face[%d] x=%d y=%d width=%d height=%d\\n\", i, faces[i].x, faces[i].y, faces[i].width, faces[i].height);\n\n\t\t//\u5207\u308a\u629c\u304f\u30b5\u30a4\u30ba\u3092\u5c11\u3057\u5927\u304d\u304f\u3057\u305f\u6642\u306b\u753b\u50cf\u3092\u306f\u307f\u51fa\u308b\u304b\u3069\u3046\u304b\u78ba\u8a8d\u3002\n\t\tint kakudai_size = (int)(faces[i].width * (FACE_KAKUDAI_WARIAI-1) / 2); //\u5de6\u53f3\u3067\u62e1\u5927\u3059\u308b\u306e\u30672\u3067\u5272\u3063\u3066\u3044\u308b\u3002\n\t\tint kirinuki_x = faces[i].x-kakudai_size;\n\t\tint kirinuki_y = faces[i].y-kakudai_size;\n\t\tint kirinuki_width  = faces[i].width + kakudai_size*2;\n\t\tint kirinuki_height = faces[i].height+ kakudai_size*2;\n\t\tif( kirinuki_x<0 ||\n\t\t\tkirinuki_y<0 ||\n\t\t\timage.size().width  <= kirinuki_x+kirinuki_width ||\n\t\t\timage.size().height <= kirinuki_y+kirinuki_height ){\n\t\t\tprintf(\"face[%d] located on corner. skip.\\n\", i);\n\t\t\texit(101);\n\t\t}\n\n\t\tMat cut_img(image, Rect(kirinuki_x, kirinuki_y, kirinuki_width, kirinuki_height));\n\t\timwrite(argv[2], cut_img);\n\t\tprintf(\"face[%d] outputed as \\\"%s\\\"\\n\", i, argv[2]);\n\n\t\trectangle(image, Point(faces[i].x, faces[i].y), Point(faces[i].x + faces[i].width, faces[i].y + faces[i].height), Scalar(0, 200, 0), 1, CV_AA);\n\t\trectangle(image, Point(kirinuki_x, kirinuki_y), Point(kirinuki_x + kirinuki_width, kirinuki_y + kirinuki_height), Scalar(200, 0, 0), 1, CV_AA);\n\t}\n\n\t//imshow(\"detect face\", image);\n\t//waitKey(0);\n\n\treturn 0;\n}\n```\n\n```bash:face_kirinuki.sh\n#!/bin/bash\n\n#\u9854\u90e8\u5206\u3060\u3051\u5207\u308a\u629c\u304f\u3002\n#\u300c./faces/temp_*_gazou/\u300d\u4ee5\u4e0b\u306e\u5404gazou_*.jpg\u306b\u5bfe\u3057\u3066\u51e6\u7406\u3092\u5b9f\u884c\u3002\n#\u300c./faces/temp_*_gazou/face_gazou_00001.jpg\u300d\u7b49\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u308b\u3002\n\nCASCADE_FILENAME='(haarcascade_frontalface_alt2.xml\u3078\u306e\u30d1\u30b9)'\n\nfor i in ./faces/temp_*_gazou; do\n  echo -e '\\n\\n\\n\\n'\n  echo target dir $i\n  pushd $i\n\n  for j in gazou_*.jpg; do\n    echo -e '\\n'\n    echo -e 'target dir is' \"$i\" ' target file is' \"$j\"\n\n    if [ -f face_$j ]; then\n      echo 'already detected face. skip'\n      continue\n    fi\n\n    echo target file $j\n\n    ./face_kirinuki.exe $j face_$j ${CASCADE_FILENAME}\n\n    #\u3082\u3057\u5224\u5225\u306b\u5931\u6557\u3057\u3066face_*.jpg\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u7a7a\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b\u3002\n    if [ $? -ne 0 ]; then\n      touch face_$j\n      continue\n    fi\n\n    #\u30b5\u30a4\u30ba\u3092\u540c\u3058\u306b\u3059\u308b\n    mogrify -geometry 256x256 face_$j\n  done\n\n  popd\ndone\n```\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n\n```bash\n  $ ./face_kirinuki.sh\n```\n\n\u7d42\u308f\u3063\u305f\u3089\u76ee\u3067\u898b\u3066\u8aa4\u691c\u77e5\u3057\u3066\u3044\u308b\u3082\u306e\u3001\u9854\u306e\u4e00\u90e8\u3057\u304b\u5199\u3063\u3066\u3044\u306a\u3044\u3082\u306e\u3001\u6b63\u9762\u5411\u304d\u3067\u306a\u3044\u3082\u306e\u3001\u305d\u3082\u305d\u3082\u5168\u7136\u9055\u3046\u4eba\u306e\u5199\u771f\u306f\u9664\u53bb\u3057\u3066\u304a\u304f\u3002\n\n\u5de8\u4e73\u306e\u65b9\u306f\u5168250\u540d2500\u679a\u7a0b\u5ea6\u3001\u8ca7\u4e73\u306e\u65b9\u306f\u5168160\u540d1700\u679a\u7a0b\u5ea6\u306b\u306a\u3063\u305f\u3002\n\n![pic_face_list.jpg](https://qiita-image-store.s3.amazonaws.com/0/108313/5bcd7aab-2ab2-1238-7d9e-38afc4ef889d.jpeg)\n\n\n# \u9854\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u5b66\u7fd2\n\u30ed\u30fc\u30ab\u30eb\u3067\u52d5\u304f\u9854\u306e\u4e38\u3055\u7b49\u306e\u7279\u5fb4\u62bd\u51fa\u7528\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u306a\u3044\u304b\u63a2\u3057\u3066\u307f\u305f\u304c\u898b\u3064\u3051\u3089\u308c\u305a\u3002\u81ea\u524d\u3067\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u306e\u306f\u8ae6\u3081\u3002\nweb\u30b5\u30fc\u30d3\u30b9\u306a\u3089\u3053\u306e\u8fba\u308a\u304c\u3042\u308b\u6a21\u69d8\u3002\n\n- detectface http://konisimple.net/lab/detectface_api/\n- ReKognition https://www.rekognition.com/\n\n\u7279\u5fb4\u91cf\u3092\u6c42\u3081\u3066\u304b\u3089\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306eChainer\u3084TensorFlow\u306b\u98df\u308f\u305b\u3088\u3046\u3068\u8003\u3048\u3066\u3044\u305f\u304c\u3001\u305d\u3082\u305d\u3082\u4f55\u3082\u8003\u3048\u305a\u306b\u3076\u3063\u3053\u3080\u3060\u3051\u3067\u753b\u50cf\u5206\u985e\u3057\u3066\u304f\u308c\u308b\u3063\u307d\u3044\u306e\u3067\u3084\u3063\u3066\u307f\u308b\u3002\n\n- TensorFlow http://kivantium.hateblo.jp/entry/2015/11/18/233834\n- Chainer http://hi-king.hatenablog.com/entry/2015/06/11/021144\n- Chainer http://d.hatena.ne.jp/shi3z/20150709/1436397615\n\nChainer\u306f\u306a\u305c\u304b\u3046\u307e\u304f\u52d5\u304b\u306a\u304b\u3063\u305f\u306e\u3067TensorFlow\u3067\u3002\n\n\n\n## TensorFlow\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\u4ee5\u4e0b\u516c\u5f0f\u30b5\u30a4\u30c8\u306e\u300cPip Installation\u300d\u3092\u53c2\u7167\u3002python\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306fpip\u3068\u3044\u3046\u3082\u306e\u3092\u4f7f\u3063\u3066\u5165\u308c\u308b\u3089\u3057\u3044\u3002\n\n- https://www.tensorflow.org/versions/master/get_started/os_setup.html\n\n```bash:\n$ sudo apt-get install python-pip python-dev\n$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n```\n\nvirtual box\u4e0aUbuntu14.04 64bit\u3092\u4f7f\u7528\u3002\n\u308d\u304f\u306aGPU\u304c\u8f09\u3063\u3066\u3044\u306a\u3044\u306e\u3067CPU\u7248\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305f\u3002\u7279\u306b\u96e3\u3057\u304f\u306a\u3044\u3002\n\u300cRun TensorFlow from the Command Line\u300d\u306e\u7b87\u6240\u3092\u53c2\u7167\u3057\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u304a\u304f\u3002\n\n\n## \u5b66\u7fd2\n\u96c6\u3081\u305f\u9854\u5199\u771f\u30d5\u30a1\u30a4\u30eb\u7fa4\u304b\u3089\u3001\u5b66\u7fd2\u7528\u306e\u65b9\u3068\u3001\u30c6\u30b9\u30c8\u7528\u306e\u65b9\u3068\u3001\u6700\u5f8c\u306b\u5de8\u4e73\u304b\u5224\u5225\u3059\u308b\u304a\u8a66\u3057\u7528\u306e\u65b9\u3092\u5206\u3051\u3066\u304a\u304f\u3002\n\u5b66\u7fd2\u7528\u306fface_filelist_train.txt\u3001\u30c6\u30b9\u30c8\u7528\u306fface_filelist_test.txt\u306b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u30ea\u30b9\u30c8\u30a2\u30c3\u30d7\u3002\n\n```\n  (\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9) (\u5206\u985e \u5de8\u4e73\u306a\u30891\u3001\u8ca7\u4e73\u306a\u30890)\n  ./faces/face_hogeko_00001.jpg 1\n  ./faces/face_mogemi_00001.jpg 0\n  \u30fb\u30fb\u30fb\n```\n\n\u3082\u3057\u304b\u3057\u305f\u3089\u30ea\u30b9\u30c8\u306f\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3066\u304a\u3044\u305f\u65b9\u304c\u826f\u3044\u304b\u3082\u3002\u524d\u534a\u304c\u5de8\u4e73\u3001\u5f8c\u534a\u304c\u8ca7\u4e73\u7b49\u4e26\u3093\u3067\u3044\u308b\u3068\u5b66\u7fd2\u304c\u9032\u307e\u306a\u3044\u304b\u3082(\u8a66\u884c\u932f\u8aa4\u3057\u3066\u3044\u305f\u6642\u3060\u3063\u305f\u306e\u3067\u4ed6\u306e\u539f\u56e0\u304b\u3082\u3057\u308c\u306a\u3044)\u3002sort\u306e-R\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u4f7f\u3048\u3070OK\u3002\n\n\u4ee5\u4e0b\u306b\u3088\u308b\u3068\u5b66\u7fd2\u7528\u306b\u306f60\uff5e80%\u5272\u308a\u5f53\u3066\u308b\u3068\u826f\u3044\u3068\u306e\u3053\u3068\u3002\n\n- https://www.researchgate.net/post/What_is_the_best_way_to_divide_our_dataset_into_training_and_test_sets\n\n\u5b66\u7fd2\u7528\u306b3000\u679a\u3001\u30c6\u30b9\u30c8\u7528\u306b1200\u679a\u3092\u5272\u308a\u5f53\u3066\u305f\u3002\n\n![pic_filelist.jpg](https://qiita-image-store.s3.amazonaws.com/0/108313/167b7856-0451-6a91-a3c2-f9baa714cf44.jpeg)\n\n\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u300c\u30b3\u30fc\u30c9\u5168\u4f53\u300d\u304b\u3089\u62dd\u501f\u3002\n\n- http://kivantium.hateblo.jp/entry/2015/11/18/233834\n\n\u305f\u3060\u7a00\u306b\u300cReluGrad input is not finite. : Tensor had NaN values\u300d\u3068\u3044\u3046\u30a8\u30e9\u30fc\u3067\u7570\u5e38\u7d42\u4e86\u3059\u308b\u3053\u3068\u304c\u3042\u3063\u305f\u306e\u3067\u3001\u4ee5\u4e0b\u53c2\u8003\u306b\u7570\u5e38\u51e6\u7406\u3092\u8ffd\u52a0\u3002\n\n- http://qiita.com/ikki8412/items/3846697668fc37e3b7e0\n\n```python:gakusyuu.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.python.platform\n\nNUM_CLASSES = 2\nIMAGE_SIZE = 28\nIMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE*3\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('train', 'face_filelist_train.txt', 'File name of train data')\nflags.DEFINE_string('test', 'face_filelist_test.txt', 'File name of train data')\nflags.DEFINE_string('train_dir', './logdata', 'Directory to put the training data.')\nflags.DEFINE_integer('max_steps', 100, 'Number of steps to run trainer.')\nflags.DEFINE_integer('batch_size', 10, 'Batch size'\n                     'Must divide evenly into the dataset sizes.')\nflags.DEFINE_float('learning_rate', 1e-4, 'Initial learning rate.')\n\ndef inference(images_placeholder, keep_prob):\n    \"\"\" \u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      images_placeholder: \u753b\u50cf\u306eplaceholder\n      keep_prob: dropout\u7387\u306eplace_holder\n\n    \u8fd4\u308a\u5024:\n      y_conv: \u5404\u30af\u30e9\u30b9\u306e\u78ba\u7387(\u306e\u3088\u3046\u306a\u3082\u306e)\n    \"\"\"\n    # \u91cd\u307f\u3092\u6a19\u6e96\u504f\u5dee0.1\u306e\u6b63\u898f\u5206\u5e03\u3067\u521d\u671f\u5316\n    def weight_variable(shape):\n      initial = tf.truncated_normal(shape, stddev=0.1)\n      return tf.Variable(initial)\n\n    # \u30d0\u30a4\u30a2\u30b9\u3092\u6a19\u6e96\u504f\u5dee0.1\u306e\u6b63\u898f\u5206\u5e03\u3067\u521d\u671f\u5316\n    def bias_variable(shape):\n      initial = tf.constant(0.1, shape=shape)\n      return tf.Variable(initial)\n\n    # \u7573\u307f\u8fbc\u307f\u5c64\u306e\u4f5c\u6210\n    def conv2d(x, W):\n      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u4f5c\u6210\n    def max_pool_2x2(x):\n      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1], padding='SAME')\n\n    # \u5165\u529b\u309228x28x3\u306b\u5909\u5f62\n    x_image = tf.reshape(images_placeholder, [-1, 28, 28, 3])\n\n    # \u7573\u307f\u8fbc\u307f\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('conv1') as scope:\n        W_conv1 = weight_variable([5, 5, 3, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('pool1') as scope:\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # \u7573\u307f\u8fbc\u307f\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('conv2') as scope:\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # \u30d7\u30fc\u30ea\u30f3\u30b0\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('pool2') as scope:\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # \u5168\u7d50\u5408\u5c641\u306e\u4f5c\u6210\n    with tf.name_scope('fc1') as scope:\n        W_fc1 = weight_variable([7*7*64, 1024])\n        b_fc1 = bias_variable([1024])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        # dropout\u306e\u8a2d\u5b9a\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # \u5168\u7d50\u5408\u5c642\u306e\u4f5c\u6210\n    with tf.name_scope('fc2') as scope:\n        W_fc2 = weight_variable([1024, NUM_CLASSES])\n        b_fc2 = bias_variable([NUM_CLASSES])\n\n    # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u306b\u3088\u308b\u6b63\u898f\u5316\n    with tf.name_scope('softmax') as scope:\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    # \u5404\u30e9\u30d9\u30eb\u306e\u78ba\u7387\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u8fd4\u3059\n    return y_conv\n\ndef loss(logits, labels):\n    \"\"\" loss\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      logits: \u30ed\u30b8\u30c3\u30c8\u306etensor, float - [batch_size, NUM_CLASSES]\n      labels: \u30e9\u30d9\u30eb\u306etensor, int32 - [batch_size, NUM_CLASSES]\n\n    \u8fd4\u308a\u5024:\n      cross_entropy: \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306etensor, float\n\n    \"\"\"\n\n    # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u8a08\u7b97\n    #cross_entropy = -tf.reduce_sum(labels*tf.log(logits)) \u7570\u5e38\u51e6\u7406 http://qiita.com/ikki8412/items/3846697668fc37e3b7e0\n    cross_entropy = -tf.reduce_sum(labels*tf.log(tf.clip_by_value(logits,1e-10,1.0)))\n    # TensorBoard\u3067\u8868\u793a\u3059\u308b\u3088\u3046\u6307\u5b9a\n    tf.scalar_summary(\"cross_entropy\", cross_entropy)\n    return cross_entropy\n\ndef training(loss, learning_rate):\n    \"\"\" \u8a13\u7df4\u306eOp\u3092\u5b9a\u7fa9\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      loss: \u640d\u5931\u306etensor, loss()\u306e\u7d50\u679c\n      learning_rate: \u5b66\u7fd2\u4fc2\u6570\n\n    \u8fd4\u308a\u5024:\n      train_step: \u8a13\u7df4\u306eOp\n\n    \"\"\"\n\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    return train_step\n\ndef accuracy(logits, labels):\n    \"\"\" \u6b63\u89e3\u7387(accuracy)\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      logits: inference()\u306e\u7d50\u679c\n      labels: \u30e9\u30d9\u30eb\u306etensor, int32 - [batch_size, NUM_CLASSES]\n\n    \u8fd4\u308a\u5024:\n      accuracy: \u6b63\u89e3\u7387(float)\n\n    \"\"\"\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    tf.scalar_summary(\"accuracy\", accuracy)\n    return accuracy\n\nif __name__ == '__main__':\n    # \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f\n    f = open(FLAGS.train, 'r')\n    # \u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u914d\u5217\n    train_image = []\n    train_label = []\n    for line in f:\n        # \u6539\u884c\u3092\u9664\u3044\u3066\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u306b\u3059\u308b\n        line = line.rstrip()\n        l = line.split()\n        # \u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u306728x28\u306b\u7e2e\u5c0f\n\n        print \"aaaaa line[%s] %s %s\"%(line,l[0],l[1])\n\n        img = cv2.imread(l[0])\n        img = cv2.resize(img, (28, 28))\n        # \u4e00\u5217\u306b\u3057\u305f\u5f8c\u30010-1\u306efloat\u5024\u306b\u3059\u308b\n        train_image.append(img.flatten().astype(np.float32)/255.0)\n        # \u30e9\u30d9\u30eb\u30921-of-k\u65b9\u5f0f\u3067\u7528\u610f\u3059\u308b\n        tmp = np.zeros(NUM_CLASSES)\n        tmp[int(l[1])] = 1\n        train_label.append(tmp)\n    # numpy\u5f62\u5f0f\u306b\u5909\u63db\n    train_image = np.asarray(train_image)\n    train_label = np.asarray(train_label)\n    f.close()\n\n    f = open(FLAGS.test, 'r')\n    test_image = []\n    test_label = []\n    for line in f:\n        line = line.rstrip()\n        l = line.split()\n        img = cv2.imread(l[0])\n        img = cv2.resize(img, (28, 28))\n\n        print \"aaaaa2 line[%s] %s %s\"%(line,l[0],l[1])\n\n        test_image.append(img.flatten().astype(np.float32)/255.0)\n        tmp = np.zeros(NUM_CLASSES)\n        tmp[int(l[1])] = 1\n        test_label.append(tmp)\n    test_image = np.asarray(test_image)\n    test_label = np.asarray(test_label)\n    f.close()\n\n    with tf.Graph().as_default():\n        # \u753b\u50cf\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        images_placeholder = tf.placeholder(\"float\", shape=(None, IMAGE_PIXELS))\n        # \u30e9\u30d9\u30eb\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        labels_placeholder = tf.placeholder(\"float\", shape=(None, NUM_CLASSES))\n        # dropout\u7387\u3092\u5165\u308c\u308b\u4eee\u306eTensor\n        keep_prob = tf.placeholder(\"float\")\n\n        # inference()\u3092\u547c\u3073\u51fa\u3057\u3066\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\n        logits = inference(images_placeholder, keep_prob)\n        # loss()\u3092\u547c\u3073\u51fa\u3057\u3066\u640d\u5931\u3092\u8a08\u7b97\n        loss_value = loss(logits, labels_placeholder)\n        # training()\u3092\u547c\u3073\u51fa\u3057\u3066\u8a13\u7df4\n        train_op = training(loss_value, FLAGS.learning_rate)\n        # \u7cbe\u5ea6\u306e\u8a08\u7b97\n        acc = accuracy(logits, labels_placeholder)\n\n        # \u4fdd\u5b58\u306e\u6e96\u5099\n        saver = tf.train.Saver()\n        # Session\u306e\u4f5c\u6210\n        sess = tf.Session()\n        # \u5909\u6570\u306e\u521d\u671f\u5316\n        sess.run(tf.initialize_all_variables())\n        # TensorBoard\u3067\u8868\u793a\u3059\u308b\u5024\u306e\u8a2d\u5b9a\n        summary_op = tf.merge_all_summaries()\n        summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph_def)\n\n        # \u8a13\u7df4\u306e\u5b9f\u884c\n        for step in range(FLAGS.max_steps):\n            for i in range(len(train_image)/FLAGS.batch_size):\n                # batch_size\u5206\u306e\u753b\u50cf\u306b\u5bfe\u3057\u3066\u8a13\u7df4\u306e\u5b9f\u884c\n                batch = FLAGS.batch_size*i\n                # feed_dict\u3067placeholder\u306b\u5165\u308c\u308b\u30c7\u30fc\u30bf\u3092\u6307\u5b9a\u3059\u308b\n                sess.run(train_op, feed_dict={\n                  images_placeholder: train_image[batch:batch+FLAGS.batch_size],\n                  labels_placeholder: train_label[batch:batch+FLAGS.batch_size],\n                  keep_prob: 0.5})\n\n            # 1 step\u7d42\u308f\u308b\u305f\u3073\u306b\u7cbe\u5ea6\u3092\u8a08\u7b97\u3059\u308b\n            train_accuracy = sess.run(acc, feed_dict={\n                images_placeholder: train_image,\n                labels_placeholder: train_label,\n                keep_prob: 1.0})\n            print \"step %d, training accuracy %g\"%(step, train_accuracy)\n\n            # 1 step\u7d42\u308f\u308b\u305f\u3073\u306bTensorBoard\u306b\u8868\u793a\u3059\u308b\u5024\u3092\u8ffd\u52a0\u3059\u308b\n            summary_str = sess.run(summary_op, feed_dict={\n                images_placeholder: train_image,\n                labels_placeholder: train_label,\n                keep_prob: 1.0})\n            summary_writer.add_summary(summary_str, step)\n\n    # \u8a13\u7df4\u304c\u7d42\u4e86\u3057\u305f\u3089\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u3092\u8868\u793a\n    print \"test accuracy %g\"%sess.run(acc, feed_dict={\n        images_placeholder: test_image,\n        labels_placeholder: test_label,\n        keep_prob: 1.0})\n\n    # \u6700\u7d42\u7684\u306a\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n    save_path = saver.save(sess, \"model.ckpt\")\n```\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n\n```bash\n  $ python gakusyuu.py\n```\n\nCore i7 2.1GHz win7\u306e\u30ce\u30fc\u30c8PC\u4e0a\u3067\u3001virtual box\u3067ubuntu14.04\u306b4CPU 4GB\u30e1\u30e2\u30ea\u5272\u308a\u5f53\u3066\u3066\u52d5\u304b\u3057\u305f\u4e0a\u3067\u5b9f\u884c\u3002100\u56de\u306e\u5b66\u7fd2\u306730\u5206\u7a0b\u5ea6\u3067\u7d42\u308f\u308b\u3002\n\n\u5b66\u7fd2\u306e\u9014\u4e2d\u7d4c\u904e\u306f\u4ee5\u4e0b\u5b9f\u884c\u3057\u305f\u5f8c\u3001\u30d6\u30e9\u30a6\u30b6\u3067\u300c http://localhost:6006 \u300d\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308c\u3070\u898b\u308c\u308b\u3002\n\n```\n  $ tensorboard --logdir=(\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u3042\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u7d76\u5bfe\u30d1\u30b9)\n  $ tensorboard --logdir=/home/hoge/kyonyuu_hanbetu/logdata\n```\n\n![pic3_tensorboard.jpg](https://qiita-image-store.s3.amazonaws.com/0/108313/b27cfc69-6672-6c2b-431f-a1de2a78420c.jpeg)\n\n\u6700\u7d42\u7684\u306b\u5b66\u7fd2\u7528\u306e\u9854\u306b\u5bfe\u3057\u3066\u306f\u300cstep 99, training accuracy 0.980135\u300d\u306b\u306a\u3063\u305f\u3002\n\n\u305f\u3060\u30c6\u30b9\u30c8\u7528\u306e\u9854\u306b\u5bfe\u3057\u3066\u306e\u5224\u5b9a\u7d50\u679c\u304c\u300ctest accuracy 0.534127\u300d\u3068\u306e\u3053\u3068\u3002\u307b\u307c\u5224\u5225\u3067\u304d\u3066\u3044\u306a\u3044\u2026\n\n\n\n# \u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u5224\u5b9a\n\u3068\u308a\u3042\u3048\u305a\u5b66\u7fd2\u7528\u306b\u3082\u30c6\u30b9\u30c8\u7528\u306b\u3082\u4f7f\u3063\u3066\u3044\u306a\u3044\u65b9\u306e\u9854\u5199\u771f\u3067\u5224\u5b9a\u3057\u3066\u307f\u308b\u3002\n\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u300c\u753b\u50cf\u306b\u5bfe\u3057\u3066\u4e88\u60f3\u30e9\u30d9\u30eb\u3092\u8868\u793a\u3059\u308b\u300d\u306e\u4e0b\u304b\u3089\u62dd\u501f\u3002\n\n- http://kivantium.hateblo.jp/entry/2015/11/18/233834\n\n\u30e1\u30a4\u30f3\u306fpython\u30b3\u30fc\u30c9\u3067\u3001\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u306f\u30ed\u30b0\u3092\u8a18\u9332\u3057\u3066\u3044\u308b\u3060\u3051\u3002\n\n```python:hanbetu__all_file.py\n#!/usr/bin/env python\n#! -*- coding: utf-8 -*-\n\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\n\nNUM_CLASSES = 2\nIMAGE_SIZE = 28\nIMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE*3\n\ndef inference(images_placeholder, keep_prob):\n    \"\"\" \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u95a2\u6570\n\n    \u5f15\u6570:\n      images_placeholder: inputs()\u3067\u4f5c\u6210\u3057\u305f\u753b\u50cf\u306eplaceholder\n      keep_prob: dropout\u7387\u306eplace_holder\n\n    \u8fd4\u308a\u5024:\n      cross_entropy: \u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u7d50\u679c\n    \"\"\"\n    def weight_variable(shape):\n      initial = tf.truncated_normal(shape, stddev=0.1)\n      return tf.Variable(initial)\n\n    def bias_variable(shape):\n      initial = tf.constant(0.1, shape=shape)\n      return tf.Variable(initial)\n\n    def conv2d(x, W):\n      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n    def max_pool_2x2(x):\n      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                            strides=[1, 2, 2, 1], padding='SAME')\n\n    x_image = tf.reshape(images_placeholder, [-1, 28, 28, 3])\n\n    with tf.name_scope('conv1') as scope:\n        W_conv1 = weight_variable([5, 5, 3, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    with tf.name_scope('pool1') as scope:\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    with tf.name_scope('conv2') as scope:\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    with tf.name_scope('pool2') as scope:\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    with tf.name_scope('fc1') as scope:\n        W_fc1 = weight_variable([7*7*64, 1024])\n        b_fc1 = bias_variable([1024])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    with tf.name_scope('fc2') as scope:\n        W_fc2 = weight_variable([1024, NUM_CLASSES])\n        b_fc2 = bias_variable([NUM_CLASSES])\n\n    with tf.name_scope('softmax') as scope:\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    return y_conv\n\nif __name__ == '__main__':\n    test_image = []\n    filenames = []\n    for i in range(1, len(sys.argv)):\n        img = cv2.imread(sys.argv[i])\n        img = cv2.resize(img, (28, 28))\n        test_image.append(img.flatten().astype(np.float32)/255.0)\n        filenames.append(sys.argv[i])\n    test_image = np.asarray(test_image)\n\n    images_placeholder = tf.placeholder(\"float\", shape=(None, IMAGE_PIXELS))\n    labels_placeholder = tf.placeholder(\"float\", shape=(None, NUM_CLASSES))\n    keep_prob = tf.placeholder(\"float\")\n\n    logits = inference(images_placeholder, keep_prob)\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    saver.restore(sess, \"model.ckpt\")\n\n    for i in range(len(test_image)):\n        pred = np.argmax(logits.eval(feed_dict={\n            images_placeholder: [test_image[i]],\n            keep_prob: 1.0 })[0])\n        pred2 = logits.eval(feed_dict={\n            images_placeholder: [test_image[i]],\n            keep_prob: 1.0 })[0]\n        print filenames[i],pred,\"{0:10.8f}\".format(pred2[0]),\"{0:10.8f}\".format(pred2[1])\n```\n\n```bash:hanbetu_all.sh\n#!/bin/bash\npython ./hanbetu__all_file.py \"$@\" >> log_hanbetu_`date +%Y%m%d_%H%M%S`.txt\n```\n\n\u4ee5\u4e0b\u3067\u5b9f\u884c\u3002\n\n```bash\n  $ ./hanbetu_all.sh (\u5224\u5225\u3057\u305f\u3044\u30d5\u30a1\u30a4\u30eb \u8907\u6570)\n```\n\nlog_hanbetu_XXX.txt\u306b\u7d50\u679c\u304c\u66f8\u304d\u8fbc\u307e\u308c\u3001\u5de8\u4e73\u3068\u5224\u5225\u3057\u305f\u3082\u306e\u306b\u306f1\u304c\u3001\u8ca7\u4e73\u3068\u5224\u5225\u3057\u305f\u3082\u306e\u306b\u306f0\u304c\u3064\u304f\u3002\u307e\u305f\u3001\u5de8\u4e73\u306e\u53ef\u80fd\u6027\u3068\u8ca7\u4e73\u306e\u53ef\u80fd\u6027\u3082\u6570\u5024\u3067\u8868\u793a\u3055\u308c\u308b\u3002\n\n\u5de8\u4e73\u306e\u65b9\u51688\u540d84\u679a\u3001\u8ca7\u4e73\u306e\u65b9\u51688\u540d81\u679a\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3002\n\u7d50\u679c\u306f\u4ee5\u4e0b\u3002\n\n- \u5de8\u4e73\u306e\u65b9\u3092\u5de8\u4e73\u3068\u6b63\u3057\u304f\u5224\u5225\u51fa\u6765\u305f\u306e\u306f82%(69/84)\n- \u8ca7\u4e73\u306e\u65b9\u3092\u8ca7\u4e73\u3068\u6b63\u3057\u304f\u5224\u5225\u51fa\u6765\u305f\u306e\u306f37%(30/81)\n\n![pic4_seikairitu.jpg](https://qiita-image-store.s3.amazonaws.com/0/108313/3cb7dc99-5581-fe44-3f2c-59a15ba5aeb7.jpeg)\n\n\u5de8\u4e73\u306e\u65b9\u3060\u3051\u898b\u308b\u3068\u3088\u3055\u6c17\u3060\u304c\u3001\u5358\u306b\u591a\u304f\u3092\u5de8\u4e73\u3068\u5224\u5225\u3057\u3066\u3044\u308b\u3060\u3051\u304b\u2026\uff1f\n\u305f\u3060\u5de8\u4e73\u306e\u65b9\u3067\u9ad8\u3044\u78ba\u7387\u3067\u5f53\u3066\u3066\u3044\u308b\u65b9\u3082\u3044\u308b\u306e\u3067\u3001\u5f53\u3066\u3084\u3059\u3044\u9854\u306e\u65b9\u306a\u306e\u304b\u3082\u3002\n\n# \u307e\u3068\u3081\n\u3044\u307e\u3044\u3061\u3046\u307e\u304f\u3044\u3063\u305f\u304b\u5fae\u5999\u3002\n\u6557\u56e0\u3068\u3057\u3066\u601d\u3044\u3064\u304f\u306e\u306f\u4ee5\u4e0b\u3002\n\n- TensorFlow\u306e\u4f7f\u3044\u65b9\u3092\u9593\u9055\u3063\u3066\u3044\u308b\uff1f\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\uff1f\u5de8\u4e73\u9854\u306b\u3082\u7a2e\u985e\u304c\u3042\u308a\u305d\u3046\u3060\u3057\u3001\u5de8\u4e73\u306a\u65b9\u306e\u4e2d\u306b\u3082\u5de8\u4e73\u9854\u306e\u4eba\u3068\u666e\u901a\u9854\u306e\u4eba\u304c\u3044\u305d\u3046\u306a\u306e\u3067\u3001\u5b66\u7fd2\u6642\u306b1\u3064\u306eCLASS\u306b\u307e\u3068\u3081\u3066\u3057\u307e\u3046\u306e\u304c\u307e\u305a\u3044\uff1f\n- \u5de8\u4e73\u3084\u8ca7\u4e73\u3092\u5224\u65ad\u3059\u308b\u6642\u306e\u57fa\u6e96\u304c\u4eba\u305d\u308c\u305e\u308c\u3067\u66d6\u6627\u3002\u672c\u5f53\u306b\u305d\u3046\u304b\u3069\u3046\u304b\u306f\u81ea\u5206\u3067\u306f\u672a\u78ba\u8a8d\u3002\n- \u6574\u5f62\u3084\u8c4a\u80f8\u3084\u507d\u4e73\u3084\u30d1\u30c3\u30c9\u7b49\u3060\u3068\u56f0\u308b\u304c\u3001\u5224\u5225\u3067\u304d\u306a\u3044\u3002\n- \u305d\u3082\u305d\u3082\u5de8\u4e73\u9854\u306a\u3093\u3066\u3082\u306e\u306f\u306a\u3044\uff1f\n\n\u3082\u3057\u3046\u307e\u304f\u5224\u5225\u51fa\u6765\u3066\u3044\u305f\u3089\u3053\u3093\u306a\u5fdc\u7528\u304c\u3067\u304d\u308b\u304b\u3082\uff1f\n\n- SNS\u7b49\u3067\u9854\u5199\u771f\u3060\u3051\u3082\u3089\u3063\u305f\u5834\u5408\u306b\u4e73\u4e88\u6e2c\n- \u5199\u771f\u304b\u3089\u507d\u4e73\u5224\u5b9a\n- \u30a4\u30e9\u30b9\u30c8\u306b\u3082\u9069\u7528\u3067\u304d\u308b\u306a\u3089\u3001\u59a5\u5f53\u306a\u4e73\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u3089\u308c\u308b\n\n\u6c17\u3065\u3044\u305f\u3053\u3068\u3002\n\n- \u5224\u5225\u7d50\u679c\u304c\u81ea\u5206\u306e\u601d\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u7570\u306a\u308b\u3068\u3001\u8a00\u308f\u308c\u3066\u307f\u308b\u3068\u305d\u3046\u304b\u3082\u3001\u3068\u3044\u3046\u6c17\u306b\u306a\u308b\u3002\n- \u5927\u91cf\u306e\u9854\u753b\u50cf\u304b\u3089\u3075\u3055\u308f\u3057\u304f\u306a\u3044\u3082\u306e\u3092\u5f3e\u304f\u305f\u3081\u306b\u9ad8\u901f\u3067\u30b9\u30af\u30ed\u30fc\u30eb\u3057\u307e\u304f\u308a\u306a\u304c\u3089\u5224\u5225\u3057\u3066\u3044\u308b\u3068\u932f\u8996\u306eFlashed Face Distortion Effect\u3068\u3084\u3089\u304c\u8d77\u304d\u3066\u6016\u3044\u3002\u591c\u306b\u3084\u3063\u3066\u306f\u3044\u3051\u306a\u3044\u3002\n- \u753b\u50cf\u3092\u96c6\u3081\u305f\u65b9\u3005\u306e\u4e2d\u306b\u3001\u5de6\u5411\u304d\u3070\u3063\u304b\u308a\u306e\u4eba\u3001\u53f3\u5411\u304d\u3070\u3063\u304b\u308a\u306e\u4eba\u304c\u3044\u308b\u3002\u305d\u3061\u3089\u304c\u305d\u306e\u4eba\u306e\u4e00\u62bc\u3057\u306e\u5411\u304d\u306a\u306e\u304b\u3082\u3002\n", "tags": ["\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "TensorFlow", "\u753b\u50cf\u51e6\u7406", "OpenCV", "DeepLearning"]}