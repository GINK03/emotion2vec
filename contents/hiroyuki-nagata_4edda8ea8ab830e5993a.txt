{"context": "\n\nHadoop\u4f7f\u304a\u3046\n\nS3\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u3092Hive\u3067HDFS\u306b\u53d6\u308a\u8fbc\u307f\u305f\u3044\n\u30d5\u30a1\u30a4\u30eb\u306fgzip\u5727\u7e2e\u3055\u308c\u3066\u304a\u308a\u3001TSV\u5f62\u5f0f\u3060\nEMR\u4e0a\u3067Hadoop+Hive, Pig\u306a\u3069\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u30c6\u30b9\u30c8\u3057\u3066\u3044\u308b\n\n\u74b0\u5883\u8a2d\u5b9a\u306f AWS\u306eEMR\u3067\u5206\u6563\u30af\u30a8\u30ea\u30a8\u30f3\u30b8\u30f3Presto\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u2192ruby\u304b\u3089\u63a5\u7d9a\u3057\u3066S3\u306e\u5927\u91cf\u30d5\u30a1\u30a4\u30eb\u64cd\u4f5c\u4f53\u9a13\u3092\u3057\u3066\u307f\u3088\u3046\uff01 \u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\nHive\n\nHow to load CSV data with enclosed by double quotes and separated by tab into HIVE table?\n\n\u3053\u3053\u306e\u56de\u7b54\u306b\u5f93\u3046\u3068\u3001Hive\u306e\u5185\u90e8\u30af\u30e9\u30b9 org.apache.hadoop.hive.serde2.OpenCSVSerde \u3092\u4f7f\u3048\u3070\u3001TSV\u5f62\u5f0f\u3067\u53d6\u308a\u8fbc\u3081\u305f\u3002\n\n\u30b3\u30de\u30f3\u30c9\u306e\u30a4\u30e1\u30fc\u30b8\n\n\nsql\n$ hive\n> CREATE EXTERNAL TABLE example (\n- \u5b9a\u7fa9\u2026\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   \"separatorChar\" = \"\\t\",\n   \"quoteChar\"     = \"\\\"\"\n)\nLOCATION 's3n://your-s3-directory-is/'\n;\n\n-- \u30b3\u30de\u30f3\u30c9\u304cOK\u3067\u8fd4\u3063\u305f\u3089hive\u304b\u3089select\u3067\u304d\u308b\n> select * from example limit 20;\n\n\nHive\u3067\u4f5c\u3063\u305f\u30c6\u30fc\u30d6\u30eb\u306fPresto\u304b\u3089\u3082\u30af\u30a8\u30ea\u3092\u304b\u3051\u3089\u308c\u308b\u3002\n\nPig\n\nHow to access data files stored in AWS S3 buckets from HDP using HDFS\n\n\u3053\u3053\u306e\u56de\u7b54\u306b\u5f93\u3063\u3066\u3001\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u8fbc\u3082\u3046\u3068\u3057\u305f\u306e\u3067\u3059\u304cPig\u304b\u3089\u306e\u5fdc\u7b54\u304c\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u2026\n\nbash\n$ pig -x tez\ngrunt> a = load 's3n://your-s3-directory-is/here.gz' USING TextLoader AS (line:chararray);\ngrunt> dump a;\n...(\u30ed\u30b0\u306f\u51fa\u308b\u304c\u5fdc\u7b54\u306a\u3057\uff09\n\n\n# Hadoop\u4f7f\u304a\u3046\n- S3\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u3092Hive\u3067HDFS\u306b\u53d6\u308a\u8fbc\u307f\u305f\u3044\n- \u30d5\u30a1\u30a4\u30eb\u306fgzip\u5727\u7e2e\u3055\u308c\u3066\u304a\u308a\u3001TSV\u5f62\u5f0f\u3060\n- EMR\u4e0a\u3067Hadoop+Hive, Pig\u306a\u3069\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u30c6\u30b9\u30c8\u3057\u3066\u3044\u308b\n\n\u74b0\u5883\u8a2d\u5b9a\u306f [AWS\u306eEMR\u3067\u5206\u6563\u30af\u30a8\u30ea\u30a8\u30f3\u30b8\u30f3Presto\u30af\u30e9\u30b9\u30bf\u4f5c\u6210\u2192ruby\u304b\u3089\u63a5\u7d9a\u3057\u3066S3\u306e\u5927\u91cf\u30d5\u30a1\u30a4\u30eb\u64cd\u4f5c\u4f53\u9a13\u3092\u3057\u3066\u307f\u3088\u3046\uff01](http://qiita.com/mix/items/887c545daf5491eed552) \u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n## Hive\n- [How to load CSV data with enclosed by double quotes and separated by tab into HIVE table?](http://stackoverflow.com/a/38852554/2565527)\n\n\u3053\u3053\u306e\u56de\u7b54\u306b\u5f93\u3046\u3068\u3001Hive\u306e\u5185\u90e8\u30af\u30e9\u30b9 `org.apache.hadoop.hive.serde2.OpenCSVSerde` \u3092\u4f7f\u3048\u3070\u3001TSV\u5f62\u5f0f\u3067\u53d6\u308a\u8fbc\u3081\u305f\u3002\n\n- \u30b3\u30de\u30f3\u30c9\u306e\u30a4\u30e1\u30fc\u30b8\n\n```:sql\n$ hive\n> CREATE EXTERNAL TABLE example (\n- \u5b9a\u7fa9\u2026\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   \"separatorChar\" = \"\\t\",\n   \"quoteChar\"     = \"\\\"\"\n)\nLOCATION 's3n://your-s3-directory-is/'\n;\n\n-- \u30b3\u30de\u30f3\u30c9\u304cOK\u3067\u8fd4\u3063\u305f\u3089hive\u304b\u3089select\u3067\u304d\u308b\n> select * from example limit 20;\n```\n\nHive\u3067\u4f5c\u3063\u305f\u30c6\u30fc\u30d6\u30eb\u306fPresto\u304b\u3089\u3082\u30af\u30a8\u30ea\u3092\u304b\u3051\u3089\u308c\u308b\u3002\n\n## Pig\n- [How to access data files stored in AWS S3 buckets from HDP using HDFS](https://community.hortonworks.com/articles/25578/how-to-access-data-files-stored-in-aws-s3-buckets.html)\n\n\u3053\u3053\u306e\u56de\u7b54\u306b\u5f93\u3063\u3066\u3001\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u8fbc\u3082\u3046\u3068\u3057\u305f\u306e\u3067\u3059\u304cPig\u304b\u3089\u306e\u5fdc\u7b54\u304c\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u2026\n\n```:bash\n$ pig -x tez\ngrunt> a = load 's3n://your-s3-directory-is/here.gz' USING TextLoader AS (line:chararray);\ngrunt> dump a;\n...(\u30ed\u30b0\u306f\u51fa\u308b\u304c\u5fdc\u7b54\u306a\u3057\uff09\n```\n", "tags": ["EMR", "S3", "hive", "pig", "hadoop"]}