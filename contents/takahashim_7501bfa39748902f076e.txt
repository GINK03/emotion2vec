{"context": "\u5ca9\u6ce2\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9 Vol.2 \u6240\u53ce\u306e\u5ca1\ufa11\u76f4\u89b3\u300c\u5358\u8a9e\u306e\u610f\u5473\u3092\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u6559\u3048\u308b\u300d\u3092\u8aad\u3080\u306b\u3042\u305f\u3063\u3066\u53c2\u8003\u306b\u306a\u308a\u305d\u3046\u306a\u60c5\u5831\u3092\u52dd\u624b\u306b\u307e\u3068\u3081\u307e\u3057\u305f\u3002\n\n\u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8\u30fb\u767a\u520a\u8a18\u5ff5\u30a4\u30d9\u30f3\u30c8\n\u300c\u5ca9\u6ce2\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u300d\u30b7\u30ea\u30fc\u30ba\u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8: https://sites.google.com/site/iwanamidatascience/\n\u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8\u5185\u300c\u5206\u6563\u8868\u73fe(\u5358\u8a9e\u57cb\u3081\u8fbc\u307f)\u300d\u3000https://sites.google.com/site/iwanamidatascience/vol2/word-embedding\n\u30a4\u30d9\u30f3\u30c8\u30da\u30fc\u30b8: http://connpass.com/event/27135/\n\u52d5\u753b: https://www.youtube.com/watch?v=EyL_TC17MkQ 19:20\u304f\u3089\u3044\u304b\u3089\ntogetter: http://togetter.com/li/950232\n\n\u53c2\u8003\u6587\u732e\u30fb\u307b\u307c\u5e74\u4ee3\u9806\n\u53c2\u8003\u6587\u732e\u3068\u3057\u3066\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587\u3068\u305d\u306e\u53c2\u8003\u60c5\u5831\u3092\u5e74\u4ee3\u9806\u306b\u4e26\u3079\u3066\u307f\u307e\u3059\u3002\n\nDistributional Structure\u3000(Z. Harris, 1954)\n\u4e00\u756a\u53e4\u3044\u3082\u306e\u3002\u5206\u6563\u4eee\u8aac\u306e\u5143\u30cd\u30bf\u3002\n\n\u8ad6\u6587PDF(\u90e8\u5206\uff1f)\n\nhttp://www.tandfonline.com/doi/abs/10.1080/00437956.1954.11659520\nhttp://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\n\n\n\u53c2\u8003\nDistributional semantics https://en.wikipedia.org/wiki/Distributional_semantics\n\u5206\u6563\u8868\u73fe\u306e\u89e3\u8aac\u8a18\u4e8b http://www.slideshare.net/unnonouno/20140206-statistical-semantics\n\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u300cDistributional Semantic Models\u300d\u8cc7\u6599 http://wordspace.collocations.de/lib/exe/fetch.php/course:acl2010:naacl2010_part1.slides.pdf\n\nDistributed Representations of Words and Phrases and their Compositionality (T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean, 2013)\nMikolov\u306eword2vec\u8ad6\u6587\uff08\u306e\u4e00\u3064\uff09\u3002Skip-gram\u306e\u9ad8\u901f\u5316\u3068\u8b0e\u306e\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u6027\u306e\u7d39\u4ecb\u3002\u73fe\u4ee3\u306e\u5206\u6563\u8868\u73fe\u306e\u5686\u77e2\u3001\u3089\u3057\u3044\u3002\n\n\u8ad6\u6587PDF\n\nhttps://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\nhttp://arxiv.org/pdf/1310.4546.pdf\n\n\nAbstract\n\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\n\n\u52d5\u753b\n\n\u6587\u732e\u7d39\u4ecb https://www.youtube.com/watch?v=45VXFmR_its\n\n\n\n\u89e3\u8aac\n\nhttp://www.slideshare.net/unnonouno/nips2013-distributed-representations-of-words-and-phrases-and-their-compositionality\nhttp://hytae.hatenablog.com/entry/2015/05/15/Mikolov%E3%81%AEword2vec%E8%AB%96%E6%96%873%E6%9C%AC%E3%81%BE%E3%81%A8%E3%82%81\nhttp://qiita.com/nishio/items/3860fe198d65d173af6b\n\n\nGloVe: Global Vectors for Word Representation (J. Pennington, R. Socher, C. Manning, 2014)\nword2vec\u306b\u4f3c\u305f\u4f55\u304b(\u5b9f\u88c5\u3067\u306f\u306a\u304f\u624b\u6cd5\u304c\u9055\u3046\u3093\u3060\u3051\u3069\u4f55\u304c\u9055\u3046\u306e\u304b\u306f\u7406\u89e3\u3067\u304d\u3066\u307e\u305b\u3093\u2026)\u3002\n\n\u8ad6\u6587PDF\n\nhttp://www-nlp.stanford.edu/pubs/glove.pdf\n\n\nAbstract\n\nRecent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.\n\n\n\u5b9f\u88c5\nhttps://github.com/stanfordnlp/GloVe\n\n\u95a2\u9023\u30b5\u30a4\u30c8\nhttp://nlp.stanford.edu/projects/glove/\n\n\u52d5\u753b\n\n\u6587\u732e\u7d39\u4ecb https://www.youtube.com/watch?v=RS2DlT6edvc\n\n\n\n\u89e3\u8aac\n\nhttp://nzw.hatenablog.jp/entry/2015/06/07/223658\nhttp://nonbiri-tereka.hatenablog.com/entry/2015/10/25/223430\n\n\nNeural Word Embedding as Implicit Matrix Factorization (O. Levy and Y. Goldberg, 2014)\nskip-gram\u306e\u30cd\u30ac\u30c6\u30a3\u30d6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068PMI\u306e\u884c\u5217\u5206\u89e3\u3068\u304c\uff08\u3042\u308b\u6761\u4ef6\u4e0b\u3067\u306f\uff1f\uff09\u540c\u3058\u306b\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u8ad6\u6587\u3002\n\n\u8ad6\u6587PDF\n\nhttp://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/LevyGoldberg2014.pdf\n\n\nAbstract\n\nWe analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context.\nWe show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks.\nWhen dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS\u2019s solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS\u2019s factorization.\n\n\n\u53c2\u8003\u6587\u732e\n\nhttp://hytae.hatenablog.com/entry/2015/05/15/%E9%9B%91%E8%AA%AD%E8%AB%96%E6%96%87%E3%81%BE%E3%81%A8%E3%82%81\nhttp://www.slideshare.net/nttdata-msi/skip-gram-shirakawa20141121-41833306\nhttp://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/word2vec_pmi/\n\n\nA Linear Dynamical System Model for Text (D. Belanger and S. Kakade, 2015)\n\n\u8ad6\u6587PDF\n\nhttp://www.jmlr.org/proceedings/papers/v37/belanger15.pdf\n\n\nAbstract\n\nLow dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words\u2019 local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous\nstates, where words\u2019 representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.\n\n\n\u5b9f\u88c5\n\nhttps://github.com/davidBelanger/TextLDS\n\n\n\u89e3\u8aac\u8a18\u4e8b\n\n\u8aad\u89e3\u652f\u63f4@2015 07-03\u3000http://www.slideshare.net/sekizawayuuki/2015-0703\n\n\n\nImproving Distributional Similarity with Lessons Learned from Word Embeddings(O. Levy, Y. Goldberg and I. Dagan, 2015)\n\n\u8ad6\u6587PDF\n\nhttp://www.aclweb.org/anthology/Q15-1016\n\n\nAbstract\n\nRecent trends suggest that neuralnetwork-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.\n\n\n\u5b9f\u88c5\n\nhttps://bitbucket.org/omerlevy/hyperwords\n\n\n\u767a\u8868\u52d5\u753b\n\nhttp://techtalks.tv/talks/improving-distributional-similarity-with-lessons-learned-from-word-embeddings/61709/\n\n\n\u53c2\u8003\n\nhttp://nzw.hatenablog.jp/entry/2015/09/24/022512\n\n\nA Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution (S. Li, J. Zhu and C. Miao, 2015)\n\n\u8ad6\u6587PDF\n\nhttp://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP183.pdf\nhttp://arxiv.org/pdf/1508.03826v1.pdf\n\n\nAbctract\n\nMost existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.\n\n\n\u5b9f\u88c5\n\nhttps://github.com/askerlee/topicvec\n\n\nModel-based Word Embeddings from Decompositions of Count Matrices (K. Stratos, M. Collins, D. Hsu, 2015)\n\n\u8ad6\u6587PDF\n\nhttp://www.cs.columbia.edu/~djhsu/papers/count_words.pdf\nhttp://aclweb.org/anthology/P/P15/P15-1124.pdf\n\n\nAbstract\n\nThis work develops a new statistical understanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a generative model, we demonstrate how canonical correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE.\n\n\n\u30b9\u30e9\u30a4\u30c9\n\nhttp://www.cs.columbia.edu/~stratos/research/acl15cca_slides.pdf\n\n\n\u5b9f\u88c5\n\nSingular (C++) https://github.com/karlstratos/singular\n\n\n\n\u52d5\u753b\n\nhttp://techtalks.tv/talks/model-based-word-embeddings-from-decompositions-of-count-matrices/61791/\n\n\u5ca9\u6ce2\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9 Vol.2 \u6240\u53ce\u306e\u5ca1\ufa11\u76f4\u89b3\u300c\u5358\u8a9e\u306e\u610f\u5473\u3092\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u306b\u6559\u3048\u308b\u300d\u3092\u8aad\u3080\u306b\u3042\u305f\u3063\u3066\u53c2\u8003\u306b\u306a\u308a\u305d\u3046\u306a\u60c5\u5831\u3092\u52dd\u624b\u306b\u307e\u3068\u3081\u307e\u3057\u305f\u3002\n\n# \u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8\u30fb\u767a\u520a\u8a18\u5ff5\u30a4\u30d9\u30f3\u30c8\n\n\u300c\u5ca9\u6ce2\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u300d\u30b7\u30ea\u30fc\u30ba\u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8: https://sites.google.com/site/iwanamidatascience/\n\u30b5\u30dd\u30fc\u30c8\u30da\u30fc\u30b8\u5185\u300c\u5206\u6563\u8868\u73fe(\u5358\u8a9e\u57cb\u3081\u8fbc\u307f)\u300d\u3000https://sites.google.com/site/iwanamidatascience/vol2/word-embedding\n\u30a4\u30d9\u30f3\u30c8\u30da\u30fc\u30b8: http://connpass.com/event/27135/\n\u52d5\u753b: https://www.youtube.com/watch?v=EyL_TC17MkQ 19:20\u304f\u3089\u3044\u304b\u3089\ntogetter: http://togetter.com/li/950232\n\n# \u53c2\u8003\u6587\u732e\u30fb\u307b\u307c\u5e74\u4ee3\u9806\n\n\u53c2\u8003\u6587\u732e\u3068\u3057\u3066\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587\u3068\u305d\u306e\u53c2\u8003\u60c5\u5831\u3092\u5e74\u4ee3\u9806\u306b\u4e26\u3079\u3066\u307f\u307e\u3059\u3002\n\n## Distributional Structure\u3000(Z. Harris, 1954)\n\n\u4e00\u756a\u53e4\u3044\u3082\u306e\u3002\u5206\u6563\u4eee\u8aac\u306e\u5143\u30cd\u30bf\u3002\n\n### \u8ad6\u6587PDF(\u90e8\u5206\uff1f)\n* http://www.tandfonline.com/doi/abs/10.1080/00437956.1954.11659520\n* http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520\n\n### \u53c2\u8003\n\nDistributional semantics https://en.wikipedia.org/wiki/Distributional_semantics\n\u5206\u6563\u8868\u73fe\u306e\u89e3\u8aac\u8a18\u4e8b http://www.slideshare.net/unnonouno/20140206-statistical-semantics\n\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u300cDistributional Semantic Models\u300d\u8cc7\u6599 http://wordspace.collocations.de/lib/exe/fetch.php/course:acl2010:naacl2010_part1.slides.pdf\n\n## Distributed Representations of Words and Phrases and their Compositionality (T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean, 2013)\n\nMikolov\u306eword2vec\u8ad6\u6587\uff08\u306e\u4e00\u3064\uff09\u3002Skip-gram\u306e\u9ad8\u901f\u5316\u3068\u8b0e\u306e\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\u6027\u306e\u7d39\u4ecb\u3002\u73fe\u4ee3\u306e\u5206\u6563\u8868\u73fe\u306e\u5686\u77e2\u3001\u3089\u3057\u3044\u3002\n\n### \u8ad6\u6587PDF\n* https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n* http://arxiv.org/pdf/1310.4546.pdf\n\n### Abstract\n\n> The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\n### \u52d5\u753b\n* \u6587\u732e\u7d39\u4ecb https://www.youtube.com/watch?v=45VXFmR_its\n\n### \u89e3\u8aac\n\n* http://www.slideshare.net/unnonouno/nips2013-distributed-representations-of-words-and-phrases-and-their-compositionality\n* http://hytae.hatenablog.com/entry/2015/05/15/Mikolov%E3%81%AEword2vec%E8%AB%96%E6%96%873%E6%9C%AC%E3%81%BE%E3%81%A8%E3%82%81\n* http://qiita.com/nishio/items/3860fe198d65d173af6b\n\n## GloVe: Global Vectors for Word Representation (J. Pennington, R. Socher, C. Manning, 2014)\n\nword2vec\u306b\u4f3c\u305f\u4f55\u304b(\u5b9f\u88c5\u3067\u306f\u306a\u304f\u624b\u6cd5\u304c\u9055\u3046\u3093\u3060\u3051\u3069\u4f55\u304c\u9055\u3046\u306e\u304b\u306f\u7406\u89e3\u3067\u304d\u3066\u307e\u305b\u3093\u2026)\u3002\n\n### \u8ad6\u6587PDF\n\n* http://www-nlp.stanford.edu/pubs/glove.pdf\n\n### Abstract\n\n> Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.\n\n### \u5b9f\u88c5\n\nhttps://github.com/stanfordnlp/GloVe\n\n### \u95a2\u9023\u30b5\u30a4\u30c8\n\nhttp://nlp.stanford.edu/projects/glove/\n\n### \u52d5\u753b\n* \u6587\u732e\u7d39\u4ecb https://www.youtube.com/watch?v=RS2DlT6edvc\n\n### \u89e3\u8aac\n* http://nzw.hatenablog.jp/entry/2015/06/07/223658\n* http://nonbiri-tereka.hatenablog.com/entry/2015/10/25/223430\n\n## Neural Word Embedding as Implicit Matrix Factorization (O. Levy and Y. Goldberg, 2014)\n\nskip-gram\u306e\u30cd\u30ac\u30c6\u30a3\u30d6\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068PMI\u306e\u884c\u5217\u5206\u89e3\u3068\u304c\uff08\u3042\u308b\u6761\u4ef6\u4e0b\u3067\u306f\uff1f\uff09\u540c\u3058\u306b\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u305f\u8ad6\u6587\u3002\n\n### \u8ad6\u6587PDF\n* http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/LevyGoldberg2014.pdf\n\n### Abstract\n\n> We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context.\nWe show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks.\n> When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS\u2019s solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS\u2019s factorization.\n\n### \u53c2\u8003\u6587\u732e\n\n* http://hytae.hatenablog.com/entry/2015/05/15/%E9%9B%91%E8%AA%AD%E8%AB%96%E6%96%87%E3%81%BE%E3%81%A8%E3%82%81\n* http://www.slideshare.net/nttdata-msi/skip-gram-shirakawa20141121-41833306\n* http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/word2vec_pmi/\n\n\n## A Linear Dynamical System Model for Text (D. Belanger and S. Kakade, 2015)\n\n### \u8ad6\u6587PDF\n\n* http://www.jmlr.org/proceedings/papers/v37/belanger15.pdf\n\n### Abstract\n\n> Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words\u2019 local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous\nstates, where words\u2019 representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.\n\n### \u5b9f\u88c5\n\n* https://github.com/davidBelanger/TextLDS\n\n### \u89e3\u8aac\u8a18\u4e8b\n\n* \u8aad\u89e3\u652f\u63f4@2015 07-03\u3000http://www.slideshare.net/sekizawayuuki/2015-0703\n\n\n\n## Improving Distributional Similarity with Lessons Learned from Word Embeddings(O. Levy, Y. Goldberg and I. Dagan, 2015)\n\n### \u8ad6\u6587PDF\n\n* http://www.aclweb.org/anthology/Q15-1016\n\n### Abstract\n\n> Recent trends suggest that neuralnetwork-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.\n\n### \u5b9f\u88c5\n\n* https://bitbucket.org/omerlevy/hyperwords\n\n### \u767a\u8868\u52d5\u753b\n\n* http://techtalks.tv/talks/improving-distributional-similarity-with-lessons-learned-from-word-embeddings/61709/\n\n### \u53c2\u8003\n\n* http://nzw.hatenablog.jp/entry/2015/09/24/022512\n\n## A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution (S. Li, J. Zhu and C. Miao, 2015)\n\n### \u8ad6\u6587PDF\n* http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP183.pdf\n* http://arxiv.org/pdf/1508.03826v1.pdf\n\n### Abctract\n\n> Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.\n\n### \u5b9f\u88c5\n* https://github.com/askerlee/topicvec\n\n\n\n\n## Model-based Word Embeddings from Decompositions of Count Matrices (K. Stratos, M. Collins, D. Hsu, 2015)\n\n### \u8ad6\u6587PDF\n\n* http://www.cs.columbia.edu/~djhsu/papers/count_words.pdf\n* http://aclweb.org/anthology/P/P15/P15-1124.pdf\n\n### Abstract\n\n> This work develops a new statistical understanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a generative model, we demonstrate how canonical correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE.\n\n### \u30b9\u30e9\u30a4\u30c9\n\n* http://www.cs.columbia.edu/~stratos/research/acl15cca_slides.pdf\n\n### \u5b9f\u88c5\n\n* Singular (C++) https://github.com/karlstratos/singular\n\n\n### \u52d5\u753b\n* http://techtalks.tv/talks/model-based-word-embeddings-from-decompositions-of-count-matrices/61791/\n", "tags": ["NLP"]}