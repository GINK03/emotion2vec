{"context": " More than 1 year has passed since last update.\n\nspark-sql\u3092\u5b9f\u884c\n\n\ngce\u3067\u3068\u66f8\u3044\u3066\u304a\u3044\u3066\u6050\u7e2e\u3067\u3059\u304c\u3001gce\u3067\u3042\u308b\u5fc5\u8981\u304c\u5168\u7136\u3042\u308a\u307e\u305b\u3093\u3002\n\u3067\u3082\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3082\u63a5\u7d9a\u5148\u3082ubuntu\u3067\u3059\u3002\nspark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u524d\u306e\u6295\u7a3f\u3067\u884c\u3063\u3066\u3044\u308b\u306e\u3067\u7701\u7565\u3057\u307e\u3059\u3002\n\n\nspark-sql\u8d77\u52d5\n\uff11\uff0egcloud\u3067\u63a5\u7d9a\u3057\u307e\u3059\u3002\n$ cd $SPARK_HOME\n$ sudo ./bin/spark-spl\n\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUnable to initialize logging using hive-log4j.properties, not found on CLASSPATH!\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n15/07/07 15:32:08 INFO SecurityManager: Changing view acls to: root,\n15/07/07 15:32:08 INFO SecurityManager: Changing modify acls to: root,\n15/07/07 15:32:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, ); users with modify permissions: Set(root, )\n15/07/07 15:32:08 INFO Slf4jLogger: Slf4jLogger started\n15/07/07 15:32:08 INFO Remoting: Starting remoting\n15/07/07 15:32:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022]\n15/07/07 15:32:08 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022]\n15/07/07 15:32:08 INFO Utils: Successfully started service 'sparkDriver' on port 58022.\n15/07/07 15:32:08 INFO SparkEnv: Registering MapOutputTracker\n15/07/07 15:32:08 INFO SparkEnv: Registering BlockManagerMaster\n15/07/07 15:32:08 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20150707153208-86b9\n15/07/07 15:32:08 INFO Utils: Successfully started service 'Connection manager for block manager' on port 39311.\n15/07/07 15:32:08 INFO ConnectionManager: Bound socket to port 39311 with id = ConnectionManagerId(instance-1.c.custom-unison-00000.internal,00000)\n15/07/07 15:32:08 INFO MemoryStore: MemoryStore started with capacity 265.1 MB\n15/07/07 15:32:08 INFO BlockManagerMaster: Trying to register BlockManager\n15/07/07 15:32:08 INFO BlockManagerMasterActor: Registering block manager instance-1.c.custom-unison-00000.internal:39311 with 265.1 MB RAM\n15/07/07 15:32:08 INFO BlockManagerMaster: Registered BlockManager\n15/07/07 15:32:08 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a8e3eb25-7a87-4138-8ec5-f387b76c21b1\n15/07/07 15:32:08 INFO HttpServer: Starting HTTP Server\n15/07/07 15:32:09 INFO Utils: Successfully started service 'HTTP file server' on port 45607.\n15/07/07 15:32:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n15/07/07 15:32:09 INFO SparkUI: Started SparkUI at http://instance-1.c.custom-unison-00000.internal:4040\n15/07/07 15:32:09 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022/user/HeartbeatReceiver\nspark-sql> \n\nsparl-spl\u304c\u8d77\u52d5\u3057\u307e\u3059\u3002\n\uff12\uff0e\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u308b\u3002\nSpark SQL programming guide\u306eexample\u3092\u305d\u306e\u307e\u307e\u4f7f\u3063\u3066\u3001\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3002\nspark-sql> CREATE TABLE IF NOT EXISTS src (key INT, value STRING);\n\n15/07/07 15:38:48 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO ParseDriver: Parse Completed\n15/07/07 15:38:48 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:38:48 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO ParseDriver: Parse Completed\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=parse start=1436283528297 end=1436283528297 duration=0>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:38:48 INFO SemanticAnalyzer: Starting Semantic Analysis\n15/07/07 15:38:48 INFO SemanticAnalyzer: Creating table src position=27\n15/07/07 15:38:48 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:38:48 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:38:48 INFO Driver: Semantic Analysis Completed\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283528298 end=1436283528347 duration=49>\n15/07/07 15:38:48 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=compile start=1436283528296 end=1436283528348 duration=52>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:38:48 INFO Driver: Starting command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283528296 end=1436283528349 duration=53>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=task.DDL.Stage-0>\n15/07/07 15:38:48 INFO DDLTask: Default to LazySimpleSerDe for table src\n15/07/07 15:38:48 INFO HiveMetaStore: 0: create_table: Table(tableName:src, dbName:default, owner:root, createTime:1436283528, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))\n15/07/07 15:38:48 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=create_table: Table(tableName:src, dbName:default, owner:root, createTime:1436283528, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))   \n15/07/07 15:38:48 INFO Driver: </PERFLOG method=task.DDL.Stage-0 start=1436283528349 end=1436283528460 duration=111>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=runTasks start=1436283528349 end=1436283528460 duration=111>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=Driver.execute start=1436283528348 end=1436283528460 duration=112>\nOK\n15/07/07 15:38:48 INFO Driver: OK\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528460 end=1436283528461 duration=1>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=Driver.run start=1436283528296 end=1436283528461 duration=165>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528461 end=1436283528461 duration=0>\nTime taken: 0.207 seconds\n15/07/07 15:38:48 INFO CliDriver: Time taken: 0.207 seconds\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528465 end=1436283528465 duration=0>\n\n\u30c6\u30fc\u30d6\u30eb\u304c\u3067\u304d\u305f\u304b\u78ba\u8a8d\u3057\u3066\u307f\u308b\u3002\nspark-sql> show tables;\n\n15/07/07 15:40:32 INFO ParseDriver: Parsing command: show tables\n15/07/07 15:40:32 INFO ParseDriver: Parse Completed\n15/07/07 15:40:32 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:40:32 INFO ParseDriver: Parsing command: show tables\n15/07/07 15:40:32 INFO ParseDriver: Parse Completed\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=parse start=1436283632094 end=1436283632095 duration=1>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:40:32 INFO Driver: Semantic Analysis Completed\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283632095 end=1436283632103 duration=8>\n15/07/07 15:40:32 INFO ListSinkOperator: Initializing Self 0 OP\n15/07/07 15:40:32 INFO ListSinkOperator: Operator 0 OP initialized\n15/07/07 15:40:32 INFO ListSinkOperator: Initialization Done 0 OP\n15/07/07 15:40:32 INFO Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=compile start=1436283632094 end=1436283632104 duration=10>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:40:32 INFO Driver: Starting command: show tables\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283632093 end=1436283632105 duration=12>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=task.DDL.Stage-0>\n15/07/07 15:40:32 INFO HiveMetaStore: 0: get_database: default\n15/07/07 15:40:32 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_database: default   \n15/07/07 15:40:32 INFO HiveMetaStore: 0: get_tables: db=default pat=.*\n15/07/07 15:40:32 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_tables: db=default pat=.*   \n15/07/07 15:40:32 INFO Driver: </PERFLOG method=task.DDL.Stage-0 start=1436283632105 end=1436283632119 duration=14>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=runTasks start=1436283632105 end=1436283632120 duration=15>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=Driver.execute start=1436283632104 end=1436283632120 duration=16>\nOK\n15/07/07 15:40:32 INFO Driver: OK\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632120 end=1436283632120 duration=0>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=Driver.run start=1436283632093 end=1436283632120 duration=27>\n15/07/07 15:40:32 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n15/07/07 15:40:32 INFO FileInputFormat: Total input paths to process : 1\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632161 end=1436283632161 duration=0>\nsrc\nTime taken: 0.106 seconds\n15/07/07 15:40:32 INFO CliDriver: Time taken: 0.106 seconds\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632169 end=1436283632170 duration=1>\n\n\u78ba\u304b\u306b[src]\u304c\u3067\u304d\u3066\u307e\u3059\u3002\n\u3067\u306f\u3001\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3057\u3066\u307f\u307e\u3059\u3002\nspark-sql> LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src;\n\n15/07/07 15:42:17 INFO ParseDriver: Parsing command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO ParseDriver: Parse Completed\n15/07/07 15:42:17 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:42:17 INFO ParseDriver: Parsing command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO ParseDriver: Parse Completed\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=parse start=1436283737848 end=1436283737849 duration=1>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:42:17 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:17 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:17 INFO Driver: Semantic Analysis Completed\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283737849 end=1436283737939 duration=90>\n15/07/07 15:42:17 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=compile start=1436283737848 end=1436283737944 duration=96>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:42:17 INFO Driver: Starting command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283737848 end=1436283737944 duration=96>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=task.COPY.Stage-0>\nCopying data from file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Task: Copying data from file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt to file:/tmp/hive-root/hive_2015-07-07_15-42-17_848_6713777572196549102-1/-ext-10000\nCopying file: file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Task: Copying file: file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=task.COPY.Stage-0 start=1436283737944 end=1436283737963 duration=19>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=task.MOVE.Stage-1>\nLoading data to table default.src\n15/07/07 15:42:17 INFO Task: Loading data to table default.src from file:/tmp/hive-root/hive_2015-07-07_15-42-17_848_6713777572196549102-1/-ext-10000\n15/07/07 15:42:17 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:17 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: alter_table: db=default tbl=src newtbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=alter_table: db=default tbl=src newtbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:18 INFO Driver: </PERFLOG method=task.MOVE.Stage-1 start=1436283737963 end=1436283738097 duration=134>\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=task.STATS.Stage-2>\n15/07/07 15:42:18 INFO StatsTask: Executing stats task\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: alter_table: db=default tbl=src newtbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=alter_table: db=default tbl=src newtbl=src  \n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \nTable default.src stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5812, raw_data_size: 0]\n15/07/07 15:42:18 INFO Task: Table default.src stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5812, raw_data_size: 0]\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=task.STATS.Stage-2 start=1436283738097 end=1436283738204 duration=107>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=runTasks start=1436283737944 end=1436283738204 duration=260>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=Driver.execute start=1436283737944 end=1436283738204 duration=260>\nOK\n15/07/07 15:42:18 INFO Driver: OK\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738204 end=1436283738204 duration=0>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=Driver.run start=1436283737848 end=1436283738205 duration=357>\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738205 end=1436283738205 duration=0>\nTime taken: 0.394 seconds\n15/07/07 15:42:18 INFO CliDriver: Time taken: 0.394 seconds\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738208 end=1436283738208 duration=0>\n\nOK\u3068\u51fa\u3066\u307e\u3059\u3002\n\u3067\u306f\u3001\u30c6\u30fc\u30d6\u30eb\u306e\u4e2d\u306b\u30c7\u30fc\u30bf\u304c\u5165\u3063\u305f\u304b\u78ba\u8a8d\u3002\nspark-sql> select count(*) FROM src;\n\n15/07/07 15:43:50 INFO ParseDriver: Parsing command: select count(*) FROM src\n15/07/07 15:43:50 INFO ParseDriver: Parse Completed\n15/07/07 15:43:50 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:43:50 INFO audit: ugi=root  ip=unknown-ip-addr  cmd=get_table : db=default tbl=src  \n15/07/07 15:43:50 INFO deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n15/07/07 15:43:50 INFO MemoryStore: ensureFreeSpace(454358) called with curMem=0, maxMem=278019440\n15/07/07 15:43:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 443.7 KB, free 264.7 MB)\n15/07/07 15:43:50 INFO SparkContext: Starting job: collect at HiveContext.scala:415\n15/07/07 15:43:50 INFO FileInputFormat: Total input paths to process : 1\n15/07/07 15:43:50 INFO DAGScheduler: Registering RDD 18 (mapPartitions at Exchange.scala:86)\n15/07/07 15:43:50 INFO DAGScheduler: Got job 0 (collect at HiveContext.scala:415) with 1 output partitions (allowLocal=false)\n15/07/07 15:43:50 INFO DAGScheduler: Final stage: Stage 0(collect at HiveContext.scala:415)\n15/07/07 15:43:50 INFO DAGScheduler: Parents of final stage: List(Stage 1)\n15/07/07 15:43:50 INFO DAGScheduler: Missing parents: List(Stage 1)\n15/07/07 15:43:50 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[18] at mapPartitions at Exchange.scala:86), which has no missing parents\n15/07/07 15:43:50 INFO MemoryStore: ensureFreeSpace(11024) called with curMem=454358, maxMem=278019440\n15/07/07 15:43:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KB, free 264.7 MB)\n15/07/07 15:43:50 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[18] at mapPartitions at Exchange.scala:86)\n15/07/07 15:43:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, PROCESS_LOCAL, 1182 bytes)\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1182 bytes)\n15/07/07 15:43:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)\n15/07/07 15:43:51 INFO Executor: Running task 1.0 in stage 1.0 (TID 1)\n15/07/07 15:43:51 INFO HadoopRDD: Input split: file:/user/hive/warehouse/src/kv1.txt:2906+2906\n15/07/07 15:43:51 INFO HadoopRDD: Input split: file:/user/hive/warehouse/src/kv1.txt:0+2906\n15/07/07 15:43:51 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n15/07/07 15:43:51 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n15/07/07 15:43:51 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n15/07/07 15:43:51 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n15/07/07 15:43:51 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n15/07/07 15:43:51 INFO Executor: Finished task 1.0 in stage 1.0 (TID 1). 1895 bytes result sent to driver\n15/07/07 15:43:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 1895 bytes result sent to driver\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 401 ms on localhost (1/2)\n15/07/07 15:43:51 INFO DAGScheduler: Stage 1 (mapPartitions at Exchange.scala:86) finished in 0.430 s\n15/07/07 15:43:51 INFO DAGScheduler: looking for newly runnable stages\n15/07/07 15:43:51 INFO DAGScheduler: running: Set()\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 1) in 413 ms on localhost (2/2)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n15/07/07 15:43:51 INFO DAGScheduler: waiting: Set(Stage 0)\n15/07/07 15:43:51 INFO DAGScheduler: failed: Set()\n15/07/07 15:43:51 INFO DAGScheduler: Missing parents for Stage 0: List()\n15/07/07 15:43:51 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[22] at map at HiveContext.scala:360), which is now runnable\n15/07/07 15:43:51 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1deaf84d\n15/07/07 15:43:51 INFO MemoryStore: ensureFreeSpace(9792) called with curMem=465382, maxMem=278019440\n15/07/07 15:43:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.6 KB, free 264.7 MB)\n15/07/07 15:43:51 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[22] at map at HiveContext.scala:360)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 948 bytes)\n15/07/07 15:43:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 2)\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 20 ms\n15/07/07 15:43:51 INFO StatsReportListener: task runtime:(count: 2, mean: 407.000000, stdev: 6.000000, max: 413.000000, min: 401.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     401.0 ms    401.0 ms    401.0 ms    401.0 ms    413.0 ms    413.0 ms    413.0 ms    413.0 ms    413.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: shuffle bytes written:(count: 2, mean: 50.000000, stdev: 0.000000, max: 50.000000, min: 50.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     50.0 B  50.0 B  50.0 B  50.0 B  50.0 B  50.0 B  50.0 B  50.0 B  50.0 B\n15/07/07 15:43:51 INFO StatsReportListener: task result size:(count: 2, mean: 1895.000000, stdev: 0.000000, max: 1895.000000, min: 1895.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     1895.0 B    1895.0 B    1895.0 B    1895.0 B    1895.0 B    1895.0 B    1895.0 B    1895.0 B    1895.0 B\n15/07/07 15:43:51 INFO StatsReportListener: executor (non-fetch) time pct: (count: 2, mean: 76.642534, stdev: 1.081437, max: 77.723971, min: 75.561097)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     76 %    76 %    76 %    76 %    78 %    78 %    78 %    78 %    78 %\n15/07/07 15:43:51 INFO StatsReportListener: other time pct: (count: 2, mean: 23.357466, stdev: 1.081437, max: 24.438903, min: 22.276029)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     22 %    22 %    22 %    22 %    24 %    24 %    24 %    24 %    24 %\n15/07/07 15:43:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 2). 1076 bytes result sent to driver\n15/07/07 15:43:51 INFO DAGScheduler: Stage 0 (collect at HiveContext.scala:415) finished in 0.114 s\n15/07/07 15:43:51 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5ad48e86\n15/07/07 15:43:51 INFO SparkContext: Job finished: collect at HiveContext.scala:415, took 0.778304976 s\n15/07/07 15:43:51 INFO StatsReportListener: task runtime:(count: 1, mean: 116.000000, stdev: 0.000000, max: 116.000000, min: 116.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     116.0 ms    116.0 ms    116.0 ms    116.0 ms    116.0 ms    116.0 ms    116.0 ms    116.0 ms    116.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     0.0 ms  0.0 ms  0.0 ms  0.0 ms  0.0 ms  0.0 ms  0.0 ms  0.0 ms  0.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     0.0 B   0.0 B   0.0 B   0.0 B   0.0 B   0.0 B   0.0 B   0.0 B   0.0 B\n500\nTime taken: 1.14 seconds\n15/07/07 15:43:51 INFO CliDriver: Time taken: 1.14 seconds\nspark-sql> 15/07/07 15:43:51 INFO StatsReportListener: task result size:(count: 1, mean: 1076.000000, stdev: 0.000000, max: 1076.000000, min: 1076.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     1076.0 B    1076.0 B    1076.0 B    1076.0 B    1076.0 B    1076.0 B    1076.0 B    1076.0 B    1076.0 B\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 2) in 116 ms on localhost (1/1)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n15/07/07 15:43:51 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.827586, stdev: 0.000000, max: 94.827586, min: 94.827586)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:     95 %    95 %    95 %    95 %    95 %    95 %    95 %    95 %    95 %\n15/07/07 15:43:51 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:      0 %     0 %     0 %     0 %     0 %     0 %     0 %     0 %     0 %\n15/07/07 15:43:51 INFO StatsReportListener: other time pct: (count: 1, mean: 5.172414, stdev: 0.000000, max: 5.172414, min: 5.172414)\n15/07/07 15:43:51 INFO StatsReportListener:     0%  5%  10% 25% 50% 75% 90% 95% 100%\n15/07/07 15:43:51 INFO StatsReportListener:      5 %     5 %     5 %     5 %     5 %     5 %     5 %     5 %     5 %\n\n\u30ed\u30b0\u304c\u591a\u304f\u3066\u308f\u304b\u308a\u306b\u304f\u3044\u3067\u3059\u304c\u3001\n\n500\nTime taken: 1.14 seconds\n\n\u3068\u306a\u3063\u3066\u3044\u3066500\u30ec\u30b3\u30fc\u30c9\u5165\u3063\u3066\u3044\u308b\u306e\u304c\u53d6\u5f97\u3067\u304d\u307e\u3057\u305f\u3002\nhive\u3068\u540c\u3058\u3088\u3046\u306b\u4f7f\u3048\u3066\u3081\u3063\u3061\u3083\u65e9\u3044\u306a\u3041\u3002\u3002\u3002\n# **spark-sql\u3092\u5b9f\u884c**\n+ gce\u3067\u3068\u66f8\u3044\u3066\u304a\u3044\u3066\u6050\u7e2e\u3067\u3059\u304c\u3001gce\u3067\u3042\u308b\u5fc5\u8981\u304c\u5168\u7136\u3042\u308a\u307e\u305b\u3093\u3002\n+ \u3067\u3082\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3082\u63a5\u7d9a\u5148\u3082ubuntu\u3067\u3059\u3002\n+ spark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u524d\u306e\u6295\u7a3f\u3067\u884c\u3063\u3066\u3044\u308b\u306e\u3067\u7701\u7565\u3057\u307e\u3059\u3002\n\n## spark-sql\u8d77\u52d5\n\uff11\uff0egcloud\u3067\u63a5\u7d9a\u3057\u307e\u3059\u3002\n\n```\n$ cd $SPARK_HOME\n$ sudo ./bin/spark-spl\n```\n\n```\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUnable to initialize logging using hive-log4j.properties, not found on CLASSPATH!\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n15/07/07 15:32:08 INFO SecurityManager: Changing view acls to: root,\n15/07/07 15:32:08 INFO SecurityManager: Changing modify acls to: root,\n15/07/07 15:32:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, ); users with modify permissions: Set(root, )\n15/07/07 15:32:08 INFO Slf4jLogger: Slf4jLogger started\n15/07/07 15:32:08 INFO Remoting: Starting remoting\n15/07/07 15:32:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022]\n15/07/07 15:32:08 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022]\n15/07/07 15:32:08 INFO Utils: Successfully started service 'sparkDriver' on port 58022.\n15/07/07 15:32:08 INFO SparkEnv: Registering MapOutputTracker\n15/07/07 15:32:08 INFO SparkEnv: Registering BlockManagerMaster\n15/07/07 15:32:08 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20150707153208-86b9\n15/07/07 15:32:08 INFO Utils: Successfully started service 'Connection manager for block manager' on port 39311.\n15/07/07 15:32:08 INFO ConnectionManager: Bound socket to port 39311 with id = ConnectionManagerId(instance-1.c.custom-unison-00000.internal,00000)\n15/07/07 15:32:08 INFO MemoryStore: MemoryStore started with capacity 265.1 MB\n15/07/07 15:32:08 INFO BlockManagerMaster: Trying to register BlockManager\n15/07/07 15:32:08 INFO BlockManagerMasterActor: Registering block manager instance-1.c.custom-unison-00000.internal:39311 with 265.1 MB RAM\n15/07/07 15:32:08 INFO BlockManagerMaster: Registered BlockManager\n15/07/07 15:32:08 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a8e3eb25-7a87-4138-8ec5-f387b76c21b1\n15/07/07 15:32:08 INFO HttpServer: Starting HTTP Server\n15/07/07 15:32:09 INFO Utils: Successfully started service 'HTTP file server' on port 45607.\n15/07/07 15:32:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n15/07/07 15:32:09 INFO SparkUI: Started SparkUI at http://instance-1.c.custom-unison-00000.internal:4040\n15/07/07 15:32:09 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@instance-1.c.custom-unison-00000.internal:58022/user/HeartbeatReceiver\nspark-sql> \n```\n\nsparl-spl\u304c\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n\uff12\uff0e\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u308b\u3002\nSpark SQL programming guide\u306eexample\u3092\u305d\u306e\u307e\u307e\u4f7f\u3063\u3066\u3001\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3002\n\n```\nspark-sql> CREATE TABLE IF NOT EXISTS src (key INT, value STRING);\n```\n\n```\n15/07/07 15:38:48 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO ParseDriver: Parse Completed\n15/07/07 15:38:48 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:38:48 INFO ParseDriver: Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO ParseDriver: Parse Completed\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=parse start=1436283528297 end=1436283528297 duration=0>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:38:48 INFO SemanticAnalyzer: Starting Semantic Analysis\n15/07/07 15:38:48 INFO SemanticAnalyzer: Creating table src position=27\n15/07/07 15:38:48 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:38:48 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:38:48 INFO Driver: Semantic Analysis Completed\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283528298 end=1436283528347 duration=49>\n15/07/07 15:38:48 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=compile start=1436283528296 end=1436283528348 duration=52>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:38:48 INFO Driver: Starting command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283528296 end=1436283528349 duration=53>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=task.DDL.Stage-0>\n15/07/07 15:38:48 INFO DDLTask: Default to LazySimpleSerDe for table src\n15/07/07 15:38:48 INFO HiveMetaStore: 0: create_table: Table(tableName:src, dbName:default, owner:root, createTime:1436283528, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))\n15/07/07 15:38:48 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=create_table: Table(tableName:src, dbName:default, owner:root, createTime:1436283528, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:int, comment:null), FieldSchema(name:value, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:null, groupPrivileges:null, rolePrivileges:null))\t\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=task.DDL.Stage-0 start=1436283528349 end=1436283528460 duration=111>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=runTasks start=1436283528349 end=1436283528460 duration=111>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=Driver.execute start=1436283528348 end=1436283528460 duration=112>\nOK\n15/07/07 15:38:48 INFO Driver: OK\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528460 end=1436283528461 duration=1>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=Driver.run start=1436283528296 end=1436283528461 duration=165>\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528461 end=1436283528461 duration=0>\nTime taken: 0.207 seconds\n15/07/07 15:38:48 INFO CliDriver: Time taken: 0.207 seconds\n15/07/07 15:38:48 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:38:48 INFO Driver: </PERFLOG method=releaseLocks start=1436283528465 end=1436283528465 duration=0>\n```\n\n\u30c6\u30fc\u30d6\u30eb\u304c\u3067\u304d\u305f\u304b\u78ba\u8a8d\u3057\u3066\u307f\u308b\u3002\n\n```\nspark-sql> show tables;\n```\n\n```\n15/07/07 15:40:32 INFO ParseDriver: Parsing command: show tables\n15/07/07 15:40:32 INFO ParseDriver: Parse Completed\n15/07/07 15:40:32 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:40:32 INFO ParseDriver: Parsing command: show tables\n15/07/07 15:40:32 INFO ParseDriver: Parse Completed\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=parse start=1436283632094 end=1436283632095 duration=1>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:40:32 INFO Driver: Semantic Analysis Completed\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283632095 end=1436283632103 duration=8>\n15/07/07 15:40:32 INFO ListSinkOperator: Initializing Self 0 OP\n15/07/07 15:40:32 INFO ListSinkOperator: Operator 0 OP initialized\n15/07/07 15:40:32 INFO ListSinkOperator: Initialization Done 0 OP\n15/07/07 15:40:32 INFO Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=compile start=1436283632094 end=1436283632104 duration=10>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:40:32 INFO Driver: Starting command: show tables\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283632093 end=1436283632105 duration=12>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=task.DDL.Stage-0>\n15/07/07 15:40:32 INFO HiveMetaStore: 0: get_database: default\n15/07/07 15:40:32 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n15/07/07 15:40:32 INFO HiveMetaStore: 0: get_tables: db=default pat=.*\n15/07/07 15:40:32 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_tables: db=default pat=.*\t\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=task.DDL.Stage-0 start=1436283632105 end=1436283632119 duration=14>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=runTasks start=1436283632105 end=1436283632120 duration=15>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=Driver.execute start=1436283632104 end=1436283632120 duration=16>\nOK\n15/07/07 15:40:32 INFO Driver: OK\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632120 end=1436283632120 duration=0>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=Driver.run start=1436283632093 end=1436283632120 duration=27>\n15/07/07 15:40:32 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n15/07/07 15:40:32 INFO FileInputFormat: Total input paths to process : 1\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632161 end=1436283632161 duration=0>\nsrc\nTime taken: 0.106 seconds\n15/07/07 15:40:32 INFO CliDriver: Time taken: 0.106 seconds\n15/07/07 15:40:32 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:40:32 INFO Driver: </PERFLOG method=releaseLocks start=1436283632169 end=1436283632170 duration=1>\n```\n\n\u78ba\u304b\u306b[src]\u304c\u3067\u304d\u3066\u307e\u3059\u3002\n\u3067\u306f\u3001\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u3057\u3066\u307f\u307e\u3059\u3002\n\n```\nspark-sql> LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src;\n```\n\n```\n15/07/07 15:42:17 INFO ParseDriver: Parsing command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO ParseDriver: Parse Completed\n15/07/07 15:42:17 INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=Driver.run>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=TimeToSubmit>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=compile>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=parse>\n15/07/07 15:42:17 INFO ParseDriver: Parsing command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO ParseDriver: Parse Completed\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=parse start=1436283737848 end=1436283737849 duration=1>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=semanticAnalyze>\n15/07/07 15:42:17 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:17 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:17 INFO Driver: Semantic Analysis Completed\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=semanticAnalyze start=1436283737849 end=1436283737939 duration=90>\n15/07/07 15:42:17 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=compile start=1436283737848 end=1436283737944 duration=96>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=Driver.execute>\n15/07/07 15:42:17 INFO Driver: Starting command: LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=TimeToSubmit start=1436283737848 end=1436283737944 duration=96>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=runTasks>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=task.COPY.Stage-0>\nCopying data from file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Task: Copying data from file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt to file:/tmp/hive-root/hive_2015-07-07_15-42-17_848_6713777572196549102-1/-ext-10000\nCopying file: file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Task: Copying file: file:/usr/local/spark-1.1.0-bin-hadoop2.4/examples/src/main/resources/kv1.txt\n15/07/07 15:42:17 INFO Driver: </PERFLOG method=task.COPY.Stage-0 start=1436283737944 end=1436283737963 duration=19>\n15/07/07 15:42:17 INFO Driver: <PERFLOG method=task.MOVE.Stage-1>\nLoading data to table default.src\n15/07/07 15:42:17 INFO Task: Loading data to table default.src from file:/tmp/hive-root/hive_2015-07-07_15-42-17_848_6713777572196549102-1/-ext-10000\n15/07/07 15:42:17 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:17 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: alter_table: db=default tbl=src newtbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=alter_table: db=default tbl=src newtbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=task.MOVE.Stage-1 start=1436283737963 end=1436283738097 duration=134>\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=task.STATS.Stage-2>\n15/07/07 15:42:18 INFO StatsTask: Executing stats task\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: alter_table: db=default tbl=src newtbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=alter_table: db=default tbl=src newtbl=src\t\n15/07/07 15:42:18 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:42:18 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\nTable default.src stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5812, raw_data_size: 0]\n15/07/07 15:42:18 INFO Task: Table default.src stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5812, raw_data_size: 0]\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=task.STATS.Stage-2 start=1436283738097 end=1436283738204 duration=107>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=runTasks start=1436283737944 end=1436283738204 duration=260>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=Driver.execute start=1436283737944 end=1436283738204 duration=260>\nOK\n15/07/07 15:42:18 INFO Driver: OK\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738204 end=1436283738204 duration=0>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=Driver.run start=1436283737848 end=1436283738205 duration=357>\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738205 end=1436283738205 duration=0>\nTime taken: 0.394 seconds\n15/07/07 15:42:18 INFO CliDriver: Time taken: 0.394 seconds\n15/07/07 15:42:18 INFO Driver: <PERFLOG method=releaseLocks>\n15/07/07 15:42:18 INFO Driver: </PERFLOG method=releaseLocks start=1436283738208 end=1436283738208 duration=0>\n```\n\nOK\u3068\u51fa\u3066\u307e\u3059\u3002\n\n\u3067\u306f\u3001\u30c6\u30fc\u30d6\u30eb\u306e\u4e2d\u306b\u30c7\u30fc\u30bf\u304c\u5165\u3063\u305f\u304b\u78ba\u8a8d\u3002\n\n```\nspark-sql> select count(*) FROM src;\n```\n\n```\n15/07/07 15:43:50 INFO ParseDriver: Parsing command: select count(*) FROM src\n15/07/07 15:43:50 INFO ParseDriver: Parse Completed\n15/07/07 15:43:50 INFO HiveMetaStore: 0: get_table : db=default tbl=src\n15/07/07 15:43:50 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=src\t\n15/07/07 15:43:50 INFO deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n15/07/07 15:43:50 INFO MemoryStore: ensureFreeSpace(454358) called with curMem=0, maxMem=278019440\n15/07/07 15:43:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 443.7 KB, free 264.7 MB)\n15/07/07 15:43:50 INFO SparkContext: Starting job: collect at HiveContext.scala:415\n15/07/07 15:43:50 INFO FileInputFormat: Total input paths to process : 1\n15/07/07 15:43:50 INFO DAGScheduler: Registering RDD 18 (mapPartitions at Exchange.scala:86)\n15/07/07 15:43:50 INFO DAGScheduler: Got job 0 (collect at HiveContext.scala:415) with 1 output partitions (allowLocal=false)\n15/07/07 15:43:50 INFO DAGScheduler: Final stage: Stage 0(collect at HiveContext.scala:415)\n15/07/07 15:43:50 INFO DAGScheduler: Parents of final stage: List(Stage 1)\n15/07/07 15:43:50 INFO DAGScheduler: Missing parents: List(Stage 1)\n15/07/07 15:43:50 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[18] at mapPartitions at Exchange.scala:86), which has no missing parents\n15/07/07 15:43:50 INFO MemoryStore: ensureFreeSpace(11024) called with curMem=454358, maxMem=278019440\n15/07/07 15:43:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KB, free 264.7 MB)\n15/07/07 15:43:50 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[18] at mapPartitions at Exchange.scala:86)\n15/07/07 15:43:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, PROCESS_LOCAL, 1182 bytes)\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1182 bytes)\n15/07/07 15:43:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)\n15/07/07 15:43:51 INFO Executor: Running task 1.0 in stage 1.0 (TID 1)\n15/07/07 15:43:51 INFO HadoopRDD: Input split: file:/user/hive/warehouse/src/kv1.txt:2906+2906\n15/07/07 15:43:51 INFO HadoopRDD: Input split: file:/user/hive/warehouse/src/kv1.txt:0+2906\n15/07/07 15:43:51 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n15/07/07 15:43:51 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n15/07/07 15:43:51 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n15/07/07 15:43:51 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n15/07/07 15:43:51 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n15/07/07 15:43:51 INFO Executor: Finished task 1.0 in stage 1.0 (TID 1). 1895 bytes result sent to driver\n15/07/07 15:43:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 1895 bytes result sent to driver\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 401 ms on localhost (1/2)\n15/07/07 15:43:51 INFO DAGScheduler: Stage 1 (mapPartitions at Exchange.scala:86) finished in 0.430 s\n15/07/07 15:43:51 INFO DAGScheduler: looking for newly runnable stages\n15/07/07 15:43:51 INFO DAGScheduler: running: Set()\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 1) in 413 ms on localhost (2/2)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n15/07/07 15:43:51 INFO DAGScheduler: waiting: Set(Stage 0)\n15/07/07 15:43:51 INFO DAGScheduler: failed: Set()\n15/07/07 15:43:51 INFO DAGScheduler: Missing parents for Stage 0: List()\n15/07/07 15:43:51 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[22] at map at HiveContext.scala:360), which is now runnable\n15/07/07 15:43:51 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1deaf84d\n15/07/07 15:43:51 INFO MemoryStore: ensureFreeSpace(9792) called with curMem=465382, maxMem=278019440\n15/07/07 15:43:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.6 KB, free 264.7 MB)\n15/07/07 15:43:51 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[22] at map at HiveContext.scala:360)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n15/07/07 15:43:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 948 bytes)\n15/07/07 15:43:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 2)\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n15/07/07 15:43:51 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 20 ms\n15/07/07 15:43:51 INFO StatsReportListener: task runtime:(count: 2, mean: 407.000000, stdev: 6.000000, max: 413.000000, min: 401.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t401.0 ms\t401.0 ms\t401.0 ms\t401.0 ms\t413.0 ms\t413.0 ms\t413.0 ms\t413.0 ms\t413.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: shuffle bytes written:(count: 2, mean: 50.000000, stdev: 0.000000, max: 50.000000, min: 50.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t50.0 B\t50.0 B\t50.0 B\t50.0 B\t50.0 B\t50.0 B\t50.0 B\t50.0 B\t50.0 B\n15/07/07 15:43:51 INFO StatsReportListener: task result size:(count: 2, mean: 1895.000000, stdev: 0.000000, max: 1895.000000, min: 1895.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\t1895.0 B\n15/07/07 15:43:51 INFO StatsReportListener: executor (non-fetch) time pct: (count: 2, mean: 76.642534, stdev: 1.081437, max: 77.723971, min: 75.561097)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t76 %\t76 %\t76 %\t76 %\t78 %\t78 %\t78 %\t78 %\t78 %\n15/07/07 15:43:51 INFO StatsReportListener: other time pct: (count: 2, mean: 23.357466, stdev: 1.081437, max: 24.438903, min: 22.276029)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t22 %\t22 %\t22 %\t22 %\t24 %\t24 %\t24 %\t24 %\t24 %\n15/07/07 15:43:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 2). 1076 bytes result sent to driver\n15/07/07 15:43:51 INFO DAGScheduler: Stage 0 (collect at HiveContext.scala:415) finished in 0.114 s\n15/07/07 15:43:51 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5ad48e86\n15/07/07 15:43:51 INFO SparkContext: Job finished: collect at HiveContext.scala:415, took 0.778304976 s\n15/07/07 15:43:51 INFO StatsReportListener: task runtime:(count: 1, mean: 116.000000, stdev: 0.000000, max: 116.000000, min: 116.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\t116.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n15/07/07 15:43:51 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n500\nTime taken: 1.14 seconds\n15/07/07 15:43:51 INFO CliDriver: Time taken: 1.14 seconds\nspark-sql> 15/07/07 15:43:51 INFO StatsReportListener: task result size:(count: 1, mean: 1076.000000, stdev: 0.000000, max: 1076.000000, min: 1076.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\t1076.0 B\n15/07/07 15:43:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 2) in 116 ms on localhost (1/1)\n15/07/07 15:43:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n15/07/07 15:43:51 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.827586, stdev: 0.000000, max: 94.827586, min: 94.827586)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\n15/07/07 15:43:51 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n15/07/07 15:43:51 INFO StatsReportListener: other time pct: (count: 1, mean: 5.172414, stdev: 0.000000, max: 5.172414, min: 5.172414)\n15/07/07 15:43:51 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n15/07/07 15:43:51 INFO StatsReportListener: \t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\n```\n\n\u30ed\u30b0\u304c\u591a\u304f\u3066\u308f\u304b\u308a\u306b\u304f\u3044\u3067\u3059\u304c\u3001\n> 500\n> Time taken: 1.14 seconds\n\n\u3068\u306a\u3063\u3066\u3044\u3066500\u30ec\u30b3\u30fc\u30c9\u5165\u3063\u3066\u3044\u308b\u306e\u304c\u53d6\u5f97\u3067\u304d\u307e\u3057\u305f\u3002\n\nhive\u3068\u540c\u3058\u3088\u3046\u306b\u4f7f\u3048\u3066\u3081\u3063\u3061\u3083\u65e9\u3044\u306a\u3041\u3002\u3002\u3002\n\n", "tags": ["gce", "Spark", "sparksql"]}