{"context": "\u63b2\u8f09\u5148 \u306f Keras\u516c\u5f0f Github\u30ea\u30dd\u30b8\u30c8\u30ea \u306e \u4ee5\u4e0b \u306e \u30d1\u30b9 \u3067\u3059\u3002\n\ngithub.com/fchollet/keras/blob/master/keras/objectives.py\n\n\n\n\nkeras/objectives.py\nfrom __future__ import absolute_import\n\nfrom . import backend as K\nfrom .utils.generic_utils import get_from_module\n\n\ndef mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(y_pred - y_true), axis=-1)\n\n\ndef mean_absolute_error(y_true, y_pred):\n    return K.mean(K.abs(y_pred - y_true), axis=-1)\n\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                            K.epsilon(),\n                                            None))\n    return 100. * K.mean(diff, axis=-1)\n\n\ndef mean_squared_logarithmic_error(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.mean(K.square(first_log - second_log), axis=-1)\n\n\ndef squared_hinge(y_true, y_pred):\n    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n\n\ndef hinge(y_true, y_pred):\n    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n\n\ndef categorical_crossentropy(y_true, y_pred):\n    return K.categorical_crossentropy(y_pred, y_true)\n\n\ndef sparse_categorical_crossentropy(y_true, y_pred):\n    return K.sparse_categorical_crossentropy(y_pred, y_true)\n\n\ndef binary_crossentropy(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n\n\ndef kullback_leibler_divergence(y_true, y_pred):\n    y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n\n\ndef poisson(y_true, y_pred):\n    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)\n\n\ndef cosine_proximity(y_true, y_pred):\n    y_true = K.l2_normalize(y_true, axis=-1)\n    y_pred = K.l2_normalize(y_pred, axis=-1)\n    return -K.mean(y_true * y_pred, axis=-1)\n\n\n# Aliases.\n\nmse = MSE = mean_squared_error\nmae = MAE = mean_absolute_error\nmape = MAPE = mean_absolute_percentage_error\nmsle = MSLE = mean_squared_logarithmic_error\nkld = KLD = kullback_leibler_divergence\ncosine = cosine_proximity\n\n\ndef get(identifier):\n    return get_from_module(identifier, globals(), 'objective')\n\n\n\n\n\uff08 \u53c2\u8003 \uff09\n\n\n\u5065\u5fd8\u75c7\u306e\u5099\u5fd8\u9332 \uff082016/7/30\uff09\u300cKeras\u3067Variational AutoEncoder\u300d\n\n__\u63b2\u8f09\u5148 \u306f Keras\u516c\u5f0f Github\u30ea\u30dd\u30b8\u30c8\u30ea \u306e \u4ee5\u4e0b \u306e \u30d1\u30b9 \u3067\u3059\u3002__\n\n* [github.com/fchollet/keras/blob/master/keras/objectives.py](https://github.com/fchollet/keras/blob/master/keras/objectives.py)\n\n<img width=\"1245\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-03 20.10.27.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/cf8019eb-984a-76a2-91ac-14e540e37714.png\">\n\n<img width=\"1244\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-03 20.10.38.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/ff51c875-2666-f243-2197-b6dbc67119ba.png\">\n\n```{python:keras/objectives.py}\nfrom __future__ import absolute_import\n\nfrom . import backend as K\nfrom .utils.generic_utils import get_from_module\n\n\ndef mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(y_pred - y_true), axis=-1)\n\n\ndef mean_absolute_error(y_true, y_pred):\n    return K.mean(K.abs(y_pred - y_true), axis=-1)\n\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                            K.epsilon(),\n                                            None))\n    return 100. * K.mean(diff, axis=-1)\n\n\ndef mean_squared_logarithmic_error(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.mean(K.square(first_log - second_log), axis=-1)\n\n\ndef squared_hinge(y_true, y_pred):\n    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n\n\ndef hinge(y_true, y_pred):\n    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n\n\ndef categorical_crossentropy(y_true, y_pred):\n    return K.categorical_crossentropy(y_pred, y_true)\n\n\ndef sparse_categorical_crossentropy(y_true, y_pred):\n    return K.sparse_categorical_crossentropy(y_pred, y_true)\n\n\ndef binary_crossentropy(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n\n\ndef kullback_leibler_divergence(y_true, y_pred):\n    y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n\n\ndef poisson(y_true, y_pred):\n    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)\n\n\ndef cosine_proximity(y_true, y_pred):\n    y_true = K.l2_normalize(y_true, axis=-1)\n    y_pred = K.l2_normalize(y_pred, axis=-1)\n    return -K.mean(y_true * y_pred, axis=-1)\n\n\n# Aliases.\n\nmse = MSE = mean_squared_error\nmae = MAE = mean_absolute_error\nmape = MAPE = mean_absolute_percentage_error\nmsle = MSLE = mean_squared_logarithmic_error\nkld = KLD = kullback_leibler_divergence\ncosine = cosine_proximity\n\n\ndef get(identifier):\n    return get_from_module(identifier, globals(), 'objective')\n```\n\n___\n\n###__\uff08 \u53c2\u8003 \uff09__\n\n* [\u5065\u5fd8\u75c7\u306e\u5099\u5fd8\u9332 \uff082016/7/30\uff09\u300cKeras\u3067Variational AutoEncoder\u300d](http://sh-tatsuno.com/blog/index.php/2016/07/30/variationalautoencoder/)\n", "tags": ["Keras", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2"]}