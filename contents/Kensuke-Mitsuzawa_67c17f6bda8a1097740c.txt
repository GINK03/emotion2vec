{"context": " More than 1 year has passed since last update.\n\n\u3053\u306e\u8a18\u4e8b\u3067\u7d39\u4ecb\u3059\u308b\u3053\u3068\n\nkmeans\u306e\u8ddd\u96e2\u95a2\u6570\u3063\u3066\u672c\u5f53\u306b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u5927\u6b63\u7fa9\u306a\u306e\uff1f\nscikit-learn\u306ek-means\u3063\u3066\u4f55\u304c\u52d5\u3044\u3066\u3044\u308b\u306e\uff1f\nscikit-learn\u306ek-means\u3067\u8ddd\u96e2\u95a2\u6570\u3092\u5909\u66f4\u3059\u308b\u306b\u306f\u3069\u3046\u3059\u308b\u306e\uff1f\n\nk-means\u6cd5\u3068\u3044\u3048\u3070\u3001\u307f\u306a\u3055\u3093\u3054\u5b58\u77e5\u3001\u3050\u306b\u3087\u3050\u306b\u3087\u3063\u3068\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u304f\u308c\u308b\u30a2\u30ec\u3067\u3059\u306d\u3002\n\u6559\u79d1\u66f8\u306b\u3082\u3088\u304f\u51fa\u3066\u304f\u308b\u30a2\u30ec\u306a\u306e\u3067\u3001\u7d30\u304b\u3044\u8aac\u660e\u306f\u7701\u304d\u307e\u3059\u304c\u3001\u3044\u307e\u3060\u306b\u4f7f\u308f\u308c\u3066\u308b\u306e\u306f\u3001\uff08\u7981\u5247\u4e8b\u9805\u3067\u3059\uff09\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3088\u306d\u3002\n\u3042\u3001\u3061\u306a\u307f\u306b\u3053\u3046\u3044\u3046\u52d5\u753b\u3068\u304b\u3053\u3046\u3044\u3046\u30c4\u30fc\u30eb\u3068\u304b\u4f7f\u3046\u3068\u3001k-means\u306e\u52d5\u304d\u304c\u3088\u304f\u308f\u304b\u3063\u3066\u30ca\u30a4\u30b9\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\u3067\u3001k-means\u3067\u306f\u30bb\u30f3\u30c8\u30ed\u30a4\u30c9\u3068\u5404\u30c7\u30fc\u30bf\u3068\u306e\u8ddd\u96e2\u3092\u6e2c\u308b\u305f\u3081\u306b\u3001\u300c\u8ddd\u96e2\u95a2\u6570\u300d\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\n\u6559\u79d1\u66f8\u7684\u306b\u306f\u300c\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u300d\u4e00\u629e\u306e\u3053\u3068\u304c\u591a\u3044\u306e\u3067\u3059\u304c\u3001\u305d\u308c\u3063\u3066\u672c\u5f53\u306b\u6b63\u3057\u3044\u306e\u3067\u3057\u3087\u3046\u304b\uff1f\n\u3053\u306e\u8ad6\u6587, Anna Huang, \"Similarity Measures for Text Document Clustering\", 2008\n\u306f\u305d\u3093\u306a\u7591\u554f\u306b\u660e\u5feb\u306b\u7b54\u3048\u3066\u304f\u308c\u307e\u3059\u3002\n\u8ad6\u6587\u304b\u3089\u62dd\u501f\u3057\u305fTable4\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002Entropy\u6307\u6a19\u306a\u306e\u3067\u3001\u4f4e\u3044\u65b9\u304c\u826f\u3044\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3067\u3059\u3002\n\n\u3042\u304f\u307e\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u306f\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u304c\u5927\u6b63\u7fa9\u3067\u306f\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u306d\u3002\n\u3067\u306f\u3001\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u3082\u8a66\u3057\u305f\u304f\u306a\u308b\u306e\u304c\u4eba\u60c5\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\n\nscikit-learn\u3067\u3084\u3063\u3066\u307f\u3088\u3046\n\u7d50\u8ad6\u304b\u3089\u3044\u3046\u3068\u3001\u305d\u30fc\u3044\u3046\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\u8abf\u3079\u308b\u306e\u30e1\u30f3\u30c9\u30af\u30bb\u30fc\u3002\u3068\u304b\u3058\u3083\u306a\u304f\u3066\u3001\u30de\u30b8\u3067\u306a\u3044\u3067\u3059\u3002\nscikit-learn\u306e\u30b3\u30fc\u30c9\u3092\u307f\u308b\u3068\u305d\u308c\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n\u3053\u308c\u306fk-means\u306e\u30b3\u30fc\u30c9\u7247\u3067\u3059\u304c\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u304c\u30cf\u30fc\u30c9\u30b3\u30fc\u30c9\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u3064\u307e\u308a\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u3068\u3057\u3066\u8ddd\u96e2\u95a2\u6570\u3092\u5909\u66f4\u3059\u308b\u4f59\u5730\u306f\u306a\u3044\u306e\u3067\u3059\u3002\n\u3067\u3082\u3001\u306a\u3093\u3067\u306a\u3044\u306e\u3067\u3057\u3087\u3046\uff1fgithub\u306eissue\u3092\u307f\u308b\u3068\u3001\u300c\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u3060\u3068\u3001\u4e00\u822c\u5316\u304c\u96e3\u3057\u3044\u304b\u3089\u300d\u307f\u305f\u3044\u306a\u3053\u3068\u304c\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u3002\n\u305d\u308c\u3067\u3082\u3001\u96a0\u3057\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u3057\u305f\u3089\u3044\u3044\u3068\u601d\u3046\u3093\u3067\u3059\u3051\u3069\u306d\u30fb\u30fb\u30fb\n\n\u3058\u3083\u3042\u3001\u3069\u30fc\u3084\u3063\u3066\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u4f7f\u3046\u306e\u3088\uff01\uff1f\n\u3044\u304f\u3064\u304b\u65b9\u6cd5\u306f\u3042\u308b\u307f\u305f\u3044\u3067\u3059\u3002\n\n\u81ea\u5206\u3067k-means\u66f8\u304f\u3002stack-overflow\u306e\u30da\u30fc\u30b8\u306b\u305d\u306e\u65b9\u6cd5\u304c\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\nk-centroid\u6cd5\u3092\u4f7f\u3046\u3002scikit-learn\u3067\u5229\u7528\u3067\u304d\u308b\u3089\u3057\u3044\u3067\u3059\u3088\u3002\u8abf\u3079\u3066\u306a\u3044\u3051\u3069\u3002\nmonkey-patch\u3092\u5229\u7528\u3059\u308b\u3002\u4eca\u56de\u306f\u3053\u308c\u3092\u3057\u307e\u3059\u3002\n\nmonkey-patch\u3068\u306f\u3001\u3068\u3069\u306e\u3064\u307e\u308a\u3001\u300c\u30e1\u30bd\u30c3\u30c9\u3092\u4e0a\u66f8\u304d\u3057\u3066\u597d\u304d\u306a\u51e6\u7406\u3092\u3059\u308b\u300d\u3053\u3068\u3067\u3059\u3002\n\u4eca\u56de\u306f\u3001k-means\u5185\u306e\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3092\u4e0a\u66f8\u304d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u3053\u306e\u30da\u30fc\u30b8\u3067\u305d\u306e\u65b9\u6cd5\u304c\u8ff0\u3079\u3089\u308c\u3066\u3044\u307e\u3059\u3002\n\u5b9f\u969b\u306b\u3001\u3084\u3063\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u4f7f\u3046\u30c7\u30fc\u30bf\u306f\u3053\u3053\u304b\u3089\u62dd\u501f\u3057\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306f\u306a\u3044\u3067\u3059\u304c\u3001\u3054\u611b\u5b0c\u3068\u3044\u3046\u3053\u3068\u3067\u30fb\u30fb\u30fb\n\nscikit-learn\u306e\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\u6319\u52d5\u3092\u898b\u3066\u307f\u308b\nk-means\u304c\u547c\u3073\u51fa\u3057\u3066\u3044\u308b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306f\u3053\u308c\u3067\u3059\u3002\n\u6319\u52d5\u3092\u5927\u304d\u304f\u5206\u3051\u308b\u3068\u3001\n\nY\u304cNone\u306e\u5834\u5408\nX\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u306earray\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306earray\u306e\u5834\u5408\nX\u304c\uff12\u30b5\u30f3\u30d7\u30eb\u306earray\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306earray\u306e\u5834\u5408\n\n\u3067\u3059\u3002\n\u3044\u305a\u308c\u306e\u5834\u5408\u3082\u3001\narray([\n    [X\u3068Y\u306e\u8ddd\u96e2]\n])\n\n\u3068\u3044\u3046\u5f62\u306earray\u3092\u8fd4\u5374\u3057\u307e\u3059\u3002\n\u306a\u306e\u3067\u3001\u65b0\u3057\u304f\u8ddd\u96e2\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u6642\u306f\u3001\u3053\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5f93\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u4fc2\u6570\u306e\u8ddd\u96e2\u95a2\u6570\u3092\u5b9f\u88c5\u3059\u308b\n\u8ad6\u6587\u306b\u7d39\u4ecb\u3055\u308c\u3066\u3044\u305f\u30d4\u30a2\u30bd\u30f3\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002python\u3067\u306fscipy\u3067\u5229\u7528\u53ef\u80fd\u3067\u3059\u3002\n\u305f\u3060\u3057\u3001\u8ad6\u6587\u3067\u3082\u89e6\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u306e\u307e\u307e\u3067\u306f\u8ddd\u96e2\u95a2\u6570\u306b\u306f\u306a\u308a\u307e\u305b\u3093\u3002\u305d\u3053\u3067\u3001\u3061\u3087\u3063\u3068\u3060\u3051\u5f0f\u5909\u5f62\u3092\u3057\u307e\u3059\u3002\nspecial_pearsonr(X, Y) = {\n    1 - pearsonr(X, Y) if pearsonr(X, Y) > 0,\n    |pearsonr(X, Y)| else\n}\n\n\u3068\u3044\u3046\u5f0f\u5909\u5f62\u3092\u3057\u307e\u3059\u3002\n\u3067\u306f\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\uff13\u3064\u306e\u5834\u5408\u5206\u3051\u3092\u8003\u616e\u3057\u3066\u3001\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3059\u3002\ndef special_pearsonr(X, Y):\n    pearsonr_value = scipy.stats.pearsonr(X, Y)\n    if pearsonr_value[0] < 0:\n        return abs(pearsonr_value[0])\n    else:\n        return 1 - pearsonr_value[0]\n\n\ndef sub_pearsonr(X, row_index_1, row_index_2):\n    pearsonr_distance = special_pearsonr(X[row_index_1], X[row_index_2])\n    return (row_index_1, row_index_2, pearsonr_distance)\n\n\ndef pearsonr_distances(X, Y=None):\n    if Y==None:\n        # X\u3060\u3051\u304c\u5165\u529b\u3055\u308c\u3066\u3044\u3066\u3001X\u304c2d-array\u306e\u5834\u5408\n        row_combinations = list(combinations(range(0, len(X)), 2))\n        pearsonr_set = [sub_pearsonr(X, index_set_tuple[0], index_set_tuple[1]) for index_set_tuple in row_combinations]\n        matrix_source_data = pearsonr_set + map(copy_inverse_index, pearsonr_set)\n\n        row = [t[0] for t in matrix_source_data]\n        col = [t[1] for t in matrix_source_data]\n        data = [t[2] for t in matrix_source_data]\n\n        pearsonr_matrix = special_pearsonr((data, (row, col)), (X.shape[0], X.shape[0]))\n        return pearsonr_matrix.toarray()\n\n    elif len(X.shape)==1 and len(Y.shape)==2:\n        # X\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306e\u5834\u5408\n        # return\u306f1 sample\u306earray\n        pearsonr_set = numpy.array([special_pearsonr(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n\n        return pearsonr_set\n\n    elif len(X.shape)==2 and len(Y.shape)==2:\n        # 1\u884c\u306b\u306f\u3001X[i]\u3068Y[j] in n(Y)\u306e\u8ddd\u96e2\u3092\u8a18\u9332\u3057\u3066\u8fd4\u3059\n        # X[i]\u3068Y\u306e\u3059\u3079\u3066\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n        pearsonr_x_and_all_y = lambda X, Y: numpy.array([special_pearsonr(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n        pearsonr_divergence_set = numpy.array([pearsonr_x_and_all_y(X[x_i], Y) for x_i in range(0, X.shape[0])])\n        return pearsonr_divergence_set\n    else:\n        raise Exception(\"Exception case caused\")\n\npearsonr_distances\u304ck-means\u3067\u547c\u3073\u51fa\u3059\u8ddd\u96e2\u95a2\u6570\u3067\u3059\u3002\n\u3067\u3001\u6b21\u306b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3092monkey-patch\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\nstart = time.time()\n\n# \u5165\u529b\u3092\u4fdd\u8a3c\u3059\u308b\u305f\u3081\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3068\u540c\u3058\u30d1\u30e9\u30e1\u30bf\u3092\u6301\u3064\u95a2\u6570\u3092\u7528\u610f\u3059\u308b\ndef new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False):\n    return pearsonr_distances(X, Y)\n\nfrom sklearn.cluster import k_means_\n\n# \u3053\u3053\u3067\u4e0a\u66f8\u304d\uff01\nk_means_.euclidean_distances = new_euclidean_distances\nkmeans_model = KMeans(n_clusters=3, random_state=10, init='random').fit(features)\nprint(kmeans_model.labels_)\nelapsed_time = time.time() - start\nprint (\"Pearsonr k-means:{0}\".format(elapsed_time))\n\n\u5b9f\u884c\u6642\u9593\u3092\u898b\u305f\u304b\u3063\u305f\u306e\u3067\u3001\u3064\u3044\u3067\u306b\u6642\u9593\u3082\u8a08\u6e2c\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\u8a00\u3044\u5fd8\u308c\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u30c7\u30fc\u30bf\u306f\nfeatures = numpy.array([\n        [  80,  85, 100 ],\n        [  96, 100, 100 ],\n        [  54,  83,  98 ],\n        [  80,  98,  98 ],\n        [  90,  92,  91 ],\n        [  84,  78,  82 ],\n        [  79, 100,  96 ],\n        [  88,  92,  92 ],\n        [  98,  73,  72 ],\n        [  75,  84,  85 ],\n        [  92, 100,  96 ],\n        [  96,  92,  90 ],\n        [  99,  76,  91 ],\n        [  75,  82,  88 ],\n        [  90,  94,  94 ],\n        [  54,  84,  87 ],\n        [  92,  89,  62 ],\n        [  88,  94,  97 ],\n        [  42,  99,  80 ],\n        [  70,  98,  70 ],\n        [  94,  78,  83 ],\n        [  52,  73,  87 ],\n        [  94,  88,  72 ],\n        [  70,  73,  80 ],\n        [  95,  84,  90 ],\n        [  95,  88,  84 ],\n        [  75,  97,  89 ],\n        [  49,  81,  86 ],\n        [  83,  72,  80 ],\n        [  75,  73,  88 ],\n        [  79,  82,  76 ],\n        [ 100,  77,  89 ],\n        [  88,  63,  79 ],\n        [ 100,  50,  86 ],\n        [  55,  96,  84 ],\n        [  92,  74,  77 ],\n        [  97,  50,  73 ],\n])\n\n\u3067\u3059\u3002\u5b9f\u884c\u3057\u3066\u307f\u308b\u3068\u3001\n[0 0 0 1 1 2 1 0 2 1 0 2 2 0 0 1 1 0 1 0 2 0 2 0 2 2 1 1 2 1 1 2 2 2 1 2 2]\nPearsonr k-means:11.0138161182\n\n\u3068\u3057\u3063\u304b\u308a\u52d5\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\uff01\u30e4\u30c3\u30bf\u30cd\uff01\n\u3061\u306a\u307f\u306b\u3001\u901a\u5e38\u306ek-means\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\n[1 1 0 1 1 2 1 1 2 1 1 1 2 1 1 0 2 1 0 0 2 0 2 0 1 1 1 0 2 2 2 2 2 2 0 2 2]\n========================================\nNormal k-means:0.016499042511\n\n\u3067\u3059\u3002\uff10\u3068\uff11\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u9055\u3046\u30c7\u30fc\u30bf\u304c\u3044\u3064\u304f\u304b\u78ba\u8a8d\u3067\u304d\u307e\u3059\u306d\u3002\n\u5b9f\u884c\u6642\u9593\u306f\u30fb\u30fb\u30fb\u6b8b\u5ff5\u306a\u304c\u3089\u3001\uff16\uff10\uff10\u500d\u8fd1\u304f\u306e\u5dee\u304c\u3067\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u30fb\u30fb\u30fb\n\u547c\u3073\u51fa\u3059\u95a2\u6570\u304c\u5897\u3048\u3066\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u4ed5\u65b9\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u3042\u305f\u308a\u3001\u5b9f\u88c5\u3092\u5de5\u592b\u3059\u308c\u3070\u3001\u3082\u3046\u5c11\u3057\u65e9\u304f\u306a\u308b\u53ef\u80fd\u6027\u306f\u5341\u5206\u306b\u3042\u308a\u305d\u3046\u3067\u3059\u3002\uff08\u65e9\u304f\u3057\u3066\u304f\u308c\u308b\u30a8\u30ed\u3044\u4eba\u3002\u52df\u96c6\uff09\n\u7279\u306b\u6587\u66f8\u5206\u985e\u306b\u4f7f\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u5dee\u5206\u306f\u898b\u3066\u307e\u305b\u3093\u304c\u3001\u8ad6\u6587\u3067\u5b9f\u8a3c\u306f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u30d4\u30a2\u30bd\u30f3\u306e\u76f8\u95a2\u4fc2\u6570\u306f\u304d\u3063\u3068\u826f\u3044\u7d50\u679c\u3092\u51fa\u3057\u3066\u304f\u308c\u308b\uff08\u306f\u305a\u30fb\u30fb\uff09\n\n\u307e\u3068\u3081\n\nk-means\u306e\u8ddd\u96e2\u95a2\u6570\u306f\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\u5927\u6b63\u7fa9\u3067\u306f\u306a\u3044\nscikit-learn\u3067\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u4f7f\u3046\u306e\u306f\u5c11\u3057\u9aa8\u304c\u6298\u308c\u308b\n\u9045\u3044\n\n\n\n\u3061\u306a\u307f\u306b\u30fb\u30fb\u30fb\n\u30ab\u30eb\u30d0\u30c3\u30af\u30fb\u30e9\u30a4\u30d6\u30ca\u30fc\u8ddd\u96e2\u3082\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u305f\u3060\u3001\u6b8b\u5ff5\u306a\u304c\u3089\u3001\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u6b62\u307e\u308a\u307e\u3059\u3002\nX, Y\u306e\u8ddd\u96e2\u3092\u8a08\u6e2c\u3059\u308b\u3068\u3001\u3069\u3046\u3057\u3066\u3082\u30de\u30a4\u30ca\u30b9\u8981\u7d20\u304c\u767a\u751f\u3057\u3066\u3057\u307e\u3044\u3001\u8ddd\u96e2\u304cinf\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u305f\u3081\u3001\u305d\u3082\u305d\u3082\u521d\u671f\u5316\u3059\u3089\u3067\u304d\u307e\u305b\u3093\u3002\n\u30de\u30a4\u30ca\u30b9\u8981\u7d20\u3092\u3069\u3046\u306b\u304b\u3059\u308c\u3070\u3044\u3044\u3093\u3067\u3057\u3087\u3046\u3051\u3069\u3001\u3069\u3046\u3057\u305f\u3089\u3044\u3044\u3093\u3060\u308d\u3046\u3002\u7279\u306b\u8ad6\u6587\u3067\u3082\u89e6\u308c\u3066\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\nfrom sklearn.preprocessing import normalize\ndef averaged_kullback_leibler(X, Y):\n    #norm_X = normalize(X=X, norm='l1')[0]\n    #norm_Y = normalize(X=Y, norm='l1')[0]\n    norm_X = X\n    norm_Y = Y\n\n    pie_1 = norm_X / (norm_X + norm_Y)\n    pie_2 = norm_Y / (norm_X + norm_Y)\n    M = (pie_1 * norm_X) + (pie_2 * norm_Y)\n\n    KLD_averaged = (pie_1 * scipy.stats.entropy(pk=norm_X, qk=M)) + (pie_2 * scipy.stats.entropy(pk=norm_Y, qk=M))\n\n    return KLD_averaged\n\ndef sub_KL_divergence(X, row_index_1, row_index_2):\n    #kl_distance = scipy.stats.entropy(pk=X[row_index_1], qk=X[row_index_2])\n    kl_distance = averaged_kullback_leibler(X[row_index_1], X[row_index_2])\n    return (row_index_1, row_index_2, kl_distance)\n\ndef copy_inverse_index(row_col_data_tuple):\n    return (row_col_data_tuple[1], row_col_data_tuple[0], row_col_data_tuple[2])\n\ndef KLD_distances(X, Y=None):\n    if Y==None:\n        # X\u3060\u3051\u304c\u5165\u529b\u3055\u308c\u3066\u3044\u3066\u3001X\u304c2d-array\u306e\u5834\u5408\n        row_combinations = list(combinations(range(0, len(X)), 2))\n        kl_divergence_set = [sub_KL_divergence(X, index_set_tuple[0], index_set_tuple[1]) for index_set_tuple in row_combinations]\n        matrix_source_data = kl_divergence_set + map(copy_inverse_index, kl_divergence_set)\n\n        row = [t[0] for t in matrix_source_data]\n        col = [t[1] for t in matrix_source_data]\n        data = [t[2] for t in matrix_source_data]\n\n        kl_distance_matrix = scipy.sparse.csr_matrix((data, (row, col)), (X.shape[0], X.shape[0]))\n        return kl_distance_matrix.toarray()\n    elif len(X.shape)==1 and len(Y.shape)==2:\n        # X\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306e\u5834\u5408\n        # return\u306f1 sample\u306earray\n        kl_divergence_set = numpy.array([averaged_kullback_leibler(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n\n        return kl_divergence_set\n    elif len(X.shape)==2 and len(Y.shape)==2:\n        # 1\u884c\u306b\u306f\u3001X[i]\u3068Y[j] in n(Y)\u306e\u8ddd\u96e2\u3092\u8a18\u9332\u3057\u3066\u8fd4\u3059\n        # X[i]\u3068Y\u306e\u3059\u3079\u3066\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n        for x_i in range(0, X.shape[0]):\n            xx = X[x_i]\n            for y_sample_index in range(0, Y.shape[0]):\n                #print(xx)\n                #print(Y[y_sample_index])\n                print(averaged_kullback_leibler(xx, Y[y_sample_index]))\n\n        kld_x_and_all_y = lambda X, Y: numpy.array([averaged_kullback_leibler(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n        kl_divergence_set = numpy.array([kld_x_and_all_y(X[x_i], Y) for x_i in range(0, X.shape[0])])\n        return kl_divergence_set\n    else:\n        raise Exception(\"Exception case caused\")\n\n# \u3053\u306e\u8a18\u4e8b\u3067\u7d39\u4ecb\u3059\u308b\u3053\u3068\n\n* kmeans\u306e\u8ddd\u96e2\u95a2\u6570\u3063\u3066\u672c\u5f53\u306b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u5927\u6b63\u7fa9\u306a\u306e\uff1f\n* scikit-learn\u306ek-means\u3063\u3066\u4f55\u304c\u52d5\u3044\u3066\u3044\u308b\u306e\uff1f\n* scikit-learn\u306ek-means\u3067\u8ddd\u96e2\u95a2\u6570\u3092\u5909\u66f4\u3059\u308b\u306b\u306f\u3069\u3046\u3059\u308b\u306e\uff1f\n\n\nk-means\u6cd5\u3068\u3044\u3048\u3070\u3001\u307f\u306a\u3055\u3093\u3054\u5b58\u77e5\u3001\u3050\u306b\u3087\u3050\u306b\u3087\u3063\u3068\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u304f\u308c\u308b\u30a2\u30ec\u3067\u3059\u306d\u3002\n\n\u6559\u79d1\u66f8\u306b\u3082\u3088\u304f\u51fa\u3066\u304f\u308b\u30a2\u30ec\u306a\u306e\u3067\u3001\u7d30\u304b\u3044\u8aac\u660e\u306f\u7701\u304d\u307e\u3059\u304c\u3001\u3044\u307e\u3060\u306b\u4f7f\u308f\u308c\u3066\u308b\u306e\u306f\u3001\uff08\u7981\u5247\u4e8b\u9805\u3067\u3059\uff09\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3088\u306d\u3002\n\n\u3042\u3001\u3061\u306a\u307f\u306b[\u3053\u3046\u3044\u3046\u52d5\u753b](https://www.youtube.com/watch?v=gSt4_kcZPxE)\u3068\u304b[\u3053\u3046\u3044\u3046\u30c4\u30fc\u30eb](http://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\u3068\u304b\u4f7f\u3046\u3068\u3001k-means\u306e\u52d5\u304d\u304c\u3088\u304f\u308f\u304b\u3063\u3066\u30ca\u30a4\u30b9\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u3067\u3001k-means\u3067\u306f\u30bb\u30f3\u30c8\u30ed\u30a4\u30c9\u3068\u5404\u30c7\u30fc\u30bf\u3068\u306e\u8ddd\u96e2\u3092\u6e2c\u308b\u305f\u3081\u306b\u3001\u300c\u8ddd\u96e2\u95a2\u6570\u300d\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u6559\u79d1\u66f8\u7684\u306b\u306f\u300c\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u300d\u4e00\u629e\u306e\u3053\u3068\u304c\u591a\u3044\u306e\u3067\u3059\u304c\u3001\u305d\u308c\u3063\u3066\u672c\u5f53\u306b\u6b63\u3057\u3044\u306e\u3067\u3057\u3087\u3046\u304b\uff1f\n\n[\u3053\u306e\u8ad6\u6587](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf), Anna Huang, \"Similarity Measures for Text Document Clustering\", 2008\n\n\u306f\u305d\u3093\u306a\u7591\u554f\u306b\u660e\u5feb\u306b\u7b54\u3048\u3066\u304f\u308c\u307e\u3059\u3002\n\n\u8ad6\u6587\u304b\u3089\u62dd\u501f\u3057\u305fTable4\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002Entropy\u6307\u6a19\u306a\u306e\u3067\u3001\u4f4e\u3044\u65b9\u304c\u826f\u3044\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3067\u3059\u3002\n\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2015-11-04 20.24.24.png](https://qiita-image-store.s3.amazonaws.com/0/33533/41094b24-3bcb-0d04-93ab-edc54eb59501.png)\n\n\u3042\u304f\u307e\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u306f\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u304c\u5927\u6b63\u7fa9\u3067\u306f\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u306d\u3002\n\n\u3067\u306f\u3001\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u3082\u8a66\u3057\u305f\u304f\u306a\u308b\u306e\u304c\u4eba\u60c5\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\n\n# scikit-learn\u3067\u3084\u3063\u3066\u307f\u3088\u3046\n\n\u7d50\u8ad6\u304b\u3089\u3044\u3046\u3068\u3001\u305d\u30fc\u3044\u3046\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u8abf\u3079\u308b\u306e\u30e1\u30f3\u30c9\u30af\u30bb\u30fc\u3002\u3068\u304b\u3058\u3083\u306a\u304f\u3066\u3001\u30de\u30b8\u3067\u306a\u3044\u3067\u3059\u3002\n\nscikit-learn\u306e\u30b3\u30fc\u30c9\u3092\u307f\u308b\u3068\u305d\u308c\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n```\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n```\n\n\u3053\u308c\u306f[k-means\u306e\u30b3\u30fc\u30c9\u7247](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/k_means_.py#L98)\u3067\u3059\u304c\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u304c\u30cf\u30fc\u30c9\u30b3\u30fc\u30c9\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\u3064\u307e\u308a\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u3068\u3057\u3066\u8ddd\u96e2\u95a2\u6570\u3092\u5909\u66f4\u3059\u308b\u4f59\u5730\u306f\u306a\u3044\u306e\u3067\u3059\u3002\n\n\u3067\u3082\u3001\u306a\u3093\u3067\u306a\u3044\u306e\u3067\u3057\u3087\u3046\uff1f[github\u306eissue](https://github.com/scikit-learn/scikit-learn/issues/1188)\u3092\u307f\u308b\u3068\u3001\u300c\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u3060\u3068\u3001\u4e00\u822c\u5316\u304c\u96e3\u3057\u3044\u304b\u3089\u300d\u307f\u305f\u3044\u306a\u3053\u3068\u304c\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u3002\n\n\u305d\u308c\u3067\u3082\u3001\u96a0\u3057\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u3057\u305f\u3089\u3044\u3044\u3068\u601d\u3046\u3093\u3067\u3059\u3051\u3069\u306d\u30fb\u30fb\u30fb\n\n# \u3058\u3083\u3042\u3001\u3069\u30fc\u3084\u3063\u3066\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u4f7f\u3046\u306e\u3088\uff01\uff1f\n\n\u3044\u304f\u3064\u304b\u65b9\u6cd5\u306f\u3042\u308b\u307f\u305f\u3044\u3067\u3059\u3002\n\n1. \u81ea\u5206\u3067k-means\u66f8\u304f\u3002[stack-overflow\u306e\u30da\u30fc\u30b8](http://stackoverflow.com/questions/5529625/is-it-possible-to-specify-your-own-distance-function-using-scikit-learn-k-means)\u306b\u305d\u306e\u65b9\u6cd5\u304c\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\n2. k-centroid\u6cd5\u3092\u4f7f\u3046\u3002scikit-learn\u3067\u5229\u7528\u3067\u304d\u308b\u3089\u3057\u3044\u3067\u3059\u3088\u3002\u8abf\u3079\u3066\u306a\u3044\u3051\u3069\u3002\n3. monkey-patch\u3092\u5229\u7528\u3059\u308b\u3002\u4eca\u56de\u306f\u3053\u308c\u3092\u3057\u307e\u3059\u3002\n\nmonkey-patch\u3068\u306f\u3001\u3068\u3069\u306e\u3064\u307e\u308a\u3001\u300c\u30e1\u30bd\u30c3\u30c9\u3092\u4e0a\u66f8\u304d\u3057\u3066\u597d\u304d\u306a\u51e6\u7406\u3092\u3059\u308b\u300d\u3053\u3068\u3067\u3059\u3002\n\n\u4eca\u56de\u306f\u3001k-means\u5185\u306e\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3092\u4e0a\u66f8\u304d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002[\u3053\u306e\u30da\u30fc\u30b8](http://fraka6.blogspot.ca/2013/04/kmeans-with-configurable-distance.html)\u3067\u305d\u306e\u65b9\u6cd5\u304c\u8ff0\u3079\u3089\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u5b9f\u969b\u306b\u3001\u3084\u3063\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u4f7f\u3046\u30c7\u30fc\u30bf\u306f[\u3053\u3053](http://qiita.com/ynakayama/items/1223b6844a1a044e2e3b)\u304b\u3089\u62dd\u501f\u3057\u307e\u3059\u3002\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306f\u306a\u3044\u3067\u3059\u304c\u3001\u3054\u611b\u5b0c\u3068\u3044\u3046\u3053\u3068\u3067\u30fb\u30fb\u30fb\n\n# scikit-learn\u306e\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\u6319\u52d5\u3092\u898b\u3066\u307f\u308b\n\nk-means\u304c\u547c\u3073\u51fa\u3057\u3066\u3044\u308b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306f[\u3053\u308c](https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/metrics/pairwise.py#L136)\u3067\u3059\u3002\n\n\u6319\u52d5\u3092\u5927\u304d\u304f\u5206\u3051\u308b\u3068\u3001\n\n1. Y\u304cNone\u306e\u5834\u5408\n2. X\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u306earray\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306earray\u306e\u5834\u5408\n3. X\u304c\uff12\u30b5\u30f3\u30d7\u30eb\u306earray\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306earray\u306e\u5834\u5408\n\n\u3067\u3059\u3002\n\n\u3044\u305a\u308c\u306e\u5834\u5408\u3082\u3001\n\n```\narray([\n    [X\u3068Y\u306e\u8ddd\u96e2]\n])\n```\n\n\u3068\u3044\u3046\u5f62\u306earray\u3092\u8fd4\u5374\u3057\u307e\u3059\u3002\n\n\u306a\u306e\u3067\u3001\u65b0\u3057\u304f\u8ddd\u96e2\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u6642\u306f\u3001\u3053\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5f93\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\n## \u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u4fc2\u6570\u306e\u8ddd\u96e2\u95a2\u6570\u3092\u5b9f\u88c5\u3059\u308b\n\n\u8ad6\u6587\u306b\u7d39\u4ecb\u3055\u308c\u3066\u3044\u305f\u30d4\u30a2\u30bd\u30f3\u306e\u76f8\u95a2\u4fc2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002python\u3067\u306f[scipy\u3067\u5229\u7528\u53ef\u80fd](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html)\u3067\u3059\u3002\n\n\u305f\u3060\u3057\u3001\u8ad6\u6587\u3067\u3082\u89e6\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u306e\u307e\u307e\u3067\u306f\u8ddd\u96e2\u95a2\u6570\u306b\u306f\u306a\u308a\u307e\u305b\u3093\u3002\u305d\u3053\u3067\u3001\u3061\u3087\u3063\u3068\u3060\u3051\u5f0f\u5909\u5f62\u3092\u3057\u307e\u3059\u3002\n\n```\nspecial_pearsonr(X, Y) = {\n    1 - pearsonr(X, Y) if pearsonr(X, Y) > 0,\n    |pearsonr(X, Y)| else\n}\n```\n\n\u3068\u3044\u3046\u5f0f\u5909\u5f62\u3092\u3057\u307e\u3059\u3002\n\n\u3067\u306f\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\uff13\u3064\u306e\u5834\u5408\u5206\u3051\u3092\u8003\u616e\u3057\u3066\u3001\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3059\u3002\n\n```\ndef special_pearsonr(X, Y):\n    pearsonr_value = scipy.stats.pearsonr(X, Y)\n    if pearsonr_value[0] < 0:\n        return abs(pearsonr_value[0])\n    else:\n        return 1 - pearsonr_value[0]\n\n\ndef sub_pearsonr(X, row_index_1, row_index_2):\n    pearsonr_distance = special_pearsonr(X[row_index_1], X[row_index_2])\n    return (row_index_1, row_index_2, pearsonr_distance)\n\n\ndef pearsonr_distances(X, Y=None):\n    if Y==None:\n        # X\u3060\u3051\u304c\u5165\u529b\u3055\u308c\u3066\u3044\u3066\u3001X\u304c2d-array\u306e\u5834\u5408\n        row_combinations = list(combinations(range(0, len(X)), 2))\n        pearsonr_set = [sub_pearsonr(X, index_set_tuple[0], index_set_tuple[1]) for index_set_tuple in row_combinations]\n        matrix_source_data = pearsonr_set + map(copy_inverse_index, pearsonr_set)\n\n        row = [t[0] for t in matrix_source_data]\n        col = [t[1] for t in matrix_source_data]\n        data = [t[2] for t in matrix_source_data]\n\n        pearsonr_matrix = special_pearsonr((data, (row, col)), (X.shape[0], X.shape[0]))\n        return pearsonr_matrix.toarray()\n\n    elif len(X.shape)==1 and len(Y.shape)==2:\n        # X\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306e\u5834\u5408\n        # return\u306f1 sample\u306earray\n        pearsonr_set = numpy.array([special_pearsonr(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n\n        return pearsonr_set\n\n    elif len(X.shape)==2 and len(Y.shape)==2:\n        # 1\u884c\u306b\u306f\u3001X[i]\u3068Y[j] in n(Y)\u306e\u8ddd\u96e2\u3092\u8a18\u9332\u3057\u3066\u8fd4\u3059\n        # X[i]\u3068Y\u306e\u3059\u3079\u3066\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n        pearsonr_x_and_all_y = lambda X, Y: numpy.array([special_pearsonr(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n        pearsonr_divergence_set = numpy.array([pearsonr_x_and_all_y(X[x_i], Y) for x_i in range(0, X.shape[0])])\n        return pearsonr_divergence_set\n    else:\n        raise Exception(\"Exception case caused\")\n```\n\n`pearsonr_distances`\u304ck-means\u3067\u547c\u3073\u51fa\u3059\u8ddd\u96e2\u95a2\u6570\u3067\u3059\u3002\n\n\u3067\u3001\u6b21\u306b\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3092monkey-patch\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n```\nstart = time.time()\n\n# \u5165\u529b\u3092\u4fdd\u8a3c\u3059\u308b\u305f\u3081\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u95a2\u6570\u3068\u540c\u3058\u30d1\u30e9\u30e1\u30bf\u3092\u6301\u3064\u95a2\u6570\u3092\u7528\u610f\u3059\u308b\ndef new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False):\n    return pearsonr_distances(X, Y)\n\nfrom sklearn.cluster import k_means_\n\n# \u3053\u3053\u3067\u4e0a\u66f8\u304d\uff01\nk_means_.euclidean_distances = new_euclidean_distances\nkmeans_model = KMeans(n_clusters=3, random_state=10, init='random').fit(features)\nprint(kmeans_model.labels_)\nelapsed_time = time.time() - start\nprint (\"Pearsonr k-means:{0}\".format(elapsed_time))\n```\n\n\u5b9f\u884c\u6642\u9593\u3092\u898b\u305f\u304b\u3063\u305f\u306e\u3067\u3001\u3064\u3044\u3067\u306b\u6642\u9593\u3082\u8a08\u6e2c\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n\u8a00\u3044\u5fd8\u308c\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u30c7\u30fc\u30bf\u306f\n\n```\nfeatures = numpy.array([\n        [  80,  85, 100 ],\n        [  96, 100, 100 ],\n        [  54,  83,  98 ],\n        [  80,  98,  98 ],\n        [  90,  92,  91 ],\n        [  84,  78,  82 ],\n        [  79, 100,  96 ],\n        [  88,  92,  92 ],\n        [  98,  73,  72 ],\n        [  75,  84,  85 ],\n        [  92, 100,  96 ],\n        [  96,  92,  90 ],\n        [  99,  76,  91 ],\n        [  75,  82,  88 ],\n        [  90,  94,  94 ],\n        [  54,  84,  87 ],\n        [  92,  89,  62 ],\n        [  88,  94,  97 ],\n        [  42,  99,  80 ],\n        [  70,  98,  70 ],\n        [  94,  78,  83 ],\n        [  52,  73,  87 ],\n        [  94,  88,  72 ],\n        [  70,  73,  80 ],\n        [  95,  84,  90 ],\n        [  95,  88,  84 ],\n        [  75,  97,  89 ],\n        [  49,  81,  86 ],\n        [  83,  72,  80 ],\n        [  75,  73,  88 ],\n        [  79,  82,  76 ],\n        [ 100,  77,  89 ],\n        [  88,  63,  79 ],\n        [ 100,  50,  86 ],\n        [  55,  96,  84 ],\n        [  92,  74,  77 ],\n        [  97,  50,  73 ],\n])\n```\n\n\u3067\u3059\u3002\u5b9f\u884c\u3057\u3066\u307f\u308b\u3068\u3001\n\n```\n[0 0 0 1 1 2 1 0 2 1 0 2 2 0 0 1 1 0 1 0 2 0 2 0 2 2 1 1 2 1 1 2 2 2 1 2 2]\nPearsonr k-means:11.0138161182\n```\n\n\u3068\u3057\u3063\u304b\u308a\u52d5\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\uff01\u30e4\u30c3\u30bf\u30cd\uff01\n\n\u3061\u306a\u307f\u306b\u3001\u901a\u5e38\u306ek-means\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\n\n```\n[1 1 0 1 1 2 1 1 2 1 1 1 2 1 1 0 2 1 0 0 2 0 2 0 1 1 1 0 2 2 2 2 2 2 0 2 2]\n========================================\nNormal k-means:0.016499042511\n```\n\n\u3067\u3059\u3002\uff10\u3068\uff11\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u304c\u9055\u3046\u30c7\u30fc\u30bf\u304c\u3044\u3064\u304f\u304b\u78ba\u8a8d\u3067\u304d\u307e\u3059\u306d\u3002\n\n\u5b9f\u884c\u6642\u9593\u306f\u30fb\u30fb\u30fb\u6b8b\u5ff5\u306a\u304c\u3089\u3001\uff16\uff10\uff10\u500d\u8fd1\u304f\u306e\u5dee\u304c\u3067\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u30fb\u30fb\u30fb\n\n\u547c\u3073\u51fa\u3059\u95a2\u6570\u304c\u5897\u3048\u3066\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u4ed5\u65b9\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u3042\u305f\u308a\u3001\u5b9f\u88c5\u3092\u5de5\u592b\u3059\u308c\u3070\u3001\u3082\u3046\u5c11\u3057\u65e9\u304f\u306a\u308b\u53ef\u80fd\u6027\u306f\u5341\u5206\u306b\u3042\u308a\u305d\u3046\u3067\u3059\u3002\uff08\u65e9\u304f\u3057\u3066\u304f\u308c\u308b\u30a8\u30ed\u3044\u4eba\u3002\u52df\u96c6\uff09\n\n\u7279\u306b\u6587\u66f8\u5206\u985e\u306b\u4f7f\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u5dee\u5206\u306f\u898b\u3066\u307e\u305b\u3093\u304c\u3001\u8ad6\u6587\u3067\u5b9f\u8a3c\u306f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u30d4\u30a2\u30bd\u30f3\u306e\u76f8\u95a2\u4fc2\u6570\u306f\u304d\u3063\u3068\u826f\u3044\u7d50\u679c\u3092\u51fa\u3057\u3066\u304f\u308c\u308b\uff08\u306f\u305a\u30fb\u30fb\uff09\n\n\n# \u307e\u3068\u3081\n\n* k-means\u306e\u8ddd\u96e2\u95a2\u6570\u306f\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u306e\u5927\u6b63\u7fa9\u3067\u306f\u306a\u3044\n* scikit-learn\u3067\u4ed6\u306e\u8ddd\u96e2\u95a2\u6570\u4f7f\u3046\u306e\u306f\u5c11\u3057\u9aa8\u304c\u6298\u308c\u308b\n* \u9045\u3044\n\n- - -\n\n#### \u3061\u306a\u307f\u306b\u30fb\u30fb\u30fb\n\n\u30ab\u30eb\u30d0\u30c3\u30af\u30fb\u30e9\u30a4\u30d6\u30ca\u30fc\u8ddd\u96e2\u3082\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u305f\u3060\u3001\u6b8b\u5ff5\u306a\u304c\u3089\u3001\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u6b62\u307e\u308a\u307e\u3059\u3002\n\nX, Y\u306e\u8ddd\u96e2\u3092\u8a08\u6e2c\u3059\u308b\u3068\u3001\u3069\u3046\u3057\u3066\u3082\u30de\u30a4\u30ca\u30b9\u8981\u7d20\u304c\u767a\u751f\u3057\u3066\u3057\u307e\u3044\u3001\u8ddd\u96e2\u304cinf\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u305f\u3081\u3001\u305d\u3082\u305d\u3082\u521d\u671f\u5316\u3059\u3089\u3067\u304d\u307e\u305b\u3093\u3002\n\n\u30de\u30a4\u30ca\u30b9\u8981\u7d20\u3092\u3069\u3046\u306b\u304b\u3059\u308c\u3070\u3044\u3044\u3093\u3067\u3057\u3087\u3046\u3051\u3069\u3001\u3069\u3046\u3057\u305f\u3089\u3044\u3044\u3093\u3060\u308d\u3046\u3002\u7279\u306b\u8ad6\u6587\u3067\u3082\u89e6\u308c\u3066\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\n```\nfrom sklearn.preprocessing import normalize\ndef averaged_kullback_leibler(X, Y):\n    #norm_X = normalize(X=X, norm='l1')[0]\n    #norm_Y = normalize(X=Y, norm='l1')[0]\n    norm_X = X\n    norm_Y = Y\n\n    pie_1 = norm_X / (norm_X + norm_Y)\n    pie_2 = norm_Y / (norm_X + norm_Y)\n    M = (pie_1 * norm_X) + (pie_2 * norm_Y)\n\n    KLD_averaged = (pie_1 * scipy.stats.entropy(pk=norm_X, qk=M)) + (pie_2 * scipy.stats.entropy(pk=norm_Y, qk=M))\n\n    return KLD_averaged\n\ndef sub_KL_divergence(X, row_index_1, row_index_2):\n    #kl_distance = scipy.stats.entropy(pk=X[row_index_1], qk=X[row_index_2])\n    kl_distance = averaged_kullback_leibler(X[row_index_1], X[row_index_2])\n    return (row_index_1, row_index_2, kl_distance)\n\ndef copy_inverse_index(row_col_data_tuple):\n    return (row_col_data_tuple[1], row_col_data_tuple[0], row_col_data_tuple[2])\n\ndef KLD_distances(X, Y=None):\n    if Y==None:\n        # X\u3060\u3051\u304c\u5165\u529b\u3055\u308c\u3066\u3044\u3066\u3001X\u304c2d-array\u306e\u5834\u5408\n        row_combinations = list(combinations(range(0, len(X)), 2))\n        kl_divergence_set = [sub_KL_divergence(X, index_set_tuple[0], index_set_tuple[1]) for index_set_tuple in row_combinations]\n        matrix_source_data = kl_divergence_set + map(copy_inverse_index, kl_divergence_set)\n\n        row = [t[0] for t in matrix_source_data]\n        col = [t[1] for t in matrix_source_data]\n        data = [t[2] for t in matrix_source_data]\n\n        kl_distance_matrix = scipy.sparse.csr_matrix((data, (row, col)), (X.shape[0], X.shape[0]))\n        return kl_distance_matrix.toarray()\n    elif len(X.shape)==1 and len(Y.shape)==2:\n        # X\u304c\uff11\u30b5\u30f3\u30d7\u30eb\u3067Y\u304c\u8907\u6570\u30b5\u30f3\u30d7\u30eb\u306e\u5834\u5408\n        # return\u306f1 sample\u306earray\n        kl_divergence_set = numpy.array([averaged_kullback_leibler(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n\n        return kl_divergence_set\n    elif len(X.shape)==2 and len(Y.shape)==2:\n        # 1\u884c\u306b\u306f\u3001X[i]\u3068Y[j] in n(Y)\u306e\u8ddd\u96e2\u3092\u8a18\u9332\u3057\u3066\u8fd4\u3059\n        # X[i]\u3068Y\u306e\u3059\u3079\u3066\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n        for x_i in range(0, X.shape[0]):\n            xx = X[x_i]\n            for y_sample_index in range(0, Y.shape[0]):\n                #print(xx)\n                #print(Y[y_sample_index])\n                print(averaged_kullback_leibler(xx, Y[y_sample_index]))\n\n        kld_x_and_all_y = lambda X, Y: numpy.array([averaged_kullback_leibler(X, Y[y_sample_index]) for y_sample_index in range(0, Y.shape[0])])\n        kl_divergence_set = numpy.array([kld_x_and_all_y(X[x_i], Y) for x_i in range(0, X.shape[0])])\n        return kl_divergence_set\n    else:\n        raise Exception(\"Exception case caused\")\n```\n", "tags": ["Python", "scikit-learn", "kmeans", "MachineLearning"]}