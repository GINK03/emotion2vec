{"context": "\u3053\u306e\u8fba\u306e\u60c5\u5831\u304c\u306a\u3055\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u3002\nhttp://shop.oreilly.com/product/9781784399788.do\n\u552f\u4e00\u306eScrapy\u672c\u306e\u300cLearning Scrapy\u300d\u672c\u306b\u3088\u308b\u3068\u3001\nDB\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u611f\u3058\u306b\u3059\u308b\u306e\u304c\u3088\u3044\u3089\u3057\u3044\u3002\n\u901a\u5e38\u306e\u540c\u671f\u7684\u306b\u66f8\u304f\u3068\u30d6\u30ed\u30c3\u30ad\u30f3\u30b0\u3055\u308c\u308b\u306e\u3067\u3001\u975e\u540c\u671f\u3067\u66f8\u304f\u3002\ntwisted\u306bDB\u30d7\u30fc\u30ea\u30f3\u30b0\u306e\u4ed5\u7d44\u307f\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u4f7f\u3046(DBAPI2\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306a\u3089\u3069\u306eDB\u3067\u3082\u3088\u3044\uff09\nimport logging\nfrom twisted.enterprise import adbapi\nfrom twisted.internet import reactor, defer\n\nclass DatabaseWriterPipeline(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        return cls(\n            settings.get('PIPELINE_DATABASE_ENGINE'),\n            settings.get('PIPELINE_DATABASE_HOST'),\n            settings.getint('PIPELINE_DATABASE_PORT'),\n            settings.get('PIPELINE_DATABASE_USER'),\n            settings.get('PIPELINE_DATABASE_PASS'),\n            settings.get('PIPELINE_DATABASE_DB'),\n        )\n\n    def __init__(self, driver, host, port, user, passwd, db):\n        self.dbpool = adbapi.ConnectionPool(driver,\n            charset='utf8',\n            use_unicode=True,\n            connect_timeout=60,\n            host=host,\n            port=port,\n            user=user,\n            passwd=passwd,\n            db=db,\n            cp_min=3,\n            cp_max=10,\n            cp_reconnect=True,\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def close_spider(self, spider):\n        self.dbpool.close()\n\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        yield self.dbpool.runInteraction(self.do_replace, item)\n        defer.returnValue(item)\n\n    @staticmethod\n    def do_replace(tx, item):\n        # \u304a\u597d\u304d\u306aitem\u306e\u8a2d\u8a08\u3068\u30c6\u30fc\u30d6\u30eb\u306b\u5408\u308f\u305b\u3066\n        sql = \"INSERT INTO text_data (url, title, content) VALUE (%s, %s, %s)\"\n        args = (\n            item[\"url\"][:2048],\n            item[\"title\"][:512],\n            item[\"content\"][:65535],\n        )\n        tx.execute(sql, args)\n\n\n\u66f8\u304d\u8fbc\u3080\u30c6\u30fc\u30d6\u30eb\u306f\u4f5c\u3063\u3066\u304a\u3044\u3066\u3001settings.py\u306b\u3053\u3093\u306a\u611f\u3058\u3067\u66f8\u304f\u3002\nDATABASE_ENGINE = 'MySQLdb'\nDATABASE_HOST = 'localhost'\nDATABASE_PORT = 3306\nDATABASE_DB = 'crawl'\nDATABASE_USER = 'root'\nDATABASE_PASS = ''\n\nITEM_PIPELINES = {\n    'extensions.pipelines.DatabaseWriterPipeline': 100, # \u4e0a\u8a18\u306e\u30af\u30e9\u30b9\u3092\u6307\u5b9a\u3059\u308b\n}\n\nDB\u304c\u8a70\u307e\u308b\u3053\u3068\u306a\u304f\u9ad8\u901f\u306b\u52d5\u304f\uff08\u3082\u3068\u3082\u3068\u30af\u30ed\u30fc\u30eb\u5074\u306e\u8ca0\u8377\u306b\u6bd4\u3079\u3066DB\u306e\u8ca0\u8377\u306f\u306f\u308b\u304b\u306b\u5c11\u306a\u3044\uff09\n\u3053\u306e\u8fba\u306e\u60c5\u5831\u304c\u306a\u3055\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u3002\n\nhttp://shop.oreilly.com/product/9781784399788.do\n\u552f\u4e00\u306eScrapy\u672c\u306e\u300cLearning Scrapy\u300d\u672c\u306b\u3088\u308b\u3068\u3001\nDB\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u611f\u3058\u306b\u3059\u308b\u306e\u304c\u3088\u3044\u3089\u3057\u3044\u3002\n\n\u901a\u5e38\u306e\u540c\u671f\u7684\u306b\u66f8\u304f\u3068\u30d6\u30ed\u30c3\u30ad\u30f3\u30b0\u3055\u308c\u308b\u306e\u3067\u3001\u975e\u540c\u671f\u3067\u66f8\u304f\u3002\ntwisted\u306bDB\u30d7\u30fc\u30ea\u30f3\u30b0\u306e\u4ed5\u7d44\u307f\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u4f7f\u3046(DBAPI2\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u306a\u3089\u3069\u306eDB\u3067\u3082\u3088\u3044\uff09\n\n```py\nimport logging\nfrom twisted.enterprise import adbapi\nfrom twisted.internet import reactor, defer\n\nclass DatabaseWriterPipeline(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        return cls(\n            settings.get('PIPELINE_DATABASE_ENGINE'),\n            settings.get('PIPELINE_DATABASE_HOST'),\n            settings.getint('PIPELINE_DATABASE_PORT'),\n            settings.get('PIPELINE_DATABASE_USER'),\n            settings.get('PIPELINE_DATABASE_PASS'),\n            settings.get('PIPELINE_DATABASE_DB'),\n        )\n\n    def __init__(self, driver, host, port, user, passwd, db):\n        self.dbpool = adbapi.ConnectionPool(driver,\n            charset='utf8',\n            use_unicode=True,\n            connect_timeout=60,\n            host=host,\n            port=port,\n            user=user,\n            passwd=passwd,\n            db=db,\n            cp_min=3,\n            cp_max=10,\n            cp_reconnect=True,\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def close_spider(self, spider):\n        self.dbpool.close()\n\n    @defer.inlineCallbacks\n    def process_item(self, item, spider):\n        yield self.dbpool.runInteraction(self.do_replace, item)\n        defer.returnValue(item)\n\n    @staticmethod\n    def do_replace(tx, item):\n        # \u304a\u597d\u304d\u306aitem\u306e\u8a2d\u8a08\u3068\u30c6\u30fc\u30d6\u30eb\u306b\u5408\u308f\u305b\u3066\n        sql = \"INSERT INTO text_data (url, title, content) VALUE (%s, %s, %s)\"\n        args = (\n            item[\"url\"][:2048],\n            item[\"title\"][:512],\n            item[\"content\"][:65535],\n        )\n        tx.execute(sql, args)\n\n```\n\n\u66f8\u304d\u8fbc\u3080\u30c6\u30fc\u30d6\u30eb\u306f\u4f5c\u3063\u3066\u304a\u3044\u3066\u3001settings.py\u306b\u3053\u3093\u306a\u611f\u3058\u3067\u66f8\u304f\u3002\n\n```py\nDATABASE_ENGINE = 'MySQLdb'\nDATABASE_HOST = 'localhost'\nDATABASE_PORT = 3306\nDATABASE_DB = 'crawl'\nDATABASE_USER = 'root'\nDATABASE_PASS = ''\n\nITEM_PIPELINES = {\n    'extensions.pipelines.DatabaseWriterPipeline': 100, # \u4e0a\u8a18\u306e\u30af\u30e9\u30b9\u3092\u6307\u5b9a\u3059\u308b\n}\n```\n\nDB\u304c\u8a70\u307e\u308b\u3053\u3068\u306a\u304f\u9ad8\u901f\u306b\u52d5\u304f\uff08\u3082\u3068\u3082\u3068\u30af\u30ed\u30fc\u30eb\u5074\u306e\u8ca0\u8377\u306b\u6bd4\u3079\u3066DB\u306e\u8ca0\u8377\u306f\u306f\u308b\u304b\u306b\u5c11\u306a\u3044\uff09\n\n\n", "tags": ["Scrapy", "scraping", "\u30af\u30ed\u30fc\u30e9\u30fc", "Python"]}