{"context": " More than 1 year has passed since last update.\n\n\u306f\u3058\u3081\u306b\n\u3053\u3093\u306b\u3061\u306f\uff0e\u5148\u65e5\uff0c\n\n\u300c10 Deep Learning Trends at NIPS 2015\u300d\u306e\u65e5\u672c\u8a9e\u5316\uff08\uff1f\uff09| \u5099\u5fd8\u9332\n\n\u3068\u3044\u3046\u8a18\u4e8b\u3092\u8aad\u3093\u3067\n\n\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u4f7f\u3063\u3066\u306a\u3044\u306a\u3089\u4eba\u751f\u640d\u3057\u3066\u308b\u3067\nIf you aren't using batch normalization you should\n\n\u3068\u3042\u3063\u305f\u306e\u3067\uff0cTheano\u306b\u3088\u308bBatch Normalization\u306e\u5b9f\u88c5\u3068\u691c\u8a3c(?)\u3092\u884c\u3063\u3066\u307f\u307e\u3057\u305f\uff0e\n\n[Survey]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n\n\u3092\u4e00\u90e8\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\uff0e\n\nBatch Normalization\n\n\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n\u30d0\u30c3\u30c1\u3054\u3068\u306b\u5e73\u5747\u304c0\uff0c\u5206\u6563\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u884c\u3044\u307e\u3059\uff0e\nBBB\u3092mini-batch\u306e\u3068\u3042\u308b\u5165\u529b\u306e\u96c6\u5408\uff0cmmm\u3092batch size\u3068\u3059\u308b\u3068\uff0c\nB = \\{x_{1...m}\\}\\\\\nB={x1...m}B={x1...m}{B = \\{x_{1...m}\\}\\\\\n}\n\u4ee5\u4e0b\u3067\uff0c\u03f5\u03f5\\epsilon\u306f\u5b89\u5b9a\u5316\u306e\u305f\u3081\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3060\u305d\u3046\u3067\u3059\uff0e\n\\epsilon = 10^{-5}\\\\\n\\mu_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_i\\\\\n\\sigma^2_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{B})^2\\\\\n\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma^2_{B} + \\epsilon}}\\\\\ny_i \\leftarrow \\gamma \\hat{x_i} + \\beta\n\u03f5=10\u22125\u03bcB\u21901mm\u2211i=1xi\u03c32B\u21901mm\u2211i=1(xi\u2212\u03bcB)2^xi\u2190xi\u2212\u03bcB\u221a\u03c32B+\u03f5yi\u2190\u03b3^xi+\u03b2\u03f5=10\u22125\u03bcB\u21901m\u2211i=1mxi\u03c32B\u21901m\u2211i=1m(xi\u2212\u03bcB)2xi^\u2190xi\u2212\u03bcB\u03c32B+\u03f5\u2212\u2212\u2212\u2212\u2212\u221ayi\u2190\u03b3xi^+\u03b2{\\epsilon = 10^{-5}\\\\\n\\mu_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_i\\\\\n\\sigma^2_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{B})^2\\\\\n\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma^2_{B} + \\epsilon}}\\\\\ny_i \\leftarrow \\gamma \\hat{x_i} + \\beta\n}\n\u4e0a\u5f0f\u306b\u3064\u3044\u3066\uff0c\u03b3\u03b3\\gamma\u3068\u03b2\u03b2\\beta\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u305d\u308c\u305e\u308c\u6b63\u898f\u5316\u3055\u308c\u305f\u5024\u3092Scaling\u53ca\u3073Shift\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3060\u305d\u3046\u3067\u3059\uff0e\u305d\u308c\u305e\u308c\uff0c\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u3067\u5b66\u7fd2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3059\u304c\uff0c\u3053\u3053\u3067\u306f\u8a73\u3057\u3044\u5f0f\u306e\u5c0e\u51fa\u3092\u5272\u611b\u3057\u307e\u3059\uff0e  \n\nFully-Connected Layer\u306e\u5834\u5408\n\u901a\u5e38\u306eFully-Connected Layer\u3067\u306f\u5165\u529b\u6b21\u5143\u5206\u3060\u3051\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u5165\u529b\u306eshape\u304c(BacthSize, 784)\u306e\u5834\u5408\uff0c784\u500b\u306e\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\nConvolutional Layer\u306e\u5834\u5408\n\u4e00\u65b9\uff0cConvolutional Layer\u3067\u306f\uff0cchannel\u6570\u3076\u3093\u3060\u3051\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u5165\u529b\u306eshape\u304c(BatchSize, 64(channel\u6570), 32, 32)\u306e\u5834\u5408\uff0c64\u500b\u306e\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\n\u30e1\u30ea\u30c3\u30c8\nBatch Normalization\u306e\u30e1\u30ea\u30c3\u30c8\u3068\u3057\u3066\u306f\uff0c\u5927\u304d\u306a\u5b66\u7fd2\u4fc2\u6570\u3092\u8a2d\u5b9a\u3067\u304d\uff0c\u5b66\u7fd2\u3092\u52a0\u901f\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u3067\u3059\uff0e\n\nTheano\u306b\u3088\u308b\u5b9f\u88c5\nclass BatchNormalizationLayer(object):\n    def __init__(self, input, shape=None):\n        self.shape = shape\n        if len(shape) == 2: # for fully connnected\n            gamma = theano.shared(value=np.ones(shape[1], dtype=theano.config.floatX), name=\"gamma\", borrow=True)\n            beta = theano.shared(value=np.zeros(shape[1], dtype=theano.config.floatX), name=\"beta\", borrow=True)\n            mean = input.mean((0,), keepdims=True)\n            var = input.var((0,), keepdims=True)\n        elif len(shape) == 4: # for cnn\n            gamma = theano.shared(value=np.ones(shape[1:], dtype=theano.config.floatX), name=\"gamma\", borrow=True)\n            beta = theano.shared(value=np.zeros(shape[1:], dtype=theano.config.floatX), name=\"beta\", borrow=True)\n            mean = input.mean((0,2,3), keepdims=True)\n            var = input.var((0,2,3), keepdims=True)\n            mean = self.change_shape(mean)\n            var = self.change_shape(var)\n\n        self.params = [gamma, beta]\n        self.output = gamma * (input - mean) / T.sqrt(var + 1e-5) + beta\n\n    def change_shape(self, vec):\n        ret = T.repeat(vec, self.shape[2]*self.shape[3])\n        ret = ret.reshape(self.shape[1:])\n        return ret\n\n\u4f7f\u3044\u65b9\u306e\u4f8b(\u307b\u3068\u3093\u3069\u64ec\u4f3c\u30b3\u30fc\u30c9)\u306f\uff0c\n...\ninput = previous_layer.output #\u30b7\u30f3\u30dc\u30eb\u5909\u6570\uff0c\u524d\u306e\u5c64\u306e\u51fa\u529b\uff0cshape=(batchsize, 784)\nh = BatchNormalizationLayer(input, shape=(batchsize, 784))\n# activation\u3059\u308b\u5834\u5408\nh.output = activation(h.output) # activation=\u4f55\u3089\u304b\u306e\u6d3b\u6027\u5316\u95a2\u6570\n...\nparams = ... + h.params + ... # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff0c\u66f4\u65b0\u306e\u6642\u306b\u4f7f\u3046\uff0e\n\n\n\u5b9f\u9a13\n\n\u5b9f\u9a13\u8a2d\u5b9a\n\u30c7\u30fc\u30bf\u306fMNIST\u3092\u4f7f\u3044\uff0c\u5358\u7d14\u306a\u591a\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b9f\u9a13\u3057\u307e\u3057\u305f\uff0e\n\n\u4e2d\u9593\u5c64\u306e\u6570\uff1a10\n\u4e2d\u9593\u5c64\u306e\u5404\u30e6\u30cb\u30c3\u30c8\u6570\uff1a\u5168\u90e8784\n\u6700\u9069\u5316\u624b\u6cd5\uff1a\u30b7\u30f3\u30d7\u30eb\u306aSGD(\u5b66\u7fd2\u4fc2\u6570\uff1a0.01)\n\u6d3b\u6027\u5316\u95a2\u6570\uff1atanh\nDropout Ratio\uff1a\u4e2d\u9593\u5c641\u5c64\u76ee\u306f0.1\uff0c\u305d\u308c\u3068\u5165\u51fa\u529b\u5c64\u4ee5\u5916\u306f0.5\nBatch Size\uff1a100\n\u8aa4\u5dee\u95a2\u6570\uff1aNegative Log Likelihood\n\n\u307e\u3041\n\u5165\u529b\u5c64\u2192(Fully-Connected Layer\u2192Batch Normalization Layer\u2192Activation)*10\u2192\u51fa\u529b\u5c64\n\u307f\u305f\u3044\u306a\u611f\u3058\u3067\u3059\uff0e\n\n\u5b9f\u9a13\u7d50\u679c\n\n\u8aa4\u5dee\u95a2\u6570\u5024\n\n\n\u5206\u985e\u7cbe\u5ea6\n\n\n\n\n\u6700\u5f8c\u306b\n\u5b9f\u9a13\u8a2d\u5b9a\u306b\u3061\u3087\u3063\u3068\u7121\u7406\u304c\u3042\u3063\u305f\u304b\u3082\u3057\u308c\u306a\u3044\u3067\u3059\u3051\u3069\uff0cBatch Normalization\u4f7f\u308f\u306a\u3044\u3068\u640d\u3063\u3066\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n# \u306f\u3058\u3081\u306b\n\u3053\u3093\u306b\u3061\u306f\uff0e\u5148\u65e5\uff0c\n\n- [\u300c10 Deep Learning Trends at NIPS 2015\u300d\u306e\u65e5\u672c\u8a9e\u5316\uff08\uff1f\uff09| \u5099\u5fd8\u9332](http://tam5917.hatenablog.com/entry/2016/03/04/192535)\n\n\u3068\u3044\u3046\u8a18\u4e8b\u3092\u8aad\u3093\u3067\n>\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u4f7f\u3063\u3066\u306a\u3044\u306a\u3089\u4eba\u751f\u640d\u3057\u3066\u308b\u3067\n>If you aren't using batch normalization you should\n\n\u3068\u3042\u3063\u305f\u306e\u3067\uff0cTheano\u306b\u3088\u308bBatch Normalization\u306e\u5b9f\u88c5\u3068\u691c\u8a3c(?)\u3092\u884c\u3063\u3066\u307f\u307e\u3057\u305f\uff0e\n\n- [[Survey]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://qiita.com/supersaiakujin/items/8a465ecb1dcbc7df8b02)\n\n\u3092\u4e00\u90e8\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\uff0e\n\n# Batch Normalization\n## \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n\u30d0\u30c3\u30c1\u3054\u3068\u306b\u5e73\u5747\u304c0\uff0c\u5206\u6563\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u884c\u3044\u307e\u3059\uff0e\n$B$\u3092mini-batch\u306e\u3068\u3042\u308b\u5165\u529b\u306e\u96c6\u5408\uff0c$m$\u3092batch size\u3068\u3059\u308b\u3068\uff0c\n\n```math\nB = \\{x_{1...m}\\}\\\\\n```\n\u4ee5\u4e0b\u3067\uff0c$\\epsilon$\u306f\u5b89\u5b9a\u5316\u306e\u305f\u3081\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3060\u305d\u3046\u3067\u3059\uff0e\n\n```math\n\\epsilon = 10^{-5}\\\\\n\\mu_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_i\\\\\n\\sigma^2_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{B})^2\\\\\n\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_{B}}{\\sqrt{\\sigma^2_{B} + \\epsilon}}\\\\\ny_i \\leftarrow \\gamma \\hat{x_i} + \\beta\n```\n\n\u4e0a\u5f0f\u306b\u3064\u3044\u3066\uff0c$\\gamma$\u3068$\\beta$\u304c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u305d\u308c\u305e\u308c\u6b63\u898f\u5316\u3055\u308c\u305f\u5024\u3092Scaling\u53ca\u3073Shift\u3059\u308b\u305f\u3081\u306e\u3082\u306e\u3060\u305d\u3046\u3067\u3059\uff0e\u305d\u308c\u305e\u308c\uff0c\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u3067\u5b66\u7fd2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3059\u304c\uff0c\u3053\u3053\u3067\u306f\u8a73\u3057\u3044\u5f0f\u306e\u5c0e\u51fa\u3092\u5272\u611b\u3057\u307e\u3059\uff0e  \n\n### Fully-Connected Layer\u306e\u5834\u5408\n\u901a\u5e38\u306eFully-Connected Layer\u3067\u306f\u5165\u529b\u6b21\u5143\u5206\u3060\u3051\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u5165\u529b\u306eshape\u304c`(BacthSize, 784)`\u306e\u5834\u5408\uff0c784\u500b\u306e\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n### Convolutional Layer\u306e\u5834\u5408\n\u4e00\u65b9\uff0cConvolutional Layer\u3067\u306f\uff0cchannel\u6570\u3076\u3093\u3060\u3051\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u5165\u529b\u306eshape\u304c`(BatchSize, 64(channel\u6570), 32, 32)`\u306e\u5834\u5408\uff0c64\u500b\u306e\u5e73\u5747\uff0c\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\n## \u30e1\u30ea\u30c3\u30c8\nBatch Normalization\u306e\u30e1\u30ea\u30c3\u30c8\u3068\u3057\u3066\u306f\uff0c\u5927\u304d\u306a\u5b66\u7fd2\u4fc2\u6570\u3092\u8a2d\u5b9a\u3067\u304d\uff0c\u5b66\u7fd2\u3092\u52a0\u901f\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u3067\u3059\uff0e\n\n## Theano\u306b\u3088\u308b\u5b9f\u88c5\n\n```py\nclass BatchNormalizationLayer(object):\n\tdef __init__(self, input, shape=None):\n\t\tself.shape = shape\n\t\tif len(shape) == 2: # for fully connnected\n\t\t\tgamma = theano.shared(value=np.ones(shape[1], dtype=theano.config.floatX), name=\"gamma\", borrow=True)\n\t\t\tbeta = theano.shared(value=np.zeros(shape[1], dtype=theano.config.floatX), name=\"beta\", borrow=True)\n\t\t\tmean = input.mean((0,), keepdims=True)\n\t\t\tvar = input.var((0,), keepdims=True)\n\t\telif len(shape) == 4: # for cnn\n\t\t\tgamma = theano.shared(value=np.ones(shape[1:], dtype=theano.config.floatX), name=\"gamma\", borrow=True)\n\t\t\tbeta = theano.shared(value=np.zeros(shape[1:], dtype=theano.config.floatX), name=\"beta\", borrow=True)\n\t\t\tmean = input.mean((0,2,3), keepdims=True)\n\t\t\tvar = input.var((0,2,3), keepdims=True)\n\t\t\tmean = self.change_shape(mean)\n\t\t\tvar = self.change_shape(var)\n\n\t\tself.params = [gamma, beta]\n\t\tself.output = gamma * (input - mean) / T.sqrt(var + 1e-5) + beta\n\t\n\tdef change_shape(self, vec):\n\t\tret = T.repeat(vec, self.shape[2]*self.shape[3])\n\t\tret = ret.reshape(self.shape[1:])\n\t\treturn ret\n```\n\u4f7f\u3044\u65b9\u306e\u4f8b(\u307b\u3068\u3093\u3069\u64ec\u4f3c\u30b3\u30fc\u30c9)\u306f\uff0c\n\n```py\n...\ninput = previous_layer.output #\u30b7\u30f3\u30dc\u30eb\u5909\u6570\uff0c\u524d\u306e\u5c64\u306e\u51fa\u529b\uff0cshape=(batchsize, 784)\nh = BatchNormalizationLayer(input, shape=(batchsize, 784))\n# activation\u3059\u308b\u5834\u5408\nh.output = activation(h.output) # activation=\u4f55\u3089\u304b\u306e\u6d3b\u6027\u5316\u95a2\u6570\n...\nparams = ... + h.params + ... # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\uff0c\u66f4\u65b0\u306e\u6642\u306b\u4f7f\u3046\uff0e\n```\n  \n# \u5b9f\u9a13\n## \u5b9f\u9a13\u8a2d\u5b9a\n\u30c7\u30fc\u30bf\u306fMNIST\u3092\u4f7f\u3044\uff0c\u5358\u7d14\u306a\u591a\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b9f\u9a13\u3057\u307e\u3057\u305f\uff0e\n\n- \u4e2d\u9593\u5c64\u306e\u6570\uff1a10\n- \u4e2d\u9593\u5c64\u306e\u5404\u30e6\u30cb\u30c3\u30c8\u6570\uff1a\u5168\u90e8784\n- \u6700\u9069\u5316\u624b\u6cd5\uff1a\u30b7\u30f3\u30d7\u30eb\u306aSGD(\u5b66\u7fd2\u4fc2\u6570\uff1a0.01)\n- \u6d3b\u6027\u5316\u95a2\u6570\uff1atanh\n- Dropout Ratio\uff1a\u4e2d\u9593\u5c641\u5c64\u76ee\u306f0.1\uff0c\u305d\u308c\u3068\u5165\u51fa\u529b\u5c64\u4ee5\u5916\u306f0.5\n- Batch Size\uff1a100\n- \u8aa4\u5dee\u95a2\u6570\uff1aNegative Log Likelihood\n\n\u307e\u3041\n\u5165\u529b\u5c64\u2192(Fully-Connected Layer\u2192Batch Normalization Layer\u2192Activation)*10\u2192\u51fa\u529b\u5c64\n\u307f\u305f\u3044\u306a\u611f\u3058\u3067\u3059\uff0e\n\n\n## \u5b9f\u9a13\u7d50\u679c\n- \u8aa4\u5dee\u95a2\u6570\u5024  \n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/31899/2cb66cbb-0581-fe69-a043-7794a2103393.png\", width=640>\n- \u5206\u985e\u7cbe\u5ea6  \n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/31899/bc14eede-2acf-591e-80b3-632269b0d19d.png\", width=640>\n\n  \n# \u6700\u5f8c\u306b\n\u5b9f\u9a13\u8a2d\u5b9a\u306b\u3061\u3087\u3063\u3068\u7121\u7406\u304c\u3042\u3063\u305f\u304b\u3082\u3057\u308c\u306a\u3044\u3067\u3059\u3051\u3069\uff0cBatch Normalization\u4f7f\u308f\u306a\u3044\u3068\u640d\u3063\u3066\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n", "tags": ["Python", "\u6df1\u5c64\u5b66\u7fd2", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "Theano"]}