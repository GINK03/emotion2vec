{"context": " More than 1 year has passed since last update.\n\n\u7d4c\u7def\n\u6700\u8fd1Apache Spark\u30921.4\u7cfb\u304b\u30891.5.1\u3078\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u307e\u3057\u305f\u3002\n\u3068\u3053\u308d\u304c\u3001\u30b8\u30e7\u30d6\u3092\u4f5c\u3063\u3066jar\u306b\u3057\u3066spark-submit\u304b\u3089\u5b9f\u884c\u3057\u305f\u3068\u3053\u308d\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u304c\u51fa\u3066fail\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\n15/10/21 15:22:12 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, spark003.example.com): java.lang.IllegalArgumentException: java.net.UnknownHostException: nameservice1\n        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)\n        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312)\n        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:665)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:601)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)\n        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:656)\n        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:436)\n        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:409)\n        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016)\n        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016)\n        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)\n        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)\n        at scala.Option.map(Option.scala:145)\n        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176)\n        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:220)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n        at org.apache.spark.scheduler.Task.run(Task.scala:88)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.UnknownHostException: nameservice1\n        ... 41 more\n\n\nnameservice1 \u3068\u3044\u3046\u306e\u306fHDFS\u306eHA\u30af\u30e9\u30b9\u30bf\u306b\u3064\u3051\u305f\u8ad6\u7406\u30b5\u30fc\u30d3\u30b9\u540d\u3067\u3059\u3002\n(\u79c1\u306e\u74b0\u5883\u3067\u306fHDFS\u306eHighAvailability\u3092\u6709\u52b9\u306b\u3057\u3066\u3044\u307e\u3059\u3002)\n\n\u3084\u3063\u305f\u3053\u3068\n\n\u540c\u3058\u5185\u5bb9\u306e\u30b8\u30e7\u30d6\u3092Spark1.4\u7cfb\u3067\u5b9f\u884c\u3002\n\u2192 \u7279\u306b\u554f\u984c\u306a\u304f\u30b8\u30e7\u30d6\u304c\u5b8c\u4e86\u3002\n(\u30b8\u30e7\u30d6\u4f5c\u6210\u306e\u969b\u3001Spark\u3078\u306e\u4f9d\u5b58\u306f1.5.1\u304b\u30891.4.1\u3078\u5909\u66f4\u3057\u3066\u3044\u307e\u3059)\n\u30b8\u30e7\u30d6\u3068\u540c\u3058\u5185\u5bb9\u3092spark-shell \u304b\u3089\u5b9f\u884c\n\u2192 \u554f\u984c\u306a\u304f\u5b9f\u884c\u53ef\u80fd\u3002\nHDFS HA\u306e\u30b5\u30fc\u30d3\u30b9\u540d\u304c\u89e3\u6c7a\u3067\u304d\u3066\u3044\u306a\u3044\u3088\u3046\u3060\u3063\u305f\u306e\u3067HDFS HA\u3092\u7121\u52b9\u306b\u3002\n\u2192 \u7279\u306b\u554f\u984c\u306a\u304f\u30b8\u30e7\u30d6\u304c\u5b8c\u4e86\u3002\nSpark\u306eJIRA\u3078issue\u3092\u4e0a\u3052\u308b\u3002\nhttps://issues.apache.org/jira/browse/SPARK-11227\n\u2192 HDFS \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u898b\u76f4\u305b\u3068\u3002\u89e3\u6c7a\u6e08\u307f\u306b\u3055\u308c\u3066\u3057\u307e\u3044\u307e\u3057\u305f \n\n\n\u25c7 \u5b9f\u884c\u30b3\u30de\u30f3\u30c9\n/opt/spark/bin/spark-submit \\\n  --class com.example.Job /jobs/job-assembly-1.0.0.jar\n\n\n\u25c7 \u30b8\u30e7\u30d6\u306e\u5185\u5bb9\nimport org.apache.spark.sql.{SaveMode, SQLContext}\n\nobject Job {\n  val sparkConfig = ...\n\n  def main( args: Array[String] ): Unit = {\n    val sc = new SparkContext( sparkConfig )\n    implicit val sqlContext = new SQLContext( sc )\n    import sqlContext.implicits._\n\n    val df = sqlContext.read.format( \"com.databricks.spark.csv\" ).option( \"header\", \"true\" ).load( input )\n    df.write.format( \"json\" ).mode( SaveMode.Overwrite ).save( output )\n  }\n}\n\nCSV\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u3093\u3067\u66f8\u304d\u51fa\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u3059  \n\n\u25c7 \u74b0\u5883\n\u5b9f\u884c\u74b0\u5883\u306f\u4e0b\u8a18\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\nOS   \u30fb\u30fb\u30fb CentOS6.6\nHDFS \u30fb\u30fb\u30fb CDH5.4.0(ClouderaManager\u3067\u69cb\u7bc9)\nCluster\u5316 \u30fb\u30fb\u30fb Zookeeper + Mesos 0.22.0\n\n\n\u7d50\u8ad6\nspark-shell\u3068\u306e\u5dee\u7570\u3092\u63a2\u3057\u305f\u3068\u3053\u308d\u3001 HiveContext \u3092\u4f7f\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306e\u9055\u3044\u304c\u3042\u308a\u307e\u3057\u305f\u306e\u3067\u3001\u8a66\u3057\u306bspark-shell\u3067 implicit val sqlContext = new SQLContext( sc ) \u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u3059\u308b\u3068\u540c\u3058\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u306e\u3067\u3001\u307e\u3042\u3053\u308c\u304b\u306a\u3068\u3002\n\u30b8\u30e7\u30d6\u5185\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u306e SQLContext \u3067\u306f\u306a\u304f HiveContext \u3092\u4f7f\u3046\u3088\u3046\u306b\u3057\u305f\u3089\u30a8\u30e9\u30fc\u306f\u89e3\u6d88\u3057\u307e\u3057\u305f  \n\n\u25c7 \u5909\u66f4\u5f8c\u306e\u30b8\u30e7\u30d6\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.hive.HiveContext\n\nobject Job {\n  val sparkConfig = ...\n\n  def main( args: Array[String] ): Unit = {\n    val sc = new SparkContext( sparkConfig )\n    implicit val sqlContext = new HiveContext( sc )\n    import sqlContext.implicits._\n\n    val df = sqlContext.read.format( \"com.databricks.spark.csv\" ).option( \"header\", \"true\" ).load( input )\n    df.write.format( \"json\" ).mode( SaveMode.Overwrite ).save( output )\n  }\n}\n\nSQLContext \u306b\u30d0\u30b0\u304c\u3042\u308b\u3068\u304a\u3082\u3046\u3093\u3060\u3051\u3069\u3002\u3002\u3002  \n## \u7d4c\u7def\n\n\u6700\u8fd1Apache Spark\u3092`1.4\u7cfb`\u304b\u3089`1.5.1`\u3078\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u307e\u3057\u305f\u3002\n\u3068\u3053\u308d\u304c\u3001\u30b8\u30e7\u30d6\u3092\u4f5c\u3063\u3066jar\u306b\u3057\u3066`spark-submit`\u304b\u3089\u5b9f\u884c\u3057\u305f\u3068\u3053\u308d\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u304c\u51fa\u3066fail\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\n\n```\n15/10/21 15:22:12 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, spark003.example.com): java.lang.IllegalArgumentException: java.net.UnknownHostException: nameservice1\n        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374)\n        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312)\n        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:665)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:601)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)\n        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:656)\n        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:436)\n        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:409)\n        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016)\n        at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$32.apply(SparkContext.scala:1016)\n        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)\n        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)\n        at scala.Option.map(Option.scala:145)\n        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176)\n        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:220)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)\n        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n        at org.apache.spark.scheduler.Task.run(Task.scala:88)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.UnknownHostException: nameservice1\n        ... 41 more\n\n```\n\n`nameservice1` \u3068\u3044\u3046\u306e\u306fHDFS\u306eHA\u30af\u30e9\u30b9\u30bf\u306b\u3064\u3051\u305f\u8ad6\u7406\u30b5\u30fc\u30d3\u30b9\u540d\u3067\u3059\u3002\n(\u79c1\u306e\u74b0\u5883\u3067\u306fHDFS\u306e`HighAvailability`\u3092\u6709\u52b9\u306b\u3057\u3066\u3044\u307e\u3059\u3002)\n\n## \u3084\u3063\u305f\u3053\u3068\n\n* \u540c\u3058\u5185\u5bb9\u306e\u30b8\u30e7\u30d6\u3092`Spark1.4\u7cfb`\u3067\u5b9f\u884c\u3002\n\u2192 \u7279\u306b\u554f\u984c\u306a\u304f\u30b8\u30e7\u30d6\u304c\u5b8c\u4e86\u3002\n(\u30b8\u30e7\u30d6\u4f5c\u6210\u306e\u969b\u3001Spark\u3078\u306e\u4f9d\u5b58\u306f`1.5.1`\u304b\u3089`1.4.1`\u3078\u5909\u66f4\u3057\u3066\u3044\u307e\u3059)\n\n* \u30b8\u30e7\u30d6\u3068\u540c\u3058\u5185\u5bb9\u3092`spark-shell` \u304b\u3089\u5b9f\u884c\n\u2192 \u554f\u984c\u306a\u304f\u5b9f\u884c\u53ef\u80fd\u3002\n\n* `HDFS HA`\u306e\u30b5\u30fc\u30d3\u30b9\u540d\u304c\u89e3\u6c7a\u3067\u304d\u3066\u3044\u306a\u3044\u3088\u3046\u3060\u3063\u305f\u306e\u3067`HDFS HA`\u3092\u7121\u52b9\u306b\u3002\n\u2192 \u7279\u306b\u554f\u984c\u306a\u304f\u30b8\u30e7\u30d6\u304c\u5b8c\u4e86\u3002\n\n* Spark\u306eJIRA\u3078issue\u3092\u4e0a\u3052\u308b\u3002\nhttps://issues.apache.org/jira/browse/SPARK-11227\n\u2192 `HDFS` \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u898b\u76f4\u305b\u3068\u3002\u89e3\u6c7a\u6e08\u307f\u306b\u3055\u308c\u3066\u3057\u307e\u3044\u307e\u3057\u305f :sweat_drops:\n\n### \u25c7 \u5b9f\u884c\u30b3\u30de\u30f3\u30c9\n\n```bash\n/opt/spark/bin/spark-submit \\\n  --class com.example.Job /jobs/job-assembly-1.0.0.jar\n```\n### \u25c7 \u30b8\u30e7\u30d6\u306e\u5185\u5bb9\n\n```scala\nimport org.apache.spark.sql.{SaveMode, SQLContext}\n\nobject Job {\n  val sparkConfig = ...\n\n  def main( args: Array[String] ): Unit = {\n    val sc = new SparkContext( sparkConfig )\n    implicit val sqlContext = new SQLContext( sc )\n    import sqlContext.implicits._\n\n    val df = sqlContext.read.format( \"com.databricks.spark.csv\" ).option( \"header\", \"true\" ).load( input )\n    df.write.format( \"json\" ).mode( SaveMode.Overwrite ).save( output )\n  }\n}\n```\n\nCSV\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u3093\u3067\u66f8\u304d\u51fa\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u3059 :cold_sweat: \n\n### \u25c7 \u74b0\u5883\n\n\u5b9f\u884c\u74b0\u5883\u306f\u4e0b\u8a18\u306e\u3068\u304a\u308a\u3067\u3059\u3002\n\n* OS   \u30fb\u30fb\u30fb CentOS6.6\n* HDFS \u30fb\u30fb\u30fb CDH5.4.0(`ClouderaManager`\u3067\u69cb\u7bc9)\n* Cluster\u5316 \u30fb\u30fb\u30fb Zookeeper + Mesos 0.22.0\n\n## \u7d50\u8ad6\n\n`spark-shell`\u3068\u306e\u5dee\u7570\u3092\u63a2\u3057\u305f\u3068\u3053\u308d\u3001 `HiveContext` \u3092\u4f7f\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306e\u9055\u3044\u304c\u3042\u308a\u307e\u3057\u305f\u306e\u3067\u3001\u8a66\u3057\u306b`spark-shell`\u3067 `implicit val sqlContext = new SQLContext( sc )` \u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u3059\u308b\u3068\u540c\u3058\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u306e\u3067\u3001\u307e\u3042\u3053\u308c\u304b\u306a\u3068\u3002\n\n\u30b8\u30e7\u30d6\u5185\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u306e `SQLContext` \u3067\u306f\u306a\u304f `HiveContext` \u3092\u4f7f\u3046\u3088\u3046\u306b\u3057\u305f\u3089\u30a8\u30e9\u30fc\u306f\u89e3\u6d88\u3057\u307e\u3057\u305f :clap: \n\n### \u25c7 \u5909\u66f4\u5f8c\u306e\u30b8\u30e7\u30d6\n\n```scala\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.hive.HiveContext\n\nobject Job {\n  val sparkConfig = ...\n\n  def main( args: Array[String] ): Unit = {\n    val sc = new SparkContext( sparkConfig )\n    implicit val sqlContext = new HiveContext( sc )\n    import sqlContext.implicits._\n\n    val df = sqlContext.read.format( \"com.databricks.spark.csv\" ).option( \"header\", \"true\" ).load( input )\n    df.write.format( \"json\" ).mode( SaveMode.Overwrite ).save( output )\n  }\n}\n```\n\n`SQLContext` \u306b\u30d0\u30b0\u304c\u3042\u308b\u3068\u304a\u3082\u3046\u3093\u3060\u3051\u3069\u3002\u3002\u3002 :sweat_drops: \n", "tags": ["Spark", "ApacheSpark"]}