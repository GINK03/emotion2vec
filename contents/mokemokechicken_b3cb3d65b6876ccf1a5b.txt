{"context": "\n\n\u306f\u3058\u3081\u306b\n\u5148\u65e5\u3001Keras\u3067VAT(Virtual Adversarial Training)\u3092\u4f7f\u3063\u3066MNIST\u3092\u3084\u3063\u3066\u307f\u308b\u3092\u6295\u7a3f\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u3082\u3046\u5c11\u3057\u30de\u30b7\u3063\u307d\u3044\u5b9f\u88c5\u304c\u3067\u304d\u305f\u306e\u3067\u5171\u6709\u3057\u307e\u3059\u3002\n\nVersion\n\nPython: 3.5.3\nKeras: 1.2.2\nTheano: 0.8.2\n\n\n\u5b9f\u88c5\n\u524d\u56de\u304b\u3089\u306e\u9055\u3044\u306f\u3001\n\n\u640d\u5931\u95a2\u6570\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001Model.losses \u306b VAT\u306eLoss\u3092\u4ed8\u3051\u308b\u3088\u3046\u306b\u3057\u3001\u5b66\u7fd2\u6642\u306b\u5999\u306a\u5909\u63db\u3092\u3057\u306a\u304f\u3066\u3088\u304f\u3057\u305f\uff08\u3053\u308c\u304c\u5927\u304d\u3044\u9055\u3044\uff09\n\u901a\u5e38\u306e\u4e88\u6e2c\u5024\u306b K.stop_gradient() \u3092\u4ed8\u3051\u3066\u304a\u308a\u3001VAT\u8a08\u7b97\u304b\u3089\u767a\u751f\u3059\u308b\u4f59\u5206\u306a(?)\u5dee\u5206\u306e\u4f1d\u64ad\u3092\u6b62\u3081\u305f\uff08\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3068\u601d\u3046\uff09(\u3042\u307e\u308a\u7d50\u679c\u306f\u5909\u308f\u3089\u306a\u3044\u3051\u3069...)\n\n\u3068\u3044\u3046\u3068\u3053\u308d\u3067\u3059\u3002\n\nmnist_with_vat_model.py\n# coding: utf8\n\"\"\"\n* VAT: https://arxiv.org/abs/1507.00677\n\n# \u53c2\u8003\u306b\u3057\u305fCode\nOriginal: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\nVAT: https://github.com/musyoku/vat/blob/master/vat.py\n\nresults example\n---------------\n\nfinish: use_dropout=False, use_vat=False: score=0.215942835068, accuracy=0.9872\nfinish: use_dropout=True, use_vat=False: score=0.261140023788, accuracy=0.9845\nfinish: use_dropout=False, use_vat=True: score=0.240192672965, accuracy=0.9894\nfinish: use_dropout=True, use_vat=True: score=0.210011005498, accuracy=0.9891\n\"\"\"\nimport numpy as np\nfrom functools import reduce\nfrom keras.engine.topology import Input, Container, to_list\nfrom keras.engine.training import Model\n\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\nSAMPLE_SIZE = 0\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\npool_size = (2, 2)\n# convolution kernel size\nkernel_size = (3, 3)\n\n\ndef main(data, use_dropout, use_vat):\n    np.random.seed(1337)  # for reproducibility\n\n    # the data, shuffled and split between train and test sets\n    (X_train, y_train), (X_test, y_test) = data\n\n    if K.image_dim_ordering() == 'th':\n        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n        input_shape = (1, img_rows, img_cols)\n    else:\n        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n        input_shape = (img_rows, img_cols, 1)\n\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    X_train /= 255.\n    X_test /= 255.\n\n    # convert class vectors to binary class matrices\n    y_train = np_utils.to_categorical(y_train, nb_classes)\n    y_test = np_utils.to_categorical(y_test, nb_classes)\n\n    if SAMPLE_SIZE:\n        X_train = X_train[:SAMPLE_SIZE]\n        y_train = y_train[:SAMPLE_SIZE]\n        X_test = X_test[:SAMPLE_SIZE]\n        y_test = y_test[:SAMPLE_SIZE]\n\n    print(\"start: use_dropout=%s, use_vat=%s\" % (use_dropout, use_vat))\n    my_model = MyModel(input_shape, use_dropout, use_vat).build()\n    my_model.training(X_train, y_train, X_test, y_test)\n\n    score = my_model.model.evaluate(X_test, y_test, verbose=0)\n    print(\"finish: use_dropout=%s, use_vat=%s: score=%s, accuracy=%s\" % (use_dropout, use_vat, score[0], score[1]))\n\n\nclass MyModel:\n    model = None\n\n    def __init__(self, input_shape, use_dropout=True, use_vat=True):\n        self.input_shape = input_shape\n        self.use_dropout = use_dropout\n        self.use_vat = use_vat\n\n    def build(self):\n        input_layer = Input(self.input_shape)\n        output_layer = self.core_data_flow(input_layer)\n        if self.use_vat:\n            self.model = VATModel(input_layer, output_layer).setup_vat_loss()\n        else:\n            self.model = Model(input_layer, output_layer)\n        return self\n\n    def core_data_flow(self, input_layer):\n        x = Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='valid')(input_layer)\n        x = Activation('relu')(x)\n        x = Convolution2D(nb_filters, kernel_size[0], kernel_size[1])(x)\n        x = Activation('relu')(x)\n        x = MaxPooling2D(pool_size=pool_size)(x)\n        if self.use_dropout:\n            x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(128, activation=\"relu\")(x)\n        if self.use_dropout:\n            x = Dropout(0.5)(x)\n        x = Dense(nb_classes, activation='softmax')(x)\n        return x\n\n    def training(self, X_train, y_train, X_test, y_test):\n        self.model.compile(loss=K.categorical_crossentropy, optimizer='adadelta', metrics=['accuracy'])\n        np.random.seed(1337)  # for reproducibility\n        self.model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n                       verbose=1, validation_data=(X_test, y_test))\n\n\nclass VATModel(Model):\n    _vat_loss = None\n\n    def setup_vat_loss(self, eps=1, xi=10, ip=1):\n        self._vat_loss = self.vat_loss(eps, xi, ip)\n        return self\n\n    @property\n    def losses(self):\n        losses = super(self.__class__, self).losses\n        if self._vat_loss:\n            losses += [self._vat_loss]\n        return losses\n\n    def vat_loss(self, eps, xi, ip):\n        normal_outputs = [K.stop_gradient(x) for x in to_list(self.outputs)]\n        d_list = [K.random_normal(x.shape) for x in self.inputs]\n\n        for _ in range(ip):\n            new_inputs = [x + self.normalize_vector(d)*xi for (x, d) in zip(self.inputs, d_list)]\n            new_outputs = to_list(self.call(new_inputs))\n            klds = [K.sum(self.kld(normal, new)) for normal, new in zip(normal_outputs, new_outputs)]\n            kld = reduce(lambda t, x: t+x, klds, 0)\n            d_list = [K.stop_gradient(d) for d in K.gradients(kld, d_list)]\n\n        new_inputs = [x + self.normalize_vector(d) * eps for (x, d) in zip(self.inputs, d_list)]\n        y_perturbations = to_list(self.call(new_inputs))\n        klds = [K.mean(self.kld(normal, new)) for normal, new in zip(normal_outputs, y_perturbations)]\n        kld = reduce(lambda t, x: t + x, klds, 0)\n        return kld\n\n    @staticmethod\n    def normalize_vector(x):\n        z = K.sum(K.batch_flatten(K.square(x)), axis=1)\n        while K.ndim(z) < K.ndim(x):\n            z = K.expand_dims(z, dim=-1)\n        return x / (K.sqrt(z) + K.epsilon())\n\n    @staticmethod\n    def kld(p, q):\n        v = p * (K.log(p + K.epsilon()) - K.log(q + K.epsilon()))\n        return K.sum(K.batch_flatten(v), axis=1, keepdims=True)\n\n\ndata = mnist.load_data()\nmain(data, use_dropout=False, use_vat=False)\nmain(data, use_dropout=True, use_vat=False)\nmain(data, use_dropout=False, use_vat=True)\nmain(data, use_dropout=True, use_vat=True)\n\n\n\n\u5b9f\u9a13\u7d50\u679c\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b\u5b9f\u9a13\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n\nDropout\nVAT\nAccuracy\n1 epoch\u306e\u6642\u9593\n\n\n\n\n\u4f7f\u308f\u306a\u3044\n\u4f7f\u308f\u306a\u3044\n98.72%\n8\u79d2\n\n\n\u4f7f\u3046\n\u4f7f\u308f\u306a\u3044\n98.45%\n8\u79d2\n\n\n\u4f7f\u308f\u306a\u3044\n\u4f7f\u3046\n98.94%\n18\u79d2\n\n\n\u4f7f\u3046\n\u4f7f\u3046\n98.91%\n18\u79d2\n\n\n\n\u3060\u3044\u305f\u3044\u540c\u3058\u3088\u3046\u306a\u7d50\u679c\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n\u3055\u3044\u3054\u306b\n\u3069\u3063\u3061\u307f\u3061Placeholder\u306b\u5bfe\u3057\u3066\u8a08\u7b97\u3059\u308b\u3093\u3060\u304b\u3089\u3001\u3053\u308c\u3067\u3082\u826f\u3044\u306f\u305a\u3060\u3068\u601d\u3063\u3066\u8272\u3005\u8a66\u884c\u932f\u8aa4\u3057\u3066\u3044\u305f\u3089\u4e0a\u624b\u304f\u3044\u3063\u305f\u6c17\u304c\u3057\u307e\u3059\u3002\u306a\u304b\u306a\u304bTensor\u306e\u6d41\u308c\u3092\u3061\u3083\u3093\u3068\u30a4\u30e1\u30fc\u30b8\u3059\u308b\u306e\u304c\u96e3\u3057\u3044\u3067\u3059\u306d\u3002\n\u672c\u5f53\u306fContainer\u306b\u3053\u306e\u6a5f\u80fd\u3092\u4ed8\u3051\u3088\u3046\u3068\u3057\u305f\u3093\u3067\u3059\u304c(\u6559\u5e2b\u306a\u3057\u3067\u3082\u4f7f\u3048\u308b\u306e\u3060\u304b\u3089)\u3001\u73fe\u5728\u306eKeras\u306e\u5b9f\u88c5\u3060\u3068Container\u304c\u4f59\u5206\u306aLoss\u3092Model\u306etotal_loss\u306b\u8db3\u3057\u8fbc\u3080\u4ed5\u7d44\u307f\u304c\u308f\u304b\u3089\u305a\u65ad\u5ff5\u3002Layer\u3060\u3068\u8907\u6570\u5165\u308c\u8fbc\u3081\u307e\u3059\u304c\u3001\u9060\u304f\u306efunction(input) -> output\u3092\u5225\u9014\u6e21\u3057\u3066\u3042\u3052\u306a\u3044\u3068VAT\u306f\u8a08\u7b97\u3067\u304d\u306a\u3044\u306e\u3067\u3042\u307e\u308a\u5b09\u3057\u304f\u306a\u3044\u3002\u307e\u3042\u3001\u524d\u56de\u3088\u308a\u30de\u30b7\u306b\u306a\u3063\u305f\u306e\u3067\u826f\u3057\u306b\u3057\u307e\u3059\u3002\n\n\u306f\u3058\u3081\u306b\n========\n\n\u5148\u65e5\u3001[Keras\u3067VAT(Virtual Adversarial Training)\u3092\u4f7f\u3063\u3066MNIST\u3092\u3084\u3063\u3066\u307f\u308b](http://qiita.com/mokemokechicken/items/69228b4c7884025e1ffe)\u3092\u6295\u7a3f\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u3082\u3046\u5c11\u3057\u30de\u30b7\u3063\u307d\u3044\u5b9f\u88c5\u304c\u3067\u304d\u305f\u306e\u3067\u5171\u6709\u3057\u307e\u3059\u3002\n\nVersion\n---------\n* Python: 3.5.3\n* Keras: 1.2.2\n* Theano: 0.8.2\n\n\u5b9f\u88c5\n=======\n\n\u524d\u56de\u304b\u3089\u306e\u9055\u3044\u306f\u3001\n\n* \u640d\u5931\u95a2\u6570\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001`Model.losses` \u306b VAT\u306eLoss\u3092\u4ed8\u3051\u308b\u3088\u3046\u306b\u3057\u3001\u5b66\u7fd2\u6642\u306b\u5999\u306a\u5909\u63db\u3092\u3057\u306a\u304f\u3066\u3088\u304f\u3057\u305f\uff08\u3053\u308c\u304c\u5927\u304d\u3044\u9055\u3044\uff09\n* \u901a\u5e38\u306e\u4e88\u6e2c\u5024\u306b `K.stop_gradient()` \u3092\u4ed8\u3051\u3066\u304a\u308a\u3001VAT\u8a08\u7b97\u304b\u3089\u767a\u751f\u3059\u308b\u4f59\u5206\u306a(?)\u5dee\u5206\u306e\u4f1d\u64ad\u3092\u6b62\u3081\u305f\uff08\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3068\u601d\u3046\uff09(\u3042\u307e\u308a\u7d50\u679c\u306f\u5909\u308f\u3089\u306a\u3044\u3051\u3069...)\n\n\u3068\u3044\u3046\u3068\u3053\u308d\u3067\u3059\u3002\n\n```python:mnist_with_vat_model.py\n# coding: utf8\n\"\"\"\n* VAT: https://arxiv.org/abs/1507.00677\n\n# \u53c2\u8003\u306b\u3057\u305fCode\nOriginal: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\nVAT: https://github.com/musyoku/vat/blob/master/vat.py\n\nresults example\n---------------\n\nfinish: use_dropout=False, use_vat=False: score=0.215942835068, accuracy=0.9872\nfinish: use_dropout=True, use_vat=False: score=0.261140023788, accuracy=0.9845\nfinish: use_dropout=False, use_vat=True: score=0.240192672965, accuracy=0.9894\nfinish: use_dropout=True, use_vat=True: score=0.210011005498, accuracy=0.9891\n\"\"\"\nimport numpy as np\nfrom functools import reduce\nfrom keras.engine.topology import Input, Container, to_list\nfrom keras.engine.training import Model\n\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\nSAMPLE_SIZE = 0\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\npool_size = (2, 2)\n# convolution kernel size\nkernel_size = (3, 3)\n\n\ndef main(data, use_dropout, use_vat):\n    np.random.seed(1337)  # for reproducibility\n\n    # the data, shuffled and split between train and test sets\n    (X_train, y_train), (X_test, y_test) = data\n\n    if K.image_dim_ordering() == 'th':\n        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n        input_shape = (1, img_rows, img_cols)\n    else:\n        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n        input_shape = (img_rows, img_cols, 1)\n\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    X_train /= 255.\n    X_test /= 255.\n\n    # convert class vectors to binary class matrices\n    y_train = np_utils.to_categorical(y_train, nb_classes)\n    y_test = np_utils.to_categorical(y_test, nb_classes)\n\n    if SAMPLE_SIZE:\n        X_train = X_train[:SAMPLE_SIZE]\n        y_train = y_train[:SAMPLE_SIZE]\n        X_test = X_test[:SAMPLE_SIZE]\n        y_test = y_test[:SAMPLE_SIZE]\n\n    print(\"start: use_dropout=%s, use_vat=%s\" % (use_dropout, use_vat))\n    my_model = MyModel(input_shape, use_dropout, use_vat).build()\n    my_model.training(X_train, y_train, X_test, y_test)\n\n    score = my_model.model.evaluate(X_test, y_test, verbose=0)\n    print(\"finish: use_dropout=%s, use_vat=%s: score=%s, accuracy=%s\" % (use_dropout, use_vat, score[0], score[1]))\n\n\nclass MyModel:\n    model = None\n\n    def __init__(self, input_shape, use_dropout=True, use_vat=True):\n        self.input_shape = input_shape\n        self.use_dropout = use_dropout\n        self.use_vat = use_vat\n\n    def build(self):\n        input_layer = Input(self.input_shape)\n        output_layer = self.core_data_flow(input_layer)\n        if self.use_vat:\n            self.model = VATModel(input_layer, output_layer).setup_vat_loss()\n        else:\n            self.model = Model(input_layer, output_layer)\n        return self\n\n    def core_data_flow(self, input_layer):\n        x = Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='valid')(input_layer)\n        x = Activation('relu')(x)\n        x = Convolution2D(nb_filters, kernel_size[0], kernel_size[1])(x)\n        x = Activation('relu')(x)\n        x = MaxPooling2D(pool_size=pool_size)(x)\n        if self.use_dropout:\n            x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(128, activation=\"relu\")(x)\n        if self.use_dropout:\n            x = Dropout(0.5)(x)\n        x = Dense(nb_classes, activation='softmax')(x)\n        return x\n\n    def training(self, X_train, y_train, X_test, y_test):\n        self.model.compile(loss=K.categorical_crossentropy, optimizer='adadelta', metrics=['accuracy'])\n        np.random.seed(1337)  # for reproducibility\n        self.model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n                       verbose=1, validation_data=(X_test, y_test))\n\n\nclass VATModel(Model):\n    _vat_loss = None\n\n    def setup_vat_loss(self, eps=1, xi=10, ip=1):\n        self._vat_loss = self.vat_loss(eps, xi, ip)\n        return self\n\n    @property\n    def losses(self):\n        losses = super(self.__class__, self).losses\n        if self._vat_loss:\n            losses += [self._vat_loss]\n        return losses\n\n    def vat_loss(self, eps, xi, ip):\n        normal_outputs = [K.stop_gradient(x) for x in to_list(self.outputs)]\n        d_list = [K.random_normal(x.shape) for x in self.inputs]\n\n        for _ in range(ip):\n            new_inputs = [x + self.normalize_vector(d)*xi for (x, d) in zip(self.inputs, d_list)]\n            new_outputs = to_list(self.call(new_inputs))\n            klds = [K.sum(self.kld(normal, new)) for normal, new in zip(normal_outputs, new_outputs)]\n            kld = reduce(lambda t, x: t+x, klds, 0)\n            d_list = [K.stop_gradient(d) for d in K.gradients(kld, d_list)]\n\n        new_inputs = [x + self.normalize_vector(d) * eps for (x, d) in zip(self.inputs, d_list)]\n        y_perturbations = to_list(self.call(new_inputs))\n        klds = [K.mean(self.kld(normal, new)) for normal, new in zip(normal_outputs, y_perturbations)]\n        kld = reduce(lambda t, x: t + x, klds, 0)\n        return kld\n\n    @staticmethod\n    def normalize_vector(x):\n        z = K.sum(K.batch_flatten(K.square(x)), axis=1)\n        while K.ndim(z) < K.ndim(x):\n            z = K.expand_dims(z, dim=-1)\n        return x / (K.sqrt(z) + K.epsilon())\n\n    @staticmethod\n    def kld(p, q):\n        v = p * (K.log(p + K.epsilon()) - K.log(q + K.epsilon()))\n        return K.sum(K.batch_flatten(v), axis=1, keepdims=True)\n\n\ndata = mnist.load_data()\nmain(data, use_dropout=False, use_vat=False)\nmain(data, use_dropout=True, use_vat=False)\nmain(data, use_dropout=False, use_vat=True)\nmain(data, use_dropout=True, use_vat=True)\n```\n\n\n\u5b9f\u9a13\u7d50\u679c\n=======\n\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b\u5b9f\u9a13\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n|Dropout|  VAT |Accuracy|1 epoch\u306e\u6642\u9593|\n|:-----:|:----:|-------:|-------:|\n|\u4f7f\u308f\u306a\u3044|\u4f7f\u308f\u306a\u3044|  98.72%|8\u79d2|\n|\u4f7f\u3046   |\u4f7f\u308f\u306a\u3044|  98.45%|8\u79d2|\n|\u4f7f\u308f\u306a\u3044|**\u4f7f\u3046**|**98.94%**|18\u79d2|\n|\u4f7f\u3046   |**\u4f7f\u3046**|**98.91%**|18\u79d2|\n\n\u3060\u3044\u305f\u3044\u540c\u3058\u3088\u3046\u306a\u7d50\u679c\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n\u3055\u3044\u3054\u306b\n=========\n\n\u3069\u3063\u3061\u307f\u3061Placeholder\u306b\u5bfe\u3057\u3066\u8a08\u7b97\u3059\u308b\u3093\u3060\u304b\u3089\u3001\u3053\u308c\u3067\u3082\u826f\u3044\u306f\u305a\u3060\u3068\u601d\u3063\u3066\u8272\u3005\u8a66\u884c\u932f\u8aa4\u3057\u3066\u3044\u305f\u3089\u4e0a\u624b\u304f\u3044\u3063\u305f\u6c17\u304c\u3057\u307e\u3059\u3002\u306a\u304b\u306a\u304bTensor\u306e\u6d41\u308c\u3092\u3061\u3083\u3093\u3068\u30a4\u30e1\u30fc\u30b8\u3059\u308b\u306e\u304c\u96e3\u3057\u3044\u3067\u3059\u306d\u3002\n\n\u672c\u5f53\u306f`Container`\u306b\u3053\u306e\u6a5f\u80fd\u3092\u4ed8\u3051\u3088\u3046\u3068\u3057\u305f\u3093\u3067\u3059\u304c(\u6559\u5e2b\u306a\u3057\u3067\u3082\u4f7f\u3048\u308b\u306e\u3060\u304b\u3089)\u3001\u73fe\u5728\u306eKeras\u306e\u5b9f\u88c5\u3060\u3068`Container`\u304c\u4f59\u5206\u306aLoss\u3092`Model`\u306e`total_loss`\u306b\u8db3\u3057\u8fbc\u3080\u4ed5\u7d44\u307f\u304c\u308f\u304b\u3089\u305a\u65ad\u5ff5\u3002`Layer`\u3060\u3068\u8907\u6570\u5165\u308c\u8fbc\u3081\u307e\u3059\u304c\u3001\u9060\u304f\u306e`function(input) -> output`\u3092\u5225\u9014\u6e21\u3057\u3066\u3042\u3052\u306a\u3044\u3068VAT\u306f\u8a08\u7b97\u3067\u304d\u306a\u3044\u306e\u3067\u3042\u307e\u308a\u5b09\u3057\u304f\u306a\u3044\u3002\u307e\u3042\u3001\u524d\u56de\u3088\u308a\u30de\u30b7\u306b\u306a\u3063\u305f\u306e\u3067\u826f\u3057\u306b\u3057\u307e\u3059\u3002\n", "tags": ["DeepLearning", "Python", "Keras", "\u6a5f\u68b0\u5b66\u7fd2"]}