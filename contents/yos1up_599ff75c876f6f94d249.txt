{"context": "\u3053\u3093\u306b\u3061\u306f\uff0e@yos1up \u3068\u7533\u3057\u307e\u3059\uff0e\u597d\u304d\u306a\u98df\u3079\u7269\u306f\uff0c\u7dd1\u8272\u306e\u30ad\u30ce\u30b3\u3067\u3059\uff0e\n2016/10/12\u306b DeepMind \u304c Nature \u306b\u6295\u7a3f\u3057\u305f\u8ad6\u6587\uff0cHybrid computing using a neural network with dynamic external memory \u306b\u3066\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb DNC (Differentiable Neural Computers) \u3092\uff0c\u5927\u6025\u304e\u3067 Chainer \u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\uff0e\n\nDNC \u306b\u3064\u3044\u3066\nDNC \u306f\uff0c\u5148\u8ff0\u306e\u8ad6\u6587\u3067\u63d0\u6848\u3055\u308c\u305f\u65b0\u3057\u3044\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\uff0c\u305d\u306e\u60c5\u5831\u51e6\u7406\u80fd\u529b\u306e\u9ad8\u3055\u304c\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u8ad6\u6587\u4e2d\u3067\u306f\uff0c\u30b0\u30e9\u30d5\u4e0a\u306e\u6700\u77ed\u7d4c\u8def\u30bf\u30b9\u30af\u3084\u3061\u3087\u3063\u3068\u3057\u305f\u30d1\u30ba\u30eb\u306e\u30bf\u30b9\u30af\u306a\u3069\uff0c\u5f93\u6765\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306f\u5b66\u7fd2\u4e0d\u53ef\u80fd\u3068\u601d\u308f\u308c\u3066\u3044\u305f\u30bf\u30b9\u30af\u304c DNC \u3067\u5b66\u7fd2\u3067\u304d\u3066\u304a\u308a\uff0c\u305d\u306e\u60c5\u5831\u51e6\u7406\u80fd\u529b\u306e\u9ad8\u3055\u304c\u3046\u304b\u304c\u3044\u77e5\u308c\u307e\u3059\uff0e\nDNC \u306f\uff0cRNN\uff08\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u306b\u300c\u30e1\u30e2\u30ea\u300d\u304c\u304f\u3063\u3064\u3044\u305f\u69cb\u9020\u3092\u3057\u3066\u3044\u307e\u3059\uff0e\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u306b\u306f\uff0c\u60c5\u5831\u3092\u8aad\u307f\u51fa\u3059\u30d8\u30c3\u30c9\u3068\u66f8\u304d\u8fbc\u3080\u30d8\u30c3\u30c9\u304c\u3064\u3044\u3066\u304a\u308a\uff0c\u30d8\u30c3\u30c9\u3092\uff08\u9650\u3089\u308c\u305f\u65b9\u6cd5\u306b\u5f93\u3063\u3066\uff09\u52d5\u304b\u3057\u305f\u308a\uff0c\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3093\u3060\u308a\uff0c\u66f8\u304d\u8fbc\u3093\u3060\u308a\u3059\u308b\u3053\u3068\u304c\u81ea\u5728\u306b\u3067\u304d\u307e\u3059\uff0e\n\u901a\u5e38\u306e RNN \u306f\uff0c\u6bce\u6642\u523b\u306b\u304a\u3044\u3066\uff0c\u4f55\u3089\u304b\u306e\u5916\u90e8\u5165\u529b\u3092\u53d7\u3051\u53d6\u308a\uff0c\u4f55\u3089\u304b\u306e\u51fa\u529b\u3092\u884c\u3044\u307e\u3059\uff08\u305d\u3057\u3066\u5185\u90e8\u72b6\u614b\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\uff09\uff0e\u4e00\u65b9\uff0cDNC \u5185\u306e RNN \u306f\uff0c\u6bce\u6642\u523b\u306b\u304a\u3044\u3066\uff0c\u901a\u5e38\u306e\u5916\u90e8\u5165\u529b\u306b\u52a0\u3048\u3066\u300c\u76f4\u524d\u306e\u6642\u523b\u306b\u300e\u30e1\u30e2\u30ea\u300f\u304b\u3089\u8aad\u307f\u51fa\u3057\u305f\u30c7\u30fc\u30bf\u300d\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\uff0e\u305d\u3057\u3066\uff0c\u901a\u5e38\u306e\u51fa\u529b\u306b\u52a0\u3048\u3066\u300c\u300e\u30e1\u30e2\u30ea\u300f\u306e\u64cd\u4f5c\u6307\u793a\u300d\u3092\u4f75\u305b\u3066\u51fa\u529b\u3057\u307e\u3059\uff0e\u3053\u306e\u64cd\u4f5c\u6307\u793a\u306b\u5f93\u3063\u3066\uff0c\u30e1\u30e2\u30ea\u306e\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u304c\u5909\u5316\u3057\uff0c\u66f8\u304d\u8fbc\u307f\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u30e1\u30e2\u30ea\u304c\u66f8\u304d\u63db\u308f\u308a\uff0c\u8aad\u307f\u51fa\u3057\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u30e1\u30e2\u30ea\u304c\u8aad\u307f\u51fa\u3055\u308c\u307e\u3059\uff0e\u8aad\u307f\u51fa\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\uff0c\u6b21\u306e\u6642\u523b\u306b\u304a\u3044\u3066 RNN \u306b\uff08\u901a\u5e38\u306e\u5916\u90e8\u5165\u529b\u3068\u4f75\u305b\u3066\uff09\u5165\u529b\u3055\u308c\u307e\u3059\uff0e\nDNC \u5185\u306e RNN \u306f\uff0c\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u3068\u3044\u3046\u5c0f\u9053\u5177\u3092\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067\uff0c\u9069\u5207\u306a\u5165\u51fa\u529b\u95a2\u4fc2\u304c\u5b9f\u73fe\u3055\u308c\u308b\u3088\u3046\u306b\uff0c\u7d50\u5408\u52a0\u91cd\u3092\uff08\u52fe\u914d\u6cd5\u3067\uff09\u5b66\u7fd2\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\u3053\u306e\u5c0f\u9053\u5177\u3092\u5982\u4f55\u306b\u4f7f\u3046\u304b\uff08\u3042\u308b\u3044\u306f\u4f7f\u308f\u306a\u3044\u304b\uff09\u306f\uff0c\u5b66\u7fd2\u6b21\u7b2c\u3067\u5909\u308f\u3063\u3066\u304d\u307e\u3059\uff0e\n\u300c\u30d8\u30c3\u30c9\u306e\u4ed8\u3044\u305f\u30e1\u30e2\u30ea\u300d\u3068\u3044\u3046\u7279\u6b8a\u306a\u5f62\u5f0f\u3092\u3057\u3066\u3044\u308b\u3082\u306e\u306e\uff0cRNN \u306e\u300c\u5185\u90e8\u72b6\u614b\u300d\u306b\u306f\u9055\u3044\u306a\u304f\uff0c\u305d\u3046\u3044\u3046\u610f\u5473\u3067 DNC \u306f\uff0cGRU \u3084 LSTM \u306e\u5ef6\u9577\u306b\u3042\u308b\u300c\u5185\u90e8\u72b6\u614b\u304c\u8457\u3057\u304f\u8907\u96d1\u306b\u306a\u3063\u305f RNN\u300d\u3060\u3068\u3044\u3048\u307e\u30591\uff0e\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u304c\u4ed8\u3044\u305f\u304a\u304b\u3052\u3067\uff0c\u5f93\u6765\u306eRNN\u3067\u306f\u6271\u3048\u306a\u304b\u3063\u305f\u8907\u96d1\u306a\u60c5\u5831\u51e6\u7406\u304c DNC \u3067\u306f\u5b9f\u73fe\u53ef\u80fd\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\n\u307e\u305f\uff0c\u3042\u3089\u3086\u308b\u7a2e\u985e\u306e\u60c5\u5831\u51e6\u7406\u3092\u884c\u3046\u969b\u306b\u305d\u3053\u305d\u3053\u4fbf\u5229\u3068\u601d\u308f\u308c\u308b\u300c\u30d8\u30c3\u30c9\u306e\u4ed8\u3044\u305f\u30e1\u30e2\u30ea\u300d\u304c\u5099\u308f\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u3088\u308a\uff0cDNC \u306b\u306f\u3042\u3089\u3086\u308b\u30bf\u30a4\u30d7\u306e\u30bf\u30b9\u30af\u3092\u305d\u308c\u306a\u308a\u306b\u3053\u306a\u305b\u308b\u6c4e\u7528\u6027\u304c\u751f\u3058\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u500b\u4eba\u7684\u306b\u671f\u5f85\u3057\u3066\u3044\u307e\u3059\uff0e\u8ad6\u6587\u4e2d\u3067 DNC \u304c\u89e3\u3044\u3066\u3044\u308b\u30bf\u30b9\u30af\u306e\u30b8\u30e3\u30f3\u30eb\u304c\u591a\u5c90\u306b\u308f\u305f\u3063\u3066\u3044\u308b\u3053\u3068\u3082\uff0c\u6c4e\u7528\u6027\u306e\u9ad8\u3055\u3092\u671f\u5f85\u3055\u305b\u307e\u3059\uff0e\n\u306a\u304a\uff0c\u5f7c\u3089\u304c 2014 \u5e74 12 \u6708\u306b\u63d0\u6848\u3057\u305f NTM (Neural Turing Machine) \u3068\u3044\u3046\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3082\u4eca\u56de\u306e DNC \u3068\u985e\u4f3c\u306e\u69cb\u9020\u3092\u3057\u3066\u3044\u307e\u3059\u304c\uff0cDNC \u306f NTM \u3088\u308a\u3082\u30e1\u30e2\u30ea\u306e\u30d8\u30c3\u30c9\u306e\u52d5\u304b\u305b\u65b9\u304c\u3088\u308a\u5408\u7406\u7684\u306b\u306a\u3063\u305f\u70b9\u3067\u30d1\u30ef\u30fc\u30a2\u30c3\u30d7\u3057\u3066\u3044\u307e\u3059\uff0e\uff08\u8ad6\u6587\u4e2d\u306e Methods \u4e2d\u306b NTM \u3068 DNC \u306e\u9055\u3044\u306b\u3064\u3044\u3066\u307e\u3068\u307e\u3063\u305f\u7b87\u6240\u304c\u3042\u308a\u307e\u3059\uff0e\uff09\n\n\u5b9f\u88c5\n\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9 (Python 2.7) \u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3059\uff0e(GitHub)\n\nmain.py\nimport numpy as np\nimport math\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import \\\n     cuda, gradient_check, optimizers, serializers, utils, \\\n     Chain, ChainList, Function, Link, Variable\n\n\ndef onehot(x,n):\n    ret = np.zeros(n).astype(np.float32)\n    ret[x] = 1.0\n    return ret\n\ndef overlap(u, v): # u, v: (1 * -) Variable  -> (1 * 1) Variable\n    denominator = F.sqrt(F.batch_l2_norm_squared(u) * F.batch_l2_norm_squared(v))\n    if (np.array_equal(denominator.data, np.array([0]))):\n        return F.matmul(u, F.transpose(v))\n    return F.matmul(u, F.transpose(v)) / F.reshape(denominator,(1,1))\n\n\ndef C(M, k, beta):\n    # (N * W), (1 * W), (1 * 1) -> (N * 1)\n    # (not (N * W), ({R,1} * W), (1 * {R,1}) -> (N * {R,1}))\n    W = M.data.shape[1]    \n    ret_list = [0] * M.data.shape[0]\n    for i in range(M.data.shape[0]):\n        ret_list[i] = overlap(F.reshape(M[i,:], (1, W)), k) * beta # pick i-th row\n    return F.transpose(F.softmax(F.transpose(F.concat(ret_list, 0)))) # concat vertically and calc softmax in each column\n\n\n\ndef u2a(u): # u, a: (N * 1) Variable\n    N = len(u.data)\n    phi = np.argsort(u.data.reshape(N)) # u.data[phi]: ascending\n    a_list = [0] * N    \n    cumprod = Variable(np.array([[1.0]]).astype(np.float32)) \n    for i in range(N):\n        a_list[phi[i]] = cumprod * (1.0 - F.reshape(u[phi[i],0], (1,1)))\n        cumprod *= F.reshape(u[phi[i],0], (1,1))\n    return F.concat(a_list, 0) # concat vertically\n\n\n\nclass DeepLSTM(Chain): # too simple?\n    def __init__(self, d_in, d_out):\n        super(DeepLSTM, self).__init__(\n            l1 = L.LSTM(d_in, d_out),\n            l2 = L.Linear(d_out, d_out),)\n    def __call__(self, x):\n        self.x = x\n        self.y = self.l2(self.l1(self.x))\n        return self.y\n    def reset_state(self):\n        self.l1.reset_state()\n\n\n\nclass DNC(Chain):\n    def __init__(self, X, Y, N, W, R):\n        self.X = X # input dimension\n        self.Y = Y # output dimension\n        self.N = N # number of memory slot\n        self.W = W # dimension of one memory slot\n        self.R = R # number of read heads\n        self.controller = DeepLSTM(W*R+X, Y+W*R+3*W+5*R+3)\n\n        super(DNC, self).__init__(\n            l_dl = self.controller,\n            l_Wr = L.Linear(self.R * self.W, self.Y) # nobias=True ? \n            )# <question : should all learnable weights be here??>\n        self.reset_state()\n    def __call__(self, x):\n        # <question : is batchsize>1 possible for RNN ? if No, I will implement calculations without batch dimension.>\n        self.chi = F.concat((x, self.r))\n        (self.nu, self.xi) = \\\n                  F.split_axis(self.l_dl(self.chi), [self.Y], 1)\n        (self.kr, self.betar, self.kw, self.betaw,\n         self.e, self.v, self.f, self.ga, self.gw, self.pi\n         ) = F.split_axis(self.xi, np.cumsum(\n             [self.W*self.R, self.R, self.W, 1, self.W, self.W, self.R, 1, 1]), 1)\n\n        self.kr = F.reshape(self.kr, (self.R, self.W)) # R * W\n        self.betar = 1 + F.softplus(self.betar) # 1 * R\n        # self.kw: 1 * W\n        self.betaw = 1 + F.softplus(self.betaw) # 1 * 1\n        self.e = F.sigmoid(self.e) # 1 * W\n        # self.v : 1 * W\n        self.f = F.sigmoid(self.f) # 1 * R\n        self.ga = F.sigmoid(self.ga) # 1 * 1\n        self.gw = F.sigmoid(self.gw) # 1 * 1\n        self.pi = F.softmax(F.reshape(self.pi, (self.R, 3))) # R * 3 (softmax for 3)\n\n        # self.wr : N * R\n        self.psi_mat = 1 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), self.f) * self.wr # N * R\n        self.psi = Variable(np.ones((self.N, 1)).astype(np.float32)) # N * 1\n        for i in range(self.R):\n            self.psi = self.psi * F.reshape(self.psi_mat[:,i],(self.N,1)) # N * 1\n\n        # self.ww, self.u : N * 1\n        self.u = (self.u + self.ww - (self.u * self.ww)) * self.psi\n\n        self.a = u2a(self.u) # N * 1\n        self.cw = C(self.M, self.kw, self.betaw) # N * 1\n        self.ww = F.matmul(F.matmul(self.a, self.ga) + F.matmul(self.cw, 1.0 - self.ga), self.gw) # N * 1\n        self.M = self.M * (np.ones((self.N, self.W)).astype(np.float32) - F.matmul(self.ww, self.e)) + F.matmul(self.ww, self.v) # N * W\n\n        self.p = (1.0 - F.matmul(Variable(np.ones((self.N,1)).astype(np.float32)), F.reshape(F.sum(self.ww),(1,1)))) \\\n                  * self.p + self.ww # N * 1\n        self.wwrep = F.matmul(self.ww, Variable(np.ones((1, self.N)).astype(np.float32))) # N * N\n        self.L = (1.0 - self.wwrep - F.transpose(self.wwrep)) * self.L + F.matmul(self.ww, F.transpose(self.p)) # N * N\n        self.L = self.L * (np.ones((self.N, self.N)) - np.eye(self.N)) # force L[i,i] == 0   \n\n        self.fo = F.matmul(self.L, self.wr) # N * R\n        self.ba = F.matmul(F.transpose(self.L), self.wr) # N * R\n\n        self.cr_list = [0] * self.R\n        for i in range(self.R):\n            self.cr_list[i] = C(self.M, F.reshape(self.kr[i,:],(1, self.W)),\n                                F.reshape(self.betar[0,i],(1, 1))) # N * 1\n        self.cr = F.concat(self.cr_list) # N * R\n\n        self.bacrfo = F.concat((F.reshape(F.transpose(self.ba),(self.R,self.N,1)),\n                               F.reshape(F.transpose(self.cr),(self.R,self.N,1)),\n                               F.reshape(F.transpose(self.fo) ,(self.R,self.N,1)),),2) # R * N * 3\n        self.pi = F.reshape(self.pi, (self.R,3,1)) # R * 3 * 1\n        self.wr = F.transpose(F.reshape(F.batch_matmul(self.bacrfo, self.pi), (self.R, self.N))) # N * R\n\n        self.r = F.reshape(F.matmul(F.transpose(self.M), self.wr),(1, self.R * self.W)) # W * R (-> 1 * RW)\n\n        self.y = self.l_Wr(self.r) + self.nu # 1 * Y\n        return self.y\n    def reset_state(self):\n        self.l_dl.reset_state()\n        self.u = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        self.p = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        self.L = Variable(np.zeros((self.N, self.N)).astype(np.float32))                           \n        self.M = Variable(np.zeros((self.N, self.W)).astype(np.float32))\n        self.r = Variable(np.zeros((1, self.R*self.W)).astype(np.float32))\n        self.wr = Variable(np.zeros((self.N, self.R)).astype(np.float32))\n        self.ww = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        # any variable else ?\n\nX = 5\nY = 5\nN = 10\nW = 10\nR = 2\nmdl = DNC(X, Y, N, W, R)\nopt = optimizers.Adam()\nopt.setup(mdl)\ndatanum = 100000\nloss = 0.0\nacc = 0.0\nfor datacnt in range(datanum):\n    lossfrac = np.zeros((1,2))\n    # x_seq = np.random.rand(X,seqlen).astype(np.float32)\n    # t_seq = np.random.rand(Y,seqlen).astype(np.float32)\n    # t_seq = np.copy(x_seq)\n\n    contentlen = np.random.randint(3,6)\n    content = np.random.randint(0,X-1,contentlen)\n    seqlen = contentlen + contentlen\n    x_seq_list = [float('nan')] * seqlen\n    t_seq_list = [float('nan')] * seqlen    \n    for i in range(seqlen):\n        if (i < contentlen):\n            x_seq_list[i] = onehot(content[i],X)\n        elif (i == contentlen):\n            x_seq_list[i] = onehot(X-1,X)\n        else:\n            x_seq_list[i] = np.zeros(X).astype(np.float32)\n\n        if (i >= contentlen):\n            t_seq_list[i] = onehot(content[i-contentlen],X)    \n\n    mdl.reset_state()\n    for cnt in range(seqlen):\n        x = Variable(x_seq_list[cnt].reshape(1,X))\n        if (isinstance(t_seq_list[cnt], np.ndarray)):\n            t = Variable(t_seq_list[cnt].reshape(1,Y))\n        else:\n            t = []\n\n        y = mdl(x)\n        if (isinstance(t,chainer.Variable)):\n            loss += (y - t)**2\n            print y.data, t.data, np.argmax(y.data)==np.argmax(t.data)\n            if (np.argmax(y.data)==np.argmax(t.data)): acc += 1\n        if (cnt+1==seqlen):\n            mdl.cleargrads()\n            loss.grad = np.ones(loss.data.shape, dtype=np.float32)\n            loss.backward()\n            opt.update()\n            loss.unchain_backward()\n            print '(', datacnt, ')', loss.data.sum()/loss.data.size/contentlen, acc/contentlen\n            lossfrac += [loss.data.sum()/loss.data.size/seqlen, 1.]\n            loss = 0.0\n            acc = 0.0\n\n\n\u4eca\u56de\u306e\u8ad6\u6587\u306f\uff0c\u30e2\u30c7\u30eb\u306e\u8a73\u7d30\u304c\u3057\u3063\u304b\u308a Methods \u3068 Supplementary Material \u306b\u66f8\u304b\u308c\u3066\u304a\u308a\uff0c\u3068\u3066\u3082\u89aa\u5207\u306b\u611f\u3058\u307e\u3057\u305f\uff0e\u7279\u306b Supplementary \u306b\u306f\u5168\u3066\u306e\u5909\u6570\u3068\u30e2\u30c7\u30eb\u5185\u306e\u6570\u5f0f\u304c\u307e\u3068\u3081\u3089\u308c\u3066\u304a\u308a\uff0c\u3053\u3053\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u5168\u3066\u306e\u5f0f\u3092\u4e0a\u304b\u3089\u9806\u306b\u30b3\u30fc\u30c9\u306b\u300c\u79fb\u690d\u300d\u3057\u3066\u3044\u304f\u3060\u3051\u3067\u30e2\u30c7\u30eb\u3092\u5b8c\u6210\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\uff08\u4e0a\u8a18\u30b3\u30fc\u30c9\u4e2d\u306e\u5909\u6570\u540d\u306f\uff0c\u307b\u307c Supplementary \u306e\u6570\u5f0f\u4e2d\u306e\u5909\u6570\u540d\u3068\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\uff0e\uff09\n\u307e\u305f\uff0c\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u904e\u7a0b\u3067\uff0cchainer \u306e Variable \u3092\u64cd\u4f5c\u3059\u308b\u3044\u308d\u3044\u308d\u306a\u95a2\u6570\u306b\u3064\u3044\u3066\uff0c\u3044\u304f\u3089\u304b\u8a73\u3057\u304f\u306a\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\uff08\u6a5f\u4f1a\u304c\u3042\u308c\u3070\u307e\u3068\u3081\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\uff09\n\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u306f\uff0c\u3068\u3066\u3082\u7c21\u5358\u306a\u30bf\u30b9\u30af\uff08\u77ed\u3044\u30b7\u30f3\u30dc\u30eb\u5217\u3092\u9045\u308c\u3066 echo \u3059\u308b\u30bf\u30b9\u30af\uff09\u3092 DNC \u306b\u5b66\u7fd2\u3055\u305b\u308b\u30b3\u30fc\u30c9\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\u4e00\u5fdc\u30a8\u30e9\u30fc\u306a\u3057\u306b\u52d5\u304d\u307e\u3059\uff0e1000\u30c7\u30fc\u30bf\u307b\u3069\u3067\u5b66\u7fd2\u3067\u304d\u3066\u3044\u307e\u3059\uff0e\uff08\u203b\u3053\u306e\u30bf\u30b9\u30af\u306f\u8ad6\u6587\u4e2d\u306b\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\uff09\n\u3057\u304b\u3057\uff0c\u6025\u3044\u3067\u5b9f\u88c5\u3057\u305f\u305f\u3081\uff0c\u8ad6\u6587\u901a\u308a\u6b63\u3057\u304f DNC \u3092\u5b9f\u88c5\u3067\u304d\u3066\u3044\u308b\u304b\u78ba\u8a3c\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\u8aa4\u308a\u304c\u898b\u3064\u304b\u308a\u307e\u3057\u305f\u3089\uff0c\u305c\u3072\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\uff0e\n\u4eca\u5f8c\u306f\uff0c\u4eca\u56de\u4f5c\u3063\u305f DNC \u3092\uff0c\u8272\u3005\u306a\u5b66\u7fd2\u30bf\u30b9\u30af\u306b\u9069\u7528\u3057\u3066\u3044\u304d\u305f\u3044\u3067\u3059\uff0eDeepMind \u306e\u8ad6\u6587\u307f\u305f\u304f\uff0c\u30d1\u30ba\u30eb\u3092\u89e3\u304b\u305b\u305f\u308a\u3057\u3066\u307f\u305f\u3044\u3067\u3059\uff0e\n\n\u66f4\u65b0\u5c65\u6b74\n2016/10/30 \u2026\u2026 Variable \u306e\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0\u304c\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u305f\u3081\uff0c\u30b3\u30fc\u30c9\u3092\u4e00\u90e8\u4fee\u6b63\u3057\u307e\u3057\u305f\uff0e\n\n\n\n\n\u305f\u3060\u3057\uff0c\u5f7c\u3089\u306f\u8ad6\u6587\u4e2d\u3067\u3053\u306e\u30e1\u30e2\u30ea\u3092\u300c\u5916\u90e8\u300d\u30e1\u30e2\u30ea (external memory) \u3068\u547c\u3093\u3067\u3044\u307e\u3059\uff0e\u4ee5\u4e0b\u8ad6\u6587\u3088\u308a\uff1aThe behaviour of the network is independent of the memory size as long as the memory is not filled to capacity, which is why we view the memory as 'external'.\uff08\u30e1\u30e2\u30ea\u304c\u5bb9\u91cf\u306b\u9054\u3055\u306a\u3044\u9650\u308a\u306f\uff0c\u6211\u3005\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6319\u52d5\u306f\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044\uff0e\u305d\u306e\u305f\u3081\uff0c\u6211\u3005\u306f\u3053\u306e\u30e1\u30e2\u30ea\u3092\u300e\u5916\u90e8\u300f\u30e1\u30e2\u30ea\u3068\u6349\u3048\u3066\u3044\u308b\uff0e\uff09\u00a0\u21a9\n\n\n\n\u3053\u3093\u306b\u3061\u306f\uff0e@yos1up \u3068\u7533\u3057\u307e\u3059\uff0e\u597d\u304d\u306a\u98df\u3079\u7269\u306f\uff0c\u7dd1\u8272\u306e\u30ad\u30ce\u30b3\u3067\u3059\uff0e\n\n2016/10/12\u306b DeepMind \u304c Nature \u306b\u6295\u7a3f\u3057\u305f\u8ad6\u6587\uff0c[Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) \u306b\u3066\u63d0\u6848\u3055\u308c\u3066\u3044\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb **DNC (Differentiable Neural Computers)** \u3092\uff0c\u5927\u6025\u304e\u3067 Chainer \u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\uff0e\n\n# DNC \u306b\u3064\u3044\u3066\nDNC \u306f\uff0c\u5148\u8ff0\u306e\u8ad6\u6587\u3067\u63d0\u6848\u3055\u308c\u305f\u65b0\u3057\u3044\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\uff0c\u305d\u306e\u60c5\u5831\u51e6\u7406\u80fd\u529b\u306e\u9ad8\u3055\u304c\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u8ad6\u6587\u4e2d\u3067\u306f\uff0c\u30b0\u30e9\u30d5\u4e0a\u306e\u6700\u77ed\u7d4c\u8def\u30bf\u30b9\u30af\u3084\u3061\u3087\u3063\u3068\u3057\u305f\u30d1\u30ba\u30eb\u306e\u30bf\u30b9\u30af\u306a\u3069\uff0c\u5f93\u6765\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306f\u5b66\u7fd2\u4e0d\u53ef\u80fd\u3068\u601d\u308f\u308c\u3066\u3044\u305f\u30bf\u30b9\u30af\u304c DNC \u3067\u5b66\u7fd2\u3067\u304d\u3066\u304a\u308a\uff0c\u305d\u306e\u60c5\u5831\u51e6\u7406\u80fd\u529b\u306e\u9ad8\u3055\u304c\u3046\u304b\u304c\u3044\u77e5\u308c\u307e\u3059\uff0e\n\nDNC \u306f\uff0cRNN\uff08\u30ea\u30ab\u30ec\u30f3\u30c8\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u306b\u300c\u30e1\u30e2\u30ea\u300d\u304c\u304f\u3063\u3064\u3044\u305f\u69cb\u9020\u3092\u3057\u3066\u3044\u307e\u3059\uff0e\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u306b\u306f\uff0c\u60c5\u5831\u3092\u8aad\u307f\u51fa\u3059\u30d8\u30c3\u30c9\u3068\u66f8\u304d\u8fbc\u3080\u30d8\u30c3\u30c9\u304c\u3064\u3044\u3066\u304a\u308a\uff0c\u30d8\u30c3\u30c9\u3092\uff08\u9650\u3089\u308c\u305f\u65b9\u6cd5\u306b\u5f93\u3063\u3066\uff09\u52d5\u304b\u3057\u305f\u308a\uff0c\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3093\u3060\u308a\uff0c\u66f8\u304d\u8fbc\u3093\u3060\u308a\u3059\u308b\u3053\u3068\u304c\u81ea\u5728\u306b\u3067\u304d\u307e\u3059\uff0e\n\n\u901a\u5e38\u306e RNN \u306f\uff0c\u6bce\u6642\u523b\u306b\u304a\u3044\u3066\uff0c\u4f55\u3089\u304b\u306e\u5916\u90e8\u5165\u529b\u3092\u53d7\u3051\u53d6\u308a\uff0c\u4f55\u3089\u304b\u306e\u51fa\u529b\u3092\u884c\u3044\u307e\u3059\uff08\u305d\u3057\u3066\u5185\u90e8\u72b6\u614b\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\uff09\uff0e\u4e00\u65b9\uff0cDNC \u5185\u306e RNN \u306f\uff0c\u6bce\u6642\u523b\u306b\u304a\u3044\u3066\uff0c\u901a\u5e38\u306e\u5916\u90e8\u5165\u529b\u306b\u52a0\u3048\u3066\u300c\u76f4\u524d\u306e\u6642\u523b\u306b\u300e\u30e1\u30e2\u30ea\u300f\u304b\u3089\u8aad\u307f\u51fa\u3057\u305f\u30c7\u30fc\u30bf\u300d\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\uff0e\u305d\u3057\u3066\uff0c\u901a\u5e38\u306e\u51fa\u529b\u306b\u52a0\u3048\u3066\u300c\u300e\u30e1\u30e2\u30ea\u300f\u306e\u64cd\u4f5c\u6307\u793a\u300d\u3092\u4f75\u305b\u3066\u51fa\u529b\u3057\u307e\u3059\uff0e\u3053\u306e\u64cd\u4f5c\u6307\u793a\u306b\u5f93\u3063\u3066\uff0c\u30e1\u30e2\u30ea\u306e\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u304c\u5909\u5316\u3057\uff0c\u66f8\u304d\u8fbc\u307f\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u30e1\u30e2\u30ea\u304c\u66f8\u304d\u63db\u308f\u308a\uff0c\u8aad\u307f\u51fa\u3057\u30d8\u30c3\u30c9\u306e\u4f4d\u7f6e\u306e\u30e1\u30e2\u30ea\u304c\u8aad\u307f\u51fa\u3055\u308c\u307e\u3059\uff0e\u8aad\u307f\u51fa\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\uff0c\u6b21\u306e\u6642\u523b\u306b\u304a\u3044\u3066 RNN \u306b\uff08\u901a\u5e38\u306e\u5916\u90e8\u5165\u529b\u3068\u4f75\u305b\u3066\uff09\u5165\u529b\u3055\u308c\u307e\u3059\uff0e\n\nDNC \u5185\u306e RNN \u306f\uff0c\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u3068\u3044\u3046\u5c0f\u9053\u5177\u3092\u4e0e\u3048\u3089\u308c\u305f\u72b6\u6cc1\u3067\uff0c\u9069\u5207\u306a\u5165\u51fa\u529b\u95a2\u4fc2\u304c\u5b9f\u73fe\u3055\u308c\u308b\u3088\u3046\u306b\uff0c\u7d50\u5408\u52a0\u91cd\u3092\uff08\u52fe\u914d\u6cd5\u3067\uff09\u5b66\u7fd2\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\u3053\u306e\u5c0f\u9053\u5177\u3092\u5982\u4f55\u306b\u4f7f\u3046\u304b\uff08\u3042\u308b\u3044\u306f\u4f7f\u308f\u306a\u3044\u304b\uff09\u306f\uff0c\u5b66\u7fd2\u6b21\u7b2c\u3067\u5909\u308f\u3063\u3066\u304d\u307e\u3059\uff0e\n\n\u300c\u30d8\u30c3\u30c9\u306e\u4ed8\u3044\u305f\u30e1\u30e2\u30ea\u300d\u3068\u3044\u3046\u7279\u6b8a\u306a\u5f62\u5f0f\u3092\u3057\u3066\u3044\u308b\u3082\u306e\u306e\uff0cRNN \u306e\u300c\u5185\u90e8\u72b6\u614b\u300d\u306b\u306f\u9055\u3044\u306a\u304f\uff0c\u305d\u3046\u3044\u3046\u610f\u5473\u3067 DNC \u306f\uff0cGRU \u3084 LSTM \u306e\u5ef6\u9577\u306b\u3042\u308b\u300c\u5185\u90e8\u72b6\u614b\u304c\u8457\u3057\u304f\u8907\u96d1\u306b\u306a\u3063\u305f RNN\u300d\u3060\u3068\u3044\u3048\u307e\u3059[^2]\uff0e\u3053\u306e\u300c\u30e1\u30e2\u30ea\u300d\u304c\u4ed8\u3044\u305f\u304a\u304b\u3052\u3067\uff0c**\u5f93\u6765\u306eRNN\u3067\u306f\u6271\u3048\u306a\u304b\u3063\u305f\u8907\u96d1\u306a\u60c5\u5831\u51e6\u7406**\u304c DNC \u3067\u306f\u5b9f\u73fe\u53ef\u80fd\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\n\n[^2]: \u305f\u3060\u3057\uff0c\u5f7c\u3089\u306f\u8ad6\u6587\u4e2d\u3067\u3053\u306e\u30e1\u30e2\u30ea\u3092\u300c\u5916\u90e8\u300d\u30e1\u30e2\u30ea (external memory) \u3068\u547c\u3093\u3067\u3044\u307e\u3059\uff0e\u4ee5\u4e0b\u8ad6\u6587\u3088\u308a\uff1aThe behaviour of the network is independent of the memory size as long as the memory is not filled to capacity, which is why we view the memory as 'external'.\uff08\u30e1\u30e2\u30ea\u304c\u5bb9\u91cf\u306b\u9054\u3055\u306a\u3044\u9650\u308a\u306f\uff0c\u6211\u3005\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6319\u52d5\u306f\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044\uff0e\u305d\u306e\u305f\u3081\uff0c\u6211\u3005\u306f\u3053\u306e\u30e1\u30e2\u30ea\u3092\u300e\u5916\u90e8\u300f\u30e1\u30e2\u30ea\u3068\u6349\u3048\u3066\u3044\u308b\uff0e\uff09\n\n\u307e\u305f\uff0c\u3042\u3089\u3086\u308b\u7a2e\u985e\u306e\u60c5\u5831\u51e6\u7406\u3092\u884c\u3046\u969b\u306b\u305d\u3053\u305d\u3053\u4fbf\u5229\u3068\u601d\u308f\u308c\u308b\u300c\u30d8\u30c3\u30c9\u306e\u4ed8\u3044\u305f\u30e1\u30e2\u30ea\u300d\u304c\u5099\u308f\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u3088\u308a\uff0cDNC \u306b\u306f\u3042\u3089\u3086\u308b\u30bf\u30a4\u30d7\u306e\u30bf\u30b9\u30af\u3092\u305d\u308c\u306a\u308a\u306b\u3053\u306a\u305b\u308b**\u6c4e\u7528\u6027**\u304c\u751f\u3058\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u500b\u4eba\u7684\u306b\u671f\u5f85\u3057\u3066\u3044\u307e\u3059\uff0e\u8ad6\u6587\u4e2d\u3067 DNC \u304c\u89e3\u3044\u3066\u3044\u308b\u30bf\u30b9\u30af\u306e\u30b8\u30e3\u30f3\u30eb\u304c\u591a\u5c90\u306b\u308f\u305f\u3063\u3066\u3044\u308b\u3053\u3068\u3082\uff0c\u6c4e\u7528\u6027\u306e\u9ad8\u3055\u3092\u671f\u5f85\u3055\u305b\u307e\u3059\uff0e\n\n\u306a\u304a\uff0c\u5f7c\u3089\u304c 2014 \u5e74 12 \u6708\u306b\u63d0\u6848\u3057\u305f [NTM (Neural Turing Machine)](https://arxiv.org/pdf/1410.5401v2.pdf) \u3068\u3044\u3046\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3082\u4eca\u56de\u306e DNC \u3068\u985e\u4f3c\u306e\u69cb\u9020\u3092\u3057\u3066\u3044\u307e\u3059\u304c\uff0cDNC \u306f NTM \u3088\u308a\u3082\u30e1\u30e2\u30ea\u306e\u30d8\u30c3\u30c9\u306e\u52d5\u304b\u305b\u65b9\u304c\u3088\u308a\u5408\u7406\u7684\u306b\u306a\u3063\u305f\u70b9\u3067\u30d1\u30ef\u30fc\u30a2\u30c3\u30d7\u3057\u3066\u3044\u307e\u3059\uff0e\uff08\u8ad6\u6587\u4e2d\u306e Methods \u4e2d\u306b NTM \u3068 DNC \u306e\u9055\u3044\u306b\u3064\u3044\u3066\u307e\u3068\u307e\u3063\u305f\u7b87\u6240\u304c\u3042\u308a\u307e\u3059\uff0e\uff09\n\n# \u5b9f\u88c5\n\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9 (Python 2.7) \u3092\u4ee5\u4e0b\u306b\u793a\u3057\u307e\u3059\uff0e([GitHub]( https://github.com/yos1up/DNC))\n\n```main.py\nimport numpy as np\nimport math\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import \\\n     cuda, gradient_check, optimizers, serializers, utils, \\\n     Chain, ChainList, Function, Link, Variable\n\n\ndef onehot(x,n):\n    ret = np.zeros(n).astype(np.float32)\n    ret[x] = 1.0\n    return ret\n\ndef overlap(u, v): # u, v: (1 * -) Variable  -> (1 * 1) Variable\n    denominator = F.sqrt(F.batch_l2_norm_squared(u) * F.batch_l2_norm_squared(v))\n    if (np.array_equal(denominator.data, np.array([0]))):\n        return F.matmul(u, F.transpose(v))\n    return F.matmul(u, F.transpose(v)) / F.reshape(denominator,(1,1))\n\n\ndef C(M, k, beta):\n    # (N * W), (1 * W), (1 * 1) -> (N * 1)\n    # (not (N * W), ({R,1} * W), (1 * {R,1}) -> (N * {R,1}))\n    W = M.data.shape[1]    \n    ret_list = [0] * M.data.shape[0]\n    for i in range(M.data.shape[0]):\n        ret_list[i] = overlap(F.reshape(M[i,:], (1, W)), k) * beta # pick i-th row\n    return F.transpose(F.softmax(F.transpose(F.concat(ret_list, 0)))) # concat vertically and calc softmax in each column\n\n\n\ndef u2a(u): # u, a: (N * 1) Variable\n    N = len(u.data)\n    phi = np.argsort(u.data.reshape(N)) # u.data[phi]: ascending\n    a_list = [0] * N    \n    cumprod = Variable(np.array([[1.0]]).astype(np.float32)) \n    for i in range(N):\n        a_list[phi[i]] = cumprod * (1.0 - F.reshape(u[phi[i],0], (1,1)))\n        cumprod *= F.reshape(u[phi[i],0], (1,1))\n    return F.concat(a_list, 0) # concat vertically\n\n\n\nclass DeepLSTM(Chain): # too simple?\n    def __init__(self, d_in, d_out):\n        super(DeepLSTM, self).__init__(\n            l1 = L.LSTM(d_in, d_out),\n            l2 = L.Linear(d_out, d_out),)\n    def __call__(self, x):\n        self.x = x\n        self.y = self.l2(self.l1(self.x))\n        return self.y\n    def reset_state(self):\n        self.l1.reset_state()\n\n\n    \nclass DNC(Chain):\n    def __init__(self, X, Y, N, W, R):\n        self.X = X # input dimension\n        self.Y = Y # output dimension\n        self.N = N # number of memory slot\n        self.W = W # dimension of one memory slot\n        self.R = R # number of read heads\n        self.controller = DeepLSTM(W*R+X, Y+W*R+3*W+5*R+3)\n        \n        super(DNC, self).__init__(\n            l_dl = self.controller,\n            l_Wr = L.Linear(self.R * self.W, self.Y) # nobias=True ? \n            )# <question : should all learnable weights be here??>\n        self.reset_state()\n    def __call__(self, x):\n        # <question : is batchsize>1 possible for RNN ? if No, I will implement calculations without batch dimension.>\n        self.chi = F.concat((x, self.r))\n        (self.nu, self.xi) = \\\n                  F.split_axis(self.l_dl(self.chi), [self.Y], 1)\n        (self.kr, self.betar, self.kw, self.betaw,\n         self.e, self.v, self.f, self.ga, self.gw, self.pi\n         ) = F.split_axis(self.xi, np.cumsum(\n             [self.W*self.R, self.R, self.W, 1, self.W, self.W, self.R, 1, 1]), 1)\n\n        self.kr = F.reshape(self.kr, (self.R, self.W)) # R * W\n        self.betar = 1 + F.softplus(self.betar) # 1 * R\n        # self.kw: 1 * W\n        self.betaw = 1 + F.softplus(self.betaw) # 1 * 1\n        self.e = F.sigmoid(self.e) # 1 * W\n        # self.v : 1 * W\n        self.f = F.sigmoid(self.f) # 1 * R\n        self.ga = F.sigmoid(self.ga) # 1 * 1\n        self.gw = F.sigmoid(self.gw) # 1 * 1\n        self.pi = F.softmax(F.reshape(self.pi, (self.R, 3))) # R * 3 (softmax for 3)\n\n        # self.wr : N * R\n        self.psi_mat = 1 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), self.f) * self.wr # N * R\n        self.psi = Variable(np.ones((self.N, 1)).astype(np.float32)) # N * 1\n        for i in range(self.R):\n            self.psi = self.psi * F.reshape(self.psi_mat[:,i],(self.N,1)) # N * 1\n\n        # self.ww, self.u : N * 1\n        self.u = (self.u + self.ww - (self.u * self.ww)) * self.psi\n        \n        self.a = u2a(self.u) # N * 1\n        self.cw = C(self.M, self.kw, self.betaw) # N * 1\n        self.ww = F.matmul(F.matmul(self.a, self.ga) + F.matmul(self.cw, 1.0 - self.ga), self.gw) # N * 1\n        self.M = self.M * (np.ones((self.N, self.W)).astype(np.float32) - F.matmul(self.ww, self.e)) + F.matmul(self.ww, self.v) # N * W\n\n        self.p = (1.0 - F.matmul(Variable(np.ones((self.N,1)).astype(np.float32)), F.reshape(F.sum(self.ww),(1,1)))) \\\n                  * self.p + self.ww # N * 1\n        self.wwrep = F.matmul(self.ww, Variable(np.ones((1, self.N)).astype(np.float32))) # N * N\n        self.L = (1.0 - self.wwrep - F.transpose(self.wwrep)) * self.L + F.matmul(self.ww, F.transpose(self.p)) # N * N\n        self.L = self.L * (np.ones((self.N, self.N)) - np.eye(self.N)) # force L[i,i] == 0   \n\n        self.fo = F.matmul(self.L, self.wr) # N * R\n        self.ba = F.matmul(F.transpose(self.L), self.wr) # N * R\n\n        self.cr_list = [0] * self.R\n        for i in range(self.R):\n            self.cr_list[i] = C(self.M, F.reshape(self.kr[i,:],(1, self.W)),\n                                F.reshape(self.betar[0,i],(1, 1))) # N * 1\n        self.cr = F.concat(self.cr_list) # N * R\n\n        self.bacrfo = F.concat((F.reshape(F.transpose(self.ba),(self.R,self.N,1)),\n                               F.reshape(F.transpose(self.cr),(self.R,self.N,1)),\n                               F.reshape(F.transpose(self.fo) ,(self.R,self.N,1)),),2) # R * N * 3\n        self.pi = F.reshape(self.pi, (self.R,3,1)) # R * 3 * 1\n        self.wr = F.transpose(F.reshape(F.batch_matmul(self.bacrfo, self.pi), (self.R, self.N))) # N * R\n            \n        self.r = F.reshape(F.matmul(F.transpose(self.M), self.wr),(1, self.R * self.W)) # W * R (-> 1 * RW)\n        \n        self.y = self.l_Wr(self.r) + self.nu # 1 * Y\n        return self.y\n    def reset_state(self):\n        self.l_dl.reset_state()\n        self.u = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        self.p = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        self.L = Variable(np.zeros((self.N, self.N)).astype(np.float32))                           \n        self.M = Variable(np.zeros((self.N, self.W)).astype(np.float32))\n        self.r = Variable(np.zeros((1, self.R*self.W)).astype(np.float32))\n        self.wr = Variable(np.zeros((self.N, self.R)).astype(np.float32))\n        self.ww = Variable(np.zeros((self.N, 1)).astype(np.float32))\n        # any variable else ?\n\nX = 5\nY = 5\nN = 10\nW = 10\nR = 2\nmdl = DNC(X, Y, N, W, R)\nopt = optimizers.Adam()\nopt.setup(mdl)\ndatanum = 100000\nloss = 0.0\nacc = 0.0\nfor datacnt in range(datanum):\n    lossfrac = np.zeros((1,2))\n    # x_seq = np.random.rand(X,seqlen).astype(np.float32)\n    # t_seq = np.random.rand(Y,seqlen).astype(np.float32)\n    # t_seq = np.copy(x_seq)\n\n    contentlen = np.random.randint(3,6)\n    content = np.random.randint(0,X-1,contentlen)\n    seqlen = contentlen + contentlen\n    x_seq_list = [float('nan')] * seqlen\n    t_seq_list = [float('nan')] * seqlen    \n    for i in range(seqlen):\n        if (i < contentlen):\n            x_seq_list[i] = onehot(content[i],X)\n        elif (i == contentlen):\n            x_seq_list[i] = onehot(X-1,X)\n        else:\n            x_seq_list[i] = np.zeros(X).astype(np.float32)\n            \n        if (i >= contentlen):\n            t_seq_list[i] = onehot(content[i-contentlen],X)    \n    \n    mdl.reset_state()\n    for cnt in range(seqlen):\n        x = Variable(x_seq_list[cnt].reshape(1,X))\n        if (isinstance(t_seq_list[cnt], np.ndarray)):\n            t = Variable(t_seq_list[cnt].reshape(1,Y))\n        else:\n            t = []\n            \n        y = mdl(x)\n        if (isinstance(t,chainer.Variable)):\n            loss += (y - t)**2\n            print y.data, t.data, np.argmax(y.data)==np.argmax(t.data)\n            if (np.argmax(y.data)==np.argmax(t.data)): acc += 1\n        if (cnt+1==seqlen):\n            mdl.cleargrads()\n            loss.grad = np.ones(loss.data.shape, dtype=np.float32)\n            loss.backward()\n            opt.update()\n            loss.unchain_backward()\n            print '(', datacnt, ')', loss.data.sum()/loss.data.size/contentlen, acc/contentlen\n            lossfrac += [loss.data.sum()/loss.data.size/seqlen, 1.]\n            loss = 0.0\n            acc = 0.0\n```\n\n\n\u4eca\u56de\u306e\u8ad6\u6587\u306f\uff0c\u30e2\u30c7\u30eb\u306e\u8a73\u7d30\u304c\u3057\u3063\u304b\u308a Methods \u3068 Supplementary Material \u306b\u66f8\u304b\u308c\u3066\u304a\u308a\uff0c\u3068\u3066\u3082\u89aa\u5207\u306b\u611f\u3058\u307e\u3057\u305f\uff0e\u7279\u306b Supplementary \u306b\u306f\u5168\u3066\u306e\u5909\u6570\u3068\u30e2\u30c7\u30eb\u5185\u306e\u6570\u5f0f\u304c\u307e\u3068\u3081\u3089\u308c\u3066\u304a\u308a\uff0c\u3053\u3053\u306b\u66f8\u304b\u308c\u3066\u3044\u308b\u5168\u3066\u306e\u5f0f\u3092\u4e0a\u304b\u3089\u9806\u306b\u30b3\u30fc\u30c9\u306b\u300c\u79fb\u690d\u300d\u3057\u3066\u3044\u304f\u3060\u3051\u3067\u30e2\u30c7\u30eb\u3092\u5b8c\u6210\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\uff08\u4e0a\u8a18\u30b3\u30fc\u30c9\u4e2d\u306e\u5909\u6570\u540d\u306f\uff0c\u307b\u307c Supplementary \u306e\u6570\u5f0f\u4e2d\u306e\u5909\u6570\u540d\u3068\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\uff0e\uff09\n\u307e\u305f\uff0c\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u904e\u7a0b\u3067\uff0cchainer \u306e Variable \u3092\u64cd\u4f5c\u3059\u308b\u3044\u308d\u3044\u308d\u306a\u95a2\u6570\u306b\u3064\u3044\u3066\uff0c\u3044\u304f\u3089\u304b\u8a73\u3057\u304f\u306a\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\uff0e\uff08\u6a5f\u4f1a\u304c\u3042\u308c\u3070\u307e\u3068\u3081\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\uff09\n\n\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u306f\uff0c\u3068\u3066\u3082\u7c21\u5358\u306a\u30bf\u30b9\u30af\uff08\u77ed\u3044\u30b7\u30f3\u30dc\u30eb\u5217\u3092\u9045\u308c\u3066 echo \u3059\u308b\u30bf\u30b9\u30af\uff09\u3092 DNC \u306b\u5b66\u7fd2\u3055\u305b\u308b\u30b3\u30fc\u30c9\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\u4e00\u5fdc\u30a8\u30e9\u30fc\u306a\u3057\u306b\u52d5\u304d\u307e\u3059\uff0e1000\u30c7\u30fc\u30bf\u307b\u3069\u3067\u5b66\u7fd2\u3067\u304d\u3066\u3044\u307e\u3059\uff0e\uff08\u203b\u3053\u306e\u30bf\u30b9\u30af\u306f\u8ad6\u6587\u4e2d\u306b\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\uff09\n\u3057\u304b\u3057\uff0c\u6025\u3044\u3067\u5b9f\u88c5\u3057\u305f\u305f\u3081\uff0c\u8ad6\u6587\u901a\u308a\u6b63\u3057\u304f DNC \u3092\u5b9f\u88c5\u3067\u304d\u3066\u3044\u308b\u304b\u78ba\u8a3c\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\u8aa4\u308a\u304c\u898b\u3064\u304b\u308a\u307e\u3057\u305f\u3089\uff0c\u305c\u3072\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\uff0e\n\n\u4eca\u5f8c\u306f\uff0c\u4eca\u56de\u4f5c\u3063\u305f DNC \u3092\uff0c\u8272\u3005\u306a\u5b66\u7fd2\u30bf\u30b9\u30af\u306b\u9069\u7528\u3057\u3066\u3044\u304d\u305f\u3044\u3067\u3059\uff0eDeepMind \u306e\u8ad6\u6587\u307f\u305f\u304f\uff0c\u30d1\u30ba\u30eb\u3092\u89e3\u304b\u305b\u305f\u308a\u3057\u3066\u307f\u305f\u3044\u3067\u3059\uff0e\n\n# \u66f4\u65b0\u5c65\u6b74\n\n2016/10/30 \u2026\u2026 Variable \u306e\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0\u304c\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u305f\u3081\uff0c\u30b3\u30fc\u30c9\u3092\u4e00\u90e8\u4fee\u6b63\u3057\u307e\u3057\u305f\uff0e\n", "tags": ["Python", "Chainer"]}