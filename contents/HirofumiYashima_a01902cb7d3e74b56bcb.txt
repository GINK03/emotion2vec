{"context": "\u6587\u66f8\u5206\u985e\u306e\u4e8b\u4f8b \u304c\u3001fchollet \u306e GitHub \u306b \u3042\u3063\u305f\u3002\n\nstackoverflow Keras - 1D Convolution How it works\ngithub.com/fchollet/keras/blob/master/examples/imdb_cnn.py\n\n\n'''This example demonstrates the use of Convolution1D for text classification. \nGets to 0.89 test accuracy after 2 epochs. \n90s/epoch on Intel i5 2.4Ghz CPU. \n10s/epoch on Tesla K40 GPU. \n\n''' \n\n\nfrom __future__ import print_function \nimport numpy as np \nnp.random.seed(1337)  # for reproducibility \n\n\nfrom keras.preprocessing import sequence \nfrom keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Activation \nfrom keras.layers import Embedding \nfrom keras.layers import Convolution1D, GlobalMaxPooling1D \nfrom keras.datasets import imdb \n\n\n\n\n# set parameters: \nmax_features = 5000 \nmaxlen = 400 \nbatch_size = 32 \nembedding_dims = 50 \nnb_filter = 250 \nfilter_length = 3 \nhidden_dims = 250 \nnb_epoch = 2 \n\n\nprint('Loading data...') \n(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features) \nprint(len(X_train), 'train sequences') \nprint(len(X_test), 'test sequences') \n\n\nprint('Pad sequences (samples x time)') \nX_train = sequence.pad_sequences(X_train, maxlen=maxlen) \nX_test = sequence.pad_sequences(X_test, maxlen=maxlen) \nprint('X_train shape:', X_train.shape) \nprint('X_test shape:', X_test.shape) \n\n\nprint('Build model...') \nmodel = Sequential() \n\n\n# we start off with an efficient embedding layer which maps \n# our vocab indices into embedding_dims dimensions \nmodel.add(Embedding(max_features, \n                    embedding_dims, \n                    input_length=maxlen, \n                    dropout=0.2)) \n\n\n# we add a Convolution1D, which will learn nb_filter \n# word group filters of size filter_length: \nmodel.add(Convolution1D(nb_filter=nb_filter, \n                        filter_length=filter_length, \n                        border_mode='valid', \n                        activation='relu', \n                        subsample_length=1)) \n# we use max pooling: \nmodel.add(GlobalMaxPooling1D()) \n\n\n# We add a vanilla hidden layer: \nmodel.add(Dense(hidden_dims)) \nmodel.add(Dropout(0.2)) \nmodel.add(Activation('relu')) \n\n\n# We project onto a single unit output layer, and squash it with a sigmoid: \nmodel.add(Dense(1)) \nmodel.add(Activation('sigmoid')) \n\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy']) \nmodel.fit(X_train, y_train, \n          batch_size=batch_size, \n          nb_epoch=nb_epoch, \n          validation_data=(X_test, y_test)) \n\n\n\n\n__\u6587\u66f8\u5206\u985e\u306e\u4e8b\u4f8b \u304c\u3001fchollet \u306e GitHub \u306b \u3042\u3063\u305f\u3002__\n\n* [stackoverflow _Keras - 1D Convolution How it works_](http://stackoverflow.com/questions/40121337/keras-1d-convolution-how-it-works)\n* [github.com/fchollet/keras/blob/master/examples/imdb_cnn.py](https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py)\n\n>```{python:}\n>'''This example demonstrates the use of Convolution1D for text classification. \n> Gets to 0.89 test accuracy after 2 epochs. \n> 90s/epoch on Intel i5 2.4Ghz CPU. \n> 10s/epoch on Tesla K40 GPU. \n>  \n> ''' \n> \n>\n> from __future__ import print_function \n> import numpy as np \n> np.random.seed(1337)  # for reproducibility \n> \n>\n> from keras.preprocessing import sequence \n> from keras.models import Sequential \n> from keras.layers import Dense, Dropout, Activation \n> from keras.layers import Embedding \n> from keras.layers import Convolution1D, GlobalMaxPooling1D \n> from keras.datasets import imdb \n> \n> \n> \n> \n> # set parameters: \n> max_features = 5000 \n> maxlen = 400 \n> batch_size = 32 \n> embedding_dims = 50 \n> nb_filter = 250 \n> filter_length = 3 \n> hidden_dims = 250 \n> nb_epoch = 2 \n> \n>\n> print('Loading data...') \n> (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features) \n> print(len(X_train), 'train sequences') \n> print(len(X_test), 'test sequences') \n> \n> \n> print('Pad sequences (samples x time)') \n> X_train = sequence.pad_sequences(X_train, maxlen=maxlen) \n> X_test = sequence.pad_sequences(X_test, maxlen=maxlen) \n> print('X_train shape:', X_train.shape) \n> print('X_test shape:', X_test.shape) \n> \n> \n> print('Build model...') \n> model = Sequential() \n> \n> \n> # we start off with an efficient embedding layer which maps \n> # our vocab indices into embedding_dims dimensions \n> model.add(Embedding(max_features, \n>                     embedding_dims, \n>                     input_length=maxlen, \n>                     dropout=0.2)) \n> \n> \n> # we add a Convolution1D, which will learn nb_filter \n> # word group filters of size filter_length: \n> model.add(Convolution1D(nb_filter=nb_filter, \n>                         filter_length=filter_length, \n>                         border_mode='valid', \n>                         activation='relu', \n>                         subsample_length=1)) \n> # we use max pooling: \n> model.add(GlobalMaxPooling1D()) \n> \n> \n> # We add a vanilla hidden layer: \n> model.add(Dense(hidden_dims)) \n> model.add(Dropout(0.2)) \n> model.add(Activation('relu')) \n> \n> \n> # We project onto a single unit output layer, and squash it with a sigmoid: \n> model.add(Dense(1)) \n> model.add(Activation('sigmoid')) \n> \n> \n> model.compile(loss='binary_crossentropy', \n>               optimizer='adam', \n>               metrics=['accuracy']) \n> model.fit(X_train, y_train, \n>           batch_size=batch_size, \n>           nb_epoch=nb_epoch, \n>           validation_data=(X_test, y_test)) \n>```\n\n\n", "tags": ["Keras", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "NLP", "\u30c7\u30fc\u30bf\u5206\u6790"]}