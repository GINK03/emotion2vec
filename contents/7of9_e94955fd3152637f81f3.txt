{"tags": ["link", "borgWarp"], "context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n\n\nTensorFlow\u3092\u4f7f\u3063\u3066\u3001input:100 node, output:100 node\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u3092\u8a66\u884c\u4e2d\u3002\nTensorFlow\u3067 input:100, output:100\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306e\u5b66\u7fd2\u3092\u691c\u8a0e v0.9 / v0.8 > output \u306eactivation function\u3092\uff13\u3064\u8a66\u3057\u305f > Linear / softplus / sigmoid\u306b\u304a\u3044\u3066\u8a66\u3057\u305factivation function\u306b\u3064\u3044\u3066\u3042\u307e\u308a\u7406\u89e3\u3067\u304d\u3066\u3044\u306a\u3044\u3002\n\u300csoftplus sigmoid\u300d\u3092\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u4ee5\u4e0b\u3092\u898b\u3064\u3051\u305f\u3002\nWhat is special about rectifier neural units used in NN learning?\n\u4ee5\u4e0b\u306f\u6c17\u306b\u306a\u3063\u305f\u90e8\u5206\u306e\u629c\u7c8b\u3002\n\nSuvash Sedhain\u6c0f\u306e\u56de\u7b54\n\nThe softplus function can be approximated by max function (or hard max ) ie max(0,x+N(0,1)) . The max function is commonly known as Rectified Linear Function (ReL).\n\nReLU\u3067\u306a\u304fReL\u3068\u3044\u3046\u30ad\u30fc\u30ef\u30fc\u30c9\u3082\u3042\u308b\u3088\u3046\u3060\u3002\n\nSigmoid function has range [0,1] whereas the ReL function has range [0,\u221e]. Hence sigmoid function can be used to model probability, whereas ReL can be used to model positive real number.\n\n\u3053\u306e\u7bc4\u56f2\u306e\u9055\u3044\u306f\u5b66\u7fd2\u306e\u5dee\u7570\u3092\u751f\u3058\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\nThe gradient of the sigmoid function vanishes as we increase or decrease x. However, the gradient of the ReL function doesn't vanish as we increase x.\n\n(\u4e2d\u7565)\n\nReLU doesn't face gradient vanishing problem as with sigmoid and tanh function. Also, It has been shown that deep networks can be trained efficiently using ReLU even without pre-training.\n\n\nAlexandre Holden Daly\u6c0f\u306e\u56de\u7b54\ngradient\u304c\u6d88\u3048\u308b\u3001\u6d88\u3048\u306a\u3044\u306b\u3064\u3044\u3066\u306e\u8a73\u7d30\u306a\u8a18\u8ff0\u3078\u306e\u30ea\u30f3\u30af\u304c\u3042\u308b\u3002\n\nAnonymous\nReLU\u3092\u4f7f\u3046\u3068\u901f\u3044\u7406\u7531\u306b\u3064\u3044\u3066\u3000\n\nChatavut Viriyasuthee\u6c0f\u306e\u56de\u7b54\n\nSince a softplus is relatively computationally expensive to compute, people have invented a neat approximation to it - Rectified linear\n\n\nAnirbit Mukherjee\u6c0f\u306e\u56de\u7b54\na lot of exciting ideas about these networks\u306b\u3064\u3044\u3066\u306e\u8ad6\u6587\u30ea\u30f3\u30af\u304c\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u3002\n\nFor example we show that with one hidden layer and with one input these networks are trainable in polynomial time!\n\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n```\n\nTensorFlow\u3092\u4f7f\u3063\u3066\u3001input:100 node, output:100 node\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u3092\u8a66\u884c\u4e2d\u3002\n\n[TensorFlow\u3067 input:100, output:100\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306e\u5b66\u7fd2\u3092\u691c\u8a0e v0.9 / v0.8 > output \u306eactivation function\u3092\uff13\u3064\u8a66\u3057\u305f > Linear / softplus / sigmoid](http://qiita.com/7of9/items/31d93a6bc53e0c3ef02a)\u306b\u304a\u3044\u3066\u8a66\u3057\u305factivation function\u306b\u3064\u3044\u3066\u3042\u307e\u308a\u7406\u89e3\u3067\u304d\u3066\u3044\u306a\u3044\u3002\n\n\u300csoftplus sigmoid\u300d\u3092\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u4ee5\u4e0b\u3092\u898b\u3064\u3051\u305f\u3002\n\n[What is special about rectifier neural units used in NN learning?](https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning)\n\n\u4ee5\u4e0b\u306f\u6c17\u306b\u306a\u3063\u305f\u90e8\u5206\u306e\u629c\u7c8b\u3002\n\n### Suvash Sedhain\u6c0f\u306e\u56de\u7b54\n\n> The softplus function can be approximated by max function (or hard max ) ie max(0,x+N(0,1)) . The max function is commonly known as Rectified Linear Function (ReL).\n\nReLU\u3067\u306a\u304fReL\u3068\u3044\u3046\u30ad\u30fc\u30ef\u30fc\u30c9\u3082\u3042\u308b\u3088\u3046\u3060\u3002\n\n> Sigmoid function has range [0,1] whereas the ReL function has range [0,\u221e]. Hence sigmoid function can be used to model probability, whereas ReL can be used to model positive real number.\n\n\u3053\u306e\u7bc4\u56f2\u306e\u9055\u3044\u306f\u5b66\u7fd2\u306e\u5dee\u7570\u3092\u751f\u3058\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\n> The gradient of the sigmoid function vanishes as we increase or decrease x. However, the gradient of the ReL function doesn't vanish as we increase x.\n\n(\u4e2d\u7565)\n\n> ReLU doesn't face gradient vanishing problem as with sigmoid and tanh function. Also, It has been shown that deep networks can be trained efficiently using ReLU even without pre-training.\n\n### Alexandre Holden Daly\u6c0f\u306e\u56de\u7b54\n\ngradient\u304c\u6d88\u3048\u308b\u3001\u6d88\u3048\u306a\u3044\u306b\u3064\u3044\u3066\u306e\u8a73\u7d30\u306a\u8a18\u8ff0\u3078\u306e\u30ea\u30f3\u30af\u304c\u3042\u308b\u3002\n\n### Anonymous\n\nReLU\u3092\u4f7f\u3046\u3068\u901f\u3044\u7406\u7531\u306b\u3064\u3044\u3066\u3000\n\n### Chatavut Viriyasuthee\u6c0f\u306e\u56de\u7b54\n\n> Since a softplus is relatively computationally expensive to compute, people have invented a neat approximation to it - Rectified linear\n\n### Anirbit Mukherjee\u6c0f\u306e\u56de\u7b54\n\na lot of exciting ideas about these networks\u306b\u3064\u3044\u3066\u306e\u8ad6\u6587\u30ea\u30f3\u30af\u304c\u63b2\u8f09\u3055\u308c\u3066\u3044\u308b\u3002\n\n> For example we show that with one hidden layer and with one input these networks are trainable in polynomial time!\n\n\n\n\n\n"}