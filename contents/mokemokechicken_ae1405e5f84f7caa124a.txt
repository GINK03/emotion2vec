{"context": " More than 1 year has passed since last update.\n\n\u306f\u3058\u3081\u306b\n\u4ee5\u524d \u8db3\u3057\u7b97\u30b2\u30fc\u30e0\u3092\u5f37\u5316\u5b66\u7fd2\u3067\u5b66\u7fd2\u3067\u304d\u308b\u304b\uff1f \u3092\u8a66\u3057\u3066\u307f\u3066\u3001\u554f\u984c\u306a\u304f\u5b66\u7fd2\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306f\u3082\u3046\u5c11\u3057\u73fe\u5b9f\u7684\u306a\u554f\u984c\u3092\u60f3\u5b9a\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\nWeb\u30b5\u30a4\u30c8\u306b\u6765\u308b\u30e6\u30fc\u30b6\u306b\u5bfe\u3057\u3066\u3001Web\u30b5\u30a4\u30c8\u904b\u55b6\u8005\u304c\u300c\u3042\u308b\u30a2\u30af\u30b7\u30e7\u30f3(\u30e1\u30fc\u30eb\uff1f\u30af\u30fc\u30dd\u30f3\uff1f\u306a\u3069\uff09\u300d\u3092\u8d77\u3053\u3059\u3068\u3001\u671b\u307e\u3057\u3044\u884c\u52d5(\u305d\u306e\u30e6\u30fc\u30b6\u304c\u4f55\u304b\u8cfc\u5165\u3059\u308b\u306a\u3069)\u3092\u53d6\u308b\u3001\u3068\u3057\u307e\u3059\n\u305d\u306e\u6642\u3001\u3069\u306e\u30e6\u30fc\u30b6\u306b\u3069\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u3069\u306e\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u53d6\u308c\u3070\u3044\u3044\u304b\u3001\u3092\u77e5\u308a\u305f\u3044\n\n\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\n\u307e\u3042\u3001\u30e1\u30fc\u30eb\u3050\u3089\u3044\u306a\u3089\u5168\u54e1\u306b\u9001\u308c\u3070\u826f\u3044\u3058\u3083\u3093\u7684\u306a\u8a71\u306f\u3042\u308a\u307e\u3059\u304c\u3001\u9001\u308a\u3059\u304e\u308b\u3068\u96e2\u8131\u306b\u7e4b\u304c\u308a\u307e\u3059\u3057\u3001\u30af\u30fc\u30dd\u30f3\u306f\u30b3\u30b9\u30c8\u3082\u304b\u304b\u308b\u306e\u3067\u3042\u307e\u308a\u4e71\u767a\u3057\u305f\u304f\u306f\u306a\u3044\u3067\u3059\u3002\n\u3053\u306e\u554f\u984c\u3092 Q-Learning\u7684\u306a\u67a0\u7d44\u307f\u3067\u3084\u3063\u305f\u3089\u3069\u3046\u306a\u308b\u306e\u3060\u308d\u3046\u304b\u3001\u3068\u3044\u3046\u306e\u304c\u4eca\u56de\u306e\u304a\u984c\u3067\u3059\u3002\nQ-Learning\u3060\u3068\u3001\u30a2\u30af\u30b7\u30e7\u30f3\u304c\u8907\u6570\u306b\u306a\u3063\u3066\u3082\u5bfe\u5fdc\u3067\u304d\u308b\u306e\u304c\u826f\u3044\u6240\u3067\u3059\u3002\n\u3068\u3044\u3063\u3066\u3082\u3001\u7c21\u5358\u306a\u5b8c\u5168\u306b\u4eee\u60f3\u7684\u306a\u30b7\u30c1\u30e5\u30a8\u30fc\u30b7\u30e7\u30f3\u3067\u8003\u3048\u307e\u3059\u3002\n\n\u304a\u984c\uff1aQ-Learning\u3067\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u7cfb\u306eAction\u3092\u884c\u3046\u3079\u304d\u304b\u306e\u5224\u65ad\u304c\u3067\u304d\u308b\u304b\uff1f \u30b2\u30fc\u30e0\n\n\u30b2\u30fc\u30e0\n\u7c21\u5358\u306b\u66f8\u304f\u3068\n\n\u30e6\u30fc\u30b6\u306e\u884c\u52d5U\u306f 0~3\n\u904b\u55b6\u8005\u306e\u30a2\u30af\u30b7\u30e7\u30f3A\u306f 0~1\nU=2 \u306e\u3068\u304d\u306b A=1 \u3092\u884c\u3046\u3068\u826f\u3044\n\u305f\u3060\u3057\u5831\u916c\u306f\u3059\u3050\u306b\u306f\u5f97\u3089\u308c\u308c\u305a\u30014\u30bf\u30fc\u30f3\u5f8c\u306b+1\u3092\u5f97\u3089\u308c\u308b\nU=2\u3058\u3083\u306a\u3044\u3068\u304d\u306b\u3001A=1\u3092\u884c\u3046\u3068\u30da\u30ca\u30eb\u30c6\u30a3\n\u72b6\u614bS\u306f (U,A)\u306e5\u56de\u5206\u306e\u5c65\u6b74\n\n\u3068\u3044\u3046\u611f\u3058\u3067\u3059\u3002\n\n\u4f55\u6545\u3053\u3093\u306a\u30b2\u30fc\u30e0\u3092\u8003\u3048\u305f\u306e\u304b\n\u3053\u306e\u30b2\u30fc\u30e0\u306e\u5834\u5408\u3001\u300c\u6700\u9069\u306a\u884c\u52d5\u3092\u3068\u3063\u3066\u304b\u3089\u5831\u916c\u3092\u8cb0\u3048\u308b\u307e\u3067\u306b\u9593\u9694\u304c\u3042\u308b\u300d\u3068\u3044\u3046\u306e\u304c\u96e3\u3057\u3044\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u5fc3\u914d\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u6700\u9069\u306a\u884c\u52d5\u306e\u76f4\u5f8c\u306b\u5831\u916c\u304c\u8cb0\u3048\u308c\u3070\u5b66\u7fd2\u304c\u65e9\u305d\u3046\u306a\u306e\u3067\u3059\u304c\u3001\u305d\u3046\u3067\u306f\u306a\u3044\u3068\u3053\u308d\u304c\u6700\u5927\u306e\u30dd\u30a4\u30f3\u30c8\u3067\u3059\u3002\u672c\u5f53\u306f\u300c\u30da\u30ca\u30eb\u30c6\u30a3\u300d\u306e\u5b9a\u7fa9\u3082\u96e3\u3057\u3044\u306e\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u4eca\u306f\u5358\u7d14\u306b\u9593\u9055\u3063\u305f\u3063\u307d\u3044\u30a2\u30af\u30b7\u30e7\u30f3\u306b\u5bfe\u3057\u3066\u3059\u3050\u306b\u4e0e\u3048\u3066\u3044\u307e\u3059\u3002\uff08\u3053\u308c\u3060\u3068\u554f\u984c\u304c\u7c21\u5358\u3059\u304e\u3066\u3057\u307e\u3046\u306e\u304b\u30fb\u30fb\u30fb\uff1f\uff09\u3002\n\u305f\u3060\u3001\u5f37\u5316\u5b66\u7fd2\u3067\u8ff7\u8def\u3092\u63a2\u7d22\u3059\u308b \u306b\u3082\u3042\u308b\u3088\u3046\u306b\u3001\u9061\u3063\u3066\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u3067\u304d\u308b\u306e\u3060\u308d\u3046\u3068\u306f\u601d\u3044\u3064\u3064\u3082\u3061\u3087\u3063\u3068\u691c\u8a3c\u3057\u3066\u307f\u305f\u304b\u3063\u305f\u3068\u3044\u3046\u3068\u3053\u308d\u3067\u3059\u3002\n\u203b \u3061\u306a\u307f\u306b\u4e0a\u8a18\u306e\u5f37\u5316\u5b66\u7fd2\u306e\u8ff7\u8def\u3092\u5b66\u7fd2\u3057\u3066\u3044\u304f\u52d5\u753b\u306f\u9762\u767d\u3044\u306e\u3067\u8208\u5473\u304c\u3042\u308c\u3070\u662f\u975e\u307f\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\uff01\n\n\u7d50\u679c\n\u5831\u916c\u304c\u8cb0\u3048\u308b\u306e\u306f\u3001U=2\u304c\u767a\u751f\u3057\u305f\u6642\u306e\u307f\u3067\u3059\u306e\u3067\u3001U=2\u304c\u767a\u751f\u3057\u305f\u6642\u306b chance_count \u3068\u3044\u3046\u306e\u3092\u5897\u3084\u3057\u307e\u3059\u3002\u3042\u308b\u4e00\u5b9a\u671f\u9593\u306e\u9593\u306b\u5f97\u3089\u308c\u308b\u5831\u916c\u306e\u6700\u5927\u306f chance_count \u306b\u306a\u308a\u307e\u3059\u3002\n\u305d\u3053\u3067\u3001 \u5f97\u3089\u308c\u305f\u5831\u916c / chance_count \u3092 hit_rate \u3068\u3057\u3066\u3001\n10000\u56de\u306e\u5b66\u7fd2\u30fb\u8a55\u4fa1\u6bce\u306b\u3069\u3046\u5909\u5316\u3057\u305f\u304b\u3092\u4ee5\u4e0b\u306e\u56f3\u306b\u793a\u3057\u307e\u3059\u3002\n\n5000\u4e07\u56de\u307b\u3069\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u304c\u30013000\u4e07\u56de\u304f\u3089\u3044\u3067\u5b66\u7fd2\u306f\u982d\u6253\u3061\u306b\u306a\u3063\u3066\u3044\u308b\u611f\u3058\u3067\u3001\u304a\u3088\u305d hit_rate=0.9 \u304f\u3089\u3044\u3067\u3057\u305f\u3002\n\n\u8003\u5bdf\n\n\u4e00\u5fdc\u826f\u3044\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u5b66\u7fd2\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u554f\u984c\u3092\u8907\u96d1\u306b\u3057\u3066\u3044\u3063\u3066\u3001\u3069\u3053\u307e\u3067\u8ffd\u968f\u3067\u304d\u308b\u306e\u304b\u306f\u4eca\u5f8c\u306e\u8ab2\u984c\u306b\u306a\u308a\u305d\u3046\u3067\u3059\u3002\n\u5272\u308a\u3068\u7c21\u5358\u306a\u30eb\u30fc\u30eb\u306a\u306e\u306b 100% \u306b\u306a\u3089\u306a\u3044\u306e\u304c\u5c11\u3057\u610f\u5916\u3067\u3057\u305f\n\n\n\u5c40\u6240\u89e3\uff1f\uff1f\u30d0\u30b0\uff1f\uff1f\n\n\n\u5b66\u7fd2\u56de\u6570\u3082\u7d50\u69cb\u591a\u3044\u306a\u3041\u3001\u3068\u3002\u72b6\u614b\u6570\u306f U(4)*A(2)^HISTORY(5) -> 32768 \u304f\u3089\u3044\u3067\u3059\u304c\u3001\u3001\u3001\u3067\u3082\u307e\u3042\u3001\u305d\u3093\u306a\u3082\u306e\u306a\u306e\u304b\u306a\u3002\u3002\n\u5b9f\u969b\u306b\u306fDeep Q Learning\u7684\u306b\u884c\u308f\u306a\u3044\u3068\u5c11\u306a\u304f\u3068\u3082\u5b9f\u7528\u6027\u306f\u306a\u3055\u305d\u3046\u3067\u3059\u304c\u3001DeepLearning\u7684\u306b\u3084\u308b\u3068\u5b66\u7fd2\u306b\u66f4\u306a\u308b\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3001\u4f55\u304b\u3046\u307e\u3044\u5de5\u592b\u304c\u5fc5\u8981\u305d\u3046\u3067\u3059\n\u5b9f\u969b\u306b\u306f\u30da\u30ca\u30eb\u30c6\u30a3\u304c\u3082\u3046\u5c11\u3057\u308f\u304b\u308a\u306b\u304f\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\uff1f\n\n\n\u4f8b\u3048\u3070\u3001\u30e6\u30fc\u30b6\u304c\u96e2\u8131\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u305a\u30fc\u3063\u3068U=0\u304c\u7d9a\u304f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u304c\u3001A\u3068\u306e\u56e0\u679c\u95a2\u4fc2\u306f\u672c\u5f53\u306b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\n\u3067\u3082\u305d\u308c\u4ee5\u5916\u306f\u3001U\u306e\u30de\u30a4\u30ca\u30b9\u306a\u884c\u52d5\uff08\u9000\u4f1a\u3001\u60aa\u53e3\u3001\u30af\u30ec\u30fc\u30e0\uff09\u3084A\u306e\u30b3\u30b9\u30c8\uff08\u8cbb\u7528\uff09\u306a\u3069\u3068\u3059\u308c\u3070\u308f\u304b\u308a\u3084\u3059\u3044\u3068\u3044\u3048\u3070\u308f\u304b\u308a\u3084\u3059\u3044\u306e\u304b\u3002\n\n\n\n\n\u3055\u3044\u3054\u306b\nQ Learning\u306f\u3042\u308b\u610f\u5473\u5358\u7d14\u306aAI\u3063\u307d\u3044\u306e\u3067\u697d\u3057\u3044\u3067\u3059\u3002\n\u4f55\u304b\u306b\u4f7f\u3048\u308b\u3068\u3044\u3044\u3093\u3060\u3051\u3069\u306a\u3041\u3002\n\n\u30b3\u30fc\u30c9\n\u53c2\u8003\u307e\u3067\u306b\u8cbc\u3063\u3066\u304a\u304d\u307e\u3059\u3002\n#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nfrom random import random, choice\n\n\nclass Game(object):\n    state = None\n    actions = None\n    game_over = False\n\n    def __init__(self, player):\n        self.player = player\n        self.turn = 0\n        self.last_reward = 0\n        self.total_reward = 0\n        self.init_state()\n\n    def player_action(self):\n        action = self.player.action(self.state, self.last_reward)\n        if action not in self.actions:\n            raise Exception(\"Invalid Action: '%s'\" % action)\n        self.state, self.last_reward = self.get_next_state_and_reward(self.state, action)\n\n    def play(self):\n        yield(self)\n        while not self.game_over:\n            self.player_action()\n            self.turn += 1\n            self.total_reward += self.last_reward\n            yield(self)\n\n    def init_state(self):\n        raise NotImplemented()\n\n    def get_next_state_and_reward(self, state, action):\n        raise NotImplemented()\n\n\nclass UserAndPushEventGame(Game):\n    \"\"\"\n    State           :S : list of (U, A)\n    UserActivity    :U : int of 0~3\n    Action          :A : int of 0 or 1\n\n    Next-State(S, A):S':\n        S[-1][1] = A\n        S.append((Next-U, None))\n        S = S[-5:]\n    Next-U          :  :\n        if S[-4] == (2, 1) then 3\n        else 10% -> 2, 10% -> 1, 80% -> 0\n    Reward(S, A)    :R :\n        if S[-1] == (3, *) then R += 1\n        wrong_action_count := Number of ({0,1,3}, 1) in S\n        R -= wrong_action_count * 0.3\n    \"\"\"\n\n    STATE_HISTORY_SIZE = 5\n\n    def init_state(self):\n        self.actions = [0, 1]\n        self.state = [(0, None)]\n        self.chance_count = 0\n\n    def get_next_state_and_reward(self, state, action):\n        next_state = (state + [(self.next_user_action(state), None)])[-self.STATE_HISTORY_SIZE:]\n        next_state[-2] = (next_state[-2][0], action)\n        reward = 0\n        if len(state) > 0 and state[-1][0] == 3:\n            reward += 1\n        action_count = reduce(lambda t, x: t+(x[1] or 0), state, 0)\n        correct_action_count = len([0 for x in state if x == (2, 1)])\n        wrong_action_count = action_count - correct_action_count\n        reward -= wrong_action_count * 0.3\n        return next_state, reward\n\n    def next_user_action(self, state):\n        if len(state) > 4 and state[-4] == (2, 1):\n            return 3\n        else:\n            rnd = np.random.random()\n            if rnd < 0.8:\n                return 0\n            elif rnd < 0.9:\n                return 1\n            else:\n                self.chance_count += 1\n                return 2\n\n\nclass HumanPlayer(object):\n    training = False\n\n    def action(self, state, last_reward):\n        print \"LastReward=%s, CurrentState: %s\" % (last_reward, state)\n        while True:\n            action_input = raw_input(\"Enter 0~1: \")\n            if int(action_input) in [0, 1]:\n                return int(action_input)\n\n\nclass QLearnPlayer(object):\n    ALPHA = 0.1\n    GAMMA = 0.99\n    E_GREEDY = 0.05\n\n    def __init__(self):\n        self.actions = [0, 1]\n        self.q_table = {}\n        self.last_state = self.last_action = None\n        self.training = True\n\n    def get_q_value(self, state, action):\n        return self.q_table.get(state, {}).get(action, (np.random.random() - 0.5)/1000)  # \u672a\u5b9a\u7fa9\u306f\u5c0f\u3055\u3044\u4e71\u6570\u3092\u8fd4\u3059\n\n    def get_all_q_values(self, state):\n        return [self.get_q_value(state, act) for act in self.actions]\n\n    def set_q_value(self, state, action, val):\n        if state in self.q_table:\n            self.q_table[state][action] = val\n        else:\n            self.q_table[state] = {action: val}\n\n    def action(self, state, last_reward):\n        state = tuple(state)\n        next_action = self.select_action(state)\n        if self.last_state is not None:\n            self.update_q_table(self.last_state, self.last_action, state, last_reward)\n        self.last_state = state\n        self.last_action = next_action\n        return next_action\n\n    def select_action(self, state):\n        if self.training and random() < self.E_GREEDY:\n            return choice(self.actions)\n        else:\n            return np.argmax(self.get_all_q_values(state))\n\n    def update_q_table(self, last_state, last_action, cur_state, last_reward):\n        if self.training:\n            d = last_reward + np.max(self.get_all_q_values(cur_state)) * self.GAMMA - self.get_q_value(last_state, last_action)\n            self.set_q_value(last_state, last_action, self.get_q_value(last_state, last_action) + self.ALPHA * d)\n\n\nif __name__ == '__main__':\n    SWITCH_MODE_TURN_NUM = 10000\n    fp = file(\"result.txt\", \"w\")\n    dt = file(\"detail.txt\", \"w\")\n    player = QLearnPlayer()\n    # player = HumanPlayer()\n    game = UserAndPushEventGame(player)\n    last_chance_count = last_score = 0\n    for g in game.play():\n        # dt.write(\"%s: isT?=%s LastReward=%s TotalReward=%s S=%s\\n\" %\n        #          (g.turn, player.training, g.last_reward, g.total_reward, g.state))\n        if g.turn % SWITCH_MODE_TURN_NUM == 0:\n            if not player.training:\n                this_term_score = game.total_reward - last_score\n                this_term_chance = game.chance_count - last_chance_count\n                if this_term_chance > 0:\n                    hit_rate = 100.0*this_term_score/this_term_chance\n                else:\n                    hit_rate = 0\n                # print \"Turn=%d: This 100 turn score=%2.2f chance=%02d: HitRate=%.1f%% %s\" % \\\n                #       (g.turn, this_term_score, this_term_chance, hit_rate, '*' * int(hit_rate/2))\n                fp.write(\"%d\\t%.2f\\t%d\\t%f\\n\" % (g.turn, this_term_score, this_term_chance, hit_rate))\n            last_score = game.total_reward\n            last_chance_count = game.chance_count\n            player.training = not player.training\n        if g.turn % 10000 == 0:\n            fp.flush()\n\n\n\n\u306f\u3058\u3081\u306b\n========\n\n\u4ee5\u524d [\u8db3\u3057\u7b97\u30b2\u30fc\u30e0\u3092\u5f37\u5316\u5b66\u7fd2\u3067\u5b66\u7fd2\u3067\u304d\u308b\u304b\uff1f](http://qiita.com/mokemokechicken/items/f1b71170d332c0543db8) \u3092\u8a66\u3057\u3066\u307f\u3066\u3001\u554f\u984c\u306a\u304f\u5b66\u7fd2\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n\n\u4eca\u56de\u306f\u3082\u3046\u5c11\u3057\u73fe\u5b9f\u7684\u306a\u554f\u984c\u3092\u60f3\u5b9a\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n* Web\u30b5\u30a4\u30c8\u306b\u6765\u308b\u30e6\u30fc\u30b6\u306b\u5bfe\u3057\u3066\u3001Web\u30b5\u30a4\u30c8\u904b\u55b6\u8005\u304c\u300c\u3042\u308b\u30a2\u30af\u30b7\u30e7\u30f3(\u30e1\u30fc\u30eb\uff1f\u30af\u30fc\u30dd\u30f3\uff1f\u306a\u3069\uff09\u300d\u3092\u8d77\u3053\u3059\u3068\u3001\u671b\u307e\u3057\u3044\u884c\u52d5(\u305d\u306e\u30e6\u30fc\u30b6\u304c\u4f55\u304b\u8cfc\u5165\u3059\u308b\u306a\u3069)\u3092\u53d6\u308b\u3001\u3068\u3057\u307e\u3059\n* \u305d\u306e\u6642\u3001\u3069\u306e\u30e6\u30fc\u30b6\u306b\u3069\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u3069\u306e\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u53d6\u308c\u3070\u3044\u3044\u304b\u3001\u3092\u77e5\u308a\u305f\u3044\n\n\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\n\u307e\u3042\u3001\u30e1\u30fc\u30eb\u3050\u3089\u3044\u306a\u3089\u5168\u54e1\u306b\u9001\u308c\u3070\u826f\u3044\u3058\u3083\u3093\u7684\u306a\u8a71\u306f\u3042\u308a\u307e\u3059\u304c\u3001\u9001\u308a\u3059\u304e\u308b\u3068\u96e2\u8131\u306b\u7e4b\u304c\u308a\u307e\u3059\u3057\u3001\u30af\u30fc\u30dd\u30f3\u306f\u30b3\u30b9\u30c8\u3082\u304b\u304b\u308b\u306e\u3067\u3042\u307e\u308a\u4e71\u767a\u3057\u305f\u304f\u306f\u306a\u3044\u3067\u3059\u3002\n\n\u3053\u306e\u554f\u984c\u3092 Q-Learning\u7684\u306a\u67a0\u7d44\u307f\u3067\u3084\u3063\u305f\u3089\u3069\u3046\u306a\u308b\u306e\u3060\u308d\u3046\u304b\u3001\u3068\u3044\u3046\u306e\u304c\u4eca\u56de\u306e\u304a\u984c\u3067\u3059\u3002\nQ-Learning\u3060\u3068\u3001\u30a2\u30af\u30b7\u30e7\u30f3\u304c\u8907\u6570\u306b\u306a\u3063\u3066\u3082\u5bfe\u5fdc\u3067\u304d\u308b\u306e\u304c\u826f\u3044\u6240\u3067\u3059\u3002\n\u3068\u3044\u3063\u3066\u3082\u3001\u7c21\u5358\u306a\u5b8c\u5168\u306b\u4eee\u60f3\u7684\u306a\u30b7\u30c1\u30e5\u30a8\u30fc\u30b7\u30e7\u30f3\u3067\u8003\u3048\u307e\u3059\u3002\n\n\u304a\u984c\uff1aQ-Learning\u3067\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u7cfb\u306eAction\u3092\u884c\u3046\u3079\u304d\u304b\u306e\u5224\u65ad\u304c\u3067\u304d\u308b\u304b\uff1f \u30b2\u30fc\u30e0\n===========\n\n\u30b2\u30fc\u30e0\n-------\n\u7c21\u5358\u306b\u66f8\u304f\u3068\n\t\n* \u30e6\u30fc\u30b6\u306e\u884c\u52d5U\u306f 0~3\n* \u904b\u55b6\u8005\u306e\u30a2\u30af\u30b7\u30e7\u30f3A\u306f 0~1\n* U=2 \u306e\u3068\u304d\u306b A=1 \u3092\u884c\u3046\u3068\u826f\u3044\n* \u305f\u3060\u3057\u5831\u916c\u306f\u3059\u3050\u306b\u306f\u5f97\u3089\u308c\u308c\u305a\u30014\u30bf\u30fc\u30f3\u5f8c\u306b`+1`\u3092\u5f97\u3089\u308c\u308b\n* U=2\u3058\u3083\u306a\u3044\u3068\u304d\u306b\u3001A=1\u3092\u884c\u3046\u3068\u30da\u30ca\u30eb\u30c6\u30a3\n* \u72b6\u614bS\u306f (U,A)\u306e5\u56de\u5206\u306e\u5c65\u6b74\n\n\u3068\u3044\u3046\u611f\u3058\u3067\u3059\u3002\n\n\u4f55\u6545\u3053\u3093\u306a\u30b2\u30fc\u30e0\u3092\u8003\u3048\u305f\u306e\u304b\n---------\n\n\u3053\u306e\u30b2\u30fc\u30e0\u306e\u5834\u5408\u3001\u300c\u6700\u9069\u306a\u884c\u52d5\u3092\u3068\u3063\u3066\u304b\u3089\u5831\u916c\u3092\u8cb0\u3048\u308b\u307e\u3067\u306b\u9593\u9694\u304c\u3042\u308b\u300d\u3068\u3044\u3046\u306e\u304c\u96e3\u3057\u3044\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u5fc3\u914d\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u6700\u9069\u306a\u884c\u52d5\u306e\u76f4\u5f8c\u306b\u5831\u916c\u304c\u8cb0\u3048\u308c\u3070\u5b66\u7fd2\u304c\u65e9\u305d\u3046\u306a\u306e\u3067\u3059\u304c\u3001\u305d\u3046\u3067\u306f\u306a\u3044\u3068\u3053\u308d\u304c\u6700\u5927\u306e\u30dd\u30a4\u30f3\u30c8\u3067\u3059\u3002\u672c\u5f53\u306f\u300c\u30da\u30ca\u30eb\u30c6\u30a3\u300d\u306e\u5b9a\u7fa9\u3082\u96e3\u3057\u3044\u306e\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u4eca\u306f\u5358\u7d14\u306b\u9593\u9055\u3063\u305f\u3063\u307d\u3044\u30a2\u30af\u30b7\u30e7\u30f3\u306b\u5bfe\u3057\u3066\u3059\u3050\u306b\u4e0e\u3048\u3066\u3044\u307e\u3059\u3002\uff08\u3053\u308c\u3060\u3068\u554f\u984c\u304c\u7c21\u5358\u3059\u304e\u3066\u3057\u307e\u3046\u306e\u304b\u30fb\u30fb\u30fb\uff1f\uff09\u3002\n\n\u305f\u3060\u3001[\u5f37\u5316\u5b66\u7fd2\u3067\u8ff7\u8def\u3092\u63a2\u7d22\u3059\u308b](http://qiita.com/hogefugabar/items/74bed2851a84e978b61c) \u306b\u3082\u3042\u308b\u3088\u3046\u306b\u3001\u9061\u3063\u3066\u5b66\u7fd2\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u3067\u304d\u308b\u306e\u3060\u308d\u3046\u3068\u306f\u601d\u3044\u3064\u3064\u3082\u3061\u3087\u3063\u3068\u691c\u8a3c\u3057\u3066\u307f\u305f\u304b\u3063\u305f\u3068\u3044\u3046\u3068\u3053\u308d\u3067\u3059\u3002\n\u203b \u3061\u306a\u307f\u306b\u4e0a\u8a18\u306e\u5f37\u5316\u5b66\u7fd2\u306e\u8ff7\u8def\u3092\u5b66\u7fd2\u3057\u3066\u3044\u304f\u52d5\u753b\u306f\u9762\u767d\u3044\u306e\u3067\u8208\u5473\u304c\u3042\u308c\u3070\u662f\u975e\u307f\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\uff01\n\n\u7d50\u679c\n=======\n\n\u5831\u916c\u304c\u8cb0\u3048\u308b\u306e\u306f\u3001U=2\u304c\u767a\u751f\u3057\u305f\u6642\u306e\u307f\u3067\u3059\u306e\u3067\u3001U=2\u304c\u767a\u751f\u3057\u305f\u6642\u306b `chance_count` \u3068\u3044\u3046\u306e\u3092\u5897\u3084\u3057\u307e\u3059\u3002\u3042\u308b\u4e00\u5b9a\u671f\u9593\u306e\u9593\u306b\u5f97\u3089\u308c\u308b\u5831\u916c\u306e\u6700\u5927\u306f `chance_count` \u306b\u306a\u308a\u307e\u3059\u3002\n\u305d\u3053\u3067\u3001 `\u5f97\u3089\u308c\u305f\u5831\u916c / chance_count` \u3092 `hit_rate` \u3068\u3057\u3066\u3001\n10000\u56de\u306e\u5b66\u7fd2\u30fb\u8a55\u4fa1\u6bce\u306b\u3069\u3046\u5909\u5316\u3057\u305f\u304b\u3092\u4ee5\u4e0b\u306e\u56f3\u306b\u793a\u3057\u307e\u3059\u3002\n\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8_2015_12_31_11_50.png](https://qiita-image-store.s3.amazonaws.com/0/11124/f2b11a5d-9a9e-af56-379f-766701242495.png \"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8_2015_12_31_11_50.png\")\n\n5000\u4e07\u56de\u307b\u3069\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u304c\u30013000\u4e07\u56de\u304f\u3089\u3044\u3067\u5b66\u7fd2\u306f\u982d\u6253\u3061\u306b\u306a\u3063\u3066\u3044\u308b\u611f\u3058\u3067\u3001\u304a\u3088\u305d hit_rate=0.9 \u304f\u3089\u3044\u3067\u3057\u305f\u3002\n\n\u8003\u5bdf\n---------\n* \u4e00\u5fdc\u826f\u3044\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u5b66\u7fd2\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u554f\u984c\u3092\u8907\u96d1\u306b\u3057\u3066\u3044\u3063\u3066\u3001\u3069\u3053\u307e\u3067\u8ffd\u968f\u3067\u304d\u308b\u306e\u304b\u306f\u4eca\u5f8c\u306e\u8ab2\u984c\u306b\u306a\u308a\u305d\u3046\u3067\u3059\u3002\n* \u5272\u308a\u3068\u7c21\u5358\u306a\u30eb\u30fc\u30eb\u306a\u306e\u306b 100% \u306b\u306a\u3089\u306a\u3044\u306e\u304c\u5c11\u3057\u610f\u5916\u3067\u3057\u305f\n\t* \u5c40\u6240\u89e3\uff1f\uff1f\u30d0\u30b0\uff1f\uff1f\n* \u5b66\u7fd2\u56de\u6570\u3082\u7d50\u69cb\u591a\u3044\u306a\u3041\u3001\u3068\u3002\u72b6\u614b\u6570\u306f U(4)*A(2)^HISTORY(5) -> 32768 \u304f\u3089\u3044\u3067\u3059\u304c\u3001\u3001\u3001\u3067\u3082\u307e\u3042\u3001\u305d\u3093\u306a\u3082\u306e\u306a\u306e\u304b\u306a\u3002\u3002\n* \u5b9f\u969b\u306b\u306fDeep Q Learning\u7684\u306b\u884c\u308f\u306a\u3044\u3068\u5c11\u306a\u304f\u3068\u3082\u5b9f\u7528\u6027\u306f\u306a\u3055\u305d\u3046\u3067\u3059\u304c\u3001DeepLearning\u7684\u306b\u3084\u308b\u3068\u5b66\u7fd2\u306b\u66f4\u306a\u308b\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3001\u4f55\u304b\u3046\u307e\u3044\u5de5\u592b\u304c\u5fc5\u8981\u305d\u3046\u3067\u3059\n* \u5b9f\u969b\u306b\u306f\u30da\u30ca\u30eb\u30c6\u30a3\u304c\u3082\u3046\u5c11\u3057\u308f\u304b\u308a\u306b\u304f\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\uff1f\n\t* \u4f8b\u3048\u3070\u3001\u30e6\u30fc\u30b6\u304c\u96e2\u8131\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u305a\u30fc\u3063\u3068U=0\u304c\u7d9a\u304f\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u304c\u3001A\u3068\u306e\u56e0\u679c\u95a2\u4fc2\u306f\u672c\u5f53\u306b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\n\t* \u3067\u3082\u305d\u308c\u4ee5\u5916\u306f\u3001U\u306e\u30de\u30a4\u30ca\u30b9\u306a\u884c\u52d5\uff08\u9000\u4f1a\u3001\u60aa\u53e3\u3001\u30af\u30ec\u30fc\u30e0\uff09\u3084A\u306e\u30b3\u30b9\u30c8\uff08\u8cbb\u7528\uff09\u306a\u3069\u3068\u3059\u308c\u3070\u308f\u304b\u308a\u3084\u3059\u3044\u3068\u3044\u3048\u3070\u308f\u304b\u308a\u3084\u3059\u3044\u306e\u304b\u3002\n\n\u3055\u3044\u3054\u306b\n=======\n\nQ Learning\u306f\u3042\u308b\u610f\u5473\u5358\u7d14\u306aAI\u3063\u307d\u3044\u306e\u3067\u697d\u3057\u3044\u3067\u3059\u3002\n\u4f55\u304b\u306b\u4f7f\u3048\u308b\u3068\u3044\u3044\u3093\u3060\u3051\u3069\u306a\u3041\u3002\n\n\u30b3\u30fc\u30c9\n-------\n\u53c2\u8003\u307e\u3067\u306b\u8cbc\u3063\u3066\u304a\u304d\u307e\u3059\u3002\n\n```python\n#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nfrom random import random, choice\n\n\nclass Game(object):\n    state = None\n    actions = None\n    game_over = False\n\n    def __init__(self, player):\n        self.player = player\n        self.turn = 0\n        self.last_reward = 0\n        self.total_reward = 0\n        self.init_state()\n\n    def player_action(self):\n        action = self.player.action(self.state, self.last_reward)\n        if action not in self.actions:\n            raise Exception(\"Invalid Action: '%s'\" % action)\n        self.state, self.last_reward = self.get_next_state_and_reward(self.state, action)\n\n    def play(self):\n        yield(self)\n        while not self.game_over:\n            self.player_action()\n            self.turn += 1\n            self.total_reward += self.last_reward\n            yield(self)\n\n    def init_state(self):\n        raise NotImplemented()\n\n    def get_next_state_and_reward(self, state, action):\n        raise NotImplemented()\n\n\nclass UserAndPushEventGame(Game):\n    \"\"\"\n    State           :S : list of (U, A)\n    UserActivity    :U : int of 0~3\n    Action          :A : int of 0 or 1\n\n    Next-State(S, A):S':\n        S[-1][1] = A\n        S.append((Next-U, None))\n        S = S[-5:]\n    Next-U          :  :\n        if S[-4] == (2, 1) then 3\n        else 10% -> 2, 10% -> 1, 80% -> 0\n    Reward(S, A)    :R :\n        if S[-1] == (3, *) then R += 1\n        wrong_action_count := Number of ({0,1,3}, 1) in S\n        R -= wrong_action_count * 0.3\n    \"\"\"\n\n    STATE_HISTORY_SIZE = 5\n\n    def init_state(self):\n        self.actions = [0, 1]\n        self.state = [(0, None)]\n        self.chance_count = 0\n\n    def get_next_state_and_reward(self, state, action):\n        next_state = (state + [(self.next_user_action(state), None)])[-self.STATE_HISTORY_SIZE:]\n        next_state[-2] = (next_state[-2][0], action)\n        reward = 0\n        if len(state) > 0 and state[-1][0] == 3:\n            reward += 1\n        action_count = reduce(lambda t, x: t+(x[1] or 0), state, 0)\n        correct_action_count = len([0 for x in state if x == (2, 1)])\n        wrong_action_count = action_count - correct_action_count\n        reward -= wrong_action_count * 0.3\n        return next_state, reward\n\n    def next_user_action(self, state):\n        if len(state) > 4 and state[-4] == (2, 1):\n            return 3\n        else:\n            rnd = np.random.random()\n            if rnd < 0.8:\n                return 0\n            elif rnd < 0.9:\n                return 1\n            else:\n                self.chance_count += 1\n                return 2\n\n\nclass HumanPlayer(object):\n    training = False\n\n    def action(self, state, last_reward):\n        print \"LastReward=%s, CurrentState: %s\" % (last_reward, state)\n        while True:\n            action_input = raw_input(\"Enter 0~1: \")\n            if int(action_input) in [0, 1]:\n                return int(action_input)\n\n\nclass QLearnPlayer(object):\n    ALPHA = 0.1\n    GAMMA = 0.99\n    E_GREEDY = 0.05\n\n    def __init__(self):\n        self.actions = [0, 1]\n        self.q_table = {}\n        self.last_state = self.last_action = None\n        self.training = True\n\n    def get_q_value(self, state, action):\n        return self.q_table.get(state, {}).get(action, (np.random.random() - 0.5)/1000)  # \u672a\u5b9a\u7fa9\u306f\u5c0f\u3055\u3044\u4e71\u6570\u3092\u8fd4\u3059\n\n    def get_all_q_values(self, state):\n        return [self.get_q_value(state, act) for act in self.actions]\n\n    def set_q_value(self, state, action, val):\n        if state in self.q_table:\n            self.q_table[state][action] = val\n        else:\n            self.q_table[state] = {action: val}\n\n    def action(self, state, last_reward):\n        state = tuple(state)\n        next_action = self.select_action(state)\n        if self.last_state is not None:\n            self.update_q_table(self.last_state, self.last_action, state, last_reward)\n        self.last_state = state\n        self.last_action = next_action\n        return next_action\n\n    def select_action(self, state):\n        if self.training and random() < self.E_GREEDY:\n            return choice(self.actions)\n        else:\n            return np.argmax(self.get_all_q_values(state))\n\n    def update_q_table(self, last_state, last_action, cur_state, last_reward):\n        if self.training:\n            d = last_reward + np.max(self.get_all_q_values(cur_state)) * self.GAMMA - self.get_q_value(last_state, last_action)\n            self.set_q_value(last_state, last_action, self.get_q_value(last_state, last_action) + self.ALPHA * d)\n\n\nif __name__ == '__main__':\n    SWITCH_MODE_TURN_NUM = 10000\n    fp = file(\"result.txt\", \"w\")\n    dt = file(\"detail.txt\", \"w\")\n    player = QLearnPlayer()\n    # player = HumanPlayer()\n    game = UserAndPushEventGame(player)\n    last_chance_count = last_score = 0\n    for g in game.play():\n        # dt.write(\"%s: isT?=%s LastReward=%s TotalReward=%s S=%s\\n\" %\n        #          (g.turn, player.training, g.last_reward, g.total_reward, g.state))\n        if g.turn % SWITCH_MODE_TURN_NUM == 0:\n            if not player.training:\n                this_term_score = game.total_reward - last_score\n                this_term_chance = game.chance_count - last_chance_count\n                if this_term_chance > 0:\n                    hit_rate = 100.0*this_term_score/this_term_chance\n                else:\n                    hit_rate = 0\n                # print \"Turn=%d: This 100 turn score=%2.2f chance=%02d: HitRate=%.1f%% %s\" % \\\n                #       (g.turn, this_term_score, this_term_chance, hit_rate, '*' * int(hit_rate/2))\n                fp.write(\"%d\\t%.2f\\t%d\\t%f\\n\" % (g.turn, this_term_score, this_term_chance, hit_rate))\n            last_score = game.total_reward\n            last_chance_count = game.chance_count\n            player.training = not player.training\n        if g.turn % 10000 == 0:\n            fp.flush()\n\n```\n", "tags": ["Python", "\u6a5f\u68b0\u5b66\u7fd2", "\u5f37\u5316\u5b66\u7fd2"]}