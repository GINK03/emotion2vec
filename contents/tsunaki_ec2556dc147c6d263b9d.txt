{"context": "\n\n\u306f\u3058\u3081\u306b\n\u524d\u56de\u3001\u30b9\u30fc\u30d1\u30fc\u30d5\u30a1\u30df\u30b3\u30f3\u306e\u30bd\u30d5\u30c8\u3092\u5b66\u7fd2\u3055\u305b\u308b\u5185\u5bb9\u3067\u3044\u308d\u3044\u308d\u8981\u671b\u304c\u591a\u304b\u3063\u305f\u306e\u3067\u3001\u4eca\u56de\u306fMacbook\u7b49\u3067\u52d5\u304f\u30d5\u30a1\u30df\u30b3\u30f3\u5b66\u7fd2\u7528\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30b7\u30a7\u30a2\u3057\u307e\u3059\u3002\n\u5b9f\u9a13\u3067\u52d5\u4f5c\u3067\u304d\u308b\u3088\u3046\u306bGit\u306b\u4e0a\u3052\u3068\u304d\u307e\u3057\u305f\u3002\nDownload\u3059\u308b\u5834\u5408\u306fStar\u304f\u3060\u3055\u3044\uff01\nhttps://github.com/tsunaki00/super_mario\n\n\n\u5b9f\u9a13\u74b0\u5883\n\u4ee5\u4e0b\u306e\u30de\u30b7\u30f3\u74b0\u5883\u3067\u30c6\u30b9\u30c8\u3057\u307e\u3057\u305f\u3002\n\n\n\n\u74b0\u5883\n\n\n\n\n\nPC\nMacbook PRO 2016 OSX\n\n\nCPU\ni7\n\n\nMEM\n16GB\n\n\n\u958b\u767a\u8a00\u8a9e\npython\n\n\n\n\u203b Macbook 12\u3067\u3082\u52d5\u304d\u307e\u3057\u305f\n\n\u88dc\u8db3\nTensorflow\u3067\u306e\u5b66\u7fd2\u306f\u9069\u5f53\u306b\u4f5c\u3063\u3066\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u6539\u5584\u3092\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\n\n\u74b0\u5883\u306b\u3064\u3044\u3066\u306e\u6ce8\u610f\n\u5b9f\u884c\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4fee\u6b63\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n$ python3 start.py \n  Traceback (most recent call last):\n    File \"start.py\", line 22, in <module>\n      import gym_pull\n    File \"/usr/local/lib/python3.6/site-packages/gym_pull/__init__.py\", line 41, in <module>\n      import gym_pull.monitoring.monitor\n    File \"/usr/local/lib/python3.6/site-packages/gym_pull/monitoring/monitor.py\", line 10, in <module>\n      class Monitor(gym.monitoring.monitor.Monitor):\n  AttributeError: module 'gym.monitoring' has no attribute 'monitor'\n\n\u2193 \u4ee5\u4e0b\u306b\u4fee\u6b63\n$ vi /usr/local/lib/python3.6/site-packages/gym_pull/monitoring/monitor.py\n   :\n   :\n  class Monitor(gym.monitoring.monitor.Monitor):\n   \u2193\n  class Monitor(gym.monitoring.monitor_manager.MonitorManager):\n\n\n\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3064\u3044\u3066\u306e\u7c21\u5358\u306a\u8aac\u660e\n\u5b66\u7fd2\u65b9\u6cd5\u306f\u5f37\u5316\u5b66\u7fd2(Reinforcement Learning)\u3067\u884c\u3044\u307e\u3059\u3002\n\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3068\u3082\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u3068\u3082\u5c11\u3057\u9055\u3044\u5b9f\u884c\u3057\u305f\u30a2\u30af\u30b7\u30e7\u30f3\u306e\u8a55\u4fa1\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002\n\n[\u53c2\u8003] Deep Q-Network\n\u4ee5\u4e0b\u306b\u8a18\u4e8b\u306bDQN\u306e\u8aac\u660e\u304c\u3042\u308a\u307e\u3057\u305f\u3002\nDQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f\n\n[\u53c2\u8003] \u4eba\u306e\u64cd\u4f5c\u3092\u5b66\u7fd2\u3059\u308b\u3002\n\u81ea\u5206\u9054\u3067\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u8a55\u4fa1\u306b\u5b66\u7fd2\u3055\u305b\u308b\u3068\u4ee5\u4e0b\u306e\u69d8\u306b\u6700\u901f\u30de\u30ea\u30aa\u3092\u4f5c\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3057\u305f\uff01\n\u8a55\u4fa1\u306f\u8ddd\u96e2\u3001\u6642\u9593\u3001\u30b9\u30b3\u30a2\u3067\u3059\u3002\n[\u3010\u4eba\u5de5\u77e5\u80fd\u3011\u4eba\u5de5\u77e5\u80fdSIVA\u306b\u30de\u30ea\u30aa\u3092\u30d7\u30ec\u30a4\u3055\u305b\u3066\u307f\u305f\uff01\u3010WORLD1-1\u3011]\nhttps://www.youtube.com/watch?v=T4dO1GKPx4Y\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\nAction\u306f\u304d\u3061\u3093\u3068\u7d5e\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u3067\u3059\u3002\n\u30df\u30cb\u30d0\u30c3\u30c1\u51e6\u7406\u306b\u3067\u304d\u308b\u3088\u3046\u306b\u914d\u5217\u306b\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u9069\u5b9c\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u305f\u307e\u306b\u30e9\u30f3\u30c0\u30e0\u3092\u5165\u308c\u308b\u3068\u3055\u3089\u306b\u826f\u304f\u306a\u308a\u307e\u3059\uff01\n\nimport tensorflow as tf\nimport gym\nimport gym_pull\nimport ppaquette_gym_super_mario\nfrom gym.wrappers import Monitor\nimport random\nimport numpy as np\nclass Game :\n\n  def __init__(self):\n    self.episode_count = 10000;\n    ## select stage\n    self.env = gym.make('ppaquette/SuperMarioBros-1-1-Tiles-v0')\n\n  def weight_variable(self, shape):\n    initial = tf.truncated_normal(shape, stddev = 0.01)\n    return tf.Variable(initial)\n\n\n  def bias_variable(self, shape):\n    initial = tf.constant(0.01, shape = shape)\n    return tf.Variable(initial)\n\n  def conv2d(self, x, W, stride):\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n\n  def max_pool_2x2(self, x):\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n\n\n  def create_network(self, action_size):\n    #2\u5c64\u306e\u30ec\u30a4\u30e4\u30fc\u306b\u3059\u308b\n    W_conv1 = self.weight_variable([8, 8, 1, 16])\n    b_conv1 = self.bias_variable([16])\n    W_conv2 = self.weight_variable([4, 4, 16, 32])\n    b_conv2 = self.bias_variable([32])\n    W_conv3 = self.weight_variable([4, 4, 32, 64])\n    b_conv3 = self.bias_variable([64])\n    W_fc1 = self.weight_variable([512, action_size])\n    b_fc1 = self.bias_variable([action_size])\n    s = tf.placeholder(\"float\", [None, 13, 16, 1])\n    # hidden layers\n    h_conv1 = tf.nn.relu(self.conv2d(s, W_conv1, 2) + b_conv1)\n    h_conv2 = tf.nn.relu(self.conv2d(h_conv1, W_conv2, 2) + b_conv2)\n    h_conv3 = tf.nn.relu(self.conv2d(h_conv2, W_conv3, 1) + b_conv3)\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 512])\n    readout = tf.matmul(h_conv3_flat, W_fc1) + b_fc1\n    return s, readout \n\n\n  def play_game(self) :\n    action_list = []\n    for i in range(64) :\n      command = format(i, 'b')\n      command = '{0:06d}'.format(int(command))\n      actions = []\n      for cmd in list(command) :\n        actions.append(int(cmd))\n      action_list.append(actions)\n    sess = tf.InteractiveSession()\n    s, readout = self.create_network(len(action_list))\n    a = tf.placeholder(\"float\", [None, len(action_list)])\n    y = tf.placeholder(\"float\", [None, 1])\n    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices = 1)\n    cost = tf.reduce_mean(tf.square(y - readout_action))\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    checkpoint = tf.train.get_checkpoint_state(\"./saved_networks/checkpoints\")\n    if checkpoint and checkpoint.model_checkpoint_path:\n      saver.restore(sess, checkpoint.model_checkpoint_path)\n      print (\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n    else:\n      print (\"Could not find old network weights\")\n    for episode in range(self.episode_count):\n      self.env.reset()\n      total_score = 0\n      distance = 0\n      is_finished = False\n      actions, rewards, images = [], [] ,[]\n      while is_finished == False :\n        # \u753b\u9762\u5de6\u4e0a\u306e\u30c9\u30c3\u30c8\u3092\u53d6\u5f97\u3057\u307e\u3059(\u753b\u50cf\u3067\u3084\u308b\u5834\u5408\u306fself.env.screen\u3067\u3068\u308c\u307e\u3059)\n        screen = np.reshape(self.env.tiles, (13, 16, 1))\n        if episode < 10 :\n          action_index = random.randint(0, len(action_list) - 1)\n        else :\n          readout_t = readout.eval(feed_dict = {s : [screen]})[0]\n          action_index = np.argmax(readout_t)\n        # (1) \u753b\u9762\u306e\u30de\u30ea\u30aa\u306b\u51e6\u7406\u3057\u3066\u307e\u3059(self.env.step)\n        obs, reward, is_finished, info = self.env.step(action_list[action_index])\n        ## MiniBatch\u5316\u3059\u308b\u3088\u3046\u306b\u914d\u5217\u306b\n        action_array = np.zeros(len(action_list))\n        action_array[action_index] = 1\n        actions.append(action_array)\n        # (2) \u5831\u916c\u3092\u4e0e\u3048\u307e\u3059\n        rewards.append([float(info['distance'])])\n        images.append(screen)\n\n        train_step.run(feed_dict = {\n          a : actions, y : rewards, s : images\n        })\n        print('Episode : ', episode, 'Actions : ', action_list[action_index], 'Rewards', reward)\n        actions, rewards, images = [], [] ,[]\n\n        self.env.render()\n      saver.save(sess, 'saved_networks/model-dqn', global_step = episode)\n\nif __name__ == '__main__' :\n  game = Game()\n  game.play_game()\n\n\n\n\u5b9f\u884c\n\n$ python3 start.py\n\n\n\n\u6700\u5f8c\u306b\nDocker + \u30d6\u30e9\u30a6\u30b6\u7248\u306f\u5225\u9014\u66f8\u304d\u307e\u3059\u3002\n\u6628\u4eca\u3001AI\u7b49\u306e\u6280\u8853\u672c\u306a\u3069\u591a\u304f\u51fa\u3066\u307e\u3059\u304c\u3001\u306a\u304b\u306a\u304b\u30cf\u30fc\u30c9\u30eb\u304c\u9ad8\u3044\u3068\u611f\u3058\u307e\u3059\u3088\u306d\u30fb\u30fb\u30fb\u30fb\n\u3053\u3046\u3044\u3046\u8eab\u8fd1\u306a\u3082\u306e\u3067\u7df4\u7fd2\u3059\u308b\u3068\u3044\u3044\u304b\u3068\u601d\u3044\u307e\u3059\uff01\nGameAI\u3084Python\u306b\u3064\u3044\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3064\u3044\u3066\u306e\u8a73\u7d30\u306b\u306f\u5225\u9014\u3001\u52c9\u5f37\u4f1a\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002\n\u3088\u304b\u3063\u305f\u3089\u53c2\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3001\nTech Twitter\u59cb\u3081\u307e\u3057\u305f\u3002\u968f\u6642\u66f4\u65b0\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3088\u304b\u3063\u305f\u3089\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\nhttps://twitter.com/gauss_club\n## \u306f\u3058\u3081\u306b\n[\u524d\u56de](http://qiita.com/tsunaki/items/170c0be6464e09f20279)\u3001\u30b9\u30fc\u30d1\u30fc\u30d5\u30a1\u30df\u30b3\u30f3\u306e\u30bd\u30d5\u30c8\u3092\u5b66\u7fd2\u3055\u305b\u308b\u5185\u5bb9\u3067\u3044\u308d\u3044\u308d\u8981\u671b\u304c\u591a\u304b\u3063\u305f\u306e\u3067\u3001\u4eca\u56de\u306fMacbook\u7b49\u3067\u52d5\u304f\u30d5\u30a1\u30df\u30b3\u30f3\u5b66\u7fd2\u7528\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30b7\u30a7\u30a2\u3057\u307e\u3059\u3002\n\n\u5b9f\u9a13\u3067\u52d5\u4f5c\u3067\u304d\u308b\u3088\u3046\u306b[Git](https://github.com/tsunaki00/super_mario)\u306b\u4e0a\u3052\u3068\u304d\u307e\u3057\u305f\u3002\nDownload\u3059\u308b\u5834\u5408\u306fStar\u304f\u3060\u3055\u3044\uff01\n    https://github.com/tsunaki00/super_mario\n\n\n![17038850_1863772750540489_5777848282422220640_o.jpg](https://qiita-image-store.s3.amazonaws.com/0/52867/90dc6c6c-fbb1-d42e-df3c-7017092de7a6.jpeg \"17038850_1863772750540489_5777848282422220640_o.jpg\")\n\n\n##\u5b9f\u9a13\u74b0\u5883\n\u4ee5\u4e0b\u306e\u30de\u30b7\u30f3\u74b0\u5883\u3067\u30c6\u30b9\u30c8\u3057\u307e\u3057\u305f\u3002\n\n|\u74b0\u5883||\n|:-----------|:------------|\n|PC | Macbook PRO 2016 OSX\n|CPU | i7\n|MEM | 16GB\n|\u958b\u767a\u8a00\u8a9e|python|\n\u203b Macbook 12\u3067\u3082\u52d5\u304d\u307e\u3057\u305f\n\n###\u88dc\u8db3\nTensorflow\u3067\u306e\u5b66\u7fd2\u306f\u9069\u5f53\u306b\u4f5c\u3063\u3066\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u6539\u5584\u3092\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\n\n\n## \u74b0\u5883\u306b\u3064\u3044\u3066\u306e\u6ce8\u610f\n\n\u5b9f\u884c\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4fee\u6b63\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n```bash\n$ python3 start.py \n  Traceback (most recent call last):\n    File \"start.py\", line 22, in <module>\n      import gym_pull\n    File \"/usr/local/lib/python3.6/site-packages/gym_pull/__init__.py\", line 41, in <module>\n      import gym_pull.monitoring.monitor\n    File \"/usr/local/lib/python3.6/site-packages/gym_pull/monitoring/monitor.py\", line 10, in <module>\n      class Monitor(gym.monitoring.monitor.Monitor):\n  AttributeError: module 'gym.monitoring' has no attribute 'monitor'\n```\n\n \u2193 \u4ee5\u4e0b\u306b\u4fee\u6b63\n\n```bash\n$ vi /usr/local/lib/python3.6/site-packages/gym_pull/monitoring/monitor.py\n   :\n   :\n  class Monitor(gym.monitoring.monitor.Monitor):\n   \u2193\n  class Monitor(gym.monitoring.monitor_manager.MonitorManager):\n```\n\n## \u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3064\u3044\u3066\u306e\u7c21\u5358\u306a\u8aac\u660e\n\n\u5b66\u7fd2\u65b9\u6cd5\u306f\u5f37\u5316\u5b66\u7fd2(Reinforcement Learning)\u3067\u884c\u3044\u307e\u3059\u3002\n\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3068\u3082\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u3068\u3082\u5c11\u3057\u9055\u3044\u5b9f\u884c\u3057\u305f\u30a2\u30af\u30b7\u30e7\u30f3\u306e\u8a55\u4fa1\u3092\u5b66\u7fd2\u3057\u307e\u3059\u3002\n\n### [\u53c2\u8003] Deep Q-Network\n\n\u4ee5\u4e0b\u306b\u8a18\u4e8b\u306bDQN\u306e\u8aac\u660e\u304c\u3042\u308a\u307e\u3057\u305f\u3002\n[DQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)\n\n### [\u53c2\u8003] \u4eba\u306e\u64cd\u4f5c\u3092\u5b66\u7fd2\u3059\u308b\u3002\n\u81ea\u5206\u9054\u3067\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u8a55\u4fa1\u306b\u5b66\u7fd2\u3055\u305b\u308b\u3068\u4ee5\u4e0b\u306e\u69d8\u306b\u6700\u901f\u30de\u30ea\u30aa\u3092\u4f5c\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3057\u305f\uff01\n\u8a55\u4fa1\u306f\u8ddd\u96e2\u3001\u6642\u9593\u3001\u30b9\u30b3\u30a2\u3067\u3059\u3002\n\n[\u3010\u4eba\u5de5\u77e5\u80fd\u3011\u4eba\u5de5\u77e5\u80fdSIVA\u306b\u30de\u30ea\u30aa\u3092\u30d7\u30ec\u30a4\u3055\u305b\u3066\u307f\u305f\uff01\u3010WORLD1-1\u3011]\nhttps://www.youtube.com/watch?v=T4dO1GKPx4Y\n\n\n### \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n- Action\u306f\u304d\u3061\u3093\u3068\u7d5e\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u3067\u3059\u3002\n- \u30df\u30cb\u30d0\u30c3\u30c1\u51e6\u7406\u306b\u3067\u304d\u308b\u3088\u3046\u306b\u914d\u5217\u306b\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u9069\u5b9c\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u305f\u307e\u306b\u30e9\u30f3\u30c0\u30e0\u3092\u5165\u308c\u308b\u3068\u3055\u3089\u306b\u826f\u304f\u306a\u308a\u307e\u3059\uff01\n\n```python\nimport tensorflow as tf\nimport gym\nimport gym_pull\nimport ppaquette_gym_super_mario\nfrom gym.wrappers import Monitor\nimport random\nimport numpy as np\nclass Game :\n\n  def __init__(self):\n    self.episode_count = 10000;\n    ## select stage\n    self.env = gym.make('ppaquette/SuperMarioBros-1-1-Tiles-v0')\n\n  def weight_variable(self, shape):\n    initial = tf.truncated_normal(shape, stddev = 0.01)\n    return tf.Variable(initial)\n\n  \n  def bias_variable(self, shape):\n    initial = tf.constant(0.01, shape = shape)\n    return tf.Variable(initial)\n\n  def conv2d(self, x, W, stride):\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n\n  def max_pool_2x2(self, x):\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n\n  \n  def create_network(self, action_size):\n    #2\u5c64\u306e\u30ec\u30a4\u30e4\u30fc\u306b\u3059\u308b\n    W_conv1 = self.weight_variable([8, 8, 1, 16])\n    b_conv1 = self.bias_variable([16])\n    W_conv2 = self.weight_variable([4, 4, 16, 32])\n    b_conv2 = self.bias_variable([32])\n    W_conv3 = self.weight_variable([4, 4, 32, 64])\n    b_conv3 = self.bias_variable([64])\n    W_fc1 = self.weight_variable([512, action_size])\n    b_fc1 = self.bias_variable([action_size])\n    s = tf.placeholder(\"float\", [None, 13, 16, 1])\n    # hidden layers\n    h_conv1 = tf.nn.relu(self.conv2d(s, W_conv1, 2) + b_conv1)\n    h_conv2 = tf.nn.relu(self.conv2d(h_conv1, W_conv2, 2) + b_conv2)\n    h_conv3 = tf.nn.relu(self.conv2d(h_conv2, W_conv3, 1) + b_conv3)\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 512])\n    readout = tf.matmul(h_conv3_flat, W_fc1) + b_fc1\n    return s, readout \n\n\n  def play_game(self) :\n    action_list = []\n    for i in range(64) :\n      command = format(i, 'b')\n      command = '{0:06d}'.format(int(command))\n      actions = []\n      for cmd in list(command) :\n        actions.append(int(cmd))\n      action_list.append(actions)\n    sess = tf.InteractiveSession()\n    s, readout = self.create_network(len(action_list))\n    a = tf.placeholder(\"float\", [None, len(action_list)])\n    y = tf.placeholder(\"float\", [None, 1])\n    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices = 1)\n    cost = tf.reduce_mean(tf.square(y - readout_action))\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    checkpoint = tf.train.get_checkpoint_state(\"./saved_networks/checkpoints\")\n    if checkpoint and checkpoint.model_checkpoint_path:\n      saver.restore(sess, checkpoint.model_checkpoint_path)\n      print (\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n    else:\n      print (\"Could not find old network weights\")\n    for episode in range(self.episode_count):\n      self.env.reset()\n      total_score = 0\n      distance = 0\n      is_finished = False\n      actions, rewards, images = [], [] ,[]\n      while is_finished == False :\n        # \u753b\u9762\u5de6\u4e0a\u306e\u30c9\u30c3\u30c8\u3092\u53d6\u5f97\u3057\u307e\u3059(\u753b\u50cf\u3067\u3084\u308b\u5834\u5408\u306fself.env.screen\u3067\u3068\u308c\u307e\u3059)\n        screen = np.reshape(self.env.tiles, (13, 16, 1))\n        if episode < 10 :\n          action_index = random.randint(0, len(action_list) - 1)\n        else :\n          readout_t = readout.eval(feed_dict = {s : [screen]})[0]\n          action_index = np.argmax(readout_t)\n        # (1) \u753b\u9762\u306e\u30de\u30ea\u30aa\u306b\u51e6\u7406\u3057\u3066\u307e\u3059(self.env.step)\n        obs, reward, is_finished, info = self.env.step(action_list[action_index])\n        ## MiniBatch\u5316\u3059\u308b\u3088\u3046\u306b\u914d\u5217\u306b\n        action_array = np.zeros(len(action_list))\n        action_array[action_index] = 1\n        actions.append(action_array)\n        # (2) \u5831\u916c\u3092\u4e0e\u3048\u307e\u3059\n        rewards.append([float(info['distance'])])\n        images.append(screen)\n\n        train_step.run(feed_dict = {\n          a : actions, y : rewards, s : images\n        })\n        print('Episode : ', episode, 'Actions : ', action_list[action_index], 'Rewards', reward)\n        actions, rewards, images = [], [] ,[]\n        \n        self.env.render()\n      saver.save(sess, 'saved_networks/model-dqn', global_step = episode)\n\nif __name__ == '__main__' :\n  game = Game()\n  game.play_game()\n  \n```\n\n## \u5b9f\u884c\n\n```bash\n\n$ python3 start.py\n\n```\n\n\n## \u6700\u5f8c\u306b\nDocker + \u30d6\u30e9\u30a6\u30b6\u7248\u306f\u5225\u9014\u66f8\u304d\u307e\u3059\u3002\n\n\u6628\u4eca\u3001AI\u7b49\u306e\u6280\u8853\u672c\u306a\u3069\u591a\u304f\u51fa\u3066\u307e\u3059\u304c\u3001\u306a\u304b\u306a\u304b\u30cf\u30fc\u30c9\u30eb\u304c\u9ad8\u3044\u3068\u611f\u3058\u307e\u3059\u3088\u306d\u30fb\u30fb\u30fb\u30fb\n\u3053\u3046\u3044\u3046\u8eab\u8fd1\u306a\u3082\u306e\u3067\u7df4\u7fd2\u3059\u308b\u3068\u3044\u3044\u304b\u3068\u601d\u3044\u307e\u3059\uff01\n\nGameAI\u3084Python\u306b\u3064\u3044\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u3064\u3044\u3066\u306e\u8a73\u7d30\u306b\u306f\u5225\u9014\u3001\u52c9\u5f37\u4f1a\u3092\u5b9f\u65bd\u3057\u307e\u3059\u3002\n\u3088\u304b\u3063\u305f\u3089\u53c2\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3001\n\nTech Twitter\u59cb\u3081\u307e\u3057\u305f\u3002\u968f\u6642\u66f4\u65b0\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3088\u304b\u3063\u305f\u3089\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n https://twitter.com/gauss_club\n\n", "tags": ["TensorFlow", "\u4eba\u5de5\u77e5\u80fd", "DeepLearning", "\u5f37\u5316\u5b66\u7fd2", "Python"]}