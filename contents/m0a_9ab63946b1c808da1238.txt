{"context": " More than 1 year has passed since last update.\n\n\u306f\u3058\u3081\u306b\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30b9\u3092\u53d7\u8b1b\u4e2d\u3067\u3059\n\ncoursera\u306eMachine Learning\n\n\u305f\u3060\u898b\u3066\u308b\u3060\u3051\u3067\u306f\u3001\u5fd8\u308c\u3066\u3044\u304f\u306e\u3067\u30e1\u30e2\u3063\u3066\u3044\u304f\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306b\u306a\u3063\u305f\u3082\u306e\n\n\u6570\u5b66\u3092\u907f\u3051\u3066\u304d\u305f\u793e\u4f1a\u4eba\u30d7\u30ed\u30b0\u30e9\u30de\u304c\u6a5f\u68b0\u5b66\u7fd2\u306e\u52c9\u5f37\u3092\u59cb\u3081\u308b\u969b\u306e\u6700\u77ed\u7d4c\u8def\n\u3053\u306e\u8a18\u4e8b\u304c\u306a\u3051\u308c\u3070\u3001\u305d\u3082\u305d\u3082\u4e0a\u8ff0\u306e\u30b3\u30fc\u30b9\u3092\u77e5\u308b\u3053\u3068\u3082\u306a\u304b\u3063\u305f\u3057\u3001\u3064\u307e\u308b\u90e8\u5206\u3092\u77e5\u3089\u305a\u306b\u60a9\u307f\u7d9a\u3051\u308b\u4e8b\u306b\u306a\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u6df1\u5c64\u5b66\u7fd2\n\u4e00\u756a\u6700\u521d\u306e\u304d\u3063\u304b\u3051\u3067\u3059\u3002\u305f\u3060\u8aad\u3093\u3067\u3044\u308b\u6642\u70b9\u3067\u306f\u809d\u5fc3\u306e\uff11\u7ae0\u304c\u7ffb\u8a33\u4e2d\u3067\u3057\u305f\u3002\u7ffb\u8a33\u306b\u53c2\u52a0\u3059\u308b\u306b\u3082\u3001\u3042\u3093\u307e\u308a\u308f\u304b\u3063\u3066\u3044\u306a\u3044\u72b6\u614b\u3067\u53c2\u52a0\u3059\u308b\u306e\u306f\n\u8ff7\u60d1\u306a\u4e88\u611f\u304c\u3057\u305f\u306e\u3067\u3001\u65ad\u5ff5\u3057\u307e\u3057\u305f\u3002coursera\u306eMachine Learning \u3068\u540c\u3058\u304f\u3001\u521d\u5fc3\u8005\u3067\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u3063\u3061\u3082\u30aa\u30b9\u30b9\u30e1\u3067\u3059\u3002\u308f\u304b\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3089\u7ffb\u8a33\u306b\u53c2\u52a0\u3057\u305f\u3044\u3002\n\n\u5b66\u7fd2\u4e2d\uff0b\u6570\u5b66\u3092\u5206\u304b\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u8272\u3005\u9593\u9055\u3048\u3066\u8a18\u8f09\u3057\u3066\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u3054\u4e86\u627f\u4e0b\u3055\u3044\u3002\n\n\u7406\u89e3\u3057\u305f\u3053\u3068\n\u3051\u3063\u3053\u3046\u3061\u3083\u3093\u3068\u52c9\u5f37\u3059\u308b\u3068\u3001\u305d\u3093\u306a\u306b\u96e3\u3057\u304f\u306a\u3044\u3002(\u307e\u3060\u3001\u52c9\u5f37\u4e2d\u3067\u3059\u304c)\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u76ee\u7684\u306f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5143\u306b\u3001\u4e00\u756a\u8fd1\u3044\u6570\u5f0f\u3092\u5c0e\u304d\u51fa\u3059\u3002\n\u305f\u3068\u3048\u3070\u3001\u3053\u3093\u306a\u30b0\u30e9\u30d5\u304c\u3042\u3063\u3066\u3002\n\n\u3053\u3053\u306b\u3001\u4e00\u756a\u8fd1\u3044\u76f4\u7dda\u306e\u30b0\u30e9\u30d5\u3092\u66f8\u304d\u307e\u3059\u3002\n\n\u76f4\u7dda\u306e\u30b0\u30e9\u30d5\u306f\u3001y=ax+by=ax+by=ax+b \u3067\u3059\u3088\u306d\u3002\n\u3053\u306e\u6642\u306e\u3001aaa , bbb \u306e\u5024\u3092\u6c42\u3081\u3088\u3046\u3002\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\u305d\u306e\u305f\u3081\u306e\u624b\u6cd5\u306e\u52c9\u5f37\u3092\u3072\u305f\u3059\u3089\u3057\u3066\u3044\u304f\u306e\u304c\u3001coursera\u306eMachine Learning\u306e\u5185\u5bb9\u307f\u305f\u3044\u3067\u3059\u3002\n\u3053\u308c\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5834\u5408\u3082\u540c\u3058\u3067\u3001\n\u5404\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u5165\u529b\u306e\u91cd\u307f\u4ed8\u3051(\u3069\u306e\u5165\u529b\u3092\u91cd\u8981\u3068\u307f\u306a\u3059\u304b\u3002)\u3092\u540c\u3058\u624b\u6cd5\u3067\u3001\n\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3063\u3066\u3053\u3068\u307f\u305f\u3044\u3067\u3001\u306f\u3084\u308a\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306b\u3064\u306a\u304c\u3063\u3066\u3044\u304f\u3088\u3046\u3067\u3059\u3002\n\n\u7dda\u5f62\u56de\u5e30(Linear Regression Model)\n\u7dda\u5f62\u56de\u5e30\u306e\u76ee\u6a19\u306f\u3001\u30b0\u30e9\u30d5\u306b\u6253\u3061\u8fbc\u307e\u308c\u305f\u30c7\u30fc\u30bf\u306b\u6700\u9069\u306a\u30e9\u30a4\u30f3(\u6570\u5f0f)\u3092\u5c0e\u304f\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\u6570\u5f0f\u304c\u3067\u304d\u308c\u3070\u3001\u4e88\u6e2c\u3082\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\u8a13\u7df4\u5165\u529bxxx \u5bfe\u5fdc\u3059\u308b\u51fa\u529byyy\u304c\u3042\u308a\u307e\u3059\u3002\n\u6c42\u3081\u308b\u76f4\u7dda\u306e\u5f0f\u3092\u3044\u304b\u306b\u5b9a\u7fa9\u3057\u307e\u3059\u3002\nHypothesis:\nh\u03b8(x)=\u03b80x0+\u03b81x1+...+\u03b8nxnh\u03b8(x)=\u03b80x0+\u03b81x1+...+\u03b8nxnh_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n   (x0=1x0=1x_0 = 1)\nn=1n=1n=1\u3067\u3042\u308c\u3070\u3001y=ax+by=ax+by=ax+b\u3068\u540c\u3058\u3067\u3059\u3002\u03b8=[\u03b80,\u03b81,...\u03b8n]\u03b8=[\u03b80,\u03b81,...\u03b8n]\\theta = [ \\theta_0, \\theta_1, ... \\theta_n]\u304c\u3001aaa,bbb\u306b\u5bfe\u5fdc\u3057\u3066\u304a\u308a\n\u3064\u307e\u308a\u3001\u03b8\u03b8\\theta\u3092\u6c42\u3081\u308b\u306e\u304c\u76ee\u7684\u3067\u3059\u3002\n\n\u30b3\u30b9\u30c8\u95a2\u6570(\u76ee\u7684\u95a2\u6570) (Cost Function)\n\u30b3\u30b9\u30c8\u95a2\u6570J(\u03b8)J(\u03b8)J(\\theta)\u306f\u7c21\u5358\u306b\u66f8\u3051\u3070\nJ(\u03b8)=|\u4e88\u6e2c\u5024\u2212\u5b9f\u5024|J(\\theta) = |\u4e88\u6e2c\u5024 - \u5b9f\u5024|\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\u3057\u305f\u304c\u3063\u3066\u4e88\u6e2c\u5024\u304c\u8fd1\u3051\u308c\u3070\u305d\u308c\u3060\u30510\u306b\u8fd1\u4f3c\u3057\u307e\u3059\u3002\n\nCostFunction\n\nJ(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2\n\nJ(\u03b8)=12mm\u2211i=1(h\u03b8(x(i))\u2212y(i))2{\nJ(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2\n}\n\u4e0a\u8a18\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u6700\u5c0f\u306b\u3059\u308b\u03b8\\theta\u3092\u63a2\u3057\u307e\u3059\u3002\n\n\u6700\u6025\u964d\u4e0b\u6cd5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0(Gradient descent algorithm)\n\u5148\u306e\u8a71\u3067\u3001\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u5177\u4f53\u7684\u306b\u3069\u3046\u3084\u3063\u3066\u6700\u5c0f\u306b\u6301\u3063\u3066\u3044\u304f\u304b\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\n\u305d\u308c\u3092\u3053\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u89e3\u6c7a\u3057\u307e\u3059\u3002\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\n\\\\\n\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\n\\\\\u5b66\u7fd2\u7387(learning rate): \\alpha \\\\\n\n\u3000\u3000\u03b8j:=\u03b8j\u2212\u03b11mm\u2211i=1[(h\u03b8(x(i)\u2212y(i))x(i)j]\u5b66\u7fd2\u7387(learningrate):\u03b1{\\\\\n\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\n\\\\\u5b66\u7fd2\u7387(learning rate): \\alpha \\\\\n\n}\n\n\u6b63\u898f\u65b9\u7a0b\u5f0f\n\u3053\u3061\u3089\u306e\u307b\u3046\u304c\u7c21\u5358\u306b\u6c42\u3081\u3089\u308c\u308b\u3088\u3046\u3067\u3059\u304c\u03b8\\theta\u306e\u500b\u6570(features)\u304c\u5897\u3048\u308b\u306b\u3064\u3051\u4e0a\u8a18\u306e\u6700\u6025\u964d\u4e0b\u6cd5\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u305d\u3046\u3067\u3059\u3002\u76ee\u5b89\u306f\u03b8\\theta \u304c1\u4e07\u3092\u8d85\u3048\u305f\u3042\u305f\u308a\u3002\n\u03b8=(XTX)\u22121XTy\\theta = (X^T X)^{-1} X^Ty\n\ncode\ntheta = pinv(X' * X)*X' * X * y;\n\n\n\nOatave\u3092\u4f7f\u3046\n\u6570\u5f0f\u306e\u30b0\u30e9\u30d5\u5316\u306b\u4fbf\u5229\n\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u304c\u5f97\u610f\u3002\n\n\nOctave\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nMac\u9650\u5b9a\u3067\u3059\u3002\nbrew tap homebrew/science\nbrew update && brew upgrade\nbrew install octave\n\nedit\u30b3\u30de\u30f3\u30c9\u3067atom\u304c\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n\n~/.octaverc\nEDITOR('atom')\n\n\n\nHypothesis\u3092\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\n\u5143\u3005\u306e\u5b9a\u7fa9\nh_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n    \n(x_0 = 1)\nh\u03b8(x)=\u03b80x0+\u03b81x1+...+\u03b8nxn(x0=1){h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n    \n(x_0 = 1)\n}\n\u5b9a\u7fa9\n\\theta = \\begin{pmatrix} \\theta_0  \\\\ \\theta_1 \\\\ \\dots \\\\ \\theta_{n} \\end{pmatrix}, x = \\begin{pmatrix} x_0  \\\\ x_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix}\n\u03b8=(\u03b80\u03b81\u2026\u03b8n),x=(x0x1\u2026xn){\\theta = \\begin{pmatrix} \\theta_0  \\\\ \\theta_1 \\\\ \\dots \\\\ \\theta_{n} \\end{pmatrix}, x = \\begin{pmatrix} x_0  \\\\ x_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix}\n}\n\u8ee2\u7f6e\u884c\u5217\u306b\n\\theta^T = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix}\n\u03b8T=(\u03b80\u03b81\u2026\u03b8n){\\theta^T = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix}\n}\n\u6700\u7d42\u7684\u306b\u3053\u3046\u306a\u308b\nh_\\theta(x) = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix} \\cdot  \\begin{pmatrix} x_0  \\\\ \nx_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix} = \\theta^T \\cdot x \nh\u03b8(x)=(\u03b80\u03b81\u2026\u03b8n)\u22c5(x0x1\u2026xn)=\u03b8T\u22c5x{h_\\theta(x) = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix} \\cdot  \\begin{pmatrix} x_0  \\\\ \nx_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix} = \\theta^T \\cdot x \n}\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 Logistic Regression Model\n\u7dda\u5f62\u56de\u5e30\u306e\u5834\u5408\u306f\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8\u306b\u5bfe\u5fdc\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308b\u305f\u3081\u306b\u4f5c\u308a\u307e\u3057\u305f\u3002\u672a\u6765\u4e88\u6e2c\u306a\u3069\u5177\u4f53\u7684\u306a\u5024\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306b\u4f7f\u3044\u307e\u3059\u3002\n\u8a72\u5f53\u3059\u308b\u30c7\u30fc\u30bf\u3092\u5206\u985e\u3059\u308b\u5206\u985e\u554f\u984c\u3092\u89e3\u304f\u3068\u304d\u306b\u4f7f\u3046\u306e\u304c\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3059\u3002\n\nHypothesis\nh\u03b8(x)=g(\u03b8Tx)h_\\theta(x) = g(\\theta^Tx)\ng\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u3059\u3002\n\n\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 Sigmoid Function(Logistic Funtion)\ng(z) = \\frac {1}{1 + e^{(-z)}}\ng(z)=11+e(\u2212z){g(z) = \\frac {1}{1 + e^{(-z)}}\n}\nz\u2192\u221ez \\to \\infty  \u306e\u6642  g(z)\u21921g(z) \\to 1\u306b\nz\u2192\u2212\u221ez \\to -\\infty \u306e\u6642  g(z)\u21920g(z) \\to 0\u306b\n\u3064\u307e\u308a 0\u2267g(z)\u22671 0 \\geqq  g(z) \\geqq  1 \u306e\u7bc4\u56f2\u306b\u306a\u308b\u3053\u3068\u304c\u4fdd\u8a3c\u3055\u308c\u308b\u3002\n\n\u30b3\u30b9\u30c8\u95a2\u6570(\u76ee\u7684\u95a2\u6570) Cost Function\nCost(h_{(\\theta)},y) = \\begin{cases}  -log(h_{\\theta}(x)) & (y=1)  \\\\  -log(1-h_{\\theta}(x)) & (y = 0) \\end{cases}\nCost(h(\u03b8),y)={\u2212log(h\u03b8(x))(y=1)\u2212log(1\u2212h\u03b8(x))(y=0){Cost(h_{(\\theta)},y) = \\begin{cases}  -log(h_{\\theta}(x)) & (y=1)  \\\\  -log(1-h_{\\theta}(x)) & (y = 0) \\end{cases}\n}\nCostFunction:\n\nJ(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} Cost(h_{(\\theta)},y) = \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\nJ(\u03b8)=1mm\u2211i=1Cost(h(\u03b8),y)=1m[m\u2211i=1y(i)log(h\u03b8(x))+(1\u2212y(i))log(1\u2212h\u03b8(x))]{\nJ(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} Cost(h_{(\\theta)},y) = \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\n}\n\u3053\u306e\u95a2\u6570\u306e\u6700\u5c0f\u306b\u3059\u308b\u03b8\\theta\u3092\u63a2\u3059\u3002\n\n\u6700\u6025\u964d\u4e0b\u6cd5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 Gradient descent algorithm\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057 \n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\\\\\n\u3000\u3000\u5b66\u7fd2\u7387(lerning rate): \\alpha\n\u3000\u3000\u03b8j:=\u03b8j\u2212\u03b11mm\u2211i=1[(h\u03b8(x(i)\u2212y(i))x(i)j]\u3000\u3000\u5b66\u7fd2\u7387(lerningrate):\u03b1{\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\\\\\n\u3000\u3000\u5b66\u7fd2\u7387(lerning rate): \\alpha\n}\n\n\u6b63\u898f\u5316\n\u4e0a\u8a18\u306e\u307e\u307e\u5229\u7528\u3059\u308b\u3068\u3042\u307e\u308a\u306b\u3082\u30b0\u30e9\u30d5\u304c\u30d5\u30a3\u30c3\u30c8\u3057\u3059\u304e\u308b\u306e\u3067\u3001\n\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u5f04\u3063\u3066\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u8ab2\u3059\u4ed5\u7d44\u307f\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\nCostFunction:\n\nJ(\\theta)= \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\n+ \\frac{\\lambda}{2m}\\sum^{n}_{j=1} \\theta_j^2\n\nJ(\u03b8)=1m[m\u2211i=1y(i)log(h\u03b8(x))+(1\u2212y(i))log(1\u2212h\u03b8(x))]+\u03bb2mn\u2211j=1\u03b82j{\nJ(\\theta)= \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\n+ \\frac{\\lambda}{2m}\\sum^{n}_{j=1} \\theta_j^2\n\n}\nGradient descent algorithm:\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\n\u3000\u3000\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[ (h_\\theta(x^{(i)} - y^{(i)} )x_{0}^{(i)} ] \\\\\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^{m}[ \\frac{1}{m} (h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_j]\\\\\n\u5b66\u7fd2\u7387(lerning rate): \\alpha,  \\\\\nj = 1,2,3,\\ldots,n\n\n\u3000\u3000\u03b80:=\u03b80\u2212\u03b11mm\u2211i=1[(h\u03b8(x(i)\u2212y(i))x(i)0]\u3000\u3000\u03b8j:=\u03b8j\u2212\u03b1m\u2211i=1[1m(h\u03b8(x(i)\u2212y(i))x(i)j+\u03bbm\u03b8j]\u5b66\u7fd2\u7387(lerningrate):\u03b1,j=1,2,3,\u2026,n{\u3000\u3000\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[ (h_\\theta(x^{(i)} - y^{(i)} )x_{0}^{(i)} ] \\\\\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^{m}[ \\frac{1}{m} (h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_j]\\\\\n\u5b66\u7fd2\u7387(lerning rate): \\alpha,  \\\\\nj = 1,2,3,\\ldots,n\n\n}\n\u30da\u30fc\u30b8\u3092\u5206\u5272\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\u4eca\u56de\u306f\u3053\u3053\u307e\u3067\u3067\u3001\u6b21\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u308a\u307e\u3059\u3002\n\n# \u306f\u3058\u3081\u306b\n\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30b9\u3092\u53d7\u8b1b\u4e2d\u3067\u3059\n\n* [coursera\u306eMachine Learning](https://www.coursera.org/learn/machine-learning/home/info)\n\n\u305f\u3060\u898b\u3066\u308b\u3060\u3051\u3067\u306f\u3001\u5fd8\u308c\u3066\u3044\u304f\u306e\u3067\u30e1\u30e2\u3063\u3066\u3044\u304f\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\n\n\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306b\u306a\u3063\u305f\u3082\u306e\n\n* [\u6570\u5b66\u3092\u907f\u3051\u3066\u304d\u305f\u793e\u4f1a\u4eba\u30d7\u30ed\u30b0\u30e9\u30de\u304c\u6a5f\u68b0\u5b66\u7fd2\u306e\u52c9\u5f37\u3092\u59cb\u3081\u308b\u969b\u306e\u6700\u77ed\u7d4c\u8def](http://qiita.com/daxanya1/items/218f2e3b922142550ef9)\n\u3053\u306e\u8a18\u4e8b\u304c\u306a\u3051\u308c\u3070\u3001\u305d\u3082\u305d\u3082\u4e0a\u8ff0\u306e\u30b3\u30fc\u30b9\u3092\u77e5\u308b\u3053\u3068\u3082\u306a\u304b\u3063\u305f\u3057\u3001\u3064\u307e\u308b\u90e8\u5206\u3092\u77e5\u3089\u305a\u306b\u60a9\u307f\u7d9a\u3051\u308b\u4e8b\u306b\u306a\u3063\u305f\u3068\u601d\u3044\u307e\u3059\u3002\n\n* [\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u6df1\u5c64\u5b66\u7fd2](http://nnadl-ja.github.io/nnadl_site_ja)\n\u4e00\u756a\u6700\u521d\u306e\u304d\u3063\u304b\u3051\u3067\u3059\u3002\u305f\u3060\u8aad\u3093\u3067\u3044\u308b\u6642\u70b9\u3067\u306f\u809d\u5fc3\u306e\uff11\u7ae0\u304c\u7ffb\u8a33\u4e2d\u3067\u3057\u305f\u3002\u7ffb\u8a33\u306b\u53c2\u52a0\u3059\u308b\u306b\u3082\u3001\u3042\u3093\u307e\u308a\u308f\u304b\u3063\u3066\u3044\u306a\u3044\u72b6\u614b\u3067\u53c2\u52a0\u3059\u308b\u306e\u306f\n\u8ff7\u60d1\u306a\u4e88\u611f\u304c\u3057\u305f\u306e\u3067\u3001\u65ad\u5ff5\u3057\u307e\u3057\u305f\u3002[coursera\u306eMachine Learning](https://www.coursera.org/learn/machine-learning/home/info) \u3068\u540c\u3058\u304f\u3001\u521d\u5fc3\u8005\u3067\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u3063\u3061\u3082\u30aa\u30b9\u30b9\u30e1\u3067\u3059\u3002\u308f\u304b\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3089\u7ffb\u8a33\u306b\u53c2\u52a0\u3057\u305f\u3044\u3002\n\n\n\u5b66\u7fd2\u4e2d\uff0b\u6570\u5b66\u3092\u5206\u304b\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u8272\u3005\u9593\u9055\u3048\u3066\u8a18\u8f09\u3057\u3066\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u3054\u4e86\u627f\u4e0b\u3055\u3044\u3002\n\n\n\n# \u7406\u89e3\u3057\u305f\u3053\u3068\n\n\u3051\u3063\u3053\u3046\u3061\u3083\u3093\u3068\u52c9\u5f37\u3059\u308b\u3068\u3001\u305d\u3093\u306a\u306b\u96e3\u3057\u304f\u306a\u3044\u3002(\u307e\u3060\u3001\u52c9\u5f37\u4e2d\u3067\u3059\u304c)\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u76ee\u7684\u306f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5143\u306b\u3001\u4e00\u756a\u8fd1\u3044\u6570\u5f0f\u3092\u5c0e\u304d\u51fa\u3059\u3002\n\n\u305f\u3068\u3048\u3070\u3001\u3053\u3093\u306a\u30b0\u30e9\u30d5\u304c\u3042\u3063\u3066\u3002\n![Figure_1.jpg](https://qiita-image-store.s3.amazonaws.com/0/3844/3e1b5e9f-d59d-c82d-6e92-fc8f1eba8145.jpeg \"Figure_1.jpg\")\n\n\n\u3053\u3053\u306b\u3001\u4e00\u756a\u8fd1\u3044\u76f4\u7dda\u306e\u30b0\u30e9\u30d5\u3092\u66f8\u304d\u307e\u3059\u3002\n![Figure_1_2.jpg](https://qiita-image-store.s3.amazonaws.com/0/3844/d62dd8d3-8411-5ec1-7023-08f08a987ebe.jpeg \"Figure_1_2.jpg\")\n\n\n\u76f4\u7dda\u306e\u30b0\u30e9\u30d5\u306f\u3001$y=ax+b$ \u3067\u3059\u3088\u306d\u3002\n\u3053\u306e\u6642\u306e\u3001$a$ , $b$ \u306e\u5024\u3092\u6c42\u3081\u3088\u3046\u3002\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\n\n\u305d\u306e\u305f\u3081\u306e\u624b\u6cd5\u306e\u52c9\u5f37\u3092\u3072\u305f\u3059\u3089\u3057\u3066\u3044\u304f\u306e\u304c\u3001[coursera\u306eMachine Learning](https://www.coursera.org/learn/machine-learning/home/info)\u306e\u5185\u5bb9\u307f\u305f\u3044\u3067\u3059\u3002\n\n\u3053\u308c\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5834\u5408\u3082\u540c\u3058\u3067\u3001\n\u5404\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u5165\u529b\u306e\u91cd\u307f\u4ed8\u3051(\u3069\u306e\u5165\u529b\u3092\u91cd\u8981\u3068\u307f\u306a\u3059\u304b\u3002)\u3092\u540c\u3058\u624b\u6cd5\u3067\u3001\n\u6700\u9069\u5316\u3057\u3066\u3044\u304f\u3063\u3066\u3053\u3068\u307f\u305f\u3044\u3067\u3001\u306f\u3084\u308a\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306b\u3064\u306a\u304c\u3063\u3066\u3044\u304f\u3088\u3046\u3067\u3059\u3002\n\n\n# \u7dda\u5f62\u56de\u5e30(Linear Regression Model)\n\n\u7dda\u5f62\u56de\u5e30\u306e\u76ee\u6a19\u306f\u3001\u30b0\u30e9\u30d5\u306b\u6253\u3061\u8fbc\u307e\u308c\u305f\u30c7\u30fc\u30bf\u306b\u6700\u9069\u306a\u30e9\u30a4\u30f3(\u6570\u5f0f)\u3092\u5c0e\u304f\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\u6570\u5f0f\u304c\u3067\u304d\u308c\u3070\u3001\u4e88\u6e2c\u3082\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u8a13\u7df4\u5165\u529b$x$ \u5bfe\u5fdc\u3059\u308b\u51fa\u529b$y$\u304c\u3042\u308a\u307e\u3059\u3002\n\u6c42\u3081\u308b\u76f4\u7dda\u306e\u5f0f\u3092\u3044\u304b\u306b\u5b9a\u7fa9\u3057\u307e\u3059\u3002\nHypothesis:\n$h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n$   ($x_0 = 1$)\n\n$n=1$\u3067\u3042\u308c\u3070\u3001$y=ax+b$\u3068\u540c\u3058\u3067\u3059\u3002$\\theta = [ \\theta_0, \\theta_1, ... \\theta_n]$\u304c\u3001$a$,$b$\u306b\u5bfe\u5fdc\u3057\u3066\u304a\u308a\n\u3064\u307e\u308a\u3001$\\theta$\u3092\u6c42\u3081\u308b\u306e\u304c\u76ee\u7684\u3067\u3059\u3002\n\n## \u30b3\u30b9\u30c8\u95a2\u6570(\u76ee\u7684\u95a2\u6570) (Cost Function)\n\n\u30b3\u30b9\u30c8\u95a2\u6570$J(\\theta)$\u306f\u7c21\u5358\u306b\u66f8\u3051\u3070\n\n$J(\\theta) = |\u4e88\u6e2c\u5024 - \u5b9f\u5024|$\u3063\u3066\u3053\u3068\u3067\u3059\u3002\n\u3057\u305f\u304c\u3063\u3066\u4e88\u6e2c\u5024\u304c\u8fd1\u3051\u308c\u3070\u305d\u308c\u3060\u30510\u306b\u8fd1\u4f3c\u3057\u307e\u3059\u3002\n\n\n```math:CostFunction\n\nJ(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2\n```\n\n\u4e0a\u8a18\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u6700\u5c0f\u306b\u3059\u308b$\\theta$\u3092\u63a2\u3057\u307e\u3059\u3002\n\n\n## \u6700\u6025\u964d\u4e0b\u6cd5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0(Gradient descent algorithm)\n\n\u5148\u306e\u8a71\u3067\u3001\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u5177\u4f53\u7684\u306b\u3069\u3046\u3084\u3063\u3066\u6700\u5c0f\u306b\u6301\u3063\u3066\u3044\u304f\u304b\u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\n\u305d\u308c\u3092\u3053\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u89e3\u6c7a\u3057\u307e\u3059\u3002\n\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\n\n```math\n\\\\\n \n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\n\\\\\u5b66\u7fd2\u7387(learning rate): \\alpha \\\\\n\n```\n\n\n## \u6b63\u898f\u65b9\u7a0b\u5f0f\n\u3053\u3061\u3089\u306e\u307b\u3046\u304c\u7c21\u5358\u306b\u6c42\u3081\u3089\u308c\u308b\u3088\u3046\u3067\u3059\u304c$\\theta$\u306e\u500b\u6570(features)\u304c\u5897\u3048\u308b\u306b\u3064\u3051\u4e0a\u8a18\u306e\u6700\u6025\u964d\u4e0b\u6cd5\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u305d\u3046\u3067\u3059\u3002\u76ee\u5b89\u306f$\\theta $\u304c1\u4e07\u3092\u8d85\u3048\u305f\u3042\u305f\u308a\u3002\n\n$\\theta = (X^T X)^{-1} X^Ty$\n\n```octave:code\ntheta = pinv(X' * X)*X' * X * y;\n```\n\n## Oatave\u3092\u4f7f\u3046\n\n\u6570\u5f0f\u306e\u30b0\u30e9\u30d5\u5316\u306b\u4fbf\u5229\n\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u304c\u5f97\u610f\u3002\n\n![](http://www.gnu.org/software/octave/images/screenshot-small.png)\n\n### Octave\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \nMac\u9650\u5b9a\u3067\u3059\u3002\n\n```\nbrew tap homebrew/science\nbrew update && brew upgrade\nbrew install octave\n```\n\nedit\u30b3\u30de\u30f3\u30c9\u3067atom\u304c\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n\n```sh:~/.octaverc\nEDITOR('atom')\n```\n\n\n### Hypothesis\u3092\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\n\n\u5143\u3005\u306e\u5b9a\u7fa9\n\n```math\nh_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... +\\theta_n x_n    \n(x_0 = 1)\n```\n\n\u5b9a\u7fa9\n\n```math\n\\theta = \\begin{pmatrix} \\theta_0  \\\\ \\theta_1 \\\\ \\dots \\\\ \\theta_{n} \\end{pmatrix}, x = \\begin{pmatrix} x_0  \\\\ x_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix}\n```\n\n\u8ee2\u7f6e\u884c\u5217\u306b\n\n```math\n\\theta^T = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix}\n```\n\n\u6700\u7d42\u7684\u306b\u3053\u3046\u306a\u308b\n\n```math\nh_\\theta(x) = \\begin{pmatrix} \\theta_0  & \\theta_1 & \\dots & \\theta_{n} \\end{pmatrix} \\cdot  \\begin{pmatrix} x_0  \\\\ \nx_1 \\\\ \\dots \\\\ x_{n} \\end{pmatrix} = \\theta^T \\cdot x \n```\n\n\n# \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 Logistic Regression Model\n\n\u7dda\u5f62\u56de\u5e30\u306e\u5834\u5408\u306f\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8\u306b\u5bfe\u5fdc\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308b\u305f\u3081\u306b\u4f5c\u308a\u307e\u3057\u305f\u3002\u672a\u6765\u4e88\u6e2c\u306a\u3069\u5177\u4f53\u7684\u306a\u5024\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306b\u4f7f\u3044\u307e\u3059\u3002\n\u8a72\u5f53\u3059\u308b\u30c7\u30fc\u30bf\u3092\u5206\u985e\u3059\u308b\u5206\u985e\u554f\u984c\u3092\u89e3\u304f\u3068\u304d\u306b\u4f7f\u3046\u306e\u304c\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3059\u3002\n\n## Hypothesis\n\n$h_\\theta(x) = g(\\theta^Tx)$\n\ng\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u3059\u3002\n\n### \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 Sigmoid Function(Logistic Funtion)\n\n\n```math\ng(z) = \\frac {1}{1 + e^{(-z)}}\n```\n\n\n$z \\to \\infty $ \u306e\u6642  $g(z) \\to 1$\u306b\n$z \\to -\\infty$ \u306e\u6642  $g(z) \\to 0$\u306b\n\n\u3064\u307e\u308a $ 0 \\geqq  g(z) \\geqq  1$ \u306e\u7bc4\u56f2\u306b\u306a\u308b\u3053\u3068\u304c\u4fdd\u8a3c\u3055\u308c\u308b\u3002\n\n\n## \u30b3\u30b9\u30c8\u95a2\u6570(\u76ee\u7684\u95a2\u6570) Cost Function\n\n```math\nCost(h_{(\\theta)},y) = \\begin{cases}  -log(h_{\\theta}(x)) & (y=1)  \\\\  -log(1-h_{\\theta}(x)) & (y = 0) \\end{cases}\n```\n\n\nCostFunction:\n\n```math\n\nJ(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} Cost(h_{(\\theta)},y) = \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\n```\n\n\n\n\u3053\u306e\u95a2\u6570\u306e\u6700\u5c0f\u306b\u3059\u308b$\\theta$\u3092\u63a2\u3059\u3002\n\n## \u6700\u6025\u964d\u4e0b\u6cd5\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 Gradient descent algorithm\n\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057 \n\n```math\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)}]\n\\\\\n\u3000\u3000\u5b66\u7fd2\u7387(lerning rate): \\alpha\n```\n\n\n##\u6b63\u898f\u5316\n\u4e0a\u8a18\u306e\u307e\u307e\u5229\u7528\u3059\u308b\u3068\u3042\u307e\u308a\u306b\u3082\u30b0\u30e9\u30d5\u304c\u30d5\u30a3\u30c3\u30c8\u3057\u3059\u304e\u308b\u306e\u3067\u3001\n\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u5f04\u3063\u3066\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u8ab2\u3059\u4ed5\u7d44\u307f\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n\n\nCostFunction:\n\n```math\n\nJ(\\theta)= \\frac{1 }{ m}[\\sum_{i=1}^{m} y^{(i)} log(h_{\\theta}(x)) +(1-y^{(i)}) log(1-h_{\\theta}(x)) ]\n+ \\frac{\\lambda}{2m}\\sum^{n}_{j=1} \\theta_j^2\n\n```\n\nGradient descent algorithm:\n\u5024\u304c\u5e73\u6ed1\u5316\u3059\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\n\n```math\n\u3000\u3000\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[ (h_\\theta(x^{(i)} - y^{(i)} )x_{0}^{(i)} ] \\\\\n\u3000\u3000\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^{m}[ \\frac{1}{m} (h_\\theta(x^{(i)} - y^{(i)} )x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_j]\\\\\n\u5b66\u7fd2\u7387(lerning rate): \\alpha,  \\\\\nj = 1,2,3,\\ldots,n\n\n```\n\n\n\n\n\u30da\u30fc\u30b8\u3092\u5206\u5272\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\u4eca\u56de\u306f\u3053\u3053\u307e\u3067\u3067\u3001\u6b21\u306f[\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af](http://qiita.com/m0a/items/da2db182f9dc859b43a9)\u306b\u5165\u308a\u307e\u3059\u3002\n", "tags": ["\u6a5f\u68b0\u5b66\u7fd2", "coursera"]}