{"context": " More than 1 year has passed since last update.\u672c\u65e5\u306f Hive on Tez \u306e\u8a71\u3092\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3057\u305f\u304c\uff0c\u6628\u65e5\u306e\u8a18\u4e8b \u306e\u4e2d\u3067\u3082\u89e6\u308c\u3089\u308c\u3066\u3044\u308b Spark \u304b\u3064 SQL on Hadoop \u3067\u3042\u308b SparkSQL on YARN \u306e\u8a71\u306b\u8a71\u984c\u3092\u5909\u66f4\u3057\u3066\u304a\u5c4a\u3051\u3057\u307e\u3059\uff0e\n\nSparkSQL \u3068\u306f\n\u540d\u524d\u901a\u308a\uff0cSpark \u3092\u30e9\u30f3\u30bf\u30a4\u30e0\u3068\u3057\u305f SQL \u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3067\u3059\uff0e\u30c7\u30b6\u30a4\u30f3\u306e\u7279\u5fb4\u3068\u3057\u3066\uff0cSparkDSL \u3068\u306e\u89aa\u548c\u6027\u3068\uff0cHive \u306e Metastore, SerDe, UDF \u4e92\u63db\u6027\u304c\u4fdd\u305f\u308c\u3066\u3044\u308b\u3068\u3044\u3046\u70b9\u304c\u3042\u3052\u3089\u308c\u307e\u3059\uff0eSparkSQL \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u3057\u3066\u306f\uff0c\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059:\n\nSparkSQL\nSpark SQL Programming Guide\n\n\nSpark on YARN \u3067\u306e HiveQL \u4e92\u63db\u30af\u30a8\u30ea\u306e\u52d5\u304b\u3057\u65b9\n\u3053\u3053\u304b\u3089\u306f\uff0cHadoop 2 \u30af\u30e9\u30b9\u30bf\u3092\u65e2\u306b\u6301\u3063\u3066\u3044\u308b\u65b9\u306e\u305f\u3081\u306b\uff0cSpark on YARN \u306b\u304a\u3051\u308b Spark \u306e\u52d5\u4f5c\u306e\u3055\u305b\u65b9\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\uff0e\u306a\u304a\uff0c\u4eca\u56de\u306e\u8aac\u660e\u3067\u306f\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u56fa\u6709\u306e\u8aac\u660e\u306f\u884c\u3044\u307e\u305b\u3093\uff0e\n\nSpark \u5074\u306e\u8a2d\u5b9a\n\u3042\u306a\u305f\u306e\u5229\u7528\u3057\u3066\u3044\u308b Hadoop2 \u7cfb\u30af\u30e9\u30b9\u30bf\u304c Spark \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u30da\u30fc\u30b8\u306b\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\uff0cSpark \u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u3066 Spark \u3092\u30d3\u30eb\u30c9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\u4f8b\u3048\u3070\uff0c\u6700\u65b0\u7248\u3067\u3042\u308b Hadoop 2.5.2 \u3084 2.6.0 \u3092\u5229\u7528\u3057\u3088\u3046\u3068\u3057\u305f\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u624b\u9806\u3067 Spark \u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\uff0e\nexport MAVEN_OPTS=\"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"\ntar xzvf spark-1.1.1.tgz\ncd spark\n./make-distribution.sh --tgz -Phadoop-2.4 -Pyarn,hive,hive-thriftserver -Dhadoop.version=2.5.2\n\n\u3059\u308b\u3068\uff0cspark-.tgz \u304c\u751f\u6210\u3055\u308c\u308b\u306e\u3067\u3053\u308c\u3092\u89e3\u51cd\u3057\uff0cSpark \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(conf/spark-env.sh, conf/spark-defaults.conf)\u3092\u8a18\u8ff0\u3057\u307e\u3059\uff0e\u3053\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u306f\uff0cSpark \u8d77\u52d5\u6642\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u307e\u3059\uff0e\nspark-env.sh \u306f\uff0cSpark on YARN \u8d77\u52d5\u306b\u5fc5\u9808\u306e\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\uff0e\u7279\u306b\uff0cHADOOP_CONF_DIR \u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u306b\u306a\u308b\u306e\u3067\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\uff0eHADOOP_CONF_DIR \u306f\uff0cYARN \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(yarn-site.xml)\u304a\u3088\u3073 hdfs \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(hdfs-site.xml)\u304c\u3042\u308b\u5834\u6240\u3092\u30dd\u30a4\u30f3\u30c8\u3057\u307e\u3059\uff0e\n\nspark-env.sh\n# Options read in YARN client mode\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_EXECUTOR_INSTANCES, Number of workers to start (Default: 2)\n# - SPARK_EXECUTOR_CORES, Number of cores for the workers (Default: 1).\n# - SPARK_EXECUTOR_MEMORY, Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n# - SPARK_YARN_APP_NAME, The name of your application (Default: Spark)\n# - SPARK_YARN_QUEUE, The hadoop queue to use for allocation requests \n# - SPARK_YARN_DIST_FILES, Comma separated list of files to be distributed with the job.\n# - SPARK_YARN_DIST_ARCHIVES, Comma separated list of archives to be distributed with the job.\nHADOOP_CONF_DIR=/home/ozawa/hadoop/etc/hadoop/\nSPARK_EXECUTOR_INSTANCES=16\nSPARK_EXECUTOR_MEMORY=4G\nSPARK_DRIVER_MEMORY=2G\n\n\n\nspark-defaults.conf\nspark.serializer                   org.apache.spark.serializer.KryoSerializer\nspark.shuffle.manager              SORT\nspark.shuffle.consolidateFiles     true\nspark.shuffle.spill                true\n\n\n\nSparkSQL \u3092\u8d77\u52d5\u3059\u308b\nSparkSQL \u3092\u8d77\u52d5\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n$ bin/spark-sql --master yarn-client\n\n--master yarn-client \u3092\u6e21\u3059\u3053\u3068\u3067\uff0cYARN \u306e\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u307f\uff0cSpark \u306e Application Master \u304c\u7acb\u3061\u4e0a\u304c\u308a\u307e\u3059\uff0e\nspark-sql > \n\nSparkSQL \u4e0a\u3067\u306f\uff0cHive \u4e92\u63db\u30af\u30a8\u30ea\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0eexternal table \u3092\u4f7f\u3063\u3066\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u307f\u307e\u3057\u3087\u3046\uff0e\nspark-sql > create external table if NOT EXISTS text100GB(text string) location 'hdfs:///user/ozawa/text';\n\n\n\u3053\u3053\u307e\u3067\u3067\u524d\u6e96\u5099\u306f\u5b8c\u4e86\u3067\uff0c\u3053\u306e table \u306b\u5bfe\u3057\u3066\uff0cHiveQL \u30af\u30a8\u30ea\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\uff0e\u3042\u3068\u306f Hive \u3068\u540c\u3058\u8981\u9818\u3067 HiveQL \u3092\u8a18\u8ff0\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\uff0e\n\nSparkSQL \u306e\u73fe\u72b6\nHive 0.12 \u30d9\u30fc\u30b9\u3067\u4f5c\u3089\u308c\u3066\u3044\u308b\u3053\u3068\u3082\u5f71\u97ff\u3057\u3066\u3044\u308b\u306e\u304b\uff0cHive \u3067\u52d5\u304f\u30af\u30a8\u30ea\u306e\u4e00\u90e8\u306f parse error \u306b\u306a\u3063\u305f\u308a\u3057\u307e\u3059\uff0e\u4e00\u65b9\u3067\uff0cHive on Spark \u3068\u3044\u3046\u53d6\u308a\u7d44\u307f\u3082\u884c\u308f\u308c\u3066\u304a\u308a\uff0c\u4e92\u63db\u6027\u3084\u65e2\u5b58\u30c4\u30fc\u30eb\u3068\u306e\u7d71\u5408\u306e\u9762\u3067\u306f Hive on SQL \u306b\u4e00\u65e5\u306e\u9577\u304c\u3042\u308a\u307e\u305d\u3046\u3067\u3059\uff0e\n\u3069\u3061\u3089\u304c\u666e\u53ca\u3057\u3066\u304f\u308b\u306e\u304b\uff0c\u898b\u308b\u306e\u304c\u697d\u3057\u307f\u306a\u72b6\u6cc1\u306b\u3042\u308a\u307e\u3059\u306e\u3067\uff0c\u9069\u5b9c\u5171\u6709\u3067\u304d\u308c\u3070\u3068\u601d\u3044\u307e\u3059\uff0e\n\n\u672c\u65e5\u306f Hive on Tez \u306e\u8a71\u3092\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3057\u305f\u304c\uff0c[\u6628\u65e5\u306e\u8a18\u4e8b](http://linux.wwing.net/WordPress/?p=1402) \u306e\u4e2d\u3067\u3082\u89e6\u308c\u3089\u308c\u3066\u3044\u308b Spark \u304b\u3064 SQL on Hadoop \u3067\u3042\u308b SparkSQL on YARN \u306e\u8a71\u306b\u8a71\u984c\u3092\u5909\u66f4\u3057\u3066\u304a\u5c4a\u3051\u3057\u307e\u3059\uff0e\n\n## SparkSQL \u3068\u306f\n\n\u540d\u524d\u901a\u308a\uff0cSpark \u3092\u30e9\u30f3\u30bf\u30a4\u30e0\u3068\u3057\u305f SQL \u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3067\u3059\uff0e\u30c7\u30b6\u30a4\u30f3\u306e\u7279\u5fb4\u3068\u3057\u3066\uff0cSparkDSL \u3068\u306e\u89aa\u548c\u6027\u3068\uff0cHive \u306e Metastore, SerDe, UDF \u4e92\u63db\u6027\u304c\u4fdd\u305f\u308c\u3066\u3044\u308b\u3068\u3044\u3046\u70b9\u304c\u3042\u3052\u3089\u308c\u307e\u3059\uff0eSparkSQL \u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3068\u3057\u3066\u306f\uff0c\u4ee5\u4e0b\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059:\n\n* [SparkSQL](https://spark.apache.org/sql/)\n* [Spark SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n\n## Spark on YARN \u3067\u306e HiveQL \u4e92\u63db\u30af\u30a8\u30ea\u306e\u52d5\u304b\u3057\u65b9\n\n\u3053\u3053\u304b\u3089\u306f\uff0cHadoop 2 \u30af\u30e9\u30b9\u30bf\u3092\u65e2\u306b\u6301\u3063\u3066\u3044\u308b\u65b9\u306e\u305f\u3081\u306b\uff0cSpark on YARN \u306b\u304a\u3051\u308b Spark \u306e\u52d5\u4f5c\u306e\u3055\u305b\u65b9\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\uff0e\u306a\u304a\uff0c\u4eca\u56de\u306e\u8aac\u660e\u3067\u306f\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u56fa\u6709\u306e\u8aac\u660e\u306f\u884c\u3044\u307e\u305b\u3093\uff0e\n\n### Spark \u5074\u306e\u8a2d\u5b9a\n\n\u3042\u306a\u305f\u306e\u5229\u7528\u3057\u3066\u3044\u308b Hadoop2 \u7cfb\u30af\u30e9\u30b9\u30bf\u304c [Spark \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u30da\u30fc\u30b8](https://spark.apache.org/downloads.html)\u306b\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\uff0cSpark \u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u3066 Spark \u3092\u30d3\u30eb\u30c9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\u4f8b\u3048\u3070\uff0c\u6700\u65b0\u7248\u3067\u3042\u308b Hadoop 2.5.2 \u3084 2.6.0 \u3092\u5229\u7528\u3057\u3088\u3046\u3068\u3057\u305f\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u624b\u9806\u3067 Spark \u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\uff0e\n\n```bash\nexport MAVEN_OPTS=\"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"\ntar xzvf spark-1.1.1.tgz\ncd spark\n./make-distribution.sh --tgz -Phadoop-2.4 -Pyarn,hive,hive-thriftserver -Dhadoop.version=2.5.2\n```\n\n\u3059\u308b\u3068\uff0cspark-<version>.tgz \u304c\u751f\u6210\u3055\u308c\u308b\u306e\u3067\u3053\u308c\u3092\u89e3\u51cd\u3057\uff0cSpark \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(conf/spark-env.sh, conf/spark-defaults.conf)\u3092\u8a18\u8ff0\u3057\u307e\u3059\uff0e\u3053\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u306f\uff0cSpark \u8d77\u52d5\u6642\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u8a2d\u5b9a\u3092\u8a18\u8ff0\u3057\u307e\u3059\uff0e\n\nspark-env.sh \u306f\uff0cSpark on YARN \u8d77\u52d5\u306b\u5fc5\u9808\u306e\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\uff0e\u7279\u306b\uff0cHADOOP_CONF_DIR \u306e\u8a2d\u5b9a\u306f\u5fc5\u9808\u306b\u306a\u308b\u306e\u3067\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\uff0eHADOOP_CONF_DIR \u306f\uff0cYARN \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(yarn-site.xml)\u304a\u3088\u3073 hdfs \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(hdfs-site.xml)\u304c\u3042\u308b\u5834\u6240\u3092\u30dd\u30a4\u30f3\u30c8\u3057\u307e\u3059\uff0e\n\n```spark-env.sh\n# Options read in YARN client mode\n# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n# - SPARK_EXECUTOR_INSTANCES, Number of workers to start (Default: 2)\n# - SPARK_EXECUTOR_CORES, Number of cores for the workers (Default: 1).\n# - SPARK_EXECUTOR_MEMORY, Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n# - SPARK_YARN_APP_NAME, The name of your application (Default: Spark)\n# - SPARK_YARN_QUEUE, The hadoop queue to use for allocation requests \n# - SPARK_YARN_DIST_FILES, Comma separated list of files to be distributed with the job.\n# - SPARK_YARN_DIST_ARCHIVES, Comma separated list of archives to be distributed with the job.\nHADOOP_CONF_DIR=/home/ozawa/hadoop/etc/hadoop/\nSPARK_EXECUTOR_INSTANCES=16\nSPARK_EXECUTOR_MEMORY=4G\nSPARK_DRIVER_MEMORY=2G\n```\n\n```spark-defaults.conf\nspark.serializer                   org.apache.spark.serializer.KryoSerializer\nspark.shuffle.manager              SORT\nspark.shuffle.consolidateFiles     true\nspark.shuffle.spill                true\n```\n\n## SparkSQL \u3092\u8d77\u52d5\u3059\u308b\n\nSparkSQL \u3092\u8d77\u52d5\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\n\n```bash\n$ bin/spark-sql --master yarn-client\n```\n\n--master yarn-client \u3092\u6e21\u3059\u3053\u3068\u3067\uff0cYARN \u306e\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u307f\uff0cSpark \u306e Application Master \u304c\u7acb\u3061\u4e0a\u304c\u308a\u307e\u3059\uff0e\n\n```\nspark-sql > \n```\n\nSparkSQL \u4e0a\u3067\u306f\uff0cHive \u4e92\u63db\u30af\u30a8\u30ea\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0eexternal table \u3092\u4f7f\u3063\u3066\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\u307f\u307e\u3057\u3087\u3046\uff0e\n\n```\nspark-sql > create external table if NOT EXISTS text100GB(text string) location 'hdfs:///user/ozawa/text';\n\n```\n\n\u3053\u3053\u307e\u3067\u3067\u524d\u6e96\u5099\u306f\u5b8c\u4e86\u3067\uff0c\u3053\u306e table \u306b\u5bfe\u3057\u3066\uff0cHiveQL \u30af\u30a8\u30ea\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\uff0e\u3042\u3068\u306f Hive \u3068\u540c\u3058\u8981\u9818\u3067 HiveQL \u3092\u8a18\u8ff0\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\uff0e\n\n## SparkSQL \u306e\u73fe\u72b6\n\nHive 0.12 \u30d9\u30fc\u30b9\u3067\u4f5c\u3089\u308c\u3066\u3044\u308b\u3053\u3068\u3082\u5f71\u97ff\u3057\u3066\u3044\u308b\u306e\u304b\uff0cHive \u3067\u52d5\u304f\u30af\u30a8\u30ea\u306e\u4e00\u90e8\u306f parse error \u306b\u306a\u3063\u305f\u308a\u3057\u307e\u3059\uff0e\u4e00\u65b9\u3067\uff0cHive on Spark \u3068\u3044\u3046\u53d6\u308a\u7d44\u307f\u3082\u884c\u308f\u308c\u3066\u304a\u308a\uff0c\u4e92\u63db\u6027\u3084\u65e2\u5b58\u30c4\u30fc\u30eb\u3068\u306e\u7d71\u5408\u306e\u9762\u3067\u306f Hive on SQL \u306b\u4e00\u65e5\u306e\u9577\u304c\u3042\u308a\u307e\u305d\u3046\u3067\u3059\uff0e\n\u3069\u3061\u3089\u304c\u666e\u53ca\u3057\u3066\u304f\u308b\u306e\u304b\uff0c\u898b\u308b\u306e\u304c\u697d\u3057\u307f\u306a\u72b6\u6cc1\u306b\u3042\u308a\u307e\u3059\u306e\u3067\uff0c\u9069\u5b9c\u5171\u6709\u3067\u304d\u308c\u3070\u3068\u601d\u3044\u307e\u3059\uff0e\n\n", "tags": ["Spark"]}