{"context": " More than 1 year has passed since last update.hive\u306e\u7df4\u7fd2\u3068\u3057\u3066\u3001\u516c\u958b\u3055\u308c\u3066\u3044\u308blivedoor\u30b0\u30eb\u30e1\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u74b0\u5883\u69cb\u7bc9\n\nInstall Java7, CDH4(\u64ec\u4f3c\u5206\u6563) and Hive\n\u57fa\u672c\u7684\u306b\u306fCDH\u306e\u516c\u5f0f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306b\u5f93\u3063\u3066\u3044\u307e\u3059\u3002\nhttp://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.2.0/CDH4-Quick-Start/cdh4qs_topic_3.html\n\u3068\u308a\u3042\u3048\u305a\u52d5\u304f\u3088\u3046\u306b\u3068\u3044\u3046\u610f\u56f3\u306a\u306e\u3067\u3001\u6539\u5584\u306e\u4f59\u5730\u306f\u591a\u5206\u3042\u308a\u307e\u3059\u3002\n# java\nsudo apt-get -y install python-software-properties\nsudo add-apt-repository -y ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get -y install oracle-java7-installer\necho \"export JAVA_HOME=/usr/lib/jvm/java-7-oracle\" >> ~/.bashrc\necho \"export PATH=$PATH:$JAVA_HOME/bin\" >> ~/.bashrc\n\n# CDH\ncurl -O http://archive.cloudera.com/cdh4/one-click-install/squeeze/amd64/cdh4-repository_1.0_all.deb\nsudo dpkg -i cdh4-repository_1.0_all.deb\ncurl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install hadoop-0.20-conf-pseudo\nsudo -u hdfs hdfs namenode -format\nfor x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done\nsudo -u hdfs hadoop fs -mkdir /tmp\nsudo -u hdfs hadoop fs -chmod -R 1777 /tmp\nsudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging\nsudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging\nsudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred\nfor x in `cd /etc/init.d ; ls hadoop-0.20-mapreduce-*` ; do sudo service $x start ; done\nsudo -u hdfs hadoop fs -mkdir /user/hdfs\nsudo -u hdfs hadoop fs -chown hdfs /user/hdfs\n\n# Hive\nsudo apt-get -y install hive\nsudo chmod -R 777 /var/lib/hive/metastore/metastore_db\nsudo -u hdfs hadoop fs -chmod -R 777 /user\nsudo -u hdfs hadoop fs -mkdir /user/vagrant\nsudo -u hdfs hadoop fs -chown -R vagrant /user/vagrant\nsudo chown vagrant:vagrant -R /var/lib/hive\n\n# \u52d5\u4f5c\u78ba\u8a8d\n$ hive\nhive> show databases;\nOK\ndefault\n\n\n\u30c7\u30fc\u30bf\u6295\u5165\n\nlivedoor\u30b0\u30eb\u30e1\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\nhttps://github.com/livedoor/datasets\n\ndatabase\u4f5c\u6210\nhive> create database ldgourmet;\n\n\nratings\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\nhive> create table ratings (id int, restaurant_id int, user_id string, total int, food int, service int, atmosphere int, cost_performance int, title string, body string, purpose int, created_on timestamp) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './ratings.csv' overwrite into table ratings;\n\n\nrestaurants\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\ncreate table restaurants (id int, name string, property string, alphabet string, name_kana string, pref_id int, area_id int, station_id1 int, station_time1 int, station_distance1 int, station_id2 int, station_time2 int, station_distance2 int, station_id3 int, station_time3 int, station_distance3 int, category_id1 int, category_id2 int, category_id3 int, category_id4 int, category_id5 int, zip string , address string , north_latitude float, east_longitude float, description string, purpose int, open_morning boolean, open_lunch boolean, open_late boolean, photo_count int, special_count int, menu_count int, fan_count int, access_count int, created_on timestamp, modified_on timestamp, closed boolean) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './restaurants.csv' overwrite into table restaurants;\n\n\narea\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\ncreate table areas (id int, pref_id int, name string) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './area.csv' overwrite into table areas;\n\n\n\u96c6\u8a08\u51e6\u7406\n\n\u9280\u5ea7\u30fb\u65e5\u672c\u6a4b\u30fb\u65b0\u6a4b\u5730\u57df(aera.id = 1)\u306b\u3042\u308b\u5404\u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u5e73\u5747\u30ec\u30fc\u30c8\nhive> select avg(ratings.total), restaurants.id, max(restaurants.name)  from ratings join restaurants on (ratings.restaurant_id = restaurants.id) where restaurants.area_id = 1 group by restaurants.id\n...\n\n\n\u6771\u4eac\u306e\u5404\u5730\u57df\u3054\u3068\u306e\u5e73\u5747\u70b9\nhive> select avg(ratings.total), max(areas.name) from ratings join restaurants on (ratings.restaurant_id = restaurants.id) join areas on (restaurants.area_id = areas.id) where areas.pref_id = 13 group by areas.id;\n\n\u305d\u306e\u4ed6\u8272\u3005\u3084\u3063\u305f\u3002\n\ndynamic partition\u3092\u4f7f\u3063\u3066\u65e2\u5b58\u306e\u30c6\u30fc\u30d6\u30eb(area)\u304b\u3089partition\u3092\u6301\u3064\u30c6\u30fc\u30d6\u30eb(p_areas)\u3092\u4f5c\u308b\u3002\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3082\u8a66\u3057\u3066\u307f\u308b\u3002\u5143\u30c7\u30fc\u30bf\u3092\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3067\u5206\u3051\u3066\u30ed\u30fc\u30c9\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u9762\u5012\u81ed\u3044\u306e\u3067\u3001\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3068\u3084\u3089\u3092\u8a66\u3057\u3066\u307f\u305f\u3002\nhive> create table p_areas (id int, name string) partitioned by (pref_id int) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nset hive.exec.dynamic.partition.mode=nonstrict; \nset hive.exec.dynamic.partition=true;\n\nfrom areas insert overwrite table p_areas partition(pref_id) select id, name, pref_id;\n\n-- partition\u6570\u304c\u5927\u304d\u304f\u306a\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u691c\u8a0e\u3059\u308b\u3002\n-- hive.exec.max.dynamic.partitions.pernode\n-- hive.exec.max.dynamic.partitions\n\n\npython\u304b\u3089\npython\u304b\u3089hive\u3092\u53e9\u304f\u306b\u306f\u3001HiveServer2\u3092\u4f7f\u3046\u3002\u8a73\u3057\u304f\u306f\u4ee5\u4e0b\u3002\nhttps://gist.github.com/YoshihitoAso/9632814\nhive\u306e\u7df4\u7fd2\u3068\u3057\u3066\u3001\u516c\u958b\u3055\u308c\u3066\u3044\u308blivedoor\u30b0\u30eb\u30e1\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n# \u74b0\u5883\u69cb\u7bc9\n## Install Java7, CDH4(\u64ec\u4f3c\u5206\u6563) and Hive\n\u57fa\u672c\u7684\u306b\u306fCDH\u306e\u516c\u5f0f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306b\u5f93\u3063\u3066\u3044\u307e\u3059\u3002\nhttp://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/4.2.0/CDH4-Quick-Start/cdh4qs_topic_3.html\n\n\u3068\u308a\u3042\u3048\u305a\u52d5\u304f\u3088\u3046\u306b\u3068\u3044\u3046\u610f\u56f3\u306a\u306e\u3067\u3001\u6539\u5584\u306e\u4f59\u5730\u306f\u591a\u5206\u3042\u308a\u307e\u3059\u3002\n\n```bash\n# java\nsudo apt-get -y install python-software-properties\nsudo add-apt-repository -y ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get -y install oracle-java7-installer\necho \"export JAVA_HOME=/usr/lib/jvm/java-7-oracle\" >> ~/.bashrc\necho \"export PATH=$PATH:$JAVA_HOME/bin\" >> ~/.bashrc\n\n# CDH\ncurl -O http://archive.cloudera.com/cdh4/one-click-install/squeeze/amd64/cdh4-repository_1.0_all.deb\nsudo dpkg -i cdh4-repository_1.0_all.deb\ncurl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install hadoop-0.20-conf-pseudo\nsudo -u hdfs hdfs namenode -format\nfor x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done\nsudo -u hdfs hadoop fs -mkdir /tmp\nsudo -u hdfs hadoop fs -chmod -R 1777 /tmp\nsudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging\nsudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging\nsudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred\nfor x in `cd /etc/init.d ; ls hadoop-0.20-mapreduce-*` ; do sudo service $x start ; done\nsudo -u hdfs hadoop fs -mkdir /user/hdfs\nsudo -u hdfs hadoop fs -chown hdfs /user/hdfs\n\n# Hive\nsudo apt-get -y install hive\nsudo chmod -R 777 /var/lib/hive/metastore/metastore_db\nsudo -u hdfs hadoop fs -chmod -R 777 /user\nsudo -u hdfs hadoop fs -mkdir /user/vagrant\nsudo -u hdfs hadoop fs -chown -R vagrant /user/vagrant\nsudo chown vagrant:vagrant -R /var/lib/hive\n\n# \u52d5\u4f5c\u78ba\u8a8d\n$ hive\nhive> show databases;\nOK\ndefault\n```\n\n# \u30c7\u30fc\u30bf\u6295\u5165\n## livedoor\u30b0\u30eb\u30e1\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\nhttps://github.com/livedoor/datasets\n\n## database\u4f5c\u6210\n```\nhive> create database ldgourmet;\n```\n\n## ratings\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\n```sql\nhive> create table ratings (id int, restaurant_id int, user_id string, total int, food int, service int, atmosphere int, cost_performance int, title string, body string, purpose int, created_on timestamp) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './ratings.csv' overwrite into table ratings;\n```\n \n## restaurants\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\n```sql\ncreate table restaurants (id int, name string, property string, alphabet string, name_kana string, pref_id int, area_id int, station_id1 int, station_time1 int, station_distance1 int, station_id2 int, station_time2 int, station_distance2 int, station_id3 int, station_time3 int, station_distance3 int, category_id1 int, category_id2 int, category_id3 int, category_id4 int, category_id5 int, zip string , address string , north_latitude float, east_longitude float, description string, purpose int, open_morning boolean, open_lunch boolean, open_late boolean, photo_count int, special_count int, menu_count int, fan_count int, access_count int, created_on timestamp, modified_on timestamp, closed boolean) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './restaurants.csv' overwrite into table restaurants;\n```\n \n## area\u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\uff06\u30ed\u30fc\u30c9\n```sql\ncreate table areas (id int, pref_id int, name string) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nload data local inpath './area.csv' overwrite into table areas;\n```\n \n# \u96c6\u8a08\u51e6\u7406\n## \u9280\u5ea7\u30fb\u65e5\u672c\u6a4b\u30fb\u65b0\u6a4b\u5730\u57df(aera.id = 1)\u306b\u3042\u308b\u5404\u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u5e73\u5747\u30ec\u30fc\u30c8\n```sql\nhive> select avg(ratings.total), restaurants.id, max(restaurants.name)  from ratings join restaurants on (ratings.restaurant_id = restaurants.id) where restaurants.area_id = 1 group by restaurants.id\n...\n```\n\n## \u6771\u4eac\u306e\u5404\u5730\u57df\u3054\u3068\u306e\u5e73\u5747\u70b9\n```sql\nhive> select avg(ratings.total), max(areas.name) from ratings join restaurants on (ratings.restaurant_id = restaurants.id) join areas on (restaurants.area_id = areas.id) where areas.pref_id = 13 group by areas.id;\n```\n\n\u305d\u306e\u4ed6\u8272\u3005\u3084\u3063\u305f\u3002\n \n## dynamic partition\u3092\u4f7f\u3063\u3066\u65e2\u5b58\u306e\u30c6\u30fc\u30d6\u30eb(area)\u304b\u3089partition\u3092\u6301\u3064\u30c6\u30fc\u30d6\u30eb(p_areas)\u3092\u4f5c\u308b\u3002\n\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3082\u8a66\u3057\u3066\u307f\u308b\u3002\u5143\u30c7\u30fc\u30bf\u3092\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3067\u5206\u3051\u3066\u30ed\u30fc\u30c9\u3059\u308b\u3068\u3044\u3046\u306e\u306f\u9762\u5012\u81ed\u3044\u306e\u3067\u3001\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3068\u3084\u3089\u3092\u8a66\u3057\u3066\u307f\u305f\u3002\n\n```sql\nhive> create table p_areas (id int, name string) partitioned by (pref_id int) row format delimited fields terminated by ',' lines terminated by '\\n';\n\nset hive.exec.dynamic.partition.mode=nonstrict; \nset hive.exec.dynamic.partition=true;\n\nfrom areas insert overwrite table p_areas partition(pref_id) select id, name, pref_id;\n \n-- partition\u6570\u304c\u5927\u304d\u304f\u306a\u308b\u5834\u5408\u306f\u4ee5\u4e0b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u691c\u8a0e\u3059\u308b\u3002\n-- hive.exec.max.dynamic.partitions.pernode\n-- hive.exec.max.dynamic.partitions\n```\n\n## python\u304b\u3089\npython\u304b\u3089hive\u3092\u53e9\u304f\u306b\u306f\u3001HiveServer2\u3092\u4f7f\u3046\u3002\u8a73\u3057\u304f\u306f\u4ee5\u4e0b\u3002\nhttps://gist.github.com/YoshihitoAso/9632814\n", "tags": ["hive"]}