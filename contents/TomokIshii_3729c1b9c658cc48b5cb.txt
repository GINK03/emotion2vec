{"context": "\n\n\u306f\u3058\u3081\u306b\n\u6a5f\u68b0\u5b66\u7fd2\u30b3\u30f3\u30da\u30b5\u30a4\u30c8\"Kaggle\"\u306b\u3066\u8a71\u984c\u306b\u4e0a\u304c\u308bLightGBM\u3067\u3042\u308b\u304c\uff0cMicrosoft\u304c\u95a2\u308f\u308bGradient Boosting\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4e00\u3064\u3067\u3042\u308b\uff0eGradient Boosting\u3068\u3044\u3046\u3068\u771f\u3063\u5148\u306bXGBoost\u304c\u601d\u3044\u6d6e\u304b\u3076\u3068\u601d\u3046\u304c\uff0cLightGBM\u306f\u9593\u9055\u3044\u306a\u304fXGBoost\u306e\u5bfe\u6297\u4f4d\u7f6e\u3092\u306d\u3089\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\uff0e\u7406\u8ad6\u306e\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u7167\u3044\u305f\u3060\u304f\u3068\u3057\u3066\uff0c\u672c\u8a18\u4e8b\u3067\u306f\u300c\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u3068\u3044\u3046\u3053\u3068\u3067\u65b0\u3057\u3044\u30c4\u30fc\u30eb\u306e\u4f7f\u3044\u65b9\u3092\u7d39\u4ecb\u3057\u305f\u3044\uff0e\n\u307e\u305a\uff0c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5192\u982d\u304b\u3089\u5f15\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u304f\uff0e\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. \nIt is designed to be distributed and efficient with the following advantages:\n\nFaster training speed and higher efficiency\nLower memory usage\nBetter accuracy\nParallel learning supported\nCapable of handling large-scale data\n\n\nLightGBM \u306e\u58f2\u308a\u6587\u53e5\u3067\u3042\u308b\u304c\uff0c\u304b\u3044\u6458\u3093\u3067\u8a00\u3046\u3068\u300c\u901f\u304f\u3067\u6b63\u78ba\uff01\u300d\u3068\u3044\u3046\u3053\u3068\uff0eXGBoost\u306e\u7279\u9577\u3068\u30c0\u30d6\u3063\u3066\u805e\u3053\u3048\u308b\u304c\uff0c\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u69cb\u6210\u3082\u3068\u3066\u3082\u4f3c\u3066\u3044\u308b\uff0e\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30a2\u90e8\u5206\u306f\uff0cC++\uff0e\u305d\u308c\u3092CLI(Command Line Inteface)\u3067\u5b9f\u884c\u53ef\u80fd\uff0e\nPython\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u306f\uff0cPython-Package\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\uff0e\nR\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u306f\uff0cR-Package\uff08\u672c\u7a3f\u57f7\u7b46\u6642\u3067beta\u30d0\u30fc\u30b8\u30e7\u30f3\uff09\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\uff0e\n\n\u4eca\u56de\u306f\uff0cPython\u306b\u3066\u30b3\u30fc\u30c9\u78ba\u8a8d\u3092\u3057\u3066\u307f\u305f\uff0e\uff08\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u74b0\u5883\u306f\uff0cUbuntu 16.04LTS, Python 3.5.2, miniconda3, LightGBM 0.1 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\uff08\u8ffd\u8a18\uff0c2017/1/14, R-Package\u306b\u3064\u3044\u3066\u3082\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\uff0e\uff09\n\nLightGBM\u306e\u7279\u5fb4 \uff70 XGBoost\u3068\u306e\u9055\u3044\n\u7c21\u5358\u306bBoosting\u306e\u30b3\u30f3\u30bb\u30d7\u30c8\u3092\u4e0b\u306e\u56f3\u3067\u78ba\u8a8d\u3059\u308b\uff0e\n\uff08\u300c\u306f\u3058\u3081\u3066\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u300d\u306e\u7b2c11\u7ae0\u304b\u3089\u306e\u5f15\u7528\uff08\u4f53\u88c1\u3092\u5c11\u3057\u5909\u66f4\uff09\u306b\u306a\u308a\u307e\u3059\uff0e\uff09\nFig. Boosting concept\n\n\u6c7a\u5b9a\u6728\u306e\u5f31\u8b58\u5225\u5668\u3092\u76f4\u5217\u306b\u8907\u6570\u4e26\u3079\uff0c\u3053\u308c\u3089\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u91cd\u307f(w1w^1, w2w^2,.. wMw^M)\u3092\u3064\u3051\u305f\u3082\u306e\u3092\u5165\u529b\u3059\u308b\uff0e\u5404\u8b58\u5225\u5668\u306e\u5b66\u7fd2\u306f\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u306b\u884c\u3044\uff0c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u91cd\u307f\u306f\u3053\u306e\u5b66\u7fd2\u7d50\u679c\u3092\u53cd\u6620\u3055\u305b\u308b\uff0e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u8b58\u5225\u5668\u306e\u4e88\u6e2c\u5024\u3068\u3057\u3066\uff0cYMY_M\u306f\uff0c\u5f31\u8b58\u5225\u5668\u306e\u4e88\u6e2c\u5024\u3092\u7dda\u5f62\u7d50\u5408\u3057\u305f\u5024\u3092\u5f97\u308b\uff0e\n\u6b21\u306b\uff0c\"LightGBM\" \u306e\u7279\u5fb4\u3092\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3066\u78ba\u8a8d\u3057\u3066\u304a\u304f\uff0e\n\uff08\u53c2\u8003\uff09Optimization in speed and memory usage\n\"XGBoost\"\u3092\u542b\u3080\u591a\u304f\u306e\u30c4\u30fc\u30eb\u3067\u306f\uff0c\u6c7a\u5b9a\u6728\u5b66\u7fd2\u306e\u306b\"pre-sorted\"\u30d9\u30fc\u30b9\u3067\u884c\u3046\u304c\uff0c\"LightGBM\"\u3067\u306f\"histogram based algorithms\"\u3092\u7528\u3044\u308b\u3068\u306e\u3053\u3068\uff0e\u3053\u306e\u624b\u6cd5\u306e\u7279\u9577\u3068\u3057\u3066\u306f\uff0c\n\nReduce calculation cost of split gain\nUse histogram subtraction for further speed-up\nReduce Memory usage\nReduce communication cost for parallel learning\n\n\u304c\u6319\u3052\u3089\u308c\u308b\u304c\uff0c\u8981\u306f\u300c\u8efd\u304f\u300d\u3066\u300c\u52b9\u7387\u304c\u3044\u3044\u300d\u3068\u3044\u3046\u3053\u3068\u3089\u3057\u3044\uff0e\n\uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u9055\u3044\u8a73\u7d30\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u306f\uff0c\u8ad6\u6587\u3092\u3044\u304f\u3064\u304b\u8aad\u307f\u8fbc\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\u53c2\u8003\u6587\u732e\u30ea\u30b9\u30c8\u306e\u30ea\u30f3\u30af\u306f \u3053\u3061\u3089 \u306b\u306a\u308a\u307e\u3059. \u30cd\u30c3\u30c8\u3067\u691c\u7d22\u3057\u3066\u307f\u307e\u3057\u305f\u304c\uff0c\u5185\u5bb9\u306f\u96e3\u3057\u305d\u3046\u3067\u3059...\uff09\n\u307e\u305f\uff0c\u5b66\u7fd2\u306e\u904e\u7a0b\u3067\uff0c\u591a\u304f\u306e\u30c4\u30fc\u30eb\u304clevel(depth)-wise\u3067\u6728\u3092\u6210\u9577\u3055\u305b\u3066\u3044\u304f\u306e\u306b\u5bfe\u3057\uff0cLightGBM\u3067\u306fleaf-wise\u3067\u6210\u9577\u3055\u305b\u308b\u3068\u306e\u3053\u3068\uff0e\nFig. Leaf-wise tree growth\n\n\uff08\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u5206\u304b\u308a\u3084\u3059\u3044 \u8aac\u660e\u56f3 \u304c\u3042\u308a\u307e\u3059\u306e\u3067\uff0c\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\uff09  \n\u3053\u306eleaf-wise\u306e\u3084\u308a\u65b9\u306b\u5bfe\u5fdc\u3057\u3066\uff0c\"overfit\"\u306b\u95a2\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3068\u306a\u308b\u304c\uff0c\u3053\u308c\u306b\u3064\u3044\u3066\u306f\u5f8c\u3067\u89e6\u308c\u308b\uff0e\n\u4ee5\u4e0a\uff0cXGBoost\u3068LightGBM\u306f\u540c\u3058\u6c7a\u5b9a\u6728(Decision Tree)\u30d9\u30fc\u30b9\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u30e2\u30c7\u30eb\u3067\u3042\u308b\u304c\uff0c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u8a73\u7d30\u306b\u9055\u3044\u304c\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u78ba\u8a8d\u3057\u305f\uff0e\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u6700\u521d\u306e\u30b3\u30fc\u30c9\n\u79c1\u306e\u74b0\u5883(Ubuntu 16.04 + miniconda)\u306b\u304a\u3044\u3066\uff0c\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u624b\u9806\u3067\u554f\u984c\u306a\u304f\u5b8c\u4e86\u3057\u305f\uff0e\u305d\u306e\u5f8c\uff0cPython-Package\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u51e6\u7406\u3067\u5c11\u3057\u624b\u9593\u53d6\u3063\u305f\uff0ePython-Package\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u306f\uff0c\u4ee5\u4e0b\u3092\u5b9f\u65bd\u3059\u308b\u304c\uff0c\ncd python-package; python setup.py install\n\n\u3053\u3053\u3067\u6b21\u306e\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u30b1\u30fc\u30b9\u304c\u3042\u308b\uff0e\nerror: Error: setup script specifies an absolute path:\n\n/Users/Microsoft/LightGBM/python-package/lightgbm/../../lib_lightgbm.so\n\nsetup() arguments must always be /-separated paths relative to the setup.py directory, never absolute paths.\n\n\uff08\u3053\u306e\u300c\u7d76\u5bfe\u30d1\u30b9\u3092\u4f7f\u3046\u306a\u300d\u306e\u30a8\u30e9\u30fc\u306b\u3064\u3044\u3066\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3082\u89e6\u308c\u3089\u308c\u3066\u3044\u308b\u304c\uff0c\u73fe\u8c61/\u518d\u73fe\u6027\u304c\u306f\u3063\u304d\u308a\u3057\u3066\u3044\u306a\u3044\uff0e\u5b9f\u969b\uff0c\u79c1\u30822\u53f0\u306ePC\uff08Ubuntu)\u306bLightGBM\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u304c\uff0c1\u53f0\u3067\u4e0a\u8a18\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\uff0c\u3082\u30461\u53f0\u3067\u306f\u30a8\u30e9\u30fc\u306e\u767a\u751f\u306f\u306a\u304b\u3063\u305f\uff0e\uff09\n\u30a8\u30e9\u30fc\u306e\u539f\u56e0\u306f\uff0cPython\u306e\"setuptools\"\u30d0\u30c3\u30b1\u30fc\u30b8\u5468\u308a\u306b\u3042\u308a\u305d\u3046\u306e\u3067\uff0c\uff08\u3042\u307e\u308a\u63a8\u5968\u3055\u308c\u308b\u65b9\u6cd5\u3067\u306f\u306a\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u304c\uff09\u4ee5\u4e0b\u306e\u3088\u3046\u306bsetup.py\u306e\u4e00\u90e8\u3092\u4fee\u6b63\u3057\u3066\u30a8\u30e9\u30fc\u56de\u907f\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\uff0e\n\u4fee\u6b63\u7b87\u6240\uff1a \u95a2\u6570\u3067`LIB_PATH`\u3092\u751f\u6210\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u3092\u30de\u30b9\u30af\u3057\uff0c\u6240\u5b9a\u306e\u30d1\u30b9\u3092\u30d9\u30bf\u66f8\u304d\uff0e\n\n libpath = {'__file__': libpath_py}\n exec(compile(open(libpath_py, \"rb\").read(), libpath_py, 'exec'), libpath, libpath)\n\n-LIB_PATH = libpath['find_lib_path']()\n+# LIB_PATH = libpath['find_lib_path']()\n+LIB_PATH = ['../lib_lightgbm.so']\n print(\"Install lib_lightgbm from: %s\" % LIB_PATH) \n\n\u3055\u3066\u30b3\u30fc\u30c9\u4f5c\u6210\u306b\u3068\u308a\u304b\u304b\u308b\u304c\uff0c\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u82e5\u3044\u304b\u3089\u304b\u30ea\u30dd\u30b8\u30c8\u30ea\u5185\u306e\u4f8b\u984c\u30b3\u30fc\u30c9\u304c\u5c11\u306a\u3044\uff0e\u4ee5\u4e0b\uff0c\u5c11\u3057\u8a66\u884c\u932f\u8aa4\u3057\u306a\u304c\u3089\uff0c\u5404\u4f8b\u984c\u3092\u6271\u3063\u3066\u307f\u305f\uff0e\n\n\u5206\u985e\u554f\u984c(Classification)\n\u307e\u305a\uff0c\"iris\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u985e\u3067\u3042\u308b\uff0e\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0)\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# LightGBM parameters\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'metric': {'multi_logloss'},\n        'num_class': 3,\n        'learning_rate': 0.1,\n        'num_leaves': 23,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 0\n}\n\n# train\ngbm = lgb.train(params,\n            lgb_train,\n            num_boost_round=50,\n            valid_sets=lgb_eval,\n            early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred = np.argmax(y_pred, axis=1)\n\n\u4f7f\u3044\u65b9\u306f\uff0c\"XGBoost\" \u3068\u304b\u306a\u308a\u4f3c\u3066\u3044\u308b\uff0e\u307e\u305a\uff0clightgbm.Dataset\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3057\u3066\uff0c\u5165\u529b\u30c7\u30fc\u30bf\u3092\u30bb\u30c3\u30c8\u3059\u308b\uff0e\u6240\u5b9a\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u7528\u610f\u3057\u3066\uff0c\u5206\u985e\u5668(Classifier)\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\uff0cTrain\u30c7\u30fc\u30bf\u306bfit\u3055\u305b\u3066\u5206\u985e\u5668\u30e2\u30c7\u30eb\u3092\u5f97\u308b\uff0e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3064\u3044\u3066\u306f\uff0c\"XGBoost\"\u3068\u985e\u4f3c\u3059\u308b\u3068\u3053\u308d\u3082\u3042\u308b\u304c\uff0c\u7570\u306a\u308b\u3068\u3053\u308d\u3082\u3042\u308b\u306e\u3067\u305d\u3053\u306f\u899a\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\n\n\u95a2\u9023\u60c5\u5831\uff08\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\uff09\u3078\u306e\u30ea\u30f3\u30af\n\nParameters\nParameter Tuning\n\n\u4e00\u3064\u899a\u3048\u3066\u304a\u304d\u305f\u3044\u306e\u304c\uff0c\u6c7a\u5b9a\u6728\u306e\u6570\u91cf\u306b\u95a2\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\uff0c\u3053\u308c\u306f\u524d\u8ff0\u3057\u305f\"level(depth)-wise\"\u304b\u3089\"leaf-wise\"\u306b\u5909\u308f\u308b\u3053\u3068\u3092\u8003\u616e\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\uff0e\u4ee5\u4e0b\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u63db\u306b\u3064\u3044\u3066\uff0c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304b\u3089\u5f15\u7528\u3059\u308b\uff0e\n\n\nConvert parameters from XGBoost\nLightGBM uses leaf-wise tree growth algorithm. But other popular tools, e.g. XGBoost, use depth-wise tree growth. So LightGBM use num_leaves to control complexity of tree model, and other tools usually use max_depth. Following table is the correspond between leaves and depths. The relation is num_leaves = 2^(max_depth).\n\n\n\nmax_depth\nnum_leaves\n\n\n\n\n1\n2\n\n\n2\n4\n\n\n3\n8\n\n\n7\n128\n\n\n10\n1024\n\n\n\n\n\u3053\u306e\u30eb\u30fc\u30eb\u3092\u899a\u3048\u3066\u3057\u307e\u3048\u3070\uff0c\u7279\u306b\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3042\u308b\uff0e\u4f8b\u3048\u3070\"XGBoost\"\u306e max_depth=6 \u306e\u8a2d\u5b9a\u3068\u540c\u7b49\u306b\u3059\u308b\u306b\u306f\uff0cnum_leaves=64 \u3068\u8a2d\u5b9a\u3059\u308c\u3070\u3088\u3044\uff0e\n\n\u56de\u5e30\u554f\u984c(Regression)\n\u6b21\u306b\u56de\u5e30(Regression)\u3092\u8a66\u3057\u3066\u307f\u308b\uff0e\u6271\u3063\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\uff0c\"boston\" \uff08housing data set\uff09\u3067\u3042\u308b\uff0e\uff08\u6700\u8fd1\u306eScikit-learn\u306b\u306f \"boston\"\u3082\u4ed8\u5c5e\u3057\u3066\u304a\u308a\uff0cload_boston() \u3067\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\uff09\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, train_test_split\n\nprint(\"Boston Housing: regression\")\nboston = load_boston()\ny = boston['target']\nX = boston['data']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=201612\n)\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n# LightGBM parameters\nparams = {\n        'task' : 'train',\n        'boosting_type' : 'gbdt',\n        'objective' : 'regression',\n        'metric' : {'l2'},\n        'num_leaves' : 31,\n        'learning_rate' : 0.1,\n        'feature_fraction' : 0.9,\n        'bagging_fraction' : 0.8,\n        'bagging_freq': 5,\n        'verbose' : 0\n}\n\n# train\ngbm = lgb.train(params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=lgb_eval,\n            early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\n\u5927\u90e8\u5206\uff0c\u5206\u985e\u554f\u984c(Classification)\u3068\u540c\u3058\u3060\u304c\uff0cLightGBM\u30e2\u30c7\u30eb\u306b\u6e21\u3059\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\uff0c'objective' \u3092 'regression'\u306b\uff08\u5206\u985e(\u591a\u30af\u30e9\u30b9\u5206\u985e\uff09\u3067\u306f 'multiclass')\uff0c'metric' \u3092 'l2' \uff08\u5206\u985e\u3067\u306f 'multi_logloss'\uff09 \u306b\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\n\nScikit-learn\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\n\u3053\u308c\u3082\u5148\u884c\u30c4\u30fc\u30eb\u540c\u69d8\u306b\uff0cScikit-learn \u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\uff0e\u30cd\u30a4\u30c6\u30a3\u30d6\u306b\u8fd1\u3044\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306e\u65b9\u304c\uff0c\u7d30\u304b\u3044\u64cd\u4f5c\u304c\u3067\u304d\u305f\u308a\uff0c\u65b0\u3057\u3044\u6a5f\u80fd\u306e\u30b5\u30dd\u30fc\u30c8\u304c\u65e9\u304b\u3063\u305f\u308a\u3059\u308b\u30b1\u30fc\u30b9\u3082\u3042\u308b\u304c\uff0c\u500b\u4eba\u7684\u306b\u306fScikit-learn\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306e\u65b9\u3092\u4f7f\u3063\u3066\u3044\u304d\u305f\u3044\uff0e\uff08\u6b21\u3005\u3068\u65b0\u898f\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u4e2d\u3067\uff0c\u3067\u304d\u308b\u3060\u3051\u899a\u3048\u308b\u4e8b\u3092\u5c11\u306a\u304f\u3057\u305f\u3044\uff0c\u3068\u3044\u3046\u306e\u304c\u7b2c\u4e00\u306e\u7406\u7531\u3067\u3042\u308b\uff0e\uff09\ngbm = lgb.LGBMClassifier(objective='multiclass',\n                        num_leaves = 31,\n                        learning_rate=0.1,\n                        min_child_samples=10,\n                        n_estimators=100)\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric='multi_logloss',\n        early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\n\u304a\u306a\u3058\u307f\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\uff0c\u5206\u985e\u5668\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3057\u3066\uff0c\u305d\u306e\u5f8c\uff0cfit() \u3059\u308b\u3068\u3044\u3046\u6d41\u308c\u3067\u3042\u308b\uff0e\nearly stopping \u3082\u4e0a\u8a18\u30b3\u30fc\u30c9\u3067\u554f\u984c\u306a\u304f\u52d5\u3044\u3066\u3044\u308b\uff0e\n\u4e3b\u306b\u4f7f\u3046\u30af\u30e9\u30b9\u306f\uff0c\u6b21\u306e2\u3064\u306b\u306a\u308b\uff0e\n\nLGBMClassifier\nLGBMRegressor\n\n\n\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3068\u3057\u3066\n\u524d\u8ff0\u306e\u901a\u308a\uff0cGradient Boosing\u81ea\u4f53\uff0c\u6c7a\u5b9a\u6728\u8b58\u5225\u5668\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u624b\u6cd5\u3067\u3042\u308b\u304c\uff0c\u3053\u3053\u3067\u306f\uff0c\"LightGBM\", \"XGBoost\"\u3092\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3068\u3057\u3066\u7528\u3044\u305f\uff0c\u30e1\u30bf\u30e2\u30c7\u30ea\u30f3\u30b0\u3068\u3044\u3046\u610f\u5473\u3067\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u53d6\u308a\u4e0a\u3052\u308b\uff0e  \n\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3059\u308b\u969b\u306b\u300cBias-Variance \u306b\u3064\u3044\u3066\u7570\u306a\u308b\u51fa\u529b\u3092\u3059\u308b\u8907\u6570\u306e\u30e2\u30c7\u30eb\u3092\u7528\u3044\u308b\u3068\u3088\u3044\u300d\u3068\u3044\u3046\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3042\u308b\u304c\uff0c\u7570\u306a\u308b\u7d50\u679c\u3092\u3082\u3089\u305f\u3089\u3059\u30e2\u30c7\u30eb\u81ea\u4f53\u306e\u4e88\u6e2c\u6027\u80fd\u304c\u3042\u308b\u7a0b\u5ea6\u78ba\u4fdd\u3055\u308c\u3066\u3044\u306a\u3044\u3068\uff0c\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3084\u3063\u3066\u3082\u5168\u4f53\u306e\u7cbe\u5ea6\u304c\u4e0a\u304c\u3089\u306a\u304f\u3066\u60a9\u3080\u3053\u3068\u304c\u591a\u3044\uff0e\u4f8b\u3048\u3070\uff0cLogistic Regression\u3068\uff08\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u3066\u3044\u306a\u3044\uff09Neural Network\u30e2\u30c7\u30eb\u3068XGBoost \u30e2\u30c7\u30eb\u3092\uff0c\u6642\u9593\u3092\u304b\u3051\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u305f\u306b\u3082\u95a2\u308f\u3089\u305a\uff0c\u305d\u306e\u7cbe\u5ea6\u304c\uff08\u4e00\u756a\u9ad8\u6027\u80fd\u306a\uff09XGBoost\u5358\u4f53\u306e\u7d50\u679c\u3068\u5909\u308f\u3089\u306a\u3044\u3068\u3044\u3046\u72b6\u6cc1\u3092\u7d4c\u9a13\u3057\u305f\u3053\u3068\u304c\u3042\u308b\uff0e\u300c\u7c92\u304c\u63c3\u3063\u305f\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u300d\u3092\u7528\u610f\u3059\u308b\u76ee\u7684\u3067\uff0c\"LightGBM\" \u3092 \"XGboost\" \u3068\u4f75\u7528\u3059\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u601d\u3044\u3064\u304f\uff0e\uff08kaggle\u3067\u3082\u305d\u3093\u306a\u30b3\u30fc\u30c9\uff0c\u30d5\u30a9\u30fc\u30e9\u30e0\u610f\u898b\u304c\u3042\u308a\u307e\u3057\u305f\uff0e\uff09\u4ee5\u4e0b\uff0c\u305d\u308c\u305e\u308c\u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u4f8b\u3092\u6319\u3052\u308b\uff0e\n\nLightGBM \u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\ndef lgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n    '''\n      Base analysis process by LightGBM\n    '''\n    kf = KFold(n_splits=n_folds, random_state=1)\n    y_preds_train = []\n    y_preds_test = []\n    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n        gbm = lgb.LGBMClassifier(objective='multiclass',\n                        num_leaves = 23,\n                        learning_rate=0.1,\n                        n_estimators=100)\n        gbm.fit(X_train[train], y_train[train],\n            eval_set=[(X_train[test], y_train[test])],\n            eval_metric='multi_logloss',\n            verbose=False,\n            early_stopping_rounds=10)\n        y_pred_train = gbm.predict_proba(X_train[test], \n                            num_iteration=gbm.best_iteration)\n        y_pred_test = gbm.predict_proba(X_test, \n                            num_iteration=gbm.best_iteration)\n        y_pred_k = np.argmax(y_pred_test, axis=1)\n        accu = accuracy_score(y_test, y_pred_k)\n        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n        y_preds_train.append(y_pred_train)\n        y_preds_test.append(y_pred_test)\n\n    return y_preds_train, y_preds_test\n\n\nXGBoost \u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\ndef xgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n    '''\n      Base analysis process by XGBoost\n    '''\n    kf = KFold(n_splits=n_folds, random_state=1)\n    y_preds_train = []\n    y_preds_test = []\n    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n        xgbclf = xgb.XGBClassifier(objective='multi:softmax',\n                        max_depth=5,\n                        learning_rate=0.1,\n                        n_estimators=100)\n        xgbclf.fit(X_train[train], y_train[train],\n            eval_set=[(X_train[test], y_train[test])],\n            eval_metric='mlogloss',\n            verbose=False,\n            early_stopping_rounds=10)\n        y_pred_train = xgbclf.predict_proba(X_train[test])\n        y_pred_test = xgbclf.predict_proba(X_test)\n        y_pred_k = np.argmax(y_pred_test, axis=1)\n        accu = accuracy_score(y_test, y_pred_k)\n        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n        y_preds_train.append(y_pred_train)\n        y_preds_test.append(y_pred_test)\n\n    return y_preds_train, y_preds_test\n\n\u3053\u308c\u3089\u306e\u95a2\u6570\u3092\u4f7f\u3063\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\uff0cscikit-learn\u306e\"digit dataset\"\u3092\u5206\u985e\u3057\u305f\u5b9f\u884c\u72b6\u6cc1\u304c\u4ee5\u4e0b\u3067\u3042\u308b\uff0e\nLightGBM process:\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  0]: accuracy = 0.9360\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  1]: accuracy = 0.9394\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  2]: accuracy = 0.9394\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\n[LightGBM] [Warning] Ignoring Column_56 , only has one value\nfold[  3]: accuracy = 0.9276\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_24 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  4]: accuracy = 0.9310\nXGBoost process:\nfold[  0]: accuracy = 0.9377\nfold[  1]: accuracy = 0.9411\nfold[  2]: accuracy = 0.9444\nfold[  3]: accuracy = 0.9343\nfold[  4]: accuracy = 0.9310\n\nStacked model:\naccuracy = 0.9512\n\nconfusion matrix:\n[[49  0  0  0  0  0  0  0  0  0]\n [ 1 57  1  0  1  0  0  0  0  1]\n [ 2  0 57  0  0  0  1  0  2  0]\n [ 0  0  0 54  0  0  0  0  0  1]\n [ 0  0  0  0 49  0  0  1  0  0]\n [ 0  0  0  1  0 61  1  0  0  2]\n [ 0  0  1  0  0  0 66  0  0  0]\n [ 0  0  0  0  1  0  0 55  0  0]\n [ 0  4  0  2  0  0  0  0 60  1]\n [ 1  0  0  2  0  2  0  0  0 57]]\n\n\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3067\u82e5\u5e72\u306e\u7cbe\u5ea6\uff08\u6b63\u7b54\u7387\uff09up\u3092\u5b9f\u73fe\u3067\u304d\u3066\u3044\u308b\uff0e\uff08Warning\u304c\u767a\u751f\u3057\u3066\u3044\u307e\u3059\u304c\uff0e\uff09\n\u3044\u304b\u304c\u3060\u308d\u3046\u304b\uff1f \u3053\u3053\u3067\u306f\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb(stacking)\u306b\u3064\u3044\u3066\u306e\u8aac\u660e\u306f\u7701\u304f\u304c\uff0c\u4e0a\u306e2\u3064\u306e\u30b3\u30fc\u30c9\u3067\u96f0\u56f2\u6c17\u306f\u611f\u3058\u3066\u3044\u305f\u3060\u3051\u308b\u304b\u3068\u601d\u3046\uff0e\"LightGBM\", \"XGBoost\" \u5171\u306bScikit-learn API\u3092\u7528\u3044\u308b\u3053\u3068\u306b\u3088\u308a\uff0c\u5927\u90e8\u5206\u540c\u3058\u3067\u3044\u304f\u3064\u304b\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5909\u3048\u308b\u3060\u3051\u3067\uff0c2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3067\u304d\u305f\uff0e  \n\u307e\u305f\uff0c\u6b32\u3057\u3044\u6027\u80fd\u306b\u3064\u3044\u3066\u3082\uff0c\u7d30\u304b\u3044\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u9055\u3044\uff08\"histogram based algorithms\"\u3084 \"leaf-wise growth\"\u306e\u7279\u5fb4)\u306b\u3088\u308a\uff0c\u300c\u4f3c\u3066\u975e\u306a\u308b\u300d\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u306e\u7d50\u679c\u304c\u671f\u5f85\u3067\u304d\u308b\u306e\u3067\u306f\u306a\u3044\u3060\u308d\u3046\u304b\uff1f  \uff08\u307e\u3060\uff0c\u6027\u80fd\u78ba\u8a8d\u4e2d\u3067\u3059\u304c...\uff09\n\u4ee5\u4e0a\uff0c\u7c21\u5358\u306b \"LightGBM\" \u306b\u3064\u3044\u3066\u7d39\u4ecb\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\uff0e\u307e\u3060 ver.0.1 \u3067\u3042\u308b\u304c\uff0c\u304b\u306a\u308a\u5b8c\u6210\u5ea6\u304c\u9ad8\u3044\u5370\u8c61\u3092\u3082\u3063\u305f\uff0eR\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3082R-Package\u304c\u3042\u308b\u306e\u3067\uff0c\u3053\u306e\"LightGBM\"\u3092\u63a8\u5968\u3057\u305f\u3044\uff0e\n\n\uff08\u8ffd\u8a18\uff09R-Package\u306b\u3064\u3044\u3066\u3082\u8a66\u3057\u305f\nR-Package\u3082\u5148\u65e5\uff0c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u305f\u306e\u3067\u8a66\u3057\u3066\u307f\u305f\uff0e\u307e\u305a\uff0cR-Package\u306e\u5165\u308c\u65b9\u306f\uff0c\u6b21\u306e\u901a\u308a\uff0e\n\u65b9\u6cd51\uff1a\uff08\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u5f8c\uff0c\uff09\ncd R-package\nR CMD INSTALL --build  .\n\n\u65b9\u6cd52\uff1a\uff08{devtools}\u3067\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff09\ndevtools::install_github(\"Microsoft/LightGBM\", subdir = \"R-package\")\n\n\u3053\u3061\u3089\u3082 \"iris\" \u306e\u5206\u985e\u3092\u884c\u3063\u305f\uff0e\nlibrary(lightgbm)\ndata(iris)\n\n# split data to train / test\nset.seed(2017)\ntrain_size = 100    # test_size is 50\ntrain_ind <- sample(seq_len(nrow(iris)), size = train_size)\n\ntrain <- iris[train_ind, ]\ntest <- iris[-train_ind, ]\nx_train <- train[, -5]\ny_train <- as.numeric(train[, 5]) - 1   # need zero start labelling, [0, 1, 2]\nx_test <- test[, -5]\ny_test <- as.numeric(test[, 5]) - 1     # need zero start labelling, [0, 1, 2]\n\n# model definition and training\nbst <- lightgbm(data = as.matrix(x_train), label = y_train,\n        num_leaves = 4, learning_rate = 0.1, nrounds = 20, \n        min_data = 20, min_hess = 20,\n        objective = \"multiclass\", metric=\"multi_error\", \n        num_class = 3, verbose = 0)\n\n# make prediction\ny_pred_proba <- matrix(predict(bst, as.matrix(x_test)), byrow=T, ncol=3)\ny_pred <- apply(y_pred_proba, 1, which.max) - 1\n\n# confusion matrix\ncat(\"\\n confusion matrix:\\n\")\ntable(y_test, y_pred)\n\n\u5c11\u3057\u306f\u307e\u3063\u305f\u306e\u304c\uff0c\u201diris\" \u306e\u6570\u5024\u5316\u3057\u305f\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\u304c [1, 2, 3] \u3067\u306f\u201dLightGBM\"\u306e\u95a2\u6570\u304c\u53d7\u3051\u4ed8\u3051\u305a\uff0c[0, 1, 2] \u306b\u30b7\u30d5\u30c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u70b9\u3067\u3042\u308b\uff0e\uff08\u521d\u898b\u306e\u30a8\u30e9\u30fc\u3067\u3057\u305f\u306e\u3067\uff0c\u5c11\u3057\u6238\u60d1\u3044\u307e\u3057\u305f\uff0e\uff09\n\n\u53c2\u8003\u6587\u732e\uff0cweb site\n\nMiscrosoft/LightGBM - GitHub\nhttps://github.com/Microsoft/LightGBM\n\nLightGBM\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u89e6\u3063\u3066\u307f\u305f - marugari2\u3055\u3093\u30d6\u30ed\u30b0\nhttp://marugari2.hatenablog.jp/entry/2016/12/14/235747\n\n\u306f\u3058\u3081\u3066\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\uff0c\u5e73\u4e95\u6c0f\u8457\uff0c\u68ee\u5317\u51fa\u7248\nhttps://www.morikita.co.jp/books/book/2235\n\n\n## \u306f\u3058\u3081\u306b\n\u6a5f\u68b0\u5b66\u7fd2\u30b3\u30f3\u30da\u30b5\u30a4\u30c8\"Kaggle\"\u306b\u3066\u8a71\u984c\u306b\u4e0a\u304c\u308b**LightGBM**\u3067\u3042\u308b\u304c\uff0cMicrosoft\u304c\u95a2\u308f\u308bGradient Boosting\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4e00\u3064\u3067\u3042\u308b\uff0eGradient Boosting\u3068\u3044\u3046\u3068\u771f\u3063\u5148\u306b**XGBoost**\u304c\u601d\u3044\u6d6e\u304b\u3076\u3068\u601d\u3046\u304c\uff0c**LightGBM**\u306f\u9593\u9055\u3044\u306a\u304f**XGBoost**\u306e\u5bfe\u6297\u4f4d\u7f6e\u3092\u306d\u3089\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\uff0e\u7406\u8ad6\u306e\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u7167\u3044\u305f\u3060\u304f\u3068\u3057\u3066\uff0c\u672c\u8a18\u4e8b\u3067\u306f\u300c\u30cf\u30f3\u30ba\u30aa\u30f3\u300d\u3068\u3044\u3046\u3053\u3068\u3067\u65b0\u3057\u3044\u30c4\u30fc\u30eb\u306e\u4f7f\u3044\u65b9\u3092\u7d39\u4ecb\u3057\u305f\u3044\uff0e\n\n\u307e\u305a\uff0c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5192\u982d\u304b\u3089\u5f15\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u304f\uff0e\n> LightGBM is a gradient boosting framework that uses tree based learning algorithms. \n> It is designed to be distributed and efficient with the following advantages:\n>\n> - Faster training speed and higher efficiency\n> - Lower memory usage\n> - Better accuracy\n> - Parallel learning supported\n> - Capable of handling large-scale data\n\n**LightGBM** \u306e\u58f2\u308a\u6587\u53e5\u3067\u3042\u308b\u304c\uff0c\u304b\u3044\u6458\u3093\u3067\u8a00\u3046\u3068\u300c\u901f\u304f\u3067\u6b63\u78ba\uff01\u300d\u3068\u3044\u3046\u3053\u3068\uff0e**XGBoost**\u306e\u7279\u9577\u3068\u30c0\u30d6\u3063\u3066\u805e\u3053\u3048\u308b\u304c\uff0c\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u69cb\u6210\u3082\u3068\u3066\u3082\u4f3c\u3066\u3044\u308b\uff0e\n\n- \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30a2\u90e8\u5206\u306f\uff0cC++\uff0e\u305d\u308c\u3092CLI(Command Line Inteface)\u3067\u5b9f\u884c\u53ef\u80fd\uff0e\n- Python\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u306f\uff0cPython-Package\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\uff0e\n- R\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u306f\uff0cR-Package\uff08\u672c\u7a3f\u57f7\u7b46\u6642\u3067beta\u30d0\u30fc\u30b8\u30e7\u30f3\uff09\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\uff0e\n\n\u4eca\u56de\u306f\uff0cPython\u306b\u3066\u30b3\u30fc\u30c9\u78ba\u8a8d\u3092\u3057\u3066\u307f\u305f\uff0e\uff08\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u74b0\u5883\u306f\uff0cUbuntu 16.04LTS, Python 3.5.2, miniconda3, LightGBM 0.1 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n**\uff08\u8ffd\u8a18\uff0c2017/1/14, R-Package\u306b\u3064\u3044\u3066\u3082[\u8a66\u3057\u3066\u307f\u307e\u3057\u305f](http://qiita.com/TomokIshii/items/3729c1b9c658cc48b5cb#%E8%BF%BD%E8%A8%98r-package%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%82%82%E8%A9%A6%E3%81%97%E3%81%9F)\uff0e\uff09**\n\n## LightGBM\u306e\u7279\u5fb4 \uff70 XGBoost\u3068\u306e\u9055\u3044\n\u7c21\u5358\u306bBoosting\u306e\u30b3\u30f3\u30bb\u30d7\u30c8\u3092\u4e0b\u306e\u56f3\u3067\u78ba\u8a8d\u3059\u308b\uff0e\n\uff08\u300c\u306f\u3058\u3081\u3066\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u300d\u306e\u7b2c11\u7ae0\u304b\u3089\u306e\u5f15\u7528\uff08\u4f53\u88c1\u3092\u5c11\u3057\u5909\u66f4\uff09\u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n**Fig. Boosting concept**\n![AdaBoost_diagram.PNG](https://qiita-image-store.s3.amazonaws.com/0/74152/144e5e35-1a45-bd45-1298-67d4add6130d.png)\n\n\u6c7a\u5b9a\u6728\u306e\u5f31\u8b58\u5225\u5668\u3092\u76f4\u5217\u306b\u8907\u6570\u4e26\u3079\uff0c\u3053\u308c\u3089\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u91cd\u307f($w^1$, $w^2$,.. $w^M$)\u3092\u3064\u3051\u305f\u3082\u306e\u3092\u5165\u529b\u3059\u308b\uff0e\u5404\u8b58\u5225\u5668\u306e\u5b66\u7fd2\u306f\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u306b\u884c\u3044\uff0c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u91cd\u307f\u306f\u3053\u306e\u5b66\u7fd2\u7d50\u679c\u3092\u53cd\u6620\u3055\u305b\u308b\uff0e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u8b58\u5225\u5668\u306e\u4e88\u6e2c\u5024\u3068\u3057\u3066\uff0c$Y_M$\u306f\uff0c\u5f31\u8b58\u5225\u5668\u306e\u4e88\u6e2c\u5024\u3092\u7dda\u5f62\u7d50\u5408\u3057\u305f\u5024\u3092\u5f97\u308b\uff0e\n\n\u6b21\u306b\uff0c\"LightGBM\" \u306e\u7279\u5fb4\u3092\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3066\u78ba\u8a8d\u3057\u3066\u304a\u304f\uff0e\n\uff08\u53c2\u8003\uff09[Optimization in speed and memory usage](https://github.com/Microsoft/LightGBM/wiki/Features#optimization-in-speed-and-memory-usage)\n\n\"XGBoost\"\u3092\u542b\u3080\u591a\u304f\u306e\u30c4\u30fc\u30eb\u3067\u306f\uff0c\u6c7a\u5b9a\u6728\u5b66\u7fd2\u306e\u306b\"pre-sorted\"\u30d9\u30fc\u30b9\u3067\u884c\u3046\u304c\uff0c\"LightGBM\"\u3067\u306f\"histogram based algorithms\"\u3092\u7528\u3044\u308b\u3068\u306e\u3053\u3068\uff0e\u3053\u306e\u624b\u6cd5\u306e\u7279\u9577\u3068\u3057\u3066\u306f\uff0c\n\n- Reduce calculation cost of split gain\n- Use histogram subtraction for further speed-up\n- Reduce Memory usage\n- Reduce communication cost for parallel learning\n\n\u304c\u6319\u3052\u3089\u308c\u308b\u304c\uff0c\u8981\u306f\u300c\u8efd\u304f\u300d\u3066\u300c\u52b9\u7387\u304c\u3044\u3044\u300d\u3068\u3044\u3046\u3053\u3068\u3089\u3057\u3044\uff0e\n\n\uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u9055\u3044\u8a73\u7d30\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u306f\uff0c\u8ad6\u6587\u3092\u3044\u304f\u3064\u304b\u8aad\u307f\u8fbc\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\u53c2\u8003\u6587\u732e\u30ea\u30b9\u30c8\u306e\u30ea\u30f3\u30af\u306f [\u3053\u3061\u3089](https://github.com/Microsoft/LightGBM/wiki/Features#references) \u306b\u306a\u308a\u307e\u3059. \u30cd\u30c3\u30c8\u3067\u691c\u7d22\u3057\u3066\u307f\u307e\u3057\u305f\u304c\uff0c\u5185\u5bb9\u306f\u96e3\u3057\u305d\u3046\u3067\u3059...\uff09\n\n\u307e\u305f\uff0c\u5b66\u7fd2\u306e\u904e\u7a0b\u3067\uff0c\u591a\u304f\u306e\u30c4\u30fc\u30eb\u304c**level(depth)-wise**\u3067\u6728\u3092\u6210\u9577\u3055\u305b\u3066\u3044\u304f\u306e\u306b\u5bfe\u3057\uff0cLightGBM\u3067\u306f**leaf-wise**\u3067\u6210\u9577\u3055\u305b\u308b\u3068\u306e\u3053\u3068\uff0e\n\n**Fig. Leaf-wise tree growth**\n\n![Leaf-wise1.png](https://qiita-image-store.s3.amazonaws.com/0/74152/9cb79ea4-436a-52d6-8784-9787c54c3ed7.png)\n\n\uff08\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u5206\u304b\u308a\u3084\u3059\u3044 [\u8aac\u660e\u56f3](https://github.com/Microsoft/LightGBM/wiki/Features#optimization-in-accuracy) \u304c\u3042\u308a\u307e\u3059\u306e\u3067\uff0c\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\uff09  \n\n\u3053\u306e**leaf-wise**\u306e\u3084\u308a\u65b9\u306b\u5bfe\u5fdc\u3057\u3066\uff0c\"overfit\"\u306b\u95a2\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3068\u306a\u308b\u304c\uff0c\u3053\u308c\u306b\u3064\u3044\u3066\u306f\u5f8c\u3067\u89e6\u308c\u308b\uff0e\n\n\u4ee5\u4e0a\uff0c**XGBoost**\u3068**LightGBM**\u306f\u540c\u3058\u6c7a\u5b9a\u6728(Decision Tree)\u30d9\u30fc\u30b9\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u30e2\u30c7\u30eb\u3067\u3042\u308b\u304c\uff0c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u8a73\u7d30\u306b\u9055\u3044\u304c\u3042\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u78ba\u8a8d\u3057\u305f\uff0e\n\n## \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u6700\u521d\u306e\u30b3\u30fc\u30c9\n\u79c1\u306e\u74b0\u5883(Ubuntu 16.04 + miniconda)\u306b\u304a\u3044\u3066\uff0c\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u624b\u9806\u3067\u554f\u984c\u306a\u304f\u5b8c\u4e86\u3057\u305f\uff0e\u305d\u306e\u5f8c\uff0cPython-Package\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u51e6\u7406\u3067\u5c11\u3057\u624b\u9593\u53d6\u3063\u305f\uff0ePython-Package\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u306f\uff0c\u4ee5\u4e0b\u3092\u5b9f\u65bd\u3059\u308b\u304c\uff0c\n\n```py\ncd python-package; python setup.py install\n```\n\n\u3053\u3053\u3067\u6b21\u306e\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u30b1\u30fc\u30b9\u304c\u3042\u308b\uff0e\n\n```text\nerror: Error: setup script specifies an absolute path:\n\n/Users/Microsoft/LightGBM/python-package/lightgbm/../../lib_lightgbm.so\n\nsetup() arguments must always be /-separated paths relative to the setup.py directory, never absolute paths.\n```\n\n\uff08\u3053\u306e\u300c\u7d76\u5bfe\u30d1\u30b9\u3092\u4f7f\u3046\u306a\u300d\u306e\u30a8\u30e9\u30fc\u306b\u3064\u3044\u3066\u306f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3082\u89e6\u308c\u3089\u308c\u3066\u3044\u308b\u304c\uff0c\u73fe\u8c61/\u518d\u73fe\u6027\u304c\u306f\u3063\u304d\u308a\u3057\u3066\u3044\u306a\u3044\uff0e\u5b9f\u969b\uff0c\u79c1\u30822\u53f0\u306ePC\uff08Ubuntu)\u306bLightGBM\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u304c\uff0c1\u53f0\u3067\u4e0a\u8a18\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\uff0c\u3082\u30461\u53f0\u3067\u306f\u30a8\u30e9\u30fc\u306e\u767a\u751f\u306f\u306a\u304b\u3063\u305f\uff0e\uff09\n\n\u30a8\u30e9\u30fc\u306e\u539f\u56e0\u306f\uff0cPython\u306e\"setuptools\"\u30d0\u30c3\u30b1\u30fc\u30b8\u5468\u308a\u306b\u3042\u308a\u305d\u3046\u306e\u3067\uff0c\uff08\u3042\u307e\u308a\u63a8\u5968\u3055\u308c\u308b\u65b9\u6cd5\u3067\u306f\u306a\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u304c\uff09\u4ee5\u4e0b\u306e\u3088\u3046\u306b`setup.py`\u306e\u4e00\u90e8\u3092\u4fee\u6b63\u3057\u3066\u30a8\u30e9\u30fc\u56de\u907f\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\uff0e\n\n```bash\n\u4fee\u6b63\u7b87\u6240\uff1a \u95a2\u6570\u3067`LIB_PATH`\u3092\u751f\u6210\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u3092\u30de\u30b9\u30af\u3057\uff0c\u6240\u5b9a\u306e\u30d1\u30b9\u3092\u30d9\u30bf\u66f8\u304d\uff0e\n\n libpath = {'__file__': libpath_py}\n exec(compile(open(libpath_py, \"rb\").read(), libpath_py, 'exec'), libpath, libpath)\n \n-LIB_PATH = libpath['find_lib_path']()\n+# LIB_PATH = libpath['find_lib_path']()\n+LIB_PATH = ['../lib_lightgbm.so']\n print(\"Install lib_lightgbm from: %s\" % LIB_PATH) \n```\n\n\u3055\u3066\u30b3\u30fc\u30c9\u4f5c\u6210\u306b\u3068\u308a\u304b\u304b\u308b\u304c\uff0c\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u82e5\u3044\u304b\u3089\u304b\u30ea\u30dd\u30b8\u30c8\u30ea\u5185\u306e\u4f8b\u984c\u30b3\u30fc\u30c9\u304c\u5c11\u306a\u3044\uff0e\u4ee5\u4e0b\uff0c\u5c11\u3057\u8a66\u884c\u932f\u8aa4\u3057\u306a\u304c\u3089\uff0c\u5404\u4f8b\u984c\u3092\u6271\u3063\u3066\u307f\u305f\uff0e\n\n### \u5206\u985e\u554f\u984c(Classification)\n\u307e\u305a\uff0c\"iris\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u985e\u3067\u3042\u308b\uff0e\n\n```py\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0)\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# LightGBM parameters\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'metric': {'multi_logloss'},\n        'num_class': 3,\n        'learning_rate': 0.1,\n        'num_leaves': 23,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 0\n}\n\n# train\ngbm = lgb.train(params,\n            lgb_train,\n            num_boost_round=50,\n            valid_sets=lgb_eval,\n            early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred = np.argmax(y_pred, axis=1)\n```\n\n\u4f7f\u3044\u65b9\u306f\uff0c\"XGBoost\" \u3068\u304b\u306a\u308a\u4f3c\u3066\u3044\u308b\uff0e\u307e\u305a\uff0clightgbm.Dataset\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3057\u3066\uff0c\u5165\u529b\u30c7\u30fc\u30bf\u3092\u30bb\u30c3\u30c8\u3059\u308b\uff0e\u6240\u5b9a\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u7528\u610f\u3057\u3066\uff0c\u5206\u985e\u5668(Classifier)\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\uff0cTrain\u30c7\u30fc\u30bf\u306bfit\u3055\u305b\u3066\u5206\u985e\u5668\u30e2\u30c7\u30eb\u3092\u5f97\u308b\uff0e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3064\u3044\u3066\u306f\uff0c\"XGBoost\"\u3068\u985e\u4f3c\u3059\u308b\u3068\u3053\u308d\u3082\u3042\u308b\u304c\uff0c\u7570\u306a\u308b\u3068\u3053\u308d\u3082\u3042\u308b\u306e\u3067\u305d\u3053\u306f\u899a\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\n\n#### \u95a2\u9023\u60c5\u5831\uff08\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\uff09\u3078\u306e\u30ea\u30f3\u30af\n- [**Parameters**](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md)\n- [**Parameter Tuning**](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md)\n\n\u4e00\u3064\u899a\u3048\u3066\u304a\u304d\u305f\u3044\u306e\u304c\uff0c\u6c7a\u5b9a\u6728\u306e\u6570\u91cf\u306b\u95a2\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\uff0c\u3053\u308c\u306f\u524d\u8ff0\u3057\u305f\"level(depth)-wise\"\u304b\u3089\"leaf-wise\"\u306b\u5909\u308f\u308b\u3053\u3068\u3092\u8003\u616e\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\uff0e\u4ee5\u4e0b\u30d1\u30e9\u30e1\u30fc\u30bf\u5909\u63db\u306b\u3064\u3044\u3066\uff0c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304b\u3089\u5f15\u7528\u3059\u308b\uff0e\n\n> ## Convert parameters from XGBoost\n>\n> LightGBM uses [leaf-wise](https://github.com/Microsoft/LightGBM/wiki/Features#optimization-in-accuracy) tree growth algorithm. But other popular tools, e.g. XGBoost, use depth-wise tree growth. So LightGBM use ```num_leaves``` to control complexity of tree model, and other tools usually use ```max_depth```. Following table is the correspond between leaves and depths. The relation is ```num_leaves = 2^(max_depth) ```.\n>\n> | max_depth | num_leaves |\n> |:---------:|:----------:|\n> | 1 | 2 |\n> | 2 | 4 |\n> | 3 | 8 |\n> | 7 | 128 |\n> | 10 | 1024 | \n\n\u3053\u306e\u30eb\u30fc\u30eb\u3092\u899a\u3048\u3066\u3057\u307e\u3048\u3070\uff0c\u7279\u306b\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3042\u308b\uff0e\u4f8b\u3048\u3070\"XGBoost\"\u306e max_depth=6 \u306e\u8a2d\u5b9a\u3068\u540c\u7b49\u306b\u3059\u308b\u306b\u306f\uff0cnum_leaves=64 \u3068\u8a2d\u5b9a\u3059\u308c\u3070\u3088\u3044\uff0e\n\n### \u56de\u5e30\u554f\u984c(Regression)\n\u6b21\u306b\u56de\u5e30(Regression)\u3092\u8a66\u3057\u3066\u307f\u308b\uff0e\u6271\u3063\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\uff0c\"boston\" \uff08housing data set\uff09\u3067\u3042\u308b\uff0e\uff08\u6700\u8fd1\u306eScikit-learn\u306b\u306f \"boston\"\u3082\u4ed8\u5c5e\u3057\u3066\u304a\u308a\uff0c`load_boston()` \u3067\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\uff09\n\n```py\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, train_test_split\n\nprint(\"Boston Housing: regression\")\nboston = load_boston()\ny = boston['target']\nX = boston['data']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=201612\n)\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n# LightGBM parameters\nparams = {\n        'task' : 'train',\n        'boosting_type' : 'gbdt',\n        'objective' : 'regression',\n        'metric' : {'l2'},\n        'num_leaves' : 31,\n        'learning_rate' : 0.1,\n        'feature_fraction' : 0.9,\n        'bagging_fraction' : 0.8,\n        'bagging_freq': 5,\n        'verbose' : 0\n}\n\n# train\ngbm = lgb.train(params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=lgb_eval,\n            early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n```\n\n\u5927\u90e8\u5206\uff0c\u5206\u985e\u554f\u984c(Classification)\u3068\u540c\u3058\u3060\u304c\uff0cLightGBM\u30e2\u30c7\u30eb\u306b\u6e21\u3059\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\uff0c'objective' \u3092 'regression'\u306b\uff08\u5206\u985e(\u591a\u30af\u30e9\u30b9\u5206\u985e\uff09\u3067\u306f 'multiclass')\uff0c'metric' \u3092 'l2' \uff08\u5206\u985e\u3067\u306f 'multi_logloss'\uff09 \u306b\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\n\n## Scikit-learn\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\n\u3053\u308c\u3082\u5148\u884c\u30c4\u30fc\u30eb\u540c\u69d8\u306b\uff0cScikit-learn \u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\uff0e\u30cd\u30a4\u30c6\u30a3\u30d6\u306b\u8fd1\u3044\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306e\u65b9\u304c\uff0c\u7d30\u304b\u3044\u64cd\u4f5c\u304c\u3067\u304d\u305f\u308a\uff0c\u65b0\u3057\u3044\u6a5f\u80fd\u306e\u30b5\u30dd\u30fc\u30c8\u304c\u65e9\u304b\u3063\u305f\u308a\u3059\u308b\u30b1\u30fc\u30b9\u3082\u3042\u308b\u304c\uff0c\u500b\u4eba\u7684\u306b\u306fScikit-learn\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306e\u65b9\u3092\u4f7f\u3063\u3066\u3044\u304d\u305f\u3044\uff0e\uff08\u6b21\u3005\u3068\u65b0\u898f\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u4e2d\u3067\uff0c\u3067\u304d\u308b\u3060\u3051\u899a\u3048\u308b\u4e8b\u3092\u5c11\u306a\u304f\u3057\u305f\u3044\uff0c\u3068\u3044\u3046\u306e\u304c\u7b2c\u4e00\u306e\u7406\u7531\u3067\u3042\u308b\uff0e\uff09\n\n```py\ngbm = lgb.LGBMClassifier(objective='multiclass',\n                        num_leaves = 31,\n                        learning_rate=0.1,\n                        min_child_samples=10,\n                        n_estimators=100)\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric='multi_logloss',\n        early_stopping_rounds=10)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n```\n\n\u304a\u306a\u3058\u307f\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\uff0c\u5206\u985e\u5668\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3057\u3066\uff0c\u305d\u306e\u5f8c\uff0c`fit()` \u3059\u308b\u3068\u3044\u3046\u6d41\u308c\u3067\u3042\u308b\uff0e\nearly stopping \u3082\u4e0a\u8a18\u30b3\u30fc\u30c9\u3067\u554f\u984c\u306a\u304f\u52d5\u3044\u3066\u3044\u308b\uff0e\n\n\u4e3b\u306b\u4f7f\u3046\u30af\u30e9\u30b9\u306f\uff0c\u6b21\u306e2\u3064\u306b\u306a\u308b\uff0e\n\n- LGBMClassifier\n- LGBMRegressor\n\n## \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3068\u3057\u3066\n\u524d\u8ff0\u306e\u901a\u308a\uff0cGradient Boosing\u81ea\u4f53\uff0c\u6c7a\u5b9a\u6728\u8b58\u5225\u5668\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u624b\u6cd5\u3067\u3042\u308b\u304c\uff0c\u3053\u3053\u3067\u306f\uff0c\"LightGBM\", \"XGBoost\"\u3092\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3068\u3057\u3066\u7528\u3044\u305f\uff0c\u30e1\u30bf\u30e2\u30c7\u30ea\u30f3\u30b0\u3068\u3044\u3046\u610f\u5473\u3067\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u53d6\u308a\u4e0a\u3052\u308b\uff0e  \n\n\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3059\u308b\u969b\u306b\u300cBias-Variance \u306b\u3064\u3044\u3066\u7570\u306a\u308b\u51fa\u529b\u3092\u3059\u308b\u8907\u6570\u306e\u30e2\u30c7\u30eb\u3092\u7528\u3044\u308b\u3068\u3088\u3044\u300d\u3068\u3044\u3046\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3042\u308b\u304c\uff0c\u7570\u306a\u308b\u7d50\u679c\u3092\u3082\u3089\u305f\u3089\u3059\u30e2\u30c7\u30eb\u81ea\u4f53\u306e\u4e88\u6e2c\u6027\u80fd\u304c\u3042\u308b\u7a0b\u5ea6\u78ba\u4fdd\u3055\u308c\u3066\u3044\u306a\u3044\u3068\uff0c\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3084\u3063\u3066\u3082\u5168\u4f53\u306e\u7cbe\u5ea6\u304c\u4e0a\u304c\u3089\u306a\u304f\u3066\u60a9\u3080\u3053\u3068\u304c\u591a\u3044\uff0e\u4f8b\u3048\u3070\uff0cLogistic Regression\u3068\uff08\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u3066\u3044\u306a\u3044\uff09Neural Network\u30e2\u30c7\u30eb\u3068XGBoost \u30e2\u30c7\u30eb\u3092\uff0c\u6642\u9593\u3092\u304b\u3051\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u305f\u306b\u3082\u95a2\u308f\u3089\u305a\uff0c\u305d\u306e\u7cbe\u5ea6\u304c\uff08\u4e00\u756a\u9ad8\u6027\u80fd\u306a\uff09XGBoost\u5358\u4f53\u306e\u7d50\u679c\u3068\u5909\u308f\u3089\u306a\u3044\u3068\u3044\u3046\u72b6\u6cc1\u3092\u7d4c\u9a13\u3057\u305f\u3053\u3068\u304c\u3042\u308b\uff0e\u300c\u7c92\u304c\u63c3\u3063\u305f\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u300d\u3092\u7528\u610f\u3059\u308b\u76ee\u7684\u3067\uff0c\"LightGBM\" \u3092 \"XGboost\" \u3068\u4f75\u7528\u3059\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u601d\u3044\u3064\u304f\uff0e\uff08kaggle\u3067\u3082\u305d\u3093\u306a\u30b3\u30fc\u30c9\uff0c\u30d5\u30a9\u30fc\u30e9\u30e0\u610f\u898b\u304c\u3042\u308a\u307e\u3057\u305f\uff0e\uff09\u4ee5\u4e0b\uff0c\u305d\u308c\u305e\u308c\u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u4f8b\u3092\u6319\u3052\u308b\uff0e\n\n### LightGBM \u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\n\n```py\ndef lgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n    '''\n      Base analysis process by LightGBM\n    '''\n    kf = KFold(n_splits=n_folds, random_state=1)\n    y_preds_train = []\n    y_preds_test = []\n    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n        gbm = lgb.LGBMClassifier(objective='multiclass',\n                        num_leaves = 23,\n                        learning_rate=0.1,\n                        n_estimators=100)\n        gbm.fit(X_train[train], y_train[train],\n            eval_set=[(X_train[test], y_train[test])],\n            eval_metric='multi_logloss',\n            verbose=False,\n            early_stopping_rounds=10)\n        y_pred_train = gbm.predict_proba(X_train[test], \n                            num_iteration=gbm.best_iteration)\n        y_pred_test = gbm.predict_proba(X_test, \n                            num_iteration=gbm.best_iteration)\n        y_pred_k = np.argmax(y_pred_test, axis=1)\n        accu = accuracy_score(y_test, y_pred_k)\n        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n        y_preds_train.append(y_pred_train)\n        y_preds_test.append(y_pred_test)\n\n    return y_preds_train, y_preds_test\n```\n\n### XGBoost \u306e\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\n\n```py\ndef xgb_analysis(X_train, X_test, y_train, y_test, n_folds=5):\n    '''\n      Base analysis process by XGBoost\n    '''\n    kf = KFold(n_splits=n_folds, random_state=1)\n    y_preds_train = []\n    y_preds_test = []\n    for k, (train, test) in enumerate(kf.split(X_train, y_train)):\n        xgbclf = xgb.XGBClassifier(objective='multi:softmax',\n                        max_depth=5,\n                        learning_rate=0.1,\n                        n_estimators=100)\n        xgbclf.fit(X_train[train], y_train[train],\n            eval_set=[(X_train[test], y_train[test])],\n            eval_metric='mlogloss',\n            verbose=False,\n            early_stopping_rounds=10)\n        y_pred_train = xgbclf.predict_proba(X_train[test])\n        y_pred_test = xgbclf.predict_proba(X_test)\n        y_pred_k = np.argmax(y_pred_test, axis=1)\n        accu = accuracy_score(y_test, y_pred_k)\n        print('fold[{:>3d}]: accuracy = {:>.4f}'.format(k, accu))\n        y_preds_train.append(y_pred_train)\n        y_preds_test.append(y_pred_test)\n    \n    return y_preds_train, y_preds_test\n```\n\n\u3053\u308c\u3089\u306e\u95a2\u6570\u3092\u4f7f\u3063\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\uff0cscikit-learn\u306e\"digit dataset\"\u3092\u5206\u985e\u3057\u305f\u5b9f\u884c\u72b6\u6cc1\u304c\u4ee5\u4e0b\u3067\u3042\u308b\uff0e\n\n```text\nLightGBM process:\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  0]: accuracy = 0.9360\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  1]: accuracy = 0.9394\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  2]: accuracy = 0.9394\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\n[LightGBM] [Warning] Ignoring Column_56 , only has one value\nfold[  3]: accuracy = 0.9276\n[LightGBM] [Warning] Ignoring Column_0 , only has one value\n[LightGBM] [Warning] Ignoring Column_24 , only has one value\n[LightGBM] [Warning] Ignoring Column_32 , only has one value\n[LightGBM] [Warning] Ignoring Column_39 , only has one value\nfold[  4]: accuracy = 0.9310\nXGBoost process:\nfold[  0]: accuracy = 0.9377\nfold[  1]: accuracy = 0.9411\nfold[  2]: accuracy = 0.9444\nfold[  3]: accuracy = 0.9343\nfold[  4]: accuracy = 0.9310\n\nStacked model:\naccuracy = 0.9512\n\nconfusion matrix:\n[[49  0  0  0  0  0  0  0  0  0]\n [ 1 57  1  0  1  0  0  0  0  1]\n [ 2  0 57  0  0  0  1  0  2  0]\n [ 0  0  0 54  0  0  0  0  0  1]\n [ 0  0  0  0 49  0  0  1  0  0]\n [ 0  0  0  1  0 61  1  0  0  2]\n [ 0  0  1  0  0  0 66  0  0  0]\n [ 0  0  0  0  1  0  0 55  0  0]\n [ 0  4  0  2  0  0  0  0 60  1]\n [ 1  0  0  2  0  2  0  0  0 57]]\n```\n\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3067\u82e5\u5e72\u306e\u7cbe\u5ea6\uff08\u6b63\u7b54\u7387\uff09up\u3092\u5b9f\u73fe\u3067\u304d\u3066\u3044\u308b\uff0e\uff08Warning\u304c\u767a\u751f\u3057\u3066\u3044\u307e\u3059\u304c\uff0e\uff09\n\n\u3044\u304b\u304c\u3060\u308d\u3046\u304b\uff1f \u3053\u3053\u3067\u306f\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb(stacking)\u306b\u3064\u3044\u3066\u306e\u8aac\u660e\u306f\u7701\u304f\u304c\uff0c\u4e0a\u306e2\u3064\u306e\u30b3\u30fc\u30c9\u3067\u96f0\u56f2\u6c17\u306f\u611f\u3058\u3066\u3044\u305f\u3060\u3051\u308b\u304b\u3068\u601d\u3046\uff0e\"LightGBM\", \"XGBoost\" \u5171\u306bScikit-learn API\u3092\u7528\u3044\u308b\u3053\u3068\u306b\u3088\u308a\uff0c\u5927\u90e8\u5206\u540c\u3058\u3067\u3044\u304f\u3064\u304b\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u5909\u3048\u308b\u3060\u3051\u3067\uff0c2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3067\u304d\u305f\uff0e  \n\n\u307e\u305f\uff0c\u6b32\u3057\u3044\u6027\u80fd\u306b\u3064\u3044\u3066\u3082\uff0c\u7d30\u304b\u3044\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u9055\u3044\uff08\"histogram based algorithms\"\u3084 \"leaf-wise growth\"\u306e\u7279\u5fb4)\u306b\u3088\u308a\uff0c\u300c\u4f3c\u3066\u975e\u306a\u308b\u300d\u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u306e\u7d50\u679c\u304c\u671f\u5f85\u3067\u304d\u308b\u306e\u3067\u306f\u306a\u3044\u3060\u308d\u3046\u304b\uff1f  \uff08\u307e\u3060\uff0c\u6027\u80fd\u78ba\u8a8d\u4e2d\u3067\u3059\u304c...\uff09\n\n\u4ee5\u4e0a\uff0c\u7c21\u5358\u306b \"LightGBM\" \u306b\u3064\u3044\u3066\u7d39\u4ecb\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\uff0e\u307e\u3060 ver.0.1 \u3067\u3042\u308b\u304c\uff0c\u304b\u306a\u308a\u5b8c\u6210\u5ea6\u304c\u9ad8\u3044\u5370\u8c61\u3092\u3082\u3063\u305f\uff0eR\u30d7\u30ed\u30b0\u30e9\u30de\u306b\u3082R-Package\u304c\u3042\u308b\u306e\u3067\uff0c\u3053\u306e\"LightGBM\"\u3092\u63a8\u5968\u3057\u305f\u3044\uff0e\n\n## \uff08\u8ffd\u8a18\uff09R-Package\u306b\u3064\u3044\u3066\u3082\u8a66\u3057\u305f\nR-Package\u3082\u5148\u65e5\uff0c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u305f\u306e\u3067\u8a66\u3057\u3066\u307f\u305f\uff0e\u307e\u305a\uff0cR-Package\u306e\u5165\u308c\u65b9\u306f\uff0c\u6b21\u306e\u901a\u308a\uff0e\n\n\u65b9\u6cd51\uff1a\uff08\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb\u5f8c\uff0c\uff09\n\n```bash\ncd R-package\nR CMD INSTALL --build  .\n```\n\n\u65b9\u6cd52\uff1a\uff08{devtools}\u3067\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff09\n\n```R\ndevtools::install_github(\"Microsoft/LightGBM\", subdir = \"R-package\")\n```\n\n\u3053\u3061\u3089\u3082 \"iris\" \u306e\u5206\u985e\u3092\u884c\u3063\u305f\uff0e\n\n```R\nlibrary(lightgbm)\ndata(iris)\n\n# split data to train / test\nset.seed(2017)\ntrain_size = 100    # test_size is 50\ntrain_ind <- sample(seq_len(nrow(iris)), size = train_size)\n\ntrain <- iris[train_ind, ]\ntest <- iris[-train_ind, ]\nx_train <- train[, -5]\ny_train <- as.numeric(train[, 5]) - 1   # need zero start labelling, [0, 1, 2]\nx_test <- test[, -5]\ny_test <- as.numeric(test[, 5]) - 1     # need zero start labelling, [0, 1, 2]\n\n# model definition and training\nbst <- lightgbm(data = as.matrix(x_train), label = y_train,\n        num_leaves = 4, learning_rate = 0.1, nrounds = 20, \n        min_data = 20, min_hess = 20,\n        objective = \"multiclass\", metric=\"multi_error\", \n        num_class = 3, verbose = 0)\n\n# make prediction\ny_pred_proba <- matrix(predict(bst, as.matrix(x_test)), byrow=T, ncol=3)\ny_pred <- apply(y_pred_proba, 1, which.max) - 1\n\n# confusion matrix\ncat(\"\\n confusion matrix:\\n\")\ntable(y_test, y_pred)\n```\n\n\u5c11\u3057\u306f\u307e\u3063\u305f\u306e\u304c\uff0c\u201diris\" \u306e\u6570\u5024\u5316\u3057\u305f\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\u304c [1, 2, 3] \u3067\u306f\u201dLightGBM\"\u306e\u95a2\u6570\u304c\u53d7\u3051\u4ed8\u3051\u305a\uff0c[0, 1, 2] \u306b\u30b7\u30d5\u30c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\u70b9\u3067\u3042\u308b\uff0e\uff08\u521d\u898b\u306e\u30a8\u30e9\u30fc\u3067\u3057\u305f\u306e\u3067\uff0c\u5c11\u3057\u6238\u60d1\u3044\u307e\u3057\u305f\uff0e\uff09\n\n## \u53c2\u8003\u6587\u732e\uff0cweb site\n- Miscrosoft/LightGBM - GitHub  \n  https://github.com/Microsoft/LightGBM\n- LightGBM\u306ePython\u30d1\u30c3\u30b1\u30fc\u30b8\u89e6\u3063\u3066\u307f\u305f - marugari2\u3055\u3093\u30d6\u30ed\u30b0  \n  http://marugari2.hatenablog.jp/entry/2016/12/14/235747\n- \u306f\u3058\u3081\u3066\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\uff0c\u5e73\u4e95\u6c0f\u8457\uff0c\u68ee\u5317\u51fa\u7248  \n  https://www.morikita.co.jp/books/book/2235\n \n\n\n", "tags": ["Python", "\u6a5f\u68b0\u5b66\u7fd2", "MachineLearning", "scikit-learn", "lightgbm"]}