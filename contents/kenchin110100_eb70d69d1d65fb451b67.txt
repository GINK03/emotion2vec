{"tags": ["Python", "Chainer", "\u6a5f\u68b0\u5b66\u7fd2", "DeepLearning", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406"], "context": "EncoderDecoder\u30e2\u30c7\u30eb\u306e\u4e00\u7a2e\u3067\u3042\u308bSequence to Sequence\uff08Seq2Seq\uff09\u306b\u3001\nAttention Model\u3092\u5c0e\u5165\u3057\u3001\u305d\u306e\u5b9f\u88c5\u3068\u691c\u8a3c\u7d50\u679c\u306b\u3064\u3044\u3066\u8aac\u660e\u3059\u308b\u3002\n\n\u306f\u3058\u3081\u306b\n\u524d\u56de\nhttp://qiita.com/kenchin110100/items/b34f5106d5a211f4c004\nSequence to Sequence\uff08Seq2Seq\uff09\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3092Chainer\u3067\u884c\u306a\u3063\u305f\u304c\u3001\n\u4eca\u56de\u306f\u305d\u306e\u30e2\u30c7\u30eb\u306bAttention Model\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\u4ee5\u964d\u3067\u306f\u3001Attention Model\u3001\u305d\u306e\u5b9f\u88c5\u6cd5\u3001\u691c\u8a3c\u7d50\u679c\u306b\u3064\u3044\u3066\u8aac\u660e\u3092\u3057\u307e\u3059\u3002\n\nAttention Model\n\nAttention Model\u3068\u306f\nLSTM\u306a\u3069\u306eRNN\u7cfb\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u6587\u306a\u3069\u306e\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\u3057\u304b\u3057\u521d\u671f\u306b\u5165\u529b\u3057\u305f\u30c7\u30fc\u30bf\u306f\u3001\u6700\u7d42\u7684\u306b\u51fa\u529b\u3055\u308c\u308b\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u53cd\u6620\u3055\u308c\u306b\u304f\u304f\u306a\u308a\u307e\u3059\u3002\n\u3064\u307e\u308a\u3001\u300c\u304a\u6bcd\u3055\u3093\u304c\u5316\u7ca7\u3092\u3057\u3066\u3001\u30b9\u30ab\u30fc\u30c8\u3092\u5c65\u3044\u3066\u8857\u306b\u51fa\u304b\u3051\u305f\u300d\u3068\u3044\u3046\u6587\u3068\u3001\u300c\u304a\u7236\u3055\u3093\u304c\u5316\u7ca7\u3092\u3057\u3066\u3001\u30b9\u30ab\u30fc\u30c8\u3092\u5c65\u3044\u3066\u8857\u306b\u51fa\u304b\u3051\u305f\u300d\u3068\u3044\u3046\u6587\u304c\u307b\u3068\u3093\u3069\u540c\u3058\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\u521d\u671f\u306b\u5165\u529b\u3057\u305f\u30c7\u30fc\u30bf\u3082\u3061\u3083\u3093\u3068\u8003\u616e\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u4ed5\u7d44\u307f\u304cAttention Model\u3067\u3059\u3002\n\nSequence to Sequence with Attention Model\n\u524d\u56de\u5b9f\u88c5\u3057\u305fSeq2Seq\u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u306e\u6d41\u308c\u3092\u56f3\u793a\u3059\u308b\u3068\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\nSequence to Sequence\n\n\n\n\n\n\n\n\n\uff08\u524d\u56de\u3068\u5c11\u3057\u56f3\u3092\u5909\u3048\u3066\u3044\u307e\u3059\uff09\n\u9752\u8272\u306e\u90e8\u5206\u304c\u767a\u8a71\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308bEncoder\u3001\u8d64\u8272\u306e\u90e8\u5206\u304c\u30d9\u30af\u30c8\u30eb\u304b\u3089\u5fdc\u7b54\u3092\u51fa\u529b\u3059\u308bDecoder\u3067\u3059\u3002\n\u3053\u308c\u306bAttention Model\u3092\u52a0\u3048\u308b\u3068\u4ee5\u4e0b\u306e\u56f3\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\nSequence to Sequence with Attention model\n\n\n\n\n\n\n\n\n\u5c11\u3057\u8907\u96d1\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u56f3\u4e2d\u306e[At]\u3068\u66f8\u3044\u3066\u3042\u308b\u3068\u3053\u308d\u304cAttention Model\u3068\u306a\u308a\u307e\u3059\u3002\nEncoder\u5074\u3067\u306f\u3001\u6bce\u56de\u51fa\u529b\u3055\u308c\u308b\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092Attention Model\u306e\u4e2d\u306b\u8a18\u61b6\u3055\u305b\u3066\u3044\u304d\u307e\u3059\u3002\nDecoder\u5074\u3067\u306f\u30011\u3064\u524d\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092Attention Model\u306b\u5165\u529b\u3057\u307e\u3059\u3002\n\u5165\u529b\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u3092\u5143\u306bAttention Model\u304cEncoder\u5074\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u3068\u3063\u3066\u30ea\u30bf\u30fc\u30f3\u3057\u307e\u3059\u3002\nEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092Decoder\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u3001\u524d\u306b\u3042\u308b\u5358\u8a9e\u3001\u5f8c\u308d\u306b\u3042\u308b\u5358\u8a9e\u3001\u3069\u3053\u3067\u3082\u6ce8\u76ee\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u306e\u304cAttention Model\u3068\u306a\u308a\u307e\u3059\u3002\nAttention Model\u306b\u306f\u5927\u304d\u304f\u5206\u3051\u30662\u7a2e\u985e\u5b58\u5728\u3057\u3001Global Attention\u3068Local Attention\u3068\u547c\u3070\u308c\u307e\u3059\u3002\n\u4ee5\u964d\u3067\u306f\u3001Global Attention\u3001Local Attention\u306e\u8aac\u660e\u3092\u884c\u3044\u307e\u3059\u3002\n\nGlobal Attention\nGlobal Attention\u304c\u63d0\u6848\u3055\u308c\u305f\u306e\u304c\u3001\u4e0b\u8a18\u306e\u8ad6\u6587\u306b\u306a\u308a\u307e\u3059\u3002\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014).\n\u5143\u3005\u306f\u6a5f\u68b0\u7ffb\u8a33\u3067\u4f7f\u308f\u308c\u3066\u3044\u305f\u3093\u3067\u3059\u306d\u3002\nGlobal Attention\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u305f\u8cc7\u6599\u306f\u3001\nhttps://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention\n\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\n\u3042\u3068\u82f1\u8a9e\u306b\u306a\u308a\u307e\u3059\u304c\u3001\nhttps://talbaumel.github.io/attention/\n\u3082\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\nEncoder\u5074\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u53d6\u308b\u4ed5\u7d44\u307f\u3092\u56f3\u793a\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\nGlobal Attention\n\n\n\n\n\n\n\n\n\u56f3\u306fEncoder\u5074\u3067[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1]\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb2]\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb3]\u306e3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5165\u529b\u3055\u308c\u3066\u308b\u72b6\u614b\u3092\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\u56f3\u4e2d\u306e[eh]\u3001[hh]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u304b\u3089\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3001[\uff0b]\u306f\u30d9\u30af\u30c8\u30eb\u306e\u8db3\u3057\u7b97\u3001[\u00d7]\u306f\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3054\u3068\u306e\u639b\u3051\u7b97\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\n[tanh]\u306fhyperbolic tangent\u3067\u3042\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3092-1\u304b\u30891\u307e\u3067\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n[hw]\u306f\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u304b\u3089\u30b5\u30a4\u30ba1\u306e\u30b9\u30ab\u30e9\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3067\u3059\u3002\n[soft max]\u306fSoftMax\u95a2\u6570\u3067\u3001\u5165\u529b\u3055\u308c\u305f\u5024\u3092\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u3057\u307e\u3059\u3002\n[Soft max]\u306b\u3088\u3063\u3066\u8a08\u7b97\u3055\u308c\u305f\u5024\u3092\u52a0\u91cd\u5e73\u5747\u306e\u30a6\u30a7\u30a4\u30c8\u3068\u3057\u3066\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u3068\u3063\u305f\u7d50\u679c\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\u3053\u308c\u304cGlobal Attention\u306e\u4ed5\u7d44\u307f\u3067\u3059\u3002\n\nLocal Attention\nLocal Attention\u304c\u63d0\u6848\u3055\u308c\u305f\u8ad6\u6587\u306f\u4e0b\u8a18\u306e\u3082\u306e\nLuong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"Effective approaches to attention-based neural machine translation.\" arXiv preprint arXiv:1508.04025 (2015).\n\u3053\u308c\u3082\u6a5f\u68b0\u7ffb\u8a33\u306e\u8ad6\u6587\u3067\u3059\u306d\u3002\n\u53c2\u8003\u306b\u3057\u305f\u8cc7\u6599\u306f\nhttps://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention\n\u3067\u3059\u3002\n\u4e0b\u306fLocal Attention\u306e\u8a08\u7b97\u30d5\u30ed\u30fc\u56f3\u3067\u3059\u3002\n\n\n\nLocal Attention\n\n\n\n\n\n\n\n\nGlobal Attention\u306b\u3055\u3089\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u4e3b\u306a\u9055\u3044\u306f\u53f3\u5074\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3002\n[ht]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u304b\u3089\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u3001[tanh]\u306f\u5148\u307b\u3069\u3068\u540c\u3058\u3001\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3092-1\u304b\u30891\u307e\u3067\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3059\u308b\u50cd\u304d\u3092\u3057\u307e\u3059\u3002\n[tw]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3067\u3042\u308a\u3001[sigmoid]\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u3001\u5165\u529b\u3055\u308c\u305f\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u3053\u307e\u3067\u5165\u529b\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u306f\u30010\u304b\u30891\u307e\u3067\u306e\u7bc4\u56f2\u306e\u30b9\u30ab\u30e9\u30fc\u306b\u306a\u308a\u307e\u3059\u3002\n\u6b21\u306b\u56f3\u4e2d\u306e[ga]\u3067\u4f55\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u3092\u8aac\u660e\u3057\u307e\u3059\u3002ga\u306e\u8a08\u7b97\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u3059\u3002\noutput = \\exp\\bigl(-\\frac{(s - input * Len(S))^2}{\\sigma^2}\\bigl)\noutput=exp(\u2212(s\u2212input\u2217Len(S))2\u03c32){output = \\exp\\bigl(-\\frac{(s - input * Len(S))^2}{\\sigma^2}\\bigl)\n}\n\u3053\u3053\u3067\u3001inputinput\u306f0\u304b\u30891\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30b9\u30ab\u30e9\u30fc\u3001Len(S)Len(S)\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u6570\u3001ss\u306f\u4e2d\u9593\u5c64\u30d9\u30af\u30c8\u30eb\u306e\u9806\u756a\uff08[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1]\u306a\u30891\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb2]\u306a\u30892\uff09\u3092\u8868\u3057\u3066\u308b\u3002\n\u3082\u3057\u4eee\u306bsigmoid\u95a2\u6570\u3067\u51fa\u529b\u3055\u308c\u305f\u5024\u304c0.1\u306a\u3089\u3070\u3001[ga]\u306f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1\u306e\u6642\u306b\u5927\u304d\u306a\u5024\u306b\u306a\u308a\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb3\u306e\u6642\u306f\u5c0f\u3055\u306a\u5024\u306b\u306a\u308b\u3002\n\u3053\u306e\u51fa\u529b\u3092Global Attention\u3067\u8a08\u7b97\u3057\u305f\u91cd\u307f\u306b\u639b\u3051\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u30d4\u30f3\u30dd\u30a4\u30f3\u30c8\u306b\u7279\u5b9a\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306b\u6ce8\u76ee\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u5b9f\u88c5\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306bchainer\u3067\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002Encoder\u90e8\u5206\u306fSeq2Seq\u306e\u6642\u3068\u540c\u3058\u3067\u3059\u3002\n\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u30b3\u30fc\u30c9\u306foda\u69d8\u306e\u3082\u306e\u3067\u3059\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\nhttps://github.com/odashi/chainer_examples\n\nAttention\n\u5b9f\u88c5\u3057\u305f\u306e\u306fGlobal Attention\u3067\u3059\u3002\u30b3\u30fc\u30c9\u306f\u4e0b\u8a18\u306e\u901a\u308a\u3001\n\nattention.py\n\nclass Attention(Chain):\n    def __init__(self, hidden_size, flag_gpu):\n        \"\"\"\n        Attention\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        :param flag_gpu: GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        \"\"\"\n        super(Attention, self).__init__(\n            # \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            fh=links.Linear(hidden_size, hidden_size),\n            # \u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            bh=links.Linear(hidden_size, hidden_size),\n            # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            hh=links.Linear(hidden_size, hidden_size),\n            # \u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u7dda\u5f62\u7d50\u5408\u5c64\n            hw=links.Linear(hidden_size, 1),\n        )\n        # \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3092\u8a18\u61b6\n        self.hidden_size = hidden_size\n        # GPU\u3092\u4f7f\u3046\u5834\u5408\u306fcupy\u3092\u4f7f\u308f\u306a\u3044\u3068\u304d\u306fnumpy\u3092\u4f7f\u3046\n        if flag_gpu:\n            self.ARR = cuda.cupy\n        else:\n            self.ARR = np\n\n    def __call__(self, fs, bs, h):\n        \"\"\"\n        Attention\u306e\u8a08\u7b97\n        :param fs: \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u304c\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :param bs: \u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u304c\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :param h: Decoder\u3067\u51fa\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        :return: \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3068\u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\n        \"\"\"\n        # \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u30b5\u30a4\u30ba\u3092\u8a18\u61b6\n        batch_size = h.data.shape[0]\n        # \u30a6\u30a7\u30a4\u30c8\u3092\u8a18\u9332\u3059\u308b\u305f\u3081\u306e\u30ea\u30b9\u30c8\u306e\u521d\u671f\u5316\n        ws = []\n        # \u30a6\u30a7\u30a4\u30c8\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u5024\u3092\u521d\u671f\u5316\n        sum_w = Variable(self.ARR.zeros((batch_size, 1), dtype='float32'))\n        # Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3068Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u30a6\u30a7\u30a4\u30c8\u306e\u8a08\u7b97\n        for f, b in zip(fs, bs):\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9006\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u30a6\u30a7\u30a4\u30c8\u306e\u8a08\u7b97\n            w = functions.tanh(self.fh(f)+self.bh(b)+self.hh(h))\n            # softmax\u95a2\u6570\u3092\u4f7f\u3063\u3066\u6b63\u898f\u5316\u3059\u308b\n            w = functions.exp(self.hw(w))\n            # \u8a08\u7b97\u3057\u305f\u30a6\u30a7\u30a4\u30c8\u3092\u8a18\u9332\n            ws.append(w)\n            sum_w += w\n        # \u51fa\u529b\u3059\u308b\u52a0\u91cd\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        att_f = Variable(self.ARR.zeros((batch_size, self.hidden_size), dtype='float32'))\n        att_b = Variable(self.ARR.zeros((batch_size, self.hidden_size), dtype='float32'))\n        for f, b, w in zip(fs, bs, ws):\n            # \u30a6\u30a7\u30a4\u30c8\u306e\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\n            w /= sum_w\n            # \u30a6\u30a7\u30a4\u30c8 * Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u30d9\u30af\u30c8\u30eb\u306b\u8db3\u3057\u3066\u3044\u304f\n            att_f += functions.reshape(functions.batch_matmul(f, w), (batch_size, self.hidden_size))\n            att_b += functions.reshape(functions.batch_matmul(f, w), (batch_size, self.hidden_size))\n        return att_f, att_b\n\n\n\n\u8aac\u660e\u3067\u306f\u3001Encoder\u306f1\u3064\u3057\u304b\u4f7f\u308f\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001\u5b9f\u306fAttention Model\u3067\u306f\u9806\u5411\u304d\u306eEncoder\u3068\u9006\u5411\u304d\u306eEncoder\u306e2\u7a2e\u985e\u3092\u4f7f\u3046\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\n\u306a\u306e\u3067Attention\u306e\u8a08\u7b97\u3092\u3059\u308b\u6642\u306b\u3001\u9806\u5411\u304d\u306eEncoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u3001\u9006\u5411\u304d\u306eEncoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u306e2\u3064\u3092\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\nDecoder\nDecoder\u3067\u5165\u529b\u3059\u308b\u5024\u306f\u3001Seq2Seq\u306e\u6642\u3068\u7570\u306a\u308a\u3001\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3001Decoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u306e3\u3064\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u305d\u3053\u3067Decoder\u306e\u5b9f\u88c5\u3092\u66f8\u304d\u63db\u3048\u3066\u3044\u307e\u3059\u3002\n\natt_decoder.py\n\nclass Att_LSTM_Decoder(Chain):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        \"\"\"\n        Attention Model\u306e\u305f\u3081\u306eDecoder\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param vocab_size: \u8a9e\u5f59\u6570\n        :param embed_size: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        \"\"\"\n        super(Att_LSTM_Decoder, self).__init__(\n            # \u5358\u8a9e\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            ye=links.EmbedID(vocab_size, embed_size, ignore_label=-1),\n            # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            eh=links.Linear(embed_size, 4 * hidden_size),\n            # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            hh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            fh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            bh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\u306b\u5909\u63db\u3059\u308b\u5c64\n            he=links.Linear(hidden_size, embed_size),\n            # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            ey=links.Linear(embed_size, vocab_size)\n        )\n\n    def __call__(self, y, c, h, f, b):\n        \"\"\"\n        Decoder\u306e\u8a08\u7b97\n        :param y: Decoder\u306b\u5165\u529b\u3059\u308b\u5358\u8a9e\n        :param c: \u5185\u90e8\u30e1\u30e2\u30ea\n        :param h: Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        :param f: Attention Model\u3067\u8a08\u7b97\u3055\u308c\u305f\u9806\u5411\u304dEncoder\u306e\u52a0\u91cd\u5e73\u5747\n        :param b: Attention Model\u3067\u8a08\u7b97\u3055\u308c\u305f\u9006\u5411\u304dEncoder\u306e\u52a0\u91cd\u5e73\u5747\n        :return: \u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3001\u66f4\u65b0\u3055\u308c\u305f\u5185\u90e8\u30e1\u30e2\u30ea\u3001\u66f4\u65b0\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        \"\"\"\n        # \u5358\u8a9e\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\n        e = functions.tanh(self.ye(y))\n        # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3001Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9806\u5411\u304dEncoder\u306eAttention\u3001\u9006\u5411\u304dEncoder\u306eAttention\u3092\u4f7f\u3063\u3066LSTM\n        c, h = functions.lstm(c, self.eh(e) + self.hh(h) + self.fh(f) + self.bh(b))\n        # LSTM\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\n        t = self.ey(functions.tanh(self.he(h)))\n        return t, c, h\n\n\n\n\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3046\u306e\u306f\u3001\u524d\u56de\u8aac\u660e\u3057\u305f\u7406\u7531\u3068\u540c\u69d8\u3067\u3059\u3002\nAttention\u306b\u3088\u308a\u8a08\u7b97\u3055\u308c\u305fEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u4f7f\u3046\u305f\u3081\u306b\u3001[fh]\u3068[bh]\u3068\u3044\u3046\u5c64\u3092\u8ffd\u52a0\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u308c\u4ee5\u5916\u306f\u540c\u3058\u3067\u3059\u3002\n\nSeq2Seq with Attention\nEncoder\u3001Decoder\u3001Attention\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u30e2\u30c7\u30eb\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\natt_seq2seq.py\n\nclass Att_Seq2Seq(Chain):\n    def __init__(self, vocab_size, embed_size, hidden_size, batch_size, flag_gpu=True):\n        \"\"\"\n        Seq2Seq + Attention\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param vocab_size: \u8a9e\u5f59\u6570\u306e\u30b5\u30a4\u30ba\n        :param embed_size: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        :param batch_size: \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u30b5\u30a4\u30ba\n        :param flag_gpu: GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        \"\"\"\n        super(Att_Seq2Seq, self).__init__(\n            # \u9806\u5411\u304d\u306eEncoder\n            f_encoder = LSTM_Encoder(vocab_size, embed_size, hidden_size),\n            # \u9006\u5411\u304d\u306eEncoder\n            b_encoder = LSTM_Encoder(vocab_size, embed_size, hidden_size),\n            # Attention Model\n            attention = Attention(hidden_size, flag_gpu),\n            # Decoder\n            decoder = Att_LSTM_Decoder(vocab_size, embed_size, hidden_size)\n        )\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.batch_size = batch_size\n\n        # GPU\u3092\u4f7f\u3046\u3068\u304d\u306fcupy\u3001\u4f7f\u308f\u306a\u3044\u3068\u304d\u306fnumpy\n        if flag_gpu:\n            self.ARR = cuda.cupy\n        else:\n            self.ARR = np\n\n        # \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30ea\u30b9\u30c8\u3092\u521d\u671f\u5316\n        self.fs = []\n        self.bs = []\n\n    def encode(self, words):\n        \"\"\"\n        Encoder\u306e\u8a08\u7b97\n        :param words: \u5165\u529b\u3067\u4f7f\u7528\u3059\u308b\u5358\u8a9e\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :return: \n        \"\"\"\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # \u5148\u305a\u306f\u9806\u5411\u304d\u306eEncoder\u306e\u8a08\u7b97\n        for w in words:\n            c, h = self.f_encoder(w, c, h)\n            # \u8a08\u7b97\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\n            self.fs.append(h)\n\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # \u9006\u5411\u304d\u306eEncoder\u306e\u8a08\u7b97\n        for w in reversed(words):\n            c, h = self.b_encoder(w, c, h)\n            # \u8a08\u7b97\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\n            self.bs.insert(0, h)\n\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        self.h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n\n    def decode(self, w):\n        \"\"\"\n        Decoder\u306e\u8a08\u7b97\n        :param w: Decoder\u3067\u5165\u529b\u3059\u308b\u5358\u8a9e\n        :return: \u4e88\u6e2c\u5358\u8a9e\n        \"\"\"\n        # Attention Model\u3092\u4f7f\u3063\u3066Encoder\u306e\u4e2d\u9593\u5c64\u306e\u52a0\u91cd\u5e73\u5747\u3092\u8a08\u7b97\n        att_f, att_b = self.attention(self.fs, self.bs, self.h)\n        # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9806\u5411\u304d\u306eAttention\u3001\u9006\u5411\u304d\u306eAttention\u3092\u4f7f\u3063\u3066\n        # \u6b21\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e88\u6e2c\u5358\u8a9e\u306e\u8a08\u7b97\n        t, self.c, self.h = self.decoder(w, self.c, self.h, att_f, att_b)\n        return t\n\n    def reset(self):\n        \"\"\"\n        \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3092\u521d\u671f\u5316\u3059\u308b\n        :return: \n        \"\"\"\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        self.h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\u3059\u308b\u30ea\u30b9\u30c8\u306e\u521d\u671f\u5316\n        self.fs = []\n        self.bs = []\n        # \u52fe\u914d\u306e\u521d\u671f\u5316\n        self.zerograds()\n\n\n\n\u9806\u5411\u304d\u306eEncoder\u3001\u9006\u5411\u304d\u306eEncoder\u3001Decoder\u3068\u5168\u90e8\u30673\u3064\u306eLSTM\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\nforward\u306e\u8a08\u7b97\u3084train\u306e\u8a08\u7b97\u306fSeq2Seq\u306e\u6642\u3068\u540c\u3058\u3067\u3059\u3002\n\u4f5c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u306f\nhttps://github.com/kenchin110100/machine_learning/blob/master/sampleAttSeq2Seq.py\n\u306b\u3042\u308a\u307e\u3059\u3002\n\n\u5b9f\u9a13\n\n\u30b3\u30fc\u30d1\u30b9\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b\u5bfe\u8a71\u7834\u7dbb\u30b3\u30fc\u30d1\u30b9\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\nhttps://sites.google.com/site/dialoguebreakdowndetection/chat-dialogue-corpus\n\n\u5b9f\u9a13\u7d50\u679c\n\u767a\u8a71\u5185\u5bb9\u3082\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b4\u7a2e\u985e\n\ntoken1 = '\u304a\u306f\u3088\u3046'\ntoken2 = '\u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f'\ntoken3 = '\u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f'\ntoken4 = '\u4eca\u65e5\u306f\u6691\u3044\u3067\u3059'\n\nEpoch\u3054\u3068\u306b\u5fdc\u7b54\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a1Epoch\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u305d\u3046', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u306f\u3044', '\u3001', '\u4f55', '\u3092', '\u898b', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u306f\u3044', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u306f\u3044', '\u3001', '\u4f55', '\u3092', '\u898b', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n\n\u305d\u3093\u306a\u306b\u3001\u3044\u3084\u3089\u3057\u3044\u76ee\u3064\u304d\u3092\u3057\u3066\u3044\u307e\u3057\u305f\u304b\u30fb\u30fb\u30fb\n3Epoch\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3053\u3093\u306b\u3061\u306f\u3002', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u305d\u3046', '\u3067\u3059', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u306a\u3044', '\u3093', '\u3067\u3059', '\u304b', '?', '</s>']\n\n5Epoch\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3042\u308a\u304c\u3068\u3046', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u30b9\u30a4\u30ab', '\u306f', '\u597d\u304d', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u305f\u3044', '\u3067\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u305f\u3044', '\u3067\u3059', '\u304b', '?', '</s>']\n\n\u71b1\u4e2d\u75c7\u306f\u3082\u3046\u308f\u304b\u3063\u305f\u304b\u3089\u30fb\u30fb\u30fb\n7Epoch\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3053\u3093\u3070\u3093\u306f', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u30b9\u30a4\u30ab', '\u306f', '\u5927\u597d\u304d', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u3070\u3044', '\u3070\u3044', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u306a\u3044', '\u3093', '\u3067\u3059', '\u304b', '?', '</s>']\n\n\u304a\u306f\u3088\u3046 => \u3053\u3093\u3070\u3093\u308f\u3000\u306f\u3001\u3072\u3069\u3044\u3067\u3059\u306d\u30fb\u30fb\u30fb\n\u306a\u3093\u304bSeq2Seq\u306e\u6642\u3088\u308a\u3082\u7cbe\u5ea6\u304c\u60aa\u304f\u306a\u3063\u305f\u6c17\u304c\u30fb\u30fb\u30fb\n\u300c\u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f\u300d\u3068\u3044\u3046\u767a\u8a71\u306b\u5bfe\u3057\u3066\u306fSeq2Seq\u306e\u6642\u3068\u5171\u901a\u3057\u3066\u3046\u307e\u304f\u8fd4\u305b\u3066\u306a\u3044\u3067\u3059\u306d\u3002\n\u304a\u305d\u3089\u304f\u300c\u8abf\u5b50\u300d\u3068\u3044\u3046\u5358\u8a9e\u304c\u30b3\u30fc\u30d1\u30b9\u306e\u4e2d\u3067\u4f7f\u308f\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u306e\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\u5143\u304c\u5bfe\u8a71\u7834\u7dbb\u30b3\u30fc\u30d1\u30b9\u306a\u306e\u3067\u7834\u7dbb\u3057\u305f\u7d50\u679c\u304c\u8fd4\u3063\u3066\u304d\u3066\u3044\u308b\u3001\u3064\u307e\u308a\u3053\u308c\u3067\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u308b\u306e\u304b\u3082\u30fb\u30fb\u30fb\n\u640d\u5931\u306e\u5408\u8a08\u5024\u306e\u63a8\u79fb\u3084\u8a08\u7b97\u6642\u9593\u306e\u6bd4\u8f03\u306f\u6700\u5f8c\u306b\u307e\u3068\u3081\u3066\u884c\u3044\u307e\u3059\u3002\n\n\u7d50\u8ad6\nchainer\u3092\u4f7f\u3063\u3066Seq2Seq + Attention Model\u306e\u8a08\u7b97\u3092\u884c\u3044\u307e\u3057\u305f\u3002\nSeq2Seq\u3060\u3051\u306e\u6642\u3068\u6bd4\u8f03\u3057\u3066\u8a08\u7b97\u6642\u9593\u304c\u3082\u306e\u3059\u3054\u304f\u9577\u304f\u306a\u3063\u305f\u3088\u3046\u306b\u611f\u3058\u307e\u3057\u305f\u3002\n\u305d\u306e\u8fba\u306e\u6bd4\u8f03\u306f\u3044\u305a\u308c\u30fb\u30fb\u30fb\n\u6b21\u56de\u306fCopyNet\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3059\u30fb\u30fb\u30fb\u884c\u3044\u305f\u3044\u3067\u3059\u3002\nEncoderDecoder\u30e2\u30c7\u30eb\u306e\u4e00\u7a2e\u3067\u3042\u308bSequence to Sequence\uff08Seq2Seq\uff09\u306b\u3001\nAttention Model\u3092\u5c0e\u5165\u3057\u3001\u305d\u306e\u5b9f\u88c5\u3068\u691c\u8a3c\u7d50\u679c\u306b\u3064\u3044\u3066\u8aac\u660e\u3059\u308b\u3002\n\n# \u306f\u3058\u3081\u306b\n\u524d\u56de\nhttp://qiita.com/kenchin110100/items/b34f5106d5a211f4c004\nSequence to Sequence\uff08Seq2Seq\uff09\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3092Chainer\u3067\u884c\u306a\u3063\u305f\u304c\u3001\n\u4eca\u56de\u306f\u305d\u306e\u30e2\u30c7\u30eb\u306bAttention Model\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n\u4ee5\u964d\u3067\u306f\u3001Attention Model\u3001\u305d\u306e\u5b9f\u88c5\u6cd5\u3001\u691c\u8a3c\u7d50\u679c\u306b\u3064\u3044\u3066\u8aac\u660e\u3092\u3057\u307e\u3059\u3002\n\n# Attention Model\n\n## Attention Model\u3068\u306f\n\nLSTM\u306a\u3069\u306eRNN\u7cfb\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3001\u6587\u306a\u3069\u306e\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3057\u304b\u3057\u521d\u671f\u306b\u5165\u529b\u3057\u305f\u30c7\u30fc\u30bf\u306f\u3001\u6700\u7d42\u7684\u306b\u51fa\u529b\u3055\u308c\u308b\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u53cd\u6620\u3055\u308c\u306b\u304f\u304f\u306a\u308a\u307e\u3059\u3002\n\n\u3064\u307e\u308a\u3001\u300c\u304a\u6bcd\u3055\u3093\u304c\u5316\u7ca7\u3092\u3057\u3066\u3001\u30b9\u30ab\u30fc\u30c8\u3092\u5c65\u3044\u3066\u8857\u306b\u51fa\u304b\u3051\u305f\u300d\u3068\u3044\u3046\u6587\u3068\u3001\u300c\u304a\u7236\u3055\u3093\u304c\u5316\u7ca7\u3092\u3057\u3066\u3001\u30b9\u30ab\u30fc\u30c8\u3092\u5c65\u3044\u3066\u8857\u306b\u51fa\u304b\u3051\u305f\u300d\u3068\u3044\u3046\u6587\u304c\u307b\u3068\u3093\u3069\u540c\u3058\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n\n\u521d\u671f\u306b\u5165\u529b\u3057\u305f\u30c7\u30fc\u30bf\u3082\u3061\u3083\u3093\u3068\u8003\u616e\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u4ed5\u7d44\u307f\u304cAttention Model\u3067\u3059\u3002\n\n## Sequence to Sequence with Attention Model\n\n\u524d\u56de\u5b9f\u88c5\u3057\u305fSeq2Seq\u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u306e\u6d41\u308c\u3092\u56f3\u793a\u3059\u308b\u3068\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n|Sequence to Sequence|\n|---|\n|![seq2seq.png](https://qiita-image-store.s3.amazonaws.com/0/155619/e025e292-de84-317d-4433-d3103da3c7ae.png)|\n\uff08\u524d\u56de\u3068\u5c11\u3057\u56f3\u3092\u5909\u3048\u3066\u3044\u307e\u3059\uff09\n\n\u9752\u8272\u306e\u90e8\u5206\u304c\u767a\u8a71\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308bEncoder\u3001\u8d64\u8272\u306e\u90e8\u5206\u304c\u30d9\u30af\u30c8\u30eb\u304b\u3089\u5fdc\u7b54\u3092\u51fa\u529b\u3059\u308bDecoder\u3067\u3059\u3002\n\n\u3053\u308c\u306bAttention Model\u3092\u52a0\u3048\u308b\u3068\u4ee5\u4e0b\u306e\u56f3\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n|Sequence to Sequence with Attention model|\n|---|\n|![attseq2seq.png](https://qiita-image-store.s3.amazonaws.com/0/155619/847bf628-c807-586b-b7f8-c4ef0eeae35a.png)|\n\n\u5c11\u3057\u8907\u96d1\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u56f3\u4e2d\u306e[At]\u3068\u66f8\u3044\u3066\u3042\u308b\u3068\u3053\u308d\u304cAttention Model\u3068\u306a\u308a\u307e\u3059\u3002\n\nEncoder\u5074\u3067\u306f\u3001\u6bce\u56de\u51fa\u529b\u3055\u308c\u308b\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092Attention Model\u306e\u4e2d\u306b\u8a18\u61b6\u3055\u305b\u3066\u3044\u304d\u307e\u3059\u3002\n\nDecoder\u5074\u3067\u306f\u30011\u3064\u524d\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092Attention Model\u306b\u5165\u529b\u3057\u307e\u3059\u3002\n\u5165\u529b\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u3092\u5143\u306bAttention Model\u304cEncoder\u5074\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u3068\u3063\u3066\u30ea\u30bf\u30fc\u30f3\u3057\u307e\u3059\u3002\n\nEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092Decoder\u306b\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u3001\u524d\u306b\u3042\u308b\u5358\u8a9e\u3001\u5f8c\u308d\u306b\u3042\u308b\u5358\u8a9e\u3001\u3069\u3053\u3067\u3082\u6ce8\u76ee\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u306e\u304cAttention Model\u3068\u306a\u308a\u307e\u3059\u3002\n\nAttention Model\u306b\u306f\u5927\u304d\u304f\u5206\u3051\u30662\u7a2e\u985e\u5b58\u5728\u3057\u3001Global Attention\u3068Local Attention\u3068\u547c\u3070\u308c\u307e\u3059\u3002\n\n\u4ee5\u964d\u3067\u306f\u3001Global Attention\u3001Local Attention\u306e\u8aac\u660e\u3092\u884c\u3044\u307e\u3059\u3002\n\n## Global Attention\n\nGlobal Attention\u304c\u63d0\u6848\u3055\u308c\u305f\u306e\u304c\u3001\u4e0b\u8a18\u306e\u8ad6\u6587\u306b\u306a\u308a\u307e\u3059\u3002\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014).\n\n\u5143\u3005\u306f\u6a5f\u68b0\u7ffb\u8a33\u3067\u4f7f\u308f\u308c\u3066\u3044\u305f\u3093\u3067\u3059\u306d\u3002\n\nGlobal Attention\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u305f\u8cc7\u6599\u306f\u3001\nhttps://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention\n\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\n\n\u3042\u3068\u82f1\u8a9e\u306b\u306a\u308a\u307e\u3059\u304c\u3001\nhttps://talbaumel.github.io/attention/\n\u3082\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\u3002\n\nEncoder\u5074\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u53d6\u308b\u4ed5\u7d44\u307f\u3092\u56f3\u793a\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n|Global Attention|\n|---|\n|![global_attention.png](https://qiita-image-store.s3.amazonaws.com/0/155619/c79bdcf2-ab31-d9d0-9212-b30f98ed31f9.png)|\n\n\u56f3\u306fEncoder\u5074\u3067[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1]\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb2]\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb3]\u306e3\u3064\u306e\u30d9\u30af\u30c8\u30eb\u304c\u5165\u529b\u3055\u308c\u3066\u308b\u72b6\u614b\u3092\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\n\u56f3\u4e2d\u306e[eh]\u3001[hh]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u304b\u3089\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3001[\uff0b]\u306f\u30d9\u30af\u30c8\u30eb\u306e\u8db3\u3057\u7b97\u3001[\u00d7]\u306f\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3054\u3068\u306e\u639b\u3051\u7b97\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\n\n[tanh]\u306fhyperbolic tangent\u3067\u3042\u308a\u3001\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3092-1\u304b\u30891\u307e\u3067\u306e\u30b9\u30b1\u30fc\u30eb\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n\n[hw]\u306f\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u304b\u3089\u30b5\u30a4\u30ba1\u306e\u30b9\u30ab\u30e9\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3067\u3059\u3002\n\n[soft max]\u306fSoftMax\u95a2\u6570\u3067\u3001\u5165\u529b\u3055\u308c\u305f\u5024\u3092\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u3057\u307e\u3059\u3002\n\n[Soft max]\u306b\u3088\u3063\u3066\u8a08\u7b97\u3055\u308c\u305f\u5024\u3092\u52a0\u91cd\u5e73\u5747\u306e\u30a6\u30a7\u30a4\u30c8\u3068\u3057\u3066\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u3068\u3063\u305f\u7d50\u679c\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n\u3053\u308c\u304cGlobal Attention\u306e\u4ed5\u7d44\u307f\u3067\u3059\u3002\n\n## Local Attention\n\nLocal Attention\u304c\u63d0\u6848\u3055\u308c\u305f\u8ad6\u6587\u306f\u4e0b\u8a18\u306e\u3082\u306e\n\nLuong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"Effective approaches to attention-based neural machine translation.\" arXiv preprint arXiv:1508.04025 (2015).\n\n\u3053\u308c\u3082\u6a5f\u68b0\u7ffb\u8a33\u306e\u8ad6\u6587\u3067\u3059\u306d\u3002\n\n\u53c2\u8003\u306b\u3057\u305f\u8cc7\u6599\u306f\nhttps://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention\n\u3067\u3059\u3002\n\n\u4e0b\u306fLocal Attention\u306e\u8a08\u7b97\u30d5\u30ed\u30fc\u56f3\u3067\u3059\u3002\n\n|Local Attention|\n|---|\n|![local_attention.png](https://qiita-image-store.s3.amazonaws.com/0/155619/40e861c9-32fb-1d0c-7bd7-71a3dd9378d2.png)|\n\nGlobal Attention\u306b\u3055\u3089\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u4e3b\u306a\u9055\u3044\u306f\u53f3\u5074\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3002\n\n[ht]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u304b\u3089\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u7dda\u5f62\u7d50\u5408\u3001[tanh]\u306f\u5148\u307b\u3069\u3068\u540c\u3058\u3001\u30d9\u30af\u30c8\u30eb\u306e\u8981\u7d20\u3092-1\u304b\u30891\u307e\u3067\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3059\u308b\u50cd\u304d\u3092\u3057\u307e\u3059\u3002\n\n[tw]\u306f\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\u3067\u3042\u308a\u3001[sigmoid]\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u3001\u5165\u529b\u3055\u308c\u305f\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001\u3053\u3053\u307e\u3067\u5165\u529b\u3055\u308c\u305f\u30d9\u30af\u30c8\u30eb\u306f\u30010\u304b\u30891\u307e\u3067\u306e\u7bc4\u56f2\u306e\u30b9\u30ab\u30e9\u30fc\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u6b21\u306b\u56f3\u4e2d\u306e[ga]\u3067\u4f55\u3092\u3057\u3066\u3044\u308b\u306e\u304b\u3092\u8aac\u660e\u3057\u307e\u3059\u3002ga\u306e\u8a08\u7b97\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u3059\u3002\n\n```math\noutput = \\exp\\bigl(-\\frac{(s - input * Len(S))^2}{\\sigma^2}\\bigl)\n```\n\n\u3053\u3053\u3067\u3001$input$\u306f0\u304b\u30891\u306e\u7bc4\u56f2\u306b\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30b9\u30ab\u30e9\u30fc\u3001$Len(S)$\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3067\u5165\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u6570\u3001$s$\u306f\u4e2d\u9593\u5c64\u30d9\u30af\u30c8\u30eb\u306e\u9806\u756a\uff08[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1]\u306a\u30891\u3001[\u4e2d\u9593\u30d9\u30af\u30c8\u30eb2]\u306a\u30892\uff09\u3092\u8868\u3057\u3066\u308b\u3002\n\n\u3082\u3057\u4eee\u306bsigmoid\u95a2\u6570\u3067\u51fa\u529b\u3055\u308c\u305f\u5024\u304c0.1\u306a\u3089\u3070\u3001[ga]\u306f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb1\u306e\u6642\u306b\u5927\u304d\u306a\u5024\u306b\u306a\u308a\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb3\u306e\u6642\u306f\u5c0f\u3055\u306a\u5024\u306b\u306a\u308b\u3002\n\n\u3053\u306e\u51fa\u529b\u3092Global Attention\u3067\u8a08\u7b97\u3057\u305f\u91cd\u307f\u306b\u639b\u3051\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u30d4\u30f3\u30dd\u30a4\u30f3\u30c8\u306b\u7279\u5b9a\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306b\u6ce8\u76ee\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n# \u5b9f\u88c5\n\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306bchainer\u3067\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002Encoder\u90e8\u5206\u306fSeq2Seq\u306e\u6642\u3068\u540c\u3058\u3067\u3059\u3002\n\n\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u30b3\u30fc\u30c9\u306foda\u69d8\u306e\u3082\u306e\u3067\u3059\u3002\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\nhttps://github.com/odashi/chainer_examples\n\n## Attention\n\n\u5b9f\u88c5\u3057\u305f\u306e\u306fGlobal Attention\u3067\u3059\u3002\u30b3\u30fc\u30c9\u306f\u4e0b\u8a18\u306e\u901a\u308a\u3001\n\n```attention.py\n\nclass Attention(Chain):\n    def __init__(self, hidden_size, flag_gpu):\n        \"\"\"\n        Attention\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        :param flag_gpu: GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        \"\"\"\n        super(Attention, self).__init__(\n            # \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            fh=links.Linear(hidden_size, hidden_size),\n            # \u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            bh=links.Linear(hidden_size, hidden_size),\n            # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u7dda\u5f62\u7d50\u5408\u5c64\n            hh=links.Linear(hidden_size, hidden_size),\n            # \u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u7dda\u5f62\u7d50\u5408\u5c64\n            hw=links.Linear(hidden_size, 1),\n        )\n        # \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3092\u8a18\u61b6\n        self.hidden_size = hidden_size\n        # GPU\u3092\u4f7f\u3046\u5834\u5408\u306fcupy\u3092\u4f7f\u308f\u306a\u3044\u3068\u304d\u306fnumpy\u3092\u4f7f\u3046\n        if flag_gpu:\n            self.ARR = cuda.cupy\n        else:\n            self.ARR = np\n\n    def __call__(self, fs, bs, h):\n        \"\"\"\n        Attention\u306e\u8a08\u7b97\n        :param fs: \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u304c\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :param bs: \u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u304c\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :param h: Decoder\u3067\u51fa\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        :return: \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3068\u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\n        \"\"\"\n        # \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u30b5\u30a4\u30ba\u3092\u8a18\u61b6\n        batch_size = h.data.shape[0]\n        # \u30a6\u30a7\u30a4\u30c8\u3092\u8a18\u9332\u3059\u308b\u305f\u3081\u306e\u30ea\u30b9\u30c8\u306e\u521d\u671f\u5316\n        ws = []\n        # \u30a6\u30a7\u30a4\u30c8\u306e\u5408\u8a08\u5024\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u5024\u3092\u521d\u671f\u5316\n        sum_w = Variable(self.ARR.zeros((batch_size, 1), dtype='float32'))\n        # Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3068Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u30a6\u30a7\u30a4\u30c8\u306e\u8a08\u7b97\n        for f, b in zip(fs, bs):\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9006\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u30a6\u30a7\u30a4\u30c8\u306e\u8a08\u7b97\n            w = functions.tanh(self.fh(f)+self.bh(b)+self.hh(h))\n            # softmax\u95a2\u6570\u3092\u4f7f\u3063\u3066\u6b63\u898f\u5316\u3059\u308b\n            w = functions.exp(self.hw(w))\n            # \u8a08\u7b97\u3057\u305f\u30a6\u30a7\u30a4\u30c8\u3092\u8a18\u9332\n            ws.append(w)\n            sum_w += w\n        # \u51fa\u529b\u3059\u308b\u52a0\u91cd\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        att_f = Variable(self.ARR.zeros((batch_size, self.hidden_size), dtype='float32'))\n        att_b = Variable(self.ARR.zeros((batch_size, self.hidden_size), dtype='float32'))\n        for f, b, w in zip(fs, bs, ws):\n            # \u30a6\u30a7\u30a4\u30c8\u306e\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\n            w /= sum_w\n            # \u30a6\u30a7\u30a4\u30c8 * Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u51fa\u529b\u3059\u308b\u30d9\u30af\u30c8\u30eb\u306b\u8db3\u3057\u3066\u3044\u304f\n            att_f += functions.reshape(functions.batch_matmul(f, w), (batch_size, self.hidden_size))\n            att_b += functions.reshape(functions.batch_matmul(f, w), (batch_size, self.hidden_size))\n        return att_f, att_b\n\n```\n\n\u8aac\u660e\u3067\u306f\u3001Encoder\u306f1\u3064\u3057\u304b\u4f7f\u308f\u306a\u304b\u3063\u305f\u3067\u3059\u304c\u3001\u5b9f\u306fAttention Model\u3067\u306f\u9806\u5411\u304d\u306eEncoder\u3068\u9006\u5411\u304d\u306eEncoder\u306e2\u7a2e\u985e\u3092\u4f7f\u3046\u3053\u3068\u304c\u4e00\u822c\u7684\u3067\u3059\u3002\n\n\u306a\u306e\u3067Attention\u306e\u8a08\u7b97\u3092\u3059\u308b\u6642\u306b\u3001\u9806\u5411\u304d\u306eEncoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u3001\u9006\u5411\u304d\u306eEncoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u306e2\u3064\u3092\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\n## Decoder\n\nDecoder\u3067\u5165\u529b\u3059\u308b\u5024\u306f\u3001Seq2Seq\u306e\u6642\u3068\u7570\u306a\u308a\u3001\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3001Decoder\u304c\u8a08\u7b97\u3057\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u306e3\u3064\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u305d\u3053\u3067Decoder\u306e\u5b9f\u88c5\u3092\u66f8\u304d\u63db\u3048\u3066\u3044\u307e\u3059\u3002\n\n```att_decoder.py\n\nclass Att_LSTM_Decoder(Chain):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        \"\"\"\n        Attention Model\u306e\u305f\u3081\u306eDecoder\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param vocab_size: \u8a9e\u5f59\u6570\n        :param embed_size: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        \"\"\"\n        super(Att_LSTM_Decoder, self).__init__(\n            # \u5358\u8a9e\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            ye=links.EmbedID(vocab_size, embed_size, ignore_label=-1),\n            # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            eh=links.Linear(embed_size, 4 * hidden_size),\n            # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            hh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            fh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u9806\u5411\u304dEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            bh=links.Linear(hidden_size, 4 * hidden_size),\n            # \u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\u306b\u5909\u63db\u3059\u308b\u5c64\n            he=links.Linear(hidden_size, embed_size),\n            # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u5c64\n            ey=links.Linear(embed_size, vocab_size)\n        )\n\n    def __call__(self, y, c, h, f, b):\n        \"\"\"\n        Decoder\u306e\u8a08\u7b97\n        :param y: Decoder\u306b\u5165\u529b\u3059\u308b\u5358\u8a9e\n        :param c: \u5185\u90e8\u30e1\u30e2\u30ea\n        :param h: Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        :param f: Attention Model\u3067\u8a08\u7b97\u3055\u308c\u305f\u9806\u5411\u304dEncoder\u306e\u52a0\u91cd\u5e73\u5747\n        :param b: Attention Model\u3067\u8a08\u7b97\u3055\u308c\u305f\u9006\u5411\u304dEncoder\u306e\u52a0\u91cd\u5e73\u5747\n        :return: \u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3001\u66f4\u65b0\u3055\u308c\u305f\u5185\u90e8\u30e1\u30e2\u30ea\u3001\u66f4\u65b0\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\n        \"\"\"\n        # \u5358\u8a9e\u3092\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\n        e = functions.tanh(self.ye(y))\n        # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3001Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9806\u5411\u304dEncoder\u306eAttention\u3001\u9006\u5411\u304dEncoder\u306eAttention\u3092\u4f7f\u3063\u3066LSTM\n        c, h = functions.lstm(c, self.eh(e) + self.hh(h) + self.fh(f) + self.bh(b))\n        # LSTM\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a9e\u5f59\u6570\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\n        t = self.ey(functions.tanh(self.he(h)))\n        return t, c, h\n\n```\n\n\u96a0\u308c\u5c64\u306e4\u500d\u306e\u30b5\u30a4\u30ba\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3046\u306e\u306f\u3001\u524d\u56de\u8aac\u660e\u3057\u305f\u7406\u7531\u3068\u540c\u69d8\u3067\u3059\u3002\n\nAttention\u306b\u3088\u308a\u8a08\u7b97\u3055\u308c\u305fEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u52a0\u91cd\u5e73\u5747\u3092\u4f7f\u3046\u305f\u3081\u306b\u3001[fh]\u3068[bh]\u3068\u3044\u3046\u5c64\u3092\u8ffd\u52a0\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u305d\u308c\u4ee5\u5916\u306f\u540c\u3058\u3067\u3059\u3002\n\n## Seq2Seq with Attention\n\nEncoder\u3001Decoder\u3001Attention\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u30e2\u30c7\u30eb\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n```att_seq2seq.py\n\nclass Att_Seq2Seq(Chain):\n    def __init__(self, vocab_size, embed_size, hidden_size, batch_size, flag_gpu=True):\n        \"\"\"\n        Seq2Seq + Attention\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        :param vocab_size: \u8a9e\u5f59\u6570\u306e\u30b5\u30a4\u30ba\n        :param embed_size: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u30b5\u30a4\u30ba\n        :param hidden_size: \u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\n        :param batch_size: \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u30b5\u30a4\u30ba\n        :param flag_gpu: GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        \"\"\"\n        super(Att_Seq2Seq, self).__init__(\n            # \u9806\u5411\u304d\u306eEncoder\n            f_encoder = LSTM_Encoder(vocab_size, embed_size, hidden_size),\n            # \u9006\u5411\u304d\u306eEncoder\n            b_encoder = LSTM_Encoder(vocab_size, embed_size, hidden_size),\n            # Attention Model\n            attention = Attention(hidden_size, flag_gpu),\n            # Decoder\n            decoder = Att_LSTM_Decoder(vocab_size, embed_size, hidden_size)\n        )\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.batch_size = batch_size\n\n        # GPU\u3092\u4f7f\u3046\u3068\u304d\u306fcupy\u3001\u4f7f\u308f\u306a\u3044\u3068\u304d\u306fnumpy\n        if flag_gpu:\n            self.ARR = cuda.cupy\n        else:\n            self.ARR = np\n\n        # \u9806\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9006\u5411\u304d\u306eEncoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30ea\u30b9\u30c8\u3092\u521d\u671f\u5316\n        self.fs = []\n        self.bs = []\n\n    def encode(self, words):\n        \"\"\"\n        Encoder\u306e\u8a08\u7b97\n        :param words: \u5165\u529b\u3067\u4f7f\u7528\u3059\u308b\u5358\u8a9e\u8a18\u9332\u3055\u308c\u305f\u30ea\u30b9\u30c8\n        :return: \n        \"\"\"\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # \u5148\u305a\u306f\u9806\u5411\u304d\u306eEncoder\u306e\u8a08\u7b97\n        for w in words:\n            c, h = self.f_encoder(w, c, h)\n            # \u8a08\u7b97\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\n            self.fs.append(h)\n\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # \u9006\u5411\u304d\u306eEncoder\u306e\u8a08\u7b97\n        for w in reversed(words):\n            c, h = self.b_encoder(w, c, h)\n            # \u8a08\u7b97\u3055\u308c\u305f\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\n            self.bs.insert(0, h)\n\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        self.h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n\n    def decode(self, w):\n        \"\"\"\n        Decoder\u306e\u8a08\u7b97\n        :param w: Decoder\u3067\u5165\u529b\u3059\u308b\u5358\u8a9e\n        :return: \u4e88\u6e2c\u5358\u8a9e\n        \"\"\"\n        # Attention Model\u3092\u4f7f\u3063\u3066Encoder\u306e\u4e2d\u9593\u5c64\u306e\u52a0\u91cd\u5e73\u5747\u3092\u8a08\u7b97\n        att_f, att_b = self.attention(self.fs, self.bs, self.h)\n        # Decoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u9806\u5411\u304d\u306eAttention\u3001\u9006\u5411\u304d\u306eAttention\u3092\u4f7f\u3063\u3066\n        # \u6b21\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3001\u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e88\u6e2c\u5358\u8a9e\u306e\u8a08\u7b97\n        t, self.c, self.h = self.decoder(w, self.c, self.h, att_f, att_b)\n        return t\n\n    def reset(self):\n        \"\"\"\n        \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3092\u521d\u671f\u5316\u3059\u308b\n        :return: \n        \"\"\"\n        # \u5185\u90e8\u30e1\u30e2\u30ea\u3001\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        self.h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n        # Encoder\u306e\u4e2d\u9593\u30d9\u30af\u30c8\u30eb\u3092\u8a18\u9332\u3059\u308b\u30ea\u30b9\u30c8\u306e\u521d\u671f\u5316\n        self.fs = []\n        self.bs = []\n        # \u52fe\u914d\u306e\u521d\u671f\u5316\n        self.zerograds()\n\n```\n\n\u9806\u5411\u304d\u306eEncoder\u3001\u9006\u5411\u304d\u306eEncoder\u3001Decoder\u3068\u5168\u90e8\u30673\u3064\u306eLSTM\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\n\nforward\u306e\u8a08\u7b97\u3084train\u306e\u8a08\u7b97\u306fSeq2Seq\u306e\u6642\u3068\u540c\u3058\u3067\u3059\u3002\n\n\u4f5c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u306f\nhttps://github.com/kenchin110100/machine_learning/blob/master/sampleAttSeq2Seq.py\n\u306b\u3042\u308a\u307e\u3059\u3002\n\n# \u5b9f\u9a13\n\n## \u30b3\u30fc\u30d1\u30b9\n\n\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b\u5bfe\u8a71\u7834\u7dbb\u30b3\u30fc\u30d1\u30b9\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\nhttps://sites.google.com/site/dialoguebreakdowndetection/chat-dialogue-corpus\n\n## \u5b9f\u9a13\u7d50\u679c\n\n\u767a\u8a71\u5185\u5bb9\u3082\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b4\u7a2e\u985e\n\n* token1 = '\u304a\u306f\u3088\u3046'\n* token2 = '\u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f'\n* token3 = '\u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f'\n* token4 = '\u4eca\u65e5\u306f\u6691\u3044\u3067\u3059'\n\nEpoch\u3054\u3068\u306b\u5fdc\u7b54\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\n\u307e\u305a1Epoch\n\n```text\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u305d\u3046', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u306f\u3044', '\u3001', '\u4f55', '\u3092', '\u898b', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u306f\u3044', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u306f\u3044', '\u3001', '\u4f55', '\u3092', '\u898b', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n```\n\u305d\u3093\u306a\u306b\u3001\u3044\u3084\u3089\u3057\u3044\u76ee\u3064\u304d\u3092\u3057\u3066\u3044\u307e\u3057\u305f\u304b\u30fb\u30fb\u30fb\n\n3Epoch\n\n```text\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3053\u3093\u306b\u3061\u306f\u3002', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u305d\u3046', '\u3067\u3059', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u307e\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u306a\u3044', '\u3093', '\u3067\u3059', '\u304b', '?', '</s>']\n```\n\n5Epoch\n\n```text\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3042\u308a\u304c\u3068\u3046', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u30b9\u30a4\u30ab', '\u306f', '\u597d\u304d', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u305f\u3044', '\u3067\u3059', '\u304b', '?', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u3066', '\u305f\u3044', '\u3067\u3059', '\u304b', '?', '</s>']\n```\n\u71b1\u4e2d\u75c7\u306f\u3082\u3046\u308f\u304b\u3063\u305f\u304b\u3089\u30fb\u30fb\u30fb\n\n7Epoch\n\n```text\n\u767a\u8a71:  \u304a\u306f\u3088\u3046 => \u5fdc\u7b54:  ['\u3053\u3093\u3070\u3093\u306f', '</s>']\n\u767a\u8a71:  \u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f => \u5fdc\u7b54:  ['\u30b9\u30a4\u30ab', '\u306f', '\u5927\u597d\u304d', '\u3067\u3059', '\u306d', '</s>']\n\u767a\u8a71:  \u304a\u8179\u304c\u7a7a\u304d\u307e\u3057\u305f => \u5fdc\u7b54:  ['\u3070\u3044', '\u3070\u3044', '</s>']\n\u767a\u8a71:  \u4eca\u65e5\u306f\u6691\u3044\u3067\u3059 => \u5fdc\u7b54:  ['\u71b1\u4e2d\u75c7', '\u306b', '\u6c17', '\u3092', '\u3064\u3051', '\u306a\u3044', '\u3093', '\u3067\u3059', '\u304b', '?', '</s>']\n```\n\u304a\u306f\u3088\u3046 => \u3053\u3093\u3070\u3093\u308f\u3000\u306f\u3001\u3072\u3069\u3044\u3067\u3059\u306d\u30fb\u30fb\u30fb\n\n\u306a\u3093\u304bSeq2Seq\u306e\u6642\u3088\u308a\u3082\u7cbe\u5ea6\u304c\u60aa\u304f\u306a\u3063\u305f\u6c17\u304c\u30fb\u30fb\u30fb\n\n\u300c\u8abf\u5b50\u306f\u3069\u3046\u3067\u3059\u304b\uff1f\u300d\u3068\u3044\u3046\u767a\u8a71\u306b\u5bfe\u3057\u3066\u306fSeq2Seq\u306e\u6642\u3068\u5171\u901a\u3057\u3066\u3046\u307e\u304f\u8fd4\u305b\u3066\u306a\u3044\u3067\u3059\u306d\u3002\n\u304a\u305d\u3089\u304f\u300c\u8abf\u5b50\u300d\u3068\u3044\u3046\u5358\u8a9e\u304c\u30b3\u30fc\u30d1\u30b9\u306e\u4e2d\u3067\u4f7f\u308f\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u306e\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u5143\u304c\u5bfe\u8a71\u7834\u7dbb\u30b3\u30fc\u30d1\u30b9\u306a\u306e\u3067\u7834\u7dbb\u3057\u305f\u7d50\u679c\u304c\u8fd4\u3063\u3066\u304d\u3066\u3044\u308b\u3001\u3064\u307e\u308a\u3053\u308c\u3067\u3046\u307e\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u308b\u306e\u304b\u3082\u30fb\u30fb\u30fb\n\n\u640d\u5931\u306e\u5408\u8a08\u5024\u306e\u63a8\u79fb\u3084\u8a08\u7b97\u6642\u9593\u306e\u6bd4\u8f03\u306f\u6700\u5f8c\u306b\u307e\u3068\u3081\u3066\u884c\u3044\u307e\u3059\u3002\n\n# \u7d50\u8ad6\n\nchainer\u3092\u4f7f\u3063\u3066Seq2Seq + Attention Model\u306e\u8a08\u7b97\u3092\u884c\u3044\u307e\u3057\u305f\u3002\n\nSeq2Seq\u3060\u3051\u306e\u6642\u3068\u6bd4\u8f03\u3057\u3066\u8a08\u7b97\u6642\u9593\u304c\u3082\u306e\u3059\u3054\u304f\u9577\u304f\u306a\u3063\u305f\u3088\u3046\u306b\u611f\u3058\u307e\u3057\u305f\u3002\n\u305d\u306e\u8fba\u306e\u6bd4\u8f03\u306f\u3044\u305a\u308c\u30fb\u30fb\u30fb\n\n\u6b21\u56de\u306fCopyNet\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3059\u30fb\u30fb\u30fb\u884c\u3044\u305f\u3044\u3067\u3059\u3002\n"}