{"context": "\n\n\u306f\u3058\u3081\u306b\n\u5f37\u5316\u5b66\u7fd2\u3092\u8a66\u3057\u3066\u307f\u305f\u3044\u984c\u6750\u306f\u3042\u308b\u3051\u3069\u3001\u81ea\u5206\u3067\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3059\u308b\u306e\u306f\u30fb\u30fb\u30fb\u3068\u3044\u3046\u65b9\u5411\u3051\u306b\u3001\n\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u984c\u6750\u306e\u74b0\u5883\u3092\u7528\u610f\u3057\u3001keras-rl\u3067\u5f37\u5316\u5b66\u7fd2\u3059\u308b\u307e\u3067\u306e\u6d41\u308c\u3092\u8aac\u660e\u3057\u307e\u3059\u3002\n\n\u5b9f\u884c\u6642\u306e\u74b0\u5883\n\nPython 3.5\nkeras 1.2.0\nkeras-rl 0.2.0rc1\nJupyter notebook\n\n\n\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\n\nkeras\nhttps://github.com/fchollet/keras\npip install keras\n\n\u7c21\u5358\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u69cb\u7bc9\u3067\u304d\u308b\u3068\u8a71\u984c\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\u3002\n\nkeras-rl\nhttps://github.com/matthiasplappert/keras-rl\nkeras\u3092\u5229\u7528\u3057\u3066\u3001DQN\u306a\u3069\u306e\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3057\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\n\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3053\u3061\u3089\u3092\u53c2\u7167\u3002\ngit\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\ngit clone https://github.com/matthiasplappert/keras-rl.git\npip install ./keras-rl\n\n\nOpenAI gym\nhttps://github.com/openai/gym\npip install gym\n\n\u5f37\u5316\u5b66\u7fd2\u5411\u3051\u306b\u3001\u3055\u307e\u3056\u307e\u306a\u74b0\u5883\u304c\u7528\u610f\u3057\u3066\u3042\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\nkeras-rl\u304c\u3001\u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306bgym\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u8981\u6c42\u3059\u308b\u306e\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\nkeras-rl\u306eexample\u306bgym\u306eCartPole\u3092DQN\u3067\u5b66\u7fd2\u3059\u308b\u30b3\u30fc\u30c9\u304c\u3042\u308b\u306e\u3067\u3001\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306e\u69cb\u7bc9\nkeras-rl\u306b\u5b66\u7fd2\u3057\u3066\u3082\u3089\u3046\u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306f\u3001OpenAI gym\u306eEnv\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\n\u5b9f\u88c5\u3059\u308bgym\u306eEnv\u306e\u30b3\u30e1\u30f3\u30c8\u306b\u306f\u3001(https://github.com/openai/gym/blob/master/gym/core.py#L27)\n    When implementing an environment, override the following methods\n    in your subclass:\n        _step\n        _reset\n        _render\n        _close\n        _configure\n        _seed\n    And set the following attributes:\n        action_space: The Space object corresponding to valid actions\n        observation_space: The Space object corresponding to valid observations\n        reward_range: A tuple corresponding to the min and max possible rewards\n\n\u3068\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u304c\u3001\u6700\u4f4e\u9650\u3001\u4e0b\u8a18\u3092\u5b9f\u88c5\u3059\u308c\u3070OK\u3067\u3059\u3002\n_step\n_reset\naction_space\nobservation_space\n\n\u4eca\u56de\u306f\u7c21\u5358\u306b\u3001\u76f4\u7dda\u4e0a\u3092\u52d5\u304f\u70b9\u3092\u4f8b\u3068\u3057\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u521d\u671f\u4f4d\u7f6e\u304b\u3089\u901f\u5ea6\u3092\u64cd\u4f5c\u3057\u3066\u3001\u539f\u70b9\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u3053\u3068\u3092\u4f8b\u3068\u3057\u307e\u3059\u3002\nimport gym\nimport gym.spaces\nimport numpy as np\n\n# \u76f4\u7dda\u4e0a\u3092\u52d5\u304f\u70b9\u306e\u901f\u5ea6\u3092\u64cd\u4f5c\u3057\u3001\u76ee\u6a19(\u539f\u70b9)\u306b\u79fb\u52d5\u3055\u305b\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u74b0\u5883\nclass PointOnLine(gym.core.Env):\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(3) # \u884c\u52d5\u7a7a\u9593\u3002\u901f\u5ea6\u3092\u4e0b\u3052\u308b\u3001\u305d\u306e\u307e\u307e\u3001\u4e0a\u3052\u308b\u306e3\u7a2e\n\n        high = np.array([1.0, 1.0]) # \u89b3\u6e2c\u7a7a\u9593(state)\u306e\u6b21\u5143 (\u4f4d\u7f6e\u3068\u901f\u5ea6\u306e2\u6b21\u5143) \u3068\u305d\u308c\u3089\u306e\u6700\u5927\u5024\n        self.observation_space = gym.spaces.Box(low=-high, high=high) # \u6700\u5c0f\u5024\u306f\u3001\u6700\u5927\u5024\u306e\u30de\u30a4\u30ca\u30b9\u304c\u3051\n\n    # \u5404step\u3054\u3068\u306b\u547c\u3070\u308c\u308b\n    # action\u3092\u53d7\u3051\u53d6\u308a\u3001\u6b21\u306estate\u3068reward\u3001episode\u304c\u7d42\u4e86\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059\u3088\u3046\u306b\u5b9f\u88c5\n    def _step(self, action):\n        # action\u3092\u53d7\u3051\u53d6\u308a\u3001\u6b21\u306estate\u3092\u6c7a\u5b9a\n        dt = 0.1\n        acc = (action - 1) * 0.1\n        self._vel += acc * dt\n        self._vel = max(-1.0,  min(self._vel, 1.0))\n        self._pos += self._vel * dt\n        self._pos = max(-1.0,  min(self._pos, 1.0))\n\n        # \u4f4d\u7f6e\u3068\u901f\u5ea6\u306e\u7d76\u5bfe\u5024\u304c\u5341\u5206\u5c0f\u3055\u304f\u306a\u3063\u305f\u3089episode\u7d42\u4e86\n        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n\n        if done:\n            # \u7d42\u4e86\u3057\u305f\u3068\u304d\u306b\u6b63\u306e\u5831\u916c\n            reward = 1.0\n        else:\n            # \u6642\u9593\u7d4c\u904e\u3054\u3068\u306b\u8ca0\u306e\u5831\u916c\n            # \u30b4\u30fc\u30eb\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3001\u8ddd\u96e2\u304c\u8fd1\u304f\u306a\u308b\u307b\u3069\u7d76\u5bfe\u5024\u3092\u6e1b\u3089\u3057\u3066\u304a\u304f\u3068\u3001\u5b66\u7fd2\u304c\u65e9\u304f\u9032\u3080\n            reward = -0.01 * abs(self._pos)\n\n        # \u6b21\u306estate\u3001reward\u3001\u7d42\u4e86\u3057\u305f\u304b\u3069\u3046\u304b\u3001\u8ffd\u52a0\u60c5\u5831\u306e\u9806\u306b\u8fd4\u3059\n        # \u8ffd\u52a0\u60c5\u5831\u306f\u7279\u306b\u306a\u3044\u306e\u3067\u7a7adict\n        return np.array([self._pos, self._vel]), reward, done, {}\n\n    # \u5404episode\u306e\u958b\u59cb\u6642\u306b\u547c\u3070\u308c\u3001\u521d\u671fstate\u3092\u8fd4\u3059\u3088\u3046\u306b\u5b9f\u88c5\n    def _reset(self):\n        # \u521d\u671fstate\u306f\u3001\u4f4d\u7f6e\u306f\u30e9\u30f3\u30c0\u30e0\u3001\u901f\u5ea6\u30bc\u30ed\n        self._pos = np.random.rand()*2 - 1\n        self._vel = 0.0\n        return np.array([self._pos, self._vel])\n\n\nDQN\u306e\u69cb\u7bc9\u3068\u5b66\u7fd2\nkeras-rl\u306eexample\u306edqn_cartpole.py\u3092\u53c2\u8003\u306b\u3057\u3066\u3001DQN\u306e\u69cb\u7bc9\u3068\u5b66\u7fd2\u3092\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u307e\u3059\u3002\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\n\nenv = PointOnLine()\nnb_actions = env.action_space.n\n\n# DQN\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# experience replay\u7528\u306ememory\nmemory = SequentialMemory(limit=50000, window_length=1)\n# \u884c\u52d5\u65b9\u7b56\u306f\u30aa\u30fc\u30bd\u30c9\u30c3\u30af\u30b9\u306aepsilon-greedy\u3002\u307b\u304b\u306b\u3001\u5404\u884c\u52d5\u306eQ\u5024\u306b\u3088\u3063\u3066\u78ba\u7387\u3092\u6c7a\u5b9a\u3059\u308bBoltzmannQPolicy\u304c\u5229\u7528\u53ef\u80fd\npolicy = EpsGreedyQPolicy(eps=0.1) \ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n               target_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\n\nhistory = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n#\u5b66\u7fd2\u306e\u69d8\u5b50\u3092\u63cf\u753b\u3057\u305f\u3044\u3068\u304d\u306f\u3001Env\u306b_render()\u3092\u5b9f\u88c5\u3057\u3066\u3001visualize=True \u306b\u3057\u307e\u3059,\n\n\n\u30c6\u30b9\u30c8\u3068\u7d50\u679c\u306e\u63cf\u753b\n\u5b66\u7fd2\u3057\u305fAgent\u3092\u30c6\u30b9\u30c8\u3057\u3066\u3001\u7d50\u679c\u3092\u63cf\u753b\u3057\u3066\u307f\u307e\u3059\u3002\n\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u60c5\u5831\u3092\u8a18\u61b6\u3059\u308bCallback\u3092\u5b9f\u88c5\u3057\u3066(keras-rl\u306b\u306f\u306a\u3044\uff1f)\u3001\ntest\u3092\u5b9f\u884c\u3057\u3001Callback\u306b\u305f\u307e\u3063\u305f\u7d50\u679c\u3092plot\u3057\u307e\u3059\u3002\nimport rl.callbacks\nclass EpisodeLogger(rl.callbacks.Callback):\n    def __init__(self):\n        self.observations = {}\n        self.rewards = {}\n        self.actions = {}\n\n    def on_episode_begin(self, episode, logs):\n        self.observations[episode] = []\n        self.rewards[episode] = []\n        self.actions[episode] = []\n\n    def on_step_end(self, step, logs):\n        episode = logs['episode']\n        self.observations[episode].append(logs['observation'])\n        self.rewards[episode].append(logs['reward'])\n        self.actions[episode].append(logs['action'])\n\ncb_ep = EpisodeLogger()\ndqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfor obs in cb_ep.observations.values():\n    plt.plot([o[0] for o in obs])\nplt.xlabel(\"step\")\nplt.ylabel(\"pos\")\n\nTesting for 10 episodes ...\nEpisode 1: reward: 0.972, steps: 17\nEpisode 2: reward: 0.975, steps: 16\nEpisode 3: reward: 0.832, steps: 44\nEpisode 4: reward: 0.973, steps: 17\nEpisode 5: reward: 0.799, steps: 51\nEpisode 6: reward: 1.000, steps: 1\nEpisode 7: reward: 0.704, steps: 56\nEpisode 8: reward: 0.846, steps: 45\nEpisode 9: reward: 0.667, steps: 63\nEpisode 10: reward: 0.944, steps: 29\n\n\n\u4f4d\u7f6e0\u306b\u30b9\u30e0\u30fc\u30ba\u306b\u5411\u304b\u3046\u3088\u3046\u306b\u5b66\u7fd2\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n# \u306f\u3058\u3081\u306b\n\u5f37\u5316\u5b66\u7fd2\u3092\u8a66\u3057\u3066\u307f\u305f\u3044\u984c\u6750\u306f\u3042\u308b\u3051\u3069\u3001\u81ea\u5206\u3067\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3059\u308b\u306e\u306f\u30fb\u30fb\u30fb\u3068\u3044\u3046\u65b9\u5411\u3051\u306b\u3001\n\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u984c\u6750\u306e\u74b0\u5883\u3092\u7528\u610f\u3057\u3001keras-rl\u3067\u5f37\u5316\u5b66\u7fd2\u3059\u308b\u307e\u3067\u306e\u6d41\u308c\u3092\u8aac\u660e\u3057\u307e\u3059\u3002\n\n# \u5b9f\u884c\u6642\u306e\u74b0\u5883\n* Python 3.5\n* keras 1.2.0\n* keras-rl 0.2.0rc1\n* Jupyter notebook\n\n# \u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\n## keras\n\nhttps://github.com/fchollet/keras\n\n```\npip install keras\n```\n\n\u7c21\u5358\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u69cb\u7bc9\u3067\u304d\u308b\u3068\u8a71\u984c\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\u3002\n\n## keras-rl\nhttps://github.com/matthiasplappert/keras-rl\n\nkeras\u3092\u5229\u7528\u3057\u3066\u3001DQN\u306a\u3069\u306e\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3057\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\n[\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3053\u3061\u3089\u3092\u53c2\u7167\u3002](https://github.com/matthiasplappert/keras-rl/wiki/Agent-Overview)\ngit\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\n```\ngit clone https://github.com/matthiasplappert/keras-rl.git\npip install ./keras-rl\n```\n\n\n\n## OpenAI gym\nhttps://github.com/openai/gym\n\n```\npip install gym\n```\n\n\u5f37\u5316\u5b66\u7fd2\u5411\u3051\u306b\u3001\u3055\u307e\u3056\u307e\u306a\u74b0\u5883\u304c\u7528\u610f\u3057\u3066\u3042\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\nkeras-rl\u304c\u3001\u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306bgym\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u8981\u6c42\u3059\u308b\u306e\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n[keras-rl\u306eexample\u306bgym\u306eCartPole\u3092DQN\u3067\u5b66\u7fd2\u3059\u308b\u30b3\u30fc\u30c9\u304c\u3042\u308b](https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py)\u306e\u3067\u3001\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\n# \u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306e\u69cb\u7bc9\nkeras-rl\u306b\u5b66\u7fd2\u3057\u3066\u3082\u3089\u3046\u5f37\u5316\u5b66\u7fd2\u306e\u74b0\u5883\u306f\u3001OpenAI gym\u306eEnv\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\n\u5b9f\u88c5\u3059\u308bgym\u306eEnv\u306e\u30b3\u30e1\u30f3\u30c8\u306b\u306f\u3001(https://github.com/openai/gym/blob/master/gym/core.py#L27)\n\n```\n    When implementing an environment, override the following methods\n    in your subclass:\n        _step\n        _reset\n        _render\n        _close\n        _configure\n        _seed\n    And set the following attributes:\n        action_space: The Space object corresponding to valid actions\n        observation_space: The Space object corresponding to valid observations\n        reward_range: A tuple corresponding to the min and max possible rewards\n```\n\n\u3068\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u304c\u3001\u6700\u4f4e\u9650\u3001\u4e0b\u8a18\u3092\u5b9f\u88c5\u3059\u308c\u3070OK\u3067\u3059\u3002\n\n```\n_step\n_reset\naction_space\nobservation_space\n```\n\n\u4eca\u56de\u306f\u7c21\u5358\u306b\u3001\u76f4\u7dda\u4e0a\u3092\u52d5\u304f\u70b9\u3092\u4f8b\u3068\u3057\u3001\u30e9\u30f3\u30c0\u30e0\u306a\u521d\u671f\u4f4d\u7f6e\u304b\u3089\u901f\u5ea6\u3092\u64cd\u4f5c\u3057\u3066\u3001\u539f\u70b9\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u3053\u3068\u3092\u4f8b\u3068\u3057\u307e\u3059\u3002\n\n```py3\nimport gym\nimport gym.spaces\nimport numpy as np\n\n# \u76f4\u7dda\u4e0a\u3092\u52d5\u304f\u70b9\u306e\u901f\u5ea6\u3092\u64cd\u4f5c\u3057\u3001\u76ee\u6a19(\u539f\u70b9)\u306b\u79fb\u52d5\u3055\u305b\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u74b0\u5883\nclass PointOnLine(gym.core.Env):\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(3) # \u884c\u52d5\u7a7a\u9593\u3002\u901f\u5ea6\u3092\u4e0b\u3052\u308b\u3001\u305d\u306e\u307e\u307e\u3001\u4e0a\u3052\u308b\u306e3\u7a2e\n\n        high = np.array([1.0, 1.0]) # \u89b3\u6e2c\u7a7a\u9593(state)\u306e\u6b21\u5143 (\u4f4d\u7f6e\u3068\u901f\u5ea6\u306e2\u6b21\u5143) \u3068\u305d\u308c\u3089\u306e\u6700\u5927\u5024\n        self.observation_space = gym.spaces.Box(low=-high, high=high) # \u6700\u5c0f\u5024\u306f\u3001\u6700\u5927\u5024\u306e\u30de\u30a4\u30ca\u30b9\u304c\u3051\n        \n    # \u5404step\u3054\u3068\u306b\u547c\u3070\u308c\u308b\n    # action\u3092\u53d7\u3051\u53d6\u308a\u3001\u6b21\u306estate\u3068reward\u3001episode\u304c\u7d42\u4e86\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059\u3088\u3046\u306b\u5b9f\u88c5\n    def _step(self, action):\n        # action\u3092\u53d7\u3051\u53d6\u308a\u3001\u6b21\u306estate\u3092\u6c7a\u5b9a\n        dt = 0.1\n        acc = (action - 1) * 0.1\n        self._vel += acc * dt\n        self._vel = max(-1.0,  min(self._vel, 1.0))\n        self._pos += self._vel * dt\n        self._pos = max(-1.0,  min(self._pos, 1.0))\n        \n        # \u4f4d\u7f6e\u3068\u901f\u5ea6\u306e\u7d76\u5bfe\u5024\u304c\u5341\u5206\u5c0f\u3055\u304f\u306a\u3063\u305f\u3089episode\u7d42\u4e86\n        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n\n        if done:\n            # \u7d42\u4e86\u3057\u305f\u3068\u304d\u306b\u6b63\u306e\u5831\u916c\n            reward = 1.0\n        else:\n            # \u6642\u9593\u7d4c\u904e\u3054\u3068\u306b\u8ca0\u306e\u5831\u916c\n            # \u30b4\u30fc\u30eb\u306b\u8fd1\u3065\u304f\u3088\u3046\u306b\u3001\u8ddd\u96e2\u304c\u8fd1\u304f\u306a\u308b\u307b\u3069\u7d76\u5bfe\u5024\u3092\u6e1b\u3089\u3057\u3066\u304a\u304f\u3068\u3001\u5b66\u7fd2\u304c\u65e9\u304f\u9032\u3080\n            reward = -0.01 * abs(self._pos)\n            \n        # \u6b21\u306estate\u3001reward\u3001\u7d42\u4e86\u3057\u305f\u304b\u3069\u3046\u304b\u3001\u8ffd\u52a0\u60c5\u5831\u306e\u9806\u306b\u8fd4\u3059\n        # \u8ffd\u52a0\u60c5\u5831\u306f\u7279\u306b\u306a\u3044\u306e\u3067\u7a7adict\n        return np.array([self._pos, self._vel]), reward, done, {}\n    \n    # \u5404episode\u306e\u958b\u59cb\u6642\u306b\u547c\u3070\u308c\u3001\u521d\u671fstate\u3092\u8fd4\u3059\u3088\u3046\u306b\u5b9f\u88c5\n    def _reset(self):\n        # \u521d\u671fstate\u306f\u3001\u4f4d\u7f6e\u306f\u30e9\u30f3\u30c0\u30e0\u3001\u901f\u5ea6\u30bc\u30ed\n        self._pos = np.random.rand()*2 - 1\n        self._vel = 0.0\n        return np.array([self._pos, self._vel])\n```\n\n# DQN\u306e\u69cb\u7bc9\u3068\u5b66\u7fd2\n[keras-rl\u306eexample\u306edqn_cartpole.py](https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py)\u3092\u53c2\u8003\u306b\u3057\u3066\u3001DQN\u306e\u69cb\u7bc9\u3068\u5b66\u7fd2\u3092\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u307e\u3059\u3002\n\n```py3\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\n\nenv = PointOnLine()\nnb_actions = env.action_space.n\n\n# DQN\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5b9a\u7fa9\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# experience replay\u7528\u306ememory\nmemory = SequentialMemory(limit=50000, window_length=1)\n# \u884c\u52d5\u65b9\u7b56\u306f\u30aa\u30fc\u30bd\u30c9\u30c3\u30af\u30b9\u306aepsilon-greedy\u3002\u307b\u304b\u306b\u3001\u5404\u884c\u52d5\u306eQ\u5024\u306b\u3088\u3063\u3066\u78ba\u7387\u3092\u6c7a\u5b9a\u3059\u308bBoltzmannQPolicy\u304c\u5229\u7528\u53ef\u80fd\npolicy = EpsGreedyQPolicy(eps=0.1) \ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n               target_model_update=1e-2, policy=policy)\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\n\nhistory = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n#\u5b66\u7fd2\u306e\u69d8\u5b50\u3092\u63cf\u753b\u3057\u305f\u3044\u3068\u304d\u306f\u3001Env\u306b_render()\u3092\u5b9f\u88c5\u3057\u3066\u3001visualize=True \u306b\u3057\u307e\u3059,\n```\n\n# \u30c6\u30b9\u30c8\u3068\u7d50\u679c\u306e\u63cf\u753b\n\u5b66\u7fd2\u3057\u305fAgent\u3092\u30c6\u30b9\u30c8\u3057\u3066\u3001\u7d50\u679c\u3092\u63cf\u753b\u3057\u3066\u307f\u307e\u3059\u3002\n\u5404\u30b9\u30c6\u30c3\u30d7\u306e\u60c5\u5831\u3092\u8a18\u61b6\u3059\u308bCallback\u3092\u5b9f\u88c5\u3057\u3066(keras-rl\u306b\u306f\u306a\u3044\uff1f)\u3001\ntest\u3092\u5b9f\u884c\u3057\u3001Callback\u306b\u305f\u307e\u3063\u305f\u7d50\u679c\u3092plot\u3057\u307e\u3059\u3002\n\n```py3\nimport rl.callbacks\nclass EpisodeLogger(rl.callbacks.Callback):\n    def __init__(self):\n        self.observations = {}\n        self.rewards = {}\n        self.actions = {}\n\n    def on_episode_begin(self, episode, logs):\n        self.observations[episode] = []\n        self.rewards[episode] = []\n        self.actions[episode] = []\n        \n    def on_step_end(self, step, logs):\n        episode = logs['episode']\n        self.observations[episode].append(logs['observation'])\n        self.rewards[episode].append(logs['reward'])\n        self.actions[episode].append(logs['action'])\n\ncb_ep = EpisodeLogger()\ndqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfor obs in cb_ep.observations.values():\n    plt.plot([o[0] for o in obs])\nplt.xlabel(\"step\")\nplt.ylabel(\"pos\")\n```\n\n```\nTesting for 10 episodes ...\nEpisode 1: reward: 0.972, steps: 17\nEpisode 2: reward: 0.975, steps: 16\nEpisode 3: reward: 0.832, steps: 44\nEpisode 4: reward: 0.973, steps: 17\nEpisode 5: reward: 0.799, steps: 51\nEpisode 6: reward: 1.000, steps: 1\nEpisode 7: reward: 0.704, steps: 56\nEpisode 8: reward: 0.846, steps: 45\nEpisode 9: reward: 0.667, steps: 63\nEpisode 10: reward: 0.944, steps: 29\n```\n\n![res.png](https://qiita-image-store.s3.amazonaws.com/0/61468/1b4b24e9-35cb-24cd-eed5-f227ab79b0af.png)\n\n\u4f4d\u7f6e0\u306b\u30b9\u30e0\u30fc\u30ba\u306b\u5411\u304b\u3046\u3088\u3046\u306b\u5b66\u7fd2\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n", "tags": ["Python", "Keras", "\u5f37\u5316\u5b66\u7fd2", "DQN", "Keras-RL"]}