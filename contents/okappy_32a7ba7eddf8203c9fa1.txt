{"context": " More than 1 year has passed since last update.\u7c73google\u306e\u7814\u7a76\u8005\u304c\u958b\u767a\u3057\u305f\u300c Word2Vec \u300d\u3068\u3044\u3046\u6280\u8853\u3092\u30d9\u30fc\u30b9\u306b\u3001\u300c\u5358\u8a9e\u300d\u3060\u3051\u3067\u306f\u306a\u304f\u300c\u6587\u66f8\u300d\u306b\u3082\u610f\u5473\u3092\u6301\u305f\u305b\u3066\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6349\u3048\u3066\u5229\u7528\u3067\u304d\u308b\u6280\u8853\u300c Doc2Vec \u300d\u3092\u3044\u3058\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\nWord2Vec\u306e\u304a\u3055\u3089\u3044\n\u904e\u53bbQiita\u306b\u6295\u7a3f\u3057\u305f\u306e\u3067\u3001\u305d\u306e\u30ea\u30f3\u30af\u3092\u5f35\u3063\u3066\u304a\u304d\u307e\u3059\u3002\nhttp://qiita.com/okappy/items/e16639178ba85edfee72\n\nDoc2Vec\u3068\u306f\uff1f\nWord2Vec\u306fWord(\u5358\u8a9e)\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6349\u3048\u308b\u304c\u3001Doc2Vec(Paragraph2Vec)\u306fDocument(\u6587\u66f8)\u3092Word\u306e\u96c6\u5408\u3068\u3057\u3066\u898b\u3066\u30d9\u30af\u30c8\u30eb\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u3067\u3001\u6587\u66f8\u9593\u306e\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u306a\u3069\u3092\u5b9f\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\u4f8b\u3048\u3070\u3001\u30cb\u30e5\u30fc\u30b9\u8a18\u4e8b\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u30ec\u30b8\u30e5\u30e1\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u672c\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u3082\u3061\u308d\u3093\u4eba\u306e\u30d7\u30ed\u30d5\u30a3\u30fc\u30eb\u3068\u672c\u306e\u985e\u4f3c\u5ea6\u306a\u3069\u3082\u7b97\u51fa\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3001\u30c6\u30ad\u30b9\u30c8\u3067\u8868\u3055\u308c\u3066\u8005\u540c\u58eb\u3067\u3042\u308c\u3070\u3001\u5168\u3066\u304c\u5bfe\u8c61\u3068\u306a\u308b\u3002\n\n\u6280\u8853\u7684\u306b\u306f\n\npython\n\n\nScipy\ngensim \n\n\n\n\u3042\u305f\u308a\u3092\u4f7f\u3044\u307e\u3059\u3002\n\ngensim\u3068\u306f\uff1f\nPython\u304b\u3089\u6271\u3048\u308b\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\n\u6a5f\u80fd\u3068\u3057\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u6319\u3052\u3089\u308c\u308b\u3002\n\n\u6f5c\u5728\u610f\u5473\u89e3\u6790\uff08LSA/LSI/SVD\uff09\n\u6f5c\u5728\u30c7\u30a3\u30ea\u30af\u30ec\u914d\u5206\u6cd5\uff08LDA\uff09\nTF-IDF\nRandom Projection\uff08RP\uff09\n\u968e\u5c64\u7684\u30c7\u30a3\u30ea\u30af\u30ec\u904e\u7a0b\uff08HDP\uff09\n\u6df1\u5c64\u5b66\u7fd2\u3092\u7528\u3044\u305fword2vec\n\u5206\u6563\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\nDynamic Topic Model\uff08DTM\uff09\nDynamic Influence Models\uff08DIM\uff09\n\ngensim\u306e\u516c\u5f0f\u30da\u30fc\u30b8\nhttp://radimrehurek.com/gensim/\n\n\u5b9f\u969b\u306b\u6587\u66f8\u9593\u306e\u985e\u4f3c\u5ea6\u3092\u51fa\u3057\u3066\u307f\u308b\n\u4eca\u56de\u306f\u3001facebook\u306e\u30c7\u30fc\u30bf\u3092\u5229\u7528\u3057\u3066\u3001\u3042\u308b\u30e6\u30fc\u30b6\u30fc\u304c\u904e\u53bbfacebook\u306b\u6295\u7a3f\u3057\u305f\u30c6\u30ad\u30b9\u30c8\u3084\u30b7\u30a7\u30a2\u3057\u305f\u30ea\u30f3\u30af\u306e\u30bf\u30a4\u30c8\u30eb\u306a\u3069\u3092\u4e00\u3064\u306e\u6587\u66f8\u3068\u898b\u7acb\u3066\u3066\u3001\u305d\u306e\u6587\u66f8\u540c\u58eb(\u8981\u3059\u308b\u306b\u30e6\u30fc\u30b6\u30fc\u540c\u58eb)\u306e\u985e\u4f3c\u5ea6\u3092\u51fa\u3057\u3066\u307f\u308b\u3002\n\n\u5b9f\u88c5(\u6e96\u5099)\n\n\u25a0 Scipy\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install scipy\n\n\n\u25a0 gensim\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install gensim\n\n\n\u25a0 doc2vec.py\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\n\u5909\u66f4\u70b9\u2460\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306edoc2vec.py\u3060\u3068\u3001\u30ec\u30b9\u30dd\u30f3\u30b9\u306e\u3068\u304d\u306elabel\u304c\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3001\n\u8a2d\u5b9a\u3057\u305flabel\u3067\u7d50\u679c\u3092\u547c\u3073\u51fa\u305b\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u5909\u66f4\u70b9\u2461\ndoc2vec.py\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001\u6587\u66f8\u306e\u4f3c\u3066\u3044\u308b\u3082\u306e\u306f\uff1f\u3063\u3066\u53e9\u304f\u3068\u3001\u6587\u66f8\u3082\u5358\u8a9e\u3082\u51fa\u529b\u3055\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u6587\u66f8\u306e\u4f3c\u3066\u3044\u308b\u6587\u66f8\u3060\u3051\u3092\u51fa\u529b\u3059\u308b\u30e1\u30bd\u30c3\u30c9\u3082\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\n\ndoc2vec.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013 Radim Rehurek <me@radimrehurek.com>\n# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n\n\n\"\"\"\nDeep learning via the distributed memory and distributed bag of words models from\n[1]_, using either hierarchical softmax or negative sampling [2]_ [3]_.\n\n**Make sure you have a C compiler before installing gensim, to use optimized (compiled)\ndoc2vec training** (70x speedup [blog]_).\n\nInitialize a model with e.g.::\n\n>>> model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)\n\nPersist a model to disk with::\n\n>>> model.save(fname)\n>>> model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n\nThe model can also be instantiated from an existing file on disk in the word2vec C format::\n\n  >>> model = Doc2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False)  # C text format\n  >>> model = Doc2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)  # C binary format\n\n.. [1] Quoc Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. http://arxiv.org/pdf/1405.4053v2.pdf\n.. [2] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n.. [3] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality.\n       In Proceedings of NIPS, 2013.\n.. [blog] Optimizing word2vec in gensim, http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/\n\n\"\"\"\n\nimport logging\nimport os\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom numpy import zeros, random, sum as np_sum\n\nlogger = logging.getLogger(__name__)\n\nfrom gensim import utils  # utility fnc for pickling, common scipy operations etc\nfrom gensim.models.word2vec import Word2Vec, Vocab, train_cbow_pair, train_sg_pair\n\ntry:\n    from gensim.models.doc2vec_inner import train_sentence_dbow, train_sentence_dm, FAST_VERSION\nexcept:\n    # failed... fall back to plain numpy (20-80x slower training than the above)\n    FAST_VERSION = -1\n\n    def train_sentence_dbow(model, sentence, lbls, alpha, work=None, train_words=True, train_lbls=True):\n        \"\"\"\n        Update distributed bag of words model by training on a single sentence.\n\n        The sentence is a list of Vocab objects (or None, where the corresponding\n        word is not in the vocabulary. Called internally from `Doc2Vec.train()`.\n\n        This is the non-optimized, Python version. If you have cython installed, gensim\n        will use the optimized version from doc2vec_inner instead.\n\n        \"\"\"\n        neg_labels = []\n        if model.negative:\n            # precompute negative labels\n            neg_labels = zeros(model.negative + 1)\n            neg_labels[0] = 1.0\n\n        for label in lbls:\n            if label is None:\n                continue  # OOV word in the input sentence => skip\n            for word in sentence:\n                if word is None:\n                    continue  # OOV word in the input sentence => skip\n                train_sg_pair(model, word, label, alpha, neg_labels, train_words, train_lbls)\n\n        return len([word for word in sentence if word is not None])\n\n    def train_sentence_dm(model, sentence, lbls, alpha, work=None, neu1=None, train_words=True, train_lbls=True):\n        \"\"\"\n        Update distributed memory model by training on a single sentence.\n\n        The sentence is a list of Vocab objects (or None, where the corresponding\n        word is not in the vocabulary. Called internally from `Doc2Vec.train()`.\n\n        This is the non-optimized, Python version. If you have a C compiler, gensim\n        will use the optimized version from doc2vec_inner instead.\n\n        \"\"\"\n        lbl_indices = [lbl.index for lbl in lbls if lbl is not None]\n        lbl_sum = np_sum(model.syn0[lbl_indices], axis=0)\n        lbl_len = len(lbl_indices)\n        neg_labels = []\n        if model.negative:\n            # precompute negative labels\n            neg_labels = zeros(model.negative + 1)\n            neg_labels[0] = 1.\n\n        for pos, word in enumerate(sentence):\n            if word is None:\n                continue  # OOV word in the input sentence => skip\n            reduced_window = random.randint(model.window)  # `b` in the original doc2vec code\n            start = max(0, pos - model.window + reduced_window)\n            window_pos = enumerate(sentence[start : pos + model.window + 1 - reduced_window], start)\n            word2_indices = [word2.index for pos2, word2 in window_pos if (word2 is not None and pos2 != pos)]\n            l1 = np_sum(model.syn0[word2_indices], axis=0) + lbl_sum  # 1 x layer1_size\n            if word2_indices and model.cbow_mean:\n                l1 /= (len(word2_indices) + lbl_len)\n            neu1e = train_cbow_pair(model, word, word2_indices, l1, alpha, neg_labels, train_words, train_words)\n            if train_lbls:\n                model.syn0[lbl_indices] += neu1e\n\n        return len([word for word in sentence if word is not None])\n\n\nclass LabeledSentence(object):\n    \"\"\"\n    A single labeled sentence = text item.\n    Replaces \"sentence as a list of words\" from Word2Vec.\n\n    \"\"\"\n    def __init__(self, words, labels):\n        \"\"\"\n        `words` is a list of tokens (unicode strings), `labels` a\n        list of text labels associated with this text.\n\n        \"\"\"\n        self.words = words\n        self.labels = labels\n\n    def __str__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.words, self.labels)\n\n\nclass Doc2Vec(Word2Vec):\n    \"\"\"Class for training, using and evaluating neural networks described in http://arxiv.org/pdf/1405.4053v2.pdf\"\"\"\n    def __init__(self, sentences=None, size=300, alpha=0.025, window=8, min_count=5,\n                 sample=0, seed=1, workers=1, min_alpha=0.0001, dm=1, hs=1, negative=0,\n                 dm_mean=0, train_words=True, train_lbls=True, **kwargs):\n        \"\"\"\n        Initialize the model from an iterable of `sentences`. Each sentence is a\n        LabeledSentence object that will be used for training.\n\n        The `sentences` iterable can be simply a list of LabeledSentence elements, but for larger corpora,\n        consider an iterable that streams the sentences directly from disk/network.\n\n        If you don't supply `sentences`, the model is left uninitialized -- use if\n        you plan to initialize it in some other way.\n\n        `dm` defines the training algorithm. By default (`dm=1`), distributed memory is used.\n        Otherwise, `dbow` is employed.\n\n        `size` is the dimensionality of the feature vectors.\n\n        `window` is the maximum distance between the current and predicted word within a sentence.\n\n        `alpha` is the initial learning rate (will linearly drop to zero as training progresses).\n\n        `seed` = for the random number generator.\n\n        `min_count` = ignore all words with total frequency lower than this.\n\n        `sample` = threshold for configuring which higher-frequency words are randomly downsampled;\n                default is 0 (off), useful value is 1e-5.\n\n        `workers` = use this many worker threads to train the model (=faster training with multicore machines).\n\n        `hs` = if 1 (default), hierarchical sampling will be used for model training (else set to 0).\n\n        `negative` = if > 0, negative sampling will be used, the int for negative\n        specifies how many \"noise words\" should be drawn (usually between 5-20).\n\n        `dm_mean` = if 0 (default), use the sum of the context word vectors. If 1, use the mean.\n        Only applies when dm is used.\n\n        \"\"\"\n        Word2Vec.__init__(self, size=size, alpha=alpha, window=window, min_count=min_count,\n                          sample=sample, seed=seed, workers=workers, min_alpha=min_alpha,\n                          sg=(1+dm) % 2, hs=hs, negative=negative, cbow_mean=dm_mean, **kwargs)\n        self.train_words = train_words\n        self.train_lbls = train_lbls\n        self.labels = set()\n        if sentences is not None:\n            self.build_vocab(sentences)\n            self.train(sentences)\n            self.build_labels(sentences)\n\n    @staticmethod\n    def _vocab_from(sentences):\n        sentence_no, vocab = -1, {}\n        total_words = 0\n        for sentence_no, sentence in enumerate(sentences):\n            if sentence_no % 10000 == 0:\n                logger.info(\"PROGRESS: at item #%i, processed %i words and %i word types\" %\n                            (sentence_no, total_words, len(vocab)))\n            sentence_length = len(sentence.words)\n            for label in sentence.labels:\n                total_words += 1\n                if label in vocab:\n                    vocab[label].count += sentence_length\n                else:\n                    vocab[label] = Vocab(count=sentence_length)\n            for word in sentence.words:\n                total_words += 1\n                if word in vocab:\n                    vocab[word].count += 1\n                else:\n                    vocab[word] = Vocab(count=1)\n        logger.info(\"collected %i word types from a corpus of %i words and %i items\" %\n                    (len(vocab), total_words, sentence_no + 1))\n        return vocab\n\n    def _prepare_sentences(self, sentences):\n        for sentence in sentences:\n            # avoid calling random_sample() where prob >= 1, to speed things up a little:\n            sampled = [self.vocab[word] for word in sentence.words\n                       if word in self.vocab and (self.vocab[word].sample_probability >= 1.0 or\n                                                  self.vocab[word].sample_probability >= random.random_sample())]\n            yield (sampled, [self.vocab[word] for word in sentence.labels if word in self.vocab])\n\n    def _get_job_words(self, alpha, work, job, neu1):\n        if self.sg:\n            return sum(train_sentence_dbow(self, sentence, lbls, alpha, work, self.train_words, self.train_lbls) for sentence, lbls in job)\n        else:\n            return sum(train_sentence_dm(self, sentence, lbls, alpha, work, neu1, self.train_words, self.train_lbls) for sentence, lbls in job)\n\n    def __str__(self):\n        return \"Doc2Vec(vocab=%s, size=%s, alpha=%s)\" % (len(self.index2word), self.layer1_size, self.alpha)\n\n    def save(self, *args, **kwargs):\n        kwargs['ignore'] = kwargs.get('ignore', ['syn0norm'])  # don't bother storing the cached normalized vectors\n        super(Doc2Vec, self).save(*args, **kwargs)\n\n    def build_labels(self, sentences):\n        self.labels |= self._labels_from(sentences)\n\n    @staticmethod\n    def _labels_from(sentences):\n        labels = set()\n        for sentence in sentences:\n            labels |= set(sentence.labels)\n        return labels\n\n    def most_similar_labels(self, positive=[], negative=[], topn=10):\n        \"\"\"\n        Find the top-N most similar labels.\n        \"\"\"\n        result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k in self.labels]\n        return result[:topn]\n\n    def most_similar_words(self, positive=[], negative=[], topn=10):\n        \"\"\"\n        Find the top-N most similar words.\n        \"\"\"\n        result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k not in self.labels]\n        return result[:topn]\n\n    def most_similar_vocab(self, positive=[], negative=[], vocab=[], topn=10, cosmul=False):\n        \"\"\"\n        Find the top-N most similar words in vocab list.\n        \"\"\"\n        if cosmul:\n            result = self.most_similar_cosmul(positive=positive, negative=negative, topn=len(self.vocab))\n        else:\n            result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k in vocab]\n        return result[:topn]\n\nclass LabeledBrownCorpus(object):\n    \"\"\"Iterate over sentences from the Brown corpus (part of NLTK data), yielding\n    each sentence out as a LabeledSentence object.\"\"\"\n    def __init__(self, dirname):\n        self.dirname = dirname\n\n    def __iter__(self):\n        for fname in os.listdir(self.dirname):\n            fname = os.path.join(self.dirname, fname)\n            if not os.path.isfile(fname):\n                continue\n            for item_no, line in enumerate(utils.smart_open(fname)):\n                line = utils.to_unicode(line)\n                # each file line is a single sentence in the Brown corpus\n                # each token is WORD/POS_TAG\n                token_tags = [t.split('/') for t in line.split() if len(t.split('/')) == 2]\n                # ignore words with non-alphabetic tags like \",\", \"!\" etc (punctuation, weird stuff)\n                words = [\"%s/%s\" % (token.lower(), tag[:2]) for token, tag in token_tags if tag[:2].isalpha()]\n                if not words:  # don't bother sending out empty sentences\n                    continue\n                yield LabeledSentence(words, ['%s_SENT_%s' % (fname, item_no)])\n\n\nclass LabeledLineSentence(object):\n    \"\"\"Simple format: one sentence = one line = one LabeledSentence object.\n\n    Words are expected to be already preprocessed and separated by whitespace,\n    labels are constructed automatically from the sentence line number.\"\"\"\n    def __init__(self, source):\n        \"\"\"\n        `source` can be either a string (filename) or a file object.\n\n        Example::\n\n            sentences = LineSentence('myfile.txt')\n\n        Or for compressed files::\n\n            sentences = LineSentence('compressed_text.txt.bz2')\n            sentences = LineSentence('compressed_text.txt.gz')\n\n        \"\"\"\n        self.source = source\n\n    def __iter__(self):\n        \"\"\"Iterate through the lines in the source.\"\"\"\n        try:\n            # Assume it is a file-like object and try treating it as such\n            # Things that don't have seek will trigger an exception\n            self.source.seek(0)\n            for item_no, line in enumerate(self.source):\n                yield LabeledSentence(utils.to_unicode(line).split(), ['SENT_%s' % item_no])\n        except AttributeError:\n            # If it didn't work like a file, use it as a string filename\n            with utils.smart_open(self.source) as fin:\n                for item_no, line in enumerate(fin):\n                    yield LabeledSentence(utils.to_unicode(line).split(), ['SENT_%s' % item_no])\n\nclass LabeledListSentence(object):\n    \"\"\"one sentence = list of words\n\n    labels are constructed automatically from the sentence line number.\"\"\"\n    def __init__(self, words_list, labels):\n        \"\"\"\n        words_list like:\n\n            words_list = [\n                ['human', 'interface', 'computer'],\n                ['survey', 'user', 'computer', 'system', 'response', 'time'],\n                ['eps', 'user', 'interface', 'system'],\n            ]\n            sentence = LabeledListSentence(words_list)\n\n        \"\"\"\n        self.words_list = words_list\n        self.labels = labels\n\n    def __iter__(self):\n        for i, words in enumerate(self.words_list):\n            yield LabeledSentence(words, ['SENT_%s' % self.labels[i]])\n\n\n\n\n\u25a0 wikipedia\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30b3\u30fc\u30d1\u30b9\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u203b\u3053\u3053\u306f\u7701\u3044\u3066\u3082\u52d5\u304d\u307e\u3059\u3002\nwget http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-  articles.xml.bz2\n#\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u306b10\u5206\u304f\u3089\u3044\u304b\u304b\u308b\u304b\u3082\npython path/to/wikicorpus.py path/to/jawiki-latest-pages-articles.xml.bz2 path/to/jawiki\n#8\u6642\u9593\u304f\u3089\u3044\u304b\u304b\u308b\u304b\u3082\n\n\n\u5b9f\u88c5(\u5b9f\u8df5)\n\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307e\u305b\u3066\u3001\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u3092\u3057\u3066\u307f\u308b\u3002\n\u4eca\u56de\u306f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(docs)\u3068\u305d\u306e\u30bf\u30a4\u30c8\u30eb(titles)\u3092\u8aad\u307f\u8fbc\u307e\u305b\u3066\u3001docs\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3066\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\nmain.py\nimport gensim\nimport mysql.connector\n\n#\u5b9a\u7fa9\nprevious_title = \"\"\ndocs = []\ntitles = []\n\n#MySQL\u306b\u63a5\u7d9a\nconfig = {\n  'user': \"USERNAME\",\n  'password': 'PASSWORD',\n  'host': 'HOST',\n  'database': 'DATABASE',\n  'port': 'PORT'\n}\nconnect = mysql.connector.connect(**config)\n#Query\u3092\u5b9f\u884c\u3059\u308b\ncur=connect.cursor(buffered=True)\n\nQUERY = \"select d.title,d.body from docs as d order by doc.id\" #\u3053\u3053\u306f\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3057\u3066\u304f\u3060\u3055\u3044\ncur.execute(QUERY)\nrows = cur.fetchall()\n\n#Query\u306e\u51fa\u529b\u7d50\u679c\u3092for\u3067\u56de\u3057\u3066sentences\u3068labels\u3092\u4f5c\u6210\ni = 0\nfor row in rows:\n  if previous_title != row[0]:\n    previous_title = row[0]\n    titles.append(row[0])\n    docs.append([])\n    i+=1\n  docs[i-1].append(row[1])\n\ncur.close()\nconnect.close()\n\n\"\"\"\n\u4e0a\u3067\u4f5c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306f\u8981\u3059\u308b\u306b\u3053\u3046\u3044\u3046\u30c7\u30fc\u30bf\u3067\u3059\u3002\ndocs = [\n    ['human', 'interface', 'computer'], #0\n    ['survey', 'user', 'computer', 'system', 'response', 'time'], #1\n    ['eps', 'user', 'interface', 'system'], #2\n    ['system', 'human', 'system', 'eps'], #3\n    ['user', 'response', 'time'], #4\n    ['trees'], #5\n    ['graph', 'trees'], #6\n    ['graph', 'minors', 'trees'], #7\n    ['graph', 'minors', 'survey'] #8\n]\n\ntitles = [\n    \"doc1\",\n    \"doc2\",\n    \"doc3\",\n    \"doc4\",\n    \"doc5\",\n    \"doc6\",\n    \"doc7\",\n    \"doc8\",\n    \"doc9\"\n]\n\"\"\"\n\nlabeledSentences = gensim.models.doc2vec.LabeledListSentence(docs,titles)\nmodel = gensim.models.doc2vec.Doc2Vec(labeledSentences, min_count=0)\n\n# \u3042\u308b\u6587\u66f8\u306b\u4f3c\u3066\u3044\u308b\u6587\u66f8\u3092\u8868\u793a\nprint model.most_similar_labels('SENT_doc1')\n\n# \u3042\u308b\u6587\u66f8\u306b\u4f3c\u3066\u3044\u308b\u5358\u8a9e\u3092\u8868\u793a\nprint model.most_similar_words('SENT_doc1')\n\n# \u8907\u6570\u306e\u6587\u66f8\u3092\u52a0\u7b97\u6e1b\u7b97\u3057\u305f\u4e0a\u3067\u3001\u4f3c\u3066\u3044\u308b\u30e6\u30fc\u30b6\u30fc\u3092\u8868\u793a\nprint model.most_similar_labels(positive=['SENT_doc1', 'SENT_doc2'], negative=['SENT_doc3'], topn=5)\n\n# \u8907\u6570\u306e\u6587\u66f8\u3092\u52a0\u7b97\u6e1b\u7b97\u3057\u305f\u4e0a\u3067\u3001\u4f3c\u3066\u3044\u308b\u5358\u8a9e\u3092\u8868\u793a\nprint model.most_similar_words(positive=['SENT_doc1', 'SENT_doc2'], negative=['SENT_doc3'], topn=5)\n\n\n\n\n\u7c73google\u306e\u7814\u7a76\u8005\u304c\u958b\u767a\u3057\u305f\u300c __Word2Vec__ \u300d\u3068\u3044\u3046\u6280\u8853\u3092\u30d9\u30fc\u30b9\u306b\u3001\u300c\u5358\u8a9e\u300d\u3060\u3051\u3067\u306f\u306a\u304f\u300c\u6587\u66f8\u300d\u306b\u3082\u610f\u5473\u3092\u6301\u305f\u305b\u3066\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6349\u3048\u3066\u5229\u7528\u3067\u304d\u308b\u6280\u8853\u300c __Doc2Vec__ \u300d\u3092\u3044\u3058\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n## Word2Vec\u306e\u304a\u3055\u3089\u3044\n\u904e\u53bbQiita\u306b\u6295\u7a3f\u3057\u305f\u306e\u3067\u3001\u305d\u306e\u30ea\u30f3\u30af\u3092\u5f35\u3063\u3066\u304a\u304d\u307e\u3059\u3002\nhttp://qiita.com/okappy/items/e16639178ba85edfee72\n\n## Doc2Vec\u3068\u306f\uff1f\nWord2Vec\u306fWord(\u5358\u8a9e)\u3092\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u6349\u3048\u308b\u304c\u3001Doc2Vec(Paragraph2Vec)\u306fDocument(\u6587\u66f8)\u3092Word\u306e\u96c6\u5408\u3068\u3057\u3066\u898b\u3066\u30d9\u30af\u30c8\u30eb\u3092\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u3067\u3001\u6587\u66f8\u9593\u306e\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u306a\u3069\u3092\u5b9f\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\n\u4f8b\u3048\u3070\u3001\u30cb\u30e5\u30fc\u30b9\u8a18\u4e8b\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u30ec\u30b8\u30e5\u30e1\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u672c\u540c\u58eb\u306e\u985e\u4f3c\u5ea6\u3001\u3082\u3061\u308d\u3093\u4eba\u306e\u30d7\u30ed\u30d5\u30a3\u30fc\u30eb\u3068\u672c\u306e\u985e\u4f3c\u5ea6\u306a\u3069\u3082\u7b97\u51fa\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3001\u30c6\u30ad\u30b9\u30c8\u3067\u8868\u3055\u308c\u3066\u8005\u540c\u58eb\u3067\u3042\u308c\u3070\u3001\u5168\u3066\u304c\u5bfe\u8c61\u3068\u306a\u308b\u3002\n\n## \u6280\u8853\u7684\u306b\u306f\n- python\n  - Scipy\n  - gensim \n\n\u3042\u305f\u308a\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n## gensim\u3068\u306f\uff1f\nPython\u304b\u3089\u6271\u3048\u308b\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\n\u6a5f\u80fd\u3068\u3057\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u6319\u3052\u3089\u308c\u308b\u3002\n\n- \u6f5c\u5728\u610f\u5473\u89e3\u6790\uff08LSA/LSI/SVD\uff09\n- \u6f5c\u5728\u30c7\u30a3\u30ea\u30af\u30ec\u914d\u5206\u6cd5\uff08LDA\uff09\n- TF-IDF\n- Random Projection\uff08RP\uff09\n- \u968e\u5c64\u7684\u30c7\u30a3\u30ea\u30af\u30ec\u904e\u7a0b\uff08HDP\uff09\n- \u6df1\u5c64\u5b66\u7fd2\u3092\u7528\u3044\u305fword2vec\n- \u5206\u6563\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\n- Dynamic Topic Model\uff08DTM\uff09\n- Dynamic Influence Models\uff08DIM\uff09\n\ngensim\u306e\u516c\u5f0f\u30da\u30fc\u30b8\nhttp://radimrehurek.com/gensim/\n\n\n## \u5b9f\u969b\u306b\u6587\u66f8\u9593\u306e\u985e\u4f3c\u5ea6\u3092\u51fa\u3057\u3066\u307f\u308b\n\u4eca\u56de\u306f\u3001facebook\u306e\u30c7\u30fc\u30bf\u3092\u5229\u7528\u3057\u3066\u3001\u3042\u308b\u30e6\u30fc\u30b6\u30fc\u304c\u904e\u53bbfacebook\u306b\u6295\u7a3f\u3057\u305f\u30c6\u30ad\u30b9\u30c8\u3084\u30b7\u30a7\u30a2\u3057\u305f\u30ea\u30f3\u30af\u306e\u30bf\u30a4\u30c8\u30eb\u306a\u3069\u3092\u4e00\u3064\u306e\u6587\u66f8\u3068\u898b\u7acb\u3066\u3066\u3001\u305d\u306e\u6587\u66f8\u540c\u58eb(\u8981\u3059\u308b\u306b\u30e6\u30fc\u30b6\u30fc\u540c\u58eb)\u306e\u985e\u4f3c\u5ea6\u3092\u51fa\u3057\u3066\u307f\u308b\u3002\n\n## \u5b9f\u88c5(\u6e96\u5099)\n\n### \u25a0 Scipy\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n```console\npip install scipy\n```\n  \n\n### \u25a0 gensim\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n```console\npip install gensim\n```\n### \u25a0 doc2vec.py\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\n__\u5909\u66f4\u70b9\u2460__\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306edoc2vec.py\u3060\u3068\u3001\u30ec\u30b9\u30dd\u30f3\u30b9\u306e\u3068\u304d\u306elabel\u304c\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3001\n\u8a2d\u5b9a\u3057\u305flabel\u3067\u7d50\u679c\u3092\u547c\u3073\u51fa\u305b\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n__\u5909\u66f4\u70b9\u2461__\ndoc2vec.py\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001\u6587\u66f8\u306e\u4f3c\u3066\u3044\u308b\u3082\u306e\u306f\uff1f\u3063\u3066\u53e9\u304f\u3068\u3001\u6587\u66f8\u3082\u5358\u8a9e\u3082\u51fa\u529b\u3055\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u6587\u66f8\u306e\u4f3c\u3066\u3044\u308b\u6587\u66f8\u3060\u3051\u3092\u51fa\u529b\u3059\u308b\u30e1\u30bd\u30c3\u30c9\u3082\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\n\n```doc2vec.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013 Radim Rehurek <me@radimrehurek.com>\n# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n\n\n\"\"\"\nDeep learning via the distributed memory and distributed bag of words models from\n[1]_, using either hierarchical softmax or negative sampling [2]_ [3]_.\n\n**Make sure you have a C compiler before installing gensim, to use optimized (compiled)\ndoc2vec training** (70x speedup [blog]_).\n\nInitialize a model with e.g.::\n\n>>> model = Doc2Vec(sentences, size=100, window=8, min_count=5, workers=4)\n\nPersist a model to disk with::\n\n>>> model.save(fname)\n>>> model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n\nThe model can also be instantiated from an existing file on disk in the word2vec C format::\n\n  >>> model = Doc2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False)  # C text format\n  >>> model = Doc2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)  # C binary format\n\n.. [1] Quoc Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. http://arxiv.org/pdf/1405.4053v2.pdf\n.. [2] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n.. [3] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality.\n       In Proceedings of NIPS, 2013.\n.. [blog] Optimizing word2vec in gensim, http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/\n\n\"\"\"\n\nimport logging\nimport os\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom numpy import zeros, random, sum as np_sum\n\nlogger = logging.getLogger(__name__)\n\nfrom gensim import utils  # utility fnc for pickling, common scipy operations etc\nfrom gensim.models.word2vec import Word2Vec, Vocab, train_cbow_pair, train_sg_pair\n\ntry:\n    from gensim.models.doc2vec_inner import train_sentence_dbow, train_sentence_dm, FAST_VERSION\nexcept:\n    # failed... fall back to plain numpy (20-80x slower training than the above)\n    FAST_VERSION = -1\n\n    def train_sentence_dbow(model, sentence, lbls, alpha, work=None, train_words=True, train_lbls=True):\n        \"\"\"\n        Update distributed bag of words model by training on a single sentence.\n\n        The sentence is a list of Vocab objects (or None, where the corresponding\n        word is not in the vocabulary. Called internally from `Doc2Vec.train()`.\n\n        This is the non-optimized, Python version. If you have cython installed, gensim\n        will use the optimized version from doc2vec_inner instead.\n\n        \"\"\"\n        neg_labels = []\n        if model.negative:\n            # precompute negative labels\n            neg_labels = zeros(model.negative + 1)\n            neg_labels[0] = 1.0\n\n        for label in lbls:\n            if label is None:\n                continue  # OOV word in the input sentence => skip\n            for word in sentence:\n                if word is None:\n                    continue  # OOV word in the input sentence => skip\n                train_sg_pair(model, word, label, alpha, neg_labels, train_words, train_lbls)\n\n        return len([word for word in sentence if word is not None])\n\n    def train_sentence_dm(model, sentence, lbls, alpha, work=None, neu1=None, train_words=True, train_lbls=True):\n        \"\"\"\n        Update distributed memory model by training on a single sentence.\n\n        The sentence is a list of Vocab objects (or None, where the corresponding\n        word is not in the vocabulary. Called internally from `Doc2Vec.train()`.\n\n        This is the non-optimized, Python version. If you have a C compiler, gensim\n        will use the optimized version from doc2vec_inner instead.\n\n        \"\"\"\n        lbl_indices = [lbl.index for lbl in lbls if lbl is not None]\n        lbl_sum = np_sum(model.syn0[lbl_indices], axis=0)\n        lbl_len = len(lbl_indices)\n        neg_labels = []\n        if model.negative:\n            # precompute negative labels\n            neg_labels = zeros(model.negative + 1)\n            neg_labels[0] = 1.\n\n        for pos, word in enumerate(sentence):\n            if word is None:\n                continue  # OOV word in the input sentence => skip\n            reduced_window = random.randint(model.window)  # `b` in the original doc2vec code\n            start = max(0, pos - model.window + reduced_window)\n            window_pos = enumerate(sentence[start : pos + model.window + 1 - reduced_window], start)\n            word2_indices = [word2.index for pos2, word2 in window_pos if (word2 is not None and pos2 != pos)]\n            l1 = np_sum(model.syn0[word2_indices], axis=0) + lbl_sum  # 1 x layer1_size\n            if word2_indices and model.cbow_mean:\n                l1 /= (len(word2_indices) + lbl_len)\n            neu1e = train_cbow_pair(model, word, word2_indices, l1, alpha, neg_labels, train_words, train_words)\n            if train_lbls:\n                model.syn0[lbl_indices] += neu1e\n\n        return len([word for word in sentence if word is not None])\n\n\nclass LabeledSentence(object):\n    \"\"\"\n    A single labeled sentence = text item.\n    Replaces \"sentence as a list of words\" from Word2Vec.\n\n    \"\"\"\n    def __init__(self, words, labels):\n        \"\"\"\n        `words` is a list of tokens (unicode strings), `labels` a\n        list of text labels associated with this text.\n\n        \"\"\"\n        self.words = words\n        self.labels = labels\n\n    def __str__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.words, self.labels)\n\n\nclass Doc2Vec(Word2Vec):\n    \"\"\"Class for training, using and evaluating neural networks described in http://arxiv.org/pdf/1405.4053v2.pdf\"\"\"\n    def __init__(self, sentences=None, size=300, alpha=0.025, window=8, min_count=5,\n                 sample=0, seed=1, workers=1, min_alpha=0.0001, dm=1, hs=1, negative=0,\n                 dm_mean=0, train_words=True, train_lbls=True, **kwargs):\n        \"\"\"\n        Initialize the model from an iterable of `sentences`. Each sentence is a\n        LabeledSentence object that will be used for training.\n\n        The `sentences` iterable can be simply a list of LabeledSentence elements, but for larger corpora,\n        consider an iterable that streams the sentences directly from disk/network.\n\n        If you don't supply `sentences`, the model is left uninitialized -- use if\n        you plan to initialize it in some other way.\n\n        `dm` defines the training algorithm. By default (`dm=1`), distributed memory is used.\n        Otherwise, `dbow` is employed.\n\n        `size` is the dimensionality of the feature vectors.\n\n        `window` is the maximum distance between the current and predicted word within a sentence.\n\n        `alpha` is the initial learning rate (will linearly drop to zero as training progresses).\n\n        `seed` = for the random number generator.\n\n        `min_count` = ignore all words with total frequency lower than this.\n\n        `sample` = threshold for configuring which higher-frequency words are randomly downsampled;\n                default is 0 (off), useful value is 1e-5.\n\n        `workers` = use this many worker threads to train the model (=faster training with multicore machines).\n\n        `hs` = if 1 (default), hierarchical sampling will be used for model training (else set to 0).\n\n        `negative` = if > 0, negative sampling will be used, the int for negative\n        specifies how many \"noise words\" should be drawn (usually between 5-20).\n\n        `dm_mean` = if 0 (default), use the sum of the context word vectors. If 1, use the mean.\n        Only applies when dm is used.\n\n        \"\"\"\n        Word2Vec.__init__(self, size=size, alpha=alpha, window=window, min_count=min_count,\n                          sample=sample, seed=seed, workers=workers, min_alpha=min_alpha,\n                          sg=(1+dm) % 2, hs=hs, negative=negative, cbow_mean=dm_mean, **kwargs)\n        self.train_words = train_words\n        self.train_lbls = train_lbls\n        self.labels = set()\n        if sentences is not None:\n            self.build_vocab(sentences)\n            self.train(sentences)\n            self.build_labels(sentences)\n\n    @staticmethod\n    def _vocab_from(sentences):\n        sentence_no, vocab = -1, {}\n        total_words = 0\n        for sentence_no, sentence in enumerate(sentences):\n            if sentence_no % 10000 == 0:\n                logger.info(\"PROGRESS: at item #%i, processed %i words and %i word types\" %\n                            (sentence_no, total_words, len(vocab)))\n            sentence_length = len(sentence.words)\n            for label in sentence.labels:\n                total_words += 1\n                if label in vocab:\n                    vocab[label].count += sentence_length\n                else:\n                    vocab[label] = Vocab(count=sentence_length)\n            for word in sentence.words:\n                total_words += 1\n                if word in vocab:\n                    vocab[word].count += 1\n                else:\n                    vocab[word] = Vocab(count=1)\n        logger.info(\"collected %i word types from a corpus of %i words and %i items\" %\n                    (len(vocab), total_words, sentence_no + 1))\n        return vocab\n\n    def _prepare_sentences(self, sentences):\n        for sentence in sentences:\n            # avoid calling random_sample() where prob >= 1, to speed things up a little:\n            sampled = [self.vocab[word] for word in sentence.words\n                       if word in self.vocab and (self.vocab[word].sample_probability >= 1.0 or\n                                                  self.vocab[word].sample_probability >= random.random_sample())]\n            yield (sampled, [self.vocab[word] for word in sentence.labels if word in self.vocab])\n\n    def _get_job_words(self, alpha, work, job, neu1):\n        if self.sg:\n            return sum(train_sentence_dbow(self, sentence, lbls, alpha, work, self.train_words, self.train_lbls) for sentence, lbls in job)\n        else:\n            return sum(train_sentence_dm(self, sentence, lbls, alpha, work, neu1, self.train_words, self.train_lbls) for sentence, lbls in job)\n\n    def __str__(self):\n        return \"Doc2Vec(vocab=%s, size=%s, alpha=%s)\" % (len(self.index2word), self.layer1_size, self.alpha)\n\n    def save(self, *args, **kwargs):\n        kwargs['ignore'] = kwargs.get('ignore', ['syn0norm'])  # don't bother storing the cached normalized vectors\n        super(Doc2Vec, self).save(*args, **kwargs)\n\n    def build_labels(self, sentences):\n        self.labels |= self._labels_from(sentences)\n\n    @staticmethod\n    def _labels_from(sentences):\n        labels = set()\n        for sentence in sentences:\n            labels |= set(sentence.labels)\n        return labels\n\n    def most_similar_labels(self, positive=[], negative=[], topn=10):\n        \"\"\"\n        Find the top-N most similar labels.\n        \"\"\"\n        result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k in self.labels]\n        return result[:topn]\n\n    def most_similar_words(self, positive=[], negative=[], topn=10):\n        \"\"\"\n        Find the top-N most similar words.\n        \"\"\"\n        result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k not in self.labels]\n        return result[:topn]\n\n    def most_similar_vocab(self, positive=[], negative=[], vocab=[], topn=10, cosmul=False):\n        \"\"\"\n        Find the top-N most similar words in vocab list.\n        \"\"\"\n        if cosmul:\n            result = self.most_similar_cosmul(positive=positive, negative=negative, topn=len(self.vocab))\n        else:\n            result = self.most_similar(positive=positive, negative=negative, topn=len(self.vocab))\n        result = [(k, v) for (k, v) in result if k in vocab]\n        return result[:topn]\n\nclass LabeledBrownCorpus(object):\n    \"\"\"Iterate over sentences from the Brown corpus (part of NLTK data), yielding\n    each sentence out as a LabeledSentence object.\"\"\"\n    def __init__(self, dirname):\n        self.dirname = dirname\n\n    def __iter__(self):\n        for fname in os.listdir(self.dirname):\n            fname = os.path.join(self.dirname, fname)\n            if not os.path.isfile(fname):\n                continue\n            for item_no, line in enumerate(utils.smart_open(fname)):\n                line = utils.to_unicode(line)\n                # each file line is a single sentence in the Brown corpus\n                # each token is WORD/POS_TAG\n                token_tags = [t.split('/') for t in line.split() if len(t.split('/')) == 2]\n                # ignore words with non-alphabetic tags like \",\", \"!\" etc (punctuation, weird stuff)\n                words = [\"%s/%s\" % (token.lower(), tag[:2]) for token, tag in token_tags if tag[:2].isalpha()]\n                if not words:  # don't bother sending out empty sentences\n                    continue\n                yield LabeledSentence(words, ['%s_SENT_%s' % (fname, item_no)])\n\n\nclass LabeledLineSentence(object):\n    \"\"\"Simple format: one sentence = one line = one LabeledSentence object.\n\n    Words are expected to be already preprocessed and separated by whitespace,\n    labels are constructed automatically from the sentence line number.\"\"\"\n    def __init__(self, source):\n        \"\"\"\n        `source` can be either a string (filename) or a file object.\n\n        Example::\n\n            sentences = LineSentence('myfile.txt')\n\n        Or for compressed files::\n\n            sentences = LineSentence('compressed_text.txt.bz2')\n            sentences = LineSentence('compressed_text.txt.gz')\n\n        \"\"\"\n        self.source = source\n\n    def __iter__(self):\n        \"\"\"Iterate through the lines in the source.\"\"\"\n        try:\n            # Assume it is a file-like object and try treating it as such\n            # Things that don't have seek will trigger an exception\n            self.source.seek(0)\n            for item_no, line in enumerate(self.source):\n                yield LabeledSentence(utils.to_unicode(line).split(), ['SENT_%s' % item_no])\n        except AttributeError:\n            # If it didn't work like a file, use it as a string filename\n            with utils.smart_open(self.source) as fin:\n                for item_no, line in enumerate(fin):\n                    yield LabeledSentence(utils.to_unicode(line).split(), ['SENT_%s' % item_no])\n\nclass LabeledListSentence(object):\n    \"\"\"one sentence = list of words\n\n    labels are constructed automatically from the sentence line number.\"\"\"\n    def __init__(self, words_list, labels):\n        \"\"\"\n        words_list like:\n\n            words_list = [\n                ['human', 'interface', 'computer'],\n                ['survey', 'user', 'computer', 'system', 'response', 'time'],\n                ['eps', 'user', 'interface', 'system'],\n            ]\n            sentence = LabeledListSentence(words_list)\n\n        \"\"\"\n        self.words_list = words_list\n        self.labels = labels\n\n    def __iter__(self):\n        for i, words in enumerate(self.words_list):\n            yield LabeledSentence(words, ['SENT_%s' % self.labels[i]])\n\n```\n \n### \u25a0 wikipedia\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30b3\u30fc\u30d1\u30b9\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u203b\u3053\u3053\u306f\u7701\u3044\u3066\u3082\u52d5\u304d\u307e\u3059\u3002\n\n```console  \nwget http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-  articles.xml.bz2\n#\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u306b10\u5206\u304f\u3089\u3044\u304b\u304b\u308b\u304b\u3082\npython path/to/wikicorpus.py path/to/jawiki-latest-pages-articles.xml.bz2 path/to/jawiki\n#8\u6642\u9593\u304f\u3089\u3044\u304b\u304b\u308b\u304b\u3082\n```\n\n\n## \u5b9f\u88c5(\u5b9f\u8df5)\n\n\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307e\u305b\u3066\u3001\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u3092\u3057\u3066\u307f\u308b\u3002\n\u4eca\u56de\u306f\u3001\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(docs)\u3068\u305d\u306e\u30bf\u30a4\u30c8\u30eb(titles)\u3092\u8aad\u307f\u8fbc\u307e\u305b\u3066\u3001docs\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3066\u985e\u4f3c\u5ea6\u3084\u30d9\u30af\u30c8\u30eb\u8a08\u7b97\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n```\u001emain.py\nimport gensim\nimport mysql.connector\n\n#\u5b9a\u7fa9\nprevious_title = \"\"\ndocs = []\ntitles = []\n\n#MySQL\u306b\u63a5\u7d9a\nconfig = {\n  'user': \"USERNAME\",\n  'password': 'PASSWORD',\n  'host': 'HOST',\n  'database': 'DATABASE',\n  'port': 'PORT'\n}\nconnect = mysql.connector.connect(**config)\n#Query\u3092\u5b9f\u884c\u3059\u308b\ncur=connect.cursor(buffered=True)\n\nQUERY = \"select d.title,d.body from docs as d order by doc.id\" #\u3053\u3053\u306f\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3057\u3066\u304f\u3060\u3055\u3044\ncur.execute(QUERY)\nrows = cur.fetchall()\n\n#Query\u306e\u51fa\u529b\u7d50\u679c\u3092for\u3067\u56de\u3057\u3066sentences\u3068labels\u3092\u4f5c\u6210\ni = 0\nfor row in rows:\n  if previous_title != row[0]:\n  \tprevious_title = row[0]\n  \ttitles.append(row[0])\n  \tdocs.append([])\n  \ti+=1\n  docs[i-1].append(row[1])\n\ncur.close()\nconnect.close()\n\n\"\"\"\n\u4e0a\u3067\u4f5c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306f\u8981\u3059\u308b\u306b\u3053\u3046\u3044\u3046\u30c7\u30fc\u30bf\u3067\u3059\u3002\ndocs = [\n    ['human', 'interface', 'computer'], #0\n    ['survey', 'user', 'computer', 'system', 'response', 'time'], #1\n    ['eps', 'user', 'interface', 'system'], #2\n    ['system', 'human', 'system', 'eps'], #3\n    ['user', 'response', 'time'], #4\n    ['trees'], #5\n    ['graph', 'trees'], #6\n    ['graph', 'minors', 'trees'], #7\n    ['graph', 'minors', 'survey'] #8\n]\n\ntitles = [\n\t\"doc1\",\n\t\"doc2\",\n\t\"doc3\",\n\t\"doc4\",\n\t\"doc5\",\n\t\"doc6\",\n\t\"doc7\",\n\t\"doc8\",\n\t\"doc9\"\n]\n\"\"\"\n\nlabeledSentences = gensim.models.doc2vec.LabeledListSentence(docs,titles)\nmodel = gensim.models.doc2vec.Doc2Vec(labeledSentences, min_count=0)\n\n# \u3042\u308b\u6587\u66f8\u306b\u4f3c\u3066\u3044\u308b\u6587\u66f8\u3092\u8868\u793a\nprint model.most_similar_labels('SENT_doc1')\n\n# \u3042\u308b\u6587\u66f8\u306b\u4f3c\u3066\u3044\u308b\u5358\u8a9e\u3092\u8868\u793a\nprint model.most_similar_words('SENT_doc1')\n\n# \u8907\u6570\u306e\u6587\u66f8\u3092\u52a0\u7b97\u6e1b\u7b97\u3057\u305f\u4e0a\u3067\u3001\u4f3c\u3066\u3044\u308b\u30e6\u30fc\u30b6\u30fc\u3092\u8868\u793a\nprint model.most_similar_labels(positive=['SENT_doc1', 'SENT_doc2'], negative=['SENT_doc3'], topn=5)\n\n# \u8907\u6570\u306e\u6587\u66f8\u3092\u52a0\u7b97\u6e1b\u7b97\u3057\u305f\u4e0a\u3067\u3001\u4f3c\u3066\u3044\u308b\u5358\u8a9e\u3092\u8868\u793a\nprint model.most_similar_words(positive=['SENT_doc1', 'SENT_doc2'], negative=['SENT_doc3'], topn=5)\n\n```\n\n", "tags": ["Python", "doc2vec", "gensim", "word2vec", "\u6a5f\u68b0\u5b66\u7fd2"]}