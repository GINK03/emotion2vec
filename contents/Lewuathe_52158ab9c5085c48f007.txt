{"tags": ["hadoop", "MapReduce"], "context": " More than 1 year has passed since last update.MaxOSX\u3067\u81ea\u5206\u3067\u66f8\u3044\u305fMapReduce\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8d70\u3089\u305b\u308b\u3002\n\n\n\nOS\nHomeBrew\nJDK\nHadoop\n\n\n\n\nMacOSX 10.9.4\n0.9.5\n1.8.0\n2.4.1\n\n\n\n\nHadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nHadoop\u306fHomebrew\u3067\u5165\u308c\u305f\u3082\u306e\u3092\u4f7f\u3046\u3002\n$ brew install hadoop\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306f\u4e0b\u8a18\n$ cd /usr/local/Celler/hadoop/2.4.1\n$ bin/hadoop version\nHadoop 2.4.1\nSubversion http://svn.apache.org/repos/asf/hadoop/common -r 1604318\nCompiled by jenkins on 2014-06-21T05:43Z\nCompiled with protoc 2.5.0\nFrom source with checksum bb7ac0a3c73dc131f4844b873c74b630\nThis command was run using /usr/local/Cellar/hadoop/2.4.1/libexec/share/hadoop/common/hadoop-common-2.4.1.jar\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u5b8c\u4e86\u3002Stand Alone\u304b\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u306e\u8a2d\u5b9a\u306f\u3053\u3053\u3092\u53c2\u7167\u3002\n\n\u81ea\u4f5cMapReduce\u306e\u4f5c\u6210\nHadoop Tutorial\u306b\u3042\u308b\u901a\u308a\u306eWordCount\u3092\u5b9f\u88c5\u3002\nMapReduce2\u306fApache Commons\u3082\u5fc5\u8981\u306a\u3088\u3046\u3060\u3063\u305f\u306e\u3067\u4eca\u56de\u306fMapReduce1\u3067\u5b9f\u88c5\u3002\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context\n                    ) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n      }\n    }\n  }\n\n  public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n                       ) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, \"word count\");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n\n\n\u30b3\u30f3\u30d1\u30a4\u30eb\n\u4e0a\u8a18\u3067\u4f5c\u6210\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u3092\u4e0b\u8a18\u306b\u7f6e\u304f\u3002\n$ mkdir -p workspace/wordcount\n$ mv WordCount.java workspace/wordcount/\n\n\u30b3\u30f3\u30d1\u30a4\u30eb\u3092\u884c\u3046\n$ javac -classpath libexec/share/hadoop/common/hadoop-common-2.4.1.jar:libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.4.1.jar -d workspace/wordcount workspace/wordcount/WordCount.java\n\nHadoop1.x\u3067\u306fhadoop-core\u3068\u3044\u3046jar\u304c\u3042\u308b\u3089\u3057\u3044\u3051\u308c\u30692.x\u3060\u3068\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u3002\u3069\u3046\u3084\u3089hadoop-common\u3068hadoop-mapreduce-client-core\u3092classpath\u306b\u542b\u3081\u308c\u3070\u30b3\u30f3\u30d1\u30a4\u30eb\u3067\u304d\u308b\u3089\u3057\u3044\u3002\u3067\u3082\u3044\u3061\u3044\u3061\u4f55\u304c\u5fc5\u8981\u5224\u65ad\u3059\u308b\u306e\u306f\u3081\u3093\u3069\u304f\u3055\u3044\u306e\u3067hadoop\u306eclasspath\u3092\u4e0e\u3048\u3066\u3084\u308b\u3002\n$ javac -classpath `bin/hadoop classpath` -d workspace/wordcount workspace/wordcount/WordCount.java\n\njar\u306b\u56fa\u3081\u308b\u3002\n$ jar -cvf workspace/wordcount/wordcount.jar -C workspace/wordcount/ .\n\n\u3053\u308c\u3067\u5fc5\u8981\u306a\u5b9f\u884c\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u305f\u3002\u3053\u308c\u3092\u5b9f\u884c\u3057\u3066\u307f\u308b\u3002\n\nMapReduce\u306e\u5b9f\u884c\n\u4eca\u56de\u306f\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u3067YARN\u4e0a\u3067\u7a3c\u50cd\u3055\u305b\u308b\u3002\n\nconf\n\ncore-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://localhost:9000</value>\n  </property>\n</configuration>\n\n\n\nhdfs-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n   <property>\n     <name>dfs.replication</name>\n     <value>1</value>\n   </property>\n</configuration>\n\n\n\nmapred-site.xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n</configuration>\n\n\n\nyarn-site.xml\n<?xml version=\"1.0\"?>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n</configuration>\n\n\n\nHDFS\u306e\u8d77\u52d5\n$ sbin/start-dfs.sh\n\n\nYARN\u306e\u8d77\u52d5\n$ sbin/start-yarn.sh\n\n\n\u78ba\u8a8d\n\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\n$ jps\n4432 DataNode\n4342 NameNode\n4539 SecondaryNameNode\n4747 NodeManager\n4654 ResourceManager\n4878 Jps\n\n\n\u5165\u529b\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\n$ cat input/word1                                                                                                                                              \na b c d a a b a a a\n$ cat input/word2\na a b c c c\n\n\u3053\u308c\u3089\u3092\u96c6\u8a08\u3059\u308b\u305f\u3081HDFS\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3002\n$ bin/hadoop fs -mkdir /hadoop/input\n$ bin/hadoop fs -copyFromLocal input/word1 /hadoop/input/word1\n$ bin/hadoop fs -copyFromLocal input/word2 /hadoop/input/word2\n\n\n\u5b9f\u884c\n# \u4f5c\u6210\u3057\u305fjar\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u5b9f\u884c\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u540dWordCount\u3092\u6307\u5b9a\u3059\u308b\n$ bin/hadoop jar workspace/wordcount/wordcount.jar WordCount /hadoop/input /hadoop/output\n\noutput\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u96c6\u8a08\u7d50\u679c\u304c\u3067\u3066\u3044\u308b\u306f\u305a\u3002\nkaisasak% bin/hadoop fs -cat /hadoop/output/part-r-00000\na       8\nb       3\nc       4\nd       1\n\nMaxOSX\u3067\u81ea\u5206\u3067\u66f8\u3044\u305fMapReduce\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8d70\u3089\u305b\u308b\u3002\n\n| OS  | HomeBrew | JDK | Hadoop \n|:---|:----|:----|:----|\n| MacOSX 10.9.4  | 0.9.5 | 1.8.0 | 2.4.1 | \n\n## Hadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nHadoop\u306fHomebrew\u3067\u5165\u308c\u305f\u3082\u306e\u3092\u4f7f\u3046\u3002\n\n```\n$ brew install hadoop\n```\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u306f\u4e0b\u8a18\n\n```\n$ cd /usr/local/Celler/hadoop/2.4.1\n$ bin/hadoop version\nHadoop 2.4.1\nSubversion http://svn.apache.org/repos/asf/hadoop/common -r 1604318\nCompiled by jenkins on 2014-06-21T05:43Z\nCompiled with protoc 2.5.0\nFrom source with checksum bb7ac0a3c73dc131f4844b873c74b630\nThis command was run using /usr/local/Cellar/hadoop/2.4.1/libexec/share/hadoop/common/hadoop-common-2.4.1.jar\n```\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u5b8c\u4e86\u3002Stand Alone\u304b\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u306e\u8a2d\u5b9a\u306f[\u3053\u3053](http://qiita.com/ysk_1031/items/26752b5da1629c9db8f7)\u3092\u53c2\u7167\u3002\n\n## \u81ea\u4f5cMapReduce\u306e\u4f5c\u6210\n[Hadoop Tutorial](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0)\u306b\u3042\u308b\u901a\u308a\u306eWordCount\u3092\u5b9f\u88c5\u3002\nMapReduce2\u306fApache Commons\u3082\u5fc5\u8981\u306a\u3088\u3046\u3060\u3063\u305f\u306e\u3067\u4eca\u56de\u306fMapReduce1\u3067\u5b9f\u88c5\u3002\n\n```java\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n  public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context\n                    ) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n      }\n    }\n  }\n\n  public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n                       ) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, \"word count\");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n```\n\n## \u30b3\u30f3\u30d1\u30a4\u30eb\n\u4e0a\u8a18\u3067\u4f5c\u6210\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u3092\u4e0b\u8a18\u306b\u7f6e\u304f\u3002\n\n```\n$ mkdir -p workspace/wordcount\n$ mv WordCount.java workspace/wordcount/\n```\n\n\u30b3\u30f3\u30d1\u30a4\u30eb\u3092\u884c\u3046\n\n```\n$ javac -classpath libexec/share/hadoop/common/hadoop-common-2.4.1.jar:libexec/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.4.1.jar -d workspace/wordcount workspace/wordcount/WordCount.java\n```\n\nHadoop1.x\u3067\u306fhadoop-core\u3068\u3044\u3046jar\u304c\u3042\u308b\u3089\u3057\u3044\u3051\u308c\u30692.x\u3060\u3068\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u3002\u3069\u3046\u3084\u3089hadoop-common\u3068hadoop-mapreduce-client-core\u3092classpath\u306b\u542b\u3081\u308c\u3070\u30b3\u30f3\u30d1\u30a4\u30eb\u3067\u304d\u308b\u3089\u3057\u3044\u3002\u3067\u3082\u3044\u3061\u3044\u3061\u4f55\u304c\u5fc5\u8981\u5224\u65ad\u3059\u308b\u306e\u306f\u3081\u3093\u3069\u304f\u3055\u3044\u306e\u3067hadoop\u306eclasspath\u3092\u4e0e\u3048\u3066\u3084\u308b\u3002\n\n```\n$ javac -classpath `bin/hadoop classpath` -d workspace/wordcount workspace/wordcount/WordCount.java\n```\n\njar\u306b\u56fa\u3081\u308b\u3002\n\n```\n$ jar -cvf workspace/wordcount/wordcount.jar -C workspace/wordcount/ .\n```\n\n\u3053\u308c\u3067\u5fc5\u8981\u306a\u5b9f\u884c\u30d5\u30a1\u30a4\u30eb\u304c\u3067\u304d\u305f\u3002\u3053\u308c\u3092\u5b9f\u884c\u3057\u3066\u307f\u308b\u3002\n\n## MapReduce\u306e\u5b9f\u884c\n\u4eca\u56de\u306f\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u3067YARN\u4e0a\u3067\u7a3c\u50cd\u3055\u305b\u308b\u3002\n\n### conf\n\n```core-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://localhost:9000</value>\n  </property>\n</configuration>\n```\n\n```hdfs-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n   <property>\n     <name>dfs.replication</name>\n     <value>1</value>\n   </property>\n</configuration>\n```\n\n```mapred-site.xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>mapreduce.framework.name</name>\n    <value>yarn</value>\n  </property>\n</configuration>\n```\n\n```yarn-site.xml\n<?xml version=\"1.0\"?>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n</configuration>\n```\n\n### HDFS\u306e\u8d77\u52d5\n\n```\n$ sbin/start-dfs.sh\n```\n\n### YARN\u306e\u8d77\u52d5\n\n```\n$ sbin/start-yarn.sh\n```\n\n### \u78ba\u8a8d\n\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\n\n```\n$ jps\n4432 DataNode\n4342 NameNode\n4539 SecondaryNameNode\n4747 NodeManager\n4654 ResourceManager\n4878 Jps\n```\n\n### \u5165\u529b\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\n\n```\n$ cat input/word1                                                                                                                                              \na b c d a a b a a a\n$ cat input/word2\na a b c c c\n```\n\n\u3053\u308c\u3089\u3092\u96c6\u8a08\u3059\u308b\u305f\u3081HDFS\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3002\n\n```\n$ bin/hadoop fs -mkdir /hadoop/input\n$ bin/hadoop fs -copyFromLocal input/word1 /hadoop/input/word1\n$ bin/hadoop fs -copyFromLocal input/word2 /hadoop/input/word2\n```\n\n### \u5b9f\u884c\n\n```\n# \u4f5c\u6210\u3057\u305fjar\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u5b9f\u884c\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u540dWordCount\u3092\u6307\u5b9a\u3059\u308b\n$ bin/hadoop jar workspace/wordcount/wordcount.jar WordCount /hadoop/input /hadoop/output\n```\n\noutput\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u96c6\u8a08\u7d50\u679c\u304c\u3067\u3066\u3044\u308b\u306f\u305a\u3002\n\n```\nkaisasak% bin/hadoop fs -cat /hadoop/output/part-r-00000\na       8\nb       3\nc       4\nd       1\n```\n\n"}