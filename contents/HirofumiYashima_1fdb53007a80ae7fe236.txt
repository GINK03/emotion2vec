{"context": "\u982d\u306e\u6574\u7406\u306e\u305f\u3081\u3001\u4eba\u5de5\u77e5\u80fd \u3092 \u642d\u8f09\u3057\u305f\u9759\u6b62\u753b\u30fb\u97f3\u58f0\u30fb\u52d5\u753b\uff08\u884c\u52d5\u5185\u5bb9\u89e3\u6790\uff06\u4e88\u6e2c\uff09\u89e3\u6790\u3001\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1 \u306e \u53ef\u80fd\u6027 \u3092 \u8003\u3048\u308b\u4e0a\u3067\u3001\u3044\u307e\u6ce8\u76ee\u3057\u3066\u3044\u308b\u8ad6\u6587 \u3092  \u5217\u6319 \u3057\u307e\u3059\u3002\n\n\uff08 \u6ce8\u76ee\u3057\u3066\u3044\u308b \uff11\uff15\u672c \u306e \u8ad6\u6587 \uff09\n\n\n\uff11.\uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09genetic-fuzzy, Genetic Fuzzy Tree ( GFT )\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\n\uff12.\uff08DNN\u69cb\u6210 \u81ea\u5f8b\u5b66\u7fd2\uff06\u81ea\u5f8b\u66f4\u65b0\uff09NeuroEvolution Evolutionary Artificial Neural Network, EANN\n\uff13.\uff08\u5de5\u5b66\u6570\u7406\u30e2\u30c7\u30eb NeuralNet\u69cb\u6210 \u81ea\u5f8b\u5b66\u7fd2\uff09GMDH Group method of data handling\n\uff14. \uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09 \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc ( Deep Kalman-Filter \uff09\n\uff15. \uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09Self-Organizing Incremental Neural Network SOINN\n\uff16. \uff08\u9759\u6b62\u753b\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\uff09Topological Data Analysis ( TDA )\n\uff17. \uff08\u9759\u6b62\u753b\uff09DCGAN\n\uff18. \uff08\u97f3\u58f0\uff09WaveNet\n\uff19. \uff08\u52d5\u753b\uff09\u52d5\u753b\u7248GAN\n\uff11\uff10. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Pixel Recurrent Neural Networks\n\uff11\uff11. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Decoupled Neural Interfaces using Synthetic Gradients\n\uff11\uff12. \uff08\u591a\u6b21\u5143\u7acb\u4f53 DNN\uff09GRID LONG SHORT-TERM MEMORY\n\uff11\uff13. \uff08\u534a\u6559\u5e2b\u3042\u308a \u6df1\u5c64\u5b66\u7fd2\uff09Ladder Network\n\uff11\uff14. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Turing Machines\n\uff11\uff15. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Random Access Machines\n\n\n\uff08 \u5404\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \uff09\n\n\n\uff11.  genetic-fuzzy, Genetic Fuzzy Tree ( GFT )\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\n\nNicholas Ernest, Kelly Cohen et.al, Genetic Fuzzy Trees and their Application Towards Autonomous Training and Control of a Squadron of Unmanned Combat Aerial Vehicles\nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/10\uff09\u300c\u3010 \u8abf\u67fb\u30e1\u30e2 \u3011\u5148\u7aefAI\u8a2d\u8a08 \u306b \u304a\u3051\u308b\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u306e \u6709\u7528\u6027 \uff5e RaspberryPi\u4e0a\u3067\u52d5\u4f5c\u53ef\u80fd \u306a \u8efd\u91cf \u7121\u4eba\u6226\u95d8\u6a5f(UCAVs) \u5236\u5fa1\u30d7\u30ed\u30b0\u30e9\u30e0 \"ALPHA\"\uff08\u7c73\u56fd Psibernetix\u793e\uff09\u304c \u793a\u3059 \u305d\u306e\u53ef\u80fd\u6027\u300d\n\n\n\n\uff12. NeuroEvolution Evolutionary Artificial Neural Network, EANN\n\n\nPhillip Verbancsics & Josh Harness, Generative NeuroEvolution for Deep Learning\nKenneth O. Stanley and Risto Miikkulainen, Evolving Neural Networks through Augmenting Topologies\nHatena Blog axxasusi's blog \uff082014-06-11\uff09\u300c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u6700\u9069\u5316\u3059\u308b\u300d\nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/13\uff09\u300cNeuroEvolution\u30e2\u30c7\u30eb \u301c \u6df1\u5c64\u5b66\u7fd2 \u3068 \u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u306e \u878d\u5408\u30e2\u30c7\u30eb \u306b\u3064\u3044\u3066\u300d\n\n\n\n\uff13. GMDH Group method of data handling\n\nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/20) \u300c\u3010 \u6bd4\u8f03 \u3011GMDH\u30e2\u30c7\u30eb \u3068 NeuroEvolution\u30e2\u30c7\u30eb \uff5e \uff08\u6df1\u5c64 & \u6d45\u5c64\uff09\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020 \u3092 \u9032\u5316\u8a08\u7b97\uff08\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff09 \u3067 \u81ea\u52d5\u63a2\u7d22 \u3059\u308b \u30a2\u30d7\u30ed\u30fc\u30c1\u300d\n\n\n\n\uff14. \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc ( Deep Kalman-Filter \uff09\n\nRahul G. Krishnan, Uri Shalit and David Sontag, Deep Kalman Filters\nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/02\uff09\u300c\u3010 \u6bd4\u8f03 \u3011 \u6df1\u5c64\u5f37\u5316\u5b66\u7fd2 \u3068 \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc\u300d\n\n\n\n\uff15. Self-Organizing Incremental Neural Network SOINN\n\n\nShen Furao Under the supervision of Dr. Osamu Hasegawa, An Algorithm for Incremental Unsupervised Learning and Topology Representation \u300c\u8010\u30ce\u30a4\u30ba\u6027\u3092\u6709\u3057\u6559\u5e2b\u306a\u3057\u8ffd\u52a0\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u30fb\u4f4d\u76f8\u5b66\u7fd2\u304c\u53ef\u80fd\u306a\u81ea\u5df1\u5897\u6b96\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u95a2\u3059\u308b\u7814\u7a76\u300d\n\uff08 ELSEVIER \uff09Shen Furaoa, Tomotaka Ogurab, Osamu Hasegawab, An enhanced self-organizing incremental neural network for online unsupervised learning\nHirofumiYashima Qiita\u8a18\u4e8b\uff082015/09/15\uff09\u300e\u3010 \u81ea\u5f8b\u5b66\u7fd2\u578b\u4eba\u5de5\u77e5\u80fd \u4e8b\u4f8b\u8abf\u67fb \u3011 \u6771\u5de5\u5927 \u9577\u8c37\u5ddd\u7814\u7a76\u5ba4 \u767a\u306e\u300cSOINN\u300d\uff08\u81ea\u5df1\u5897\u6b96\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u304c\u3059\u3054\u3044\u300f\n\n\n\n\uff16. Topological Data Analysis ( TDA )\n\n[HirofumiYashima Qiita\u8a18\u4e8b\uff082015/05/29\uff09\u300eDeep Learning \u306e\u6b21\u306f\u3001TDA \u300c\u30c8\u30dd\u30ed\u30b8\u30ab\u30eb\u30fb\u30c7\u30fc\u30bf\u30fb\u30a2\u30ca\u30ea\u30b7\u30b9\u300d (Topological data analysis) \u304c\u6765\u308b ? \uff5e \u305d\u306e\u6982\u8981\u3068\u3001R\u8a00\u8a9e / Python\u8a00\u8a9e \u5b9f\u88c5\u30e9\u30a4\u30d6\u30e9\u30ea \u3092\u3061\u3089\u3063\u3068\u8abf\u3079\u3066\u307f\u305f\u300f\n\n\n\n\n\uff17. \uff08\u9759\u6b62\u753b\uff09DCGAN\n\n\uff08 Arxiv\uff09Alec Radford et.al, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/07\uff09\u300c\u3010 \u8003\u5bdf \u3011GAN\uff08Generative Adversarial Network\uff09\u306e \u5b9f\u7528\u9014 \u3067\u306e \u300c\u4f7f\u3044\u3069\u3053\u308d\u300d \u3092 \u8003\u3048\u308b\u300d \nHirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/13\uff09\u300cCGAN\uff08 Deep Convolutional Generative adversarial networks \uff09\u306e \u30b5\u30fc\u30d3\u30b9\u4e8b\u696d\u5316 \u306e\u30d2\u30f3\u30c8 \u3092 \u63a2\u308b\u300d\n\n\n\n\uff18. \uff08\u97f3\u58f0\uff09WaveNet\n\n\uff08 Arxiv\uff09Aaron van den Oord et.al, WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\n\uff08 Google DeepMind\u793e \u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \uff09WaveNet: A Generative Model for Raw Audio\nITPro \uff082016/09/12\uff09\u300eGoogle\u306eAI\u90e8\u9580\u3001\u4eba\u9593\u306b\u8fd1\u3044\u5408\u6210\u97f3\u58f0\u300cWaveNet\u300d\u3092\u958b\u767a\u300f\n\n\n\n\uff19. \uff08\u52d5\u753b\uff09\u52d5\u753b\u7248GAN\n\nCarl Vondrick Hamed Pirsiavash Antonio Torralba, Generating Videos with Scene Dynamics\n\n\nAbstract\nWe capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction).\nWe propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. \nExperiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images.\nMoreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. \nWe believe generative video models can impact many applications in video understanding and simulation.\n\n\nHtena::Diary shi3z\u306e\u9577\u6587\u65e5\u8a18 \uff082016-09-09\uff09\u300c\u9759\u6b62\u753b\u3092\u898b\u305b\u308b\u3068\u52d5\u753b\u3092\u81ea\u52d5\u751f\u6210\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092MIT\u304c\u958b\u767a\u300d\nHirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300c\u3010 \u8ad6\u6587\u8aad\u307f \u3011\u9759\u6b62\u753b\u304b\u3089\u52d5\u753b\u3092\u81ea\u52d5\u751f\u6210 \uff5e \u52d5\u753b\u7248 GAN\u30e2\u30c7\u30eb\uff08Generative Adversarial Networks Model\uff09\u8ad6\u6587\u7fa4\u300d\n\n\n\n\uff11\uff10. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Pixel Recurrent Neural Networks\n\n\uff08 Arxiv\uff09Aaron van den Oord et.al, Pixel Recurrent Neural Networks\n\uff08 SlideShare \uff09Seiya Tokui \u300c\u8ad6\u6587\u7d39\u4ecb Pixel Recurrent Neural Networks\u300d\n\n\n\n\uff11\uff11. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Decoupled Neural Interfaces using Synthetic Gradients\n\nMax Jaderberg et.al, Decoupled Neural Interfaces using Synthetic Gradients\n\n\nTraining directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates.\nAll layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. \n_In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. _\nThese models predict what the result of the modelled subgraph will produce using only local information.\nIn particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. \nWe show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.\n\n\n\n\uff11\uff12. GRID LONG SHORT-TERM MEMORY \uff08\u591a\u6b21\u5143\u7acb\u4f53 \u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \uff09\n\n\uff08 Arxiv\uff09Nal Kalchbrenner & Ivo Danihelka & Alex Graves, GRID LONG SHORT-TERM MEMORY\n\n\nABSTRACT\nThis paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images.\nThe network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data.\nThe network provides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. \nWe then give results for two empirical tasks.\nWe find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches.\nIn addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.\n\n\n\n\uff11\uff13. Ladder Network\uff08\u534a\u6559\u5e2b\u3042\u308a \u6df1\u5c64\u5b66\u7fd2\uff09\n\nAntti Rasmus and Harri Valpola et.al, Semi-Supervised Learning with Ladder Networks\n\uff08 SlideShare \uff09Eiichi Matsumoto \u300cNIPS2015\u8aad\u307f\u4f1a: Ladder Networks\u300d\n\n\n\uff11\uff14. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Turing Machines\nHirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300cNeural Turing Machines \u3068 Neural Random Access Machines\u300d\n\n\uff11\uff15. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Random Access Machines\nHirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300cNeural Turing Machines \u3068 Neural Random Access Machines\u300d\n\n\u982d\u306e\u6574\u7406\u306e\u305f\u3081\u3001\u4eba\u5de5\u77e5\u80fd \u3092 \u642d\u8f09\u3057\u305f\u9759\u6b62\u753b\u30fb\u97f3\u58f0\u30fb\u52d5\u753b\uff08\u884c\u52d5\u5185\u5bb9\u89e3\u6790\uff06\u4e88\u6e2c\uff09\u89e3\u6790\u3001\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1 \u306e \u53ef\u80fd\u6027 \u3092 \u8003\u3048\u308b\u4e0a\u3067\u3001\u3044\u307e\u6ce8\u76ee\u3057\u3066\u3044\u308b\u8ad6\u6587 \u3092  \u5217\u6319 \u3057\u307e\u3059\u3002\n\n##__\uff08 \u6ce8\u76ee\u3057\u3066\u3044\u308b \uff11\uff15\u672c \u306e \u8ad6\u6587 \uff09__\n\n___\n\n\uff11.\uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09genetic-fuzzy, Genetic Fuzzy Tree ( _GFT_ )\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\n\n\uff12.\uff08DNN\u69cb\u6210 \u81ea\u5f8b\u5b66\u7fd2\uff06\u81ea\u5f8b\u66f4\u65b0\uff09NeuroEvolution Evolutionary Artificial Neural Network, _EANN_\n\n\uff13.\uff08\u5de5\u5b66\u6570\u7406\u30e2\u30c7\u30eb NeuralNet\u69cb\u6210 \u81ea\u5f8b\u5b66\u7fd2\uff09_GMDH_ Group method of data handling\n\n\uff14. \uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09 \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc ( Deep Kalman-Filter \uff09\n\n\uff15. \uff08\u30ed\u30dc\u30c6\u30a3\u30af\u30b9\u5236\u5fa1\uff09Self-Organizing Incremental Neural Network _SOINN_\n\n\uff16. \uff08\u9759\u6b62\u753b\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\uff09Topological Data Analysis ( _TDA_ )\n\n\uff17. \uff08\u9759\u6b62\u753b\uff09DCGAN\n\n\uff18. \uff08\u97f3\u58f0\uff09WaveNet\n\n\uff19. \uff08\u52d5\u753b\uff09\u52d5\u753b\u7248GAN\n\n\uff11\uff10. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Pixel Recurrent Neural Networks\n\n\uff11\uff11. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Decoupled Neural Interfaces using Synthetic Gradients\n\n\uff11\uff12. \uff08\u591a\u6b21\u5143\u7acb\u4f53 DNN\uff09GRID LONG SHORT-TERM MEMORY\n\n\uff11\uff13. \uff08\u534a\u6559\u5e2b\u3042\u308a \u6df1\u5c64\u5b66\u7fd2\uff09Ladder Network\n\n\uff11\uff14. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Turing Machines\n\n\uff11\uff15. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Random Access Machines\n___\n\n##__\uff08 \u5404\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \uff09__\n\n###\uff11.  genetic-fuzzy, Genetic Fuzzy Tree ( _GFT_ )\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\n\n* [Nicholas Ernest, Kelly Cohen _et.al, Genetic Fuzzy Trees and their Application Towards Autonomous Training and Control of a Squadron of Unmanned Combat Aerial Vehicles_](https://www.researchgate.net/publication/277020517_Genetic_Fuzzy_Trees_and_their_Application_Towards_Autonomous_Training_and_Control_of_a_Squadron_of_Unmanned_Combat_Aerial_Vehicles)\n\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/10\uff09\u300c\u3010 \u8abf\u67fb\u30e1\u30e2 \u3011\u5148\u7aefAI\u8a2d\u8a08 \u306b \u304a\u3051\u308b\u300c\u907a\u4f1d\u7684\u30d5\u30a1\u30b8\u30a3\u6c7a\u5b9a\u6728\u300d\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u306e \u6709\u7528\u6027 \uff5e RaspberryPi\u4e0a\u3067\u52d5\u4f5c\u53ef\u80fd \u306a \u8efd\u91cf \u7121\u4eba\u6226\u95d8\u6a5f(UCAVs) \u5236\u5fa1\u30d7\u30ed\u30b0\u30e9\u30e0 \"ALPHA\"\uff08\u7c73\u56fd Psibernetix\u793e\uff09\u304c \u793a\u3059 \u305d\u306e\u53ef\u80fd\u6027\u300d](http://qiita.com/HirofumiYashima/items/33b45cdbe59b55ca7e87)\n\n___\n\n###\uff12. NeuroEvolution Evolutionary Artificial Neural Network, _EANN_\n\n* [Phillip Verbancsics & Josh Harness, _Generative NeuroEvolution for Deep Learning_](https://arxiv.org/pdf/1312.5355.pdf)\n\n* [Kenneth O. Stanley and Risto Miikkulainen, _Evolving Neural Networks through Augmenting Topologies_](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)\n\n* [Hatena Blog axxasusi's blog \uff082014-06-11\uff09\u300c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u6700\u9069\u5316\u3059\u308b\u300d](http://axxasusi.hatenablog.com/entry/2014/06/11/201323)\n\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/13\uff09\u300cNeuroEvolution\u30e2\u30c7\u30eb \u301c \u6df1\u5c64\u5b66\u7fd2 \u3068 \u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u306e \u878d\u5408\u30e2\u30c7\u30eb \u306b\u3064\u3044\u3066\u300d](http://qiita.com/HirofumiYashima/items/12fd2d4761844ea29386)\n\n___\n\n###\uff13. _GMDH_ Group method of data handling\n\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/20) \u300c\u3010 \u6bd4\u8f03 \u3011GMDH\u30e2\u30c7\u30eb \u3068 NeuroEvolution\u30e2\u30c7\u30eb \uff5e \uff08\u6df1\u5c64 & \u6d45\u5c64\uff09\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020 \u3092 \u9032\u5316\u8a08\u7b97\uff08\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\uff09 \u3067 \u81ea\u52d5\u63a2\u7d22 \u3059\u308b \u30a2\u30d7\u30ed\u30fc\u30c1\u300d](http://qiita.com/HirofumiYashima/items/eb966252be54ee2cb2b1)\n\n___\n\n###\uff14. \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc ( Deep Kalman-Filter \uff09\n\n* [Rahul G. Krishnan, Uri Shalit and David Sontag, _Deep Kalman Filters_](http://approximateinference.org/accepted/KrishnanEtAl2015.pdf)\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/02\uff09\u300c\u3010 \u6bd4\u8f03 \u3011 \u6df1\u5c64\u5f37\u5316\u5b66\u7fd2 \u3068 \u6df1\u5c64\u30ab\u30eb\u30de\u30f3\u30d5\u30a3\u30eb\u30bf\u30fc\u300d](http://qiita.com/HirofumiYashima/items/392dd331d8f30593ffdb)\n\n\n___\n\n###\uff15. Self-Organizing Incremental Neural Network _SOINN_\n\n* [Shen Furao Under the supervision of Dr. Osamu Hasegawa, An Algorithm for Incremental Unsupervised Learning and Topology Representation \u300c\u8010\u30ce\u30a4\u30ba\u6027\u3092\u6709\u3057\u6559\u5e2b\u306a\u3057\u8ffd\u52a0\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u30fb\u4f4d\u76f8\u5b66\u7fd2\u304c\u53ef\u80fd\u306a\u81ea\u5df1\u5897\u6b96\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u95a2\u3059\u308b\u7814\u7a76\u300d](http://haselab.info/soinn/img/pdf/01_shen_doctoralThesis.pdf)\n* [\uff08 ELSEVIER \uff09Shen Furaoa, Tomotaka Ogurab, Osamu Hasegawab, An enhanced self-organizing incremental neural network for online unsupervised learning](http://www.sciencedirect.com/science/article/pii/S0893608007001190)\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082015/09/15\uff09\u300e\u3010 \u81ea\u5f8b\u5b66\u7fd2\u578b\u4eba\u5de5\u77e5\u80fd \u4e8b\u4f8b\u8abf\u67fb \u3011 \u6771\u5de5\u5927 \u9577\u8c37\u5ddd\u7814\u7a76\u5ba4 \u767a\u306e\u300cSOINN\u300d\uff08\u81ea\u5df1\u5897\u6b96\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09\u304c\u3059\u3054\u3044\u300f](http://qiita.com/HirofumiYashima/items/b6e69170d4167659136c)\n\n\n___\n\n\n###\uff16. Topological Data Analysis ( _TDA_ )\n\n* [[HirofumiYashima Qiita\u8a18\u4e8b\uff082015/05/29\uff09\u300eDeep Learning \u306e\u6b21\u306f\u3001TDA \u300c\u30c8\u30dd\u30ed\u30b8\u30ab\u30eb\u30fb\u30c7\u30fc\u30bf\u30fb\u30a2\u30ca\u30ea\u30b7\u30b9\u300d (Topological data analysis) \u304c\u6765\u308b ? \uff5e \u305d\u306e\u6982\u8981\u3068\u3001R\u8a00\u8a9e / Python\u8a00\u8a9e \u5b9f\u88c5\u30e9\u30a4\u30d6\u30e9\u30ea \u3092\u3061\u3089\u3063\u3068\u8abf\u3079\u3066\u307f\u305f\u300f](http://qiita.com/HirofumiYashima/items/b07483af7ef31c30dacc)\n\n___\n\n###\uff17. \uff08\u9759\u6b62\u753b\uff09DCGAN\n\n* [\uff08 Arxiv\uff09Alec Radford _et.al, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks_](https://arxiv.org/abs/1511.06434)\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/07\uff09\u300c\u3010 \u8003\u5bdf \u3011GAN\uff08Generative Adversarial Network\uff09\u306e \u5b9f\u7528\u9014 \u3067\u306e \u300c\u4f7f\u3044\u3069\u3053\u308d\u300d \u3092 \u8003\u3048\u308b\u300d ](http://qiita.com/HirofumiYashima/items/01089ad30b8a11a75e3f)\n* [HirofumiYashima Qiita\u8a18\u4e8b\uff082016/09/13\uff09\u300cCGAN\uff08 Deep Convolutional Generative adversarial networks \uff09\u306e \u30b5\u30fc\u30d3\u30b9\u4e8b\u696d\u5316 \u306e\u30d2\u30f3\u30c8 \u3092 \u63a2\u308b\u300d](http://qiita.com/HirofumiYashima/items/43e07a47fbab7142f119)\n\n___\n\n\n###\uff18. \uff08\u97f3\u58f0\uff09WaveNet\n\n* [\uff08 Arxiv\uff09Aaron van den Oord _et.al, WAVENET: A GENERATIVE MODEL FOR RAW AUDIO_](https://arxiv.org/pdf/1609.03499.pdf)\n\n* [\uff08 Google DeepMind\u793e \u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \uff09WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n\n* [ITPro \uff082016/09/12\uff09\u300eGoogle\u306eAI\u90e8\u9580\u3001\u4eba\u9593\u306b\u8fd1\u3044\u5408\u6210\u97f3\u58f0\u300cWaveNet\u300d\u3092\u958b\u767a\u300f](http://itpro.nikkeibp.co.jp/atcl/news/16/091202649/?rt=nocnt)\n\n___\n\n\n###\uff19. \uff08\u52d5\u753b\uff09\u52d5\u753b\u7248GAN\n\n* [Carl Vondrick Hamed Pirsiavash Antonio Torralba, _Generating Videos with Scene Dynamics_](http://web.mit.edu/vondrick/tinyvideo/)\n\n> __Abstract__\n>\n> We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction).\n>\n> We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. \n>\n> Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images.\n>\n> Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. \n>\n> We believe generative video models can impact many applications in video understanding and simulation.\n\n* [Htena::Diary shi3z\u306e\u9577\u6587\u65e5\u8a18 \uff082016-09-09\uff09\u300c\u9759\u6b62\u753b\u3092\u898b\u305b\u308b\u3068\u52d5\u753b\u3092\u81ea\u52d5\u751f\u6210\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092MIT\u304c\u958b\u767a\u300d](http://d.hatena.ne.jp/shi3z/20160909/1473363354)\n\n\n* [HirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300c\u3010 \u8ad6\u6587\u8aad\u307f \u3011\u9759\u6b62\u753b\u304b\u3089\u52d5\u753b\u3092\u81ea\u52d5\u751f\u6210 \uff5e \u52d5\u753b\u7248 GAN\u30e2\u30c7\u30eb\uff08Generative Adversarial Networks Model\uff09\u8ad6\u6587\u7fa4\u300d](http://qiita.com/HirofumiYashima/items/37d978bc98e1b31f2b11)\n\n\n___\n\n###\uff11\uff10. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Pixel Recurrent Neural Networks\n\n* [\uff08 Arxiv\uff09Aaron van den Oord _et.al, Pixel Recurrent Neural Networks_](https://arxiv.org/pdf/1601.06759v2.pdf)\n\n* [\uff08 SlideShare \uff09Seiya Tokui \u300c\u8ad6\u6587\u7d39\u4ecb Pixel Recurrent Neural Networks\u300d](http://www.slideshare.net/beam2d/pixel-recurrent-neural-networks)\n\n___\n\n###\uff11\uff11. \uff08DNN \u9ad8\u901f\u4e26\u5217 \u52fe\u914d\u8a08\u7b97\uff09Decoupled Neural Interfaces using Synthetic Gradients\n\n* [Max Jaderberg _et.al, Decoupled Neural Interfaces using Synthetic Gradients_](https://arxiv.org/abs/1608.05343)\n\n> Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates.\n> All layers, or more generally, modules, of the network are therefore __locked__, in the sense that __they must wait for the remainder of the network to execute forwards and propagate error backwards__ before they can be updated. \n>\n> __In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. __\n>\n> These models __predict what the result of the modelled subgraph will produce using only local information__.\n>\n>  In particular we focus on modelling error gradients: by using __the modelled synthetic gradient in place of true backpropagated error gradients__ we decouple __subgraphs__, and __can update them independently and asynchronously__ i.e. we realise decoupled neural interfaces. \n>\n> We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.\n\n___\n\n###\uff11\uff12. GRID LONG SHORT-TERM MEMORY \uff08\u591a\u6b21\u5143\u7acb\u4f53 \u6df1\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \uff09\n\n* [\uff08 Arxiv\uff09Nal Kalchbrenner & Ivo Danihelka & Alex Graves, _GRID LONG SHORT-TERM MEMORY_](https://arxiv.org/pdf/1507.01526v3.pdf)\n\n> __ABSTRACT__\n>\n> This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images.\n>\n> __The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data.__\n>\n> The network provides a unified way of using LSTM for __both deep and sequential__ computation.\n>\n> We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to __significantly outperform the standard LSTM.__ \n>\n> We then give results for two empirical tasks.\n>\n> We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches.\n>\n> In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.\n\n___\n\n###\uff11\uff13. Ladder Network\uff08\u534a\u6559\u5e2b\u3042\u308a \u6df1\u5c64\u5b66\u7fd2\uff09\n\n* [Antti Rasmus and Harri Valpola _et.al, Semi-Supervised Learning with Ladder Networks_](https://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf)\n\n* [\uff08 SlideShare \uff09Eiichi Matsumoto \u300cNIPS2015\u8aad\u307f\u4f1a: Ladder Networks\u300d](http://www.slideshare.net/eiichimatsumoto106/nips2015-ladder-network)\n\n___\n\n\uff11\uff14. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Turing Machines\n\n [HirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300cNeural Turing Machines \u3068 Neural Random Access Machines\u300d](http://qiita.com/HirofumiYashima/items/b49c63a8a0f0f5dcb6f4)\n\n___\n\n\uff11\uff15. \uff08\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u81ea\u52d5\u5b66\u7fd2\uff09Neural Random Access Machines\n\n [HirofumiYashima Qiita\u8a18\u4e8b \uff082016/09/20\uff09\u300cNeural Turing Machines \u3068 Neural Random Access Machines\u300d](http://qiita.com/HirofumiYashima/items/b49c63a8a0f0f5dcb6f4)\n", "tags": ["\u4eba\u5de5\u77e5\u80fd", "DeepLearning", "MachineLearning", "\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0", "IoT"]}