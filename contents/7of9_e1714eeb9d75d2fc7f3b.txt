{"tags": ["TensorFlow", "link", "borgWarp", "TF-Slim"], "context": "http://qiita.com/learn_tensorflow/items/3e46b2512a1bab73f5b2\n\u306etrain.py\u306b\u3066\u4f7f\u308f\u308c\u3066\u3044\u308bSlim\u306e\u5b66\u7fd2\u4e2d\u3002\n\u4ee5\u4e0b\u306b\u6ce8\u76ee\u3057\u305f\u3002\n\ntrain.py\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(inputs_batch, slim.fully_connected, [2, 2], activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n\nFurthermore, TF-Slim's slim.stack operator allows a caller to repeatedly apply the same operation with different arguments to create a stack or tower of layers. slim.stack also creates a new tf.variable_scope for each operation created. For example, a simple way to create a Multi-Layer Perceptron (MLP):\n\n# Verbose way:\nx = slim.fully_connected(x, 32, scope='fc/fc_1')\nx = slim.fully_connected(x, 64, scope='fc/fc_2')\nx = slim.fully_connected(x, 128, scope='fc/fc_3')\n\n# Equivalent, TF-Slim way using slim.stack:\nslim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')\n\n\u4e0a\u306e3\u884c\u306e\u66f8\u304d\u65b9\u30921\u884c\u3067\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3068\u306e\u3053\u3068\u3089\u3057\u3044\u3002\nhttp://qiita.com/learn_tensorflow/items/3e46b2512a1bab73f5b2\n\u306etrain.py\u306b\u3066\u4f7f\u308f\u308c\u3066\u3044\u308bSlim\u306e\u5b66\u7fd2\u4e2d\u3002\n\n\u4ee5\u4e0b\u306b\u6ce8\u76ee\u3057\u305f\u3002\n\n```train.py\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(inputs_batch, slim.fully_connected, [2, 2], activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n```\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n\n> Furthermore, TF-Slim's slim.stack operator allows a caller to repeatedly apply the same operation with different arguments to create a stack or tower of layers. slim.stack also creates a new tf.variable_scope for each operation created. For example, a simple way to create a Multi-Layer Perceptron (MLP):\n\n```py\n# Verbose way:\nx = slim.fully_connected(x, 32, scope='fc/fc_1')\nx = slim.fully_connected(x, 64, scope='fc/fc_2')\nx = slim.fully_connected(x, 128, scope='fc/fc_3')\n\n# Equivalent, TF-Slim way using slim.stack:\nslim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')\n```\n\n\u4e0a\u306e3\u884c\u306e\u66f8\u304d\u65b9\u30921\u884c\u3067\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3068\u306e\u3053\u3068\u3089\u3057\u3044\u3002\n\n\n"}