{"context": " More than 1 year has passed since last update.\n\n1. TensorFlow\u306e\u30ab\u30fc\u30cd\u30eb\u8d77\u52d5\u306e\u4ed5\u7d44\u307f\u306b\u3064\u3044\u3066\n\nTensorFlow\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u4ed5\u7d44\u307f\u3068\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n - \u9759\u7684\u306a\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u30d7\u30ed\u30bb\u30b9\u8d77\u52d5\u3068\u540c\u6642\u306b\u81ea\u52d5\u3067\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u3001\u81ea\u52d5\u3067\u30ab\u30fc\u30cd\u30eb\u74b0\u5883\u3092\u4f5c\u6210\u3059\u308b\n   \u21d2 \u5c06\u6765\u306f\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u306bAPI\u3068\u3057\u3066\u7a74\u3042\u3051\u3055\u308c\u308b\u3060\u308d\u3046\u601d\u308f\u308c\u308b\u3002\n - GPU\u306e\u8a00\u8a9e\u306f\u3001CUDA\u3092\u4f7f\u7528\u3057\u3066\u304a\u308aNVIDIA\u306eGPU\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u4e8b\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u308b (Dual\u5bfe\u5fdc)\n - C++ 11\u306e\u30b3\u30fc\u30c9\u898f\u7d04\u306b\u57fa\u3065\u3044\u305f\u8a18\u8ff0\u3068\u306a\u3063\u3066\u3044\u308b\u3002(auto\u578b\u306a\u3069)\n - \u30b3\u30de\u30f3\u30c9\u3092\u898b\u308b\u9650\u308a\u3067\u306f\u3001Linux Kernel\u74b0\u5883\u3092\u60f3\u5b9a\u3057\u305f\u4f5c\u308a\u3068\u306a\u3063\u3066\u3044\u308b\nCUDA\u306b\u3064\u3044\u3066\u88dc\u8db3:\n\nNVIDIA\u88fd\u306e\u4e26\u5217\u8a08\u7b97\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7dcf\u79f0\u3067\u3042\u308b\u3002\n\u8a73\u7d30\u306f\u3001\u4ee5\u4e0b\u306e\u901a\u308a\nURL: CUDA\n\n\u30fbGPU\u306e\u8a2d\u5b9a\u65b9\u6cd5\n\u516c\u5f0f\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u308bGPU\u306e\u8a2d\u5b9a\u65b9\u6cd5\u3002\n\u203b\u672c\u8a18\u4e8b\u306f\u3001\u51fa\u3060\u3057\u304b\u3089\u9593\u9055\u3048\u3066\u3044\u308b\u306e\u3067\u3001\u4fee\u6b63\u3057\u307e\u3059\u3002\n\n2. \u5927\u307e\u304b\u306a\u30b7\u30fc\u30b1\u30f3\u30b9\n\n\u73fe\u72b6\u306e\u30bd\u30fc\u30b9\u3067\u3001\u5927\u307e\u304b\u306a\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u30bd\u30fc\u30b9\u3067\u8ffd\u3063\u3066\u3044\u304d\u307e\u3059\u3002\nC++\u306e\u30af\u30e9\u30b9\u8a2d\u8a08\u306b\u7d50\u69cb\u6d3b\u304b\u305b\u308b\u30ce\u30a6\u30cf\u30a6\u3082\u8a70\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u9806\u756a\u306b\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u3001\u8cb4\u65b9\u306e\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u529b\u30a2\u30c3\u30d7\u306b\u7e4b\u304c\u308b\u3053\u3068\u306f\u9593\u9055\u3044\u306a\u3044\u3067\u3059\u3002\n\u203b\u30af\u30e9\u30b9\u30dd\u30a4\u30f3\u30bf\u306a\u3069\u306e\u6982\u5ff5\u3082\u666e\u901a\u306b\u51fa\u3066\u304d\u307e\u3059\u304c\u3001\u5909\u6570\u306e\u30dd\u30a4\u30f3\u30bf\u3068\u540c\u3058\u8003\u3048\u65b9\u3067\u6349\u3048\u308b\u3068\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3057\u3087\u3046\u3002\n  \u5b9f\u969b\u306b\u3001\u95a2\u6570\u540d\u306f\u5909\u6570\u540d\u306e\u7528\u306b\u6271\u3048\u307e\u3059\u3002\n  \u7c21\u5358\u306a\u30b1\u30fc\u30b9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u901a\u308a\u306b\u306a\u308a\u307e\u3059\u3002\n\nsample.cpp\n  // \u4f55\u306e\u5909\u54f2\u3082\u306a\u3044func1\u3067\u3059\u3002\n  void func1(void* pData) {\n    func1(\"func1\");\n  }\n  // \u4f55\u306e\u5909\u54f2\u3082\u306a\u3044func2\u3067\u3059\u3002\n  void func2(void* pData) {\n    func1(\"func2\");\n  }\n\n  // \u5171\u901a\u306e\u5f62\u5f0f\u306e\u95a2\u6570\u3092\u30dd\u30a4\u30f3\u30bf\u3092\u4f7f\u3063\u3066\u7e8f\u3081\u307e\u3059\n  void (*pFuncList[])(void* pData) = {\n    func1,  // func1\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001func1\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u304c\u6e21\u3055\u308c\u307e\u3059\u3002\n    func2   // func2\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001func2\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u304c\u6e21\u3055\u308c\u307e\u3059\u3002\n  }\n\n  void main(void) {\n    char* pBuffer = \"Test\";\n    for(int i = 0; i < sizeof(pFuncList) / sizeof(pFuncList[0])) {\n      pFuncList[++i](pBuffer);\n    }\n  }\n\n\n\u305d\u308c\u3067\u306f\u3001\u672c\u756a\u306e\u30b3\u30fc\u30c9\u306b\u5165\u308a\u307e\u3059\u3002\n\u307e\u305a\u306f\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u78ba\u7acb\u3092\u3057\u3066\u3044\u308b\u90e8\u5206\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u203b\u30b3\u30fc\u30c9\u4e2d\u306e\u65e5\u672c\u8a9e\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u3001\u4eca\u56de\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3067\u3059\u3002\n  \u5b9f\u969b\u306f\u3001\u591a\u304f\u306e\u30b3\u30fc\u30c9\u3067\u30b3\u30e1\u30f3\u30c8\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n2.0 \u30b9\u30bf\u30fc\u30c8\u5730\u70b9\nSource URL: direct_session.cc\nLine. 554 - 572\n\ndirect_session.cc\nclass DirectSessionFactory : public SessionFactory {\n public:\n  DirectSessionFactory() {} // \u73fe\u72b6\u306f\u3001\u3053\u3061\u3089\u306b\u901a\u3063\u3066\u4f55\u3082\u3057\u3066\u3044\u306a\u3044\n\n  // \u5c06\u6765\u306f\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3057\u3066\u4e0b\u306e\u95a2\u6570\u3092\u901a\u3057\u305f\u3044\u3089\u3057\u3044\u2026\n  // \u4ee5\u4e0b\u3092\u4f5c\u52d5\u3055\u305b\u308c\u3070\u3001Dual CPU\u53ca\u3073Dual GPU\u3092\u5229\u7528\u3057\u305fAI \u5b9f\u884c\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n  // \u7a74\u3042\u3051\u304c\u5927\u5909\u305d\u3046\u3067\u3059\u306d\u3000 :disappointed_relieved: \n  Session* NewSession(const SessionOptions& options) override {\n    std::vector<Device*> devices;\n    DeviceFactory::AddDevices(options, \"/job:localhost/replica:0/task:0\",\n                              &devices);\n    return new DirectSession(options, new DeviceMgr(devices));\n  }\n};\n\nclass DirectSessionRegistrar {\n public:\n  DirectSessionRegistrar() {\n    SessionFactory::Register(\"DIRECT_SESSION\", new DirectSessionFactory());\n  }\n};\n// \u30b0\u30ed\u30fc\u30d0\u30eb\u306e\u9759\u7684\u5909\u6570\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3002\n// \u3053\u308c\u304c\u3001\u73fe\u72b6\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4ed5\u7d44\u307f\u306e\u30b9\u30bf\u30fc\u30c8\u3067\u3059\u3002\n// \u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3067\u306f\u3001\u4f55\u3082\u3057\u306a\u3044\u306e\u3067\u8d77\u52d5\u3057\u3066\u3044\u306a\u3044\u3002\n// \u73fe\u72b6\u306f\u3001\u5225\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304c\u52d5\u3044\u3066\u3044\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u2026\nstatic DirectSessionRegistrar registrar;\n\n\n2.1 \u30b9\u30ec\u30c3\u30c9\u306e\u4f5c\u6210\u3068\u8d77\u52d5\n2.0\u3067\u6a5f\u80fd\u3057\u3066\u3044\u306a\u3044\u3053\u3068\u304c\u5206\u304b\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3068\u308a\u3042\u3048\u305aoption\u3042\u308a\u3067\u6a5f\u80fd\u3059\u308b\u3068\u4eee\u5b9a\u3057\u3066\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u306f\u3001NewThreadPool\u3068\u3044\u3046\u30ed\u30fc\u30ab\u30eb\u95a2\u6570\u3067\u3001\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nLine: 52 - 70\n\ndirect_session.cc\n// \u4e26\u5217\u51e6\u7406\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nthread::ThreadPool* NewThreadPool(const SessionOptions& options) {\n  int32 inter_op_parallelism_threads =\n      options.config.inter_op_parallelism_threads();\n  if (inter_op_parallelism_threads == 0) {\n    // Default to using the number of cores available in the process.\n    inter_op_parallelism_threads = port::NumSchedulableCPUs();\n  }\n  VLOG(1) << \"Direct session inter op parallelism threads: \"\n          << inter_op_parallelism_threads;\n  return new thread::ThreadPool(options.env, \"Compute\",\n                                inter_op_parallelism_threads);\n}\n\nthread::ThreadPool* GlobalThreadPool(const SessionOptions& options) {\n  static thread::ThreadPool* const thread_pool = NewThreadPool(options);\n  return thread_pool;\n}\n\n\n2.2 \u30b9\u30ec\u30c3\u30c9\u304b\u3089\u306e\u5272\u8fbc\u307f\n\u4ee5\u4e0b\u306e\u90e8\u5206\u3067\u30b9\u30ec\u30c3\u30c9\u306b\u767b\u9332\u3057\u305f\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\nLine: 208 - 427\n\ndirect_session.cc\nStatus DirectSession::Run(const std::vector<std::pair<string, Tensor>>& inputs,\n                          const std::vector<string>& output_names,\n                          const std::vector<string>& target_nodes,\n                          std::vector<Tensor>* outputs) {\n  {\n    mutex_lock l(graph_def_lock_);\n    if (!graph_created_) {\n      return errors::InvalidArgument(\n          \"Session was not created with a graph before Run()!\");\n    }\n  }\n\n  // Extract the inputs names for this run of the session.\n  std::vector<string> input_tensor_names;\n  input_tensor_names.reserve(inputs.size());\n  for (const auto& it : inputs) {\n    input_tensor_names.push_back(it.first);\n  }\n\n  // Check if we already have an executor for these arguments.\n  // \u3053\u3053\u3067\u3001\u3059\u3067\u306b\u8d77\u52d5\u3057\u3066\u3044\u308b\u30c7\u30d0\u30a4\u30b9\u53ca\u3073\u65b0\u898f\u306b\u8d77\u52d5\u3059\u308b\u30c7\u30d0\u30a4\u30b9\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3059\u308b\n  ExecutorsAndKeys* executors_and_keys;\n  Status s = GetOrCreateExecutors(input_tensor_names, output_names,\n                                  target_nodes, &executors_and_keys);\n  if (!s.ok()) {\n    return s;\n  }\n\n  IntraProcessRendezvous* rendez =\n      new IntraProcessRendezvous(device_mgr_.get());\n  core::ScopedUnref rendez_unref(rendez);\n\n  // Insert the input tensors into the local rendezvous by their\n  // rendezvous key.\n  for (const auto& input : inputs) {\n    const string& input_key = executors_and_keys->input_keys[input.first];\n    s = rendez->Send(input_key, Rendezvous::Args(), input.second, false);\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      return s;\n    }\n  }\n\n  // Start parallel Executors.\n  Notification executors_done;\n  const int num_executors = executors_and_keys->items.size();\n  ExecutorBarrier* barrier = new ExecutorBarrier(\n      num_executors, rendez, [&executors_done, &s](const Status& ret) {\n        s = ret;\n        executors_done.Notify();\n      });\n\n  Executor::Args args;\n  args.rendezvous = rendez;\n  args.cancellation_manager = cancellation_manager_;\n  args.runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n\n  // \u3053\u3053\u3067\u5b9f\u884c\u51e6\u7406\u3092\u3057\u3066\u3044\u308b\n  // item.executor\u306e\u5185\u5bb9\u306f\u3001\u5f8c\u3067\u51fa\u3066\u304f\u308b\u306e\u3067\u5f8c\u306b\u6574\u7406\u3057\u307e\u3059\u3002\n  // item.executor\u306f\u3001ExecutorImpl\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30dd\u30a4\u30f3\u30bf\u3067\u3042\u308b\u3002\n  for (const auto& item : executors_and_keys->items) {\n    item.executor->RunAsync(args, barrier->Get());\n  }\n\n  // \u30b9\u30ec\u30c3\u30c9\u304c\u7d42\u308f\u308b\u307e\u3067\u540c\u671f\u3059\u308b\u3002\n  executors_done.WaitForNotification();\n\n  TF_RETURN_IF_ERROR(s);\n\n  if (!output_names.empty()) {\n    outputs->resize(output_names.size());\n  }\n\n  // Get the outputs from the rendezvous\n  for (size_t output_offset = 0; output_offset < output_names.size();\n       ++output_offset) {\n    const string& output_key =\n        executors_and_keys->output_keys[output_names[output_offset]];\n    Tensor output_tensor;\n    bool is_dead;\n\n    // Fetch data from the Rendezvous.\n    s = rendez->Recv(output_key, Rendezvous::Args(), &output_tensor, &is_dead);\n    if (is_dead) {\n      s = errors::InvalidArgument(\"The tensor returned for \",\n                                  output_names[output_offset],\n                                  \" was not valid.\");\n    }\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      outputs->clear();\n      return s;\n    }\n\n    (*outputs)[output_offset] = output_tensor;\n  }\n\n  return s;\n}\n\nStatus DirectSession::GetOrCreateExecutors(\n    gtl::ArraySlice<string> inputs, gtl::ArraySlice<string> outputs,\n    gtl::ArraySlice<string> target_nodes,\n    ExecutorsAndKeys** executors_and_keys) {\n  // Sort the inputs and outputs, so we don't create separate\n  // executors when a user passes in the same inputs/outputs in\n  // different orders.\n  //\n  // We could consider some other signature instead of sorting that\n  // preserves the same property to avoid the sort in the future.\n  std::vector<string> inputs_sorted(inputs.begin(), inputs.end());\n  std::vector<string> outputs_sorted(outputs.begin(), outputs.end());\n  std::vector<string> tn_sorted(target_nodes.begin(), target_nodes.end());\n  std::sort(inputs_sorted.begin(), inputs_sorted.end());\n  std::sort(outputs_sorted.begin(), outputs_sorted.end());\n  std::sort(tn_sorted.begin(), tn_sorted.end());\n\n  const string key = strings::StrCat(str_util::Join(inputs_sorted, \",\"), \"->\",\n                                     str_util::Join(outputs_sorted, \",\"), \"/\",\n                                     str_util::Join(tn_sorted, \",\"));\n\n  // See if we already have the executors for this run.\n  {\n    mutex_lock l(executor_lock_);  // could use reader lock\n    auto it = executors_.find(key);\n    if (it != executors_.end()) {\n      *executors_and_keys = it->second;\n      return Status::OK();\n    }\n  }\n\n  // The executor_lock_ is intentionally released while executor is\n  // being created.\n  FunctionLibraryDefinition* fdefs;\n  std::unordered_map<string, Graph*> graphs;\n  Status s = CreateGraphs(inputs, outputs, target_nodes, &fdefs, &graphs);\n  if (!s.ok()) {\n    return s;\n  }\n\n  bool has_control_flow = false;\n  for (const auto& graph : graphs) {\n    for (const Node* n : graph.second->nodes()) {\n      if (IsControlFlow(n)) {\n        has_control_flow = true;\n        break;\n      }\n    }\n    if (has_control_flow) break;\n  }\n\n  std::unique_ptr<ExecutorsAndKeys> ek(new ExecutorsAndKeys);\n  ek->func_defs = fdefs;\n  ek->items.reserve(graphs.size());\n  auto runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n  for (const auto& graph : graphs) {\n    const string& partition_name = graph.first;\n    Graph* partition_graph = graph.second;\n    const int graph_def_version = partition_graph->version();\n\n    Device* device;\n    s = device_mgr_->LookupDevice(partition_name, &device);\n    if (!s.ok()) {\n      return s;\n    }\n\n    ek->items.resize(ek->items.size() + 1);\n    auto* item = &(ek->items.back());\n    // \u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u3092\u767b\u9332\u3057\u307e\u3059\u3002\n    // \u30ab\u30fc\u30cd\u30eb\u4f5c\u6210\u7528\u306e\u95a2\u6570\u306a\u3069\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u8a2d\u7f6e\u3057\u307e\u3059\u3002\n    item->flib =\n        NewFunctionLibraryRuntime(device, runner, graph_def_version, fdefs);\n\n    LocalExecutorParams params;\n    params.has_control_flow = has_control_flow;\n    params.device = device;\n    params.function_library = item->flib;\n    auto lib = item->flib;\n    auto opseg = device->op_segment();\n\n    // \u30ab\u30fc\u30cd\u30eb\u4f5c\u6210\u95a2\u6570\u3067\u3059\u3002\n    // create_kernel\u3092\u95a2\u6570\u306e\u69d8\u306b\u66f8\u304f\u3053\u3068\u3067\u95a2\u6570\u3068\u3057\u3066\u30b3\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n    // OpKernel* pKernel;\n    // params.create_kernel(pKernel);\n    params.create_kernel = [this, lib, opseg](const NodeDef& ndef,\n                                              OpKernel** kernel) {\n      auto create_fn = [lib, &ndef](OpKernel** kernel) {\n        return lib->CreateKernel(ndef, kernel);\n      };\n      // Kernels created for subgraph nodes need to be cached.  On\n      // cache miss, create_fn() is invoked to create a kernel based\n      // on the function library here + global op registry.\n      return opseg->FindOrCreate(session_handle_, ndef.name(), kernel,\n                                 create_fn);\n    };\n    // \u30ab\u30fc\u30cd\u30eb\u524a\u9664\u95a2\u6570\u3067\u3059\u3002 (\u4f55\u3082\u3084\u3063\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u306d\u2026)\n    // create_kernel\u3092\u95a2\u6570\u306e\u69d8\u306b\u66f8\u304f\u3053\u3068\u3067\u95a2\u6570\u3068\u3057\u3066\u30b3\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n    params.delete_kernel = [](OpKernel* kernel) {\n      // Do nothing because 'kernel' is owned by opseg above.\n    };\n\n    // \u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30af\u30e9\u30b9\u3092item->executor\u306b\u4f5c\u6210\u3059\u308b\n    s = NewLocalExecutor(params, partition_graph, &item->executor);\n    if (!s.ok()) {\n      return s;\n    }\n  }\n\n  // \u4eca\u56de\u306f\u3001\u91cd\u8981\u3067\u306f\u306a\u3044\u306e\u3067\u7701\u7565\n\n  return Status::OK();\n}\n\n\n2.3 \u30ab\u30fc\u30cd\u30eb\u306e\u4f5c\u6210\u30fb\u521d\u671f\u5316\n\u30ab\u30fc\u30cd\u30eb\u306e\u521d\u671f\u5316\u3092\u3053\u306e\u4e2d\u3067\u3084\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u3053\u304b\u3089NewLocalExecutor\u5185\u90e8\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\nnew\u6f14\u7b97\u5b50\u3092\u4f7f\u7528\u3057\u3066\u3001\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3057\u305f\u5f8c\u306eInitilize\u306e\u4e2d\u3067\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nSource URL: executor.cc\nLine: 2130 - 2140\n\nexecutor.cc\nStatus NewLocalExecutor(const LocalExecutorParams& params, const Graph* graph,\n                        Executor** executor) {\n  ExecutorImpl* impl = new ExecutorImpl(params, graph);\n  // \u3053\u306e\u4e2d\u3067\u30ab\u30fc\u30cd\u30eb\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n  Status s = impl->Initialize();\n  if (s.ok()) {\n    *executor = impl;\n  } else {\n    delete impl;\n  }\n  return s;\n}\n\n\nExecutorImpl::Initialize()\u306e\u4e2d\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nLine: 244 - 280\n\nexecutor.cc\nStatus ExecutorImpl::Initialize() {\n  const int num_nodes = graph_->num_node_ids();\n  nodes_.resize(num_nodes);\n\n  Status s;\n  total_tensors_ = 0;\n\n  // Preprocess every node in the graph to create an instance of op\n  // kernel for each node;\n  for (const Node* n : graph_->nodes()) {\n    const int id = n->id();\n    NodeItem* item = &nodes_[id];\n    item->node = n;\n    item->input_start = total_tensors_;\n    total_tensors_ += n->num_inputs();\n    // \u3053\u3053\u3067\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n    s = params_.create_kernel(n->def(), &item->kernel);\n    if (!s.ok()) {\n      // \u306a\u306b\u3084\u3089\u30ce\u30fc\u30c9\u306e\u30a2\u30bf\u30c3\u30c1\u3092\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u95a2\u4fc2\u306a\u3044\u305f\u3081\u672a\u78ba\u8a8d\u3067\u3059\u3002\n      s = AttachDef(s, n->def());\n      LOG(ERROR) << \"Executor failed to create kernel. \" << s;\n      break;\n    }\n    CHECK(item->kernel);\n\n    // Initialize static information about the frames in the graph.\n    if (IsEnter(n)) {\n      string frame_name;\n      s = GetNodeAttr(n->def(), \"frame_name\", &frame_name);\n      if (!s.ok()) return s;\n      ++frame_input_count_[frame_name];\n    }\n  }\n  if (params_.has_control_flow) {\n    VLOG(2) << \"Graph has control flow.\";\n  }\n  if (!s.ok()) return s;\n  return SetAllocAttrs();\n}\n\n\n2.4 \u30ab\u30fc\u30cd\u30eb\u306e\u65b0\u898f\u4f5c\u6210\nparams_.create_kernel(n->def(), &item->kernel);\n\u304c\u3001\u3069\u3053\u306b\u7e4b\u304c\u308b\u304b\u3068\u8a00\u3046\u3068\u3001\nSource URL: direct_session.cc\nLine: 380 -390\n\ndirect_session.cc\n    params.create_kernel = [this, lib, opseg](const NodeDef& ndef,\n                                              OpKernel** kernel) {\n      // \u3053\u306e\u4e2d\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n\n      // create_fn\u306bCreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b\u3002\n      auto create_fn = [lib, &ndef](OpKernel** kernel) {\n        return lib->CreateKernel(ndef, kernel);\n      };\n      // Kernels created for subgraph nodes need to be cached.  On\n      // cache miss, create_fn() is invoked to create a kernel based\n      // on the function library here + global op registry.\n\n      // \u30c7\u30d0\u30a4\u30b9\u304b\u3089\u53d6\u5f97\u3057\u305f\u30bb\u30b0\u30e1\u30f3\u30c8\u304b\u3089\u8d77\u52d5\u6e08\u307f\u306e\u30c7\u30d0\u30a4\u30b9\u3092\u691c\u7d22\u307e\u305f\u306f\n      // CreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u304b\u3092\u5b9f\u884c\u3059\u308b\n      return opseg->FindOrCreate(session_handle_, ndef.name(), kernel,\n                                 create_fn);\n    };\n\n\n2.5 \u30ab\u30fc\u30cd\u30eb\u306e\u691c\u7d22\u53ca\u3073\u65b0\u898f\u4f5c\u6210\nOpSegment::FindOrCreate\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n\u3059\u3067\u306b\u8d77\u52d5\u3057\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u4f5c\u6210\u3057\u3066\u3057\u3066\u3044\u308b\u30ab\u30fc\u30cd\u30eb\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u8fd4\u3057\u307e\u3059\u3002\n\u521d\u56de\u8d77\u52d5\u7b49\u3067\u3001\u307e\u3060\u4f5c\u6210\u3057\u3066\u3044\u306a\u3044\u30b1\u30fc\u30b9\u306fCreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\nSource URL: op_segment.cc\nLine: 36 - 70\n\nop_segment.cc\nStatus OpSegment::FindOrCreate(const string& session_handle,\n                               const string& node_name, OpKernel** kernel,\n                               CreateKernelFn create_fn) {\n  // \u30b9\u30ec\u30c3\u30c9\u306e\u30c7\u30c3\u30c9\u30ed\u30c3\u30af\u5bfe\u7b56\u306e\u305f\u3081\u306e\u30df\u30e5\u30fc\u30c6\u30c3\u30af\u30b9 - ON\n  {\n    mutex_lock l(mu_);\n    auto item = gtl::FindPtrOrNull(sessions_, session_handle);\n    if (item == nullptr) {\n      return errors::NotFound(\"Session \", session_handle, \" is not found.\");\n    }\n    // SymbolicGradientHelper\u7d4c\u7531\u3067\u4f5c\u6210\u3055\u308c\u305fNode\u3067\u306f\u306a\u304f\u3001\n    // \u30ab\u30fc\u30cd\u30eb\u540d\u3068\u30ce\u30fc\u30c9\u540d\u3067\u691c\u7d22\u3057\u3066\u3001\u30de\u30c3\u30c1\u3057\u305f\u5834\u5408\u306f\u30ab\u30fc\u30cd\u30eb\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u4f7f\u7528\u3059\u308b\u3002\n    *kernel = gtl::FindPtrOrNull(item->name_kernel, node_name);\n    if (*kernel != nullptr) {\n      return Status::OK();\n    }\n  }\n\n  // \u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001CreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\n  // \u3053\u306e\u4e2d\u8eab\u306f\u3001\u300clib->CreateKernel(ndef, kernel)\u300d\u306b\u306a\u308a\u307e\u3059\u3002\n  // lib\u306f\u3001FunctionLibraryRuntimeImpl\u306a\u306e\u3067\u3001\u305d\u3061\u3089\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n  // \u7406\u7531\u306f\u3001NewFunctionLibraryRuntime\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002\n  Status s = create_fn(kernel);\n  if (!s.ok()) {\n    LOG(ERROR) << \"Create kernel failed: \" << s;\n    return s;\n  }\n\n  // \u30b9\u30ec\u30c3\u30c9\u306e\u30c7\u30c3\u30c9\u30ed\u30c3\u30af\u5bfe\u7b56\u306e\u305f\u3081\u306e\u30df\u30e5\u30fc\u30c6\u30c3\u30af\u30b9 - OFF\n  {\n    mutex_lock l(mu_);\n    auto item = gtl::FindPtrOrNull(sessions_, session_handle);\n    if (item == nullptr) {\n      return errors::NotFound(\"Session \", session_handle, \" is not found.\");\n    }\n    OpKernel** p_kernel = &(item->name_kernel[node_name]);\n    if (*p_kernel == nullptr) {\n      *p_kernel = *kernel;  // Inserts 'kernel' in the map.\n    } else {\n      delete *kernel;\n      *kernel = *p_kernel;\n    }\n  }\n  return Status::OK();\n}\n\n\n\u88dc\u8db3\n\ngtl::FindPtrOrNull\u306f\u3001\u4ee5\u4e0b\u306e\u30d8\u30c3\u30c0\u30fc\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nSource URL: map_util.h\nNewFunctionLibraryRuntime\u95a2\u6570\u306f\u3001\u4ee5\u4e0b\u306e\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nSource URL: function.cc\n\nFunctionLibraryRuntimeImpl::CreateKernel\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nSource URL: Source URL: function.cc\nLine: 374 - 401\n\nfunction.cc\nStatus FunctionLibraryRuntimeImpl::CreateKernel(const NodeDef& ndef,\n                                                OpKernel** kernel) {\n  // lib_def_\u306f\u3001DirectSession::CreateGraphs\u95a2\u6570\u3067\u4f5c\u6210\u3057\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u7fa4\u3068\u601d\u308f\u308c\u308b\u304c\u3001\n  // \u8a73\u7d30\u306f\u3001\u8907\u96d1\u306b\u306a\u308a\u305d\u3046\u306a\u306e\u3067\u5225\u306e\u6a5f\u4f1a\u3068\u3059\u308b\u3002\n  // ndef\u306f\u3001Node\u95a2\u9023\u3060\u308d\u3046\u304c\u30b3\u30c1\u30e9\u3082\u8a73\u7d30\u306f\u5225\u306e\u6a5f\u4f1a\u306b\u89e3\u6790\u3059\u308b\u3002\n  if (ndef.op() != kGradientOp && (lib_def_->Find(ndef.op()) == nullptr)) {\n    return CreateNonCachedKernel(device_, this, ndef, graph_def_version_,\n                                 kernel);\n  }\n\n  // Try to instantiate this function for the func/attr. Maybe its\n  // cached already.\n  // \u95a2\u6570\u3068\u5c5e\u6027\u304b\u3089\u306e\u95a2\u6570\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3092\u8a66\u307f\u308b\n  // \u8a73\u7d30\u306a\u4ed5\u7d44\u307f\u306f\u3001\u6b21\u306e\u6a5f\u4f1a\u306b\u5b9f\u65bd\u3059\u308b\n  Handle handle;\n  TF_RETURN_IF_ERROR(Instantiate(ndef.op(), ndef.attr(), &handle));\n\n  const FunctionBody* fbody = GetFunctionBody(handle);\n  CHECK_NOTNULL(fbody);\n\n  // Constructs a CallOp kernel for running the instantiated function.\n  // CallOp(Call Operation\u306e\u3053\u3068\uff1f)\u30ab\u30fc\u30cd\u30eb\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u8d77\u52d5\u3059\u308b\u3002\n  // \u3053\u306e\u8fba\u308a\u3082\u3001\u6b21\u306e\u6a5f\u4f1a\u3068\u3059\u308b\u3002\n  Status s;\n  auto device_type = DeviceType(device_->attributes().device_type());\n  OpKernelConstruction construction(\n      device_type, device_, device_->GetAllocator(AllocatorAttributes()), &ndef,\n      &fbody->fdef.signature(), this, fbody->arg_types, fbody->ret_types,\n      graph_def_version_, &s);\n  *kernel = new CallOp(handle, &construction);\n  if (!s.ok()) {\n    delete kernel;\n  }\n  return s;\n}\n\n\n2.6 Executor\u3092\u5b9f\u884c\n2.2\u307e\u3067\u623b\u3063\u3066\u30012.3\u3067\u4f5c\u6210\u3057\u305fExecutor\u3092\u975e\u540c\u671f\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u305d\u306e\u6d41\u308c\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nSource URL: direct_session.cc\nLine: 208 - 427\n\ndirect_session.cc\nStatus DirectSession::Run(const std::vector<std::pair<string, Tensor>>& inputs,\n                          const std::vector<string>& output_names,\n                          const std::vector<string>& target_nodes,\n                          std::vector<Tensor>* outputs) {\n  {\n    mutex_lock l(graph_def_lock_);\n    if (!graph_created_) {\n      return errors::InvalidArgument(\n          \"Session was not created with a graph before Run()!\");\n    }\n  }\n\n  // Extract the inputs names for this run of the session.\n  std::vector<string> input_tensor_names;\n  input_tensor_names.reserve(inputs.size());\n  for (const auto& it : inputs) {\n    input_tensor_names.push_back(it.first);\n  }\n\n  // Check if we already have an executor for these arguments.\n  ExecutorsAndKeys* executors_and_keys;\n  Status s = GetOrCreateExecutors(input_tensor_names, output_names,\n                                  target_nodes, &executors_and_keys);\n  if (!s.ok()) {\n    return s;\n  }\n\n  // -----------------------\u3053\u3053\u307e\u3067 2.2 ----------------------------\n\n  IntraProcessRendezvous* rendez =\n      new IntraProcessRendezvous(device_mgr_.get());\n  core::ScopedUnref rendez_unref(rendez);\n\n  // Insert the input tensors into the local rendezvous by their\n  // rendezvous key.\n  for (const auto& input : inputs) {\n    const string& input_key = executors_and_keys->input_keys[input.first];\n    s = rendez->Send(input_key, Rendezvous::Args(), input.second, false);\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      return s;\n    }\n  }\n\n  // Start parallel Executors.\n  Notification executors_done;\n  const int num_executors = executors_and_keys->items.size();\n  ExecutorBarrier* barrier = new ExecutorBarrier(\n      num_executors, rendez, [&executors_done, &s](const Status& ret) {\n        s = ret;\n        executors_done.Notify();\n      });\n\n  Executor::Args args;\n  args.rendezvous = rendez;\n  args.cancellation_manager = cancellation_manager_;\n  args.runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n\n  // \u30dd\u30a4\u30f3\u30c8\u306f\u3001\u3053\u3053\u3067\u3059\u3002\n  // \u3053\u3053\u3067\u3001executor\u3092\u975e\u540c\u671f\u5b9f\u884c\u3057\u307e\u3059\u3002\n  for (const auto& item : executors_and_keys->items) {\n    item.executor->RunAsync(args, barrier->Get());\n  }\n\n  executors_done.WaitForNotification();\n\n  // \u4eca\u56de\u306f\u3001\u7121\u7528\u306a\u306e\u3067\u7701\u7565\n\n  return s;\n}\n\n\n\u3067\u306f\u3001item.executor->RunAsync\u306e\u4e2d\u3092\u8ffd\u3044\u307e\u3057\u3087\u3046\u3002\nExecutorImpl::RunAsync\u306b\u7e4b\u304c\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u3053\u3061\u3089\u306e\u30b3\u30fc\u30c9\u306b\u79fb\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\nSource URL: executor.cc\nLine: 2118 - 2125\n\nexecutor.cc\n// NOTE(yuanbyu): Use the executor that supports control flow by default.\nconst bool use_control_flow_executor = true;\nvoid ExecutorImpl::RunAsync(const Args& args, DoneCallback done) {\n  // ControlFlow\u3068\u3044\u3046\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3067\u3057\u3087\u3046\u304b\uff1f\n  // \u5168\u4f53\u898b\u3066\u306a\u3044\u306e\u3067\u3001\u307e\u3060\u4e0d\u660e\u3067\u3059\u3002 (\u5206\u304b\u308a\u6b21\u7b2c\u8ffd\u8a18\u3057\u307e\u3059\u3002)\n  if (params_.has_control_flow || use_control_flow_executor) {\n    (new ExecutorState(args, this))->RunAsync(done);\n  } else {\n    // \u4eca\u56de\u306f\u3001\u3053\u3061\u3089\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u8ffd\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n    (new SimpleExecutorState(args, this))->RunAsync(done);\n  }\n}\n\n\nSimpleExecutorState::RunAsync\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\nLine: 1794 - 1819\n\nexecutor.cc\nvoid SimpleExecutorState::RunAsync(Executor::DoneCallback done) {\n  const Graph* graph = impl_->graph_;\n  ReadyNodeIds ready;\n\n  // Ask the device to fill in the device context map.\n  Device* device = impl_->params_.device;\n  device->FillContextMap(graph, &device_context_map_);\n\n  // \u4e2d\u8eab\u307e\u3067\u898b\u5207\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u63a8\u6e2c\u3067\u3059\u304c\u3001\u30ce\u30fc\u30c9\u306b\u30ea\u30f3\u30af\u3055\u308c\u3066\u3044\u308b\u753b\u50cf\u3092\u89e3\u6790\u3057\u3066\n  // \u4e2d\u306b\u30a8\u30c3\u30c2\u304c\u3042\u308b\u304b\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u3044\u308b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n  // \u30a8\u30c3\u30c2\u304c\u7121\u3044\u753b\u50cf\u304c\u4e00\u3064\u3067\u3082\u3042\u308c\u3070\u51e6\u7406\u3092\u7d9a\u884c\u3059\u308b\u3002 (\u3053\u306e\u5834\u5408\u306e\u51e6\u7406\u3068\u306f\u3001ExecutorBarrier\u306e\u3053\u3068)\n  // \u30a8\u30c3\u30c2\u304c\u3059\u3079\u3066\u3042\u308b\u5834\u5408\u306f\u3001ScheduleReady\u306b\u5165\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n  // \u3053\u3053\u3067\u306e\u30a8\u30c3\u30c2\u306e\u5b9a\u7fa9\u306f\u3001\u753b\u50cf\u306b\u95a2\u9023\u4ed8\u3044\u3066\u3044\u308b\u30ce\u30fc\u30c9\u306e\u6570\u3068\u601d\u308f\u308c\u308b\u3002\n\u3000// \u7406\u7531\u306f\u3001\u88dc\u8db3\u306b\u8a18\u8f09\u3059\u308b\u3002\n  for (const Node* n : graph->nodes()) {\n    const int id = n->id();\n    const int num_in_edges = n->in_edges().size();\n    pending_[id].Set(num_in_edges);\n    if (num_in_edges == 0) {\n      ready.push_back(id);\n    }\n  }\n  if (ready.empty()) {\n    done(Status::OK());\n  } else {\n    num_active_ = ready.size();\n    done_cb_ = done;\n    input_tensors_.resize(impl_->total_tensors_);\n    // Schedule to run all the ready ops in thread pool.\n    ScheduleReady(ready, nullptr);\n  }\n}\n\n\n\u88dc\u8db3\n\n\u4ee5\u4e0b\u306e\u30bd\u30fc\u30b9\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3001Graph::AddEdge\u95a2\u6570\u306b\u3066\u633f\u5165\u51e6\u7406\u3057\u3066\u3044\u307e\u3059\u3002\n\u633f\u5165\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u304cNode\u306e\u5897\u6e1b\u306b\u4f34\u3063\u3066\u3044\u308b\u3053\u3068\u304b\u3089, \u753b\u50cf\u3068\u30ea\u30f3\u30af\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u306e\u6570\u3068\u63a8\u6e2c\u3057\u307e\u3057\u305f\u3002\nSource URL: graph.cc\nExecutorBarrier\u306e\u5b9a\u7fa9\u306f\u3001\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\nSource URL: executor.h\n\n\n3. \u7d50\u8ad6\n\n\u73fe\u5728\u306f\u3001\u30b9\u30ec\u30c3\u30c9\u4f5c\u6210\u3059\u308b\u30eb\u30fc\u30c8\u306f\u4f7f\u7528\u3055\u308c\u3066\u304a\u3089\u305a\u30c7\u30d0\u30c3\u30b0\u4e2d\u3089\u3057\u3044\u2026\n  \u9577\u304f\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u4e00\u65e6\u3053\u3053\u3067\u533a\u5207\u308a\u307e\u3059\u3002\n  \u3050\u3061\u3083\u3050\u3061\u3083\u3067\u3059\u307f\u307e\u305b\u3093\u3002  \n\u6b21\u56de\u306f\u3001\u300eOpKernelContext\u3092\u4f5c\u6210\u300f\u304b\u3089GPU\u306e\u8d77\u52d5\u30fb\u30a2\u30bf\u30c3\u30c1\u307e\u3067\u306e\u6d41\u308c\u3092\u30bd\u30fc\u30b9\u3068\u7167\u3089\u3057\u5408\u308f\u305b\u3064\u3064\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\u521d\u3081\u3066\u66f8\u3044\u305f\u306e\u3067\u5206\u304b\u308a\u3065\u3089\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u610f\u898b\u3084\u8981\u671b\u7b49\u3042\u308a\u307e\u3057\u305f\u3089\u3001\u9060\u616e\u306a\u304f\u4e0b\u3055\u3044\u3002\n\u304a\u5f85\u3061\u3057\u3066\u3044\u307e\u3059\u3002\n# **1. TensorFlow\u306e\u30ab\u30fc\u30cd\u30eb\u8d77\u52d5\u306e\u4ed5\u7d44\u307f\u306b\u3064\u3044\u3066**\n\n TensorFlow\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u4ed5\u7d44\u307f\u3068\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n - \u9759\u7684\u306a\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u30d7\u30ed\u30bb\u30b9\u8d77\u52d5\u3068\u540c\u6642\u306b\u81ea\u52d5\u3067\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u3001\u81ea\u52d5\u3067\u30ab\u30fc\u30cd\u30eb\u74b0\u5883\u3092\u4f5c\u6210\u3059\u308b\n   \u21d2 \u5c06\u6765\u306f\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3067\u304d\u308b\u3088\u3046\u306bAPI\u3068\u3057\u3066\u7a74\u3042\u3051\u3055\u308c\u308b\u3060\u308d\u3046\u601d\u308f\u308c\u308b\u3002\n - GPU\u306e\u8a00\u8a9e\u306f\u3001CUDA\u3092\u4f7f\u7528\u3057\u3066\u304a\u308aNVIDIA\u306eGPU\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u4e8b\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u308b (Dual\u5bfe\u5fdc)\n - C++ 11\u306e\u30b3\u30fc\u30c9\u898f\u7d04\u306b\u57fa\u3065\u3044\u305f\u8a18\u8ff0\u3068\u306a\u3063\u3066\u3044\u308b\u3002(auto\u578b\u306a\u3069)\n - \u30b3\u30de\u30f3\u30c9\u3092\u898b\u308b\u9650\u308a\u3067\u306f\u3001Linux Kernel\u74b0\u5883\u3092\u60f3\u5b9a\u3057\u305f\u4f5c\u308a\u3068\u306a\u3063\u3066\u3044\u308b\n\nCUDA\u306b\u3064\u3044\u3066\u88dc\u8db3:\n > NVIDIA\u88fd\u306e\u4e26\u5217\u8a08\u7b97\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7dcf\u79f0\u3067\u3042\u308b\u3002\n > \u8a73\u7d30\u306f\u3001\u4ee5\u4e0b\u306e\u901a\u308a\n > URL: [CUDA](http://www.nvidia.com/object/cuda_home_new.html)\n\n\u30fbGPU\u306e\u8a2d\u5b9a\u65b9\u6cd5\n\u516c\u5f0f\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u308b[GPU\u306e\u8a2d\u5b9a\u65b9\u6cd5](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/using_gpu/index.md)\u3002\n\n\u203b\u672c\u8a18\u4e8b\u306f\u3001\u51fa\u3060\u3057\u304b\u3089\u9593\u9055\u3048\u3066\u3044\u308b\u306e\u3067\u3001\u4fee\u6b63\u3057\u307e\u3059\u3002\n\n# **2. \u5927\u307e\u304b\u306a\u30b7\u30fc\u30b1\u30f3\u30b9**\n\u73fe\u72b6\u306e\u30bd\u30fc\u30b9\u3067\u3001\u5927\u307e\u304b\u306a\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u30bd\u30fc\u30b9\u3067\u8ffd\u3063\u3066\u3044\u304d\u307e\u3059\u3002\nC++\u306e\u30af\u30e9\u30b9\u8a2d\u8a08\u306b\u7d50\u69cb\u6d3b\u304b\u305b\u308b\u30ce\u30a6\u30cf\u30a6\u3082\u8a70\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u9806\u756a\u306b\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u3001\u8cb4\u65b9\u306e\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u529b\u30a2\u30c3\u30d7\u306b\u7e4b\u304c\u308b\u3053\u3068\u306f\u9593\u9055\u3044\u306a\u3044\u3067\u3059\u3002\n\u203b\u30af\u30e9\u30b9\u30dd\u30a4\u30f3\u30bf\u306a\u3069\u306e\u6982\u5ff5\u3082\u666e\u901a\u306b\u51fa\u3066\u304d\u307e\u3059\u304c\u3001\u5909\u6570\u306e\u30dd\u30a4\u30f3\u30bf\u3068\u540c\u3058\u8003\u3048\u65b9\u3067\u6349\u3048\u308b\u3068\u5206\u304b\u308a\u3084\u3059\u3044\u3067\u3057\u3087\u3046\u3002\n  \u5b9f\u969b\u306b\u3001\u95a2\u6570\u540d\u306f\u5909\u6570\u540d\u306e\u7528\u306b\u6271\u3048\u307e\u3059\u3002\n  \u7c21\u5358\u306a\u30b1\u30fc\u30b9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u901a\u308a\u306b\u306a\u308a\u307e\u3059\u3002\n\n```cpp:sample.cpp\n  // \u4f55\u306e\u5909\u54f2\u3082\u306a\u3044func1\u3067\u3059\u3002\n  void func1(void* pData) {\n    func1(\"func1\");\n  }\n  // \u4f55\u306e\u5909\u54f2\u3082\u306a\u3044func2\u3067\u3059\u3002\n  void func2(void* pData) {\n    func1(\"func2\");\n  }\n\n  // \u5171\u901a\u306e\u5f62\u5f0f\u306e\u95a2\u6570\u3092\u30dd\u30a4\u30f3\u30bf\u3092\u4f7f\u3063\u3066\u7e8f\u3081\u307e\u3059\n  void (*pFuncList[])(void* pData) = {\n    func1,  // func1\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001func1\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u304c\u6e21\u3055\u308c\u307e\u3059\u3002\n    func2   // func2\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001func2\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u304c\u6e21\u3055\u308c\u307e\u3059\u3002\n  }\n\n  void main(void) {\n    char* pBuffer = \"Test\";\n    for(int i = 0; i < sizeof(pFuncList) / sizeof(pFuncList[0])) {\n      pFuncList[++i](pBuffer);\n    }\n  }\n```\n\n\u305d\u308c\u3067\u306f\u3001\u672c\u756a\u306e\u30b3\u30fc\u30c9\u306b\u5165\u308a\u307e\u3059\u3002\n\u307e\u305a\u306f\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u78ba\u7acb\u3092\u3057\u3066\u3044\u308b\u90e8\u5206\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\u203b\u30b3\u30fc\u30c9\u4e2d\u306e\u65e5\u672c\u8a9e\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u3001\u4eca\u56de\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3067\u3059\u3002\n  \u5b9f\u969b\u306f\u3001\u591a\u304f\u306e\u30b3\u30fc\u30c9\u3067\u30b3\u30e1\u30f3\u30c8\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\n**2.0 \u30b9\u30bf\u30fc\u30c8\u5730\u70b9**\nSource URL: [direct_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc)\nLine. 554 - 572\n\n```cpp:direct_session.cc\nclass DirectSessionFactory : public SessionFactory {\n public:\n  DirectSessionFactory() {} // \u73fe\u72b6\u306f\u3001\u3053\u3061\u3089\u306b\u901a\u3063\u3066\u4f55\u3082\u3057\u3066\u3044\u306a\u3044\n\n  // \u5c06\u6765\u306f\u3001\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3057\u3066\u4e0b\u306e\u95a2\u6570\u3092\u901a\u3057\u305f\u3044\u3089\u3057\u3044\u2026\n  // \u4ee5\u4e0b\u3092\u4f5c\u52d5\u3055\u305b\u308c\u3070\u3001Dual CPU\u53ca\u3073Dual GPU\u3092\u5229\u7528\u3057\u305fAI \u5b9f\u884c\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n  // \u7a74\u3042\u3051\u304c\u5927\u5909\u305d\u3046\u3067\u3059\u306d\u3000 :disappointed_relieved: \n  Session* NewSession(const SessionOptions& options) override {\n    std::vector<Device*> devices;\n    DeviceFactory::AddDevices(options, \"/job:localhost/replica:0/task:0\",\n                              &devices);\n    return new DirectSession(options, new DeviceMgr(devices));\n  }\n};\n\nclass DirectSessionRegistrar {\n public:\n  DirectSessionRegistrar() {\n    SessionFactory::Register(\"DIRECT_SESSION\", new DirectSessionFactory());\n  }\n};\n// \u30b0\u30ed\u30fc\u30d0\u30eb\u306e\u9759\u7684\u5909\u6570\u3068\u3057\u3066\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3002\n// \u3053\u308c\u304c\u3001\u73fe\u72b6\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4ed5\u7d44\u307f\u306e\u30b9\u30bf\u30fc\u30c8\u3067\u3059\u3002\n// \u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3067\u306f\u3001\u4f55\u3082\u3057\u306a\u3044\u306e\u3067\u8d77\u52d5\u3057\u3066\u3044\u306a\u3044\u3002\n// \u73fe\u72b6\u306f\u3001\u5225\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304c\u52d5\u3044\u3066\u3044\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u2026\nstatic DirectSessionRegistrar registrar;\n```\n\n**2.1 \u30b9\u30ec\u30c3\u30c9\u306e\u4f5c\u6210\u3068\u8d77\u52d5**\n2.0\u3067\u6a5f\u80fd\u3057\u3066\u3044\u306a\u3044\u3053\u3068\u304c\u5206\u304b\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3068\u308a\u3042\u3048\u305aoption\u3042\u308a\u3067\u6a5f\u80fd\u3059\u308b\u3068\u4eee\u5b9a\u3057\u3066\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\n\u307e\u305a\u306f\u3001NewThreadPool\u3068\u3044\u3046\u30ed\u30fc\u30ab\u30eb\u95a2\u6570\u3067\u3001\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nLine: 52 - 70\n\n```cpp:direct_session.cc\n// \u4e26\u5217\u51e6\u7406\u30b9\u30ec\u30c3\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nthread::ThreadPool* NewThreadPool(const SessionOptions& options) {\n  int32 inter_op_parallelism_threads =\n      options.config.inter_op_parallelism_threads();\n  if (inter_op_parallelism_threads == 0) {\n    // Default to using the number of cores available in the process.\n    inter_op_parallelism_threads = port::NumSchedulableCPUs();\n  }\n  VLOG(1) << \"Direct session inter op parallelism threads: \"\n          << inter_op_parallelism_threads;\n  return new thread::ThreadPool(options.env, \"Compute\",\n                                inter_op_parallelism_threads);\n}\n\nthread::ThreadPool* GlobalThreadPool(const SessionOptions& options) {\n  static thread::ThreadPool* const thread_pool = NewThreadPool(options);\n  return thread_pool;\n}\n```\n\n**2.2 \u30b9\u30ec\u30c3\u30c9\u304b\u3089\u306e\u5272\u8fbc\u307f**\n\u4ee5\u4e0b\u306e\u90e8\u5206\u3067\u30b9\u30ec\u30c3\u30c9\u306b\u767b\u9332\u3057\u305f\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\nLine: 208 - 427\n\n```cpp:direct_session.cc\nStatus DirectSession::Run(const std::vector<std::pair<string, Tensor>>& inputs,\n                          const std::vector<string>& output_names,\n                          const std::vector<string>& target_nodes,\n                          std::vector<Tensor>* outputs) {\n  {\n    mutex_lock l(graph_def_lock_);\n    if (!graph_created_) {\n      return errors::InvalidArgument(\n          \"Session was not created with a graph before Run()!\");\n    }\n  }\n\n  // Extract the inputs names for this run of the session.\n  std::vector<string> input_tensor_names;\n  input_tensor_names.reserve(inputs.size());\n  for (const auto& it : inputs) {\n    input_tensor_names.push_back(it.first);\n  }\n\n  // Check if we already have an executor for these arguments.\n  // \u3053\u3053\u3067\u3001\u3059\u3067\u306b\u8d77\u52d5\u3057\u3066\u3044\u308b\u30c7\u30d0\u30a4\u30b9\u53ca\u3073\u65b0\u898f\u306b\u8d77\u52d5\u3059\u308b\u30c7\u30d0\u30a4\u30b9\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3059\u308b\n  ExecutorsAndKeys* executors_and_keys;\n  Status s = GetOrCreateExecutors(input_tensor_names, output_names,\n                                  target_nodes, &executors_and_keys);\n  if (!s.ok()) {\n    return s;\n  }\n\n  IntraProcessRendezvous* rendez =\n      new IntraProcessRendezvous(device_mgr_.get());\n  core::ScopedUnref rendez_unref(rendez);\n\n  // Insert the input tensors into the local rendezvous by their\n  // rendezvous key.\n  for (const auto& input : inputs) {\n    const string& input_key = executors_and_keys->input_keys[input.first];\n    s = rendez->Send(input_key, Rendezvous::Args(), input.second, false);\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      return s;\n    }\n  }\n\n  // Start parallel Executors.\n  Notification executors_done;\n  const int num_executors = executors_and_keys->items.size();\n  ExecutorBarrier* barrier = new ExecutorBarrier(\n      num_executors, rendez, [&executors_done, &s](const Status& ret) {\n        s = ret;\n        executors_done.Notify();\n      });\n\n  Executor::Args args;\n  args.rendezvous = rendez;\n  args.cancellation_manager = cancellation_manager_;\n  args.runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n\n  // \u3053\u3053\u3067\u5b9f\u884c\u51e6\u7406\u3092\u3057\u3066\u3044\u308b\n  // item.executor\u306e\u5185\u5bb9\u306f\u3001\u5f8c\u3067\u51fa\u3066\u304f\u308b\u306e\u3067\u5f8c\u306b\u6574\u7406\u3057\u307e\u3059\u3002\n  // item.executor\u306f\u3001ExecutorImpl\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30dd\u30a4\u30f3\u30bf\u3067\u3042\u308b\u3002\n  for (const auto& item : executors_and_keys->items) {\n    item.executor->RunAsync(args, barrier->Get());\n  }\n\n  // \u30b9\u30ec\u30c3\u30c9\u304c\u7d42\u308f\u308b\u307e\u3067\u540c\u671f\u3059\u308b\u3002\n  executors_done.WaitForNotification();\n\n  TF_RETURN_IF_ERROR(s);\n\n  if (!output_names.empty()) {\n    outputs->resize(output_names.size());\n  }\n\n  // Get the outputs from the rendezvous\n  for (size_t output_offset = 0; output_offset < output_names.size();\n       ++output_offset) {\n    const string& output_key =\n        executors_and_keys->output_keys[output_names[output_offset]];\n    Tensor output_tensor;\n    bool is_dead;\n\n    // Fetch data from the Rendezvous.\n    s = rendez->Recv(output_key, Rendezvous::Args(), &output_tensor, &is_dead);\n    if (is_dead) {\n      s = errors::InvalidArgument(\"The tensor returned for \",\n                                  output_names[output_offset],\n                                  \" was not valid.\");\n    }\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      outputs->clear();\n      return s;\n    }\n\n    (*outputs)[output_offset] = output_tensor;\n  }\n\n  return s;\n}\n\nStatus DirectSession::GetOrCreateExecutors(\n    gtl::ArraySlice<string> inputs, gtl::ArraySlice<string> outputs,\n    gtl::ArraySlice<string> target_nodes,\n    ExecutorsAndKeys** executors_and_keys) {\n  // Sort the inputs and outputs, so we don't create separate\n  // executors when a user passes in the same inputs/outputs in\n  // different orders.\n  //\n  // We could consider some other signature instead of sorting that\n  // preserves the same property to avoid the sort in the future.\n  std::vector<string> inputs_sorted(inputs.begin(), inputs.end());\n  std::vector<string> outputs_sorted(outputs.begin(), outputs.end());\n  std::vector<string> tn_sorted(target_nodes.begin(), target_nodes.end());\n  std::sort(inputs_sorted.begin(), inputs_sorted.end());\n  std::sort(outputs_sorted.begin(), outputs_sorted.end());\n  std::sort(tn_sorted.begin(), tn_sorted.end());\n\n  const string key = strings::StrCat(str_util::Join(inputs_sorted, \",\"), \"->\",\n                                     str_util::Join(outputs_sorted, \",\"), \"/\",\n                                     str_util::Join(tn_sorted, \",\"));\n\n  // See if we already have the executors for this run.\n  {\n    mutex_lock l(executor_lock_);  // could use reader lock\n    auto it = executors_.find(key);\n    if (it != executors_.end()) {\n      *executors_and_keys = it->second;\n      return Status::OK();\n    }\n  }\n\n  // The executor_lock_ is intentionally released while executor is\n  // being created.\n  FunctionLibraryDefinition* fdefs;\n  std::unordered_map<string, Graph*> graphs;\n  Status s = CreateGraphs(inputs, outputs, target_nodes, &fdefs, &graphs);\n  if (!s.ok()) {\n    return s;\n  }\n\n  bool has_control_flow = false;\n  for (const auto& graph : graphs) {\n    for (const Node* n : graph.second->nodes()) {\n      if (IsControlFlow(n)) {\n        has_control_flow = true;\n        break;\n      }\n    }\n    if (has_control_flow) break;\n  }\n\n  std::unique_ptr<ExecutorsAndKeys> ek(new ExecutorsAndKeys);\n  ek->func_defs = fdefs;\n  ek->items.reserve(graphs.size());\n  auto runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n  for (const auto& graph : graphs) {\n    const string& partition_name = graph.first;\n    Graph* partition_graph = graph.second;\n    const int graph_def_version = partition_graph->version();\n\n    Device* device;\n    s = device_mgr_->LookupDevice(partition_name, &device);\n    if (!s.ok()) {\n      return s;\n    }\n\n    ek->items.resize(ek->items.size() + 1);\n    auto* item = &(ek->items.back());\n    // \u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u3092\u767b\u9332\u3057\u307e\u3059\u3002\n    // \u30ab\u30fc\u30cd\u30eb\u4f5c\u6210\u7528\u306e\u95a2\u6570\u306a\u3069\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u8a2d\u7f6e\u3057\u307e\u3059\u3002\n    item->flib =\n        NewFunctionLibraryRuntime(device, runner, graph_def_version, fdefs);\n\n    LocalExecutorParams params;\n    params.has_control_flow = has_control_flow;\n    params.device = device;\n    params.function_library = item->flib;\n    auto lib = item->flib;\n    auto opseg = device->op_segment();\n\n    // \u30ab\u30fc\u30cd\u30eb\u4f5c\u6210\u95a2\u6570\u3067\u3059\u3002\n    // create_kernel\u3092\u95a2\u6570\u306e\u69d8\u306b\u66f8\u304f\u3053\u3068\u3067\u95a2\u6570\u3068\u3057\u3066\u30b3\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n    // OpKernel* pKernel;\n    // params.create_kernel(pKernel);\n    params.create_kernel = [this, lib, opseg](const NodeDef& ndef,\n                                              OpKernel** kernel) {\n      auto create_fn = [lib, &ndef](OpKernel** kernel) {\n        return lib->CreateKernel(ndef, kernel);\n      };\n      // Kernels created for subgraph nodes need to be cached.  On\n      // cache miss, create_fn() is invoked to create a kernel based\n      // on the function library here + global op registry.\n      return opseg->FindOrCreate(session_handle_, ndef.name(), kernel,\n                                 create_fn);\n    };\n    // \u30ab\u30fc\u30cd\u30eb\u524a\u9664\u95a2\u6570\u3067\u3059\u3002 (\u4f55\u3082\u3084\u3063\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u306d\u2026)\n    // create_kernel\u3092\u95a2\u6570\u306e\u69d8\u306b\u66f8\u304f\u3053\u3068\u3067\u95a2\u6570\u3068\u3057\u3066\u30b3\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n    params.delete_kernel = [](OpKernel* kernel) {\n      // Do nothing because 'kernel' is owned by opseg above.\n    };\n\n    // \u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30af\u30e9\u30b9\u3092item->executor\u306b\u4f5c\u6210\u3059\u308b\n    s = NewLocalExecutor(params, partition_graph, &item->executor);\n    if (!s.ok()) {\n      return s;\n    }\n  }\n\n  // \u4eca\u56de\u306f\u3001\u91cd\u8981\u3067\u306f\u306a\u3044\u306e\u3067\u7701\u7565\n\n  return Status::OK();\n}\n```\n\n**2.3 \u30ab\u30fc\u30cd\u30eb\u306e\u4f5c\u6210\u30fb\u521d\u671f\u5316**\n\u30ab\u30fc\u30cd\u30eb\u306e\u521d\u671f\u5316\u3092\u3053\u306e\u4e2d\u3067\u3084\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u3053\u304b\u3089NewLocalExecutor\u5185\u90e8\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\nnew\u6f14\u7b97\u5b50\u3092\u4f7f\u7528\u3057\u3066\u3001\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3057\u305f\u5f8c\u306eInitilize\u306e\u4e2d\u3067\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\nSource URL: [executor.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc)\nLine: 2130 - 2140\n\n```cpp:executor.cc\nStatus NewLocalExecutor(const LocalExecutorParams& params, const Graph* graph,\n                        Executor** executor) {\n  ExecutorImpl* impl = new ExecutorImpl(params, graph);\n  // \u3053\u306e\u4e2d\u3067\u30ab\u30fc\u30cd\u30eb\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n  Status s = impl->Initialize();\n  if (s.ok()) {\n    *executor = impl;\n  } else {\n    delete impl;\n  }\n  return s;\n}\n```\n\nExecutorImpl::Initialize()\u306e\u4e2d\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nLine: 244 - 280\n\n```cpp:executor.cc\nStatus ExecutorImpl::Initialize() {\n  const int num_nodes = graph_->num_node_ids();\n  nodes_.resize(num_nodes);\n\n  Status s;\n  total_tensors_ = 0;\n\n  // Preprocess every node in the graph to create an instance of op\n  // kernel for each node;\n  for (const Node* n : graph_->nodes()) {\n    const int id = n->id();\n    NodeItem* item = &nodes_[id];\n    item->node = n;\n    item->input_start = total_tensors_;\n    total_tensors_ += n->num_inputs();\n    // \u3053\u3053\u3067\u30ab\u30fc\u30cd\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n    s = params_.create_kernel(n->def(), &item->kernel);\n    if (!s.ok()) {\n      // \u306a\u306b\u3084\u3089\u30ce\u30fc\u30c9\u306e\u30a2\u30bf\u30c3\u30c1\u3092\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u95a2\u4fc2\u306a\u3044\u305f\u3081\u672a\u78ba\u8a8d\u3067\u3059\u3002\n      s = AttachDef(s, n->def());\n      LOG(ERROR) << \"Executor failed to create kernel. \" << s;\n      break;\n    }\n    CHECK(item->kernel);\n\n    // Initialize static information about the frames in the graph.\n    if (IsEnter(n)) {\n      string frame_name;\n      s = GetNodeAttr(n->def(), \"frame_name\", &frame_name);\n      if (!s.ok()) return s;\n      ++frame_input_count_[frame_name];\n    }\n  }\n  if (params_.has_control_flow) {\n    VLOG(2) << \"Graph has control flow.\";\n  }\n  if (!s.ok()) return s;\n  return SetAllocAttrs();\n}\n```\n\n**2.4 \u30ab\u30fc\u30cd\u30eb\u306e\u65b0\u898f\u4f5c\u6210**\nparams_.create_kernel(n->def(), &item->kernel);\n\u304c\u3001\u3069\u3053\u306b\u7e4b\u304c\u308b\u304b\u3068\u8a00\u3046\u3068\u3001\nSource URL: [direct_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc)\nLine: 380 -390\n\n```cpp:direct_session.cc\n    params.create_kernel = [this, lib, opseg](const NodeDef& ndef,\n                                              OpKernel** kernel) {\n      // \u3053\u306e\u4e2d\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n\n      // create_fn\u306bCreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u3092\u30a2\u30bf\u30c3\u30c1\u3059\u308b\u3002\n      auto create_fn = [lib, &ndef](OpKernel** kernel) {\n        return lib->CreateKernel(ndef, kernel);\n      };\n      // Kernels created for subgraph nodes need to be cached.  On\n      // cache miss, create_fn() is invoked to create a kernel based\n      // on the function library here + global op registry.\n\n      // \u30c7\u30d0\u30a4\u30b9\u304b\u3089\u53d6\u5f97\u3057\u305f\u30bb\u30b0\u30e1\u30f3\u30c8\u304b\u3089\u8d77\u52d5\u6e08\u307f\u306e\u30c7\u30d0\u30a4\u30b9\u3092\u691c\u7d22\u307e\u305f\u306f\n      // CreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u304b\u3092\u5b9f\u884c\u3059\u308b\n      return opseg->FindOrCreate(session_handle_, ndef.name(), kernel,\n                                 create_fn);\n    };\n```\n\n**2.5 \u30ab\u30fc\u30cd\u30eb\u306e\u691c\u7d22\u53ca\u3073\u65b0\u898f\u4f5c\u6210**\nOpSegment::FindOrCreate\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n\u3059\u3067\u306b\u8d77\u52d5\u3057\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u4f5c\u6210\u3057\u3066\u3057\u3066\u3044\u308b\u30ab\u30fc\u30cd\u30eb\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u8fd4\u3057\u307e\u3059\u3002\n\u521d\u56de\u8d77\u52d5\u7b49\u3067\u3001\u307e\u3060\u4f5c\u6210\u3057\u3066\u3044\u306a\u3044\u30b1\u30fc\u30b9\u306fCreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\nSource URL: [op_segment.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_segment.cc)\nLine: 36 - 70\n\n```cpp:op_segment.cc\nStatus OpSegment::FindOrCreate(const string& session_handle,\n                               const string& node_name, OpKernel** kernel,\n                               CreateKernelFn create_fn) {\n  // \u30b9\u30ec\u30c3\u30c9\u306e\u30c7\u30c3\u30c9\u30ed\u30c3\u30af\u5bfe\u7b56\u306e\u305f\u3081\u306e\u30df\u30e5\u30fc\u30c6\u30c3\u30af\u30b9 - ON\n  {\n    mutex_lock l(mu_);\n    auto item = gtl::FindPtrOrNull(sessions_, session_handle);\n    if (item == nullptr) {\n      return errors::NotFound(\"Session \", session_handle, \" is not found.\");\n    }\n    // SymbolicGradientHelper\u7d4c\u7531\u3067\u4f5c\u6210\u3055\u308c\u305fNode\u3067\u306f\u306a\u304f\u3001\n    // \u30ab\u30fc\u30cd\u30eb\u540d\u3068\u30ce\u30fc\u30c9\u540d\u3067\u691c\u7d22\u3057\u3066\u3001\u30de\u30c3\u30c1\u3057\u305f\u5834\u5408\u306f\u30ab\u30fc\u30cd\u30eb\u306e\u30dd\u30a4\u30f3\u30bf\u3092\u4f7f\u7528\u3059\u308b\u3002\n    *kernel = gtl::FindPtrOrNull(item->name_kernel, node_name);\n    if (*kernel != nullptr) {\n      return Status::OK();\n    }\n  }\n\n  // \u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001CreateKernel\u95a2\u6570\u3092\u30b3\u30fc\u30eb\u3059\u308b\n  // \u3053\u306e\u4e2d\u8eab\u306f\u3001\u300clib->CreateKernel(ndef, kernel)\u300d\u306b\u306a\u308a\u307e\u3059\u3002\n  // lib\u306f\u3001FunctionLibraryRuntimeImpl\u306a\u306e\u3067\u3001\u305d\u3061\u3089\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\n  // \u7406\u7531\u306f\u3001NewFunctionLibraryRuntime\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002\n  Status s = create_fn(kernel);\n  if (!s.ok()) {\n    LOG(ERROR) << \"Create kernel failed: \" << s;\n    return s;\n  }\n\n  // \u30b9\u30ec\u30c3\u30c9\u306e\u30c7\u30c3\u30c9\u30ed\u30c3\u30af\u5bfe\u7b56\u306e\u305f\u3081\u306e\u30df\u30e5\u30fc\u30c6\u30c3\u30af\u30b9 - OFF\n  {\n    mutex_lock l(mu_);\n    auto item = gtl::FindPtrOrNull(sessions_, session_handle);\n    if (item == nullptr) {\n      return errors::NotFound(\"Session \", session_handle, \" is not found.\");\n    }\n    OpKernel** p_kernel = &(item->name_kernel[node_name]);\n    if (*p_kernel == nullptr) {\n      *p_kernel = *kernel;  // Inserts 'kernel' in the map.\n    } else {\n      delete *kernel;\n      *kernel = *p_kernel;\n    }\n  }\n  return Status::OK();\n}\n```\n\n**\u88dc\u8db3**\n> gtl::FindPtrOrNull\u306f\u3001\u4ee5\u4e0b\u306e\u30d8\u30c3\u30c0\u30fc\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n> Source URL: [map_util.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/gtl/map_util.h)\n> \n> NewFunctionLibraryRuntime\u95a2\u6570\u306f\u3001\u4ee5\u4e0b\u306e\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n> Source URL: [function.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/function.cc)\n\nFunctionLibraryRuntimeImpl::CreateKernel\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nSource URL: Source URL: [function.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/function.cc)\nLine: 374 - 401\n\n```cpp:function.cc\nStatus FunctionLibraryRuntimeImpl::CreateKernel(const NodeDef& ndef,\n                                                OpKernel** kernel) {\n  // lib_def_\u306f\u3001DirectSession::CreateGraphs\u95a2\u6570\u3067\u4f5c\u6210\u3057\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u95a2\u6570\u30dd\u30a4\u30f3\u30bf\u7fa4\u3068\u601d\u308f\u308c\u308b\u304c\u3001\n  // \u8a73\u7d30\u306f\u3001\u8907\u96d1\u306b\u306a\u308a\u305d\u3046\u306a\u306e\u3067\u5225\u306e\u6a5f\u4f1a\u3068\u3059\u308b\u3002\n  // ndef\u306f\u3001Node\u95a2\u9023\u3060\u308d\u3046\u304c\u30b3\u30c1\u30e9\u3082\u8a73\u7d30\u306f\u5225\u306e\u6a5f\u4f1a\u306b\u89e3\u6790\u3059\u308b\u3002\n  if (ndef.op() != kGradientOp && (lib_def_->Find(ndef.op()) == nullptr)) {\n    return CreateNonCachedKernel(device_, this, ndef, graph_def_version_,\n                                 kernel);\n  }\n\n  // Try to instantiate this function for the func/attr. Maybe its\n  // cached already.\n  // \u95a2\u6570\u3068\u5c5e\u6027\u304b\u3089\u306e\u95a2\u6570\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3092\u8a66\u307f\u308b\n  // \u8a73\u7d30\u306a\u4ed5\u7d44\u307f\u306f\u3001\u6b21\u306e\u6a5f\u4f1a\u306b\u5b9f\u65bd\u3059\u308b\n  Handle handle;\n  TF_RETURN_IF_ERROR(Instantiate(ndef.op(), ndef.attr(), &handle));\n\n  const FunctionBody* fbody = GetFunctionBody(handle);\n  CHECK_NOTNULL(fbody);\n\n  // Constructs a CallOp kernel for running the instantiated function.\n  // CallOp(Call Operation\u306e\u3053\u3068\uff1f)\u30ab\u30fc\u30cd\u30eb\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u8d77\u52d5\u3059\u308b\u3002\n  // \u3053\u306e\u8fba\u308a\u3082\u3001\u6b21\u306e\u6a5f\u4f1a\u3068\u3059\u308b\u3002\n  Status s;\n  auto device_type = DeviceType(device_->attributes().device_type());\n  OpKernelConstruction construction(\n      device_type, device_, device_->GetAllocator(AllocatorAttributes()), &ndef,\n      &fbody->fdef.signature(), this, fbody->arg_types, fbody->ret_types,\n      graph_def_version_, &s);\n  *kernel = new CallOp(handle, &construction);\n  if (!s.ok()) {\n    delete kernel;\n  }\n  return s;\n}\n```\n\n**2.6 Executor\u3092\u5b9f\u884c**\n2.2\u307e\u3067\u623b\u3063\u3066\u30012.3\u3067\u4f5c\u6210\u3057\u305fExecutor\u3092\u975e\u540c\u671f\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u305d\u306e\u6d41\u308c\u3092\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\nSource URL: [direct_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc)\nLine: 208 - 427\n\n```cpp:direct_session.cc\nStatus DirectSession::Run(const std::vector<std::pair<string, Tensor>>& inputs,\n                          const std::vector<string>& output_names,\n                          const std::vector<string>& target_nodes,\n                          std::vector<Tensor>* outputs) {\n  {\n    mutex_lock l(graph_def_lock_);\n    if (!graph_created_) {\n      return errors::InvalidArgument(\n          \"Session was not created with a graph before Run()!\");\n    }\n  }\n\n  // Extract the inputs names for this run of the session.\n  std::vector<string> input_tensor_names;\n  input_tensor_names.reserve(inputs.size());\n  for (const auto& it : inputs) {\n    input_tensor_names.push_back(it.first);\n  }\n\n  // Check if we already have an executor for these arguments.\n  ExecutorsAndKeys* executors_and_keys;\n  Status s = GetOrCreateExecutors(input_tensor_names, output_names,\n                                  target_nodes, &executors_and_keys);\n  if (!s.ok()) {\n    return s;\n  }\n\n  // -----------------------\u3053\u3053\u307e\u3067 2.2 ----------------------------\n\n  IntraProcessRendezvous* rendez =\n      new IntraProcessRendezvous(device_mgr_.get());\n  core::ScopedUnref rendez_unref(rendez);\n\n  // Insert the input tensors into the local rendezvous by their\n  // rendezvous key.\n  for (const auto& input : inputs) {\n    const string& input_key = executors_and_keys->input_keys[input.first];\n    s = rendez->Send(input_key, Rendezvous::Args(), input.second, false);\n    if (!s.ok()) {\n      rendez->StartAbort(s);\n      return s;\n    }\n  }\n\n  // Start parallel Executors.\n  Notification executors_done;\n  const int num_executors = executors_and_keys->items.size();\n  ExecutorBarrier* barrier = new ExecutorBarrier(\n      num_executors, rendez, [&executors_done, &s](const Status& ret) {\n        s = ret;\n        executors_done.Notify();\n      });\n\n  Executor::Args args;\n  args.rendezvous = rendez;\n  args.cancellation_manager = cancellation_manager_;\n  args.runner = [this](Executor::Args::Closure c) { SchedClosure(c); };\n\n  // \u30dd\u30a4\u30f3\u30c8\u306f\u3001\u3053\u3053\u3067\u3059\u3002\n  // \u3053\u3053\u3067\u3001executor\u3092\u975e\u540c\u671f\u5b9f\u884c\u3057\u307e\u3059\u3002\n  for (const auto& item : executors_and_keys->items) {\n    item.executor->RunAsync(args, barrier->Get());\n  }\n\n  executors_done.WaitForNotification();\n\n  // \u4eca\u56de\u306f\u3001\u7121\u7528\u306a\u306e\u3067\u7701\u7565\n\n  return s;\n}\n```\n\n\u3067\u306f\u3001item.executor->RunAsync\u306e\u4e2d\u3092\u8ffd\u3044\u307e\u3057\u3087\u3046\u3002\nExecutorImpl::RunAsync\u306b\u7e4b\u304c\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u3053\u3061\u3089\u306e\u30b3\u30fc\u30c9\u306b\u79fb\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\n\nSource URL: [executor.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc)\nLine: 2118 - 2125\n\n```cpp:executor.cc\n// NOTE(yuanbyu): Use the executor that supports control flow by default.\nconst bool use_control_flow_executor = true;\nvoid ExecutorImpl::RunAsync(const Args& args, DoneCallback done) {\n  // ControlFlow\u3068\u3044\u3046\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u3067\u3057\u3087\u3046\u304b\uff1f\n  // \u5168\u4f53\u898b\u3066\u306a\u3044\u306e\u3067\u3001\u307e\u3060\u4e0d\u660e\u3067\u3059\u3002 (\u5206\u304b\u308a\u6b21\u7b2c\u8ffd\u8a18\u3057\u307e\u3059\u3002)\n  if (params_.has_control_flow || use_control_flow_executor) {\n    (new ExecutorState(args, this))->RunAsync(done);\n  } else {\n    // \u4eca\u56de\u306f\u3001\u3053\u3061\u3089\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u8ffd\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n    (new SimpleExecutorState(args, this))->RunAsync(done);\n  }\n}\n```\n\nSimpleExecutorState::RunAsync\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002\nLine: 1794 - 1819\n\n```cpp:executor.cc\nvoid SimpleExecutorState::RunAsync(Executor::DoneCallback done) {\n  const Graph* graph = impl_->graph_;\n  ReadyNodeIds ready;\n\n  // Ask the device to fill in the device context map.\n  Device* device = impl_->params_.device;\n  device->FillContextMap(graph, &device_context_map_);\n\n  // \u4e2d\u8eab\u307e\u3067\u898b\u5207\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u63a8\u6e2c\u3067\u3059\u304c\u3001\u30ce\u30fc\u30c9\u306b\u30ea\u30f3\u30af\u3055\u308c\u3066\u3044\u308b\u753b\u50cf\u3092\u89e3\u6790\u3057\u3066\n  // \u4e2d\u306b\u30a8\u30c3\u30c2\u304c\u3042\u308b\u304b\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u3044\u308b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n  // \u30a8\u30c3\u30c2\u304c\u7121\u3044\u753b\u50cf\u304c\u4e00\u3064\u3067\u3082\u3042\u308c\u3070\u51e6\u7406\u3092\u7d9a\u884c\u3059\u308b\u3002 (\u3053\u306e\u5834\u5408\u306e\u51e6\u7406\u3068\u306f\u3001ExecutorBarrier\u306e\u3053\u3068)\n  // \u30a8\u30c3\u30c2\u304c\u3059\u3079\u3066\u3042\u308b\u5834\u5408\u306f\u3001ScheduleReady\u306b\u5165\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n  // \u3053\u3053\u3067\u306e\u30a8\u30c3\u30c2\u306e\u5b9a\u7fa9\u306f\u3001\u753b\u50cf\u306b\u95a2\u9023\u4ed8\u3044\u3066\u3044\u308b\u30ce\u30fc\u30c9\u306e\u6570\u3068\u601d\u308f\u308c\u308b\u3002\n\u3000// \u7406\u7531\u306f\u3001\u88dc\u8db3\u306b\u8a18\u8f09\u3059\u308b\u3002\n  for (const Node* n : graph->nodes()) {\n    const int id = n->id();\n    const int num_in_edges = n->in_edges().size();\n    pending_[id].Set(num_in_edges);\n    if (num_in_edges == 0) {\n      ready.push_back(id);\n    }\n  }\n  if (ready.empty()) {\n    done(Status::OK());\n  } else {\n    num_active_ = ready.size();\n    done_cb_ = done;\n    input_tensors_.resize(impl_->total_tensors_);\n    // Schedule to run all the ready ops in thread pool.\n    ScheduleReady(ready, nullptr);\n  }\n}\n```\n\n**\u88dc\u8db3**\n> \u4ee5\u4e0b\u306e\u30bd\u30fc\u30b9\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u3001Graph::AddEdge\u95a2\u6570\u306b\u3066\u633f\u5165\u51e6\u7406\u3057\u3066\u3044\u307e\u3059\u3002\n> \u633f\u5165\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u304cNode\u306e\u5897\u6e1b\u306b\u4f34\u3063\u3066\u3044\u308b\u3053\u3068\u304b\u3089, \u753b\u50cf\u3068\u30ea\u30f3\u30af\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u306e\u6570\u3068\u63a8\u6e2c\u3057\u307e\u3057\u305f\u3002\n> Source URL: [graph.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph.cc)\n> \n> ExecutorBarrier\u306e\u5b9a\u7fa9\u306f\u3001\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\n> Source URL: [executor.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.h)\n\n\n# **3. \u7d50\u8ad6**\n  \u73fe\u5728\u306f\u3001\u30b9\u30ec\u30c3\u30c9\u4f5c\u6210\u3059\u308b\u30eb\u30fc\u30c8\u306f\u4f7f\u7528\u3055\u308c\u3066\u304a\u3089\u305a\u30c7\u30d0\u30c3\u30b0\u4e2d\u3089\u3057\u3044\u2026\n  \u9577\u304f\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u4e00\u65e6\u3053\u3053\u3067\u533a\u5207\u308a\u307e\u3059\u3002\n  \u3050\u3061\u3083\u3050\u3061\u3083\u3067\u3059\u307f\u307e\u305b\u3093\u3002 :fearful: \n\n\u6b21\u56de\u306f\u3001\u300eOpKernelContext\u3092\u4f5c\u6210\u300f\u304b\u3089GPU\u306e\u8d77\u52d5\u30fb\u30a2\u30bf\u30c3\u30c1\u307e\u3067\u306e\u6d41\u308c\u3092\u30bd\u30fc\u30b9\u3068\u7167\u3089\u3057\u5408\u308f\u305b\u3064\u3064\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\n\u521d\u3081\u3066\u66f8\u3044\u305f\u306e\u3067\u5206\u304b\u308a\u3065\u3089\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u610f\u898b\u3084\u8981\u671b\u7b49\u3042\u308a\u307e\u3057\u305f\u3089\u3001\u9060\u616e\u306a\u304f\u4e0b\u3055\u3044\u3002\n\u304a\u5f85\u3061\u3057\u3066\u3044\u307e\u3059\u3002\n", "tags": ["TensorFlow", "C++", "kernel", "CUDA"]}