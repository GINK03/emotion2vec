{"context": "\n\n\u306f\u3058\u3081\u306b\nDQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b\u3068\u3044\u3046\u5927\u5909\u7d20\u6674\u3089\u3057\u3044\u8a18\u4e8b\u3092\u8aad\u3093\u3067\u5b9f\u969b\u306bDQN\u3092\u52d5\u304b\u3057\u3066\u307f\u305f\u304f\u306a\u3063\u305f\u3002\u5f37\u5316\u5b66\u7fd2\u306f\u305d\u308c\u307b\u3069\u8208\u5473\u304c\u306a\u304b\u3063\u305f\u3093\u3060\u3051\u3069\u30d6\u30ed\u30c3\u30af\u5d29\u3057\u304c\u3060\u3093\u3060\u3093\u304b\u3057\u3053\u304f\u306a\u3063\u3066\u3044\u304f\u306e\u3092\u5b9f\u969b\u306b\u898b\u3066\u307f\u305f\u3044\u306a\u3068\u601d\u3063\u3066\u3002\n\u3067\u3001\u624b\u6301\u3061\u306eMac\u3067\u52d5\u304b\u3059\u3068\u3053\u308d\u307e\u3067\u3084\u3063\u305f\u3093\u3060\u3051\u3069\u4ed5\u4e8b\u306b\u4f7f\u3063\u3066\u308bPC\u306a\u306e\u3067\u9577\u6642\u9593\u5b66\u7fd2\u3055\u305b\u3089\u308c\u308b\u74b0\u5883\u3058\u3083\u306a\u3044\u3002\u5b66\u7fd2\u7528\u306b\u653e\u7f6e\u3057\u3066\u304a\u3051\u308b\u74b0\u5883\u3092\u3064\u304f\u308b\u305f\u3081\u3001Ubuntu Linux\u3067OpenAI Gym&TensorFlow&Keras\u306e\u74b0\u5883\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u3066\u8a18\u4e8b\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3042\u308belix-tech/dqn\u3092\u52d5\u304b\u3059\u3068\u3053\u308d\u307e\u3067\u3084\u3063\u3066\u307f\u305f\u3002\u30b3\u30fc\u30c9\u304cPython3.x\u3058\u3083\u52d5\u304b\u306a\u304f\u3066\u4fee\u6b63\u3057\u305f\u308a\u3057\u305f\u306e\u3067\u3001\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u30e1\u30e2\u3063\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\nUbuntu 16.04\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u76f4\u5f8c\u306e\u72b6\u614b\u304b\u3089\u3002\n\u307e\u305a\u306fUbuntu\u3092\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u3066\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\ncd ~\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get -y install git gcc make openssl libssl-dev libbz2-dev libreadline-dev libsqlite3-dev  python3-tk tk-dev python-tk libfreetype6-dev python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n\n\u6b21\u306bpyenv, virtualenv\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\ngit clone https://github.com/yyuu/pyenv.git ~/.pyenv\ngit clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv\n\n~/.bash_profile \u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\u3059\u308b\u3002\n# pyenv\nexport PYENV_ROOT=$HOME/.pyenv\nexport PATH=$PYENV_ROOT/bin:$PATH\neval \"$(pyenv init -)\"\n# virtualenv\neval \"$(pyenv virtualenv-init -)\"\nexport PYENV_VIRTUALENV_DISABLE_PROMPT=1\n\n.bash_profile\u3092\u53cd\u6620\u3059\u308b\u3002\nsource ~/.bash_profile\n\nPython\u4eee\u60f3\u74b0\u5883\u3092\u3064\u304f\u308b\npyenv install 3.5.3\npyenv virtualenv 3.5.3 gym\npyenv activate gym\npython -V\nPython 3.5.3\n\nTensorFlow\u3001Keras\u4ed6\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u3068\u308a\u3042\u3048\u305a\u4eca\u56de\u306fGPU\u304c\u4f7f\u3048\u306a\u3044\u4eee\u60f3\u30de\u30b7\u30f3\u3067\u7acb\u3066\u305f\u306e\u3067TensorFlow\u306fCPU\u306b\u3057\u307e\u3057\u305f\u3002tensorflow-gpu\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f\u3053\u3061\u3089\u3092\u53c2\u7167\u3059\u308b\u306a\u3069\u3057\u3066\u5225\u9014CUDA\u306e\u74b0\u5883\u3092\u6574\u3048\u3066\u304f\u3060\u3055\u3044\u3002\npip install numpy\npip install h5py\npip install pillow\npip install matplotlib\npip install pandas\npip install ipython\npip install scipy\npip install sympy\npip install nose\npip install scikit-image\npip install tensorflow\npip install keras\n\nOpenAI Gym\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u4eca\u56de\u306e\u8a13\u7df4\u74b0\u5883\u306fATARI\u306e\u30b2\u30fc\u30e0\u306e\u307f\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3002\u5168\u90e8\u5165\u308c\u305f\u3044\u3068\u304d\u306fpip install -e '.[all]'\u3059\u308c\u3070\u3044\u3044\u307f\u305f\u3044\u3002\ngit clone https://github.com/openai/gym.git\ncd gym\npip install -e .\npip install -e '.[atari]'\n\n\u3046\u307e\u304f\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3067\u304d\u305f\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001\u30a4\u30f3\u30d9\u30fc\u30c0\u30fc\u30b2\u30fc\u30e0\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\ncd ..\nvi invaders.py\n\n\ninvaders.py\nimport gym\nenv = gym.make('SpaceInvaders-v0')\nenv.reset()\nfor _ in range(10000):\n    env.render()\n    env.step(env.action_space.sample())\n\n\npython invaders.py\n\n\u52d5\u3044\u305f\u3002\n\nOpenAI Gym\u304c\u3061\u3083\u3093\u3068\u52d5\u3044\u305f\u306e\u3067\u3001DQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b\u306e\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\n\u307e\u305a\u306f~/.keras/keras.json\u3092\u7de8\u96c6\u3057\u3066\u914d\u5217\u9806\u3092Theano\u4ed5\u69d8\u306b\u5909\u66f4\u3059\u308b\u3002\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u304c\u305d\u3046\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3002\n{\n    \"image_dim_ordering\": \"th\",\n    \"epsilon\": 1e-07,\n    \"backend\": \"tensorflow\",\n    \"floatx\": \"float32\"\n}\n\n\u6b21\u306b\u3001\u30b3\u30fc\u30c9\u3092clone\u3059\u308b\u3002\ngit clone https://github.com/elix-tech/dqn\ncd dqn\n\ndqn.py\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u307e\u308b\u3054\u3068\u7f6e\u304d\u63db\u3048\u308b\u3002gist\u306b\u3082\u8cbc\u3063\u3068\u304d\u307e\u3057\u305f\u3002\n\u5909\u66f4\u3057\u3066\u308b\u306e\u306f\u4e3b\u306b\u4ee5\u4e0b\u304c\u7406\u7531\u3002\n\nxrange\u306fPython3.x\u3067\u524a\u9664\u3055\u308c\u305f\nTensorFlow1.0\u3068\u305d\u308c\u4ee5\u524d\u3068\u3067\u30af\u30e9\u30b9\u4f53\u7cfb\u3068\u30e1\u30bd\u30c3\u30c9\u540d\u304c\u5909\u308f\u3063\u305f\u306e\u3067\u5927\u91cf\u306bAttributeError\u304c\u51fa\u308b\u3002TensorFlow \u306e \"AttributeError: 'module' object has no attribute 'xxxx'\" \u30a8\u30e9\u30fc\u3067\u3064\u307e\u3065\u3044\u3066\u3057\u307e\u3046\u4eba\u306e\u305f\u3081\u306e\u79fb\u884c\u30ac\u30a4\u30c9\u3092\u53c2\u8003\u306b\u30e1\u30bd\u30c3\u30c9\u540d\u3092\u66f8\u304d\u63db\u3048\u305f\u3002\n\n\u3042\u3068\u3001dqn.py\u306e37\u884c\u76ee\u3092TRAIN = False\u3068\u3057\u3066\u304a\u304f\u3002\u3068\u308a\u3042\u3048\u305a\u52d5\u304f\u304b\u30c6\u30b9\u30c8\u30e2\u30fc\u30c9\u3067\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3002\n\ndqn.py\n# coding:utf-8\n\nimport os\nimport gym\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom collections import deque\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, Flatten, Dense\n\nKERAS_BACKEND = 'tensorflow'\n\nENV_NAME = 'Breakout-v0'  # Environment name\nFRAME_WIDTH = 84  # Resized frame width\nFRAME_HEIGHT = 84  # Resized frame height\nNUM_EPISODES = 12000  # Number of episodes the agent plays\nSTATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\nGAMMA = 0.99  # Discount factor\nEXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\nINITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\nFINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\nINITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\nNUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\nBATCH_SIZE = 32  # Mini batch size\nTARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\nACTION_INTERVAL = 4  # The agent sees only every 4th input\nTRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\nLEARNING_RATE = 0.00025  # Learning rate used by RMSProp\nMOMENTUM = 0.95  # Momentum used by RMSProp\nMIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\nSAVE_INTERVAL = 300000  # The frequency with which the network is saved\nNO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\nLOAD_NETWORK = False\nTRAIN = False\nSAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\nSAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\nNUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time\n\n\nclass Agent():\n    def __init__(self, num_actions):\n        self.num_actions = num_actions\n        self.epsilon = INITIAL_EPSILON\n        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n        self.t = 0\n        self.repeated_action = 0\n\n        # Parameters used for summary\n        self.total_reward = 0\n        self.total_q_max = 0\n        self.total_loss = 0\n        self.duration = 0\n        self.episode = 0\n\n        # Create replay memory\n        self.replay_memory = deque()\n\n        # Create q network\n        self.s, self.q_values, q_network = self.build_network()\n        q_network_weights = q_network.trainable_weights\n\n        # Create target network\n        self.st, self.target_q_values, target_network = self.build_network()\n        target_network_weights = target_network.trainable_weights\n\n        # Define target network update operation\n        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n\n        # Define loss and gradient update operation\n        self.a, self.y, self.loss, self.grad_update = self.build_training_op(q_network_weights)\n\n        self.sess = tf.InteractiveSession()\n        self.saver = tf.train.Saver(q_network_weights)\n        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n        self.summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n\n        if not os.path.exists(SAVE_NETWORK_PATH):\n            os.makedirs(SAVE_NETWORK_PATH)\n\n        self.sess.run(tf.global_variables_initializer())\n\n        # Load network\n        if LOAD_NETWORK:\n            self.load_network()\n\n        # Initialize target network\n        self.sess.run(self.update_target_network)\n\n    def build_network(self):\n        model = Sequential()\n        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n        model.add(Flatten())\n        model.add(Dense(512, activation='relu'))\n        model.add(Dense(self.num_actions))\n\n        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n        q_values = model(s)\n\n        return s, q_values, model\n\n    def build_training_op(self, q_network_weights):\n        a = tf.placeholder(tf.int64, [None])\n        y = tf.placeholder(tf.float32, [None])\n\n        # Convert action to one hot vector\n        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n\n        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n        error = tf.abs(y - q_value)\n        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n        linear_part = error - quadratic_part\n        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n\n        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n        grad_update = optimizer.minimize(loss, var_list=q_network_weights)\n\n        return a, y, loss, grad_update\n\n    def get_initial_state(self, observation, last_observation):\n        processed_observation = np.maximum(observation, last_observation)\n        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n        state = [processed_observation for _ in range(STATE_LENGTH)]\n        return np.stack(state, axis=0)\n\n    def get_action(self, state):\n        action = self.repeated_action\n\n        if self.t % ACTION_INTERVAL == 0:\n            if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n                action = random.randrange(self.num_actions)\n            else:\n                action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n            self.repeated_action = action\n\n        # Anneal epsilon linearly over time\n        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n            self.epsilon -= self.epsilon_step\n\n        return action\n\n    def run(self, state, action, reward, terminal, observation):\n        next_state = np.append(state[1:, :, :], observation, axis=0)\n\n        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n        reward = np.sign(reward)\n\n        # Store transition in replay memory\n        self.replay_memory.append((state, action, reward, next_state, terminal))\n        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n            self.replay_memory.popleft()\n\n        if self.t >= INITIAL_REPLAY_SIZE:\n            # Train network\n            if self.t % TRAIN_INTERVAL == 0:\n                self.train_network()\n\n            # Update target network\n            if self.t % TARGET_UPDATE_INTERVAL == 0:\n                self.sess.run(self.update_target_network)\n\n            # Save network\n            if self.t % SAVE_INTERVAL == 0:\n                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=(self.t))\n                print('Successfully saved: ' + save_path)\n\n        self.total_reward += reward\n        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n        self.duration += 1\n\n        if terminal:\n            # Write summary\n            if self.t >= INITIAL_REPLAY_SIZE:\n                stats = [self.total_reward, self.total_q_max / float(self.duration),\n                        self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n                for i in range(len(stats)):\n                    self.sess.run(self.update_ops[i], feed_dict={\n                        self.summary_placeholders[i]: float(stats[i])\n                    })\n                summary_str = self.sess.run(self.summary_op)\n                self.summary_writer.add_summary(summary_str, self.episode + 1)\n\n            # Debug\n            if self.t < INITIAL_REPLAY_SIZE:\n                mode = 'random'\n            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n                mode = 'explore'\n            else:\n                mode = 'exploit'\n            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n                self.episode + 1, self.t, self.duration, self.epsilon,\n                self.total_reward, self.total_q_max / float(self.duration),\n                self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n\n            self.total_reward = 0\n            self.total_q_max = 0\n            self.total_loss = 0\n            self.duration = 0\n            self.episode += 1\n\n        self.t += 1\n\n        return next_state\n\n    def train_network(self):\n        state_batch = []\n        action_batch = []\n        reward_batch = []\n        next_state_batch = []\n        terminal_batch = []\n        y_batch = []\n\n        # Sample random minibatch of transition from replay memory\n        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n        for data in minibatch:\n            state_batch.append(data[0])\n            action_batch.append(data[1])\n            reward_batch.append(data[2])\n            next_state_batch.append(data[3])\n            terminal_batch.append(data[4])\n\n        # Convert True to 1, False to 0\n        terminal_batch = np.array(terminal_batch) + 0\n\n        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n\n        loss, _ = self.sess.run([self.loss, self.grad_update], feed_dict={\n            self.s: np.float32(np.array(state_batch) / 255.0),\n            self.a: action_batch,\n            self.y: y_batch\n        })\n\n        self.total_loss += loss\n\n    def setup_summary(self):\n        episode_total_reward = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Total Reward/Episode', episode_total_reward)\n        episode_avg_max_q = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Average Max Q/Episode', episode_avg_max_q)\n        episode_duration = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n        episode_avg_loss = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Average Loss/Episode', episode_avg_loss)\n        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n        summary_op = tf.summary.merge_all()\n        return summary_placeholders, update_ops, summary_op\n\n    def load_network(self):\n        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n        if checkpoint and checkpoint.model_checkpoint_path:\n            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n        else:\n            print('Training new network...')\n\n    def get_action_at_test(self, state):\n        action = self.repeated_action\n\n        if self.t % ACTION_INTERVAL == 0:\n            if random.random() <= 0.05:\n                action = random.randrange(self.num_actions)\n            else:\n                action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n            self.repeated_action = action\n\n        self.t += 1\n\n        return action\n\n\ndef preprocess(observation, last_observation):\n    processed_observation = np.maximum(observation, last_observation)\n    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n    return np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n\n\ndef main():\n    env = gym.make(ENV_NAME)\n    agent = Agent(num_actions=env.action_space.n)\n\n    if TRAIN:  # Train mode\n        for _ in range(NUM_EPISODES):\n            terminal = False\n            observation = env.reset()\n            for _ in range(random.randint(1, NO_OP_STEPS)):\n                last_observation = observation\n                observation, _, _, _ = env.step(0)  # Do nothing\n            state = agent.get_initial_state(observation, last_observation)\n            while not terminal:\n                last_observation = observation\n                action = agent.get_action(state)\n                observation, reward, terminal, _ = env.step(action)\n                # env.render()\n                processed_observation = preprocess(observation, last_observation)\n                state = agent.run(state, action, reward, terminal, processed_observation)\n    else:  # Test mode\n        # env.monitor.start(ENV_NAME + '-test')\n        for _ in range(NUM_EPISODES_AT_TEST):\n            terminal = False\n            observation = env.reset()\n            for _ in range(random.randint(1, NO_OP_STEPS)):\n                last_observation = observation\n                observation, _, _, _ = env.step(0)  # Do nothing\n            state = agent.get_initial_state(observation, last_observation)\n            while not terminal:\n                last_observation = observation\n                action = agent.get_action_at_test(state)\n                observation, _, terminal, _ = env.step(action)\n                env.render()\n                processed_observation = preprocess(observation, last_observation)\n                state = np.append(state[1:, :, :], processed_observation, axis=0)\n        # env.monitor.close()\n\n\nif __name__ == '__main__':\n    main()\n\n\n\u5b9f\u884c\u3059\u308b\u3002\npython dqn.py\n\n\u52d5\u3044\u305f\u3002\n\n\u5f37\u5316\u5b66\u7fd2\u3092\u3055\u305b\u3066\u307f\u308b\u3002dqn.py\u306e37\u884c\u76ee\u3092TRAIN = True\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3002\n\u52d5\u3044\u305f\u3002\u3061\u3083\u3093\u3068\u5b66\u7fd2\u3057\u3066\u308b\u307f\u305f\u3044\u3002\u9045\u3044\u3051\u3069\u3002\n\n\n\u307e\u3068\u3081\nDQN\u306e\u7406\u8ad6\u3068\u304b\u30b3\u30fc\u30c9\u306e\u4e2d\u8eab\u3068\u304b\u307b\u3068\u3093\u3069\u308f\u304b\u3063\u3066\u306a\u3044\u3051\u3069\u52d5\u304b\u3059\u3053\u3068\u304c\u3067\u304d\u305f\u306e\u3067AI\u306b\u30b2\u30fc\u30e0\u3092\u5b66\u7fd2\u3055\u305b\u306a\u304c\u3089\u30dc\u30af\u3082DQN\u306e\u5b66\u7fd2\u3092\u3057\u3066\u3044\u3053\u3046\u3068\u601d\u3044\u307e\u3059\u3002\u3042\u3089\u305f\u3081\u3066\u8a18\u4e8b\u3061\u3083\u3093\u3068\u8aad\u3082\u3002\n# \u306f\u3058\u3081\u306b\n\n[DQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html)\u3068\u3044\u3046\u5927\u5909\u7d20\u6674\u3089\u3057\u3044\u8a18\u4e8b\u3092\u8aad\u3093\u3067\u5b9f\u969b\u306bDQN\u3092\u52d5\u304b\u3057\u3066\u307f\u305f\u304f\u306a\u3063\u305f\u3002\u5f37\u5316\u5b66\u7fd2\u306f\u305d\u308c\u307b\u3069\u8208\u5473\u304c\u306a\u304b\u3063\u305f\u3093\u3060\u3051\u3069\u30d6\u30ed\u30c3\u30af\u5d29\u3057\u304c\u3060\u3093\u3060\u3093\u304b\u3057\u3053\u304f\u306a\u3063\u3066\u3044\u304f\u306e\u3092\u5b9f\u969b\u306b\u898b\u3066\u307f\u305f\u3044\u306a\u3068\u601d\u3063\u3066\u3002\n\n\u3067\u3001\u624b\u6301\u3061\u306eMac\u3067\u52d5\u304b\u3059\u3068\u3053\u308d\u307e\u3067\u3084\u3063\u305f\u3093\u3060\u3051\u3069\u4ed5\u4e8b\u306b\u4f7f\u3063\u3066\u308bPC\u306a\u306e\u3067\u9577\u6642\u9593\u5b66\u7fd2\u3055\u305b\u3089\u308c\u308b\u74b0\u5883\u3058\u3083\u306a\u3044\u3002\u5b66\u7fd2\u7528\u306b\u653e\u7f6e\u3057\u3066\u304a\u3051\u308b\u74b0\u5883\u3092\u3064\u304f\u308b\u305f\u3081\u3001Ubuntu Linux\u3067[OpenAI Gym](https://gym.openai.com/)&[TensorFlow](https://www.tensorflow.org/)&[Keras](https://keras.io/)\u306e\u74b0\u5883\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u3066\u8a18\u4e8b\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3042\u308b[elix-tech/dqn](https://github.com/elix-tech/dqn)\u3092\u52d5\u304b\u3059\u3068\u3053\u308d\u307e\u3067\u3084\u3063\u3066\u307f\u305f\u3002\u30b3\u30fc\u30c9\u304cPython3.x\u3058\u3083\u52d5\u304b\u306a\u304f\u3066\u4fee\u6b63\u3057\u305f\u308a\u3057\u305f\u306e\u3067\u3001\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u30e1\u30e2\u3063\u3066\u304a\u304d\u307e\u3059\u3002\n\n# \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\nUbuntu 16.04\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u76f4\u5f8c\u306e\u72b6\u614b\u304b\u3089\u3002\n\u307e\u305a\u306fUbuntu\u3092\u30a2\u30c3\u30d7\u30b0\u30ec\u30fc\u30c9\u3057\u3066\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\n\n```\ncd ~\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get -y install git gcc make openssl libssl-dev libbz2-dev libreadline-dev libsqlite3-dev  python3-tk tk-dev python-tk libfreetype6-dev python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n```\n\n\u6b21\u306bpyenv, virtualenv\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\n\n```\ngit clone https://github.com/yyuu/pyenv.git ~/.pyenv\ngit clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv\n```\n\n`~/.bash_profile` \u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\u3059\u308b\u3002\n\n```.bash_profile\n# pyenv\nexport PYENV_ROOT=$HOME/.pyenv\nexport PATH=$PYENV_ROOT/bin:$PATH\neval \"$(pyenv init -)\"\n# virtualenv\neval \"$(pyenv virtualenv-init -)\"\nexport PYENV_VIRTUALENV_DISABLE_PROMPT=1\n```\n\n.bash_profile\u3092\u53cd\u6620\u3059\u308b\u3002\n\n```\nsource ~/.bash_profile\n```\n\nPython\u4eee\u60f3\u74b0\u5883\u3092\u3064\u304f\u308b\n\n```\npyenv install 3.5.3\npyenv virtualenv 3.5.3 gym\npyenv activate gym\npython -V\nPython 3.5.3\n```\n\nTensorFlow\u3001Keras\u4ed6\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u3068\u308a\u3042\u3048\u305a\u4eca\u56de\u306fGPU\u304c\u4f7f\u3048\u306a\u3044\u4eee\u60f3\u30de\u30b7\u30f3\u3067\u7acb\u3066\u305f\u306e\u3067TensorFlow\u306fCPU\u306b\u3057\u307e\u3057\u305f\u3002`tensorflow-gpu`\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u306f[\u3053\u3061\u3089](http://qiita.com/bathtimefish/items/9f62042c7424486f514c)\u3092\u53c2\u7167\u3059\u308b\u306a\u3069\u3057\u3066\u5225\u9014CUDA\u306e\u74b0\u5883\u3092\u6574\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\n```\npip install numpy\npip install h5py\npip install pillow\npip install matplotlib\npip install pandas\npip install ipython\npip install scipy\npip install sympy\npip install nose\npip install scikit-image\npip install tensorflow\npip install keras\n```\n\nOpenAI Gym\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u4eca\u56de\u306e\u8a13\u7df4\u74b0\u5883\u306f[ATARI\u306e\u30b2\u30fc\u30e0](https://gym.openai.com/envs#atari)\u306e\u307f\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3002\u5168\u90e8\u5165\u308c\u305f\u3044\u3068\u304d\u306f`pip install -e '.[all]'`\u3059\u308c\u3070\u3044\u3044\u307f\u305f\u3044\u3002\n\n```\ngit clone https://github.com/openai/gym.git\ncd gym\npip install -e .\npip install -e '.[atari]'\n```\n\n\u3046\u307e\u304f\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3067\u304d\u305f\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001\u30a4\u30f3\u30d9\u30fc\u30c0\u30fc\u30b2\u30fc\u30e0\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\n\n```\ncd ..\nvi invaders.py\n```\n\n```invaders.py\nimport gym\nenv = gym.make('SpaceInvaders-v0')\nenv.reset()\nfor _ in range(10000):\n    env.render()\n    env.step(env.action_space.sample())\n```\n\n```\npython invaders.py\n```\n\n\u52d5\u3044\u305f\u3002\n\n![\u30ad\u30e3\u30d7\u30c1\u30e3.PNG](https://qiita-image-store.s3.amazonaws.com/0/61995/108042fc-8808-0e22-3e76-6ccef076a7ba.png)\n\nOpenAI Gym\u304c\u3061\u3083\u3093\u3068\u52d5\u3044\u305f\u306e\u3067\u3001[DQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html)\u306e\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\n\n\u307e\u305a\u306f`~/.keras/keras.json`\u3092\u7de8\u96c6\u3057\u3066\u914d\u5217\u9806\u3092Theano\u4ed5\u69d8\u306b\u5909\u66f4\u3059\u308b\u3002\u4e0a\u8a18\u306e\u30b3\u30fc\u30c9\u304c\u305d\u3046\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3002\n\n```\n{\n    \"image_dim_ordering\": \"th\",\n    \"epsilon\": 1e-07,\n    \"backend\": \"tensorflow\",\n    \"floatx\": \"float32\"\n}\n```\n\n\u6b21\u306b\u3001\u30b3\u30fc\u30c9\u3092clone\u3059\u308b\u3002\n\n```\ngit clone https://github.com/elix-tech/dqn\ncd dqn\n```\n\n`dqn.py`\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u306b\u307e\u308b\u3054\u3068\u7f6e\u304d\u63db\u3048\u308b\u3002[gist\u306b\u3082\u8cbc\u3063\u3068\u304d\u307e\u3057\u305f\u3002](https://gist.github.com/bathtimefish/6b555e674c5ac3cd34b2e868b75ee355)\n\n\u5909\u66f4\u3057\u3066\u308b\u306e\u306f\u4e3b\u306b\u4ee5\u4e0b\u304c\u7406\u7531\u3002\n\n* [xrange\u306fPython3.x\u3067\u524a\u9664\u3055\u308c\u305f](https://docs.python.jp/3/whatsnew/3.0.html)\n\n* TensorFlow1.0\u3068\u305d\u308c\u4ee5\u524d\u3068\u3067\u30af\u30e9\u30b9\u4f53\u7cfb\u3068\u30e1\u30bd\u30c3\u30c9\u540d\u304c\u5909\u308f\u3063\u305f\u306e\u3067\u5927\u91cf\u306bAttributeError\u304c\u51fa\u308b\u3002[TensorFlow \u306e \"AttributeError: 'module' object has no attribute 'xxxx'\" \u30a8\u30e9\u30fc\u3067\u3064\u307e\u3065\u3044\u3066\u3057\u307e\u3046\u4eba\u306e\u305f\u3081\u306e\u79fb\u884c\u30ac\u30a4\u30c9](http://qiita.com/shu223/items/ef160cbe1e9d9f57c248)\u3092\u53c2\u8003\u306b\u30e1\u30bd\u30c3\u30c9\u540d\u3092\u66f8\u304d\u63db\u3048\u305f\u3002\n\n\u3042\u3068\u3001`dqn.py`\u306e37\u884c\u76ee\u3092`TRAIN = False`\u3068\u3057\u3066\u304a\u304f\u3002\u3068\u308a\u3042\u3048\u305a\u52d5\u304f\u304b\u30c6\u30b9\u30c8\u30e2\u30fc\u30c9\u3067\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3002\n\n```dqn.py\n# coding:utf-8\n\nimport os\nimport gym\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom collections import deque\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, Flatten, Dense\n\nKERAS_BACKEND = 'tensorflow'\n\nENV_NAME = 'Breakout-v0'  # Environment name\nFRAME_WIDTH = 84  # Resized frame width\nFRAME_HEIGHT = 84  # Resized frame height\nNUM_EPISODES = 12000  # Number of episodes the agent plays\nSTATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\nGAMMA = 0.99  # Discount factor\nEXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\nINITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\nFINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\nINITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\nNUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\nBATCH_SIZE = 32  # Mini batch size\nTARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\nACTION_INTERVAL = 4  # The agent sees only every 4th input\nTRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\nLEARNING_RATE = 0.00025  # Learning rate used by RMSProp\nMOMENTUM = 0.95  # Momentum used by RMSProp\nMIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\nSAVE_INTERVAL = 300000  # The frequency with which the network is saved\nNO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\nLOAD_NETWORK = False\nTRAIN = False\nSAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\nSAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\nNUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time\n\n\nclass Agent():\n    def __init__(self, num_actions):\n        self.num_actions = num_actions\n        self.epsilon = INITIAL_EPSILON\n        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n        self.t = 0\n        self.repeated_action = 0\n\n        # Parameters used for summary\n        self.total_reward = 0\n        self.total_q_max = 0\n        self.total_loss = 0\n        self.duration = 0\n        self.episode = 0\n\n        # Create replay memory\n        self.replay_memory = deque()\n\n        # Create q network\n        self.s, self.q_values, q_network = self.build_network()\n        q_network_weights = q_network.trainable_weights\n\n        # Create target network\n        self.st, self.target_q_values, target_network = self.build_network()\n        target_network_weights = target_network.trainable_weights\n\n        # Define target network update operation\n        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n\n        # Define loss and gradient update operation\n        self.a, self.y, self.loss, self.grad_update = self.build_training_op(q_network_weights)\n\n        self.sess = tf.InteractiveSession()\n        self.saver = tf.train.Saver(q_network_weights)\n        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n        self.summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n\n        if not os.path.exists(SAVE_NETWORK_PATH):\n            os.makedirs(SAVE_NETWORK_PATH)\n\n        self.sess.run(tf.global_variables_initializer())\n\n        # Load network\n        if LOAD_NETWORK:\n            self.load_network()\n\n        # Initialize target network\n        self.sess.run(self.update_target_network)\n\n    def build_network(self):\n        model = Sequential()\n        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n        model.add(Flatten())\n        model.add(Dense(512, activation='relu'))\n        model.add(Dense(self.num_actions))\n\n        s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n        q_values = model(s)\n\n        return s, q_values, model\n\n    def build_training_op(self, q_network_weights):\n        a = tf.placeholder(tf.int64, [None])\n        y = tf.placeholder(tf.float32, [None])\n\n        # Convert action to one hot vector\n        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n\n        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n        error = tf.abs(y - q_value)\n        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n        linear_part = error - quadratic_part\n        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n\n        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n        grad_update = optimizer.minimize(loss, var_list=q_network_weights)\n\n        return a, y, loss, grad_update\n\n    def get_initial_state(self, observation, last_observation):\n        processed_observation = np.maximum(observation, last_observation)\n        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n        state = [processed_observation for _ in range(STATE_LENGTH)]\n        return np.stack(state, axis=0)\n\n    def get_action(self, state):\n        action = self.repeated_action\n\n        if self.t % ACTION_INTERVAL == 0:\n            if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n                action = random.randrange(self.num_actions)\n            else:\n                action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n            self.repeated_action = action\n\n        # Anneal epsilon linearly over time\n        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n            self.epsilon -= self.epsilon_step\n\n        return action\n\n    def run(self, state, action, reward, terminal, observation):\n        next_state = np.append(state[1:, :, :], observation, axis=0)\n\n        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n        reward = np.sign(reward)\n\n        # Store transition in replay memory\n        self.replay_memory.append((state, action, reward, next_state, terminal))\n        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n            self.replay_memory.popleft()\n\n        if self.t >= INITIAL_REPLAY_SIZE:\n            # Train network\n            if self.t % TRAIN_INTERVAL == 0:\n                self.train_network()\n\n            # Update target network\n            if self.t % TARGET_UPDATE_INTERVAL == 0:\n                self.sess.run(self.update_target_network)\n\n            # Save network\n            if self.t % SAVE_INTERVAL == 0:\n                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=(self.t))\n                print('Successfully saved: ' + save_path)\n\n        self.total_reward += reward\n        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n        self.duration += 1\n\n        if terminal:\n            # Write summary\n            if self.t >= INITIAL_REPLAY_SIZE:\n                stats = [self.total_reward, self.total_q_max / float(self.duration),\n                        self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n                for i in range(len(stats)):\n                    self.sess.run(self.update_ops[i], feed_dict={\n                        self.summary_placeholders[i]: float(stats[i])\n                    })\n                summary_str = self.sess.run(self.summary_op)\n                self.summary_writer.add_summary(summary_str, self.episode + 1)\n\n            # Debug\n            if self.t < INITIAL_REPLAY_SIZE:\n                mode = 'random'\n            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n                mode = 'explore'\n            else:\n                mode = 'exploit'\n            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n                self.episode + 1, self.t, self.duration, self.epsilon,\n                self.total_reward, self.total_q_max / float(self.duration),\n                self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n\n            self.total_reward = 0\n            self.total_q_max = 0\n            self.total_loss = 0\n            self.duration = 0\n            self.episode += 1\n\n        self.t += 1\n\n        return next_state\n\n    def train_network(self):\n        state_batch = []\n        action_batch = []\n        reward_batch = []\n        next_state_batch = []\n        terminal_batch = []\n        y_batch = []\n\n        # Sample random minibatch of transition from replay memory\n        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n        for data in minibatch:\n            state_batch.append(data[0])\n            action_batch.append(data[1])\n            reward_batch.append(data[2])\n            next_state_batch.append(data[3])\n            terminal_batch.append(data[4])\n\n        # Convert True to 1, False to 0\n        terminal_batch = np.array(terminal_batch) + 0\n\n        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n\n        loss, _ = self.sess.run([self.loss, self.grad_update], feed_dict={\n            self.s: np.float32(np.array(state_batch) / 255.0),\n            self.a: action_batch,\n            self.y: y_batch\n        })\n\n        self.total_loss += loss\n\n    def setup_summary(self):\n        episode_total_reward = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Total Reward/Episode', episode_total_reward)\n        episode_avg_max_q = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Average Max Q/Episode', episode_avg_max_q)\n        episode_duration = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n        episode_avg_loss = tf.Variable(0.)\n        tf.summary.scalar(ENV_NAME + '/Average Loss/Episode', episode_avg_loss)\n        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n        summary_op = tf.summary.merge_all()\n        return summary_placeholders, update_ops, summary_op\n\n    def load_network(self):\n        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n        if checkpoint and checkpoint.model_checkpoint_path:\n            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n        else:\n            print('Training new network...')\n\n    def get_action_at_test(self, state):\n        action = self.repeated_action\n\n        if self.t % ACTION_INTERVAL == 0:\n            if random.random() <= 0.05:\n                action = random.randrange(self.num_actions)\n            else:\n                action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n            self.repeated_action = action\n\n        self.t += 1\n\n        return action\n\n\ndef preprocess(observation, last_observation):\n    processed_observation = np.maximum(observation, last_observation)\n    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n    return np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n\n\ndef main():\n    env = gym.make(ENV_NAME)\n    agent = Agent(num_actions=env.action_space.n)\n\n    if TRAIN:  # Train mode\n        for _ in range(NUM_EPISODES):\n            terminal = False\n            observation = env.reset()\n            for _ in range(random.randint(1, NO_OP_STEPS)):\n                last_observation = observation\n                observation, _, _, _ = env.step(0)  # Do nothing\n            state = agent.get_initial_state(observation, last_observation)\n            while not terminal:\n                last_observation = observation\n                action = agent.get_action(state)\n                observation, reward, terminal, _ = env.step(action)\n                # env.render()\n                processed_observation = preprocess(observation, last_observation)\n                state = agent.run(state, action, reward, terminal, processed_observation)\n    else:  # Test mode\n        # env.monitor.start(ENV_NAME + '-test')\n        for _ in range(NUM_EPISODES_AT_TEST):\n            terminal = False\n            observation = env.reset()\n            for _ in range(random.randint(1, NO_OP_STEPS)):\n                last_observation = observation\n                observation, _, _, _ = env.step(0)  # Do nothing\n            state = agent.get_initial_state(observation, last_observation)\n            while not terminal:\n                last_observation = observation\n                action = agent.get_action_at_test(state)\n                observation, _, terminal, _ = env.step(action)\n                env.render()\n                processed_observation = preprocess(observation, last_observation)\n                state = np.append(state[1:, :, :], processed_observation, axis=0)\n        # env.monitor.close()\n\n\nif __name__ == '__main__':\n    main()\n```\n\n\u5b9f\u884c\u3059\u308b\u3002\n\n```\npython dqn.py\n```\n\n\u52d5\u3044\u305f\u3002\n\n![\u30ad\u30e3\u30d7\u30c1\u30e32.PNG](https://qiita-image-store.s3.amazonaws.com/0/61995/5f8c5e98-c01e-8a30-4628-0a7e6bccfc1f.png)\n\n\u5f37\u5316\u5b66\u7fd2\u3092\u3055\u305b\u3066\u307f\u308b\u3002`dqn.py`\u306e37\u884c\u76ee\u3092`TRAIN = True`\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u3002\n\n\u52d5\u3044\u305f\u3002\u3061\u3083\u3093\u3068\u5b66\u7fd2\u3057\u3066\u308b\u307f\u305f\u3044\u3002\u9045\u3044\u3051\u3069\u3002\n\n![\u30ad\u30e3\u30d7\u30c1\u30e33.PNG](https://qiita-image-store.s3.amazonaws.com/0/61995/1e14a5e2-35fa-986c-256c-fd0227c284c8.png)\n\n# \u307e\u3068\u3081\n\nDQN\u306e\u7406\u8ad6\u3068\u304b\u30b3\u30fc\u30c9\u306e\u4e2d\u8eab\u3068\u304b\u307b\u3068\u3093\u3069\u308f\u304b\u3063\u3066\u306a\u3044\u3051\u3069\u52d5\u304b\u3059\u3053\u3068\u304c\u3067\u304d\u305f\u306e\u3067AI\u306b\u30b2\u30fc\u30e0\u3092\u5b66\u7fd2\u3055\u305b\u306a\u304c\u3089\u30dc\u30af\u3082DQN\u306e\u5b66\u7fd2\u3092\u3057\u3066\u3044\u3053\u3046\u3068\u601d\u3044\u307e\u3059\u3002\u3042\u3089\u305f\u3081\u3066[\u8a18\u4e8b](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html)\u3061\u3083\u3093\u3068\u8aad\u3082\u3002\n", "tags": ["DeepLearning", "OpenAIGym", "Keras", "TensorFlow", "DQN"]}