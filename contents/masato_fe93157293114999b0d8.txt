{"context": " More than 1 year has passed since last update.\u65e5\u7d4cBP\u306eIT\u30a4\u30f3\u30d5\u30e9\u30c6\u30af\u30ce\u30ed\u30b8\u30fcAWARD 2015\u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u30022015\u5e74\u306b\u30d6\u30ec\u30fc\u30af\u3059\u308b\u3068\u4e88\u60f3\u3055\u308c\u308b\u30af\u30e9\u30a6\u30c9\u3084\u30d3\u30c3\u30b0\u30c7\u30fc\u30bf\u306e\u88fd\u54c1\u3084\u30b5\u30fc\u30d3\u30b9\u3092\u9078\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\u30b0\u30e9\u30f3\u30d7\u30ea\u306bDocker\u3001\u6e96\u30b0\u30e9\u30f3\u30d7\u30ea\u306bApache Spark\u304c\u9078\u3070\u308c\u307e\u3057\u305f\u3002Spark\u306f2014\u5e74\u306b\u5165\u308a\u76db\u308a\u4e0a\u304c\u3063\u3066\u304d\u3066\u3044\u307e\u3059\u3002\u30a4\u30f3\u30e1\u30e2\u30ea\u3067\u9ad8\u901f\u306b\u5206\u6563\u51e6\u7406\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u6a5f\u68b0\u5b66\u7fd2\u306e\u3088\u3046\u306a\u7e70\u308a\u8fd4\u3057\u51e6\u7406\u306b\u5411\u3044\u3066\u3044\u307e\u3059\u3002MLib\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30ea\u3082\u3042\u308b\u306e\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3068\u3057\u3066\u6ce8\u76ee\u3092\u96c6\u3081\u3066\u3044\u307e\u3059\u3002\u305d\u3093\u306aDocker\u3068Spark\u3092\u4f7f\u3044\u624b\u8efd\u306b\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u306e\u74b0\u5883\u3092\u3064\u304f\u308a\u52c9\u5f37\u3057\u3066\u3044\u3053\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u3053\u306e\u30b7\u30ea\u30fc\u30ba\n\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 1: \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 2: Ubuntu\u3067IPython Notebook\u3092\u4f7f\u3046\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 3: \u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 4: Ambari\u3067Hadoop\u3068Spark\u306esingle node\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\n\nSpark\u306e\u66f8\u7c4d\n\u65b0\u3057\u3044\u6280\u8853\u3092\u5b66\u7fd2\u3059\u308b\u5834\u5408\u3001\u6700\u521d\u306f\u66f8\u7c4d\u304b\u3089\u7db2\u7f85\u7684\u306b\u306f\u3044\u3063\u305f\u307b\u3046\u304c\u6982\u5ff5\u3092\u3064\u304b\u307f\u3084\u3059\u3044\u3067\u3059\u3002\n\nAdvanced Analytics with Spark Patterns for Learning from Data at Scale\nLearning Spark Lightning-Fast Big Data Analytics\nFast Data Processing with Spark\n\nAdvanced Analytics with Spark Patterns for Learning from Data at Scale\u306fO\u2019Reilly Web Ops & Performance Newsletter\u304b\u3089Happy Holidays\u30ae\u30d5\u30c8\u3067\u30d7\u30ec\u30bc\u30f3\u30c8\u3057\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\n\nSequenceIQ\u306eDocker\u30a4\u30e1\u30fc\u30b8\nDocker\u30a4\u30e1\u30fc\u30b8\u306fSequenceIQ\u306esequenceiq/spark\u3092\u4f7f\u3044\u307e\u3059\u3002SequenceIQ\u306fHadoop-as-a-Service API\u306eCloudbreak\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u3067\u958b\u767a\u3057\u3066\u3044\u307e\u3059\u3002Cloudbreak\u306fAmbari\u3068Docker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nsequenceiq/spark\u306eDocker\u30a4\u30e1\u30fc\u30b8\u3092pull\u3057\u307e\u3059\u30022014-12-18\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305fSpark 1.2.0\u3092\u4f7f\u3044\u307e\u3059\u3002\n$ docker pull sequenceiq/spark:1.2.0\n\n\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n$ docker run -i -t -h sandbox sequenceiq/spark:1.2.0 /etc/bootstrap.sh -bash\n/\nStarting sshd:                                             [  OK  ]\nStarting namenodes on [sandbox]\nsandbox: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-sandbox.out\nlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-sandbox.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-sandbox.out\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-sandbox.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-sandbox.out\nbash-4.1#\n\n\u6700\u5f8c\u306e-bash\u306f/etc/bootstrap.sh\u3092\u5b9f\u884c\u3059\u308b\u3068\u304d\u306e\u30d5\u30e9\u30b0\u3067\u3059\u3002/bin/bash\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\nbootstrap.sh\n...\nif [[ $1 == \"-d\" ]]; then\n  while true; do sleep 1000; done\nfi\n\nif [[ $1 == \"-bash\" ]]; then\n  /bin/bash\nfi\n\n\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30d0\u30fc\u30b7\u30e7\u30f3\u306e\u78ba\u8a8d\nHadoop\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.6.0\u3067\u3059\u3002\n$ hadoop version\nHadoop 2.6.0\nSubversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1\nCompiled by jenkins on 2014-11-13T21:10Z\nCompiled with protoc 2.5.0\nFrom source with checksum 18e43357c8f927c0695f1e9522859d6a\nThis command was run using /usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar\n\nSpark\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.2.0\u3001Scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.10.4\u3067\u3059\u3002\n$ spark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)\n...\nscala> :quit\n\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u3059\u3002\n$ echo $SPARK_HOME\n$ /usr/local/spark\n\nHadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u3059\u3002\n$ echo HADOOP_YARN_HOME\n/usr/local/hadoop\n\nspark-shell\u3068pyspark\u30b3\u30de\u30f3\u30c9\u306f$SPARK_HOME/bin\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n$ which spark-shell\n/usr/local/spark/bin/spark-shell\n$ which pyspark\n/usr/local/spark/bin/pyspark\n\n\nSpark Shell (Scala\u3068Python)\nSequenceIQ\u306e\u30d6\u30ed\u30b0Apache Spark 1.2.0 on Docker\u3092\u8aad\u307f\u306a\u304c\u3089\u8a66\u3057\u3066\u307f\u307e\u3059\u3002Quick Start\u306b\u3082Spark Shell\u306e\u30b5\u30f3\u30d7\u30eb\u304c\u3042\u308a\u307e\u3059\u3002\n\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u5206\u6790\u306b\u4f7f\u3046Spark Shell\u306b\u306f\u3001Scala\u306espark-shell\u3068Python\u306epyspark\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u306f\u3058\u3081\u306bScala API\u306espark-shell\u3092\u8d77\u52d5\u3057\u3066\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n$ spark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)\nType in expressions to have them evaluated.\n...\nscala> sc.parallelize(1 to 1000).count()\n...\nres0: Long = 1000\n\n\u6b21\u306bSpark\u306ePython API\u3067\u3042\u308bPySpark\u3092\u4f7f\u3044\u307e\u3059\u3002pyspark\u3092\u8d77\u52d5\u3057\u3066\u540c\u69d8\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n$ pyspark\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Python version 2.6.6 (r266:84292, Jan 22 2014 09:42:36)\nSparkContext available as sc.\n>>> sc.parallelize(range(1000)).count()\n...\n1000\n\n\n\u5186\u5468\u7387\u8a08\u7b97\u306e\u30b5\u30f3\u30d7\u30eb\nspark-examples-1.2.0-hadoop2.4.0.jar\u304b\u3089\u5186\u5468\u7387\u8a08\u7b97\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\nYARN\u4e0a\u3067\u5b9f\u884c\u3059\u308bSpark\u30a2\u30d7\u30ea\u306f2\u3064\u306e\u30e2\u30fc\u30c9\u304c\u3042\u308a\u307e\u3059\u3002\n\nyarn-cluster mode\n\n\nSpark\u30a2\u30d7\u30ea\u306fYARN\u30af\u30e9\u30b9\u30bf\u3067\u5b9f\u884c\u3055\u308c\u308b\n\u901a\u5e38\u306e\u30d0\u30c3\u30c1\u51e6\u7406\u3067\u4f7f\u3046\n\n\nyarn-client mode\n\n\nSpark\u30a2\u30d7\u30ea\u306f\u30ed\u30fc\u30ab\u30eb\u30db\u30b9\u30c8\u3067\u5b9f\u884c\u3055\u308c\u308b\n\u30c7\u30d0\u30c3\u30b0\u306a\u3069\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u51e6\u7406\u3067\u4f7f\u3046\n\n\n\nyarn-cluster mode\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001\u51e6\u7406\u7d50\u679c\u306f$HADOOP_YARN_HOME/logs\u306b\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n$ spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 $SPARK_HOME/lib/spark-examples-1.2.0-hadoop2.4.0.jar\n...\n15/01/10 02:00:43 INFO yarn.Client:\n         client token: N/A\n         diagnostics: N/A\n         ApplicationMaster host: sandbox\n         ApplicationMaster RPC port: 0\n         queue: default\n         start time: 1420873225740\n         final status: SUCCEEDED\n         tracking URL: http://sandbox:8088/proxy/application_1420873088326_0001/A\n         user: root\n\n\u30ed\u30b0\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n$ cat /usr/local/hadoop/logs/userlogs/application_1420873088326_0001/container_1420873088326_0001_01_000001/stdout\nPi is roughly 3.1451\n\nyarn-client mode\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001\u51e6\u7406\u7d50\u679c\u306f\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u6a19\u6e96\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n$ spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 $SPARK_HOME/lib/spark-examples-1.2.0-hadoop2.4.0.jar\n...\nPi is roughly 3.14515\n\n\u65e5\u7d4cBP\u306e[IT\u30a4\u30f3\u30d5\u30e9\u30c6\u30af\u30ce\u30ed\u30b8\u30fcAWARD 2015](http://corporate.nikkeibp.co.jp/information/newsrelease/newsrelease20141224.shtml)\u304c\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u30022015\u5e74\u306b\u30d6\u30ec\u30fc\u30af\u3059\u308b\u3068\u4e88\u60f3\u3055\u308c\u308b\u30af\u30e9\u30a6\u30c9\u3084\u30d3\u30c3\u30b0\u30c7\u30fc\u30bf\u306e\u88fd\u54c1\u3084\u30b5\u30fc\u30d3\u30b9\u3092\u9078\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\u30b0\u30e9\u30f3\u30d7\u30ea\u306bDocker\u3001\u6e96\u30b0\u30e9\u30f3\u30d7\u30ea\u306bApache Spark\u304c\u9078\u3070\u308c\u307e\u3057\u305f\u3002Spark\u306f2014\u5e74\u306b\u5165\u308a\u76db\u308a\u4e0a\u304c\u3063\u3066\u304d\u3066\u3044\u307e\u3059\u3002\u30a4\u30f3\u30e1\u30e2\u30ea\u3067\u9ad8\u901f\u306b\u5206\u6563\u51e6\u7406\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u6a5f\u68b0\u5b66\u7fd2\u306e\u3088\u3046\u306a\u7e70\u308a\u8fd4\u3057\u51e6\u7406\u306b\u5411\u3044\u3066\u3044\u307e\u3059\u3002MLib\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30ea\u3082\u3042\u308b\u306e\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3068\u3057\u3066\u6ce8\u76ee\u3092\u96c6\u3081\u3066\u3044\u307e\u3059\u3002\u305d\u3093\u306aDocker\u3068Spark\u3092\u4f7f\u3044\u624b\u8efd\u306b\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u306e\u74b0\u5883\u3092\u3064\u304f\u308a\u52c9\u5f37\u3057\u3066\u3044\u3053\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n## \u3053\u306e\u30b7\u30ea\u30fc\u30ba\n\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 1: \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb](http://qiita.com/masato/items/fe93157293114999b0d8)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 2: Ubuntu\u3067IPython Notebook\u3092\u4f7f\u3046](http://qiita.com/masato/items/be383a81e323f3b5b42e)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 3: \u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b](http://qiita.com/masato/items/9398cccde46cf62aa609)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 4: Ambari\u3067Hadoop\u3068Spark\u306esingle node\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7](http://qiita.com/masato/items/34cf33b6a956a2f723f9)\n\n## Spark\u306e\u66f8\u7c4d\n\n\u65b0\u3057\u3044\u6280\u8853\u3092\u5b66\u7fd2\u3059\u308b\u5834\u5408\u3001\u6700\u521d\u306f\u66f8\u7c4d\u304b\u3089\u7db2\u7f85\u7684\u306b\u306f\u3044\u3063\u305f\u307b\u3046\u304c\u6982\u5ff5\u3092\u3064\u304b\u307f\u3084\u3059\u3044\u3067\u3059\u3002\n\n* [Advanced Analytics with Spark Patterns for Learning from Data at Scale](http://shop.oreilly.com/product/0636920035091.do)\n* [Learning Spark Lightning-Fast Big Data Analytics](http://shop.oreilly.com/product/0636920028512.do)\n* [Fast Data Processing with Spark](https://www.packtpub.com/big-data-and-business-intelligence/fast-data-processing-spark)\n\nAdvanced Analytics with Spark Patterns for Learning from Data at Scale\u306f[O\u2019Reilly Web Ops & Performance Newsletter](http://radar.oreilly.com/webops-perf)\u304b\u3089Happy Holidays\u30ae\u30d5\u30c8\u3067\u30d7\u30ec\u30bc\u30f3\u30c8\u3057\u3066\u3082\u3089\u3044\u307e\u3057\u305f\u3002\n\n## SequenceIQ\u306eDocker\u30a4\u30e1\u30fc\u30b8\n\nDocker\u30a4\u30e1\u30fc\u30b8\u306f[SequenceIQ](http://sequenceiq.com/)\u306e[sequenceiq/spark](https://registry.hub.docker.com/u/sequenceiq/spark/)\u3092\u4f7f\u3044\u307e\u3059\u3002SequenceIQ\u306fHadoop-as-a-Service API\u306e[Cloudbreak](https://github.com/sequenceiq/cloudbreak)\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u3067\u958b\u767a\u3057\u3066\u3044\u307e\u3059\u3002Cloudbreak\u306fAmbari\u3068Docker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\n## \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n[sequenceiq/spark](https://registry.hub.docker.com/u/sequenceiq/spark/)\u306eDocker\u30a4\u30e1\u30fc\u30b8\u3092pull\u3057\u307e\u3059\u30022014-12-18\u306b\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305fSpark 1.2.0\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n``` bash\n$ docker pull sequenceiq/spark:1.2.0\n```\n\n\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n``` bash\n$ docker run -i -t -h sandbox sequenceiq/spark:1.2.0 /etc/bootstrap.sh -bash\n/\nStarting sshd:                                             [  OK  ]\nStarting namenodes on [sandbox]\nsandbox: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-sandbox.out\nlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-sandbox.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-sandbox.out\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-sandbox.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-sandbox.out\nbash-4.1#\n```\n\n\u6700\u5f8c\u306e`-bash`\u306f[/etc/bootstrap.sh](https://github.com/sequenceiq/hadoop-docker/blob/master/bootstrap.sh\n)\u3092\u5b9f\u884c\u3059\u308b\u3068\u304d\u306e\u30d5\u30e9\u30b0\u3067\u3059\u3002`/bin/bash`\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n``` bash:bootstrap.sh\n...\nif [[ $1 == \"-d\" ]]; then\n  while true; do sleep 1000; done\nfi\n\nif [[ $1 == \"-bash\" ]]; then\n  /bin/bash\nfi\n```\n\n## \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u30d0\u30fc\u30b7\u30e7\u30f3\u306e\u78ba\u8a8d\n\nHadoop\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.6.0\u3067\u3059\u3002\n\n``` bash\n$ hadoop version\nHadoop 2.6.0\nSubversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1\nCompiled by jenkins on 2014-11-13T21:10Z\nCompiled with protoc 2.5.0\nFrom source with checksum 18e43357c8f927c0695f1e9522859d6a\nThis command was run using /usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar\n```\n\nSpark\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.2.0\u3001Scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.10.4\u3067\u3059\u3002\n\n``` bash\n$ spark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)\n...\nscala> :quit\n```\n\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u3059\u3002\n\n``` bash\n$ echo $SPARK_HOME\n$ /usr/local/spark\n```\n\nHadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u3059\u3002\n\n``` bash\n$ echo HADOOP_YARN_HOME\n/usr/local/hadoop\n```\n\nspark-shell\u3068pyspark\u30b3\u30de\u30f3\u30c9\u306f`$SPARK_HOME/bin`\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n``` bash\n$ which spark-shell\n/usr/local/spark/bin/spark-shell\n$ which pyspark\n/usr/local/spark/bin/pyspark\n```\n\n\n## Spark Shell (Scala\u3068Python)\n\nSequenceIQ\u306e\u30d6\u30ed\u30b0[Apache Spark 1.2.0 on Docker](http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/)\u3092\u8aad\u307f\u306a\u304c\u3089\u8a66\u3057\u3066\u307f\u307e\u3059\u3002[Quick Start](https://spark.apache.org/docs/1.2.0/quick-start.html)\u306b\u3082Spark Shell\u306e\u30b5\u30f3\u30d7\u30eb\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u5206\u6790\u306b\u4f7f\u3046Spark Shell\u306b\u306f\u3001Scala\u306espark-shell\u3068Python\u306epyspark\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u306f\u3058\u3081\u306bScala API\u306espark-shell\u3092\u8d77\u52d5\u3057\u3066\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n``` bash\n$ spark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_51)\nType in expressions to have them evaluated.\n...\nscala> sc.parallelize(1 to 1000).count()\n...\nres0: Long = 1000\n```\n\n\u6b21\u306bSpark\u306ePython API\u3067\u3042\u308bPySpark\u3092\u4f7f\u3044\u307e\u3059\u3002pyspark\u3092\u8d77\u52d5\u3057\u3066\u540c\u69d8\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n``` bash\n$ pyspark\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Python version 2.6.6 (r266:84292, Jan 22 2014 09:42:36)\nSparkContext available as sc.\n>>> sc.parallelize(range(1000)).count()\n...\n1000\n``` \n\n\n## \u5186\u5468\u7387\u8a08\u7b97\u306e\u30b5\u30f3\u30d7\u30eb\n\nspark-examples-1.2.0-hadoop2.4.0.jar\u304b\u3089\u5186\u5468\u7387\u8a08\u7b97\u306e\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\nYARN\u4e0a\u3067\u5b9f\u884c\u3059\u308bSpark\u30a2\u30d7\u30ea\u306f2\u3064\u306e\u30e2\u30fc\u30c9\u304c\u3042\u308a\u307e\u3059\u3002\n\n* yarn-cluster mode\n * Spark\u30a2\u30d7\u30ea\u306fYARN\u30af\u30e9\u30b9\u30bf\u3067\u5b9f\u884c\u3055\u308c\u308b\n * \u901a\u5e38\u306e\u30d0\u30c3\u30c1\u51e6\u7406\u3067\u4f7f\u3046\n* yarn-client mode\n * Spark\u30a2\u30d7\u30ea\u306f\u30ed\u30fc\u30ab\u30eb\u30db\u30b9\u30c8\u3067\u5b9f\u884c\u3055\u308c\u308b\n * \u30c7\u30d0\u30c3\u30b0\u306a\u3069\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u51e6\u7406\u3067\u4f7f\u3046\n\nyarn-cluster mode\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001\u51e6\u7406\u7d50\u679c\u306f`$HADOOP_YARN_HOME/logs`\u306b\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n``` bash\n$ spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 $SPARK_HOME/lib/spark-examples-1.2.0-hadoop2.4.0.jar\n...\n15/01/10 02:00:43 INFO yarn.Client:\n         client token: N/A\n         diagnostics: N/A\n         ApplicationMaster host: sandbox\n         ApplicationMaster RPC port: 0\n         queue: default\n         start time: 1420873225740\n         final status: SUCCEEDED\n         tracking URL: http://sandbox:8088/proxy/application_1420873088326_0001/A\n         user: root\n```\n\n\u30ed\u30b0\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n\n``` bash\n$ cat /usr/local/hadoop/logs/userlogs/application_1420873088326_0001/container_1420873088326_0001_01_000001/stdout\nPi is roughly 3.1451\n```\n\nyarn-client mode\u3067\u5b9f\u884c\u3059\u308b\u3068\u3001\u51e6\u7406\u7d50\u679c\u306f\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u6a19\u6e96\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n``` bash\n$ spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --driver-memory 1g --executor-memory 1g --executor-cores 1 $SPARK_HOME/lib/spark-examples-1.2.0-hadoop2.4.0.jar\n...\nPi is roughly 3.14515\n```\n", "tags": ["Spark1.2.0", "hadoop2.6.0", "\u6a5f\u68b0\u5b66\u7fd2"]}