{"context": "\n\nhive\u3067\u666e\u901a\u306btsv\u3092load\u3059\u308b\n\ntsv\u30c7\u30fc\u30bf\u3092hdfs\u306b\u304a\u304f\nhive\u304b\u3089load data \u3067insert\u3059\u308b\u3002\nhive\u306etable\u306f\u6700\u3082\u52b9\u7387(\u51e6\u7406\u3001\u7a7a\u9593\uff09\u304c\u60aa\u3044\u3063\u307d\u3044 textformat \nand sequencefile\n\n7.84 MB 2016/7/26 10:58:24  3   128 MB  sample.tsv\n\n9.07 MB 2016/7/26 12:22:09  2   128 MB  000000_0\n\n\u3057\u305f\u5834\u5408\n\ntextformat\u306e\u5834\u5408\u3001tsv\u304c\u305d\u306e\u307e\u307e, hive\u306etable data\u306b\u306a\u308b\u3002\n\u975e\u5727\u7e2e\u3067\u306esequence file\u306e\u5834\u5408\u3001\u5f53\u7136\u3001\u5143\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3068\u540c\u3058\u304f\u3089\u3044\u306b\u306a\u308b\u3001\n\n\nspark sql data frame\u306esaveAsTable\u3067table\u4f5c\u6210\u3001insert\u3092\u3057\u305f\u5834\u5408\n\ntsv\u3092textFile\u3067\u8aad\u307f\u8fbc\u307f\u3001case\u30af\u30e9\u30b9\u3067Row\u578b\u306b\u5909\u63db\u3057\u3066\u3001toDF\u3067dataframe\u306b\u5909\u63db\u3057\u3066\u3001saveAsTable\u3067hive table\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\u3002\n\u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u306f\u306a\u3057\u3001default\n\n175.17 KB   2016/7/26 11:27:47  2   128 MB  part-r-00000-3261acea-9b2b-495a-8db5-8f81cef47ca1.gz.parquet\n171.1 KB    2016/7/26 11:27:47  2   128 MB  part-r-00001-3261acea-9b2b-495a-8db5-8f81cef47ca1.gz.parquet\n\n\u306e\u5834\u5408\n\ngz.parquet suffix\u306efile\u304c2\u3064\u751f\u6210\n\u30b5\u30a4\u30ba\u3001suffix\u304b\u3089gz\u5727\u7e2e\uff0bparquet format\n\nshow create table\u306e\u8868\u793a\u304b\u3089\u3082\u78ba\u8a8d\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \nWITH SERDEPROPERTIES ( \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='false', \n  'EXTERNAL'='FALSE', \n  'numFiles'='2', \n  'numRows'='-1', \n  'rawDataSize'='-1', \n  'spark.sql.sources.provider'='org.apache.spark.sql.parquet', \n  'spark.sql.sources.schema.numParts'='1', \n\n\nsnappy codec \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u305f\u5834\u5408\n\nsqlContext.setConf\u3067\n\n\nsqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\nsqlContext.setConf(\"parquet.compression\", \"SNAPPY\")\n\n\u3067saveAsTable\u3067\u4fdd\u5b58\u3057\u305f\u5834\u5408\n235.3 KB    2016/7/26 11:19:10  2   128 MB  part-r-00000-5d10d91c-d05d-4be6-9c40-d6c7db9c56cc.snappy.parquet\n228.73 KB   2016/7/26 11:19:07  2   128 MB  part-r-00001-5d10d91c-d05d-4be6-9c40-d6c7db9c56cc.snappy.parquet\n\n\nsnappy + parquet\u3067\u4fdd\u5b58\u3055\u308c\u308b\u3002\n\u2193\u306eshow create table \u306e\u7d50\u679c\u3067\u3082\u308f\u304b\u308b\u3002\n\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \nWITH SERDEPROPERTIES ( \n  'spark.hadoop.mapred.output.compress'='true', \n  'spark.hadoop.mapred.output.compression.codec'='org.apache.hadoop.io.compress.SnappyCodec', \n  'spark.hadoop.mapred.output.compression.type'='BLOCK') \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='false', \n  'EXTERNAL'='FALSE', \n  'numFiles'='2', \n  'numRows'='-1', \n  'rawDataSize'='-1', \n  'spark.sql.sources.provider'='org.apache.spark.sql.parquet', \n  'spark.sql.sources.schema.numParts'='1', \n\n\n# hive\u3067\u666e\u901a\u306btsv\u3092load\u3059\u308b\n\n- tsv\u30c7\u30fc\u30bf\u3092hdfs\u306b\u304a\u304f\n- hive\u304b\u3089load data \u3067insert\u3059\u308b\u3002\n- hive\u306etable\u306f\u6700\u3082\u52b9\u7387(\u51e6\u7406\u3001\u7a7a\u9593\uff09\u304c\u60aa\u3044\u3063\u307d\u3044 textformat \n- and sequencefile\n\n```textformat\n7.84 MB\t2016/7/26 10:58:24\t3\t128 MB\tsample.tsv\n```\n\n\n```sequencefile\n9.07 MB\t2016/7/26 12:22:09\t2\t128 MB\t000000_0\n```\n\u3057\u305f\u5834\u5408\n\n- textformat\u306e\u5834\u5408\u3001tsv\u304c\u305d\u306e\u307e\u307e, hive\u306etable data\u306b\u306a\u308b\u3002\n- \u975e\u5727\u7e2e\u3067\u306esequence file\u306e\u5834\u5408\u3001\u5f53\u7136\u3001\u5143\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3068\u540c\u3058\u304f\u3089\u3044\u306b\u306a\u308b\u3001\n\n# spark sql data frame\u306esaveAsTable\u3067table\u4f5c\u6210\u3001insert\u3092\u3057\u305f\u5834\u5408\n\n- tsv\u3092textFile\u3067\u8aad\u307f\u8fbc\u307f\u3001case\u30af\u30e9\u30b9\u3067Row\u578b\u306b\u5909\u63db\u3057\u3066\u3001toDF\u3067dataframe\u306b\u5909\u63db\u3057\u3066\u3001saveAsTable\u3067hive table\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\u3002\n\n- \u30aa\u30d7\u30b7\u30e7\u30f3\u8a2d\u5b9a\u306f\u306a\u3057\u3001default\n\n```\n175.17 KB\t2016/7/26 11:27:47\t2\t128 MB\tpart-r-00000-3261acea-9b2b-495a-8db5-8f81cef47ca1.gz.parquet\n171.1 KB\t2016/7/26 11:27:47\t2\t128 MB\tpart-r-00001-3261acea-9b2b-495a-8db5-8f81cef47ca1.gz.parquet\n```\n\u306e\u5834\u5408\n\n- gz.parquet suffix\u306efile\u304c2\u3064\u751f\u6210\n- \u30b5\u30a4\u30ba\u3001suffix\u304b\u3089gz\u5727\u7e2e\uff0bparquet format\n\nshow create table\u306e\u8868\u793a\u304b\u3089\u3082\u78ba\u8a8d\n\n```\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \nWITH SERDEPROPERTIES ( \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='false', \n  'EXTERNAL'='FALSE', \n  'numFiles'='2', \n  'numRows'='-1', \n  'rawDataSize'='-1', \n  'spark.sql.sources.provider'='org.apache.spark.sql.parquet', \n  'spark.sql.sources.schema.numParts'='1', \n```\n\n# snappy codec \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u305f\u5834\u5408\n\n- sqlContext.setConf\u3067\n\n```\n\nsqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\nsqlContext.setConf(\"parquet.compression\", \"SNAPPY\")\n```\n\n\u3067saveAsTable\u3067\u4fdd\u5b58\u3057\u305f\u5834\u5408\n\n```\n235.3 KB\t2016/7/26 11:19:10\t2\t128 MB\tpart-r-00000-5d10d91c-d05d-4be6-9c40-d6c7db9c56cc.snappy.parquet\n228.73 KB\t2016/7/26 11:19:07\t2\t128 MB\tpart-r-00001-5d10d91c-d05d-4be6-9c40-d6c7db9c56cc.snappy.parquet\n```\n\n- snappy + parquet\u3067\u4fdd\u5b58\u3055\u308c\u308b\u3002\n- \u2193\u306eshow create table \u306e\u7d50\u679c\u3067\u3082\u308f\u304b\u308b\u3002\n\n```\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \nWITH SERDEPROPERTIES ( \n  'spark.hadoop.mapred.output.compress'='true', \n  'spark.hadoop.mapred.output.compression.codec'='org.apache.hadoop.io.compress.SnappyCodec', \n  'spark.hadoop.mapred.output.compression.type'='BLOCK') \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nTBLPROPERTIES (\n  'COLUMN_STATS_ACCURATE'='false', \n  'EXTERNAL'='FALSE', \n  'numFiles'='2', \n  'numRows'='-1', \n  'rawDataSize'='-1', \n  'spark.sql.sources.provider'='org.apache.spark.sql.parquet', \n  'spark.sql.sources.schema.numParts'='1', \n```\n", "tags": ["Spark", "sparksql", "hive"]}