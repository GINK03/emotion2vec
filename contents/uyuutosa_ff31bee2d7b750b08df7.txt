{"context": "Python\u3067\u81ea\u4f5c\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f5c\u308a\u307e\u3057\u305f(\u9014\u4e2d)\u3002\u81ea\u8eab\u306e\u52c9\u5f37\u7528\u3067\u3059\u3002\n\u540d\u524d\u306f\u4f55\u306e\u637b\u308a\u3082\u306a\u3044\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u3067\u3059\u3002\n\u5c11\u306a\u3044\u6253\u9375\u6570\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u304b\u3051\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002\n\u5185\u90e8\u3067\u306ftheano\u3092\u7528\u3044\u3066\u307e\u3059\u304c\u3001Define by Run \u306achainer\u306e\u307b\u3046\u304c\u4e0a\u4f4d\u4e92\u63db\u306a\u6c17\u304c\u3059\u308b\u306e\u3067\u3001\nchainer\u306b\u5909\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\u8a18\u4e8b\u306e\u6700\u5f8c\u306b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002\nLSTM\u3068Taylor\u306e\u5b9f\u88c5\u306f\u602a\u3057\u3044\u3067\u3059\u3002\n\n\u4f7f\u7528\u4f8b\nKeras\u306e\u69d8\u306b\u828b\u3065\u308b\u5f0f\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u66f8\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\u3092\u6b63\u89e3\u5024\u3068\u3057\u30663\u5c64\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308a\u63a8\u5b9a\u3057\u307e\u3059\u3002\n# \u30c7\u30fc\u30bf\u3068\u30e9\u30d9\u30eb\u306e\u5b9a\u7fa9 \uff1a\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\nx_arr = random.rand(50).astype(theano.config.floatX) * 10\ny_arr = sin(x_arr / 5. * pi)\n\n# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u7bc9\no = optimizer(x_arr, y_arr)\no = o.taylor(1,6)\\         # 1\u5c64\u76ee\uff1a(\u81ea\u79f0)\u30c6\u30a4\u30e9\u30fc\u5c55\u958b\u5c64\n     .tanh()\\              # tanh\u306e\u6d3b\u6027\u5316\u95a2\u6570\n     .dense(1)\\      \u3000    # 2\u5c64\u76ee\uff1a\u5168\u7d50\u5408\u5c64\n     .loss(alpha=0.1)\\     # \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9(\u5b66\u7fd2\u73870.1)\n     .optimize()           # \u6700\u9069\u5316\n\noptimize\u3059\u308b\u3068\u4ee5\u4e0b\u306eoutput\u304c\u51fa\u3066\u304d\u307e\u3059\u3002\nloss: 1.51645341078\nloss: 0.0678153863793\nloss: 0.0198567226285\nloss: 0.0202341528014\nloss: 0.00505698460546\nloss: 0.00519401907162\nloss: 0.00594055800437\nloss: 0.00498228924313\nloss: 0.0044779176335\nloss: 0.00413105668256\n\n\u51fa\u529b\u3092view()\u3067\u30b0\u30e9\u30d5\u3067\no.view()\n\n\n\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\u6b63\u89e3\u3068\u63a8\u5b9a\u306e\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6210\u308a\u307e\u3059\u3002\u5b66\u7fd2\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066\u63a8\u5b9a\u3059\u308b\u306b\u306f\u3001pred()\u3092\u4f7f\u3044\u307e\u3059\u3002\nplt.scatter(arange(0,10,0.1), o.pred(arange(0,10,0.1)), label=\"Exact\")\nplt.scatter(x_arr, y_arr, color=\"r\", label=\"True data\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u8868\u793a\n\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u8b0e\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\no = optimizer(x_arr, y_arr)\no = o.dense(1).sigmoid()\\\n     .dense(2).relu()\\\n     .dense(3).tanh()\\\n     .dense(4).sigmoid()\\\n     .lstm()\\\n     .loss()\n\n\u3092\u4ee5\u4e0b\u306e\u30e1\u30bd\u30c3\u30c9\no.view_graph()\n\n\u3067\u53ef\u8996\u5316\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6210\u308a\u307e\u3059\u3002\n\n\nLSTM\n\u518d\u5e30\u7684\u306a\u69cb\u9020\u304c\u5358\u7d14\u306a\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306b\u3069\u306e\u3088\u3046\u306b\u52b9\u3044\u3066\u304f\u308b\u306e\u304b\u4e0d\u660e\u77ad\u3067\u3059\u304c\u3001\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\narr = arange(0, 10., 0.01).astype(theano.config.floatX)\ny_arr = sin(arr[100:]   * pi)\nx_arr = sin(arr[:-100]   * pi)\no = optimizer(x_arr, y_arr)\no = o.lstm().loss().optimize()\n\nsin\u6ce2\u304c\u30c7\u30fc\u30bf\u3067\u3001\u305d\u308c\u3088\u308a$\\pi$\u4f4d\u76f8\u3092\u305a\u3089\u3057\u305f\u3082\u306e\u304c\u30e9\u30d9\u30eb\u3067\u3059\u3002\n\u5927\u4f53loss\u304c\u4e0b\u304c\u3063\u305f\u3089Ctrl-c\u3067\u5207\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\u7d50\u679c\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059\u3002\nplt.scatter(arange(x_arr.size), x_arr, color=\"b\")\nplt.scatter(arange(x_arr.size), o.pred(x_arr), color=\"r\")\nplt.scatter(arange(x_arr.size), o.y_arr, color=\"g\")\n\n\ndata\u306b\u5bfe\u3057\u3066label\u306f\u3072\u3063\u304f\u308a\u8fd4\u3063\u3066\u3044\u307e\u3059\u3002predict\u304clabel\u3068\u307b\u307c\u4e00\u81f4\u3057\u305f\u306e\u3067\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u3046\u307e\u304f\u3044\u3063\u305f\u3082\u306e\u3060\u3051\u8f09\u305b\u3066\u307e\u3059\u3002\n\u304d\u308c\u3044\u306b\u5b66\u7fd2\u3067\u304d\u305f\u306e\u3067\u3001\u7570\u306a\u308b\u5468\u6ce2\u6570\u3067\u63a8\u5b9a\u3057\u3066\u307f\u307e\u3059\u3002\narr = arange(0,10,0.01)\nplt.subplot(311)\ndata = sin(arr * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(312)\ndata = sin(arr / 2 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(313)\ndata = sin(arr / 4 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u7570\u306a\u308b\u5468\u6ce2\u6570\u3067\u3082\u304d\u308c\u3044\u306b\u53cd\u8ee2\u3057\u3066\u3044\u307e\u3059\u3002\n\u5b66\u7fd2\u5668\u306b\u306f\u30c7\u30fc\u30bf\u3092\u5de6\u304b\u3089\u53f3\u306b\u5165\u308c\u3066\u3044\u307e\u3059\u3002LSTM\u306f\u30ea\u30ab\u30ec\u30f3\u30c8\u306a\u69cb\u9020\u3092\u6301\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u306e\u63a8\u5b9a\u7d50\u679c\u306f\u5165\u529b\u306e\u9806\u756a\u306b\u4f9d\u5b58\u3059\u308b\u306f\u305a\u3001\u3068\u601d\u3063\u305f\u306e\u3067\u3001\u4e0a\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306brandom.shuffle(arr)\u3092\u305f\u3057\u3066\u5b66\u7fd2\u5668\u306b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u9806\u756a\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u3066\u307f\u307e\u3057\u305f\u3002\narr = arange(0,10,0.01)\nrandom.shuffle(arr)\n#arr = random.rand(1000) * 10\nplt.subplot(311)\ndata = sin(arr * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(312)\ndata = sin(arr / 2 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(313)\ndata = sin(arr / 4 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\u7d50\u679c\u306f\u3053\u306e\u901a\u308a\u3067\u3059\u3002\n\n\u4f4e\u5468\u6ce2\u6570\u306f\u518d\u73fe\u3067\u304d\u3066\u307e\u3059\u304c\u3001\u30ce\u30a4\u30ba\u306e\u3088\u3046\u306b\u9ad8\u5468\u6ce2\u304c\u76ee\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u3084\u306f\u308a\u9806\u756a\u304c\u5927\u4e8b\u307f\u305f\u3044\u3067\u3059\u306d\u3002\n\nMNIST\n\n\u5168\u7d50\u5408\n\u3044\u304f\u3064\u304b\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092set_datasets()\u306b\u3066\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002\no = optimizer(n_batch=500)\no.set_datasets(\"mnist\", is_one_hot=False)\no = o.dense(10).loss_softmax_cross_entropy().opt_sgd(0.000001).optimize(100000,10000)\n\nimport pylab as p\nfor n in range(9):\n\n    idx = random.randint(0,59999)\n    p.subplot(331 + n)\n    p.tick_params(labelbottom=\"off\")\n    p.tick_params(labelleft=\"off\")\n    p.title(o.pred_func(o.x_train_arr[idx:idx+1]).argmax(axis=1)[0])\n    p.imshow(o.x_train_arr[idx:idx+1].reshape(28,28))\n\n\u51fa\u529b\nIter. 0: loss = 2.3025851249694824\nIter. 10000: loss = 0.3245639503002167\nIter. 20000: loss = 0.22792282700538635\nKeyboardInterrupt\n\n\n\nCNN\no = optimizer(n_batch=5)\no.set_datasets(\"mnist\", is_one_hot=False)\n#o = o.reshape((n_batch,1,28,28)).conv2d(kshape=(4,1,24,24)).relu().pool()\no = o.reshape((1, 28, 28))\\\n     .conv_and_pool(8, 20, 20)\\\n     .conv_and_pool( 4,  5,  5).sigmoid()\no = o.flatten().dropout(0.5).dense(10).loss_softmax_cross_entropy()\no = o.opt_Adam(0.008).optimize(10000000,1000)\n\n\n\u30b3\u30fc\u30c9\nimport theano\nimport theano.tensor as T\nimport theano.tensor.nnet as nnet\nimport theano.tensor.signal as signal \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy as cp\nfrom theano.printing import pydotprint\nfrom PIL import Image\nimport os\nimport matplotlib as mpl\nfrom theano.tensor.shared_randomstreams import RandomStreams\nfrom sklearn.datasets import *\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.cross_validation import train_test_split\nmpl.rc(\"savefig\", dpi=1200)\n\n%config InlineBackend.rc = {'font.size': 20, 'figure.figsize': (12.0, 8.0),'figure.facecolor': 'white', 'savefig.dpi': 72, 'figure.subplot.bottom': 0.125, 'figure.edgecolor': 'white'}\n%matplotlib inline\n\nclass optimizer:\n    def __init__(self, x_arr=None, y_arr=None, \n                 out=None, thetalst=None, nodelst=None,\n                 test_size=0.1, n_batch=500):\n\n        self.n_batch = theano.shared(int(n_batch))\n\n        if x_arr is not None and y_arr is not None:\n            self.set_data(x_arr, y_arr, test_size)\n            self.set_variables()\n\n\n\n        self.thetalst = [] #if thetalst is None else thetalst\n\n        self.n_view = None\n        self.updatelst = []\n        self.tmplst = []\n\n    def set_data(self, x_arr, y_arr, test_size=0.1):\n        self.x_train_arr, \\\n        self.x_test_arr,\\\n        self.y_train_arr,\\\n        self.y_test_arr,\\\n        = train_test_split(x_arr.astype(theano.config.floatX),\n                           y_arr.astype(theano.config.floatX),\n                           test_size = test_size)\n\n        self.nodelst = [[int(np.prod(self.x_train_arr.shape[1:]))]] # if nodelst is None else nodelst\n\n\n    def set_variables(self):\n        if self.n_batch.get_value() > self.x_train_arr.shape[0]: \n            self.n_batch.set_value(int(self.x_train_arr.shape[0]))\n        self.n_data = self.x_train_arr.shape[0]\n        n_xdim = self.x_train_arr.ndim\n        n_ydim = self.y_train_arr.ndim\n        if  n_xdim == 0:\n            self.x = T.scalar()\n        if  n_xdim == 1:\n            self.x_train_arr = self.x_train_arr[:,None]\n            self.x_test_arr = self.x_test_arr[:,None]\n            self.x = T.matrix()\n        elif n_xdim == 2:\n            self.x = T.matrix()\n        elif n_xdim == 3:\n            self.x = T.tensor3()\n        else:\n            self.x = T.tensor4()\n\n        if n_ydim == 0:\n            self.y = T.scalar()\n        if n_ydim == 1:\n            self.y_train_arr = self.y_train_arr[:,None]\n            self.y_test_arr = self.y_test_arr[:,None]\n            self.y = T.matrix()\n        elif n_ydim == 2:\n            self.y = T.matrix()\n        elif n_ydim == 3:\n            self.y = T.tensor3()\n        else:\n            self.y = T.tensor4()\n\n        self.out = self.x  #if out is None else out\n        self.batch_shape_of_C = T.concatenate([T.as_tensor([self.n_batch]), theano.shared(np.array([3]))], axis=0)\n\n    def set_datasets(self, data=\"mnist\", data_home=\"data_dir_for_optimizer\", is_one_hot=True):\n\n        if data == \"mnist\":\n            data_dic = fetch_mldata('MNIST original', data_home=data_home)\n            if is_one_hot == True:\n                idx = data_dic[\"target\"]\n                arr = np.zeros((idx.shape[0],10)).flatten()\n                arr[idx.flatten().astype(int) + np.arange(idx.shape[0]) * 10]  = 1\n                data_dic[\"target\"] = arr.reshape(idx.shape[0], 10)\n        elif data == \"boston\":\n            data_dic = load_boston()   \n        elif data == \"digits\":\n            data_dic = load_digits()\n        elif data == \"iris\":\n            data_dic = load_iris()\n        elif data == \"linnerud\":\n            data_dic = load_linnerud()\n        elif data == \"xor\":\n            data_dic = {\"data\": np.array([[0,0], [0,1], [1,0], [1,1]]),##.repeat(20, axis=0),\n                        \"target\": np.array([0,1,1,0])}#.repeat(20, axis=0)}\n            #data_dic = {\"data\": np.array([[0,0], [0,1], [1,0], [1,1]]).repeat(20, axis=0),\n            #            \"target\": np.array([0,1,1,0]).repeat(20, axis=0)}\n        elif data == \"serial\":\n            data_dic = {\"data\": np.array(np.arange(20).reshape(5,4)).repeat(20, axis=0),\n                        \"target\": np.arange(5).repeat(20, axis=0)}\n        elif data == \"sin\":\n            data_dic = {\"data\": np.arange(0,10,0.01)[:,None],\n                        \"target\": np.sin(np.arange(0,10,0.01) * np.pi)}\n\n        self.set_data(data_dic[\"data\"], data_dic[\"target\"])\n        self.set_variables()\n\n    def copy(self):\n        return cp.copy(self)\n\n    def update_node(self, n_out):\n        self.nodelst = self.nodelst + [n_out]\n\n    def get_curr_node(self):\n        return list(self.nodelst[-1])\n\n    def dropout(self, rate=0.5, seed=None):\n        obj = self.copy()\n\n        srng = RandomStreams(seed)\n        obj.out = T.where(srng.uniform(size=obj.out.shape) > rate, obj.out, 0)\n\n        return obj\n\n\n    def dense(self, n_out):\n        obj = self.copy()\n        n_in = obj.get_curr_node()[-1]\n\n        #theta =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        #b  =     theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        theta =  theano.shared(np.zeros((n_in, n_out)).astype(theano.config.floatX))\n        #b     =  theano.shared(np.zeros((n_out)).astype(theano.config.floatX))\n        b = theano.shared(np.random.rand(1).astype(dtype=theano.config.floatX)[0])\n\n        obj.out = obj.out.dot(theta) + b\n        #obj.out   = theta.dot(obj.out.flatten())+b\n        obj.thetalst += [theta,b]\n\n        obj.update_node([n_out])\n\n        return obj\n\n    def lstm(self):\n        obj = self.copy()\n\n        curr_shape = obj.get_curr_node()\n        n_in = n_out = curr_shape[-1]\n\n        #batch_shape_of_h = T.concatenate(\n        #batch_shape_of_C = T.concatenate(, axis=0)\n#        h = T.ones(theano.sharedl())\n        h = T.zeros([obj.n_batch, *curr_shape], dtype=theano.config.floatX)\n        C = T.zeros([obj.n_batch, n_out], dtype=theano.config.floatX)\n\n        Wi =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wf =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wc =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wo =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        bi =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bf =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bc =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bo =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        Ui =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uf =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uc =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uo =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n\n        i = nnet.sigmoid(obj.out.dot(Wi) + h.dot(Ui) + bi)\n\n        C_tilde = T.tanh(obj.out.dot(Wc) + h.dot(Uc) + bc)\n\n        f = nnet.sigmoid(obj.out.dot(Wf) + h.dot(Uf) + bf)\n\n        tmp = (i * C_tilde + f * C).reshape(C.shape)\n\n        obj.tmplst += [(C, tmp)]\n\n        C = tmp\n\n        o = nnet.sigmoid(obj.out.dot(Wo) + h.dot(Uo) + bo)\n\n        tmp = (o * T.tanh(C)).reshape(h.shape)\n\n        obj.tmplst += [(h, tmp)]\n\n        obj.out =  tmp\n\n        obj.thetalst += [Wi, bi, Ui, Wf, bf, Uf, Wc, bc, Uc, Wo, bo, Uo]\n\n        obj.update_node([n_out])\n\n        return obj\n\n\n    def conv2d(self, kshape=(1,1,3,3), mode=\"full\", reshape=None):\n        obj = self.copy()\n\n        n_in = obj.get_curr_node()\n\n        if reshape is not None:\n            obj.out = obj.out.reshape(reshape)\n            n_in = reshape\n\n        if obj.out.ndim == 2:\n            obj.out = obj.out[None, :, :]\n            n_in = (1, n_in[1], n_in[2])\n        elif obj.out.ndim == 1:\n            obj.out = obj.out[None, None, :]\n            n_in = [1, 1] + list(n_in)\n\n        if mode == \"full\":\n            n_out = [kshape[0], n_in[-2] + (kshape[-2] - 1), n_in[-1] + (kshape[-2] - 1)]\n            #n_out = [n_in[0], kshape[0], n_in[-2] + (kshape[-2] - 1), n_in[-1] + (kshape[-2] - 1)]\n        elif mode == \"valid\":\n            n_out = [kshape[0], n_in[-2] - (kshape[-2] - 1), n_in[-1] - (kshape[-2] - 1)]\n\n        theta = theano.shared(np.random.rand(*kshape).astype(dtype=theano.config.floatX))\n        b = theano.shared(np.random.rand(1).astype(dtype=theano.config.floatX)[0])\n        obj.b = b\n\n\n        obj.out = nnet.conv2d(obj.out, theta, border_mode=mode) + b\n        obj.thetalst += [theta, b]\n\n        obj.update_node(n_out)\n\n        return obj\n\n    def conv_and_pool(self, fnum, height, width, mode=\"full\", ds=(2,2)):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n        kshape = (fnum, n_in[0], height, width)\n        n_batch = obj.n_batch.get_value()\n        obj = obj.conv2d(kshape=kshape)\n        #.reshape((n_batch, np.array(n_in, dtype=int).sum()))\\\n\n        if mode == \"full\":\n            obj = obj.relu().pool(ds=ds)\n            n_in = obj.get_curr_node()\n            obj = obj.reshape((kshape[0], *n_in[-2:]))\n            #obj = obj.reshape((kshape[0], M[0]+(m[0]-1),M[1]+(m[1]-1)))\n        elif mode == \"valid\":\n            n_in = obj.get_curr_node()\n            obj = obj.reshape((kshape[0], *n_in[-2:]))\n        return obj\n\n\n    def pool(self, ds=(2,2)):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n\n        obj.out = signal.pool.pool_2d(obj.out, ds=ds, ignore_border=True)\n\n        n_in[-2] = n_in[-2] // ds[0] #+ (1 if (n_in[-2] % ds[0]) else 0) \n        n_in[-1] = n_in[-1] // ds[1] #+ (1 if (n_in[-1] % ds[1]) else 0) \n\n        obj.update_node(n_in)\n        #print(obj.nodelst)\n\n        return obj\n\n    def mean(self, axis):\n        obj = self.copy()\n\n        n_in = obj.get_curr_node()\n        obj.out = obj.out.mean(axis=axis)\n        obj.update_node(np.ones(n_in).mean(axis=axis).shape)\n        return obj\n\n    def reshape(self, shape):\n        obj = self.copy()\n        obj.out = obj.out.reshape([obj.n_batch, *shape])\n        obj.update_node(shape)\n        return obj\n    def reshape_(self, shape):\n        obj = self.copy()\n        obj.out = obj.out.reshape(shape)\n        obj.update_node(shape[1:])\n        return obj\n\n    def flatten(self):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n        last_ndim = np.array(n_in).prod()\n        obj.out = obj.out.reshape((obj.n_batch, last_ndim))\n        obj.update_node([last_ndim])\n        return obj\n\n    def taylor(self, M, n_out):\n        obj = self.copy()\n        n_in = int(np.asarray(obj.get_curr_node()).sum())\n\n        x_times = T.concatenate([obj.out, T.ones((obj.n_batch, 1)).astype(theano.config.floatX)],axis=1)\n        for i in range(M-1):\n            idx = np.array([\":,\"]).repeat(i+1).tolist()\n            a = dict()\n            exec (\"x_times = x_times[:,\" + \"\".join(idx) + \"None] * x_times\", locals(), a)\n            x_times = a[\"x_times\"]\n\n        x_times = x_times.reshape((obj.n_batch, -1))\n        #x_times = x_times.reshape(self.n_batch, (n_in+1) ** M)\n\n        theta = theano.shared(np.ones(((n_in+1) ** M, n_out)).astype(theano.config.floatX))\n        obj.theta = theano.shared(np.ones(((n_in+1) ** M, n_out)).astype(theano.config.floatX))\n        #theta = theano.shared(np.random.rand(n_out, (n_in+1) ** M).astype(theano.config.floatX))\n\n        obj.out = x_times.dot(theta.astype(theano.config.floatX))\n        obj.thetalst += [theta]\n\n        obj.update_node([n_out])\n\n        return obj\n\n\n    def relu(self, ):\n        obj = self.copy()\n        obj.out = nnet.relu(obj.out)\n        return obj\n\n    def tanh(self, ):\n        obj = self.copy()\n        obj.out = T.tanh(obj.out)\n        return obj\n\n    def sigmoid(self, ):\n        obj = self.copy()\n        obj.out = nnet.sigmoid(obj.out)\n        return obj\n\n    def softmax(self, ):\n        obj = self.copy()\n        obj.out = nnet.softmax(obj.out)\n        return obj\n\n    def loss_msq(self):\n        obj = self.copy()\n        obj.loss =  T.mean((obj.out - obj.y) ** 2)\n        return obj\n\n    def loss_cross_entropy(self):\n        obj = self.copy()\n        obj.loss =  -T.mean(obj.y * T.log(obj.out + 1e-4))\n        return obj\n\n    def loss_softmax_cross_entropy(self):\n        obj = self.copy()\n        obj.out = nnet.softmax(obj.out)\n        tmp_y = T.cast(obj.y, \"int32\")\n        obj.loss  = -T.mean(T.log(obj.out)[T.arange(obj.y.shape[0]), tmp_y.flatten()])\n        obj.out = obj.out.argmax(axis=1)\n        #obj.out2 = obj.out.argmax(axis=1)\n        #obj.out2 = obj.out[T.arange(obj.y.shape[0]), tmp_y.flatten()]\n\n\n#        obj.loss2 = T.log(obj.out)[T.arange(obj.y.shape[0]), tmp_y.flatten()]\n        return obj\n\n    def opt_sgd(self, alpha=0.1):\n        obj = self.copy()\n        obj.updatelst = []\n        for theta in obj.thetalst:\n            obj.updatelst += [(theta, theta - (alpha * T.grad(obj.loss, wrt=theta)))]\n\n        obj.updatelst += obj.tmplst\n        return obj\n\n    def opt_RMSProp(self, alpha=0.1, gamma=0.9, ep=1e-8):\n        obj = self.copy()\n        obj.updatelst = []\n        obj.rlst = [theano.shared(x.shape).astype(theano.config.floatX) for x in obj.thetalst]\n\n        for r, theta in zip(obj.rlst, obj.thetalst):\n            g = T.grad(obj.loss, wrt=theta)\n            obj.updatelst += [(r, gamma * r + (1 - gamma) * g ** 2),\\\n                              (theta, theta - (alpha / (T.sqrt(r) + ep)) * g)]\n        obj.updatelst += obj.tmplst\n        return obj\n\n    def opt_AdaGrad(self, ini_eta=0.001, ep=1e-8):\n        obj = self.copy()\n        obj.updatelst = []\n\n        obj.hlst = [theano.shared(ep*np.ones(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n        obj.etalst = [theano.shared(ini_eta*np.ones(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n\n        for h, eta, theta in zip(obj.hlst, obj.etalst, obj.thetalst):\n            g   = T.grad(obj.loss, wrt=theta)\n            obj.updatelst += [(h,     h + g ** 2),\n                              (eta,   eta / T.sqrt(h+1e-4)),\n                              (theta, theta - eta * g)]\n\n        obj.updatelst += obj.tmplst\n        return obj\n\n    def opt_Adam(self, alpha=0.001, beta=0.9, gamma=0.999, ep=1e-8, t=3):\n        obj = self.copy()\n        obj.updatelst = []\n        obj.nulst = [theano.shared(np.zeros(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n        obj.rlst = [theano.shared(np.zeros(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n\n        for nu, r, theta in zip(obj.nulst, obj.rlst, obj.thetalst):\n            g = T.grad(obj.loss, wrt=theta)\n            nu_hat = nu / (1 - beta)\n            r_hat = r / (1 - gamma)\n            obj.updatelst += [(nu, beta * nu + (1 - beta) * g),\\\n                              (r, gamma * r +  (1 - gamma) * g ** 2),\\\n                              (theta, theta - alpha*(nu_hat / (T.sqrt(r_hat) + ep)))]\n\n        obj.updatelst += obj.tmplst\n        return obj\n\n\n\n    def optimize(self, n_epoch=10, n_view=1000):\n        obj = self.copy()\n        obj.dsize = obj.x_train_arr.shape[0]\n        if obj.n_view is None: obj.n_view = n_view  \n\n        x_train_arr_shared = theano.shared(obj.x_train_arr).astype(theano.config.floatX)\n        y_train_arr_shared = theano.shared(obj.y_train_arr).astype(theano.config.floatX)\n\n        #rng = T.shared_randomstreams.RandomStreams(seed=0)\n        #obj.idx = rng.permutation(size=(1,), n=obj.x_train_arr.shape[0]).astype(\"int64\")\n        #obj.idx = rng.permutation(size=(1,), n=obj.x_train_arr.shape[0])[0, 0:obj.n_batch].astype(\"int64\")\n        #idx = obj.idx * 1\n        i = theano.shared(0).astype(\"int32\")\n        idx = theano.shared(np.random.permutation(obj.x_train_arr.shape[0]))\n\n        obj.loss_func = theano.function(inputs=[i],\n                                      outputs=obj.loss,\n                                      givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                              (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                                      updates=obj.updatelst,\n                                      on_unused_input='warn')\n\n        i = theano.shared(0).astype(\"int32\")\n        idx = theano.shared(np.random.permutation(obj.x_train_arr.shape[0]))\n\n\n        obj.acc_train = theano.function(inputs=[i],\n                                      #outputs=((T.eq(obj.out2[:,None],obj.y))),\n                                      outputs=((T.eq(obj.out[:,None],obj.y)).sum().astype(theano.config.floatX) / obj.n_batch),\n                                      givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                              (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                                      on_unused_input='warn')\n        #idx = rng.permutation(size=(1,), a=obj.x_train_arr.shape[0])#.astype(\"int8\")\n\n        #idx = rng.permutation(size=(1,), n=500, ndim=None)[0,0:10]#.astype(\"int8\")\n\n        c = T.concatenate([obj.x,obj.y],axis=1)\n        obj.test_func = theano.function(inputs=[i],\n                               outputs=c,\n                               givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                       (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                               on_unused_input='warn')\n\n#        obj.test_func = theano.function(inputs=[i],\n#                               outputs=c,\n#                               givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n#                                       (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n#                               on_unused_input='warn')\n\n        obj.pred_func = theano.function(inputs  = [obj.x],\n                                        outputs = obj.out,\n                                        updates = obj.tmplst,\n                                        on_unused_input='warn')\n\n        obj.v_loss = []\n        try:\n            for epoch in range(n_epoch):\n                for i in range(0,obj.dsize-obj.n_batch.get_value(),obj.n_batch.get_value()):\n                    mean_loss = obj.loss_func(i) \n\n                print(\"Epoch. %s: loss = %s, acc = %s.\" %(epoch, mean_loss, obj.acc_train(i)))\n\n                obj.v_loss += [mean_loss]\n\n\n        except KeyboardInterrupt  :\n            print ( \"KeyboardInterrupt\\n\" )\n            return obj\n        return obj\n\n    def view_params(self):\n        for theta in self.thetalst:\n            print(theta.get_value().shape)\n            print(theta.get_value())\n\n\n    def view(self, yscale=\"log\"):\n        if not len(self.v_loss):\n            raise ValueError(\"Loss value is not be set.\")\n        plt.clf()\n\n        plt.xlabel(\"IterNum [x%s]\" %self.n_view)\n        plt.ylabel(\"Loss\")\n        plt.yscale(yscale)\n        plt.plot(self.v_loss)\n        plt.show()\n\n    def view_graph(self, width='100%', res=60):\n        path = 'examples'; name = 'mlp.png'\n        path_name = path + '/' + name \n        if not os.path.exists(path):\n            os.mkdirs(path)\n        pydotprint(self.loss, path_name)\n        plt.figure(figsize=(res, res), dpi=80)\n        plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0, hspace=0.0, wspace=0.0)\n        plt.axis('off')\n        plt.imshow(np.array(Image.open(path_name)))\n        plt.show()\n\n    def pred(self, x_arr):\n        x_arr = np.array(x_arr).astype(theano.config.floatX)\n        self.n_batch.set_value(int(x_arr.shape[0]))\n        return self.pred_func(x_arr)\n\nPython\u3067\u81ea\u4f5c\u306e\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f5c\u308a\u307e\u3057\u305f(\u9014\u4e2d)\u3002\u81ea\u8eab\u306e\u52c9\u5f37\u7528\u3067\u3059\u3002\n\u540d\u524d\u306f\u4f55\u306e\u637b\u308a\u3082\u306a\u3044\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u3067\u3059\u3002\n\u5c11\u306a\u3044\u6253\u9375\u6570\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u304b\u3051\u308b\u3053\u3068\u3092\u76ee\u6307\u3057\u3066\u3044\u307e\u3059\u3002\n\u5185\u90e8\u3067\u306ftheano\u3092\u7528\u3044\u3066\u307e\u3059\u304c\u3001Define by Run \u306achainer\u306e\u307b\u3046\u304c\u4e0a\u4f4d\u4e92\u63db\u306a\u6c17\u304c\u3059\u308b\u306e\u3067\u3001\nchainer\u306b\u5909\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\u8a18\u4e8b\u306e\u6700\u5f8c\u306b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002\n\nLSTM\u3068Taylor\u306e\u5b9f\u88c5\u306f\u602a\u3057\u3044\u3067\u3059\u3002\n\n# \u4f7f\u7528\u4f8b\nKeras\u306e\u69d8\u306b\u828b\u3065\u308b\u5f0f\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u66f8\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\u3092\u6b63\u89e3\u5024\u3068\u3057\u30663\u5c64\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308a\u63a8\u5b9a\u3057\u307e\u3059\u3002\n\n```python\n# \u30c7\u30fc\u30bf\u3068\u30e9\u30d9\u30eb\u306e\u5b9a\u7fa9 \uff1a\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\nx_arr = random.rand(50).astype(theano.config.floatX) * 10\ny_arr = sin(x_arr / 5. * pi)\n\n# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u7bc9\no = optimizer(x_arr, y_arr)\no = o.taylor(1,6)\\         # 1\u5c64\u76ee\uff1a(\u81ea\u79f0)\u30c6\u30a4\u30e9\u30fc\u5c55\u958b\u5c64\n     .tanh()\\              # tanh\u306e\u6d3b\u6027\u5316\u95a2\u6570\n     .dense(1)\\      \u3000    # 2\u5c64\u76ee\uff1a\u5168\u7d50\u5408\u5c64\n     .loss(alpha=0.1)\\     # \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9(\u5b66\u7fd2\u73870.1)\n     .optimize()           # \u6700\u9069\u5316\n```\noptimize\u3059\u308b\u3068\u4ee5\u4e0b\u306eoutput\u304c\u51fa\u3066\u304d\u307e\u3059\u3002\n\n```txt\nloss: 1.51645341078\nloss: 0.0678153863793\nloss: 0.0198567226285\nloss: 0.0202341528014\nloss: 0.00505698460546\nloss: 0.00519401907162\nloss: 0.00594055800437\nloss: 0.00498228924313\nloss: 0.0044779176335\nloss: 0.00413105668256\n```\n\n\u51fa\u529b\u3092view()\u3067\u30b0\u30e9\u30d5\u3067\n\n```python\no.view()\n```\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/99da5712-757c-353c-6d1c-6c50c1572f38.png)\n\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\n\u6b63\u89e3\u3068\u63a8\u5b9a\u306e\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6210\u308a\u307e\u3059\u3002\u5b66\u7fd2\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066\u63a8\u5b9a\u3059\u308b\u306b\u306f\u3001pred()\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n```python\nplt.scatter(arange(0,10,0.1), o.pred(arange(0,10,0.1)), label=\"Exact\")\nplt.scatter(x_arr, y_arr, color=\"r\", label=\"True data\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n```\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/7b87f468-cbbd-8baa-8c86-1531ae986698.png)\n\n## \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u8868\u793a\n\n\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u8b0e\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\n```python\no = optimizer(x_arr, y_arr)\no = o.dense(1).sigmoid()\\\n     .dense(2).relu()\\\n     .dense(3).tanh()\\\n     .dense(4).sigmoid()\\\n     .lstm()\\\n     .loss()\n```\n\n\u3092\u4ee5\u4e0b\u306e\u30e1\u30bd\u30c3\u30c9\n\n```python\no.view_graph()\n```\n\n\u3067\u53ef\u8996\u5316\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6210\u308a\u307e\u3059\u3002\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/9e96a399-f664-a827-c8ff-476dc35defb7.png)\n\n\n## LSTM\n\u518d\u5e30\u7684\u306a\u69cb\u9020\u304c\u5358\u7d14\u306a\u30b5\u30a4\u30f3\u30ab\u30fc\u30d6\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306b\u3069\u306e\u3088\u3046\u306b\u52b9\u3044\u3066\u304f\u308b\u306e\u304b\u4e0d\u660e\u77ad\u3067\u3059\u304c\u3001\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n```python\narr = arange(0, 10., 0.01).astype(theano.config.floatX)\ny_arr = sin(arr[100:]   * pi)\nx_arr = sin(arr[:-100]   * pi)\no = optimizer(x_arr, y_arr)\no = o.lstm().loss().optimize()\n```\nsin\u6ce2\u304c\u30c7\u30fc\u30bf\u3067\u3001\u305d\u308c\u3088\u308a$\\pi$\u4f4d\u76f8\u3092\u305a\u3089\u3057\u305f\u3082\u306e\u304c\u30e9\u30d9\u30eb\u3067\u3059\u3002\n\n\u5927\u4f53loss\u304c\u4e0b\u304c\u3063\u305f\u3089Ctrl-c\u3067\u5207\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\u7d50\u679c\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nplt.scatter(arange(x_arr.size), x_arr, color=\"b\")\nplt.scatter(arange(x_arr.size), o.pred(x_arr), color=\"r\")\nplt.scatter(arange(x_arr.size), o.y_arr, color=\"g\")\n```\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/c23c0f58-907b-5c5d-56cc-590eb59af5aa.png)\n\ndata\u306b\u5bfe\u3057\u3066label\u306f\u3072\u3063\u304f\u308a\u8fd4\u3063\u3066\u3044\u307e\u3059\u3002predict\u304clabel\u3068\u307b\u307c\u4e00\u81f4\u3057\u305f\u306e\u3067\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u3046\u307e\u304f\u3044\u3063\u305f\u3082\u306e\u3060\u3051\u8f09\u305b\u3066\u307e\u3059\u3002\n\n\u304d\u308c\u3044\u306b\u5b66\u7fd2\u3067\u304d\u305f\u306e\u3067\u3001\u7570\u306a\u308b\u5468\u6ce2\u6570\u3067\u63a8\u5b9a\u3057\u3066\u307f\u307e\u3059\u3002\n\n```python\narr = arange(0,10,0.01)\nplt.subplot(311)\ndata = sin(arr * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(312)\ndata = sin(arr / 2 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(313)\ndata = sin(arr / 4 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n```\n\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/3797c1d5-c1ee-3f97-5b45-f4575f23166b.png)\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u7570\u306a\u308b\u5468\u6ce2\u6570\u3067\u3082\u304d\u308c\u3044\u306b\u53cd\u8ee2\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u5b66\u7fd2\u5668\u306b\u306f\u30c7\u30fc\u30bf\u3092\u5de6\u304b\u3089\u53f3\u306b\u5165\u308c\u3066\u3044\u307e\u3059\u3002LSTM\u306f\u30ea\u30ab\u30ec\u30f3\u30c8\u306a\u69cb\u9020\u3092\u6301\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3053\u306e\u63a8\u5b9a\u7d50\u679c\u306f\u5165\u529b\u306e\u9806\u756a\u306b\u4f9d\u5b58\u3059\u308b\u306f\u305a\u3001\u3068\u601d\u3063\u305f\u306e\u3067\u3001\u4e0a\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306brandom.shuffle(arr)\u3092\u305f\u3057\u3066\u5b66\u7fd2\u5668\u306b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u9806\u756a\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n```python\narr = arange(0,10,0.01)\nrandom.shuffle(arr)\n#arr = random.rand(1000) * 10\nplt.subplot(311)\ndata = sin(arr * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(312)\ndata = sin(arr / 2 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.subplot(313)\ndata = sin(arr / 4 * pi)\nplt.scatter(arr, data, color=\"r\", label=\"True data\")\nplt.scatter(arr, o.pred(data), label=\"Predict\")\nplt.xlabel(\"x\");plt.ylabel(\"y\")\nplt.legend()\nplt.show()\n```\n\n\u7d50\u679c\u306f\u3053\u306e\u901a\u308a\u3067\u3059\u3002\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/fc758e82-e3b7-7a97-1798-ed867df20299.png)\n\n\u4f4e\u5468\u6ce2\u6570\u306f\u518d\u73fe\u3067\u304d\u3066\u307e\u3059\u304c\u3001\u30ce\u30a4\u30ba\u306e\u3088\u3046\u306b\u9ad8\u5468\u6ce2\u304c\u76ee\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u3084\u306f\u308a\u9806\u756a\u304c\u5927\u4e8b\u307f\u305f\u3044\u3067\u3059\u306d\u3002\n\n# MNIST\n\n## \u5168\u7d50\u5408\n\n\u3044\u304f\u3064\u304b\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092```set_datasets()```\u306b\u3066\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002\n\n```python\no = optimizer(n_batch=500)\no.set_datasets(\"mnist\", is_one_hot=False)\no = o.dense(10).loss_softmax_cross_entropy().opt_sgd(0.000001).optimize(100000,10000)\n\nimport pylab as p\nfor n in range(9):\n    \n    idx = random.randint(0,59999)\n    p.subplot(331 + n)\n    p.tick_params(labelbottom=\"off\")\n    p.tick_params(labelleft=\"off\")\n    p.title(o.pred_func(o.x_train_arr[idx:idx+1]).argmax(axis=1)[0])\n    p.imshow(o.x_train_arr[idx:idx+1].reshape(28,28))\n```\n\u51fa\u529b\n\n```\nIter. 0: loss = 2.3025851249694824\nIter. 10000: loss = 0.3245639503002167\nIter. 20000: loss = 0.22792282700538635\nKeyboardInterrupt\n```\n\n![Unknown.png](https://qiita-image-store.s3.amazonaws.com/0/85968/140220ee-b829-26fc-7a81-5f32c3478999.png)\n\n## CNN\n\n```python\no = optimizer(n_batch=5)\no.set_datasets(\"mnist\", is_one_hot=False)\n#o = o.reshape((n_batch,1,28,28)).conv2d(kshape=(4,1,24,24)).relu().pool()\no = o.reshape((1, 28, 28))\\\n     .conv_and_pool(8, 20, 20)\\\n     .conv_and_pool( 4,  5,  5).sigmoid()\no = o.flatten().dropout(0.5).dense(10).loss_softmax_cross_entropy()\no = o.opt_Adam(0.008).optimize(10000000,1000)\n```\n\n# \u30b3\u30fc\u30c9\n\n```python\nimport theano\nimport theano.tensor as T\nimport theano.tensor.nnet as nnet\nimport theano.tensor.signal as signal \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport copy as cp\nfrom theano.printing import pydotprint\nfrom PIL import Image\nimport os\nimport matplotlib as mpl\nfrom theano.tensor.shared_randomstreams import RandomStreams\nfrom sklearn.datasets import *\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.cross_validation import train_test_split\nmpl.rc(\"savefig\", dpi=1200)\n\n%config InlineBackend.rc = {'font.size': 20, 'figure.figsize': (12.0, 8.0),'figure.facecolor': 'white', 'savefig.dpi': 72, 'figure.subplot.bottom': 0.125, 'figure.edgecolor': 'white'}\n%matplotlib inline\n\nclass optimizer:\n    def __init__(self, x_arr=None, y_arr=None, \n                 out=None, thetalst=None, nodelst=None,\n                 test_size=0.1, n_batch=500):\n        \n        self.n_batch = theano.shared(int(n_batch))\n        \n        if x_arr is not None and y_arr is not None:\n            self.set_data(x_arr, y_arr, test_size)\n            self.set_variables()\n        \n        \n        \n        self.thetalst = [] #if thetalst is None else thetalst\n        \n        self.n_view = None\n        self.updatelst = []\n        self.tmplst = []\n    \n    def set_data(self, x_arr, y_arr, test_size=0.1):\n        self.x_train_arr, \\\n        self.x_test_arr,\\\n        self.y_train_arr,\\\n        self.y_test_arr,\\\n        = train_test_split(x_arr.astype(theano.config.floatX),\n                           y_arr.astype(theano.config.floatX),\n                           test_size = test_size)\n        \n        self.nodelst = [[int(np.prod(self.x_train_arr.shape[1:]))]] # if nodelst is None else nodelst\n        \n    \n    def set_variables(self):\n        if self.n_batch.get_value() > self.x_train_arr.shape[0]: \n            self.n_batch.set_value(int(self.x_train_arr.shape[0]))\n        self.n_data = self.x_train_arr.shape[0]\n        n_xdim = self.x_train_arr.ndim\n        n_ydim = self.y_train_arr.ndim\n        if  n_xdim == 0:\n            self.x = T.scalar()\n        if  n_xdim == 1:\n            self.x_train_arr = self.x_train_arr[:,None]\n            self.x_test_arr = self.x_test_arr[:,None]\n            self.x = T.matrix()\n        elif n_xdim == 2:\n            self.x = T.matrix()\n        elif n_xdim == 3:\n            self.x = T.tensor3()\n        else:\n            self.x = T.tensor4()\n            \n        if n_ydim == 0:\n            self.y = T.scalar()\n        if n_ydim == 1:\n            self.y_train_arr = self.y_train_arr[:,None]\n            self.y_test_arr = self.y_test_arr[:,None]\n            self.y = T.matrix()\n        elif n_ydim == 2:\n            self.y = T.matrix()\n        elif n_ydim == 3:\n            self.y = T.tensor3()\n        else:\n            self.y = T.tensor4()\n            \n        self.out = self.x  #if out is None else out\n        self.batch_shape_of_C = T.concatenate([T.as_tensor([self.n_batch]), theano.shared(np.array([3]))], axis=0)\n        \n    def set_datasets(self, data=\"mnist\", data_home=\"data_dir_for_optimizer\", is_one_hot=True):\n        \n        if data == \"mnist\":\n            data_dic = fetch_mldata('MNIST original', data_home=data_home)\n            if is_one_hot == True:\n                idx = data_dic[\"target\"]\n                arr = np.zeros((idx.shape[0],10)).flatten()\n                arr[idx.flatten().astype(int) + np.arange(idx.shape[0]) * 10]  = 1\n                data_dic[\"target\"] = arr.reshape(idx.shape[0], 10)\n        elif data == \"boston\":\n            data_dic = load_boston()   \n        elif data == \"digits\":\n            data_dic = load_digits()\n        elif data == \"iris\":\n            data_dic = load_iris()\n        elif data == \"linnerud\":\n            data_dic = load_linnerud()\n        elif data == \"xor\":\n            data_dic = {\"data\": np.array([[0,0], [0,1], [1,0], [1,1]]),##.repeat(20, axis=0),\n                        \"target\": np.array([0,1,1,0])}#.repeat(20, axis=0)}\n            #data_dic = {\"data\": np.array([[0,0], [0,1], [1,0], [1,1]]).repeat(20, axis=0),\n            #            \"target\": np.array([0,1,1,0]).repeat(20, axis=0)}\n        elif data == \"serial\":\n            data_dic = {\"data\": np.array(np.arange(20).reshape(5,4)).repeat(20, axis=0),\n                        \"target\": np.arange(5).repeat(20, axis=0)}\n        elif data == \"sin\":\n            data_dic = {\"data\": np.arange(0,10,0.01)[:,None],\n                        \"target\": np.sin(np.arange(0,10,0.01) * np.pi)}\n            \n        self.set_data(data_dic[\"data\"], data_dic[\"target\"])\n        self.set_variables()\n    \n    def copy(self):\n        return cp.copy(self)\n    \n    def update_node(self, n_out):\n        self.nodelst = self.nodelst + [n_out]\n        \n    def get_curr_node(self):\n        return list(self.nodelst[-1])\n    \n    def dropout(self, rate=0.5, seed=None):\n        obj = self.copy()\n        \n        srng = RandomStreams(seed)\n        obj.out = T.where(srng.uniform(size=obj.out.shape) > rate, obj.out, 0)\n        \n        return obj\n    \n        \n    def dense(self, n_out):\n        obj = self.copy()\n        n_in = obj.get_curr_node()[-1]\n        \n        #theta =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        #b  =     theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        theta =  theano.shared(np.zeros((n_in, n_out)).astype(theano.config.floatX))\n        #b     =  theano.shared(np.zeros((n_out)).astype(theano.config.floatX))\n        b = theano.shared(np.random.rand(1).astype(dtype=theano.config.floatX)[0])\n        \n        obj.out = obj.out.dot(theta) + b\n        #obj.out   = theta.dot(obj.out.flatten())+b\n        obj.thetalst += [theta,b]\n        \n        obj.update_node([n_out])\n        \n        return obj\n    \n    def lstm(self):\n        obj = self.copy()\n        \n        curr_shape = obj.get_curr_node()\n        n_in = n_out = curr_shape[-1]\n        \n        #batch_shape_of_h = T.concatenate(\n        #batch_shape_of_C = T.concatenate(, axis=0)\n#        h = T.ones(theano.sharedl())\n        h = T.zeros([obj.n_batch, *curr_shape], dtype=theano.config.floatX)\n        C = T.zeros([obj.n_batch, n_out], dtype=theano.config.floatX)\n        \n        Wi =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wf =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wc =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Wo =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        bi =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bf =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bc =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        bo =  theano.shared(np.random.rand(n_out).astype(theano.config.floatX))\n        Ui =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uf =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uc =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        Uo =  theano.shared(np.random.rand(n_in, n_out).astype(theano.config.floatX))\n        \n        i = nnet.sigmoid(obj.out.dot(Wi) + h.dot(Ui) + bi)\n        \n        C_tilde = T.tanh(obj.out.dot(Wc) + h.dot(Uc) + bc)\n        \n        f = nnet.sigmoid(obj.out.dot(Wf) + h.dot(Uf) + bf)\n        \n        tmp = (i * C_tilde + f * C).reshape(C.shape)\n        \n        obj.tmplst += [(C, tmp)]\n        \n        C = tmp\n        \n        o = nnet.sigmoid(obj.out.dot(Wo) + h.dot(Uo) + bo)\n        \n        tmp = (o * T.tanh(C)).reshape(h.shape)\n        \n        obj.tmplst += [(h, tmp)]\n        \n        obj.out =  tmp\n        \n        obj.thetalst += [Wi, bi, Ui, Wf, bf, Uf, Wc, bc, Uc, Wo, bo, Uo]\n        \n        obj.update_node([n_out])\n        \n        return obj\n\n    \n    def conv2d(self, kshape=(1,1,3,3), mode=\"full\", reshape=None):\n        obj = self.copy()\n        \n        n_in = obj.get_curr_node()\n        \n        if reshape is not None:\n            obj.out = obj.out.reshape(reshape)\n            n_in = reshape\n            \n        if obj.out.ndim == 2:\n            obj.out = obj.out[None, :, :]\n            n_in = (1, n_in[1], n_in[2])\n        elif obj.out.ndim == 1:\n            obj.out = obj.out[None, None, :]\n            n_in = [1, 1] + list(n_in)\n            \n        if mode == \"full\":\n            n_out = [kshape[0], n_in[-2] + (kshape[-2] - 1), n_in[-1] + (kshape[-2] - 1)]\n            #n_out = [n_in[0], kshape[0], n_in[-2] + (kshape[-2] - 1), n_in[-1] + (kshape[-2] - 1)]\n        elif mode == \"valid\":\n            n_out = [kshape[0], n_in[-2] - (kshape[-2] - 1), n_in[-1] - (kshape[-2] - 1)]\n        \n        theta = theano.shared(np.random.rand(*kshape).astype(dtype=theano.config.floatX))\n        b = theano.shared(np.random.rand(1).astype(dtype=theano.config.floatX)[0])\n        obj.b = b\n            \n        \n        obj.out = nnet.conv2d(obj.out, theta, border_mode=mode) + b\n        obj.thetalst += [theta, b]\n        \n        obj.update_node(n_out)\n        \n        return obj\n    \n    def conv_and_pool(self, fnum, height, width, mode=\"full\", ds=(2,2)):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n        kshape = (fnum, n_in[0], height, width)\n        n_batch = obj.n_batch.get_value()\n        obj = obj.conv2d(kshape=kshape)\n        #.reshape((n_batch, np.array(n_in, dtype=int).sum()))\\\n        \n        if mode == \"full\":\n            obj = obj.relu().pool(ds=ds)\n            n_in = obj.get_curr_node()\n            obj = obj.reshape((kshape[0], *n_in[-2:]))\n            #obj = obj.reshape((kshape[0], M[0]+(m[0]-1),M[1]+(m[1]-1)))\n        elif mode == \"valid\":\n            n_in = obj.get_curr_node()\n            obj = obj.reshape((kshape[0], *n_in[-2:]))\n        return obj\n            \n    \n    def pool(self, ds=(2,2)):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n        \n        obj.out = signal.pool.pool_2d(obj.out, ds=ds, ignore_border=True)\n        \n        n_in[-2] = n_in[-2] // ds[0] #+ (1 if (n_in[-2] % ds[0]) else 0) \n        n_in[-1] = n_in[-1] // ds[1] #+ (1 if (n_in[-1] % ds[1]) else 0) \n        \n        obj.update_node(n_in)\n        #print(obj.nodelst)\n        \n        return obj\n    \n    def mean(self, axis):\n        obj = self.copy()\n        \n        n_in = obj.get_curr_node()\n        obj.out = obj.out.mean(axis=axis)\n        obj.update_node(np.ones(n_in).mean(axis=axis).shape)\n        return obj\n    \n    def reshape(self, shape):\n        obj = self.copy()\n        obj.out = obj.out.reshape([obj.n_batch, *shape])\n        obj.update_node(shape)\n        return obj\n    def reshape_(self, shape):\n        obj = self.copy()\n        obj.out = obj.out.reshape(shape)\n        obj.update_node(shape[1:])\n        return obj\n    \n    def flatten(self):\n        obj = self.copy()\n        n_in = obj.get_curr_node()\n        last_ndim = np.array(n_in).prod()\n        obj.out = obj.out.reshape((obj.n_batch, last_ndim))\n        obj.update_node([last_ndim])\n        return obj\n    \n    def taylor(self, M, n_out):\n        obj = self.copy()\n        n_in = int(np.asarray(obj.get_curr_node()).sum())\n        \n        x_times = T.concatenate([obj.out, T.ones((obj.n_batch, 1)).astype(theano.config.floatX)],axis=1)\n        for i in range(M-1):\n            idx = np.array([\":,\"]).repeat(i+1).tolist()\n            a = dict()\n            exec (\"x_times = x_times[:,\" + \"\".join(idx) + \"None] * x_times\", locals(), a)\n            x_times = a[\"x_times\"]\n\n        x_times = x_times.reshape((obj.n_batch, -1))\n        #x_times = x_times.reshape(self.n_batch, (n_in+1) ** M)\n\n        theta = theano.shared(np.ones(((n_in+1) ** M, n_out)).astype(theano.config.floatX))\n        obj.theta = theano.shared(np.ones(((n_in+1) ** M, n_out)).astype(theano.config.floatX))\n        #theta = theano.shared(np.random.rand(n_out, (n_in+1) ** M).astype(theano.config.floatX))\n        \n        obj.out = x_times.dot(theta.astype(theano.config.floatX))\n        obj.thetalst += [theta]\n        \n        obj.update_node([n_out])\n        \n        return obj\n\n        \n    def relu(self, ):\n        obj = self.copy()\n        obj.out = nnet.relu(obj.out)\n        return obj\n    \n    def tanh(self, ):\n        obj = self.copy()\n        obj.out = T.tanh(obj.out)\n        return obj\n    \n    def sigmoid(self, ):\n        obj = self.copy()\n        obj.out = nnet.sigmoid(obj.out)\n        return obj\n    \n    def softmax(self, ):\n        obj = self.copy()\n        obj.out = nnet.softmax(obj.out)\n        return obj\n        \n    def loss_msq(self):\n        obj = self.copy()\n        obj.loss =  T.mean((obj.out - obj.y) ** 2)\n        return obj\n    \n    def loss_cross_entropy(self):\n        obj = self.copy()\n        obj.loss =  -T.mean(obj.y * T.log(obj.out + 1e-4))\n        return obj\n    \n    def loss_softmax_cross_entropy(self):\n        obj = self.copy()\n        obj.out = nnet.softmax(obj.out)\n        tmp_y = T.cast(obj.y, \"int32\")\n        obj.loss  = -T.mean(T.log(obj.out)[T.arange(obj.y.shape[0]), tmp_y.flatten()])\n        obj.out = obj.out.argmax(axis=1)\n        #obj.out2 = obj.out.argmax(axis=1)\n        #obj.out2 = obj.out[T.arange(obj.y.shape[0]), tmp_y.flatten()]\n        \n        \n#        obj.loss2 = T.log(obj.out)[T.arange(obj.y.shape[0]), tmp_y.flatten()]\n        return obj\n    \n    def opt_sgd(self, alpha=0.1):\n        obj = self.copy()\n        obj.updatelst = []\n        for theta in obj.thetalst:\n            obj.updatelst += [(theta, theta - (alpha * T.grad(obj.loss, wrt=theta)))]\n            \n        obj.updatelst += obj.tmplst\n        return obj\n    \n    def opt_RMSProp(self, alpha=0.1, gamma=0.9, ep=1e-8):\n        obj = self.copy()\n        obj.updatelst = []\n        obj.rlst = [theano.shared(x.shape).astype(theano.config.floatX) for x in obj.thetalst]\n        \n        for r, theta in zip(obj.rlst, obj.thetalst):\n            g = T.grad(obj.loss, wrt=theta)\n            obj.updatelst += [(r, gamma * r + (1 - gamma) * g ** 2),\\\n                              (theta, theta - (alpha / (T.sqrt(r) + ep)) * g)]\n        obj.updatelst += obj.tmplst\n        return obj\n                               \n    def opt_AdaGrad(self, ini_eta=0.001, ep=1e-8):\n        obj = self.copy()\n        obj.updatelst = []\n                               \n        obj.hlst = [theano.shared(ep*np.ones(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n        obj.etalst = [theano.shared(ini_eta*np.ones(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n        \n        for h, eta, theta in zip(obj.hlst, obj.etalst, obj.thetalst):\n            g   = T.grad(obj.loss, wrt=theta)\n            obj.updatelst += [(h,     h + g ** 2),\n                              (eta,   eta / T.sqrt(h+1e-4)),\n                              (theta, theta - eta * g)]\n            \n        obj.updatelst += obj.tmplst\n        return obj\n    \n    def opt_Adam(self, alpha=0.001, beta=0.9, gamma=0.999, ep=1e-8, t=3):\n        obj = self.copy()\n        obj.updatelst = []\n        obj.nulst = [theano.shared(np.zeros(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n        obj.rlst = [theano.shared(np.zeros(x.get_value().shape, theano.config.floatX)) for x in obj.thetalst]\n                               \n        for nu, r, theta in zip(obj.nulst, obj.rlst, obj.thetalst):\n            g = T.grad(obj.loss, wrt=theta)\n            nu_hat = nu / (1 - beta)\n            r_hat = r / (1 - gamma)\n            obj.updatelst += [(nu, beta * nu + (1 - beta) * g),\\\n                              (r, gamma * r +  (1 - gamma) * g ** 2),\\\n                              (theta, theta - alpha*(nu_hat / (T.sqrt(r_hat) + ep)))]\n            \n        obj.updatelst += obj.tmplst\n        return obj\n                               \n    \n    \n    def optimize(self, n_epoch=10, n_view=1000):\n        obj = self.copy()\n        obj.dsize = obj.x_train_arr.shape[0]\n        if obj.n_view is None: obj.n_view = n_view  \n        \n        x_train_arr_shared = theano.shared(obj.x_train_arr).astype(theano.config.floatX)\n        y_train_arr_shared = theano.shared(obj.y_train_arr).astype(theano.config.floatX)\n\n        #rng = T.shared_randomstreams.RandomStreams(seed=0)\n        #obj.idx = rng.permutation(size=(1,), n=obj.x_train_arr.shape[0]).astype(\"int64\")\n        #obj.idx = rng.permutation(size=(1,), n=obj.x_train_arr.shape[0])[0, 0:obj.n_batch].astype(\"int64\")\n        #idx = obj.idx * 1\n        i = theano.shared(0).astype(\"int32\")\n        idx = theano.shared(np.random.permutation(obj.x_train_arr.shape[0]))\n        \n        obj.loss_func = theano.function(inputs=[i],\n                                      outputs=obj.loss,\n                                      givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                              (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                                      updates=obj.updatelst,\n                                      on_unused_input='warn')\n        \n        i = theano.shared(0).astype(\"int32\")\n        idx = theano.shared(np.random.permutation(obj.x_train_arr.shape[0]))\n        \n        \n        obj.acc_train = theano.function(inputs=[i],\n                                      #outputs=((T.eq(obj.out2[:,None],obj.y))),\n                                      outputs=((T.eq(obj.out[:,None],obj.y)).sum().astype(theano.config.floatX) / obj.n_batch),\n                                      givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                              (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                                      on_unused_input='warn')\n        #idx = rng.permutation(size=(1,), a=obj.x_train_arr.shape[0])#.astype(\"int8\")\n        \n        #idx = rng.permutation(size=(1,), n=500, ndim=None)[0,0:10]#.astype(\"int8\")\n        \n        c = T.concatenate([obj.x,obj.y],axis=1)\n        obj.test_func = theano.function(inputs=[i],\n                               outputs=c,\n                               givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n                                       (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n                               on_unused_input='warn')\n        \n#        obj.test_func = theano.function(inputs=[i],\n#                               outputs=c,\n#                               givens=[(obj.x,x_train_arr_shared[idx[i:obj.n_batch+i],]), \n#                                       (obj.y,y_train_arr_shared[idx[i:obj.n_batch+i],])],\n#                               on_unused_input='warn')\n        \n        obj.pred_func = theano.function(inputs  = [obj.x],\n                                        outputs = obj.out,\n                                        updates = obj.tmplst,\n                                        on_unused_input='warn')\n            \n        obj.v_loss = []\n        try:\n            for epoch in range(n_epoch):\n                for i in range(0,obj.dsize-obj.n_batch.get_value(),obj.n_batch.get_value()):\n                    mean_loss = obj.loss_func(i) \n                \n                print(\"Epoch. %s: loss = %s, acc = %s.\" %(epoch, mean_loss, obj.acc_train(i)))\n                     \n                obj.v_loss += [mean_loss]\n            \n            \n        except KeyboardInterrupt  :\n            print ( \"KeyboardInterrupt\\n\" )\n            return obj\n        return obj\n    \n    def view_params(self):\n        for theta in self.thetalst:\n            print(theta.get_value().shape)\n            print(theta.get_value())\n            \n        \n    def view(self, yscale=\"log\"):\n        if not len(self.v_loss):\n            raise ValueError(\"Loss value is not be set.\")\n        plt.clf()\n        \n        plt.xlabel(\"IterNum [x%s]\" %self.n_view)\n        plt.ylabel(\"Loss\")\n        plt.yscale(yscale)\n        plt.plot(self.v_loss)\n        plt.show()\n    \n    def view_graph(self, width='100%', res=60):\n        path = 'examples'; name = 'mlp.png'\n        path_name = path + '/' + name \n        if not os.path.exists(path):\n            os.mkdirs(path)\n        pydotprint(self.loss, path_name)\n        plt.figure(figsize=(res, res), dpi=80)\n        plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0, hspace=0.0, wspace=0.0)\n        plt.axis('off')\n        plt.imshow(np.array(Image.open(path_name)))\n        plt.show()\n    \n    def pred(self, x_arr):\n        x_arr = np.array(x_arr).astype(theano.config.floatX)\n        self.n_batch.set_value(int(x_arr.shape[0]))\n        return self.pred_func(x_arr)\n```\n", "tags": ["Python", "DeepLearning", "Theano", "LSTM"]}