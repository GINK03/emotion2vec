{"context": "\n\n\u306f\u3058\u3081\u306b\nChainer\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u3002\u3067\u3082\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002\n\u305d\u3046\u3060\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308bXOR\u306e\u5b66\u7fd2\u304b\u3089\u521d\u3081\u3066\u307f\u3088\u3046\u3002\n\u203bChainer\u3092\u4f7f\u3048\u308b\u74b0\u5883\u304c\u6574\u3063\u3066\u3044\u308b\u524d\u63d0\u3067\u672c\u8a18\u4e8b\u306f\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002\n\u672c\u8a18\u4e8b\u3067\u4f7f\u7528\u3059\u308b\u30b3\u30fc\u30c9\n\nGitHub/ichiroex/xor-mlp-chainer\n\n\n\u74b0\u5883\n\npython 2.7\u7cfb\nchainer 1.6.2.1\n\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\n# Prepare dataset\nsource = [[0, 0], [1, 0], [0, 1], [1, 1]]\ntarget = [[0], [1], [1], [0]]\ndataset = {}\ndataset['source'] = np.array(source, dtype=np.float32)\ndataset['target'] = np.array(target, dtype=np.float32)\n\n\n\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u5229\u7528\u3059\u308b\u30e2\u30c7\u30eb\u306f\u30012\u5165\u529b\u30011\u51fa\u529b\u3067\u3059\u3002\nN = len(source) # train data size\n\nin_units  = 2   # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_units   = 2   # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nout_units = 1   # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = chainer.Chain(l1=L.Linear(in_units, n_units),\n                      l2=L.Linear(n_units , out_units))\n\n\n\n\u9806\u4f1d\u642c\ndef forward(x, t):\n    h1 = F.sigmoid(model.l1(x))\n    return model.l2(h1)\n\n\n\u5b66\u7fd2\n\u8a13\u7df4\u8aa4\u5dee\u304c0.00001\u672a\u6e80\u3001\u307e\u305f\u306f\u3001epoch\u304cn_epoch\u4ee5\u4e0a\u306b\u306a\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\u307e\u3059\u3002\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n# Learning loop\nloss_val = 100\nepoch = 0\nwhile loss_val > 1e-5:\n\n    # training\n    x = chainer.Variable(xp.asarray(dataset['source'])) #source\n    t = chainer.Variable(xp.asarray(dataset['target'])) #target\n\n    model.zerograds()       # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n    y    = forward(x, t)    # \u9806\u4f1d\u642c\n\n    loss = F.mean_squared_error(y, t) #\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n\n    loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n    optimizer.update()           # \u6700\u9069\u5316 \n\n    # \u9014\u4e2d\u7d50\u679c\u3092\u8868\u793a\n    if epoch % 1000 == 0:\n        #\u8aa4\u5dee\u3068\u6b63\u89e3\u7387\u3092\u8a08\u7b97\n        loss_val = loss.data\n\n        print 'epoch:', epoch\n        print 'x:\\n', x.data\n        print 't:\\n', t.data\n        print 'y:\\n', y.data\n\n        print('train mean loss={}'.format(loss_val)) # \u8a13\u7df4\u8aa4\u5dee, \u6b63\u89e3\u7387\n        print ' - - - - - - - - - '\n\n    # n_epoch\u4ee5\u4e0a\u306b\u306a\u308b\u3068\u7d42\u4e86\n    if epoch >= n_epoch:\n        break\n\n    epoch += 1\n\n#model\u3068optimizer\u3092\u4fdd\u5b58\nprint 'save the model'\nserializers.save_npz('xor_mlp.model', model)\nprint 'save the optimizer'\nserializers.save_npz('xor_mlp.state', optimizer)\n\n\n\u5b9f\u884c\u7d50\u679c\n\u56de\u5e30\u554f\u984c\u3068\u3057\u3066\u5b66\u7fd2\u3057\u3066\u3044\u307e\u3059\u3002\n\u4e88\u6e2c\u3059\u308b\u6642\u306f\u30010.5\u4ee5\u4e0a\u306a\u30891\u30010.5\u672a\u6e80\u306a\u30890\u3001\u306e\u3088\u3046\u306b\u95be\u5024\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n$ python train_xor.py --gpu 1\nepoch: 0\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[-0.62479508]  # 0\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.85900736]  # 1\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.4117983 ]  # 1\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.62129647]] # 0\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\ntrain mean loss=1.55636525154  # \u8a13\u7df4\u8aa4\u5dee (\u5c0f\u3055\u304f\u306a\u3063\u3066\u307b\u3057\u3044)\n - - - - - - - - -\nepoch: 1000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[ 0.39130747]\n [ 0.40636665]\n [ 0.50217605]\n [ 0.52426183]]\ntrain mean loss=0.257050335407\n - - - - - - - - -\n\n...\n\n\n - - - - - - - - -\nepoch: 8000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[ 0.00557911]\n [ 0.98262894]\n [ 0.98446763]\n [ 0.02371788]]\ntrain mean loss=0.000284168170765\n - - - - - - - - -\nepoch: 9000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[  5.99622726e-05] # 0\u306b\u8fd1\u3065\u3044\u305f\n [  9.99812365e-01] # 1\u306b\u8fd1\u3065\u3044\u305f\n [  9.99832511e-01] # 1\u306b\u8fd1\u3065\u3044\u305f\n [  2.56299973e-04]] # 0\u306b\u8fd1\u3065\u3044\u305f\ntrain mean loss=3.31361960093e-08\n - - - - - - - - -\nsave the model\nsave the optimizer\n\n\n\n\n\u53c2\u8003\u8a18\u4e8b\n\nXOR\u306e\u5b66\u7fd2\nchainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b66\u3093\u3067\u307f\u308b\u3088(chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c82)\n\n#\u306f\u3058\u3081\u306b\nChainer\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u3002\u3067\u3082\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002\n\u305d\u3046\u3060\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308bXOR\u306e\u5b66\u7fd2\u304b\u3089\u521d\u3081\u3066\u307f\u3088\u3046\u3002\n\n**\u203bChainer\u3092\u4f7f\u3048\u308b\u74b0\u5883\u304c\u6574\u3063\u3066\u3044\u308b\u524d\u63d0\u3067\u672c\u8a18\u4e8b\u306f\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002**\n\n**\u672c\u8a18\u4e8b\u3067\u4f7f\u7528\u3059\u308b\u30b3\u30fc\u30c9**\n\n - [GitHub/ichiroex/xor-mlp-chainer](https://github.com/ichiroex/xor-mlp-chainer)\n\n#\u74b0\u5883\n- python 2.7\u7cfb\n- chainer 1.6.2.1\n\n#\u5b66\u7fd2\u30c7\u30fc\u30bf\n```\n# Prepare dataset\nsource = [[0, 0], [1, 0], [0, 1], [1, 1]]\ntarget = [[0], [1], [1], [0]]\ndataset = {}\ndataset['source'] = np.array(source, dtype=np.float32)\ndataset['target'] = np.array(target, dtype=np.float32)\n```\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u5229\u7528\u3059\u308b\u30e2\u30c7\u30eb\u306f\u30012\u5165\u529b\u30011\u51fa\u529b\u3067\u3059\u3002\n\n```\nN = len(source) # train data size\n\nin_units  = 2   # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_units   = 2   # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nout_units = 1   # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = chainer.Chain(l1=L.Linear(in_units, n_units),\n                      l2=L.Linear(n_units , out_units))\n\n```\n\n#\u9806\u4f1d\u642c\n```\ndef forward(x, t):\n    h1 = F.sigmoid(model.l1(x))\n    return model.l2(h1)\n```\n\n#\u5b66\u7fd2\n\u8a13\u7df4\u8aa4\u5dee\u304c0.00001\u672a\u6e80\u3001\u307e\u305f\u306f\u3001epoch\u304cn_epoch\u4ee5\u4e0a\u306b\u306a\u308b\u307e\u3067\u7e70\u308a\u8fd4\u3057\u307e\u3059\u3002\n\n```\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n# Learning loop\nloss_val = 100\nepoch = 0\nwhile loss_val > 1e-5:\n\n    # training\n    x = chainer.Variable(xp.asarray(dataset['source'])) #source\n    t = chainer.Variable(xp.asarray(dataset['target'])) #target\n    \n    model.zerograds()       # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n    y    = forward(x, t)    # \u9806\u4f1d\u642c\n\n    loss = F.mean_squared_error(y, t) #\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n    \n    loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n    optimizer.update()           # \u6700\u9069\u5316 \n    \n    # \u9014\u4e2d\u7d50\u679c\u3092\u8868\u793a\n    if epoch % 1000 == 0:\n        #\u8aa4\u5dee\u3068\u6b63\u89e3\u7387\u3092\u8a08\u7b97\n        loss_val = loss.data\n\n        print 'epoch:', epoch\n        print 'x:\\n', x.data\n        print 't:\\n', t.data\n        print 'y:\\n', y.data\n\n        print('train mean loss={}'.format(loss_val)) # \u8a13\u7df4\u8aa4\u5dee, \u6b63\u89e3\u7387\n        print ' - - - - - - - - - '\n    \n    # n_epoch\u4ee5\u4e0a\u306b\u306a\u308b\u3068\u7d42\u4e86\n    if epoch >= n_epoch:\n        break\n\n    epoch += 1\n\n#model\u3068optimizer\u3092\u4fdd\u5b58\nprint 'save the model'\nserializers.save_npz('xor_mlp.model', model)\nprint 'save the optimizer'\nserializers.save_npz('xor_mlp.state', optimizer)\n```\n\n#\u5b9f\u884c\u7d50\u679c\n\u56de\u5e30\u554f\u984c\u3068\u3057\u3066\u5b66\u7fd2\u3057\u3066\u3044\u307e\u3059\u3002\n\u4e88\u6e2c\u3059\u308b\u6642\u306f\u30010.5\u4ee5\u4e0a\u306a\u30891\u30010.5\u672a\u6e80\u306a\u30890\u3001\u306e\u3088\u3046\u306b\u95be\u5024\u3092\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n```\n$ python train_xor.py --gpu 1\nepoch: 0\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[-0.62479508]  # 0\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.85900736]  # 1\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.4117983 ]  # 1\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\n [-0.62129647]] # 0\u306b\u8fd1\u3065\u3044\u3066\u6b32\u3057\u3044\ntrain mean loss=1.55636525154  # \u8a13\u7df4\u8aa4\u5dee (\u5c0f\u3055\u304f\u306a\u3063\u3066\u307b\u3057\u3044)\n - - - - - - - - -\nepoch: 1000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[ 0.39130747]\n [ 0.40636665]\n [ 0.50217605]\n [ 0.52426183]]\ntrain mean loss=0.257050335407\n - - - - - - - - -\n\n...\n\n\n - - - - - - - - -\nepoch: 8000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[ 0.00557911]\n [ 0.98262894]\n [ 0.98446763]\n [ 0.02371788]]\ntrain mean loss=0.000284168170765\n - - - - - - - - -\nepoch: 9000\nx:\n[[ 0.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 1.  1.]]\nt:\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\ny:\n[[  5.99622726e-05] # 0\u306b\u8fd1\u3065\u3044\u305f\n [  9.99812365e-01] # 1\u306b\u8fd1\u3065\u3044\u305f\n [  9.99832511e-01] # 1\u306b\u8fd1\u3065\u3044\u305f\n [  2.56299973e-04]] # 0\u306b\u8fd1\u3065\u3044\u305f\ntrain mean loss=3.31361960093e-08\n - - - - - - - - -\nsave the model\nsave the optimizer\n\n```\n\n![chart.png](https://qiita-image-store.s3.amazonaws.com/0/92685/6bc5948e-e44a-aba1-c72f-e42fc456b0d1.png)\n\n\n#\u53c2\u8003\u8a18\u4e8b\n- [XOR\u306e\u5b66\u7fd2](http://chainernlpman.hatenablog.com/entry/2016/02/17/024158)\n- [chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b66\u3093\u3067\u307f\u308b\u3088(chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c82)](http://hi-king.hatenablog.com/entry/2015/06/27/194630)\n", "tags": ["Chainer", "Python", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "\u521d\u5fc3\u8005"]}