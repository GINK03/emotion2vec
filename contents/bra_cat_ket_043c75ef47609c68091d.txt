{"context": " More than 1 year has passed since last update.\n\n\u306f\u3058\u3081\u306b\nRWH\u304c\u3042\u307e\u308a\u306b\"\u5b9f\u201d\u904e\u304e\u3066\u3061\u3087\u3063\u3068\u98df\u50b7\u6c17\u5473\u306a\u4e0a\u306b\u672a\u6d88\u5316\u3060\u3063\u305f\u3082\u306e\u3067\u305b\u3081\u3066\u3053\u306e\u51ac\u306b\u4f55\u304b\u8aad\u3082\u3046\u3068\u601d\u3063\u3066\u4f55\u304b\u30cd\u30bf\u306f\u3068\u63a2\u3057\u3066\u307f\u305f\u3068\u3053\u308d\u3053\u3093\u306a\u3082\u306e\u3092\u898b\u3064\u3051\u305f\u306e\u3067\u8aad\u66f8\u30e1\u30e2\u304c\u3066\u3089\u3002\n\u672c\u306e\u8a73\u7d30\u306f\u4e0a\u8a18\u306e\u30b5\u30a4\u30c8\u3067\u306f\nAlgorithms:\nA Functional Programming Approach\nFethi Rabhi\nGuy Lapalme\nAddison-Wesley, ISBN 0-201-59604-0\n256 pages, paperback, 1999\n\u3068\u306a\u3063\u3066\u307e\u3059\u3002\n\u6975\u3081\u3066\u30b0\u30ec\u30fc\u3060\u3068\u601d\u3046\u306e\u3067\u8cbc\u308c\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u672c\u306e\u540d\u524d\u3067\u691c\u7d22\u3059\u308b\u3068pdf\u3082\u843d\u3061\u3066\u3044\u308b\u306e\u3067\u8208\u5473\u3042\u308b\u65b9\u306f\u3001\u63a2\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3061\u306a\u307f\u306b\u7b2c\u4e8c\u7248\u306b\u306a\u3063\u3066\u308b\u307f\u305f\u3044\u3067\u3059\u306d\u8868\u7d19\u304c\u304b\u3063\u3053\u3044\u3044\u3002\n\u60f3\u5b9a\u3055\u308c\u308b\u8aad\u8005\u306f\u8a08\u7b97\u6a5f\u79d1\u5b66\u5c02\u653b\u306e\u5927\u5b66\u751f\u3067\u3001\u3068\u306e\u3053\u3068\u3060\u305d\u3046\u3067\u3059\u3002\n\u79c1\u306fLYH\u3092\u8aad\u3093\u3060\u304f\u3089\u3044\u306e\u30da\u30fc\u30da\u30fc\u3067\u5b9f\u696d\u52d9\u3067\u306f\u4f7f\u3063\u3066\u306a\u3044\u30a2\u30de\u30c1\u30e5\u30a2\u3067\u3059\u304c\u3001\u307e\u3041\u8aad\u3081\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\nIntroduction\n\u8a00\u8a9e\u306e\u6982\u8aac\u3001\u306a\u3069\u306a\u3069\u3002\nHaskell \u3092\u77e5\u3063\u3066\u308b\u4eba\u306f\u3059\u3093\u306a\u308a\u8aad\u3081\u307e\u3059\u306a\u3002\n1.2.4 \u306e\n\n\u95a2\u6570\u578b\u8a00\u8a9e\u306f\"\u5b9f\u884c\u53ef\u80fd\u306a\u6570\u5b66\"\n\n\u3068\u3044\u3046\u306e\u306f\u6b63\u76f4\u3050\u3063\u3068\u304f\u308b\u3002\n\u306a\u304a\u95a2\u6570\u3068\u3044\u3063\u305f\u5834\u5408\u96c6\u5408\u8ad6\u3067\u306f\u5165\u529b\u306b\u5bfe\u3057\u3066\u4e00\u610f\u306b\u6c7a\u307e\u308b\u51fa\u529b\u3068\u3044\u3046\u201d\u30b0\u30e9\u30d5\u201d\u3067\u8868\u3055\u308c\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u6975\u3081\u3066\u5916\u5ef6\u7684\uff08\u5217\u6319\uff09\u306a\u306e\u3067\u5185\u5305\u7684\u306a\u30e9\u30e0\u30c0\u8a08\u7b97\u3068\u3044\u3046\u306e\u306f\u79c1\u306e\u3088\u3046\u306a\u9580\u5916\u6f22\u306b\u306f\u9762\u767d\u3044\u304a\u3082\u3061\u3083\u3067\u3057\u305f\u3002\n\u7279\u306b\u500b\u4eba\u7684\u306b\u306f\uff08\u7269\u7406\u306e\uff09\u5e7e\u4f55\u5316\u306b\u8208\u5473\u304c\u3042\u3063\u305f\u3053\u3068\u304c\u3042\u3063\u3066\u3001\u305d\u306e\u969b\u591a\u5909\u6570\u95a2\u6570\u306a\u3069\u3092\u6271\u3046\u3068\u304d\u306b\u660e\u793a\u7684\u306b\u90e8\u5206\u9069\u7528\u3092\u66f8\u3051\u308b\u306e\u306b\u500b\u4eba\u7684\u306b\u30e9\u30e0\u30c0\u8a08\u7b97\u306e\u8a18\u6cd5\u3092\u4f7f\u3063\u305f\u308a\u3057\u3066\u307e\u3059\u3002\n\u4ee5\u4e0b\u30e1\u30e2\u7528\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u30d5\u30a1\u30a4\u30eb\u3002\n\nI_01.lhs\n\n> module I_01 where\n\nAlgorithms\na functional programming approach\n\nchapter 1\nIntroduction\n\n1.1 Algorithms\n... a breif introduction to the concept of an algorithm\n\n1.2 Functional languages\n1.2.1 Functions, lambda-calculus and induction\nThe lambda-calculus is a way of an intensional notation for a function.\n\n> induction base comb n\n>   | n == 0 = base\n>   | n > 0  = comb n $ induction base comb (n-1)\n\ninduction :: (Num a, Ord a) => r -> (a -> r -> r) -> a -> r\n\n\n\n\nFunctional programming in Haskell\nHaskell \u306b\u3082\u914d\u5217\u3042\u308b\u3093\u3060\u3001\u8981\u7d20\u3078\u306e\u30a2\u30af\u30bb\u30b9\u304c\u65e9\u3044\u3068\u304b\u5229\u70b9\u304c\u3042\u308b\u306e\u304b\u306a\uff1f\n\u30a4\u30de\u30a4\u30c1\u65e8\u5473\u304c\u308f\u304b\u3089\u306a\u3044\u3051\u3069\u3001\u591a\u8a00\u8a9e\u304b\u3089\u6765\u305f\u4eba\u306f\u4f7f\u3044\u3084\u3059\u3044\u306e\u304b\u3082\u3002\n\u305d\u308c\u4ee5\u5916\u3082\u76ee\u65b0\u3057\u3044\u3053\u3068\u306f\u7121\u3057\u3001\u304b\u306a\u3002\n\nFPIH_02.lhs\n\n> module FPIH_02 where\n\nChapter 2\nFunctional programming in Haskell\n\n2.1 About the language\n2.2 Equations and functions\n2.2.1 Function definitions\n2.2.2 Infix and prefix operators\n\n2.3 Basic types and constructed types\n\n> isB :: Char -> Bool\n> isB c = (c == 'B') || (c == 'b')\n\n2.4 Lists\n\n2.5 Higher-order functional programming techniques\n\n2.6 Algebraic types and polymorphisms\n2.6.5 Trees\n\n> data Tree a = Node a [Tree a] \n>             deriving (Show)\n\n> depth :: Tree a -> Int\n> depth (Node _ [])    = 1\n> depth (Node _ succs) = 1 + maximum (map depth succs)\n\n> data BinTree a \n>   = Empty\n>   | NodeBT a (BinTree a) (BinTree a)\n>   deriving (Show)\n\nFlatten into lists\nConsider now the problem of converting a tree into a list.\nThis can be done in 3 ways, depending on when the node is visited:\n\nPreorder; the node is visited before its left and right subtrees are visited.\nInorder; the node is visited after the left subtree has been visited and before the right subtree is visited.\nPostorder; the node is visited after its left and right subtrees have been visited.\n\n> preorder :: BinTree a -> [a]\n> preorder Empty = []\n> preorder (NodeBT a left right)\n>   = [a] ++ preorder left ++ preorder right\n\n> inorder :: BinTree a -> [a]\n> inorder Empty = []\n> inorder (NodeBT a left right)\n>   = inorder left ++ [a] ++ inorder right\n\n> postorder :: BinTree a -> [a]\n> postorder Empty = []\n> postorder (NodeBT a left right)\n>   = postorder left ++ postorder right ++ [a]\n\n> aBT = NodeBT 5 (NodeBT 8 (NodeBT 3 Empty Empty)\n>                          (NodeBT 1 Empty Empty)\n>                )\n>                (NodeBT 6 Empty\n>                          (NodeBT 4 Empty Empty)\n>                )\n\n  *FPIH_02 Data.List> preorder aBT \n  [5,8,3,1,6,4]\n  *FPIH_02 Data.List> inorder aBT\n  [3,8,1,5,6,4]\n  *FPIH_02 Data.List> postorder aBT\n  [3,1,8,4,6,5]\n\n2.7 Arrays\nAn array is used to store and retrieve a set of elements, each element having a unique index.\n\n2.8 Type classes and class methods\n\n\n\n\nThe efficiency of functional programs\n\u6b63\u683c\u8a55\u4fa1\u306e\u4ef6\u306f\u6b63\u76f4\u3001out dated \u304b\u3082\u5206\u304b\u3089\u306a\u3044\u3001seq \u3068\u304b bang pattern \u3068\u304b\u898b\u305f\u307b\u3046\u304c\u3088\u3044\u304b\u3082\u3001\u79c1\u306f\u3061\u306a\u307f\u306b\u4f55\u3082\u77e5\u3089\u306a\u3044\u3002\nlazy evaluation \u3068call by need \u304c\u540c\u3058\u8a55\u4fa1\u6226\u7565\u306a\u306e\u304b\u3069\u3046\u304b\u79c1\u306f\u77e5\u3089\u306a\u3044\u3001\u3053\u3053\u3067\u3082\u30ad\u30c1\u30f3\u3068\u306f\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u304c\u3001\u304a\u305d\u3089\u304f\u540c\u3058\u3088\u3046\u306a\u3082\u306e\u3092\u6307\u3057\u3066\u3044\u308b\u3068\u601d\u3046\u3002\ngraph reduction \u306f\u8ce2\u3044\u65b9\u6cd5\u3060\u306a\u3068\u3001\u6728\u69cb\u9020\u3067\u3076\u3089\u4e0b\u304c\u3063\u3066\u3044\u308b\u201d\u540c\u3058\u201d\u3082\u306e\u3092\u30dd\u30a4\u30f3\u30bf\u3067\u540c\u3058\u3082\u306e\u3092\u6307\u3059\u3053\u3068\u3067\u4e00\u6c17\u306b\u6271\u3046\u3068\u3044\u3046\u3001\u540d\u524d\u304b\u3089\u3057\u3066\u3082\u30c7\u30fc\u30bf\u69cb\u9020\u304c\u3061\u3083\u3093\u3068\u898b\u3048\u3066\u305f\u4eba\u304c\u8003\u3048\u305f\u3093\u3060\u308d\u3046\u306a\u3002\n\u6a19\u8a9e\u7684\u306b\n\n\u95a2\u6570\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306f\u7a7a\u9593\uff08\u30e1\u30e2\u30ea\uff1f\uff09\u3092\u55b0\u3044\u304c\u3061\n\n\u3068\u3044\u3046\u306e\u306f\u805e\u3044\u305f\u3053\u3068\u304c\u5408\u3063\u305f\u304c\u5024\u3058\u3083\u306a\u304f\u3066\u672a\u8a55\u4fa1\u306e\u95a2\u6570\u3084\u95a2\u6570\u3068\u5f15\u6570\u306f\u305f\u3057\u304b\u306b\u69cb\u9020\u304c\u8907\u96d1\u306a\u5206\u30e1\u30e2\u30ea\u3092\u55b0\u3044\u305d\u3046\u3067\u3042\u308b\u3001\u3053\u308c\u3089\u3092\u30af\u30ed\u30fc\u30b8\u30e3\u3068\u547c\u3076\u306e\u3060\u308d\u3046\u304b\uff1f\n\u6642\u9593\u30b3\u30b9\u30c8\u3068\u7a7a\u9593\u30b3\u30b9\u30c8\u3092\u8003\u3048\u308b\u969b\u306f\u7a7a\u9593\u30b3\u30b9\u30c8\u304b\u3089\u4f55\u3068\u304b\u3059\u308b\u3079\u304d\u3067\u3042\u308b\u3001\u3068\u306e\u3053\u3068\u3002\n\u304b\u304b\u3063\u305f\u7a7a\u9593\u30b3\u30b9\u30c8\u306e\u5206GC \u306e\u6642\u9593\u306a\u3069\u3082\u639b\u304b\u3063\u3066\u6765\u308b\u306e\u3067\u3001\u7a7a\u9593\u30b3\u30b9\u30c8\u304b\u3089\u524a\u6e1b\u3059\u308b\u306e\u306f\u6226\u7565\u3068\u3057\u3066\u306f\u9806\u5f53\u3002\nBurstall-Darlington transformation \u306f\u304a\u521d\u306b\u304a\u76ee\u306b\u304b\u304b\u308a\u307e\u3057\u305f\u304c\u3001\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u306e\u3053\u306e\u4f8b\u304b\u3089\u4e00\u822c\u8ad6\u306f\u4f59\u308a\u308f\u304b\u3063\u305f\u6c17\u304c\u3057\u306a\u3044\u3002\n\u672b\u5c3e\u518d\u5e30\u306f\u4e00\u90e8\u306eLisp \u3067\u6709\u540d\u306a\u3042\u308c\u3060\u308d\u3046\u304b\uff1f\u304a\u305d\u3089\u304f\u672b\u5c3e\u518d\u5e30\u3092\u5358\u7d14\u30eb\u30fc\u30d7\u3067\u7f6e\u304d\u63db\u3048\u3066\u65e9\u304f\u306a\u308b\u3088\u3068\u3044\u3046\u3053\u3068\u3060\u3063\u305f\u3068\u601d\u3046\u3002\nHaskell \u306e\u5834\u5408\u306f\u9045\u5ef6\u8a55\u4fa1\u3068\u3044\u3046\u6226\u7565\u3092\u9078\u629e\u3057\u305f\u304a\u304b\u3052\u3067\u672b\u5c3e\u518d\u5e30\u6700\u9069\u5316\u306e\u6069\u6075\u3092\u5fc5\u305a\u3057\u3082\u4eab\u53d7\u3057\u306a\u3044\u306e\u3067\u5834\u6240\u306b\u3088\u3063\u3066\u306f\u6b63\u683c\u8a55\u4fa1\u3092\u633f\u5165\u3057\u305f\u307b\u3046\u304c\u826f\u3044\u3068\u306e\u3053\u3068\u3002\n\u3084\u306f\u308a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u3057\u3063\u304b\u308a\u8003\u3048\u3066\u6226\u7565\u3092\u7df4\u3063\u305f\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5927\u4e8b\u3067\u3059\u306d\u3002\n\u3053\u306e\u8fba\u304b\u3089\u3084\u3084\u96e3\u3057\u304f\u306a\u3063\u3066\u304d\u305f\u3001\u3042\u3068\u6c17\u304c\u3064\u3044\u305f\u306e\u3060\u3051\u3069\u30ea\u30c6\u30e9\u30eb\u30cf\u30b9\u30b1\u30eb\u30d5\u30a1\u30a4\u30eb\u3060\u3068\u30cf\u30a4\u30e9\u30a4\u30c8\u3055\u308c\u306a\u3044\u3093\u3067\u3059\u306d\u3001\u6b8b\u5ff5\u3002\n\nTEOFP_03.lhs\n\n> module TEOFP_03 where\n\nchapter 3\nThe efficiency of functional programs\n\nThe design of a program is guided by two considerations: \nfirst, it should give the correct result, and\nsecond, it should run with a reasonable speed.\n\n3.1 Reduction order\ncall by name vs. lazy evaluation (with a graph reduction)\n\n3.2 Analyzing the efficiency of programs\n3.2.1 Graph reduction\nA heap is a tree-based (partial ordered) data structure.\n\n3.2.2 Time efficiency analysis\n\n3.2.3 Step-counting analysis\nThe analysis proceeds in 3 successive phases:\n\n1. For each function f we derive a step-counting version T_f.\nBy definition, # of calls required to compute f applied to some arguments under a strict regime is equal to T_f applied to the same arguments.\n\n2. The second phase consists of finding for recursive functions the structural property that the complexity depends upon.\nThis is called the size.\n\n3. A closed expression expressed in terms of the size of the inputs is derived from the corresponding step-counting version.\nThis often involves solving a system of recurrence equation.\nThe closed expression must be a composite of well-known functions such as arithmetic operators, logarithm, exponential, etc.\n\nTransformation rules\nEach expression e in the program has a cost T(e), and\n  f a1 a2 .. an = e => T_f a1 .. an = 1 + T(e)\nwhere 1 means that we've payed one function calling.\n\nThe costs\nT(consts) => 0\nT(variables) => 0\nT(if a then b else c) => T(a) + (if a then T(b) else T(c))\nT(p a1 .. an) = T(a1) + ..\n  where p is primitive function\n\nThe cost of a function call\n  f a1 a2 .. an\nconsists of the costs of evaluating the arguments plus the cost of performing the call to f which is equal to T_f a1 .. an:\n  T(f a1 a2 .. an) => T(a1) + .. + T(an) + (T_f a1 .. an)\n\nExamples\n\n> mySum :: Num a => [a] -> a\n> mySum [] = 0\n> mySum (x:xs) = x + (mySum xs)\n>\n> myLength :: [a] -> Int\n> myLength [] = 0\n> myLength (x:xs) = 1 + myLength xs\n>\n> average :: Fractional a => [a] -> a\n> average xs = (mySum xs) / fromIntegral (myLength xs)\n\nThe step-counting versions can be determined as follows:\n  T_sum [] = 1\n  T_sum (x:xs) = 1 + (T_sum xs)\n\n  T_length [] = 1\n  T_length (x:xs) = 1 + (T_length xs)\n\n  T_average xs = 1 + (T_sum xs) + (T_length xs)\n\nWe can solve abave ressursive equations for T_sum and T_length and\n  T_sum = n+1 = T_length\nand thus\n  T_average = 1 + (n+1) + (n+1)\n            = 2*n + 3\nUsing Landau notation,\n  T_sum, T_length \\in O(n)\nwhere n is the \"size\" of the argument list.\nSo\n  T_average \\in O(n)\n\nOur next example is the reverse function:\n\n> myReverse :: [a] -> [a]\n> myReverse [] = []\n> myReverse (x:xs) = myReverse xs ++ [x]\n\nNow the concatenation part requires 1 + (n-1) steps, where n-1 is the length of xs.\nThus,\n  T_reverse [] = 1\n  T_reverse (x:xs) = 1 + (1+ n-1) + T_reverse xs\nand this can be solved:\n  T_reverse = 1 + 3*n/2 + n^2/2 \\in O(n^2)\n\nAs a last example, which illustrates the difference between strict and lazy evaluation, consider the function foo defined as follows:\n\n> foo :: Num a => [a] -> a\n> foo l = head (map double l)\n>   where double x = x + x\n\nThe step-counting analysis of this function yields an O(n) complexity despite the fact it runs in O(1) under lazy evaluation.\n\nRestrictions\nAbove analysis rules contain a great deal of simplifications; they do not deal with higher-order functions.\nAnother restriction is related to \"syntactic sugar\".\n\nE.g.\n  f x | p x       = a\n      | otherwise = b\nis analyzed as\n  f x = if p x then a\n               else b\n\n3.2.4 Space efficiency analysis\nFunctional programs can be very greedy for space.\n\nAccumulated and largest space efficiency\nWe define 2 types of space analysis:\n\nAccumulated space analysis where the complexity is expressed in terms of the total of the units selected as a measure; this is the space that would be required if no GC was carried out.\n\nLargest space analysis where the complexity is equal to the largest number of units in use during the reduction sequence; this analysis only takes into account the \"live\" space used during the computation so the GC can be invoked as may times as necessary.\n\nThe 2nd analysis is more appropriate but is more difficult to carry out.\nIt is relatively easier to do an accumulated space analysis for strict programs as it is very similar to step-counting analysis.\n\nExample\nConsider the function reverse as defined in 3.2.3.\nThe definition is slightly altered to expose the call the cons(:) hidden behind the term [x].\n\n> myReverse' :: [a] -> [a]\n> myReverse' [] = []\n> myReverse' (x:xs) = (myReverse' xs) `myAppend` (x : [])\n>\n> myAppend :: [a] -> [a] -> [a]\n> []     `myAppend` ys = ys\n> (x:xs) `myAppend` ys = x : (xs `myAppend` ys)\n\nWith the same argument of step-countings, we get the total cost of reversing a list of length n as n^2/2 + n/2, so O(n^2).\nNote that this function will run in exactly the same sequence under lazy evaluation.\nTherefore the issue of lazy or strict evaluation does not matter in this case.\n\nTo determine the largest amount of space used during the reduction sequence, the following observations can be made.\n\nEach call to the reverse function creates a new list cell.\nHowever, the list cell containing the head of its argument list is no longer referenced and can be GC immediately.\n\nEach call to the concatenation(++) creates a new list cell but also consumes the list cell containing the head of its argument list which can be GC.\n\nWhen applied to a list of length n, the recursive calls to the reverse function create n list cells but the space used by the original list can be reclaimed.\nWhen (++) takes over, it does not affect the total number of \"active\" list cells in the heap so the largest amount of space used during the computation is n, therefore it is in O(n).\n\n3.2.5 Space leaks\nLaziness can produce very strange behavior.\nIn some instances, expressions may be held unevaluated even if their result my occupy less space.\nIn other cases, it my be cheaper to recompute an expression than sharing it.\nThese types of abnormal behavior, called space leaks, occur when:\n  the memory space leaks away invisibly;\n  memory is used when this could have been avoided;\n  memory remains referenced although it could be GC.\n\nSuch problems are very difficult to detect when examining the program.\nRecently, the situation has improved with the availability of heap profiling tools which allow inspection of heap nodes consumption after execution.\n\n3.3 Program transformation\n3.3.1 The Burstall-Darlington transformation system\n... which is useful for transforming recursive functional programs.\nThe basic idea is to treat a program as a set of equations and then perform a series of \"equal-to-equal\" substitutions in order to get an equivalent program.\nFor example, consider the following program expressed as 2 equations\n\n  double x = 2 * x\n  quad x = double (double x)\n\nBy replacing calls to double in the definition of quad by their right-hand side definition, we obtain the following definition of quad:\n\n  quad x = 2 * (2 * x)\n\nThis is one type of primitive transformation called unfolding.\nAnother transformation would be simply to use the laws of multiplication to replace the expression by 4 * x.\nOther primitive transformations include:\n  Definition introduces a new equation based on known equations.\n  Instantiation creates a specialization of a given equation, by giving values to some variables.\n  Folding, the opposite of unfolding, replaces an occurrence of a right-hand side by the appropriate left-hand side definition.\n  Abstraction introduces local definitions.\n\nE.g. consider an inefficient Fibonacci functions:\n\n> f 0 = 1\n> f 1 = 1\n> f n = f (n-1) + f (n-2)\n\nSuppose that we introduce the following definition of g, based on f, which returns a tuple:\n\n> g n = (f (n+1), f n)\n\nIf we manage to find a more efficient version of g, then we can easily define another f' that uses g and is equivalent to f:\n\n  f' n = x+y where (x,y) = g (n-2)\n\nTo find an efficient version of g, we need to eliminate references to f in the definition of g.\nThis can be achieved in 2 stages.\nThe first stage is to instatiate this equation with the particular case n = 0, and unfold using the definition of f:\n\n  g 0 = (f 1, f 0)\n      = (1, 1)\n\nThe second stage is to determine the general case g n, using where clause.\n\n  g n = (x+y, x) where (x,y) = (f n, f (n-1))\n\nNext, a fold transformation using the definition of g can be used to replace the expression (f n, f (n-1)) by g (n-1).\n\n> g' 0 = (1,1)\n> g' n = (x+y, x) where (x,y) = g' (n-1)\n> \n> efficientFib n = x+y where (x,y) = g' (n-2)\n\n  *TEOFP_03 Data.Array> map f [20..30] \n  [10946,17711,28657,46368,75025,121393,196418,317811,514229,832040,1346269]\n  (10.08 secs, 2,537,815,144 bytes)\n  *TEOFP_03 Data.Array> map efficientFib  [20..30] \n  [10946,17711,28657,46368,75025,121393,196418,317811,514229,832040,1346269]\n  (0.01 secs, 3,657,904 bytes)\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\n3.3.2 Tail recursively optimization     \nWe now move on to a different kind of transformation which only increases space efficiency if the compiler implements one particular optimization.\nConsider\n\n> fact 0 = 1\n> fact n = n * fact (n-1)\n\nThis function requires O(n) space to remember the arguments of the operator (*) through the successive recursive calls.\nWe can see that the multiplication cannot start until the recursion has completely unrolled.\nIn addition, since recursive function calls and returns are usually implemented by a stack, the stack growth is also in O(n).\n\nNow \n\n> betterFact = helper 1\n>   where helper result 0 = result\n>         helper result n = helper (n*result) (n-1)\n\nExpressed this way, the function is said to be tail-recursive.\nDuring evaluation, only one instance of the call to the helper function is needed at any time.\nFor this reason, the space occupied by the old function call can be reused by the new function call.\nAnother advantage is that the stack does not need to grow at all.\nThis optimization, carried out by most implementations of functional languages, is called the tail recursively optimization.\n\nNote that in a lazy language, this optimization only works if the parameters of the recursive call are strictly evaluated.\nIf not, the space occupied by the old function call cannot be reused since it contains references to unevaluated arguments.\nIn Haskell, to force strict evaluation of both arguments of the recursive call betterFact, the operator $! needs to be used.\nTherefore, the conditions for tail recursivity optimization are\n  1. the relevant function must be tail-recursive, \n  2. the parameters of the recursive call must be evaluated strictly.\n\nTo allow the compiler to carry out the tail recursivity optimization in myLength and mySum, they must be written as\n\n> myLength' xs = lengthTR xs 0\n>   where lengthTR []     r = r\n>         lengthTR (x:xs) r = lengthTR xs $! (r+1)\n> mySum' xs = sumTR xs 0\n>   where sumTR []     r = r\n>         sumTR (x:xs) r = (sumTR xs) $! (r+x)\n\n  *TEOFP_03 Data.Array> mySum $ take 1000000 [1,1..]\n  1000000\n  (0.93 secs, 293,063,840 bytes)\n  *TEOFP_03 Data.Array> mySum' $ take 1000000 [1,1..]\n  1000000\n  (0.70 secs, 354,411,120 bytes)\n\nBoth tail-recursive programs were obtained by using an accumulating parameter for the result.\nThe stack space usage in this case is in O(1) since the parameters of the tail-recursive functions are evaluated strictly.\n\n3.4 Conclusion\n\n\n\n\nConcrete data types\n\u9045\u5ef6\u8a55\u4fa1\u306e\u304a\u304b\u3052\u3067\u3001\u30ea\u30b9\u30c8\u64cd\u4f5c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\uff09\u306e\u7a7a\u9593\u52b9\u7387\u306f\u9ad8\u3044\u3088\u3001\u3068\u306e\u3053\u3068\u3002\n\u5f93\u3063\u3066\u7121\u9650\u30ea\u30b9\u30c8\u3082\u6271\u3048\u307e\u3059\u3068\u3044\u3046\u306e\u306f\u6709\u540d\u4e8b\u5b9f\u3067\u3059\u306d\u3002\n\u305f\u3060\u72b6\u614b\u3092\u6301\u305f\u306a\u3044\uff08\u53c2\u7167\u900f\u660e\uff09\u306e\u3067\u5f0a\u5bb3\u3082\u30a2\u30ea\u307e\u3059\u3088\u3001\u3067\u3082\u5834\u5408\u306b\u3088\u308a\u3051\u308a\u3067\u907f\u3051\u308c\u307e\u3059\u4f8b\u3048\u3070as patterns \u3092\u4f7f\u3046\u3068\u304b\u3002\ntail-strict \u306afilter \u306e\u5b9f\u88c5\u306f\u9762\u767d\u304b\u3063\u305f\u3051\u3069\u3001\u9045\u5ef6\u8a55\u4fa1\u306e\u65e8\u5473\u3092\u6d88\u3059\u306e\u3067Haskell \u3089\u3057\u304b\u3089\u306c\u306a\u3041\u3002\nO(n) \u306b\u306a\u308breverse \u306f\u5287\u7684\u306b\u9055\u3044\u304c\u51fa\u3066\u9762\u767d\u3044\u3001(++) \u304c\u9ad8\u4fa1\u306a\u3093\u3060\u306a\u3041\u3001\u3001\u3001\u3002\n\u3053\u3053\u306b\u51fa\u3066\u3044\u308b\u6700\u9069\u5316\u304c\u3044\u308f\u3086\u308bCPS \u306a\u306e\u304b\u306f\u5206\u304b\u3089\u306a\u3044\u3001\u305f\u3060\u5024(obj)\u3067\u306f\u306a\u304f\u95a2\u6570(arr)\u3068\u3044\u3046\u5909\u63db\u306f\u7c73\u7530\u306e\u306b\u307b\u3072\u304c\u3059\u308b\u3002\ntree \u304b\u3089\u3088\u304f\u308f\u304b\u3089\u306a\u304f\u306a\u3063\u3066\u304d\u305f\u3001\u914d\u5217\u3082\u3002\n\u3068\u3044\u3046\u308f\u3051\u3067\u4f59\u308a\u89e6\u3089\u305a\u3067\u3059\u3001\u7406\u89e3\u304c\u60aa\u3044\u3068\u30e1\u30e2\u304c\u305f\u3060\u306e\u52a3\u5316\u30b3\u30d4\u30fc\u306b\u306a\u308b\u306a\u3041\u3002\n\nCDT_04.lhs\n\n> module CDT_04 where\n\nChapter 4\nConcrete data types\n\n4.1 Lists\nAs an example,\n\n> ldouble, ltriple :: (Num a) => [a] -> [a]\n> ldouble = map (2*)\n> ltriple = map (3*)\n\nThe composition of n list processing functions forms a pipeline.\n\nUnder strict evaluation, the computation requires as much space as the largest function in the pipeline.\nHowever, under lazy evaluation, it is possible for a part of the result of one function to be passed to the next function \"on demand\".\n\n  *CDT_04> let sequence = ldouble . ltriple $ [1..]\n  *CDT_04> take 10 sequence\n  [6,12,18,24,30,36,42,48,54,60]\n\nWe can see that partial results produced by ltriple are immediately consumed by ldouble, making the intermediate list immediately GC.\nProviding that all the functions in the pipeline produce partial results, their composition operates in constant space in O(1).\nWe'll return such intermediate lists called transient lists in 4.1.4.\n\n4.1.2 The copying problem\n\n4.1.3 Tail strictness and tail recursivity\nA function that does not produce partial results i.e. it traverses the entire list before producing its result, is called a tail-strict function, e.g. filter.\n\n> myFilter :: (a -> Bool) -> [a] -> [a]\n> myFilter _ [] = []\n> myFilter p (x:xs) \n>   | p x       = x : myFilter p xs\n>   | otherwise = myFilter p xs\n\n(Accidentally, this is the same definition in Prelude.)\n\nWe could write a tail-recursive version of filter using an accumulating parameter which collects the elements in reverse order (the list must be reversed at the end).\n\n> filterTR :: (a -> Bool) -> [a] -> [a]\n> filterTR p xs = filterTR' p xs []\n> \n> filterTR' p [] r = reverse r\n> filterTR' p (x:xs) r\n>   | p x       = filterTR' p xs (x:r)\n>   | otherwise = filterTR' p xs r\n\nThis is tail-recursive and tail-strict because the entire list must be traversed before delivering a result.\nThe main advantage is a reduction in stack space usage if the compiler implements tail recursivity optimization.\nHowever, using a tail-strict function may cause an increase of the space used during the computation from O(1) to O(n) if used in a pipeline.\nThis is why care must be taken when changing the definition of a function into its tail-recursive version.\n\nAnother issue is well represented in the follwoing trial:\n\n  *CDT_04> let evenNums = filterTR even [1..]\n  (0.00 secs, 1,028,240 bytes)\n  *CDT_04> take 10 even\n  even      evenNums\n  *CDT_04> take 10 evenNums \n  ^CInterrupted.\n\nIncidentally, the filter function can also be expressed using foldr:\n\n> filterR :: (a -> Bool) -> [a] -> [a]\n> filterR p xs = foldr helper [] xs\n>   where helper x result \n>           | p x       = x : result\n>           | otherwise = result\n\nThis definition is also not tail-strict since the head of the list if produced without unfolding all the cell to the operator foldr.\n\n4.1.4 Deforestation with lists\nWe've seen that function composition of list functions can create intermediate lists.\nDespite the fact that these intermediate lists are GC, it may be desirable to avoid creating them at all.\n\nIn some cases, this can be achieved by using Burstall-Darlington transformation system, see 3.3.1.\nFor example, the composition of ldouble and ltriple becomes\n\n> ldt :: (Num a) => [a] -> [a]\n> ldt = map (6*)\n\nThis pdocedure to remove intermediate lists forms the basis of Wadler's deforestation algorithm and could be automatically implemented by a compiler but is most often done \"by hand\".\nThis also can be seen as an application of functor law of composition.\n  (map (2*)) . (map (3*)) = map (2*3*) \n\n4.1.5 Removing appends\nIn this section, we consider another example of using the Bustall-Darlington transformation approach to remove calls to (++).\nThe description presented here is based on a technique described by Wadler and inspired by his work on deformation.\nFirst, here are 3 laws to which (++) obeys:\n  [] ++ xs         = xs\n  (x : ys) ++ zs   = x : (ys ++ zs)\n  (xs ++ yx) ++ zs = xs ++ (ys ++ zs)\n\nThe aim of the transformation is to eliminate (++) from expressions of the form\n  (f x1 .. xn) ++ y \nby defining a function f' s.t.\n  f' x1 .. xn y = (f x1 .. xn) ++ y\n\nExpressions of the form (f x1 .. xn) will be replaced by (f' x1 .. xn []).\nTo derived the function f', each definition of the form \n  {f x1 .. xn = e}\nis replaced by\n  {f' x1 .. xn y = e ++ y}\nThis technique is known as generalization because f' is a generalization of f.\n\nFor example,\n  reverse []     = []\n  reverse (x:xs) = (reverse xs) ++ (x : [])\nWe need to define a function reverse' according to the rules outlined earlier.\nWe can use\n  reverse' xs y = (reverse xs) ++ y\nFirst, for an empty list,\n  reverse' [] y = reverse [] ++ y\n                = [] ++ y\n                = y\nNext is the case of a non-empty list\n  reverse' (x:xs) y = ((reverse xs) ++ (x : [])) ++ y\n                    = (reverse xs) ++ ((x : []) ++ y)\n                    = (reverse xs) ++ (x : ([] ++ y))\n                    = (reverse xs) ++ (x : y)\n                    = reverse' xs (x : y)\n\nPutting the 2 cases together,\n\n> reverse' []     y = y\n> reverse' (x:xs) y = reverse' xs (x : y)\n\nReplacing calls of the form\n  reverse xs\nby\n  reverse' xs []\nleads to a dramatic improvement from O(n^2) to O(n) in both time and space!\nThis technique cannot always guarantee an improvement for every function and requires some skill to carry it out successfully.\n\n> myReverse :: [a] -> [a]\n> myReverse []     = []\n> myReverse (x:xs) = (myReverse xs) ++ [x]\n\n> myReverse' xs = reverse' xs []\n\n  *CDT_04> myReverse [1..10000]\n  (4.15 secs, 4,340,343,400 bytes)\n  *CDT_04> myReverse' [1..10000]\n  (0.20 secs, 49,522,584 bytes)\n\n4.1.6 Reducing the number of passes\nSome functions need to traverse a list several times before delivering a result.\nOften, it is desirable to reduce the number of passes, particularly if the list involved is large.\nFor example,\n  average xs = (mySum xs) / fromIntegral (myLength xs)\n\n2 list traversals are neede, one for sum and the other for length.\nIn addition, the entire list must reside in memory at once because it is shared by both functions.\n\nTupling can beused:\n\n> averageT xs = summedList / fromIntegral lengthOfList\n>   where (summedList, lengthOfList) = av xs\n>         av [] = (0, 0)\n>         av (x:xs) = (x+s, n+1)\n>           where (s,n) = av xs\n\nHowever, there are 2 problems with using tuples, extra space and space leak, i.e. computing the average will not run in constant largest space as the tail-recursive versions of sum and length will do.\n\nA solution is to include both results as parameters to the function.\n\n  av' xs s n = (sum' xs + s) / fromInteger (length' xs + n)\n\nA more efficient version of average can be derived in the usual transformation style:\n\n> average' :: Fractional a => [a] -> a\n> average' xs = av' xs 0 0\n>   where av' []     s n = s / fromInteger n\n>         av' (x:xs) s n = av' xs (x+s) (n+1)\n\nThe advantage of this version is that there is no need for the entire list to reside in memory so the space leak is avoided.\nProviding that the expressions (x+s) and (n+1) are evaluated strictly, the compiler can implement tail recursivity optimization so the overall space costs are reduced.\n\n4.2 Trees\n4.2.1 Terminology\n\nIf a binary tree has n nodes and a depth d, some interesting properties follow:\n1.\nThe minimum depth is the smallest integer at least (log(n+1)).\nIf n is the form 2^k -1, the minimum depth is k, and the tree is said to be perfectly balanced in which case there are 2^(k-1)-1 interior nodes and 2^(k-1) leaves.\n\n2.\nThe maximum depth is d=n (a chain of nodes).\n\nUnless mentioned otherwise, all logarithms are base 2.\nIn the rest of this section, we assume the following binary tree\n\n> data BinTree a = Empty\n>                | NodeBT a (BinTree a) (BinTree a)\n>                deriving (Show)\n\n> t1 :: BinTree Int\n> t1 = NodeBT 5 (NodeBT 8 (NodeBT 3 Empty Empty)\n>                         (NodeBT 1 Empty Empty)\n>               )\n>               (NodeBT 6 Empty\n>                         (NodeBT 4 Empty Empty)\n>               )\n\n4.2.2 Composing tree operations\nA function taking a tree as input may need to consume either the whole tree(O(n)) or one path(O(d)) at a time before delivering partial results.\nFor example,\n\n> tcomp :: BinTree Int -> Int\n> tcomp t = (tsum . tdouble) t\n\n> tdouble :: BinTree Int -> BinTree Int\n> tdouble Empty            = Empty\n> tdouble (NodeBT v lf rt) = NodeBT (2*v) (tdouble lf) (tdouble rt)\n\n> -- tsum :: BinTree Int -> Int\n> -- tsum Empty            = 0\n> -- tsum (NodeBT v lf rt) = v + (tsum lf) + (tsum rt)\n\nWe can see that largest space used is proportional to the logest path in the tree.\n\nUsing the deforestation algorithm mensioned in 4.1.4,\n\n  tcomp' Empty            = 0\n  tcomp' (NodeBt v lf rt) = 2*v + (tcomp' lf) + (tcomp' rt)\n\nIn addition to lists and trees, the deforestation algorithm can deal with any other ADT.\n\n4.2.3 Reducing the number of passes\nAs with lists, the Burstall-Darlington transformation style can be used to improve the efficiency of tree processing programs.\n\nAs with lists, \n\n> count_depth Empty = (1, 0)\n> count_depth (NodeBT v lf rt) = (c1 + c2, 1 + (max d1 d2))\n>   where (c1, d1) = count_depth lf\n>         (c2, d2) = count_depth rt\n\nSometimes, there might be a dependency between the two traversals, that is, the second traversal needs a value computed by the first traversal.\nFor example, replacing each value in the tree by a percentage value (the original value divided by the total of all the values) can be specified using the following set of equations:\n\n> comp t = perc (tsum t) t\n\n> tsum Empty            = 0\n> tsum (NodeBT v lf rt) = v + tsum lf + tsum rt\n\n> perc x Empty            = Empty\n> perc x (NodeBT v lf rt) = NodeBT (fromInteger v / fromInteger x)\n>                                  (perc x lf)\n>                                  (perc x rt)\n\nWe can see that the tree must be traversed twice during evaluation of the function comp.\nBird show a technique to achieve the computation in one traversal.\nIt consists of introducing a function\n\n  comp'' x t = (perc x t, tsum t)\n\nGiven the this function, an alternative function to comp can be defined\n\n> comp' t = t' where (t', x) = comp'' x t\n\nSo the problem now is to define an efficient version of the comp'' function.\nTo achieve this, the definition of comp'' is instantiated with the 2 cases, Empty and Node.\n\n> comp'' x Empty = (Empty, 0) -- (perc x Empty, tsum Empty)\n\n> comp'' x (NodeBT v lf rt)\n>   = (NodeBT (fromInteger v / fromInteger x) p1 p2, v + s1 + s2)\n>       where (p1,s1) = comp'' x lf\n>             (p2,s2) = comp'' x rt\n\n4.2.4 Removing appends revised\nIt is often the case that gathering information from a tree into a list produces multiple recursive calls to (++).\nFor example, consider again the function which converts a tree into list function using the inorder(2.6.5) traversal.\n\n> inorder Empty            = []\n> inorder (NodeBT a lf rt) = inorder lf ++ [a] ++ inorder rt\n\nIn this case, an improvement can be made by removing calls to the append function (4.1.5).\n\n> inorder' t = helper t []\n>   where helper Empty            z = z\n>         helper (NodeBT a lf rt) z = helper lf (a : (helper rt z))\n\n4.2.5 Copying in trees\nIn some instances, copying can be avoided through the use of labels as we demonstrated with lists.\n\nConsider the following binary trees which only contain values at the leaves.\n\n> data BinTree'' a = Leaf'' a\n>                  | NodeBT'' (BinTree'' a) (BinTree'' a)\n\nNow, consider the following function that flips all the left-right pairs:\n\n> flipT :: BinTree'' a -> BinTree'' a\n> flipT (NodeBT'' a b) = NodeBT'' (flipT b) (flipT a)\n> -- flipT (Leaf'' a)  = Leaf'' a\n\nThe evaluation of flipT as defined above causes all the leaves in the original tree to be unnecessarily replicated.\nTo avoid this problem, use as pattern:\n\n> flipT x@(Leaf'' a)   = x\n\n4.2.6 Storing additional information in the tree\nIn some cases, some additional information can be stored in the nodes to avoid multiple traversals of the tree.\nFor example, consider the problem of inserting a node at the lowest level of the smallest (in size) subtree:\n\n  tInsert v Empty = NodeBT v Empty Empty\n  tInsert v (NodeBT w lf rt) \n    | (size lf) <= (size rt) = NodeBT w (tInsert v lf) rt\n    | otherwise              = NodeBT w lf             (tInsert v rt)\n\nAn alternative is to use the following tree declaration where the sizes of the right and the left subtrees are stored together with the value of the root node.\n\n> data BinTreeSz a = EmptySz\n>                  | NodeBTSz (Int, Int) a (BinTreeSz a) (BinTreeSz a)\n\nIn this case, the tree insertion function becomes\n\n> tInsertSz :: a -> BinTreeSz a -> BinTreeSz a\n> tInsertSz v EmptySz = NodeBTSz (0,0) v EmptySz EmptySz\n> tInsertSz v (NodeBTSz (s1,s2) w lf rt)\n>   | s1 <= s2  = NodeBTSz (1+s1, s2) w (tInsertSz v lf) rt\n>   | otherwise = NodeBTSz (s1, 1+s2) w lf               (tInsertSz v rt)\n\n4.3 Arrays\n4.3.1 Functional and imperative arrays\n.. there is no general consensus yet,\n\n4.3.2 Handling array updates\nThe main problem is with the update operations.\n(see 10.2.2. state monad)\n\n4.3.3 Higher-order array functions\n\n\n\n# \u306f\u3058\u3081\u306b\n[RWH](http://book.realworldhaskell.org/read/)\u304c\u3042\u307e\u308a\u306b\"\u5b9f\u201d\u904e\u304e\u3066\u3061\u3087\u3063\u3068\u98df\u50b7\u6c17\u5473\u306a\u4e0a\u306b\u672a\u6d88\u5316\u3060\u3063\u305f\u3082\u306e\u3067\u305b\u3081\u3066\u3053\u306e\u51ac\u306b\u4f55\u304b\u8aad\u3082\u3046\u3068\u601d\u3063\u3066\u4f55\u304b\u30cd\u30bf\u306f\u3068\u63a2\u3057\u3066\u307f\u305f\u3068\u3053\u308d[\u3053\u3093\u306a\u3082\u306e](http://www.iro.umontreal.ca/~lapalme/Algorithms-functional.html)\u3092\u898b\u3064\u3051\u305f\u306e\u3067\u8aad\u66f8\u30e1\u30e2\u304c\u3066\u3089\u3002\n\n\u672c\u306e\u8a73\u7d30\u306f\u4e0a\u8a18\u306e\u30b5\u30a4\u30c8\u3067\u306f\n\nAlgorithms:\nA Functional Programming Approach\n\nFethi Rabhi\nGuy Lapalme\n\nAddison-Wesley, ISBN 0-201-59604-0\n256 pages, paperback, 1999\n\n\u3068\u306a\u3063\u3066\u307e\u3059\u3002\n\n\u6975\u3081\u3066\u30b0\u30ec\u30fc\u3060\u3068\u601d\u3046\u306e\u3067\u8cbc\u308c\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u672c\u306e\u540d\u524d\u3067\u691c\u7d22\u3059\u308b\u3068pdf\u3082\u843d\u3061\u3066\u3044\u308b\u306e\u3067\u8208\u5473\u3042\u308b\u65b9\u306f\u3001\u63a2\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3061\u306a\u307f\u306b\u7b2c\u4e8c\u7248\u306b\u306a\u3063\u3066\u308b\u307f\u305f\u3044\u3067\u3059\u306d\u8868\u7d19\u304c\u304b\u3063\u3053\u3044\u3044\u3002\n\n\u60f3\u5b9a\u3055\u308c\u308b\u8aad\u8005\u306f\u8a08\u7b97\u6a5f\u79d1\u5b66\u5c02\u653b\u306e\u5927\u5b66\u751f\u3067\u3001\u3068\u306e\u3053\u3068\u3060\u305d\u3046\u3067\u3059\u3002\n\u79c1\u306f[LYH](http://learnyouahaskell.com/chapters)\u3092\u8aad\u3093\u3060\u304f\u3089\u3044\u306e\u30da\u30fc\u30da\u30fc\u3067\u5b9f\u696d\u52d9\u3067\u306f\u4f7f\u3063\u3066\u306a\u3044\u30a2\u30de\u30c1\u30e5\u30a2\u3067\u3059\u304c\u3001\u307e\u3041\u8aad\u3081\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n# Introduction\n\u8a00\u8a9e\u306e\u6982\u8aac\u3001\u306a\u3069\u306a\u3069\u3002\nHaskell \u3092\u77e5\u3063\u3066\u308b\u4eba\u306f\u3059\u3093\u306a\u308a\u8aad\u3081\u307e\u3059\u306a\u3002\n1.2.4 \u306e\n\n>\u95a2\u6570\u578b\u8a00\u8a9e\u306f\"\u5b9f\u884c\u53ef\u80fd\u306a\u6570\u5b66\"\n\n\u3068\u3044\u3046\u306e\u306f\u6b63\u76f4\u3050\u3063\u3068\u304f\u308b\u3002\n\n\u306a\u304a\u95a2\u6570\u3068\u3044\u3063\u305f\u5834\u5408\u96c6\u5408\u8ad6\u3067\u306f\u5165\u529b\u306b\u5bfe\u3057\u3066\u4e00\u610f\u306b\u6c7a\u307e\u308b\u51fa\u529b\u3068\u3044\u3046\u201d\u30b0\u30e9\u30d5\u201d\u3067\u8868\u3055\u308c\u307e\u3059\u304c\u3001\u3053\u308c\u306f\u6975\u3081\u3066\u5916\u5ef6\u7684\uff08\u5217\u6319\uff09\u306a\u306e\u3067\u5185\u5305\u7684\u306a\u30e9\u30e0\u30c0\u8a08\u7b97\u3068\u3044\u3046\u306e\u306f\u79c1\u306e\u3088\u3046\u306a\u9580\u5916\u6f22\u306b\u306f\u9762\u767d\u3044\u304a\u3082\u3061\u3083\u3067\u3057\u305f\u3002\n\u7279\u306b\u500b\u4eba\u7684\u306b\u306f\uff08\u7269\u7406\u306e\uff09\u5e7e\u4f55\u5316\u306b\u8208\u5473\u304c\u3042\u3063\u305f\u3053\u3068\u304c\u3042\u3063\u3066\u3001\u305d\u306e\u969b\u591a\u5909\u6570\u95a2\u6570\u306a\u3069\u3092\u6271\u3046\u3068\u304d\u306b\u660e\u793a\u7684\u306b\u90e8\u5206\u9069\u7528\u3092\u66f8\u3051\u308b\u306e\u306b\u500b\u4eba\u7684\u306b\u30e9\u30e0\u30c0\u8a08\u7b97\u306e\u8a18\u6cd5\u3092\u4f7f\u3063\u305f\u308a\u3057\u3066\u307e\u3059\u3002\n\n\u4ee5\u4e0b\u30e1\u30e2\u7528\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u30d5\u30a1\u30a4\u30eb\u3002\n\n```lang:I_01.lhs\n\n> module I_01 where\n\nAlgorithms\na functional programming approach\n\nchapter 1\nIntroduction\n\n1.1 Algorithms\n... a breif introduction to the concept of an algorithm\n\n1.2 Functional languages\n1.2.1 Functions, lambda-calculus and induction\nThe lambda-calculus is a way of an intensional notation for a function.\n\n> induction base comb n\n>   | n == 0 = base\n>   | n > 0  = comb n $ induction base comb (n-1)\n\ninduction :: (Num a, Ord a) => r -> (a -> r -> r) -> a -> r\n\n```\n\n# Functional programming in Haskell\nHaskell \u306b\u3082\u914d\u5217\u3042\u308b\u3093\u3060\u3001\u8981\u7d20\u3078\u306e\u30a2\u30af\u30bb\u30b9\u304c\u65e9\u3044\u3068\u304b\u5229\u70b9\u304c\u3042\u308b\u306e\u304b\u306a\uff1f\n\u30a4\u30de\u30a4\u30c1\u65e8\u5473\u304c\u308f\u304b\u3089\u306a\u3044\u3051\u3069\u3001\u591a\u8a00\u8a9e\u304b\u3089\u6765\u305f\u4eba\u306f\u4f7f\u3044\u3084\u3059\u3044\u306e\u304b\u3082\u3002\n\u305d\u308c\u4ee5\u5916\u3082\u76ee\u65b0\u3057\u3044\u3053\u3068\u306f\u7121\u3057\u3001\u304b\u306a\u3002\n\n```lang:FPIH_02.lhs\n\n> module FPIH_02 where\n\nChapter 2\nFunctional programming in Haskell\n\n2.1 About the language\n2.2 Equations and functions\n2.2.1 Function definitions\n2.2.2 Infix and prefix operators\n\n2.3 Basic types and constructed types\n\n> isB :: Char -> Bool\n> isB c = (c == 'B') || (c == 'b')\n\n2.4 Lists\n\n2.5 Higher-order functional programming techniques\n\n2.6 Algebraic types and polymorphisms\n2.6.5 Trees\n\n> data Tree a = Node a [Tree a] \n>             deriving (Show)\n\n> depth :: Tree a -> Int\n> depth (Node _ [])    = 1\n> depth (Node _ succs) = 1 + maximum (map depth succs)\n\n> data BinTree a \n>   = Empty\n>   | NodeBT a (BinTree a) (BinTree a)\n>   deriving (Show)\n\nFlatten into lists\nConsider now the problem of converting a tree into a list.\nThis can be done in 3 ways, depending on when the node is visited:\n\nPreorder; the node is visited before its left and right subtrees are visited.\nInorder; the node is visited after the left subtree has been visited and before the right subtree is visited.\nPostorder; the node is visited after its left and right subtrees have been visited.\n\n> preorder :: BinTree a -> [a]\n> preorder Empty = []\n> preorder (NodeBT a left right)\n>   = [a] ++ preorder left ++ preorder right\n\n> inorder :: BinTree a -> [a]\n> inorder Empty = []\n> inorder (NodeBT a left right)\n>   = inorder left ++ [a] ++ inorder right\n\n> postorder :: BinTree a -> [a]\n> postorder Empty = []\n> postorder (NodeBT a left right)\n>   = postorder left ++ postorder right ++ [a]\n\n> aBT = NodeBT 5 (NodeBT 8 (NodeBT 3 Empty Empty)\n>                          (NodeBT 1 Empty Empty)\n>                )\n>                (NodeBT 6 Empty\n>                          (NodeBT 4 Empty Empty)\n>                )\n  \n  *FPIH_02 Data.List> preorder aBT \n  [5,8,3,1,6,4]\n  *FPIH_02 Data.List> inorder aBT\n  [3,8,1,5,6,4]\n  *FPIH_02 Data.List> postorder aBT\n  [3,1,8,4,6,5]\n\n2.7 Arrays\nAn array is used to store and retrieve a set of elements, each element having a unique index.\n\n2.8 Type classes and class methods\n\n```\n\n# The efficiency of functional programs\n\u6b63\u683c\u8a55\u4fa1\u306e\u4ef6\u306f\u6b63\u76f4\u3001out dated \u304b\u3082\u5206\u304b\u3089\u306a\u3044\u3001seq \u3068\u304b bang pattern \u3068\u304b\u898b\u305f\u307b\u3046\u304c\u3088\u3044\u304b\u3082\u3001\u79c1\u306f\u3061\u306a\u307f\u306b\u4f55\u3082\u77e5\u3089\u306a\u3044\u3002\nlazy evaluation \u3068call by need \u304c\u540c\u3058\u8a55\u4fa1\u6226\u7565\u306a\u306e\u304b\u3069\u3046\u304b\u79c1\u306f\u77e5\u3089\u306a\u3044\u3001\u3053\u3053\u3067\u3082\u30ad\u30c1\u30f3\u3068\u306f\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u304c\u3001\u304a\u305d\u3089\u304f\u540c\u3058\u3088\u3046\u306a\u3082\u306e\u3092\u6307\u3057\u3066\u3044\u308b\u3068\u601d\u3046\u3002\n\ngraph reduction \u306f\u8ce2\u3044\u65b9\u6cd5\u3060\u306a\u3068\u3001\u6728\u69cb\u9020\u3067\u3076\u3089\u4e0b\u304c\u3063\u3066\u3044\u308b\u201d\u540c\u3058\u201d\u3082\u306e\u3092\u30dd\u30a4\u30f3\u30bf\u3067\u540c\u3058\u3082\u306e\u3092\u6307\u3059\u3053\u3068\u3067\u4e00\u6c17\u306b\u6271\u3046\u3068\u3044\u3046\u3001\u540d\u524d\u304b\u3089\u3057\u3066\u3082\u30c7\u30fc\u30bf\u69cb\u9020\u304c\u3061\u3083\u3093\u3068\u898b\u3048\u3066\u305f\u4eba\u304c\u8003\u3048\u305f\u3093\u3060\u308d\u3046\u306a\u3002\n\n\u6a19\u8a9e\u7684\u306b\n\n> \u95a2\u6570\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306f\u7a7a\u9593\uff08\u30e1\u30e2\u30ea\uff1f\uff09\u3092\u55b0\u3044\u304c\u3061\n\n\u3068\u3044\u3046\u306e\u306f\u805e\u3044\u305f\u3053\u3068\u304c\u5408\u3063\u305f\u304c\u5024\u3058\u3083\u306a\u304f\u3066\u672a\u8a55\u4fa1\u306e\u95a2\u6570\u3084\u95a2\u6570\u3068\u5f15\u6570\u306f\u305f\u3057\u304b\u306b\u69cb\u9020\u304c\u8907\u96d1\u306a\u5206\u30e1\u30e2\u30ea\u3092\u55b0\u3044\u305d\u3046\u3067\u3042\u308b\u3001\u3053\u308c\u3089\u3092\u30af\u30ed\u30fc\u30b8\u30e3\u3068\u547c\u3076\u306e\u3060\u308d\u3046\u304b\uff1f\n\n\u6642\u9593\u30b3\u30b9\u30c8\u3068\u7a7a\u9593\u30b3\u30b9\u30c8\u3092\u8003\u3048\u308b\u969b\u306f\u7a7a\u9593\u30b3\u30b9\u30c8\u304b\u3089\u4f55\u3068\u304b\u3059\u308b\u3079\u304d\u3067\u3042\u308b\u3001\u3068\u306e\u3053\u3068\u3002\n\u304b\u304b\u3063\u305f\u7a7a\u9593\u30b3\u30b9\u30c8\u306e\u5206GC \u306e\u6642\u9593\u306a\u3069\u3082\u639b\u304b\u3063\u3066\u6765\u308b\u306e\u3067\u3001\u7a7a\u9593\u30b3\u30b9\u30c8\u304b\u3089\u524a\u6e1b\u3059\u308b\u306e\u306f\u6226\u7565\u3068\u3057\u3066\u306f\u9806\u5f53\u3002\n\nBurstall-Darlington transformation \u306f\u304a\u521d\u306b\u304a\u76ee\u306b\u304b\u304b\u308a\u307e\u3057\u305f\u304c\u3001\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u306e\u3053\u306e\u4f8b\u304b\u3089\u4e00\u822c\u8ad6\u306f\u4f59\u308a\u308f\u304b\u3063\u305f\u6c17\u304c\u3057\u306a\u3044\u3002\n\n\u672b\u5c3e\u518d\u5e30\u306f\u4e00\u90e8\u306eLisp \u3067\u6709\u540d\u306a\u3042\u308c\u3060\u308d\u3046\u304b\uff1f\u304a\u305d\u3089\u304f\u672b\u5c3e\u518d\u5e30\u3092\u5358\u7d14\u30eb\u30fc\u30d7\u3067\u7f6e\u304d\u63db\u3048\u3066\u65e9\u304f\u306a\u308b\u3088\u3068\u3044\u3046\u3053\u3068\u3060\u3063\u305f\u3068\u601d\u3046\u3002\nHaskell \u306e\u5834\u5408\u306f\u9045\u5ef6\u8a55\u4fa1\u3068\u3044\u3046\u6226\u7565\u3092\u9078\u629e\u3057\u305f\u304a\u304b\u3052\u3067\u672b\u5c3e\u518d\u5e30\u6700\u9069\u5316\u306e\u6069\u6075\u3092\u5fc5\u305a\u3057\u3082\u4eab\u53d7\u3057\u306a\u3044\u306e\u3067\u5834\u6240\u306b\u3088\u3063\u3066\u306f\u6b63\u683c\u8a55\u4fa1\u3092\u633f\u5165\u3057\u305f\u307b\u3046\u304c\u826f\u3044\u3068\u306e\u3053\u3068\u3002\n\u3084\u306f\u308a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u3057\u3063\u304b\u308a\u8003\u3048\u3066\u6226\u7565\u3092\u7df4\u3063\u305f\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5927\u4e8b\u3067\u3059\u306d\u3002\n\n\u3053\u306e\u8fba\u304b\u3089\u3084\u3084\u96e3\u3057\u304f\u306a\u3063\u3066\u304d\u305f\u3001\u3042\u3068\u6c17\u304c\u3064\u3044\u305f\u306e\u3060\u3051\u3069\u30ea\u30c6\u30e9\u30eb\u30cf\u30b9\u30b1\u30eb\u30d5\u30a1\u30a4\u30eb\u3060\u3068\u30cf\u30a4\u30e9\u30a4\u30c8\u3055\u308c\u306a\u3044\u3093\u3067\u3059\u306d\u3001\u6b8b\u5ff5\u3002\n\n```lang:TEOFP_03.lhs\n\n> module TEOFP_03 where\n\nchapter 3\nThe efficiency of functional programs\n\nThe design of a program is guided by two considerations: \nfirst, it should give the correct result, and\nsecond, it should run with a reasonable speed.\n\n3.1 Reduction order\ncall by name vs. lazy evaluation (with a graph reduction)\n\n3.2 Analyzing the efficiency of programs\n3.2.1 Graph reduction\nA heap is a tree-based (partial ordered) data structure.\n\n3.2.2 Time efficiency analysis\n\n3.2.3 Step-counting analysis\nThe analysis proceeds in 3 successive phases:\n\n1. For each function f we derive a step-counting version T_f.\nBy definition, # of calls required to compute f applied to some arguments under a strict regime is equal to T_f applied to the same arguments.\n\n2. The second phase consists of finding for recursive functions the structural property that the complexity depends upon.\nThis is called the size.\n\n3. A closed expression expressed in terms of the size of the inputs is derived from the corresponding step-counting version.\nThis often involves solving a system of recurrence equation.\nThe closed expression must be a composite of well-known functions such as arithmetic operators, logarithm, exponential, etc.\n\nTransformation rules\nEach expression e in the program has a cost T(e), and\n  f a1 a2 .. an = e => T_f a1 .. an = 1 + T(e)\nwhere 1 means that we've payed one function calling.\n\nThe costs\nT(consts) => 0\nT(variables) => 0\nT(if a then b else c) => T(a) + (if a then T(b) else T(c))\nT(p a1 .. an) = T(a1) + ..\n  where p is primitive function\n\nThe cost of a function call\n  f a1 a2 .. an\nconsists of the costs of evaluating the arguments plus the cost of performing the call to f which is equal to T_f a1 .. an:\n  T(f a1 a2 .. an) => T(a1) + .. + T(an) + (T_f a1 .. an)\n\nExamples\n\n> mySum :: Num a => [a] -> a\n> mySum [] = 0\n> mySum (x:xs) = x + (mySum xs)\n>\n> myLength :: [a] -> Int\n> myLength [] = 0\n> myLength (x:xs) = 1 + myLength xs\n>\n> average :: Fractional a => [a] -> a\n> average xs = (mySum xs) / fromIntegral (myLength xs)\n                         \nThe step-counting versions can be determined as follows:\n  T_sum [] = 1\n  T_sum (x:xs) = 1 + (T_sum xs)\n\n  T_length [] = 1\n  T_length (x:xs) = 1 + (T_length xs)\n\n  T_average xs = 1 + (T_sum xs) + (T_length xs)\n\nWe can solve abave ressursive equations for T_sum and T_length and\n  T_sum = n+1 = T_length\nand thus\n  T_average = 1 + (n+1) + (n+1)\n            = 2*n + 3\nUsing Landau notation,\n  T_sum, T_length \\in O(n)\nwhere n is the \"size\" of the argument list.\nSo\n  T_average \\in O(n)\n\nOur next example is the reverse function:\n\n> myReverse :: [a] -> [a]\n> myReverse [] = []\n> myReverse (x:xs) = myReverse xs ++ [x]\n\nNow the concatenation part requires 1 + (n-1) steps, where n-1 is the length of xs.\nThus,\n  T_reverse [] = 1\n  T_reverse (x:xs) = 1 + (1+ n-1) + T_reverse xs\nand this can be solved:\n  T_reverse = 1 + 3*n/2 + n^2/2 \\in O(n^2)\n\nAs a last example, which illustrates the difference between strict and lazy evaluation, consider the function foo defined as follows:\n\n> foo :: Num a => [a] -> a\n> foo l = head (map double l)\n>   where double x = x + x\n\nThe step-counting analysis of this function yields an O(n) complexity despite the fact it runs in O(1) under lazy evaluation.\n\nRestrictions\nAbove analysis rules contain a great deal of simplifications; they do not deal with higher-order functions.\nAnother restriction is related to \"syntactic sugar\".\n\nE.g.\n  f x | p x       = a\n      | otherwise = b\nis analyzed as\n  f x = if p x then a\n               else b\n\n3.2.4 Space efficiency analysis\nFunctional programs can be very greedy for space.\n\nAccumulated and largest space efficiency\nWe define 2 types of space analysis:\n\nAccumulated space analysis where the complexity is expressed in terms of the total of the units selected as a measure; this is the space that would be required if no GC was carried out.\n\nLargest space analysis where the complexity is equal to the largest number of units in use during the reduction sequence; this analysis only takes into account the \"live\" space used during the computation so the GC can be invoked as may times as necessary.\n\nThe 2nd analysis is more appropriate but is more difficult to carry out.\nIt is relatively easier to do an accumulated space analysis for strict programs as it is very similar to step-counting analysis.\n\nExample\nConsider the function reverse as defined in 3.2.3.\nThe definition is slightly altered to expose the call the cons(:) hidden behind the term [x].\n\n> myReverse' :: [a] -> [a]\n> myReverse' [] = []\n> myReverse' (x:xs) = (myReverse' xs) `myAppend` (x : [])\n>\n> myAppend :: [a] -> [a] -> [a]\n> []     `myAppend` ys = ys\n> (x:xs) `myAppend` ys = x : (xs `myAppend` ys)\n\nWith the same argument of step-countings, we get the total cost of reversing a list of length n as n^2/2 + n/2, so O(n^2).\nNote that this function will run in exactly the same sequence under lazy evaluation.\nTherefore the issue of lazy or strict evaluation does not matter in this case.\n\nTo determine the largest amount of space used during the reduction sequence, the following observations can be made.\n\nEach call to the reverse function creates a new list cell.\nHowever, the list cell containing the head of its argument list is no longer referenced and can be GC immediately.\n\nEach call to the concatenation(++) creates a new list cell but also consumes the list cell containing the head of its argument list which can be GC.\n\nWhen applied to a list of length n, the recursive calls to the reverse function create n list cells but the space used by the original list can be reclaimed.\nWhen (++) takes over, it does not affect the total number of \"active\" list cells in the heap so the largest amount of space used during the computation is n, therefore it is in O(n).\n\n3.2.5 Space leaks\nLaziness can produce very strange behavior.\nIn some instances, expressions may be held unevaluated even if their result my occupy less space.\nIn other cases, it my be cheaper to recompute an expression than sharing it.\nThese types of abnormal behavior, called space leaks, occur when:\n  the memory space leaks away invisibly;\n  memory is used when this could have been avoided;\n  memory remains referenced although it could be GC.\n\nSuch problems are very difficult to detect when examining the program.\nRecently, the situation has improved with the availability of heap profiling tools which allow inspection of heap nodes consumption after execution.\n\n3.3 Program transformation\n3.3.1 The Burstall-Darlington transformation system\n... which is useful for transforming recursive functional programs.\nThe basic idea is to treat a program as a set of equations and then perform a series of \"equal-to-equal\" substitutions in order to get an equivalent program.\nFor example, consider the following program expressed as 2 equations\n\n  double x = 2 * x\n  quad x = double (double x)\n\nBy replacing calls to double in the definition of quad by their right-hand side definition, we obtain the following definition of quad:\n\n  quad x = 2 * (2 * x)\n\nThis is one type of primitive transformation called unfolding.\nAnother transformation would be simply to use the laws of multiplication to replace the expression by 4 * x.\nOther primitive transformations include:\n  Definition introduces a new equation based on known equations.\n  Instantiation creates a specialization of a given equation, by giving values to some variables.\n  Folding, the opposite of unfolding, replaces an occurrence of a right-hand side by the appropriate left-hand side definition.\n  Abstraction introduces local definitions.\n\nE.g. consider an inefficient Fibonacci functions:\n\n> f 0 = 1\n> f 1 = 1\n> f n = f (n-1) + f (n-2)\n\nSuppose that we introduce the following definition of g, based on f, which returns a tuple:\n\n> g n = (f (n+1), f n)\n\nIf we manage to find a more efficient version of g, then we can easily define another f' that uses g and is equivalent to f:\n\n  f' n = x+y where (x,y) = g (n-2)\n\nTo find an efficient version of g, we need to eliminate references to f in the definition of g.\nThis can be achieved in 2 stages.\nThe first stage is to instatiate this equation with the particular case n = 0, and unfold using the definition of f:\n\n  g 0 = (f 1, f 0)\n      = (1, 1)\n\nThe second stage is to determine the general case g n, using where clause.\n\n  g n = (x+y, x) where (x,y) = (f n, f (n-1))\n\nNext, a fold transformation using the definition of g can be used to replace the expression (f n, f (n-1)) by g (n-1).\n\n> g' 0 = (1,1)\n> g' n = (x+y, x) where (x,y) = g' (n-1)\n> \n> efficientFib n = x+y where (x,y) = g' (n-2)\n\n  *TEOFP_03 Data.Array> map f [20..30] \n  [10946,17711,28657,46368,75025,121393,196418,317811,514229,832040,1346269]\n  (10.08 secs, 2,537,815,144 bytes)\n  *TEOFP_03 Data.Array> map efficientFib  [20..30] \n  [10946,17711,28657,46368,75025,121393,196418,317811,514229,832040,1346269]\n  (0.01 secs, 3,657,904 bytes)\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n\nExcept for introducing the definition of g, all other transformation can be carried out in a mechanical way.\nThere are techniques, mostly based on intuition, for making such introductions, depending on the function being transformed.\n  \n3.3.2 Tail recursively optimization     \nWe now move on to a different kind of transformation which only increases space efficiency if the compiler implements one particular optimization.\nConsider\n\n> fact 0 = 1\n> fact n = n * fact (n-1)\n\nThis function requires O(n) space to remember the arguments of the operator (*) through the successive recursive calls.\nWe can see that the multiplication cannot start until the recursion has completely unrolled.\nIn addition, since recursive function calls and returns are usually implemented by a stack, the stack growth is also in O(n).\n\nNow \n\n> betterFact = helper 1\n>   where helper result 0 = result\n>         helper result n = helper (n*result) (n-1)\n\nExpressed this way, the function is said to be tail-recursive.\nDuring evaluation, only one instance of the call to the helper function is needed at any time.\nFor this reason, the space occupied by the old function call can be reused by the new function call.\nAnother advantage is that the stack does not need to grow at all.\nThis optimization, carried out by most implementations of functional languages, is called the tail recursively optimization.\n\nNote that in a lazy language, this optimization only works if the parameters of the recursive call are strictly evaluated.\nIf not, the space occupied by the old function call cannot be reused since it contains references to unevaluated arguments.\nIn Haskell, to force strict evaluation of both arguments of the recursive call betterFact, the operator $! needs to be used.\nTherefore, the conditions for tail recursivity optimization are\n  1. the relevant function must be tail-recursive, \n  2. the parameters of the recursive call must be evaluated strictly.\n\nTo allow the compiler to carry out the tail recursivity optimization in myLength and mySum, they must be written as\n\n> myLength' xs = lengthTR xs 0\n>   where lengthTR []     r = r\n>         lengthTR (x:xs) r = lengthTR xs $! (r+1)\n> mySum' xs = sumTR xs 0\n>   where sumTR []     r = r\n>         sumTR (x:xs) r = (sumTR xs) $! (r+x)\n\n  *TEOFP_03 Data.Array> mySum $ take 1000000 [1,1..]\n  1000000\n  (0.93 secs, 293,063,840 bytes)\n  *TEOFP_03 Data.Array> mySum' $ take 1000000 [1,1..]\n  1000000\n  (0.70 secs, 354,411,120 bytes)\n\nBoth tail-recursive programs were obtained by using an accumulating parameter for the result.\nThe stack space usage in this case is in O(1) since the parameters of the tail-recursive functions are evaluated strictly.\n\n3.4 Conclusion\n\n```\n\n# Concrete data types\n\u9045\u5ef6\u8a55\u4fa1\u306e\u304a\u304b\u3052\u3067\u3001\u30ea\u30b9\u30c8\u64cd\u4f5c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\uff09\u306e\u7a7a\u9593\u52b9\u7387\u306f\u9ad8\u3044\u3088\u3001\u3068\u306e\u3053\u3068\u3002\n\u5f93\u3063\u3066\u7121\u9650\u30ea\u30b9\u30c8\u3082\u6271\u3048\u307e\u3059\u3068\u3044\u3046\u306e\u306f\u6709\u540d\u4e8b\u5b9f\u3067\u3059\u306d\u3002\n\n\u305f\u3060\u72b6\u614b\u3092\u6301\u305f\u306a\u3044\uff08\u53c2\u7167\u900f\u660e\uff09\u306e\u3067\u5f0a\u5bb3\u3082\u30a2\u30ea\u307e\u3059\u3088\u3001\u3067\u3082\u5834\u5408\u306b\u3088\u308a\u3051\u308a\u3067\u907f\u3051\u308c\u307e\u3059\u4f8b\u3048\u3070as patterns \u3092\u4f7f\u3046\u3068\u304b\u3002\n\ntail-strict \u306afilter \u306e\u5b9f\u88c5\u306f\u9762\u767d\u304b\u3063\u305f\u3051\u3069\u3001\u9045\u5ef6\u8a55\u4fa1\u306e\u65e8\u5473\u3092\u6d88\u3059\u306e\u3067Haskell \u3089\u3057\u304b\u3089\u306c\u306a\u3041\u3002\n\nO(n) \u306b\u306a\u308breverse \u306f\u5287\u7684\u306b\u9055\u3044\u304c\u51fa\u3066\u9762\u767d\u3044\u3001(++) \u304c\u9ad8\u4fa1\u306a\u3093\u3060\u306a\u3041\u3001\u3001\u3001\u3002\n\n\u3053\u3053\u306b\u51fa\u3066\u3044\u308b\u6700\u9069\u5316\u304c\u3044\u308f\u3086\u308bCPS \u306a\u306e\u304b\u306f\u5206\u304b\u3089\u306a\u3044\u3001\u305f\u3060\u5024(obj)\u3067\u306f\u306a\u304f\u95a2\u6570(arr)\u3068\u3044\u3046\u5909\u63db\u306f\u7c73\u7530\u306e\u306b\u307b\u3072\u304c\u3059\u308b\u3002\n\ntree \u304b\u3089\u3088\u304f\u308f\u304b\u3089\u306a\u304f\u306a\u3063\u3066\u304d\u305f\u3001\u914d\u5217\u3082\u3002\n\u3068\u3044\u3046\u308f\u3051\u3067\u4f59\u308a\u89e6\u3089\u305a\u3067\u3059\u3001\u7406\u89e3\u304c\u60aa\u3044\u3068\u30e1\u30e2\u304c\u305f\u3060\u306e\u52a3\u5316\u30b3\u30d4\u30fc\u306b\u306a\u308b\u306a\u3041\u3002\n\n```lang:CDT_04.lhs\n\n> module CDT_04 where\n\nChapter 4\nConcrete data types\n\n4.1 Lists\nAs an example,\n\n> ldouble, ltriple :: (Num a) => [a] -> [a]\n> ldouble = map (2*)\n> ltriple = map (3*)\n\nThe composition of n list processing functions forms a pipeline.\n\nUnder strict evaluation, the computation requires as much space as the largest function in the pipeline.\nHowever, under lazy evaluation, it is possible for a part of the result of one function to be passed to the next function \"on demand\".\n  \n  *CDT_04> let sequence = ldouble . ltriple $ [1..]\n  *CDT_04> take 10 sequence\n  [6,12,18,24,30,36,42,48,54,60]\n\nWe can see that partial results produced by ltriple are immediately consumed by ldouble, making the intermediate list immediately GC.\nProviding that all the functions in the pipeline produce partial results, their composition operates in constant space in O(1).\nWe'll return such intermediate lists called transient lists in 4.1.4.\n\n4.1.2 The copying problem\n\n4.1.3 Tail strictness and tail recursivity\nA function that does not produce partial results i.e. it traverses the entire list before producing its result, is called a tail-strict function, e.g. filter.\n\n> myFilter :: (a -> Bool) -> [a] -> [a]\n> myFilter _ [] = []\n> myFilter p (x:xs) \n>   | p x       = x : myFilter p xs\n>   | otherwise = myFilter p xs\n\n(Accidentally, this is the same definition in Prelude.)\n\nWe could write a tail-recursive version of filter using an accumulating parameter which collects the elements in reverse order (the list must be reversed at the end).\n\n> filterTR :: (a -> Bool) -> [a] -> [a]\n> filterTR p xs = filterTR' p xs []\n> \n> filterTR' p [] r = reverse r\n> filterTR' p (x:xs) r\n>   | p x       = filterTR' p xs (x:r)\n>   | otherwise = filterTR' p xs r\n\nThis is tail-recursive and tail-strict because the entire list must be traversed before delivering a result.\nThe main advantage is a reduction in stack space usage if the compiler implements tail recursivity optimization.\nHowever, using a tail-strict function may cause an increase of the space used during the computation from O(1) to O(n) if used in a pipeline.\nThis is why care must be taken when changing the definition of a function into its tail-recursive version.\n\nAnother issue is well represented in the follwoing trial:\n\n  *CDT_04> let evenNums = filterTR even [1..]\n  (0.00 secs, 1,028,240 bytes)\n  *CDT_04> take 10 even\n  even      evenNums\n  *CDT_04> take 10 evenNums \n  ^CInterrupted.\n\nIncidentally, the filter function can also be expressed using foldr:\n\n> filterR :: (a -> Bool) -> [a] -> [a]\n> filterR p xs = foldr helper [] xs\n>   where helper x result \n>           | p x       = x : result\n>           | otherwise = result\n\nThis definition is also not tail-strict since the head of the list if produced without unfolding all the cell to the operator foldr.\n\n4.1.4 Deforestation with lists\nWe've seen that function composition of list functions can create intermediate lists.\nDespite the fact that these intermediate lists are GC, it may be desirable to avoid creating them at all.\n\nIn some cases, this can be achieved by using Burstall-Darlington transformation system, see 3.3.1.\nFor example, the composition of ldouble and ltriple becomes\n\n> ldt :: (Num a) => [a] -> [a]\n> ldt = map (6*)\n\nThis pdocedure to remove intermediate lists forms the basis of Wadler's deforestation algorithm and could be automatically implemented by a compiler but is most often done \"by hand\".\nThis also can be seen as an application of functor law of composition.\n  (map (2*)) . (map (3*)) = map (2*3*) \n\n4.1.5 Removing appends\nIn this section, we consider another example of using the Bustall-Darlington transformation approach to remove calls to (++).\nThe description presented here is based on a technique described by Wadler and inspired by his work on deformation.\nFirst, here are 3 laws to which (++) obeys:\n  [] ++ xs         = xs\n  (x : ys) ++ zs   = x : (ys ++ zs)\n  (xs ++ yx) ++ zs = xs ++ (ys ++ zs)\n\nThe aim of the transformation is to eliminate (++) from expressions of the form\n  (f x1 .. xn) ++ y \nby defining a function f' s.t.\n  f' x1 .. xn y = (f x1 .. xn) ++ y\n\nExpressions of the form (f x1 .. xn) will be replaced by (f' x1 .. xn []).\nTo derived the function f', each definition of the form \n  {f x1 .. xn = e}\nis replaced by\n  {f' x1 .. xn y = e ++ y}\nThis technique is known as generalization because f' is a generalization of f.\n\nFor example,\n  reverse []     = []\n  reverse (x:xs) = (reverse xs) ++ (x : [])\nWe need to define a function reverse' according to the rules outlined earlier.\nWe can use\n  reverse' xs y = (reverse xs) ++ y\nFirst, for an empty list,\n  reverse' [] y = reverse [] ++ y\n                = [] ++ y\n                = y\nNext is the case of a non-empty list\n  reverse' (x:xs) y = ((reverse xs) ++ (x : [])) ++ y\n                    = (reverse xs) ++ ((x : []) ++ y)\n                    = (reverse xs) ++ (x : ([] ++ y))\n                    = (reverse xs) ++ (x : y)\n                    = reverse' xs (x : y)\n\nPutting the 2 cases together,\n\n> reverse' []     y = y\n> reverse' (x:xs) y = reverse' xs (x : y)\n\nReplacing calls of the form\n  reverse xs\nby\n  reverse' xs []\nleads to a dramatic improvement from O(n^2) to O(n) in both time and space!\nThis technique cannot always guarantee an improvement for every function and requires some skill to carry it out successfully.\n\n> myReverse :: [a] -> [a]\n> myReverse []     = []\n> myReverse (x:xs) = (myReverse xs) ++ [x]\n\n> myReverse' xs = reverse' xs []\n\n  *CDT_04> myReverse [1..10000]\n  (4.15 secs, 4,340,343,400 bytes)\n  *CDT_04> myReverse' [1..10000]\n  (0.20 secs, 49,522,584 bytes)\n\n4.1.6 Reducing the number of passes\nSome functions need to traverse a list several times before delivering a result.\nOften, it is desirable to reduce the number of passes, particularly if the list involved is large.\nFor example,\n  average xs = (mySum xs) / fromIntegral (myLength xs)\n\n2 list traversals are neede, one for sum and the other for length.\nIn addition, the entire list must reside in memory at once because it is shared by both functions.\n\nTupling can beused:\n\n> averageT xs = summedList / fromIntegral lengthOfList\n>   where (summedList, lengthOfList) = av xs\n>         av [] = (0, 0)\n>         av (x:xs) = (x+s, n+1)\n>           where (s,n) = av xs\n\nHowever, there are 2 problems with using tuples, extra space and space leak, i.e. computing the average will not run in constant largest space as the tail-recursive versions of sum and length will do.\n\nA solution is to include both results as parameters to the function.\n\n  av' xs s n = (sum' xs + s) / fromInteger (length' xs + n)\n\nA more efficient version of average can be derived in the usual transformation style:\n\n> average' :: Fractional a => [a] -> a\n> average' xs = av' xs 0 0\n>   where av' []     s n = s / fromInteger n\n>         av' (x:xs) s n = av' xs (x+s) (n+1)\n\nThe advantage of this version is that there is no need for the entire list to reside in memory so the space leak is avoided.\nProviding that the expressions (x+s) and (n+1) are evaluated strictly, the compiler can implement tail recursivity optimization so the overall space costs are reduced.\n\n4.2 Trees\n4.2.1 Terminology\n\nIf a binary tree has n nodes and a depth d, some interesting properties follow:\n1.\nThe minimum depth is the smallest integer at least (log(n+1)).\nIf n is the form 2^k -1, the minimum depth is k, and the tree is said to be perfectly balanced in which case there are 2^(k-1)-1 interior nodes and 2^(k-1) leaves.\n\n2.\nThe maximum depth is d=n (a chain of nodes).\n\nUnless mentioned otherwise, all logarithms are base 2.\nIn the rest of this section, we assume the following binary tree\n\n> data BinTree a = Empty\n>                | NodeBT a (BinTree a) (BinTree a)\n>                deriving (Show)\n\n> t1 :: BinTree Int\n> t1 = NodeBT 5 (NodeBT 8 (NodeBT 3 Empty Empty)\n>                         (NodeBT 1 Empty Empty)\n>               )\n>               (NodeBT 6 Empty\n>                         (NodeBT 4 Empty Empty)\n>               )\n\n4.2.2 Composing tree operations\nA function taking a tree as input may need to consume either the whole tree(O(n)) or one path(O(d)) at a time before delivering partial results.\nFor example,\n\n> tcomp :: BinTree Int -> Int\n> tcomp t = (tsum . tdouble) t\n\n> tdouble :: BinTree Int -> BinTree Int\n> tdouble Empty            = Empty\n> tdouble (NodeBT v lf rt) = NodeBT (2*v) (tdouble lf) (tdouble rt)\n\n> -- tsum :: BinTree Int -> Int\n> -- tsum Empty            = 0\n> -- tsum (NodeBT v lf rt) = v + (tsum lf) + (tsum rt)\n\nWe can see that largest space used is proportional to the logest path in the tree.\n\nUsing the deforestation algorithm mensioned in 4.1.4,\n\n  tcomp' Empty            = 0\n  tcomp' (NodeBt v lf rt) = 2*v + (tcomp' lf) + (tcomp' rt)\n\nIn addition to lists and trees, the deforestation algorithm can deal with any other ADT.\n\n4.2.3 Reducing the number of passes\nAs with lists, the Burstall-Darlington transformation style can be used to improve the efficiency of tree processing programs.\n\nAs with lists, \n\n> count_depth Empty = (1, 0)\n> count_depth (NodeBT v lf rt) = (c1 + c2, 1 + (max d1 d2))\n>   where (c1, d1) = count_depth lf\n>         (c2, d2) = count_depth rt\n\nSometimes, there might be a dependency between the two traversals, that is, the second traversal needs a value computed by the first traversal.\nFor example, replacing each value in the tree by a percentage value (the original value divided by the total of all the values) can be specified using the following set of equations:\n\n> comp t = perc (tsum t) t\n\n> tsum Empty            = 0\n> tsum (NodeBT v lf rt) = v + tsum lf + tsum rt\n\n> perc x Empty            = Empty\n> perc x (NodeBT v lf rt) = NodeBT (fromInteger v / fromInteger x)\n>                                  (perc x lf)\n>                                  (perc x rt)\n\nWe can see that the tree must be traversed twice during evaluation of the function comp.\nBird show a technique to achieve the computation in one traversal.\nIt consists of introducing a function\n\n  comp'' x t = (perc x t, tsum t)\n\nGiven the this function, an alternative function to comp can be defined\n\n> comp' t = t' where (t', x) = comp'' x t\n\nSo the problem now is to define an efficient version of the comp'' function.\nTo achieve this, the definition of comp'' is instantiated with the 2 cases, Empty and Node.\n\n> comp'' x Empty = (Empty, 0) -- (perc x Empty, tsum Empty)\n\n> comp'' x (NodeBT v lf rt)\n>   = (NodeBT (fromInteger v / fromInteger x) p1 p2, v + s1 + s2)\n>       where (p1,s1) = comp'' x lf\n>             (p2,s2) = comp'' x rt\n\n4.2.4 Removing appends revised\nIt is often the case that gathering information from a tree into a list produces multiple recursive calls to (++).\nFor example, consider again the function which converts a tree into list function using the inorder(2.6.5) traversal.\n\n> inorder Empty            = []\n> inorder (NodeBT a lf rt) = inorder lf ++ [a] ++ inorder rt\n\nIn this case, an improvement can be made by removing calls to the append function (4.1.5).\n\n> inorder' t = helper t []\n>   where helper Empty            z = z\n>         helper (NodeBT a lf rt) z = helper lf (a : (helper rt z))\n\n4.2.5 Copying in trees\nIn some instances, copying can be avoided through the use of labels as we demonstrated with lists.\n\nConsider the following binary trees which only contain values at the leaves.\n\n> data BinTree'' a = Leaf'' a\n>                  | NodeBT'' (BinTree'' a) (BinTree'' a)\n\nNow, consider the following function that flips all the left-right pairs:\n\n> flipT :: BinTree'' a -> BinTree'' a\n> flipT (NodeBT'' a b) = NodeBT'' (flipT b) (flipT a)\n> -- flipT (Leaf'' a)  = Leaf'' a\n\nThe evaluation of flipT as defined above causes all the leaves in the original tree to be unnecessarily replicated.\nTo avoid this problem, use as pattern:\n\n> flipT x@(Leaf'' a)   = x\n\n4.2.6 Storing additional information in the tree\nIn some cases, some additional information can be stored in the nodes to avoid multiple traversals of the tree.\nFor example, consider the problem of inserting a node at the lowest level of the smallest (in size) subtree:\n\n  tInsert v Empty = NodeBT v Empty Empty\n  tInsert v (NodeBT w lf rt) \n    | (size lf) <= (size rt) = NodeBT w (tInsert v lf) rt\n    | otherwise              = NodeBT w lf             (tInsert v rt)\n\nAn alternative is to use the following tree declaration where the sizes of the right and the left subtrees are stored together with the value of the root node.\n\n> data BinTreeSz a = EmptySz\n>                  | NodeBTSz (Int, Int) a (BinTreeSz a) (BinTreeSz a)\n\nIn this case, the tree insertion function becomes\n\n> tInsertSz :: a -> BinTreeSz a -> BinTreeSz a\n> tInsertSz v EmptySz = NodeBTSz (0,0) v EmptySz EmptySz\n> tInsertSz v (NodeBTSz (s1,s2) w lf rt)\n>   | s1 <= s2  = NodeBTSz (1+s1, s2) w (tInsertSz v lf) rt\n>   | otherwise = NodeBTSz (s1, 1+s2) w lf               (tInsertSz v rt)\n\n4.3 Arrays\n4.3.1 Functional and imperative arrays\n.. there is no general consensus yet,\n\n4.3.2 Handling array updates\nThe main problem is with the update operations.\n(see 10.2.2. state monad)\n\n4.3.3 Higher-order array functions\n\n```\n", "tags": ["Haskell", "algorithm", "\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0"]}