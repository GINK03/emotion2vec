{"context": " More than 1 year has passed since last update.\n\n\u306f\u3058\u3081\u306b\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u3064\u3044\u3066\u306fApache Spark\u5165\u9580(\u7fd4\u6cf3\u793e)\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\u6700\u7d42\u7684\u306b\u4f5c\u308b\u74b0\u5883\u306fGCE\u3067\u4ee5\u4e0b\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\u3059\u308b\nspark-master x1\nspark-worker x2\nspark-client x1\n\n1. \u5171\u901a\u30a4\u30e1\u30fc\u30b8\u306e\u4f5c\u6210\u3002\u5168\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5171\u901a\u306e\u8a2d\u5b9a\u3092\u3057\u3066\u304a\u304f\u3002\n\u5171\u901a\u30a4\u30e1\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u3092\u5916\u3057\u3066\u304a\u304f\u3002\u3053\u308c\u306f\u30c7\u30a3\u30b9\u30af\u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u308b\u969b\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u30c7\u30a3\u30b9\u30af\u306e\u7d10\u4ed8\u3051\u3092\u89e3\u9664\u3059\u308b\u305f\u3081\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u524a\u9664\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3042\u308b\u304c\u3001\u305d\u306e\u969b\u306b\u30c7\u30fc\u30bf\u3092\u6d88\u3055\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u3002\n\u300c\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u524a\u9664\u3059\u308b\u969b\u306b\u30d6\u30fc\u30c8\u30c7\u30a3\u30b9\u30af\u3092\u524a\u9664\u3059\u308b\u300d\n\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30af\u30ea\u30c3\u30af\u3057\u3001ssh\u30ad\u30fc\u306e\u7b87\u6240\u3067\u516c\u958b\u9375\u3092\u767b\u9332\u3059\u308b\u3002\n$ ssh -o UserKnownHostsFile=/dev/null -o CheckHostIP=no -o StrictHostKeyChecking=no user\u540d@IP\u30a2\u30c9\u30ec\u30b9\n\n\nJDK\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\nOracke JDK\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002Cookie\u306bAccept\u3057\u305f\u3068\u3044\u3046\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u542b\u3081\u3066wget\u3059\u308c\u3070\u3001\u540c\u610f\u3057\u305f\u3068\u307f\u306a\u3055\u308c\u3001URL\u76f4\u6307\u5b9a\u3067\u3082\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u3002\n$ wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u66-b17/jdk-8u66-linux-x64.rpm\n\n$ sudo rpm -ivh jdk-8u66-linux-x64.rpm \n\n\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\n\n/etc/profile.d/java.sh\n$ export JAVA_HOME=/usr/java/default\n$ export PATH=$JAVA_HOME/bin/:$PATH\n\n\n\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\nSpark\u306fHadoop2.6\u4ee5\u4e0a\u5411\u3051\u306b\u30d3\u30eb\u30c9\u3055\u308c\u305f\u3082\u306e\u3092\u4f7f\u7528\u3059\u308b\u3002\n$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz\n$ sudo tar zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /opt/\n$ cd /opt\n$ sudo ln -s spark-1.5.2-bin-hadoop2.6 spark\n\n\n/etc/profile.d/spark.sh\nexport SPARK_HOME=/opt/spark\n\n\nsource /etc/profile\n\n$ /opt/spark/bin/spark-submit --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n      /_/\n\nType --help for more information.\n\n$ /opt/spark/bin/spark-submit --class org.apache.spark.examples.SparkPi /opt/spark/lib\n/spark-examples-1.5.2-hadoop2.6.0.jar 10\n\u30fb\u30fb\u30fb\u30fb\nPi is roughly 3.142552\n\u30fb\u30fb\u30fb\n\n$ sudo rpm --import http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\n\n$ sudo rpm -ivh http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\n\n\n/etc/yum.repos.d/cloudera-cdh5.repo\n[cloudera-cdh5]\n# Packages for Cloudera's Distribution for Hadoop, Version 5, on RedHat or CentOS 6 x86_64\nname=Cloudera's Distribution for Hadoop, Version 5\nbaseurl=http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5/\ngpgkey = http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera    \ngpgcheck = 1\n\n\n$ yum clean all\nLoaded plugins: fastestmirror, security\nCleaning repos: base cloudera-cdh5 extras scl updates\nCleaning up Everything\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/pkgtups-checksums\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/file-requires\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/version\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/conflicts\n\n\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u30ea\u30d6\u30fc\u30c8\u3059\u308b\u3068/etc/hosts\u304c\u306a\u305c\u304b\u521d\u671f\u5316\u3055\u308c\u308b\uff1f\u3088\u3046\u306a\u306e\u3067\u3001/etc/rc.local\u3067hosts\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3059\u308b\u3088\u3046\u306b\u4ed5\u8fbc\u3080\u3002\u672c\u5f53\u306fchef\u3084ansible\u3001\u3082\u3057\u304f\u306fgce\u306e\u6a19\u6e96\u306e\u6a5f\u80fd\u3067\u3067\u304d\u305d\u3046\u3067\u3042\u308b\u304c\u3001\u5f8c\u3067\u3084\u308a\u65b9\u3092\u8abf\u3079\u308b\u3002\n\u3053\u3053\u307e\u3067\u3067\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30c7\u30a3\u30b9\u30af\u306f\u3067\u304d\u305f\u306e\u3067\u3001\u3053\u3053\u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u30a4\u30e1\u30fc\u30b8=>\u65b0\u3057\u3044\u30a4\u30e1\u30fc\u30b8\u4f5c\u6210\u3067\u3001\u5148\u307b\u3069\u4f5c\u6210\u3057\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u540d\u3092\u6307\u5b9a\u3057\u3001\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b\u3002\n\n2. Hadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\n\n2.1. master\u30ce\u30fc\u30c9\nspark-master\u306bNameNode\u3068\u3001ResourceManager\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n$ sudo yum install -y hadoop-hdfs-namenode\n$ sudo yum install -y hadoop-yarn-resourcemanager\n\n\n2.2. worker\u30ce\u30fc\u30c9\n$ sudo yum install -y hadoop-hdfs-datanode\n$ sudo yum install -y hadoop-yarn-nodemanager\n\nworker\u30ce\u30fc\u30c9\u306f\u8907\u6570\u4f5c\u6210\u3059\u308b\u306e\u3067\u3001\u3053\u306e\u30ce\u30fc\u30c9\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u4f7f\u3063\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u4f5c\u6210\u3002\u305d\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30b0\u30eb\u30fc\u30d7\u3067\uff14\u53f0\u4f5c\u6210\u3059\u308b\u3002\n\n2.3. Client\u30ce\u30fc\u30c9\n$ sudo yum install -y hadoop-hdfs hadoop-yarn\n\n\n2.4. Hadoop\u5171\u901a\u8a2d\u5b9a\n\u5168\u3066\u306e\u30ce\u30fc\u30c9\u3067\u4ee5\u4e0b\u306e\u8a2d\u5b9a\u3092\u884c\u3046\u3002\n\ncore-site.xml\n<configuration>\n   <property>\n      <name>fs.defaultFS</name> #\u5229\u7528\u3059\u308b\u5206\u6563\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u6307\u5b9a\n      <value>hdfs://spark-master01:8020</value>\n   </property>\n   <property>\n      <name>hadoop.tmp.dir</name> #HDFS\u3084YARN\u304c\u5229\u7528\u3059\u308b\u4e00\u6642\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\n      <value>/hadoop/tmp</value>\n   </property>\n</configuration>\n\n\n\nhdfs-site.xml\n<configuration>\n  <property>\n     <name>dfs.namenode.name.dir</name> #NameNode\u304cHDFS\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a\n     <value>file:///hadoop/hdfs/name</value>\n  </property>\n\n  <property>\n     <name>dfs.datanode.data.dir</name> #DataNode\u304c\u30d6\u30ed\u30c3\u30af\u3092\u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a\u3002\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u306e\u5408\u8a08\u5024/3(\u30d6\u30ed\u30c3\u30af\u306e\u8907\u88fd\u6570\uff09\u304cHDFS\u3068\u3057\u3066\u683c\u7d0d\u3067\u304d\u308b\u30c7\u30fc\u30bf\u5bb9\u91cf\u3002\n     <value>file:///hadoop/hdfs/data</value>\n  </property>\n</configuration>\n\n\n\nyarn-site.xml\n<configuration>\n  <property>\n    <name>yarn.resourcemanager.hostname</name> #ResourceManager\u304c\u52d5\u4f5c\u3059\u308b\u30de\u30b7\u30f3\u306e\u30db\u30b9\u30c8\u540d\n    <value>spark-master</value>\n  </property>\n\n  <property>\n    <name>yarn.nodemanager.log-dirs</name> #\u30b3\u30f3\u30c6\u30ca\u306e\u30ed\u30b0\u306e\u683c\u7d0d\u5148\u3002\n    <value>file:///hadoop/yarn/node-manager/logs</value>\n  </property>\n\n  <property>\n    <name>yarn.nodemanager.local-dirs</name> #NodeManager\u304c\u5229\u7528\u3059\u308b\u30ed\u30fc\u30ab\u30eb\u30c7\u30a3\u30b9\u30af\u306e\u30d1\u30b9\n    <value>file:///hadoop/yarn/node-manager/local</value>\n  </property>\n</configuration>\n\n\nHDFS\u3084YARN\u306e\u52d5\u4f5c\u306b\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\n$ sudo mkdir -p /hadoop/hdfs\n$ sudo chown hdfs:hadoop /hadoop/hdfs\n$ sudo chmod 775 /hadoop/hdfs\n$ sudo mkdir -p /hadoop/yarn\n$ sudo chown yarn:hadoop /hadoop/yarn\n$ sudo chmod 775 /hadoop/yarn\n$ sudo mkdir -p /hadoop/tmp\n$ sudo chmod 777 /hadoop/tmp\n\n# sudo -u hdfs hdfs namenode -format\n\n\n2.5. HDFS\u306e\u8d77\u52d5\u3068\u52d5\u4f5c\u78ba\u8a8d\nspark-worker01,02\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n# service hadoop-hdfs-datanode start\n\nspark-master01\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n# service hadoop-hdfs-namenode start\n\n\u78ba\u8a8d\n# sudo -u hdfs hdfs dfsadmin -report\nConfigured Capacity: 10432602112 (9.72 GB)\nPresent Capacity: 7175352320 (6.68 GB)\nDFS Remaining: 7175311360 (6.68 GB)\nDFS Used: 40960 (40 KB)\nDFS Used%: 0.00%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\nMissing blocks (with replication factor 1): 0\n-------------------------------------------------\nLive datanodes (1):\nName: 10.240.0.5:50010 (spark-worker01.c.yotsu-1130.internal)\nHostname: spark-worker01.c.yotsu-1130.internal\nDecommission Status : Normal\nConfigured Capacity: 10432602112 (9.72 GB)\nDFS Used: 40960 (40 KB)\nNon DFS Used: 3257249792 (3.03 GB)\nDFS Remaining: 7175311360 (6.68 GB)\nDFS Used%: 0.00%\nDFS Remaining%: 68.78%\nConfigured Cache Capacity: 0 (0 B)\nCache Used: 0 (0 B)\nCache Remaining: 0 (0 B)\nCache Used%: 100.00%\nCache Remaining%: 0.00%\nXceivers: 1\nLast contact: Wed Nov 18 01:03:44 UTC 2015\n\nHDFS\u4e0a\u306b\u5404\u7a2e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\n# sudo -u hdfs hdfs dfs -mkdir -p /hadoop/tmp\n# sudo -u hdfs hdfs dfs -chmod 777 /hadoop/tmp\n# sudo -u hdfs hdfs dfs -mkdir -p /tmp\n# sudo -u hdfs hdfs dfs -chmod777 /tmp\n# sudo -u hdfs hdfs dfs -mkdir -p /hadoop/yarn/app-logs\n# sudo -u hdfs hdfs dfs -chmod 777 /hadoop/yarn/app-logs\n# sudo -u hdfs hdfs dfs -mkdir -p /user/y_tadayasu\n# sudo -u hdfs hdfs dfs -chown y_tadayasu:y_tadayasu /user/y_tadayasu\n\n\n2.6. YARN\u306e\u8d77\u52d5\u3068\u52d5\u4f5c\u78ba\u8a8d\nspark-worker01,02\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n# service hadoop-yarn-nodemanager start\n\nspark-master01\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n# service hadoop-yarn-resourcemanager start\n\n\u78ba\u8a8d\n# sudo -u yarn yarn node -list -all\n15/11/18 01:14:07 INFO client.RMProxy: Connecting to ResourceManager at spark-master01/10.240.0.2:8032\nTotal Nodes:2\n         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers\nspark-worker02.c.yotsu-1130.internal:38198              RUNNING spark-worker02.c.yotsu-1130.internal:8042       \n                          0\nspark-worker01.c.yotsu-1130.internal:41966              RUNNING spark-worker01.c.yotsu-1130.internal:8042       \n                          0\n\n\n3. Spark\u306e\u52d5\u4f5c\u78ba\u8a8d\n\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3059\u308b\u3002\n$ ./bin/spark-submit --master yarn-client --class org.apache.spark.examples.Spark\nPi /opt/spark/lib/spark-examples-1.5.2-hadoop2.6.0.jar 10\n\u30fb\u30fb\u30fb\nPi is roughly 3.1425\n\u30fb\u30fb\u30fb\n\n\n# \u306f\u3058\u3081\u306b\nSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u3064\u3044\u3066\u306fApache Spark\u5165\u9580(\u7fd4\u6cf3\u793e)\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\u6700\u7d42\u7684\u306b\u4f5c\u308b\u74b0\u5883\u306fGCE\u3067\u4ee5\u4e0b\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\u3059\u308b\nspark-master x1\nspark-worker x2\nspark-client x1\n\n# 1. \u5171\u901a\u30a4\u30e1\u30fc\u30b8\u306e\u4f5c\u6210\u3002\u5168\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5171\u901a\u306e\u8a2d\u5b9a\u3092\u3057\u3066\u304a\u304f\u3002\n\u5171\u901a\u30a4\u30e1\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u9805\u76ee\u306e\u30c1\u30a7\u30c3\u30af\u3092\u5916\u3057\u3066\u304a\u304f\u3002\u3053\u308c\u306f\u30c7\u30a3\u30b9\u30af\u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u308b\u969b\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u30c7\u30a3\u30b9\u30af\u306e\u7d10\u4ed8\u3051\u3092\u89e3\u9664\u3059\u308b\u305f\u3081\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u524a\u9664\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3042\u308b\u304c\u3001\u305d\u306e\u969b\u306b\u30c7\u30fc\u30bf\u3092\u6d88\u3055\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u3002\n\u300c\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u524a\u9664\u3059\u308b\u969b\u306b\u30d6\u30fc\u30c8\u30c7\u30a3\u30b9\u30af\u3092\u524a\u9664\u3059\u308b\u300d\n\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30af\u30ea\u30c3\u30af\u3057\u3001ssh\u30ad\u30fc\u306e\u7b87\u6240\u3067\u516c\u958b\u9375\u3092\u767b\u9332\u3059\u308b\u3002\n\n```\n$ ssh -o UserKnownHostsFile=/dev/null -o CheckHostIP=no -o StrictHostKeyChecking=no user\u540d@IP\u30a2\u30c9\u30ec\u30b9\n```\n#### JDK\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\nOracke JDK\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002Cookie\u306bAccept\u3057\u305f\u3068\u3044\u3046\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u542b\u3081\u3066wget\u3059\u308c\u3070\u3001\u540c\u610f\u3057\u305f\u3068\u307f\u306a\u3055\u308c\u3001URL\u76f4\u6307\u5b9a\u3067\u3082\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u3002\n\n```\n$ wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u66-b17/jdk-8u66-linux-x64.rpm\n\n$ sudo rpm -ivh jdk-8u66-linux-x64.rpm \n```\n\n\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\n\n```/etc/profile.d/java.sh\n$ export JAVA_HOME=/usr/java/default\n$ export PATH=$JAVA_HOME/bin/:$PATH\n```\n \n#### Spark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\nSpark\u306fHadoop2.6\u4ee5\u4e0a\u5411\u3051\u306b\u30d3\u30eb\u30c9\u3055\u308c\u305f\u3082\u306e\u3092\u4f7f\u7528\u3059\u308b\u3002\n\n```\n\b$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz\n$ sudo tar zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /opt/\n$ cd /opt\n$ sudo ln -s spark-1.5.2-bin-hadoop2.6 spark\n```\n\n```/etc/profile.d/spark.sh\nexport SPARK_HOME=/opt/spark\n```\n\n```\nsource /etc/profile\n```\n\n```\n$ /opt/spark/bin/spark-submit --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n      /_/\n                        \nType --help for more information.\n```\n\n```\n$ /opt/spark/bin/spark-submit --class org.apache.spark.examples.SparkPi /opt/spark/lib\n/spark-examples-1.5.2-hadoop2.6.0.jar 10\n\u30fb\u30fb\u30fb\u30fb\nPi is roughly 3.142552\n\u30fb\u30fb\u30fb\n```\n\n```\n$ sudo rpm --import http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\n\n$ sudo rpm -ivh http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\n```\n\n```/etc/yum.repos.d/cloudera-cdh5.repo\n[cloudera-cdh5]\n# Packages for Cloudera's Distribution for Hadoop, Version 5, on RedHat or CentOS 6 x86_64\nname=Cloudera's Distribution for Hadoop, Version 5\nbaseurl=http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5/\ngpgkey = http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera    \ngpgcheck = 1\n```            \n\n```\n$ yum clean all\nLoaded plugins: fastestmirror, security\nCleaning repos: base cloudera-cdh5 extras scl updates\nCleaning up Everything\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/pkgtups-checksums\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/file-requires\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/version\nCannot remove rpmdb file /var/lib/yum/rpmdb-indexes/conflicts\n```\n\n\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u30ea\u30d6\u30fc\u30c8\u3059\u308b\u3068/etc/hosts\u304c\u306a\u305c\u304b\u521d\u671f\u5316\u3055\u308c\u308b\uff1f\u3088\u3046\u306a\u306e\u3067\u3001/etc/rc.local\u3067hosts\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3059\u308b\u3088\u3046\u306b\u4ed5\u8fbc\u3080\u3002\u672c\u5f53\u306fchef\u3084ansible\u3001\u3082\u3057\u304f\u306fgce\u306e\u6a19\u6e96\u306e\u6a5f\u80fd\u3067\u3067\u304d\u305d\u3046\u3067\u3042\u308b\u304c\u3001\u5f8c\u3067\u3084\u308a\u65b9\u3092\u8abf\u3079\u308b\u3002\n\n\u3053\u3053\u307e\u3067\u3067\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30c7\u30a3\u30b9\u30af\u306f\u3067\u304d\u305f\u306e\u3067\u3001\u3053\u3053\u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b\u3002\n\n\u30a4\u30e1\u30fc\u30b8=>\u65b0\u3057\u3044\u30a4\u30e1\u30fc\u30b8\u4f5c\u6210\u3067\u3001\u5148\u307b\u3069\u4f5c\u6210\u3057\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u540d\u3092\u6307\u5b9a\u3057\u3001\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b\u3002\n\n# 2. Hadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u8a2d\u5b9a\n#### 2.1. master\u30ce\u30fc\u30c9\nspark-master\u306bNameNode\u3068\u3001ResourceManager\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n```\n$ sudo yum install -y hadoop-hdfs-namenode\n$ sudo yum install -y hadoop-yarn-resourcemanager\n```\n\n#### 2.2. worker\u30ce\u30fc\u30c9\n\n```\n$ sudo yum install -y hadoop-hdfs-datanode\n$ sudo yum install -y hadoop-yarn-nodemanager\n```\n\nworker\u30ce\u30fc\u30c9\u306f\u8907\u6570\u4f5c\u6210\u3059\u308b\u306e\u3067\u3001\u3053\u306e\u30ce\u30fc\u30c9\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u4f7f\u3063\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u4f5c\u6210\u3002\u305d\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30b0\u30eb\u30fc\u30d7\u3067\uff14\u53f0\u4f5c\u6210\u3059\u308b\u3002\n\n#### 2.3. Client\u30ce\u30fc\u30c9\n\n```\n$ sudo yum install -y hadoop-hdfs hadoop-yarn\n```\n\n#### 2.4. Hadoop\u5171\u901a\u8a2d\u5b9a\n\u5168\u3066\u306e\u30ce\u30fc\u30c9\u3067\u4ee5\u4e0b\u306e\u8a2d\u5b9a\u3092\u884c\u3046\u3002\n\n```core-site.xml\n<configuration>\n   <property>\n      <name>fs.defaultFS</name> #\u5229\u7528\u3059\u308b\u5206\u6563\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u6307\u5b9a\n      <value>hdfs://spark-master01:8020</value>\n   </property>\n   <property>\n      <name>hadoop.tmp.dir</name> #HDFS\u3084YARN\u304c\u5229\u7528\u3059\u308b\u4e00\u6642\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\n      <value>/hadoop/tmp</value>\n   </property>\n</configuration>\n```\n\n```hdfs-site.xml\n<configuration>\n  <property>\n     <name>dfs.namenode.name.dir</name> #NameNode\u304cHDFS\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a\n     <value>file:///hadoop/hdfs/name</value>\n  </property>\n\n  <property>\n     <name>dfs.datanode.data.dir</name> #DataNode\u304c\u30d6\u30ed\u30c3\u30af\u3092\u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a\u3002\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u306e\u5408\u8a08\u5024/3(\u30d6\u30ed\u30c3\u30af\u306e\u8907\u88fd\u6570\uff09\u304cHDFS\u3068\u3057\u3066\u683c\u7d0d\u3067\u304d\u308b\u30c7\u30fc\u30bf\u5bb9\u91cf\u3002\n     <value>file:///hadoop/hdfs/data</value>\n  </property>\n</configuration>\n```\n\n```yarn-site.xml\n<configuration>\n  <property>\n    <name>yarn.resourcemanager.hostname</name> #ResourceManager\u304c\u52d5\u4f5c\u3059\u308b\u30de\u30b7\u30f3\u306e\u30db\u30b9\u30c8\u540d\n    <value>spark-master</value>\n  </property>\n\n  <property>\n    <name>yarn.nodemanager.log-dirs</name> #\u30b3\u30f3\u30c6\u30ca\u306e\u30ed\u30b0\u306e\u683c\u7d0d\u5148\u3002\n    <value>file:///hadoop/yarn/node-manager/logs</value>\n  </property>\n\n  <property>\n    <name>yarn.nodemanager.local-dirs</name> #NodeManager\u304c\u5229\u7528\u3059\u308b\u30ed\u30fc\u30ab\u30eb\u30c7\u30a3\u30b9\u30af\u306e\u30d1\u30b9\n    <value>file:///hadoop/yarn/node-manager/local</value>\n  </property>\n</configuration>\n```\n\nHDFS\u3084YARN\u306e\u52d5\u4f5c\u306b\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\n\n```\n$ sudo mkdir -p /hadoop/hdfs\n$ sudo chown hdfs:hadoop /hadoop/hdfs\n$ sudo chmod 775 /hadoop/hdfs\n$ sudo mkdir -p /hadoop/yarn\n$ sudo chown yarn:hadoop /hadoop/yarn\n$ sudo chmod 775 /hadoop/yarn\n$ sudo mkdir -p /hadoop/tmp\n$ sudo chmod 777 /hadoop/tmp\n```\n\n```\n# sudo -u hdfs hdfs namenode -format\n```\n\n#### 2.5. HDFS\u306e\u8d77\u52d5\u3068\u52d5\u4f5c\u78ba\u8a8d\nspark-worker01,02\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n\n```\n# service hadoop-hdfs-datanode start\n```\n\nspark-master01\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n\n```\n# service hadoop-hdfs-namenode start\n```\n\n\u78ba\u8a8d\n\n```\n# sudo -u hdfs hdfs dfsadmin -report\nConfigured Capacity: 10432602112 (9.72 GB)\nPresent Capacity: 7175352320 (6.68 GB)\nDFS Remaining: 7175311360 (6.68 GB)\nDFS Used: 40960 (40 KB)\nDFS Used%: 0.00%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\nMissing blocks (with replication factor 1): 0\n-------------------------------------------------\nLive datanodes (1):\nName: 10.240.0.5:50010 (spark-worker01.c.yotsu-1130.internal)\nHostname: spark-worker01.c.yotsu-1130.internal\nDecommission Status : Normal\nConfigured Capacity: 10432602112 (9.72 GB)\nDFS Used: 40960 (40 KB)\nNon DFS Used: 3257249792 (3.03 GB)\nDFS Remaining: 7175311360 (6.68 GB)\nDFS Used%: 0.00%\nDFS Remaining%: 68.78%\nConfigured Cache Capacity: 0 (0 B)\nCache Used: 0 (0 B)\nCache Remaining: 0 (0 B)\nCache Used%: 100.00%\nCache Remaining%: 0.00%\nXceivers: 1\nLast contact: Wed Nov 18 01:03:44 UTC 2015\n```\n\nHDFS\u4e0a\u306b\u5404\u7a2e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\n\n```\n# sudo -u hdfs hdfs dfs -mkdir -p /hadoop/tmp\n# sudo -u hdfs hdfs dfs -chmod 777 /hadoop/tmp\n# sudo -u hdfs hdfs dfs -mkdir -p /tmp\n# sudo -u hdfs hdfs dfs -chmod777 /tmp\n# sudo -u hdfs hdfs dfs -mkdir -p /hadoop/yarn/app-logs\n# sudo -u hdfs hdfs dfs -chmod 777 /hadoop/yarn/app-logs\n# sudo -u hdfs hdfs dfs -mkdir -p /user/y_tadayasu\n# sudo -u hdfs hdfs dfs -chown y_tadayasu:y_tadayasu /user/y_tadayasu\n```\n\n#### 2.6. YARN\u306e\u8d77\u52d5\u3068\u52d5\u4f5c\u78ba\u8a8d\nspark-worker01,02\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n\n```\n# service hadoop-yarn-nodemanager start\n```\n\nspark-master01\u3067\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n\n```\n# service hadoop-yarn-resourcemanager start\n```\n\n\u78ba\u8a8d\n\n```\n# sudo -u yarn yarn node -list -all\n15/11/18 01:14:07 INFO client.RMProxy: Connecting to ResourceManager at spark-master01/10.240.0.2:8032\nTotal Nodes:2\n         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers\nspark-worker02.c.yotsu-1130.internal:38198              RUNNING spark-worker02.c.yotsu-1130.internal:8042       \n                          0\nspark-worker01.c.yotsu-1130.internal:41966              RUNNING spark-worker01.c.yotsu-1130.internal:8042       \n                          0\n```\n\n# 3. Spark\u306e\u52d5\u4f5c\u78ba\u8a8d\n\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3059\u308b\u3002\n\n```\n$ ./bin/spark-submit --master yarn-client --class org.apache.spark.examples.Spark\nPi /opt/spark/lib/spark-examples-1.5.2-hadoop2.6.0.jar 10\n\u30fb\u30fb\u30fb\nPi is roughly 3.1425\n\u30fb\u30fb\u30fb\n```\n", "tags": ["Spark", "googlecomputeengine"]}