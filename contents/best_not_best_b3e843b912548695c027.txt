{"context": "\u7686\u3055\u3093\u3053\u3093\u306b\u3061\u306f\u3002@best_not_best\u3067\u3059\u3002\n\u4eca\u56de\u306f\u62c5\u5f53\u3057\u3066\u3044\u308b\u696d\u52d9\u306b\u6cbf\u3063\u305f\u6280\u8853\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u6982\u8981\n\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3092\u7528\u3044\u3066\u3001\u3042\u308b\u30e6\u30fc\u30b6\u30fc\u304c\u3042\u308b\u5546\u54c1\u3092\u8cfc\u5165\u3059\u308b\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002\u8a08\u7b97\u91cf\u304c\u591a\u304f\u3001\u5927\u898f\u6a21\u306a\u30c7\u30fc\u30bf\u3060\u3068\u51e6\u7406\u306b\u6642\u9593\u304c\u304b\u304b\u308b\u305f\u3081\u3001PySpark\u3067\u5206\u6563\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u74b0\u5883\n\n\u30de\u30b7\u30f3/OS\n\n\nMacBook Pro (Retina, 15-inch, Mid 2014)\nOS X Yosemite 10.10.5\n\n\nPython\n\n\nPython 3.5.2 :: Anaconda 4.1.1 (x86_64)\n\n\nPython\u30d1\u30c3\u30b1\u30fc\u30b8\n\n\n\u7279\u306b\u306a\u3057\n\n\nSpark\n\n\n2.0.0\n\n\n\nSpark\u5074\u3067Python 3.x\u3092\u547c\u3073\u51fa\u305b\u308b\u3088\u3046\u3001spark-env.sh\u306b\u4ee5\u4e0b\u3092\u8ffd\u8a18\u3057\u3066\u304a\u304d\u307e\u3059\u3002\nexport PYSPARK_PYTHON=python3\nPYSPARK_DRIVER_PYTHON=python3\n\n\n\u624b\u9806\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\n\uff08\u5fc5\u8981\u306b\u5fdc\u3058\u3066\uff09\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\n\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3059\u308b\n\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\n\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u7b49\u304b\u3089\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u3001\u300cuser\u300d\u300citem\u300d\u300crating\u300d\u306e3\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u300c\u904e\u53bb1\u30f6\u6708\u5206\u306e\u5546\u54c1\u8cfc\u5165\u30c7\u30fc\u30bf\u300d\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u300c\u30e6\u30fc\u30b6\u30fcID\u300d\u300c\u5546\u54c1ID\u300d\u300c\u305d\u306e\u5546\u54c1\u3092\u8cfc\u5165\u3057\u305f\u304b\u3069\u3046\u304b\uff08\u672a\u8cfc\u5165:0/\u8cfc\u5165:1\uff09\u300d\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u306b\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u3001CSV\u30d5\u30a1\u30a4\u30eb\u306e\u4f8b\u3067\u3059\u3002\n\n\n\nuser\nitem\nrating\n\n\n\n\n1xxxxxxxx2\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx9\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n\n\n\nrating\u306e\u540d\u524d\u306e\u901a\u308a\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u5546\u54c1\u306b\u3069\u308c\u3060\u3051\u8a55\u4fa1\u5024\u3092\u4ed8\u3051\u305f\u304b\u3069\u3046\u304b\u300d\u304c\u672c\u6765\u306e\u4f7f\u3044\u65b9\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u4e0a\u8a18\u306e\u901a\u308a\u300c\u5546\u54c1\u3092\u8cfc\u5165\u3057\u305f\u304b\u3069\u3046\u304b\u300d\u3001\u307e\u305f\u306f\u300c\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3057\u305f\u304b\u3069\u3046\u304b\u300d\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u3067\u3082\u5b9f\u88c5\u306f\u53ef\u80fd\u3067\u3059\u3002\u524d\u8005\u306e\u5834\u5408\u306f\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u5546\u54c1\u3092\u8cfc\u5165\u3059\u308b\u30b9\u30b3\u30a2\u306f\u3069\u306e\u304f\u3089\u3044\u304b\u300d\u3001\u5f8c\u8005\u306f\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3069\u306e\u304f\u3089\u3044\u304b\u300d\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\n\u30e6\u30fc\u30b6\u30fcID\u3084\u5546\u54c1ID\u304cint32\u306e\u6700\u5927\u5024\uff082,147,483,647\uff09\u307e\u3067\u3057\u304b\u6271\u3048\u306a\u3044\u305f\u3081\u3001\u305d\u308c\u3092\u8d85\u3048\u308bID\u304c\u3042\u308b\u5834\u5408\u306bID\u3092\u6539\u3081\u3066\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u307e\u3059\u3002\u307e\u305f\u6574\u6570\u5024\u3057\u304b\u6271\u3048\u306a\u3044\u306e\u3067\u3001\u6587\u5b57\u5217\u542b\u307e\u308c\u308b\u5834\u5408\u3082\u540c\u69d8\u306b\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u307e\u3059\u3002\nID\u304c\u6574\u6570\u5024\u304b\u3064int32\u306e\u6700\u5927\u5024\u3092\u8d85\u3048\u306a\u3044\u5834\u5408\u306f\u3001\u3053\u306e\u5de5\u7a0b\u306f\u98db\u3070\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"processing training data.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\n\nclass ProcessTrainingData(object):\n    \"\"\"get training data from Redshift, and add sequence number to data.\"\"\"\n\n    def __get_action_log(\n        self,\n        sqlContext: SQLContext,\n        unprocessed_data_file_path: str\n    ) -> DataFrame:\n        \"\"\"get data.\"\"\"\n        df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(unprocessed_data_file_path)\n\n        return df\n\n    def run(\n        self,\n        unprocessed_data_file_path: str,\n        training_data_dir_path: str\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('process_training_data')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # get data\n        df = self.__get_action_log(sqlContext, unprocessed_data_file_path)\n\n        # make sequence number of users\n        unique_users_rdd = df.rdd.map(lambda l: l[0]).distinct().zipWithIndex()\n        unique_users_df = sqlContext.createDataFrame(\n            unique_users_rdd,\n            ('user', 'unique_user_id')\n        )\n\n        # make sequence number of items\n        unique_items_rdd = df.rdd.map(lambda l: l[1]).distinct().zipWithIndex()\n        unique_items_df = sqlContext.createDataFrame(\n            unique_items_rdd,\n            ('item', 'unique_item_id')\n        )\n\n        # add sequence number of users, sequence number of items to data\n        df = df.join(\n            unique_users_df,\n            df['user'] == unique_users_df['user'],\n            'inner'\n        ).drop(unique_users_df['user'])\n        df = df.join(\n            unique_items_df,\n            df['item'] == unique_items_df['item'],\n            'inner'\n        ).drop(unique_items_df['item'])\n\n        # save\n        saved_data_file_path = training_data_dir_path + 'cf_training_data.csv'\n        df.write\\\n            .format('csv')\\\n            .mode('overwrite')\\\n            .options(header='true')\\\n            .save(saved_data_file_path)\n\n        return True\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306eCSV\u3092\u8aad\u307f\u8fbc\u307f\u3001zipWithIndex()\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u30ab\u30e9\u30e0\u3092\u8ffd\u52a0\u3057\u3001\u5225\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\nptd = ProcessTrainingData()\nptd.run(unprocessed_data_file_path, training_data_dir_path)\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\nunprocessed_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\ntraining_data_dir_path : \u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u306e\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\n\n\u524d\u8ff0\u306e\u901a\u308a\u3001unprocessed_data_file_path\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n\n\nuser\nitem\nrating\n\n\n\n\n1xxxxxxxx2\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx9\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n\n\n\n\u5b9f\u884c\u3059\u308b\u3068\u3001training_data_dir_path\u306b\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n\n\nuser\nitem\nrating\nunique_user_id\nunique_item_id\n\n\n\n\n1xxxxxxxx7\n3xxxxxxxx3\n1\n57704\n32419\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n115460\n32419\n\n\n1xxxxxxxx6\n3xxxxxxxx3\n1\n48853\n32419\n\n\n\n\n\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u4fdd\u5b58\u3057\u307e\u3059\u3002\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"create collaborative filtering model.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\n\nclass CreateCfModel(object):\n    \"\"\"create collaborative filtering model.\"\"\"\n\n    def run(\n        self,\n        processed_training_data_file_path: str,\n        model_dir_path: str,\n        rank: int,\n        max_iter: int,\n        implicit_prefs: str,\n        alpha: float,\n        num_user_blocks: int,\n        num_item_blocks: int\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('create_cf_model')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # create model\n        als = ALS(\n            rank=int(rank),\n            maxIter=int(max_iter),\n            implicitPrefs=bool(implicit_prefs),\n            alpha=float(alpha),\n            numUserBlocks=int(num_user_blocks),\n            numItemBlocks=int(num_item_blocks),\n            userCol='unique_user_id',\n            itemCol='unique_item_id'\n        )\n\n        # load training data\n        custom_schema = StructType([\n            StructField('user', StringType(), True),\n            StructField('item', StringType(), True),\n            StructField('rating', FloatType(), True),\n            StructField('unique_user_id', IntegerType(), True),\n            StructField('unique_item_id', IntegerType(), True),\n        ])\n        df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(processed_training_data_file_path, schema=custom_schema)\n\n        # fitting\n        model = als.fit(df)\n\n        # save\n        saved_data_dir_path = model_dir_path + 'als_model'\n        model.write().overwrite().save(saved_data_dir_path)\n\n        return True\n\n\u524d\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3059\u5fc5\u8981\u304c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001# create model\u3001# load training data\u306e\u7b87\u6240\u3092CSV\u30d5\u30a1\u30a4\u30eb\u306e\u30ab\u30e9\u30e0\u540d\u306b\u5408\u308f\u305b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n# create model\nals = ALS(\n    rank=int(rank),\n    maxIter=int(max_iter),\n    implicitPrefs=bool(implicit_prefs),\n    alpha=float(alpha),\n    numUserBlocks=int(num_user_blocks),\n    numItemBlocks=int(num_item_blocks),\n    userCol='user',\n    itemCol='item'\n)\n\n# load training data\ncustom_schema = StructType([\n    StructField('user', IntegerType(), True),\n    StructField('item', IntegerType(), True),\n    StructField('rating', FloatType(), True),\n])\n\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\nccm = CreateCfModel()\nccm.run(\n    processed_training_data_file_path,\n    model_dir_path,\n    rank,\n    max_iter,\n    implicit_prefs,\n    alpha,\n    num_user_blocks,\n    num_item_blocks\n)\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002rank\u3001max_iter\u3001implicit_prefs\u3001alpha\u3001num_user_blocks\u3001num_item_blocks\u306fPySpark\u306eALS\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u306a\u308a\u307e\u3059\u3002\n\nprocessed_training_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\nmodel_dir_path : \u30e2\u30c7\u30eb\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\nrank : Marix Factorization\u306e\u30e9\u30f3\u30af\nmax_iter : \u6700\u5927\u53cd\u5fa9\u56de\u6570\nimplicit_prefs : \u6f5c\u5728\u7684\u9078\u597d\nalpha : \u30a2\u30eb\u30d5\u30a1\u5024\nnum_user_blocks : \u30e6\u30fc\u30b6\u30fc\u30d6\u30ed\u30c3\u30af\u6570\nnum_item_blocks : \u30a2\u30a4\u30c6\u30e0\u30d6\u30ed\u30c3\u30af\u6570\n\n\u524d\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u305f\u5834\u5408\u3001processed_training_data_file_path\u306b\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n\n\nuser\nitem\nrating\nunique_user_id\nunique_item_id\n\n\n\n\n1xxxxxxxx7\n3xxxxxxxx3\n1\n57704\n32419\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n115460\n32419\n\n\n1xxxxxxxx6\n3xxxxxxxx3\n1\n48853\n32419\n\n\n\n\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n\n\nuser\nitem\nrating\n\n\n\n\n1xxxxxxxx2\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx9\n3xxxxxxxx5\n1\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n\n\n\n\n\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3059\u308b\n\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u3001\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u7d44\u307f\u5408\u308f\u305b\u304b\u3089\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u306f\u5225\u306b\u3001\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u305d\u308c\u305e\u308c\u306e\u4e00\u89a7\u30c7\u30fc\u30bf\u3092\u6e96\u5099\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e88\u6e2c\u7d50\u679c\u306f\u300c\u5168\u7d50\u679c\u300d\u3068\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u3054\u3068\u306b\u30b9\u30b3\u30a2\u4e0a\u4f4dN\u4ef6\u306e\u7d50\u679c\u300d\u3092CSV\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306f\u5272\u308a\u5207\u3063\u3066\u3001\u5143\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5b58\u5728\u3057\u306a\u3044\uff08 = \u884c\u52d5\u30ed\u30b0\u304c\u5b58\u5728\u3057\u306a\u3044\uff09\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306f\u4e88\u6e2c\u5bfe\u8c61\u3068\u3057\u307e\u305b\u3093\u3002\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"predict score from collaborative filtering model.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.ml.recommendation import ALSModel\nfrom pyspark.sql import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\n\nclass CreatePredictedScore(object):\n    \"\"\"predict score from collaborative filtering model.\"\"\"\n\n    def run(\n        self,\n        model_file_path: str,\n        predict_data_dir_path: str,\n        user_data_file_path: str,\n        item_data_file_path: str,\n        processed_training_data_file_path: str,\n        data_limit: int=-1\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('create_predicted_score')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # load user data\n        users_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='false')\\\n            .load(user_data_file_path)\n        users_id_rdd = users_df.rdd.map(lambda l: Row(user_id=l[0]))\n        users_id_df = sqlContext.createDataFrame(users_id_rdd)\n\n        # load item data\n        items_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='false')\\\n            .load(item_data_file_path)\n        items_id_rdd = items_df.rdd.map(lambda l: Row(item_id=l[0]))\n        items_id_df = sqlContext.createDataFrame(items_id_rdd)\n\n        # cross join user_id and item_id\n        joined_df = users_id_df.join(items_id_df)\n        joined_df.cache()\n\n        # delete unnecessary variables\n        del(users_df)\n        del(users_id_rdd)\n        del(users_id_df)\n        del(items_df)\n        del(items_id_rdd)\n        del(items_id_df)\n\n        # load training data\n        custom_schema = StructType([\n            StructField('user', StringType(), True),\n            StructField('item', StringType(), True),\n            StructField('rating', FloatType(), True),\n            StructField('unique_user_id', IntegerType(), True),\n            StructField('unique_item_id', IntegerType(), True),\n        ])\n        training_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(processed_training_data_file_path, schema=custom_schema)\n        # users\n        unique_users_rdd = training_df.rdd.map(lambda l: [l[0], l[3]])\n        unique_users_df = sqlContext.createDataFrame(\n            unique_users_rdd,\n            ('user', 'unique_user_id')\n        ).dropDuplicates()\n        unique_users_df.cache()\n        # items\n        unique_items_rdd = training_df.rdd.map(lambda l: [l[1], l[4]])\n        unique_items_df = sqlContext.createDataFrame(\n            unique_items_rdd,\n            ('item', 'unique_item_id')\n        ).dropDuplicates()\n        unique_items_df.cache()\n\n        # delete unnecessary variables\n        del(training_df)\n        del(unique_users_rdd)\n        del(unique_items_rdd)\n\n        # add unique user id\n        joined_df = joined_df.join(\n            unique_users_df,\n            joined_df['user_id'] == unique_users_df['user'],\n            'inner'\n        ).drop(unique_users_df['user'])\n\n        # add unique item id\n        joined_df = joined_df.join(\n            unique_items_df,\n            joined_df['item_id'] == unique_items_df['item'],\n            'inner'\n        ).drop(unique_items_df['item'])\n\n        # load model\n        model = ALSModel.load(model_file_path)\n\n        # predict score\n        predictions = model.transform(joined_df)\n        all_predict_data = predictions\\\n            .select('user_id', 'item_id', 'prediction')\\\n            .filter('prediction > 0')\n\n        # save\n        # all score\n        saved_data_file_path = predict_data_dir_path + 'als_predict_data_all.csv'\n        all_predict_data.write\\\n            .format('csv')\\\n            .mode('overwrite')\\\n            .options(header='true')\\\n            .save(saved_data_file_path)\n\n        # limited score\n        data_limit = int(data_limit)\n        if data_limit > 0:\n            all_predict_data.registerTempTable('predictions')\n            sql = 'SELECT user_id, item_id, prediction ' \\\n                + 'FROM ( ' \\\n                + '  SELECT user_id, item_id, prediction, dense_rank() ' \\\n                + '  OVER (PARTITION BY user_id ORDER BY prediction DESC) AS rank ' \\\n                + '  FROM predictions ' \\\n                + ') tmp WHERE rank <= %d' % (data_limit)\n            limited_predict_data = sqlContext.sql(sql)\n\n            saved_data_file_path = predict_data_dir_path + 'als_predict_data_limit.csv'\n            limited_predict_data.write\\\n                .format('csv')\\\n                .mode('overwrite')\\\n                .options(header='true')\\\n                .save(saved_data_file_path)\n\n        return True\n\n\u4e88\u6e2c\u5bfe\u8c61\u306e\u7d44\u307f\u5408\u308f\u305b\u306f\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u5168\u7d44\u307f\u5408\u308f\u305b\u3092\u3001\u305d\u308c\u305e\u308c\u306e\u4e00\u89a7\u30c7\u30fc\u30bf\u304b\u3089\u4f5c\u6210\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u30011.\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305fID\u3092\u8ffd\u52a0\u3001\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u51fa\u6765\u306a\u304b\u3063\u305f\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306f\u524a\u9664\n\n\u6700\u521d\u306e\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3059\u5fc5\u8981\u304c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001run\u30e1\u30bd\u30c3\u30c9\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\ndef run(\n    self,\n    model_file_path: str,\n    predict_data_dir_path: str,\n    processed_training_data_file_path: str,\n    data_limit: int=-1\n) -> bool:\n    \"\"\"execute.\"\"\"\n    # make spark context\n    spark = SparkSession\\\n        .builder\\\n        .appName('create_predicted_score')\\\n        .config('spark.sql.crossJoin.enabled', 'true')\\\n        .config('spark.debug.maxToStringFields', 500)\\\n        .getOrCreate()\n    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n    # load training data\n    custom_schema = StructType([\n        StructField('user', IntegerType(), True),\n        StructField('item', IntegerType(), True),\n        StructField('rating', FloatType(), True),\n    ])\n    training_df = sqlContext\\\n        .read\\\n        .format('csv')\\\n        .options(header='true')\\\n        .load(processed_training_data_file_path, schema=custom_schema)\n\n    # load user data\n    users_id_rdd = training_df.rdd.map(lambda l: Row(user_id=l[0]))\n    users_id_df = sqlContext.createDataFrame(users_id_rdd)\n\n    # load item data\n    items_id_rdd = training_df.rdd.map(lambda l: Row(item_id=l[1]))\n    items_id_df = sqlContext.createDataFrame(items_id_rdd)\n\n    # cross join user_id and item_id\n    joined_df = users_id_df.join(items_id_df)\n    joined_df.cache()\n\n    # delete unnecessary variables\n    del(training_df)\n    del(users_id_rdd)\n    del(users_id_df)\n    del(items_id_rdd)\n    del(items_id_df)\n\n    # load model\n    model = ALSModel.load(model_file_path)\n    \uff08\u4ee5\u4e0b\u540c\u3058\uff09\n\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\ncps = CreatePredictedScore()\ncps.run(\n    model_file_path,\n    predict_data_dir_path,\n    user_data_file_path,\n    item_data_file_path,\n    processed_training_data_file_path,\n    data_limit\n)\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002data_limit\u3067\u30b9\u30b3\u30a2\u4e0a\u4f4dN\u4ef6\u306eN\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u30020\u4ee5\u4e0b\u3092\u6307\u5b9a\u3057\u305f\u5834\u5408\u3001\u4e0a\u4f4dN\u4ef6\u306e\u30c7\u30fc\u30bf\u306f\u4f5c\u6210\u3057\u307e\u305b\u3093\u3002\n\nmodel_file_path : \u30e2\u30c7\u30eb\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\npredict_data_dir_path : \u4e88\u6e2c\u30c7\u30fc\u30bf\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\nuser_data_file_path : \u30e6\u30fc\u30b6\u30fcID\u306e\u4e00\u89a7\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\uff08\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u5834\u5408\u306e\u307f\uff09\nitem_data_file_path : \u5546\u54c1ID\u306e\u4e00\u89a7\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\uff08\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u5834\u5408\u306e\u307f\uff09\nprocessed_training_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\ndata_limit : \u30e6\u30fc\u30b6\u3054\u3068\u306e\u30c7\u30fc\u30bf\u4fdd\u5b58\u6570\u4e0a\u9650\uff08default: \u4e0a\u9650\u306a\u3057\uff09\n\nuser_data_file_path\u306b\u306f1\u5217\u76ee\u306b\u30e6\u30fc\u30b6\u30fcID\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\u30d8\u30c3\u30c0\u306a\u3057\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\nitem_data_file_path\u306b\u306f1\u5217\u76ee\u306b\u5546\u54c1ID\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u540c\u69d8\u306b\u30d8\u30c3\u30c0\u306a\u3057\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\nprocessed_training_data_file_path\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n\n\nuser\nitem\nrating\nunique_user_id\nunique_item_id\n\n\n\n\n1xxxxxxxx7\n3xxxxxxxx3\n1\n57704\n32419\n\n\n1xxxxxxxx8\n3xxxxxxxx3\n0\n115460\n32419\n\n\n1xxxxxxxx6\n3xxxxxxxx3\n1\n48853\n32419\n\n\n\npredict_data_dir_path\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n\n\nuser_id\nuser_id\nprediction\n\n\n\n\n1xxxxxxxx3\n3xxxxxxxx4\n0.15594198\n\n\n1xxxxxxxx3\n3xxxxxxxx0\n0.19135818\n\n\n1xxxxxxxx3\n3xxxxxxxx8\n0.048197098\n\n\n\nprediction\u304c\u4e88\u6e2c\u5024\u3068\u306a\u308a\u307e\u3059\u3002\n\n\u307e\u3068\u3081\nPySpark\u3067ALS\u3092\u7528\u3044\u305f\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\u30e1\u30bd\u30c3\u30c9\u5206\u3051\u3057\u3066\u306a\u304f\u3001\u8aad\u307f\u8f9b\u304f\u3066\u3059\u3044\u307e\u305b\u3093\u30fb\u30fb\u30fb\u3002\n\u6b21\u56de\u306f\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002\n\n\u53c2\u8003\u30ea\u30f3\u30af\n\n\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\uff08\u30ec\u30b3\u30e1\u30f3\u30c9\uff09\u3092redis\u3068python\u3067\u5b9f\u88c5\u3057\u3066\u307f\u305f - Qiita\nCollaborative Filtering - Spark Documentation\nEvaluation Metrics - RDD-based API - Spark Documentation\nML Tuning - Spark Documentation\n\n\u7686\u3055\u3093\u3053\u3093\u306b\u3061\u306f\u3002@best_not_best\u3067\u3059\u3002\n\u4eca\u56de\u306f\u62c5\u5f53\u3057\u3066\u3044\u308b\u696d\u52d9\u306b\u6cbf\u3063\u305f\u6280\u8853\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n## \u6982\u8981\n\n\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3092\u7528\u3044\u3066\u3001\u3042\u308b\u30e6\u30fc\u30b6\u30fc\u304c\u3042\u308b\u5546\u54c1\u3092\u8cfc\u5165\u3059\u308b\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002\u8a08\u7b97\u91cf\u304c\u591a\u304f\u3001\u5927\u898f\u6a21\u306a\u30c7\u30fc\u30bf\u3060\u3068\u51e6\u7406\u306b\u6642\u9593\u304c\u304b\u304b\u308b\u305f\u3081\u3001PySpark\u3067\u5206\u6563\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n## \u74b0\u5883\n\n* \u30de\u30b7\u30f3/OS\n  * MacBook Pro (Retina, 15-inch, Mid 2014)\n  * OS X Yosemite 10.10.5\n* Python\n  * Python 3.5.2 :: Anaconda 4.1.1 (x86_64)\n* Python\u30d1\u30c3\u30b1\u30fc\u30b8\n  * \u7279\u306b\u306a\u3057\n* Spark\n  * 2.0.0\n\nSpark\u5074\u3067Python 3.x\u3092\u547c\u3073\u51fa\u305b\u308b\u3088\u3046\u3001spark-env.sh\u306b\u4ee5\u4e0b\u3092\u8ffd\u8a18\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```\nexport PYSPARK_PYTHON=python3\nPYSPARK_DRIVER_PYTHON=python3\n```\n\n## \u624b\u9806\n\n1. \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\n1. \uff08\u5fc5\u8981\u306b\u5fdc\u3058\u3066\uff09\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\n1. \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\n1. \u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3059\u308b\n\n## \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\n\n\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u7b49\u304b\u3089\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u3001\u300cuser\u300d\u300citem\u300d\u300crating\u300d\u306e3\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u300c\u904e\u53bb1\u30f6\u6708\u5206\u306e\u5546\u54c1\u8cfc\u5165\u30c7\u30fc\u30bf\u300d\u3092\u7528\u3044\u308b\u5834\u5408\u3001\u300c\u30e6\u30fc\u30b6\u30fcID\u300d\u300c\u5546\u54c1ID\u300d\u300c\u305d\u306e\u5546\u54c1\u3092\u8cfc\u5165\u3057\u305f\u304b\u3069\u3046\u304b\uff08\u672a\u8cfc\u5165:0/\u8cfc\u5165:1\uff09\u300d\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u306b\u306a\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u3001CSV\u30d5\u30a1\u30a4\u30eb\u306e\u4f8b\u3067\u3059\u3002\n\n| user | item | rating |\n|:------:|:------:|--------:|\n|1xxxxxxxx2|3xxxxxxxx5|1|\n|1xxxxxxxx9|3xxxxxxxx5|1|\n|1xxxxxxxx8|3xxxxxxxx3|0|\n\n`rating`\u306e\u540d\u524d\u306e\u901a\u308a\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u5546\u54c1\u306b\u3069\u308c\u3060\u3051\u8a55\u4fa1\u5024\u3092\u4ed8\u3051\u305f\u304b\u3069\u3046\u304b\u300d\u304c\u672c\u6765\u306e\u4f7f\u3044\u65b9\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u4e0a\u8a18\u306e\u901a\u308a\u300c\u5546\u54c1\u3092\u8cfc\u5165\u3057\u305f\u304b\u3069\u3046\u304b\u300d\u3001\u307e\u305f\u306f\u300c\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3057\u305f\u304b\u3069\u3046\u304b\u300d\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u3067\u3082\u5b9f\u88c5\u306f\u53ef\u80fd\u3067\u3059\u3002\u524d\u8005\u306e\u5834\u5408\u306f\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u5546\u54c1\u3092\u8cfc\u5165\u3059\u308b\u30b9\u30b3\u30a2\u306f\u3069\u306e\u304f\u3089\u3044\u304b\u300d\u3001\u5f8c\u8005\u306f\u300c\u30e6\u30fc\u30b6\u30fc\u304c\u305d\u306e\u30da\u30fc\u30b8\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3069\u306e\u304f\u3089\u3044\u304b\u300d\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u306b\u306a\u308a\u307e\u3059\u3002\n\n## \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u52a0\u5de5\u3059\u308b\n\n\u30e6\u30fc\u30b6\u30fcID\u3084\u5546\u54c1ID\u304c`int32`\u306e\u6700\u5927\u5024\uff082,147,483,647\uff09\u307e\u3067\u3057\u304b\u6271\u3048\u306a\u3044\u305f\u3081\u3001\u305d\u308c\u3092\u8d85\u3048\u308bID\u304c\u3042\u308b\u5834\u5408\u306bID\u3092\u6539\u3081\u3066\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u307e\u3059\u3002\u307e\u305f\u6574\u6570\u5024\u3057\u304b\u6271\u3048\u306a\u3044\u306e\u3067\u3001\u6587\u5b57\u5217\u542b\u307e\u308c\u308b\u5834\u5408\u3082\u540c\u69d8\u306b\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u307e\u3059\u3002\nID\u304c\u6574\u6570\u5024\u304b\u3064`int32`\u306e\u6700\u5927\u5024\u3092\u8d85\u3048\u306a\u3044\u5834\u5408\u306f\u3001\u3053\u306e\u5de5\u7a0b\u306f\u98db\u3070\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n```py3\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"processing training data.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\n\nclass ProcessTrainingData(object):\n    \"\"\"get training data from Redshift, and add sequence number to data.\"\"\"\n\n    def __get_action_log(\n        self,\n        sqlContext: SQLContext,\n        unprocessed_data_file_path: str\n    ) -> DataFrame:\n        \"\"\"get data.\"\"\"\n        df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(unprocessed_data_file_path)\n\n        return df\n\n    def run(\n        self,\n        unprocessed_data_file_path: str,\n        training_data_dir_path: str\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('process_training_data')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # get data\n        df = self.__get_action_log(sqlContext, unprocessed_data_file_path)\n\n        # make sequence number of users\n        unique_users_rdd = df.rdd.map(lambda l: l[0]).distinct().zipWithIndex()\n        unique_users_df = sqlContext.createDataFrame(\n            unique_users_rdd,\n            ('user', 'unique_user_id')\n        )\n\n        # make sequence number of items\n        unique_items_rdd = df.rdd.map(lambda l: l[1]).distinct().zipWithIndex()\n        unique_items_df = sqlContext.createDataFrame(\n            unique_items_rdd,\n            ('item', 'unique_item_id')\n        )\n\n        # add sequence number of users, sequence number of items to data\n        df = df.join(\n            unique_users_df,\n            df['user'] == unique_users_df['user'],\n            'inner'\n        ).drop(unique_users_df['user'])\n        df = df.join(\n            unique_items_df,\n            df['item'] == unique_items_df['item'],\n            'inner'\n        ).drop(unique_items_df['item'])\n\n        # save\n        saved_data_file_path = training_data_dir_path + 'cf_training_data.csv'\n        df.write\\\n            .format('csv')\\\n            .mode('overwrite')\\\n            .options(header='true')\\\n            .save(saved_data_file_path)\n\n        return True\n```\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306eCSV\u3092\u8aad\u307f\u8fbc\u307f\u3001`zipWithIndex()`\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u30ab\u30e9\u30e0\u3092\u8ffd\u52a0\u3057\u3001\u5225\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```py3\nptd = ProcessTrainingData()\nptd.run(unprocessed_data_file_path, training_data_dir_path)\n```\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n* unprocessed_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\n* training_data_dir_path : \u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u306e\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\n\n\u524d\u8ff0\u306e\u901a\u308a\u3001`unprocessed_data_file_path`\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n| user | item | rating |\n|:------:|:------:|--------:|\n|1xxxxxxxx2|3xxxxxxxx5|1|\n|1xxxxxxxx9|3xxxxxxxx5|1|\n|1xxxxxxxx8|3xxxxxxxx3|0|\n\n\u5b9f\u884c\u3059\u308b\u3068\u3001`training_data_dir_path`\u306b\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n| user | item | rating | unique_user_id | unique_item_id |\n|:------:|:------:|--------:|----------------:|----------------:|\n|1xxxxxxxx7|3xxxxxxxx3|1|57704|32419|\n|1xxxxxxxx8|3xxxxxxxx3|0|115460|32419|\n|1xxxxxxxx6|3xxxxxxxx3|1|48853|32419|\n\n## \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\n```py3\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"create collaborative filtering model.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\n\nclass CreateCfModel(object):\n    \"\"\"create collaborative filtering model.\"\"\"\n\n    def run(\n        self,\n        processed_training_data_file_path: str,\n        model_dir_path: str,\n        rank: int,\n        max_iter: int,\n        implicit_prefs: str,\n        alpha: float,\n        num_user_blocks: int,\n        num_item_blocks: int\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('create_cf_model')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # create model\n        als = ALS(\n            rank=int(rank),\n            maxIter=int(max_iter),\n            implicitPrefs=bool(implicit_prefs),\n            alpha=float(alpha),\n            numUserBlocks=int(num_user_blocks),\n            numItemBlocks=int(num_item_blocks),\n            userCol='unique_user_id',\n            itemCol='unique_item_id'\n        )\n\n        # load training data\n        custom_schema = StructType([\n            StructField('user', StringType(), True),\n            StructField('item', StringType(), True),\n            StructField('rating', FloatType(), True),\n            StructField('unique_user_id', IntegerType(), True),\n            StructField('unique_item_id', IntegerType(), True),\n        ])\n        df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(processed_training_data_file_path, schema=custom_schema)\n\n        # fitting\n        model = als.fit(df)\n\n        # save\n        saved_data_dir_path = model_dir_path + 'als_model'\n        model.write().overwrite().save(saved_data_dir_path)\n\n        return True\n```\n\n\u524d\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3059\u5fc5\u8981\u304c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001`# create model`\u3001`# load training data`\u306e\u7b87\u6240\u3092CSV\u30d5\u30a1\u30a4\u30eb\u306e\u30ab\u30e9\u30e0\u540d\u306b\u5408\u308f\u305b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n```py3\n# create model\nals = ALS(\n    rank=int(rank),\n    maxIter=int(max_iter),\n    implicitPrefs=bool(implicit_prefs),\n    alpha=float(alpha),\n    numUserBlocks=int(num_user_blocks),\n    numItemBlocks=int(num_item_blocks),\n    userCol='user',\n    itemCol='item'\n)\n\n# load training data\ncustom_schema = StructType([\n    StructField('user', IntegerType(), True),\n    StructField('item', IntegerType(), True),\n    StructField('rating', FloatType(), True),\n])\n```\n\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```py3\nccm = CreateCfModel()\nccm.run(\n    processed_training_data_file_path,\n    model_dir_path,\n    rank,\n    max_iter,\n    implicit_prefs,\n    alpha,\n    num_user_blocks,\n    num_item_blocks\n)\n```\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002`rank`\u3001`max_iter`\u3001`implicit_prefs`\u3001`alpha`\u3001`num_user_blocks`\u3001`num_item_blocks`\u306f[PySpark\u306eALS\u306e\u30d1\u30e9\u30e1\u30fc\u30bf](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS)\u306b\u306a\u308a\u307e\u3059\u3002\n\n* processed_training_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\n* model_dir_path : \u30e2\u30c7\u30eb\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\n* rank : Marix Factorization\u306e\u30e9\u30f3\u30af\n* max_iter : \u6700\u5927\u53cd\u5fa9\u56de\u6570\n* implicit_prefs : \u6f5c\u5728\u7684\u9078\u597d\n* alpha : \u30a2\u30eb\u30d5\u30a1\u5024\n* num_user_blocks : \u30e6\u30fc\u30b6\u30fc\u30d6\u30ed\u30c3\u30af\u6570\n* num_item_blocks : \u30a2\u30a4\u30c6\u30e0\u30d6\u30ed\u30c3\u30af\u6570\n\n\u524d\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u305f\u5834\u5408\u3001`processed_training_data_file_path`\u306b\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n| user | item | rating | unique_user_id | unique_item_id |\n|:------:|:------:|--------:|----------------:|----------------:|\n|1xxxxxxxx7|3xxxxxxxx3|1|57704|32419|\n|1xxxxxxxx8|3xxxxxxxx3|0|115460|32419|\n|1xxxxxxxx6|3xxxxxxxx3|1|48853|32419|\n\n\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n| user | item | rating |\n|:------:|:------:|--------:|\n|1xxxxxxxx2|3xxxxxxxx5|1|\n|1xxxxxxxx9|3xxxxxxxx5|1|\n|1xxxxxxxx8|3xxxxxxxx3|0|\n\n## \u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3059\u308b\n\n\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u3001\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u7d44\u307f\u5408\u308f\u305b\u304b\u3089\u30b9\u30b3\u30a2\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u306f\u5225\u306b\u3001\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u305d\u308c\u305e\u308c\u306e\u4e00\u89a7\u30c7\u30fc\u30bf\u3092\u6e96\u5099\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e88\u6e2c\u7d50\u679c\u306f\u300c\u5168\u7d50\u679c\u300d\u3068\u3001\u300c\u30e6\u30fc\u30b6\u30fc\u3054\u3068\u306b\u30b9\u30b3\u30a2\u4e0a\u4f4dN\u4ef6\u306e\u7d50\u679c\u300d\u3092CSV\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306f\u5272\u308a\u5207\u3063\u3066\u3001\u5143\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5b58\u5728\u3057\u306a\u3044\uff08 = \u884c\u52d5\u30ed\u30b0\u304c\u5b58\u5728\u3057\u306a\u3044\uff09\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306f\u4e88\u6e2c\u5bfe\u8c61\u3068\u3057\u307e\u305b\u3093\u3002\n\n```py3\n#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\"\"\"predict score from collaborative filtering model.\"\"\"\n\nfrom datetime import datetime\nfrom pyspark.ml.recommendation import ALSModel\nfrom pyspark.sql import Row\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\n\nclass CreatePredictedScore(object):\n    \"\"\"predict score from collaborative filtering model.\"\"\"\n\n    def run(\n        self,\n        model_file_path: str,\n        predict_data_dir_path: str,\n        user_data_file_path: str,\n        item_data_file_path: str,\n        processed_training_data_file_path: str,\n        data_limit: int=-1\n    ) -> bool:\n        \"\"\"execute.\"\"\"\n        # make spark context\n        spark = SparkSession\\\n            .builder\\\n            .appName('create_predicted_score')\\\n            .config('spark.sql.crossJoin.enabled', 'true')\\\n            .config('spark.debug.maxToStringFields', 500)\\\n            .getOrCreate()\n        sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n        # load user data\n        users_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='false')\\\n            .load(user_data_file_path)\n        users_id_rdd = users_df.rdd.map(lambda l: Row(user_id=l[0]))\n        users_id_df = sqlContext.createDataFrame(users_id_rdd)\n\n        # load item data\n        items_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='false')\\\n            .load(item_data_file_path)\n        items_id_rdd = items_df.rdd.map(lambda l: Row(item_id=l[0]))\n        items_id_df = sqlContext.createDataFrame(items_id_rdd)\n\n        # cross join user_id and item_id\n        joined_df = users_id_df.join(items_id_df)\n        joined_df.cache()\n\n        # delete unnecessary variables\n        del(users_df)\n        del(users_id_rdd)\n        del(users_id_df)\n        del(items_df)\n        del(items_id_rdd)\n        del(items_id_df)\n\n        # load training data\n        custom_schema = StructType([\n            StructField('user', StringType(), True),\n            StructField('item', StringType(), True),\n            StructField('rating', FloatType(), True),\n            StructField('unique_user_id', IntegerType(), True),\n            StructField('unique_item_id', IntegerType(), True),\n        ])\n        training_df = sqlContext\\\n            .read\\\n            .format('csv')\\\n            .options(header='true')\\\n            .load(processed_training_data_file_path, schema=custom_schema)\n        # users\n        unique_users_rdd = training_df.rdd.map(lambda l: [l[0], l[3]])\n        unique_users_df = sqlContext.createDataFrame(\n            unique_users_rdd,\n            ('user', 'unique_user_id')\n        ).dropDuplicates()\n        unique_users_df.cache()\n        # items\n        unique_items_rdd = training_df.rdd.map(lambda l: [l[1], l[4]])\n        unique_items_df = sqlContext.createDataFrame(\n            unique_items_rdd,\n            ('item', 'unique_item_id')\n        ).dropDuplicates()\n        unique_items_df.cache()\n\n        # delete unnecessary variables\n        del(training_df)\n        del(unique_users_rdd)\n        del(unique_items_rdd)\n\n        # add unique user id\n        joined_df = joined_df.join(\n            unique_users_df,\n            joined_df['user_id'] == unique_users_df['user'],\n            'inner'\n        ).drop(unique_users_df['user'])\n\n        # add unique item id\n        joined_df = joined_df.join(\n            unique_items_df,\n            joined_df['item_id'] == unique_items_df['item'],\n            'inner'\n        ).drop(unique_items_df['item'])\n\n        # load model\n        model = ALSModel.load(model_file_path)\n\n        # predict score\n        predictions = model.transform(joined_df)\n        all_predict_data = predictions\\\n            .select('user_id', 'item_id', 'prediction')\\\n            .filter('prediction > 0')\n\n        # save\n        # all score\n        saved_data_file_path = predict_data_dir_path + 'als_predict_data_all.csv'\n        all_predict_data.write\\\n            .format('csv')\\\n            .mode('overwrite')\\\n            .options(header='true')\\\n            .save(saved_data_file_path)\n\n        # limited score\n        data_limit = int(data_limit)\n        if data_limit > 0:\n            all_predict_data.registerTempTable('predictions')\n            sql = 'SELECT user_id, item_id, prediction ' \\\n                + 'FROM ( ' \\\n                + '  SELECT user_id, item_id, prediction, dense_rank() ' \\\n                + '  OVER (PARTITION BY user_id ORDER BY prediction DESC) AS rank ' \\\n                + '  FROM predictions ' \\\n                + ') tmp WHERE rank <= %d' % (data_limit)\n            limited_predict_data = sqlContext.sql(sql)\n\n            saved_data_file_path = predict_data_dir_path + 'als_predict_data_limit.csv'\n            limited_predict_data.write\\\n                .format('csv')\\\n                .mode('overwrite')\\\n                .options(header='true')\\\n                .save(saved_data_file_path)\n\n        return True\n```\n\n\u4e88\u6e2c\u5bfe\u8c61\u306e\u7d44\u307f\u5408\u308f\u305b\u306f\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n1. \u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306e\u5168\u7d44\u307f\u5408\u308f\u305b\u3092\u3001\u305d\u308c\u305e\u308c\u306e\u4e00\u89a7\u30c7\u30fc\u30bf\u304b\u3089\u4f5c\u6210\n1. \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u30011.\u306e\u7d44\u307f\u5408\u308f\u305b\u306b\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305fID\u3092\u8ffd\u52a0\u3001\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u51fa\u6765\u306a\u304b\u3063\u305f\u30e6\u30fc\u30b6\u30fcID\u3068\u5546\u54c1ID\u306f\u524a\u9664\n\n\u6700\u521d\u306e\u9805\u3067\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3059\u5fc5\u8981\u304c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001`run`\u30e1\u30bd\u30c3\u30c9\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n```py3\ndef run(\n    self,\n    model_file_path: str,\n    predict_data_dir_path: str,\n    processed_training_data_file_path: str,\n    data_limit: int=-1\n) -> bool:\n    \"\"\"execute.\"\"\"\n    # make spark context\n    spark = SparkSession\\\n        .builder\\\n        .appName('create_predicted_score')\\\n        .config('spark.sql.crossJoin.enabled', 'true')\\\n        .config('spark.debug.maxToStringFields', 500)\\\n        .getOrCreate()\n    sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n\n    # load training data\n    custom_schema = StructType([\n        StructField('user', IntegerType(), True),\n        StructField('item', IntegerType(), True),\n        StructField('rating', FloatType(), True),\n    ])\n    training_df = sqlContext\\\n        .read\\\n        .format('csv')\\\n        .options(header='true')\\\n        .load(processed_training_data_file_path, schema=custom_schema)\n\n    # load user data\n    users_id_rdd = training_df.rdd.map(lambda l: Row(user_id=l[0]))\n    users_id_df = sqlContext.createDataFrame(users_id_rdd)\n\n    # load item data\n    items_id_rdd = training_df.rdd.map(lambda l: Row(item_id=l[1]))\n    items_id_df = sqlContext.createDataFrame(items_id_rdd)\n\n    # cross join user_id and item_id\n    joined_df = users_id_df.join(items_id_df)\n    joined_df.cache()\n\n    # delete unnecessary variables\n    del(training_df)\n    del(users_id_rdd)\n    del(users_id_df)\n    del(items_id_rdd)\n    del(items_id_df)\n\n    # load model\n    model = ALSModel.load(model_file_path)\n    \uff08\u4ee5\u4e0b\u540c\u3058\uff09\n```\n\n\u4ee5\u4e0b\u306e\u69d8\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```py3\ncps = CreatePredictedScore()\ncps.run(\n    model_file_path,\n    predict_data_dir_path,\n    user_data_file_path,\n    item_data_file_path,\n    processed_training_data_file_path,\n    data_limit\n)\n```\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002`data_limit`\u3067\u30b9\u30b3\u30a2\u4e0a\u4f4dN\u4ef6\u306eN\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u30020\u4ee5\u4e0b\u3092\u6307\u5b9a\u3057\u305f\u5834\u5408\u3001\u4e0a\u4f4dN\u4ef6\u306e\u30c7\u30fc\u30bf\u306f\u4f5c\u6210\u3057\u307e\u305b\u3093\u3002\n\n* model_file_path : \u30e2\u30c7\u30eb\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\n* predict_data_dir_path : \u4e88\u6e2c\u30c7\u30fc\u30bf\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\n* user_data_file_path : \u30e6\u30fc\u30b6\u30fcID\u306e\u4e00\u89a7\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\uff08\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u5834\u5408\u306e\u307f\uff09\n* item_data_file_path : \u5546\u54c1ID\u306e\u4e00\u89a7\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\uff08\u30ca\u30f3\u30d0\u30ea\u30f3\u30b0\u3057\u76f4\u3057\u305f\u5834\u5408\u306e\u307f\uff09\n* processed_training_data_file_path: \u5b66\u7fd2\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\n* data_limit : \u30e6\u30fc\u30b6\u3054\u3068\u306e\u30c7\u30fc\u30bf\u4fdd\u5b58\u6570\u4e0a\u9650\uff08default: \u4e0a\u9650\u306a\u3057\uff09\n\n`user_data_file_path`\u306b\u306f1\u5217\u76ee\u306b\u30e6\u30fc\u30b6\u30fcID\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\u30d8\u30c3\u30c0\u306a\u3057\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n`item_data_file_path`\u306b\u306f1\u5217\u76ee\u306b\u5546\u54c1ID\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u540c\u69d8\u306b\u30d8\u30c3\u30c0\u306a\u3057\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n`processed_training_data_file_path`\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\n| user | item | rating | unique_user_id | unique_item_id |\n|:------:|:------:|--------:|----------------:|----------------:|\n|1xxxxxxxx7|3xxxxxxxx3|1|57704|32419|\n|1xxxxxxxx8|3xxxxxxxx3|0|115460|32419|\n|1xxxxxxxx6|3xxxxxxxx3|1|48853|32419|\n\n`predict_data_dir_path`\u306b\u306f\u4ee5\u4e0b\u306e\u30ab\u30e9\u30e0\u3092\u6301\u3064CSV\u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n| user_id | user_id | prediction |\n|:---------:|:---------:|:------------|\n|1xxxxxxxx3|3xxxxxxxx4|0.15594198|\n|1xxxxxxxx3|3xxxxxxxx0|0.19135818|\n|1xxxxxxxx3|3xxxxxxxx8|0.048197098|\n\n`prediction`\u304c\u4e88\u6e2c\u5024\u3068\u306a\u308a\u307e\u3059\u3002\n\n## \u307e\u3068\u3081\n\nPySpark\u3067ALS\u3092\u7528\u3044\u305f\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\u30e1\u30bd\u30c3\u30c9\u5206\u3051\u3057\u3066\u306a\u304f\u3001\u8aad\u307f\u8f9b\u304f\u3066\u3059\u3044\u307e\u305b\u3093\u30fb\u30fb\u30fb\u3002\n\u6b21\u56de\u306f\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002\n\n## \u53c2\u8003\u30ea\u30f3\u30af\n\n* [\u5354\u8abf\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\uff08\u30ec\u30b3\u30e1\u30f3\u30c9\uff09\u3092redis\u3068python\u3067\u5b9f\u88c5\u3057\u3066\u307f\u305f - Qiita](http://qiita.com/haminiku/items/cdbf8eb488e0cf6a62fe)\n* [Collaborative Filtering - Spark Documentation](http://spark.apache.org/docs/latest/ml-collaborative-filtering.html)\n* [Evaluation Metrics - RDD-based API - Spark Documentation](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html)\n* [ML Tuning - Spark Documentation](http://spark.apache.org/docs/latest/ml-tuning.html)\n", "tags": ["Python", "Spark", "\u6a5f\u68b0\u5b66\u7fd2"]}