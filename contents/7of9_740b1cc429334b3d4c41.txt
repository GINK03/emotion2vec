{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n\n\nTensorFlow\u3092\u4f7f\u3063\u305f100 input nodes, 100 output nodes\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u691c\u8a0e\u4e2d\u3002\n\"Universal approximation theorem\"\u3068\u3044\u3046\u306e\u304c\u6c17\u306b\u306a\u308b\u3002\nMorgan\u6c0f\u306b\u3088\u308b\u4ee5\u4e0b\u306e\u8a18\u4e8b\u3092\u898b\u3064\u3051\u305f\u3002\nhttps://blog.metaflow.fr/tensorflow-howto-a-universal-approximator-inside-a-neural-net-bb034430b71e#.oqlbq282n\n\u4ee5\u4e0b\u3001\u629c\u7c8b\u3002\n\nBasically, this theorem states that (without all the nitty-gritty details):\n- for any continuous function f defined in R^n \u2026\n- \u2026 you can find a wide enough 1-hidden layer neural net \u2026\n- \u2026 that will approximate f as well as you want on a closed interval\n\n\u610f\u8a33\u300cn\u6b21\u5143\u306eR(\u5b9f\u6570)\u7a7a\u9593\u306b\u3066\u5b9a\u7fa9\u3055\u308c\u308b\u95a2\u6570f\u306f\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\uff11\u5c64\u306ehidden layer\u3092\u6301\u3064neural net\u3067\u518d\u73fe\u3067\u304d\u308b\u300d\n\u305d\u306e\u30b3\u30fc\u30c9\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\nFinally, notice the existence of the Relu function. We could have used a lot of different functions instead of it but it doesn\u2019t really matter for the theorem as long as it is an increasing function: it only matters for the \u201cspeed of learning\u201d.\n\n\u3053\u3053\u3067\u306fReLU\u3092\u4f7f\u3063\u3066\u3044\u3066\u3001\u5b66\u7fd2\u901f\u5ea6\u306b\u5f71\u97ff\u3059\u308b\u3068\u3042\u308b\u3002\n\n\u201cuniversal approximator\u201d (UA).\n\n...\n\nthe sine function\n\n...\nhidden_dim\u3068\u3044\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u3067hidden layer\u306eneurons\u6570\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u30b3\u30fc\u30c9\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\nYET our UA implementation has a huge drawback, we can\u2019t reuse it if the input_dim starts to vary\u2026\n\n...\n2\u3064\u76ee\u306e\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u3066\u3001activation function\u305d\u306e\u3082\u306e\u3092neural network\u3067\u51e6\u7406\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u90e8\u5206\uff08\u4ee5\u4e0b\uff09\u304c\u6c17\u306b\u306a\u3063\u305f\u3002\n\nSo we have a neural network using a neural network as an activation function\n\n...\n\nI\u2019m using the ELU function inside the second UA\n\nELU function\u306f\u672a\u6d88\u5316\u3002\n...\n\nTensorFlow best practice series\n\n\u95a2\u9023\u8a18\u4e8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n```\n\nTensorFlow\u3092\u4f7f\u3063\u305f100 input nodes, 100 output nodes\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u691c\u8a0e\u4e2d\u3002\n\n\"Universal approximation theorem\"\u3068\u3044\u3046\u306e\u304c\u6c17\u306b\u306a\u308b\u3002\n\nMorgan\u6c0f\u306b\u3088\u308b\u4ee5\u4e0b\u306e\u8a18\u4e8b\u3092\u898b\u3064\u3051\u305f\u3002\n\nhttps://blog.metaflow.fr/tensorflow-howto-a-universal-approximator-inside-a-neural-net-bb034430b71e#.oqlbq282n\n\n\u4ee5\u4e0b\u3001\u629c\u7c8b\u3002\n\n> Basically, this theorem states that (without all the nitty-gritty details):\n- for any continuous function f defined in R^n \u2026\n- \u2026 you can find a wide enough 1-hidden layer neural net \u2026\n- \u2026 that will approximate f as well as you want on a closed interval\n\n\u610f\u8a33\u300cn\u6b21\u5143\u306eR(\u5b9f\u6570)\u7a7a\u9593\u306b\u3066\u5b9a\u7fa9\u3055\u308c\u308b\u95a2\u6570f\u306f\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\uff11\u5c64\u306ehidden layer\u3092\u6301\u3064neural net\u3067\u518d\u73fe\u3067\u304d\u308b\u300d\n\n\u305d\u306e\u30b3\u30fc\u30c9\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\n> Finally, notice the existence of the Relu function. We could have used a lot of different functions instead of it but it doesn\u2019t really matter for the theorem as long as it is an increasing function: it only matters for the \u201cspeed of learning\u201d.\n\n\u3053\u3053\u3067\u306fReLU\u3092\u4f7f\u3063\u3066\u3044\u3066\u3001\u5b66\u7fd2\u901f\u5ea6\u306b\u5f71\u97ff\u3059\u308b\u3068\u3042\u308b\u3002\n\n> \u201cuniversal approximator\u201d (UA).\n\n...\n\n> the sine function\n\n...\n\nhidden_dim\u3068\u3044\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u3067hidden layer\u306eneurons\u6570\u3092\u8a2d\u5b9a\u3067\u304d\u308b\u30b3\u30fc\u30c9\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\n> YET our UA implementation has a huge drawback, we can\u2019t reuse it if the input_dim starts to vary\u2026\n\n...\n\n2\u3064\u76ee\u306e\u4f8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u3066\u3001activation function\u305d\u306e\u3082\u306e\u3092neural network\u3067\u51e6\u7406\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u90e8\u5206\uff08\u4ee5\u4e0b\uff09\u304c\u6c17\u306b\u306a\u3063\u305f\u3002\n\n> So we have a neural network using a neural network as an activation function\n\n...\n\n> I\u2019m using the ELU function inside the second UA\n\nELU function\u306f\u672a\u6d88\u5316\u3002\n\n...\n\n> TensorFlow best practice series\n\n\u95a2\u9023\u8a18\u4e8b\u3082\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\n\n\n\n\n", "tags": ["TensorFlow", "link", "borgWarp"]}