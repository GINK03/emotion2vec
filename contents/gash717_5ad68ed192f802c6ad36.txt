{"context": " More than 1 year has passed since last update.\u4ee5\u524d\u306e\u8a18\u4e8b\u3067\u306f\u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u4f7f\u3063\u305f\u304c,\n\u4eca\u56de\u306f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046.\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046\u3053\u3068\u306b\u3088\u308a, \u3069\u3053\u306b\u5206\u985e\u3059\u308b\u304b\u3068\u3044\u3046\u306e\u3092\u78ba\u7387\u3092\u7528\u3044\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b.\n\u524d\u56de\u3068\u540c\u69d8, Python Machine Learning\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u308b.\n\n\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\niris = datasets.load_iris()\nprint(iris)\nX = iris.data[:, [2, 3]] # iris\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7b2c3, 4\u30ab\u30e9\u30e0\ny = iris.target # iris\u306e\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272, \u4eca\u306ftest_size=0.3\u306a\u306e\u3067\u8a13\u7df4\u30c7\u30fc\u30bf7\u5272\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf3\u5272\u306b\u3057\u3066\u3044\u308b\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nsc = StandardScaler()\nsc.fit(X_train)\n# \u5e73\u57470, \u5206\u65631\u306b\u6a19\u6e96\u5316\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\nX_combined_std = np.vstack((X_train_std, X_test_std)) # \u7e26\u306b\u9023\u7d50\ny_combined = np.hstack((y_train, y_test)) # \u6a2a\u306b\u9023\u7d50\n\n\u307e\u305f, \u30b0\u30e9\u30d5\u3092\u63cf\u753b\u3059\u308b\u95a2\u6570\u3092\u4f5c\u3063\u3066\u304a\u304f.\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    #setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    # \u6700\u5c0f\u5024, \u6700\u5927\u5024\u304b\u3089\u30a8\u30ea\u30a2\u306e\u9818\u57df\u3092\u5272\u308a\u51fa\u3059\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    # resolution\u306e\u9593\u9694\u3067\u533a\u5207\u3063\u305f\u9818\u57df\u3092\u5b9a\u7fa9\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                            np.arange(x2_min, x2_max, resolution))\n    # print(xx1.shape)\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot all samples\n    X_test, y_test = X[test_idx, :], y[test_idx]\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='',\n            alpha=1.0, linewidth=1, marker='o',\n            s=55, label='test set')\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067scikit-learn\u304b\u3089\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046.\n### \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=1000.0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X=X_combined_std,\n                        y=y_combined,\n                        classifier=lr,\n                        test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n\n\u3053\u308c\u306b\u3088\u308a\u4ee5\u4e0b\u306e\u30b0\u30e9\u30d5\u304c\u8868\u793a\u3055\u308c\u308b.\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u3069\u306e\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u3068\u3044\u3046\u3053\u3068\u3092\u78ba\u7387\u3067\u8868\u3059\u3053\u3068\u304c\u3067\u304d,\n\u3053\u308c\u306fLogisticRegression\u306epredict_proba\u30e1\u30bd\u30c3\u30c9\u3092\u7528\u3044\u308b\u3068\u8868\u793a\u3067\u304d\u308b.\nIn [37]: lr.predict_proba(X_test_std[3,:])\nOut[37]: array([[  1.60866539e-11,   4.14084479e-01,   5.85915521e-01]])\n\n\n\u53c2\u8003\nPython Machine Learning\n[\u4ee5\u524d\u306e\u8a18\u4e8b](http://qiita.com/gash717/items/cd9b97a9d26f6ec90df3)\u3067\u306f\u5358\u7d14\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3092\u4f7f\u3063\u305f\u304c,\n\u4eca\u56de\u306f\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046.\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046\u3053\u3068\u306b\u3088\u308a, \u3069\u3053\u306b\u5206\u985e\u3059\u308b\u304b\u3068\u3044\u3046\u306e\u3092\u78ba\u7387\u3092\u7528\u3044\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b.\n\u524d\u56de\u3068\u540c\u69d8, [Python Machine Learning](https://github.com/rasbt/python-machine-learning-book)\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u308b.\n\n### \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\n\n```\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\niris = datasets.load_iris()\nprint(iris)\nX = iris.data[:, [2, 3]] # iris\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u7b2c3, 4\u30ab\u30e9\u30e0\ny = iris.target # iris\u306e\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3054\u3068\u306e\u30e9\u30d9\u30eb\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272, \u4eca\u306ftest_size=0.3\u306a\u306e\u3067\u8a13\u7df4\u30c7\u30fc\u30bf7\u5272\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf3\u5272\u306b\u3057\u3066\u3044\u308b\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nsc = StandardScaler()\nsc.fit(X_train)\n# \u5e73\u57470, \u5206\u65631\u306b\u6a19\u6e96\u5316\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n\nX_combined_std = np.vstack((X_train_std, X_test_std)) # \u7e26\u306b\u9023\u7d50\ny_combined = np.hstack((y_train, y_test)) # \u6a2a\u306b\u9023\u7d50\n```\n\n\u307e\u305f, \u30b0\u30e9\u30d5\u3092\u63cf\u753b\u3059\u308b\u95a2\u6570\u3092\u4f5c\u3063\u3066\u304a\u304f.\n\n```\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    #setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    # \u6700\u5c0f\u5024, \u6700\u5927\u5024\u304b\u3089\u30a8\u30ea\u30a2\u306e\u9818\u57df\u3092\u5272\u308a\u51fa\u3059\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    # resolution\u306e\u9593\u9694\u3067\u533a\u5207\u3063\u305f\u9818\u57df\u3092\u5b9a\u7fa9\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                            np.arange(x2_min, x2_max, resolution))\n    # print(xx1.shape)\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot all samples\n    X_test, y_test = X[test_idx, :], y[test_idx]\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        plt.scatter(X_test[:, 0], X_test[:, 1], c='',\n            alpha=1.0, linewidth=1, marker='o',\n            s=55, label='test set')\n```\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067scikit-learn\u304b\u3089\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u4f7f\u3046.\n\n```\n### \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=1000.0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X=X_combined_std,\n                        y=y_combined,\n                        classifier=lr,\n                        test_idx=range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n\u3053\u308c\u306b\u3088\u308a\u4ee5\u4e0b\u306e\u30b0\u30e9\u30d5\u304c\u8868\u793a\u3055\u308c\u308b.\n![logistic_regression.png](https://qiita-image-store.s3.amazonaws.com/0/70855/029658f8-3489-b6d4-faea-a3e53efd7714.png)\n\n\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u3069\u306e\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u3068\u3044\u3046\u3053\u3068\u3092\u78ba\u7387\u3067\u8868\u3059\u3053\u3068\u304c\u3067\u304d,\n\u3053\u308c\u306fLogisticRegression\u306epredict_proba\u30e1\u30bd\u30c3\u30c9\u3092\u7528\u3044\u308b\u3068\u8868\u793a\u3067\u304d\u308b.\n\n```\nIn [37]: lr.predict_proba(X_test_std[3,:])\nOut[37]: array([[  1.60866539e-11,   4.14084479e-01,   5.85915521e-01]])\n```\n\n## \u53c2\u8003\n[Python Machine Learning](https://github.com/rasbt/python-machine-learning-book)\n", "tags": ["Python", "scikit-learn"]}