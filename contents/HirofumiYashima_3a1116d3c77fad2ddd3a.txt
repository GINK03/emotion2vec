{"context": "\n\n\uff08 \u4f5c\u6210\u4e2d \uff09\n\n\u4f7f\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3046\u306e \u3067\u3001\u60c5\u5831\u96c6\u7d04\u4e2d\u3002\n\nXingjian Shi, Zhourong Chen, Hao Wang and Dit-Yan Yeung, Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting\nmasataka46\u3055\u3093 Qiita\u8a18\u4e8b \u300cLSTM\u3092\u6539\u826f\u3057\u3066convLSTM\u306b\u3059\u308b\u300d\n\n\nconvLSTM\u306b\u3064\u3044\u3066\nconvLSTM\u306fX. Shi\u3089\u304c\u63d0\u6848\u3057\u3066\u3044\u308bconvolution\u3068LSTM\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u624b\u6cd5\u3067\u3042\u308b\u3002\nhttps://arxiv.org/pdf/1506.04214.pdf\n\u901a\u5e38\u306eLSTM\u306flinear\u306a\u72b6\u614b\u3067\u51e6\u7406\u3055\u308c\u308b\u306e\u3067\u3001\u753b\u50cf\u306a\u3069\u306f\u4f4d\u7f6e\u7684\u306a\u60c5\u5831\u304c\u6b7b\u3093\u3067\u3057\u307e\u3046\u3002\nconvLSTM\u306f\u753b\u50cf\u306e\u72b6\u614b\u3092\u7dad\u6301\u3057\u305f\u307e\u307e\u5165\u529b\u3059\u308b\u306e\u3067\u4f4d\u7f6e\u60c5\u5831\u304c\u4fdd\u6301\u3055\u308c\u308b\u3002\nlinear\u306e\u5834\u5408\u306e\u884c\u5217\u6f14\u7b97\u306fconvolution\u306b\u7f6e\u304d\u63db\u3048\u308c\u3089\u308c\u308b\u3002\n\u3053\u308c\u306b\u3088\u308aLSTM\u306e\u6642\u9593\u60c5\u5831\u3001convolution\u306e\u4f4d\u7f6e\u60c5\u5831\u304c\u540c\u6642\u306b\u3044\u304b\u3055\u308c\u308b\u3002\n\n\n\u3010 \u5b9f\u88c5\u4e8b\u4f8b \u3011\n\n\nchainer\n\n\n\uff08 GitHub \uff09masataka46/convLSTM_minimum\n\n\ntensorflow\n\n\n\uff08 GitHub \uff09loliverhennigh/Convolutional-LSTM-in-Tensorflow\n\n\nAn implementation of convolutional lstms in tensorflow. \nThe code is written in the same style as the basiclstmcell function in tensorflow\n\n\nkeras\n\n\nfchollet/keras Convolutional LSTM #1773\n\n\nI think you will also be interested by #443. \nCurrently, keras-extra does not implement a convolutional LSTM. It provides tools to process sequences of 2D images.\n\n\n\nkeras-extra\n\n\n\uff08GitHub\uff09anayebi/keras-extra Extra Layers for Keras to connect CNN with RNN\n\n\nThese layers allow you to connect a Convolutional Neural Network (CNN) with a Recurrent Neural Network (RNN) of your choice by allowing the CNN layers to be time distributed.\nFor an example of using these layers, see here: https://github.com/jamesmf/mnistCRNN\n\n\njamesmf/mnistCRNN Simple Recurrent Convolutional Neural Network in Keras adding MNIST digits\n\n\n\nKeras ConvLSTM Docker\u30a4\u30e1\u30fc\u30b8\n\n\nkunimasa-kawasaki/keras-convlstm-docker Docker image for running Keras with ConvLSTM\n\n\n\n\nimport numpy as np\nfrom keras.models import Sequential,Graph\nfrom keras.layers.convolutional import Convolution2D,Convolution3D\nfrom keras.layers.recurrent_convolutional import LSTMConv2D\n\nseq = Sequential()\nseq.add(LSTMConv2D(nb_filter=15, nb_row=3, nb_col=3, input_shape=(10,40,40,1),\n                  border_mode=\"same\",return_sequences=True))\nseq.add(LSTMConv2D(nb_filter=15,nb_row=3, nb_col=3,\n                  border_mode=\"same\", return_sequences=True))\nseq.add(LSTMConv2D(nb_filter=15, nb_row=3, nb_col=3,\n                  border_mode=\"same\", return_sequences=True))\nseq.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n                     kernel_dim3=3, activation='sigmoid',\n                  border_mode=\"same\", dim_ordering=\"tf\"))\n\nseq.compile(loss=\"binary_crossentropy\",optimizer=\"adadelta\")\n\nX_train = np.ones((320, 10,40,40,1))\nY_train = np.ones((320, 10,40,40,1))\nseq.fit(X_train, Y_train, batch_size=32, verbose=1)\n\n\n\n\nfchollet/keras Added recurrent convolution network #1818\n\n\n\n\n\n\nLin Wu, Chunhua Shen and  Anton van den Hengel, Convolutional LSTM Networks for Video-based Person Re-identification\n\n\nGiven sequential video frames of a person, the spatial information encoded in the frames is first extracted by a set of CNNs. \nAn encoder-decoder framework derived from LSTMs is employed to encode the resulting temporal of CNN outputs.\nThis approach leads to a refined feature representation that is able to explicitly model the video as an ordered sequence, while preserving the spatial information.\n\n\nencoder-decoder\u30e2\u30c7\u30eb \u306b \u3064\u3044\u3066 \u306f\u3001\u4ee5\u4e0b \u304c \u5206\u304b\u308a\u3084\u3059\u3044\u3002\n\nodashi_t\u3055\u3093 Qiita\u8a18\u4e8b \u300cChainer\u3068RNN\u3068\u6a5f\u68b0\u7ffb\u8a33\u300d\n\u83ca\u6c60\u60a0\u592a\u307b\u304b \u300cEncoder-Decoder \u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u51fa\u529b\u9577\u5236\u5fa1\u300d\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a Vol.2016-NL-227 No.5 2016/7/29\n\nseq2seq\u53c2\u8003\n\nHatena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3 \uff082016-02-17\uff09 \u300cSeq2Seq\u30e1\u30e2\u300d\n329\u3055\u3093 Qiita\u8a18\u4e8b \u300cChainer\u3067\u30b7\u30f3\u30d7\u30eb\u306a\u718a\u672c\u5f01\u7ffb\u8a33\u300d\nkenchin110100\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u4eca\u66f4\u306a\u304c\u3089chainer\u3067Seq2Seq\uff081\uff09\u300d \nkenchin110100\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u4eca\u66f4\u306a\u304c\u3089chainer\u3067Seq2Seq\uff082\uff09\u301cAttention Model\u7de8\u301c\u300d\n\uff08 GitHub \uff09 harvardnlp/seq2seq-attn Sequence-to-sequence model with LSTM encoder/decoders and attention\n\uff08 SlideShare\uff09Yuta Kikuchi \u300c\u6700\u8fd1\u306eDeep Learning (NLP) \u754c\u9688\u306b\u304a\u3051\u308bAttention\u4e8b\u60c5\u300d\n\u677e\u5c3e\u6620\u91cc\u307b\u304b \u300c\u6df1\u5c64\u5b66\u7fd2\u306b\u3088\u308b\u753b\u50cf\u8aac\u660e\u6587\u751f\u6210\u624b\u6cd5\u306e\u8133\u6d3b\u52d5\u30c7\u30fc\u30bf\u3078\u306e\u9069\u7528\u300d\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a \u7b2c22\u56de\u5e74\u6b21\u5927\u4f1a \u767a\u8868\u8ad6\u6587\u96c6 (2016\u5e743\u6708)\n\n\uff08 \u6bd4\u8f03 \uff09\n\nOriol Vinyals, Alexander Toshev and Samy Bengio Show and Tell: A Neural Image Caption Generator\n\n\n\nRevolutions (September 26, 2016) Deep Learning Part 3: Combining Deep Convolutional Neural Network with Recurrent Neural Network\nOrd\u00f3\u00f1ez FJ and Roggen D, Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition. \n\n##__\uff08 \u4f5c\u6210\u4e2d \uff09__\n\n\u4f7f\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3046\u306e \u3067\u3001\u60c5\u5831\u96c6\u7d04\u4e2d\u3002\n\n* [Xingjian Shi, Zhourong Chen, Hao Wang and Dit-Yan Yeung, _Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting_](https://arxiv.org/pdf/1506.04214.pdf)\n\n* [masataka46\u3055\u3093 Qiita\u8a18\u4e8b \u300cLSTM\u3092\u6539\u826f\u3057\u3066convLSTM\u306b\u3059\u308b\u300d](http://qiita.com/masataka46/items/0c77fef1e48446c7d329)\n\n> __convLSTM\u306b\u3064\u3044\u3066__\n>\n> convLSTM\u306fX. Shi\u3089\u304c\u63d0\u6848\u3057\u3066\u3044\u308bconvolution\u3068LSTM\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u624b\u6cd5\u3067\u3042\u308b\u3002\n> https://arxiv.org/pdf/1506.04214.pdf\n>\n> \u901a\u5e38\u306eLSTM\u306flinear\u306a\u72b6\u614b\u3067\u51e6\u7406\u3055\u308c\u308b\u306e\u3067\u3001\u753b\u50cf\u306a\u3069\u306f\u4f4d\u7f6e\u7684\u306a\u60c5\u5831\u304c\u6b7b\u3093\u3067\u3057\u307e\u3046\u3002\n>\n> convLSTM\u306f\u753b\u50cf\u306e\u72b6\u614b\u3092\u7dad\u6301\u3057\u305f\u307e\u307e\u5165\u529b\u3059\u308b\u306e\u3067\u4f4d\u7f6e\u60c5\u5831\u304c\u4fdd\u6301\u3055\u308c\u308b\u3002\n>\n> linear\u306e\u5834\u5408\u306e\u884c\u5217\u6f14\u7b97\u306fconvolution\u306b\u7f6e\u304d\u63db\u3048\u308c\u3089\u308c\u308b\u3002\n>\n> \u3053\u308c\u306b\u3088\u308aLSTM\u306e\u6642\u9593\u60c5\u5831\u3001convolution\u306e\u4f4d\u7f6e\u60c5\u5831\u304c\u540c\u6642\u306b\u3044\u304b\u3055\u308c\u308b\u3002\n\n###__\u3010 \u5b9f\u88c5\u4e8b\u4f8b \u3011__\n\n####__chainer__\n\n* [\uff08 GitHub \uff09masataka46/convLSTM_minimum](https://github.com/masataka46/convLSTM_minimum)\n\n####__tensorflow__\n\n* [\uff08 GitHub \uff09loliverhennigh/Convolutional-LSTM-in-Tensorflow](https://github.com/loliverhennigh/Convolutional-LSTM-in-Tensorflow)\n\n> An implementation of convolutional lstms in tensorflow. \n>\n> The code is written in the same style as the basiclstmcell function in tensorflow\n\n####__keras__\n\n* [fchollet/keras Convolutional LSTM #1773](https://github.com/fchollet/keras/issues/1773)\n\n> I think you will also be interested by #443. \n>\n> Currently, keras-extra does not implement a convolutional LSTM. It provides tools to process sequences of 2D images.\n___\n\n###__keras-extra__\n\n* [\uff08GitHub\uff09anayebi/keras-extra _Extra Layers for Keras to connect CNN with RNN_](https://github.com/anayebi/keras-extra)\n\n> These layers allow you to connect a Convolutional Neural Network (CNN) with a Recurrent Neural Network (RNN) of your choice by allowing the CNN layers to be time distributed.\n>\nFor an example of using these layers, see here: https://github.com/jamesmf/mnistCRNN\n\n* [jamesmf/mnistCRNN _Simple Recurrent Convolutional Neural Network in Keras adding MNIST digits_](https://github.com/jamesmf/mnistCRNN)\n\n___\n\n###__Keras ConvLSTM Docker\u30a4\u30e1\u30fc\u30b8__\n\n* [kunimasa-kawasaki/keras-convlstm-docker _Docker image for running Keras with ConvLSTM_](https://github.com/kunimasa-kawasaki/keras-convlstm-docker)\n\n<img width=\"1245\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 18.21.20.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/35805677-d1b9-1745-9988-250b5f508b61.png\">\n\n<img width=\"1241\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 18.21.31.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/cb8eaa07-c776-c5a4-00f8-cf5b4fee71dc.png\">\n\n>```{python:}\n> import numpy as np\n>from keras.models import Sequential,Graph\n>from keras.layers.convolutional import Convolution2D,Convolution3D\n>from keras.layers.recurrent_convolutional import LSTMConv2D\n>\n>seq = Sequential()\n>seq.add(LSTMConv2D(nb_filter=15, nb_row=3, nb_col=3, input_shape=(10,40,40,1),\n>                   border_mode=\"same\",return_sequences=True))\n>seq.add(LSTMConv2D(nb_filter=15,nb_row=3, nb_col=3,\n>                   border_mode=\"same\", return_sequences=True))\n>seq.add(LSTMConv2D(nb_filter=15, nb_row=3, nb_col=3,\n>                   border_mode=\"same\", return_sequences=True))\n>seq.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n>                      kernel_dim3=3, activation='sigmoid',\n>                   border_mode=\"same\", dim_ordering=\"tf\"))\n>\n>seq.compile(loss=\"binary_crossentropy\",optimizer=\"adadelta\")\n>\n>X_train = np.ones((320, 10,40,40,1))\n>Y_train = np.ones((320, 10,40,40,1))\n>seq.fit(X_train, Y_train, batch_size=32, verbose=1)\n>```\n\n___\n\n\n* [fchollet/keras _Added recurrent convolution network #1818_](https://github.com/fchollet/keras/pull/1818)\n\n<img width=\"1235\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 18.25.31.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/5155c811-28e4-b576-90d2-f44d31e3574b.png\">\n\n<img width=\"1239\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 18.25.39.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/96bec8d3-1292-aa66-1974-40b2e57257a4.png\">\n\n<img width=\"1243\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 18.25.55.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/43846a26-df02-8763-04b3-3538b297b637.png\">\n\n\n\n___\n\n* [Lin Wu, Chunhua Shen and  Anton van den Hengel, _Convolutional LSTM Networks for Video-based Person Re-identification_](https://www.researchgate.net/publication/303821584_Convolutional_LSTM_Networks_for_Video-based_Person_Re-identification)\n\n> Given sequential video frames of a person, the spatial information encoded in the frames is first extracted by a set of CNNs. \n>\n> An encoder-decoder framework derived from LSTMs is employed to encode the resulting temporal of CNN outputs.\n>\n> This approach leads to a refined feature representation that is able to explicitly model the video as an ordered sequence, while preserving the spatial information.\n\n___\n\n__encoder-decoder\u30e2\u30c7\u30eb \u306b \u3064\u3044\u3066 \u306f\u3001\u4ee5\u4e0b \u304c \u5206\u304b\u308a\u3084\u3059\u3044\u3002__\n\n* [odashi_t\u3055\u3093 Qiita\u8a18\u4e8b \u300cChainer\u3068RNN\u3068\u6a5f\u68b0\u7ffb\u8a33\u300d](http://qiita.com/odashi_t/items/a1be7c4964fbea6a116e)\n* [\u83ca\u6c60\u60a0\u592a\u307b\u304b \u300cEncoder-Decoder \u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u51fa\u529b\u9577\u5236\u5fa1\u300d\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a Vol.2016-NL-227 No.5 2016/7/29](http://www.phontron.com/paper/kikuchi16nl07.pdf)\n\n__seq2seq\u53c2\u8003__\n\n* [Hatena Blog chainer\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u304d\u308b\u304b\u30de\u30f3 \uff082016-02-17\uff09 \u300cSeq2Seq\u30e1\u30e2\u300d](http://chainernlpman.hatenablog.com/entry/2016/02/17/023909)\n* [_329_\u3055\u3093 Qiita\u8a18\u4e8b \u300cChainer\u3067\u30b7\u30f3\u30d7\u30eb\u306a\u718a\u672c\u5f01\u7ffb\u8a33\u300d](http://qiita.com/_329_/items/1d5de7b8213b112a3df7)\n* [kenchin110100\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u4eca\u66f4\u306a\u304c\u3089chainer\u3067Seq2Seq\uff081\uff09\u300d ](http://qiita.com/kenchin110100/items/b34f5106d5a211f4c004)\n* [kenchin110100\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u4eca\u66f4\u306a\u304c\u3089chainer\u3067Seq2Seq\uff082\uff09\u301cAttention Model\u7de8\u301c\u300d](http://qiita.com/kenchin110100/items/eb70d69d1d65fb451b67)\n* [\uff08 GitHub \uff09 harvardnlp/seq2seq-attn _Sequence-to-sequence model with LSTM encoder/decoders and attention_](https://github.com/harvardnlp/seq2seq-attn)\n* [\uff08 SlideShare\uff09Yuta Kikuchi \u300c\u6700\u8fd1\u306eDeep Learning (NLP) \u754c\u9688\u306b\u304a\u3051\u308bAttention\u4e8b\u60c5\u300d](https://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention)\n* [\u677e\u5c3e\u6620\u91cc\u307b\u304b \u300c\u6df1\u5c64\u5b66\u7fd2\u306b\u3088\u308b\u753b\u50cf\u8aac\u660e\u6587\u751f\u6210\u624b\u6cd5\u306e\u8133\u6d3b\u52d5\u30c7\u30fc\u30bf\u3078\u306e\u9069\u7528\u300d\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a \u7b2c22\u56de\u5e74\u6b21\u5927\u4f1a \u767a\u8868\u8ad6\u6587\u96c6 (2016\u5e743\u6708)](http://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/P4-2.pdf)\n\n\uff08 \u6bd4\u8f03 \uff09\n\n* [Oriol Vinyals, Alexander Toshev and Samy Bengio _Show and Tell: A Neural Image Caption Generator_](https://arxiv.org/pdf/1411.4555.pdf)\n\n___\n\n\n* [Revolutions (September 26, 2016) _Deep Learning Part 3: Combining Deep Convolutional Neural Network with Recurrent Neural Network_](http://blog.revolutionanalytics.com/2016/09/deep-learning-part-3.html)\n\n* [Ord\u00f3\u00f1ez FJ and Roggen D, _Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition._ ](https://www.ncbi.nlm.nih.gov/pubmed/26797612)\n", "tags": ["DeepLearning", "convLSTM", "\u6642\u7cfb\u5217\u89e3\u6790", "\u753b\u50cf\u89e3\u6790", "\u6df1\u5c64\u5b66\u7fd2"]}