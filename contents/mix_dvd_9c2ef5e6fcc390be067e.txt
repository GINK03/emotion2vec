{"context": "\n\n\u306f\u3058\u3081\u306b\n\u78ba\u5b9a\u7533\u544a\u306e\u6642\u671f\u306b\u306a\u308a\u3001\u4f1a\u8a08\u30c7\u30fc\u30bf\u3092\u518d\u78ba\u8a8d\u3059\u308b\u3068\u52d8\u5b9a\u79d1\u76ee\u306e\u9593\u9055\u3044\u304c\u3061\u3089\u307b\u3089\u898b\u3064\u304b\u308b\u306e\u3067\u3001\u6a5f\u68b0\u5b66\u7fd2\u3067\u4f1a\u8a08\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3001\u52d8\u5b9a\u79d1\u76ee\u3092\u4e88\u6e2c\u3067\u304d\u308b\u3068\u826f\u3044\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3044\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u4f1a\u8a08\u30c7\u30fc\u30bf\u306e\u8aad\u8fbc\n\u4f1a\u8a08\u30bd\u30d5\u30c8\u304b\u3089CSV\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3001\u305d\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u306e\u30c7\u30fc\u30bf\u306f\u300cJDL IBEX\u51fa\u7d0d\u5e33\u300d\u3068\u3044\u3046\u30bd\u30d5\u30c8\u306e\u3082\u306e\u3092\u4f8b\u306b\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\nhttp://www.jdl.co.jp/co/soft/ibex-ab/\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\nimport pandas as pd\n\nfilename = \"JDL\u51fa\u7d0d\u5e33-xxxx-xxxx-\u4ed5\u8a33.csv\"\ndf = pd.read_csv(filename, encoding=\"Shift-JIS\", skiprows=3)\n\n\u8aad\u307f\u8fbc\u3093\u3060\u30c7\u30fc\u30bf\u304b\u3089\u4f7f\u7528\u3059\u308b\u30c7\u30fc\u30bf\u3092\u7d5e\u308a\u8fbc\u3080\u3002\n\u3053\u3053\u3067\u306f\u3001\u6458\u8981\u3068\u501f\u65b9\u79d1\u76ee\u306e\u30b3\u30fc\u30c9\u3068\u540d\u79f0\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\ncolumns = [\"\u6458\u8981\", \"\u501f\u65b9\u79d1\u76ee\", \"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\"]\ndf_counts = df[columns].dropna()\n\n\n\u5f62\u614b\u7d20\u89e3\u6790\n\u6458\u8981\u306e\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u5f62\u614b\u7d20\u89e3\u6790\u306b\u3088\u308a\u3001\u6587\u5b57\u30c7\u30fc\u30bf\u3092\u6307\u5411\u6027\u306e\u3042\u308b\u6570\u5024\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u5316\u3057\u307e\u3059\u3002\n\u5f62\u614b\u7d20\u89e3\u6790\u306b\u306fJanome\u3068\u3044\u3046\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\nhttp://mocobeta.github.io/janome/\n\u3082\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n$ pip install janome\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u3001\u6458\u8981\u306e\u30c7\u30fc\u30bf\u3092\u30c8\u30fc\u30af\u30f3\u306b\u308f\u3051\u305f\u30c7\u30fc\u30bf\u306b\u5909\u63db\u3057\u307e\u3059\u3002\nfrom janome.tokenizer import Tokenizer\n\nt = Tokenizer()\n\nnotes = []\nfor ix in df_counts.index:\n    note = df_counts.ix[ix,\"\u6458\u8981\"]\n    tokens = t.tokenize(note.replace('\u3000',' '))\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words.replace(' \\u3000', ''))\n\n\u3053\u306e\u7d50\u679c\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5909\u63db\u304c\u884c\u308f\u308c\u3001\u5358\u8a9e\u6bce\u306b\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u305f\u6587\u5b57\u5217\u306b\u306a\u308a\u307e\u3059\u3002\n\u5143\u306e\u6458\u8981\u30c7\u30fc\u30bf\u3000\u300c\u624b\u571f\u7523\u4ee3\u3000BLUESKY\u7fbd\u7530\u300d\n\u5909\u5316\u5f8c\u306e\u30c7\u30fc\u30bf\u3000\u300c\u624b\u571f\u7523 \u4ee3 BLUESKY \u7fbd\u7530\u300d\n\u3053\u306e\u6587\u5b57\u5217\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer()\nvect.fit(notes)\n\nX = vect.transform(notes)\n\n\u307e\u305f\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u52d8\u5b9a\u79d1\u76ee\u306e\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\ny = df_counts.\u501f\u65b9\u79d1\u76ee.as_matrix().astype(\"int\").flatten()\n\n\n\u6a5f\u68b0\u5b66\u7fd2\n\u6570\u5024\u306b\u5909\u63db\u3057\u305f\u30c7\u30fc\u30bf\u3092\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u307e\u3059\u3002\nfrom sklearn import cross_validation\n\ntest_size = 0.2\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size)\n\n\u5206\u5272\u3057\u305f\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3057\u307e\u3059\u3002\n\u3053\u3053\u3067\u306f\u30e2\u30c7\u30eb\u3068\u3057\u3066LinearSVC\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\nfrom sklearn.svm import LinearSVC\n\nclf = LinearSVC(C=120.0, random_state=42)\nclf.fit(X_train, y_train)\n\nclf.score(X_test, y_test)\n\n\u30b9\u30b3\u30a2\u306f\u300c0.89932885906040272\u300d\u3067\u3057\u305f\u3002\n\n\u4e88\u6e2c\n\u5b66\u7fd2\u3057\u305f\u7d50\u679c\u304b\u3089\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u3001\u3069\u3093\u306a\u52d8\u5b9a\u79d1\u76ee\u304c\u4e88\u6e2c\u3055\u308c\u308b\u304b\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\ntests = [\n    \"\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599\",\n    \"\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3\",\n    \"\u5207\u624b\u4ee3\"\n]\n\nnotes = []\nfor note in tests:\n    tokens = t.tokenize(note)\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words)\n\nX = vect.transform(notes)\n\nresult = clf.predict(X)\n\ndf_rs = df_counts[[\"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\", \"\u501f\u65b9\u79d1\u76ee\"]]\ndf_rs.index = df_counts[\"\u501f\u65b9\u79d1\u76ee\"].astype(\"int\")\ndf_rs = df_rs[~df_rs.index.duplicated()][\"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\"]\n\nfor i in range(len(tests)):\n    print(tests[i], \"\\t[\",df_rs.ix[result[i]], \"]\")\n\n\u51fa\u529b\u7d50\u679c\u306f...\n\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599   [ \u65c5\u8cbb\u4ea4\u901a ]\n\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3   [ \u6d88\u8017\u54c1\u8cbb ]\n\u5207\u624b\u4ee3   [ \u901a\u4fe1\u8cbb ]\n\n\u304b\u306a\u308a\u826f\u3044\u611f\u3058(^-^)\n\u3061\u306a\u307f\u306b\u632f\u66ff\u4f1d\u7968\u306f\u3082\u3046\u5c11\u3057\u5de5\u592b\u304c\u5fc5\u8981\u3067\u3059\u306d\u3002\n\u4ed6\u306b\u3082\u6708\u3084\u66dc\u65e5\u3001\u6c7a\u7b97\u306e\u60c5\u5831\u3068\u304b\u3082\u5229\u7528\u3067\u304d\u308b\u3068\u3082\u3063\u3068\u826f\u3044\u304b\u306a\u3068\u601d\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6271\u3044\u65b9\u304c\u3044\u307e\u3044\u3061\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u307e\u305f\u5f8c\u65e5\u8abf\u3079\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\u3055\u3066\u3001\u6b21\u306f\u4f55\u3057\u3088\u3046\u304b\u306a\u3002\n\n\uff08\u304a\u307e\u3051\uff09\u5b66\u7fd2\u3068\u4e88\u6e2c\u306e\u5207\u308a\u96e2\u3057\n\u4e0a\u8ff0\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u305d\u306e\u307e\u307e\u4f7f\u304a\u3046\u3068\u3059\u308b\u3068\u3001\u5b9f\u884c\u3059\u308b\u305f\u3073\u306b\u5b9f\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u3001\u5b66\u7fd2\u3057\u3066\u304b\u3089\u4e88\u6e2c\u3059\u308b\u305f\u3081\u3001\u52b9\u7387\u7684\u3068\u306f\u8a00\u3048\u307e\u305b\u3093\u3002\n\u305d\u3053\u3067\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3057\u3001\u4e88\u6e2c\u3059\u308b\u90e8\u5206\u3067\u306f\u5b66\u7fd2\u6e08\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\u3059\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u5b66\u7fd2\u7d50\u679c\u306e\u4fdd\u5b58\n\u524d\u8ff0\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6700\u5f8c\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30fc\u30c9\u3092\u8ffd\u52a0\u3059\u308b\u3068\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\nfrom sklearn.externals import joblib\n\njoblib.dump(vect, 'data/vect.pkl')\njoblib.dump(clf, 'data/clf.pkl')\ndf_rs.to_csv(\"data/code.csv\")\n\n\n\u5b66\u7fd2\u7d50\u679c\u306e\u8aad\u8fbc\n\u65b0\u3057\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\nimport pandas as pd\n\nfilename = \"data/code.csv\"\ndf = pd.read_csv(filename, header=None)\ndf.index = df.pop(0)\ndf_rs = df.pop(1)\n\nfrom sklearn.externals import joblib\n\nclf = joblib.load('data/clf.pkl')\nvect = joblib.load('data/vect.pkl')\n\n\n\u4e88\u6e2c\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3060\u3089\u3001\u7d9a\u3051\u3066\u4e88\u6e2c\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\nfrom janome.tokenizer import Tokenizer\n\nt = Tokenizer()\ntests = [\n    \"\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599\",\n    \"\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3\",\n    \"\u5207\u624b\u4ee3\",\n]\n\nnotes = []\nfor note in tests:\n    tokens = t.tokenize(note)\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words)\n\nX = vect.transform(notes)\n\nresult = clf.predict(X)\n\nfor i in range(len(tests)):\n    print(tests[i], \"\\t[\",df_rs.loc[result[i]], \"]\")\n\n\u5b9f\u884c\u7d50\u679c\u306f...\n\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599   [ \u65c5\u8cbb\u4ea4\u901a ]\n\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3   [ \u6d88\u8017\u54c1\u8cbb ]\n\u5207\u624b\u4ee3   [ \u901a\u4fe1\u8cbb ]\n\n\u3067\u304d\u305f\uff01\n# \u306f\u3058\u3081\u306b\n\n\u78ba\u5b9a\u7533\u544a\u306e\u6642\u671f\u306b\u306a\u308a\u3001\u4f1a\u8a08\u30c7\u30fc\u30bf\u3092\u518d\u78ba\u8a8d\u3059\u308b\u3068\u52d8\u5b9a\u79d1\u76ee\u306e\u9593\u9055\u3044\u304c\u3061\u3089\u307b\u3089\u898b\u3064\u304b\u308b\u306e\u3067\u3001\u6a5f\u68b0\u5b66\u7fd2\u3067\u4f1a\u8a08\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3001\u52d8\u5b9a\u79d1\u76ee\u3092\u4e88\u6e2c\u3067\u304d\u308b\u3068\u826f\u3044\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u3044\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n# \u4f1a\u8a08\u30c7\u30fc\u30bf\u306e\u8aad\u8fbc\n\n\u4f1a\u8a08\u30bd\u30d5\u30c8\u304b\u3089CSV\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3001\u305d\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u306e\u30c7\u30fc\u30bf\u306f\u300cJDL IBEX\u51fa\u7d0d\u5e33\u300d\u3068\u3044\u3046\u30bd\u30d5\u30c8\u306e\u3082\u306e\u3092\u4f8b\u306b\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\nhttp://www.jdl.co.jp/co/soft/ibex-ab/\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```py3:\nimport pandas as pd\n\nfilename = \"JDL\u51fa\u7d0d\u5e33-xxxx-xxxx-\u4ed5\u8a33.csv\"\ndf = pd.read_csv(filename, encoding=\"Shift-JIS\", skiprows=3)\n```\n\n\u8aad\u307f\u8fbc\u3093\u3060\u30c7\u30fc\u30bf\u304b\u3089\u4f7f\u7528\u3059\u308b\u30c7\u30fc\u30bf\u3092\u7d5e\u308a\u8fbc\u3080\u3002\n\u3053\u3053\u3067\u306f\u3001\u6458\u8981\u3068\u501f\u65b9\u79d1\u76ee\u306e\u30b3\u30fc\u30c9\u3068\u540d\u79f0\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```py3:\ncolumns = [\"\u6458\u8981\", \"\u501f\u65b9\u79d1\u76ee\", \"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\"]\ndf_counts = df[columns].dropna()\n```\n\n# \u5f62\u614b\u7d20\u89e3\u6790\n\n\u6458\u8981\u306e\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u5f62\u614b\u7d20\u89e3\u6790\u306b\u3088\u308a\u3001\u6587\u5b57\u30c7\u30fc\u30bf\u3092\u6307\u5411\u6027\u306e\u3042\u308b\u6570\u5024\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u5316\u3057\u307e\u3059\u3002\n\n\u5f62\u614b\u7d20\u89e3\u6790\u306b\u306fJanome\u3068\u3044\u3046\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\nhttp://mocobeta.github.io/janome/\n\n\u3082\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n```sh:\n$ pip install janome\n```\n\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u3001\u6458\u8981\u306e\u30c7\u30fc\u30bf\u3092\u30c8\u30fc\u30af\u30f3\u306b\u308f\u3051\u305f\u30c7\u30fc\u30bf\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n\n```py3:\nfrom janome.tokenizer import Tokenizer\n\nt = Tokenizer()\n\nnotes = []\nfor ix in df_counts.index:\n    note = df_counts.ix[ix,\"\u6458\u8981\"]\n    tokens = t.tokenize(note.replace('\u3000',' '))\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words.replace(' \\u3000', ''))\n```\n\n\u3053\u306e\u7d50\u679c\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5909\u63db\u304c\u884c\u308f\u308c\u3001\u5358\u8a9e\u6bce\u306b\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u305f\u6587\u5b57\u5217\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u5143\u306e\u6458\u8981\u30c7\u30fc\u30bf\u3000\u300c\u624b\u571f\u7523\u4ee3\u3000BLUESKY\u7fbd\u7530\u300d\n\u5909\u5316\u5f8c\u306e\u30c7\u30fc\u30bf\u3000\u300c\u624b\u571f\u7523 \u4ee3 BLUESKY \u7fbd\u7530\u300d\n\n\u3053\u306e\u6587\u5b57\u5217\u3092\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```py3:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer()\nvect.fit(notes)\n\nX = vect.transform(notes)\n```\n\n\u307e\u305f\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u52d8\u5b9a\u79d1\u76ee\u306e\u30b3\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```py3:\ny = df_counts.\u501f\u65b9\u79d1\u76ee.as_matrix().astype(\"int\").flatten()\n```\n\n# \u6a5f\u68b0\u5b66\u7fd2\n\n\u6570\u5024\u306b\u5909\u63db\u3057\u305f\u30c7\u30fc\u30bf\u3092\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u307e\u3059\u3002\n\n```py3:\nfrom sklearn import cross_validation\n\ntest_size = 0.2\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size)\n```\n\n\u5206\u5272\u3057\u305f\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3057\u307e\u3059\u3002\n\u3053\u3053\u3067\u306f\u30e2\u30c7\u30eb\u3068\u3057\u3066LinearSVC\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```py3:\nfrom sklearn.svm import LinearSVC\n\nclf = LinearSVC(C=120.0, random_state=42)\nclf.fit(X_train, y_train)\n\nclf.score(X_test, y_test)\n```\n\n\u30b9\u30b3\u30a2\u306f\u300c0.89932885906040272\u300d\u3067\u3057\u305f\u3002\n\n# \u4e88\u6e2c\n\n\u5b66\u7fd2\u3057\u305f\u7d50\u679c\u304b\u3089\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u3001\u3069\u3093\u306a\u52d8\u5b9a\u79d1\u76ee\u304c\u4e88\u6e2c\u3055\u308c\u308b\u304b\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n```py3:\ntests = [\n    \"\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599\",\n    \"\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3\",\n    \"\u5207\u624b\u4ee3\"\n]\n\nnotes = []\nfor note in tests:\n    tokens = t.tokenize(note)\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words)\n\nX = vect.transform(notes)\n\nresult = clf.predict(X)\n\ndf_rs = df_counts[[\"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\", \"\u501f\u65b9\u79d1\u76ee\"]]\ndf_rs.index = df_counts[\"\u501f\u65b9\u79d1\u76ee\"].astype(\"int\")\ndf_rs = df_rs[~df_rs.index.duplicated()][\"\u501f\u65b9\u79d1\u76ee\u540d\u79f0\"]\n\nfor i in range(len(tests)):\n    print(tests[i], \"\\t[\",df_rs.ix[result[i]], \"]\")\n```\n\n\u51fa\u529b\u7d50\u679c\u306f...\n\n```sh:\n\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599 \t[ \u65c5\u8cbb\u4ea4\u901a ]\n\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3 \t[ \u6d88\u8017\u54c1\u8cbb ]\n\u5207\u624b\u4ee3 \t[ \u901a\u4fe1\u8cbb ]\n```\n\n\u304b\u306a\u308a\u826f\u3044\u611f\u3058(^-^)\n\n\u3061\u306a\u307f\u306b\u632f\u66ff\u4f1d\u7968\u306f\u3082\u3046\u5c11\u3057\u5de5\u592b\u304c\u5fc5\u8981\u3067\u3059\u306d\u3002\n\n\u4ed6\u306b\u3082\u6708\u3084\u66dc\u65e5\u3001\u6c7a\u7b97\u306e\u60c5\u5831\u3068\u304b\u3082\u5229\u7528\u3067\u304d\u308b\u3068\u3082\u3063\u3068\u826f\u3044\u304b\u306a\u3068\u601d\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6271\u3044\u65b9\u304c\u3044\u307e\u3044\u3061\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u307e\u305f\u5f8c\u65e5\u8abf\u3079\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u3055\u3066\u3001\u6b21\u306f\u4f55\u3057\u3088\u3046\u304b\u306a\u3002\n\n# \uff08\u304a\u307e\u3051\uff09\u5b66\u7fd2\u3068\u4e88\u6e2c\u306e\u5207\u308a\u96e2\u3057\n\n\u4e0a\u8ff0\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u305d\u306e\u307e\u307e\u4f7f\u304a\u3046\u3068\u3059\u308b\u3068\u3001\u5b9f\u884c\u3059\u308b\u305f\u3073\u306b\u5b9f\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u3001\u5b66\u7fd2\u3057\u3066\u304b\u3089\u4e88\u6e2c\u3059\u308b\u305f\u3081\u3001\u52b9\u7387\u7684\u3068\u306f\u8a00\u3048\u307e\u305b\u3093\u3002\n\u305d\u3053\u3067\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3057\u3001\u4e88\u6e2c\u3059\u308b\u90e8\u5206\u3067\u306f\u5b66\u7fd2\u6e08\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\u3059\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u3066\u307f\u307e\u3059\u3002\n\n## \u5b66\u7fd2\u7d50\u679c\u306e\u4fdd\u5b58\n\n\u524d\u8ff0\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6700\u5f8c\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30fc\u30c9\u3092\u8ffd\u52a0\u3059\u308b\u3068\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n```py3:\nfrom sklearn.externals import joblib\n\njoblib.dump(vect, 'data/vect.pkl')\njoblib.dump(clf, 'data/clf.pkl')\ndf_rs.to_csv(\"data/code.csv\")\n```\n\n## \u5b66\u7fd2\u7d50\u679c\u306e\u8aad\u8fbc\n\n\u65b0\u3057\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```py3:\nimport pandas as pd\n\nfilename = \"data/code.csv\"\ndf = pd.read_csv(filename, header=None)\ndf.index = df.pop(0)\ndf_rs = df.pop(1)\n\nfrom sklearn.externals import joblib\n\nclf = joblib.load('data/clf.pkl')\nvect = joblib.load('data/vect.pkl')\n```\n\n## \u4e88\u6e2c\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3060\u3089\u3001\u7d9a\u3051\u3066\u4e88\u6e2c\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```py3:\nfrom janome.tokenizer import Tokenizer\n\nt = Tokenizer()\ntests = [\n    \"\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599\",\n    \"\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3\",\n    \"\u5207\u624b\u4ee3\",\n]\n\nnotes = []\nfor note in tests:\n    tokens = t.tokenize(note)\n    words = \"\"\n    for token in tokens:\n        words += \" \" + token.surface\n    notes.append(words)\n\nX = vect.transform(notes)\n\nresult = clf.predict(X)\n\nfor i in range(len(tests)):\n    print(tests[i], \"\\t[\",df_rs.loc[result[i]], \"]\")\n```\n\n\u5b9f\u884c\u7d50\u679c\u306f...\n\n```sh:\n\u9ad8\u901f\u9053\u8def\u5229\u7528\u6599 \t[ \u65c5\u8cbb\u4ea4\u901a ]\n\u30d1\u30bd\u30b3\u30f3\u90e8\u54c1\u4ee3 \t[ \u6d88\u8017\u54c1\u8cbb ]\n\u5207\u624b\u4ee3 \t[ \u901a\u4fe1\u8cbb ]\n```\n\n\u3067\u304d\u305f\uff01\n", "tags": ["Python", "\u6a5f\u68b0\u5b66\u7fd2", "scikit-learn"]}