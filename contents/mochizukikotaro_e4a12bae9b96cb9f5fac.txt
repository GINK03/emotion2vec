{"tags": ["TensorFlow", "word2vec"], "context": "\n\n\u306f\u3058\u3081\u306b\nTensorFlow\u306eword2vec\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3092\u52c9\u5f37\u3057\u3066\u3044\u307e\u3059\u3002\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u6975\u7aef\u306b\u5c11\u306a\u304f\u3057\u3066\u3001\u3069\u306e\u3088\u3046\u306a\u6d41\u308c\u306b\u306a\u308b\u306e\u304b\u3092\u63b4\u3082\u3046\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30b3\u30fc\u30c9\u5f04\u308a\u3092\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u3059\uff01\u305f\u3060\u3001\u622f\u308c\u3066\u3044\u308b\u3060\u3051\u3067\u3059\u3002\u516c\u5f0f\u30b3\u30fc\u30c9\u306e\u30b3\u30d4\u30da\u3067\u3059\u304b\u3089\u3001\u3053\u306e\u8a18\u4e8b\u304b\u3089\u5f97\u3089\u308c\u308b\u3053\u3068\u306f\u6b86\u3069\u3042\u308a\u307e\u305b\u3093\u3002\n\u666e\u901a\u306b\u5b66\u7fd2\u3055\u308c\u308b\u65b9\u306f\u516c\u5f0f\u306e\u30b3\u30fc\u30c9\u3092\u3054\u89a7\u9802\u304f\u306e\u304c\u3088\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n\n\u5b9f\u884c\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\u4eca\u56de\u300cword2vec_study.py\u300d\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\n\u5b9f\u884c\u3059\u308b\u3068\u3001\u9014\u4e2d\u3067\u4e00\u56de\u3060\u3051\u8abf\u3079\u308b\u5358\u8a9e\u3092\u5165\u529b\u3057\u3001\u305d\u306e\u5358\u8a9e\u306b\u8fd1\u3044\u5358\u8a9e\u3092\u9806\u306b6\u500b\u8fd4\u3057\u307e\u3059\u3002\n% python word2vec_study.py\nYou can select from below dictionary.\n['a', 'love', 'like', 'pig', 'I', 'am', 'dog', 'cat', 'programer', 'UNK']\n---------------------------\nEnter the target_word: like\nCalculating...\nloss:  4.47126\nloss:  2.03654\nloss:  1.96496\nloss:  2.31992\nloss:  1.62561\nloss:  1.30696\nloss:  1.63599\nloss:  1.57053\nloss:  1.8238\nloss:  1.27289\nNearest to [like]: love, programer, am, UNK, pig, dog,\n\n\u3061\u306a\u307f\u306b\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u5c11\u306a\u304f\u3066\u3001\u3053\u308c\u3060\u3051\u3067\u3059\u3002\nwords = [\n'I' , 'am', 'a', 'programer',\n'I', 'like', 'dog',\n'I', 'love', 'cat',\n'I', 'like', 'pig',\n'I', 'love', 'dog'\n]\n\n\n\u30d7\u30ed\u30b0\u30e9\u30e0\n\nword2vec_study.py\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\n\n# Step 1: Set the data.\nwords = [\n'I' , 'am', 'a', 'programer',\n'I', 'like', 'dog',\n'I', 'love', 'cat',\n'I', 'like', 'pig',\n'I', 'love', 'dog'\n]\n\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 10\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\ndel words  # Hint to reduce memory.\ndata_index = 0\n\nprint(\"You can select from below dictionary.\")\nprint(dictionary.keys())\nprint('---------------------------')\n\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [ skip_window ]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\n\n# Step 4: Build and train a skip-gram model.\nbatch_size = 20\nembedding_size = 3  # Dimension of the embedding vector.\nskip_window = 1       # How many words to consider left and right.\nnum_skips = 2         # How many times to reuse an input to generate a label.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 1     # Random set of words to evaluate similarity on.\nvalid_window = 10  # Only pick dev samples in the head of the distribution.\n\ntarget_word = raw_input('Enter the target_word: ')\nprint('Calculating...')\nvalid_examples = [dictionary[target_word]]\n\nnum_sampled = 3    # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n  # Ops and variables pinned to the CPU because of missing GPU implementation\n  with tf.device('/cpu:0'):\n    # Look up embeddings for inputs.\n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n    # Construct the variables for the NCE loss\n    nce_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Compute the average NCE loss for the batch.\n  # tf.nce_loss automatically draws a new sample of the negative labels each\n  # time we evaluate the loss.\n  loss = tf.reduce_mean(\n      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n                     num_sampled, vocabulary_size))\n\n  # Construct the SGD optimizer using a learning rate of 1.0.\n  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n  # Compute the cosine similarity between minibatch examples and all embeddings.\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(\n      normalized_embeddings, valid_dataset)\n  similarity = tf.matmul(\n      valid_embeddings, normalized_embeddings, transpose_b=True)\n\n  # Add variable initializer.\n  init = tf.initialize_all_variables()\n\n\n# Step 5: Begin training.\nnum_steps = 1000\n\nwith tf.Session(graph=graph) as session:\n  # We must initialize all variables before we use them.\n  init.run()\n\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_inputs, batch_labels = generate_batch(\n        batch_size, num_skips, skip_window)\n\n    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n\n    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n\n    if step % 100 == 0:\n      print(\"loss: \", loss_val)\n\n  sim = similarity.eval()\n  for i in xrange(valid_size):\n    valid_word = reverse_dictionary[valid_examples[i]]\n    top_k = 6 # number of nearest neighbors\n    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n    log_str = \"Nearest to [%s]:\" % valid_word\n    for k in xrange(top_k):\n      close_word = reverse_dictionary[nearest[k]]\n      log_str = \"%s %s,\" % (log_str, close_word)\n    print(log_str)\n\n\n# \u306f\u3058\u3081\u306b\n\nTensorFlow\u306eword2vec\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3092\u52c9\u5f37\u3057\u3066\u3044\u307e\u3059\u3002\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u6975\u7aef\u306b\u5c11\u306a\u304f\u3057\u3066\u3001\u3069\u306e\u3088\u3046\u306a\u6d41\u308c\u306b\u306a\u308b\u306e\u304b\u3092\u63b4\u3082\u3046\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30b3\u30fc\u30c9\u5f04\u308a\u3092\u3057\u3066\u3044\u308b\u3060\u3051\u3067\u3059\uff01\u305f\u3060\u3001\u622f\u308c\u3066\u3044\u308b\u3060\u3051\u3067\u3059\u3002**\u516c\u5f0f\u30b3\u30fc\u30c9\u306e\u30b3\u30d4\u30da\u3067\u3059\u304b\u3089\u3001\u3053\u306e\u8a18\u4e8b\u304b\u3089\u5f97\u3089\u308c\u308b\u3053\u3068\u306f\u6b86\u3069\u3042\u308a\u307e\u305b\u3093\u3002**\n\n\u666e\u901a\u306b\u5b66\u7fd2\u3055\u308c\u308b\u65b9\u306f\u516c\u5f0f\u306e\u30b3\u30fc\u30c9\u3092\u3054\u89a7\u9802\u304f\u306e\u304c\u3088\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\n\n# \u5b9f\u884c\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u4eca\u56de\u300cword2vec_study.py\u300d\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\n\n\u5b9f\u884c\u3059\u308b\u3068\u3001\u9014\u4e2d\u3067\u4e00\u56de\u3060\u3051\u8abf\u3079\u308b\u5358\u8a9e\u3092\u5165\u529b\u3057\u3001\u305d\u306e\u5358\u8a9e\u306b\u8fd1\u3044\u5358\u8a9e\u3092\u9806\u306b6\u500b\u8fd4\u3057\u307e\u3059\u3002\n\n\n```zsh\n% python word2vec_study.py\nYou can select from below dictionary.\n['a', 'love', 'like', 'pig', 'I', 'am', 'dog', 'cat', 'programer', 'UNK']\n---------------------------\nEnter the target_word: like\nCalculating...\nloss:  4.47126\nloss:  2.03654\nloss:  1.96496\nloss:  2.31992\nloss:  1.62561\nloss:  1.30696\nloss:  1.63599\nloss:  1.57053\nloss:  1.8238\nloss:  1.27289\nNearest to [like]: love, programer, am, UNK, pig, dog,\n```\n\n\u3061\u306a\u307f\u306b\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u5c11\u306a\u304f\u3066\u3001\u3053\u308c\u3060\u3051\u3067\u3059\u3002\n\n```py\nwords = [\n'I' , 'am', 'a', 'programer',\n'I', 'like', 'dog',\n'I', 'love', 'cat',\n'I', 'like', 'pig',\n'I', 'love', 'dog'\n]\n```\n\n# \u30d7\u30ed\u30b0\u30e9\u30e0\n\n```py:word2vec_study.py\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\n\n# Step 1: Set the data.\nwords = [\n'I' , 'am', 'a', 'programer',\n'I', 'like', 'dog',\n'I', 'love', 'cat',\n'I', 'like', 'pig',\n'I', 'love', 'dog'\n]\n\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 10\n\ndef build_dataset(words):\n  count = [['UNK', -1]]\n  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n  dictionary = dict()\n  for word, _ in count:\n    dictionary[word] = len(dictionary)\n  data = list()\n  unk_count = 0\n  for word in words:\n    if word in dictionary:\n      index = dictionary[word]\n    else:\n      index = 0  # dictionary['UNK']\n      unk_count += 1\n    data.append(index)\n  count[0][1] = unk_count\n  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n  return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\ndel words  # Hint to reduce memory.\ndata_index = 0\n\nprint(\"You can select from below dictionary.\")\nprint(dictionary.keys())\nprint('---------------------------')\n\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  for _ in range(span):\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  for i in range(batch_size // num_skips):\n    target = skip_window  # target label at the center of the buffer\n    targets_to_avoid = [ skip_window ]\n    for j in range(num_skips):\n      while target in targets_to_avoid:\n        target = random.randint(0, span - 1)\n      targets_to_avoid.append(target)\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[target]\n    buffer.append(data[data_index])\n    data_index = (data_index + 1) % len(data)\n  return batch, labels\n\n\n# Step 4: Build and train a skip-gram model.\nbatch_size = 20\nembedding_size = 3  # Dimension of the embedding vector.\nskip_window = 1       # How many words to consider left and right.\nnum_skips = 2         # How many times to reuse an input to generate a label.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 1     # Random set of words to evaluate similarity on.\nvalid_window = 10  # Only pick dev samples in the head of the distribution.\n\ntarget_word = raw_input('Enter the target_word: ')\nprint('Calculating...')\nvalid_examples = [dictionary[target_word]]\n\nnum_sampled = 3    # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n  # Ops and variables pinned to the CPU because of missing GPU implementation\n  with tf.device('/cpu:0'):\n    # Look up embeddings for inputs.\n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n    # Construct the variables for the NCE loss\n    nce_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n  # Compute the average NCE loss for the batch.\n  # tf.nce_loss automatically draws a new sample of the negative labels each\n  # time we evaluate the loss.\n  loss = tf.reduce_mean(\n      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n                     num_sampled, vocabulary_size))\n\n  # Construct the SGD optimizer using a learning rate of 1.0.\n  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n  # Compute the cosine similarity between minibatch examples and all embeddings.\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  valid_embeddings = tf.nn.embedding_lookup(\n      normalized_embeddings, valid_dataset)\n  similarity = tf.matmul(\n      valid_embeddings, normalized_embeddings, transpose_b=True)\n\n  # Add variable initializer.\n  init = tf.initialize_all_variables()\n\n\n# Step 5: Begin training.\nnum_steps = 1000\n\nwith tf.Session(graph=graph) as session:\n  # We must initialize all variables before we use them.\n  init.run()\n\n  average_loss = 0\n  for step in xrange(num_steps):\n    batch_inputs, batch_labels = generate_batch(\n        batch_size, num_skips, skip_window)\n\n    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n\n    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n\n    if step % 100 == 0:\n      print(\"loss: \", loss_val)\n\n  sim = similarity.eval()\n  for i in xrange(valid_size):\n    valid_word = reverse_dictionary[valid_examples[i]]\n    top_k = 6 # number of nearest neighbors\n    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n    log_str = \"Nearest to [%s]:\" % valid_word\n    for k in xrange(top_k):\n      close_word = reverse_dictionary[nearest[k]]\n      log_str = \"%s %s,\" % (log_str, close_word)\n    print(log_str)\n```\n"}