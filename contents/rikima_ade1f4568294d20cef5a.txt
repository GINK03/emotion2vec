{"context": " More than 1 year has passed since last update.\n\ninstall\ngit clone https://github.com/brianmhess/cassandra-loader.git\ncd cassandra-load\ngradle loader\n\n\nload\u52d5\u4f5c\u78ba\u8a8d\n\nmake_test_csv.sh\n\n#/bin/sh\nfor i in $(seq 1 100000) ; do\n   echo \"$i,$((i * 10))\"\ndone\n\n\nmake_test_csv.sh > test.csv\ncat test.csv\n>1,10\n>2,20\n>3,30\n>....\n\n\nbulk insert\nbuild/cassandra-loader -f test.csv -schema \"test_from_spark.fun(k,v)\" -host 127.0.0.1\n\n\ncqlsh\u3067\u78ba\u8a8d\ncqlsh\n\ncqlsh:test_from_spark> select * from fun limit 10;\n\n k    | v\n------+-------\n 4317 | 43170\n 3372 | 33720\n 1584 | 15840\n 7034 | 70340\n 9892 | 98920\n 9640 | 96400\n 9067 | 90670\n 4830 | 48300\n 2731 | 27310\n 5056 | 50560\n\n(10 rows)\n\n\n\nspark-shell\u4e0a\u3067\u78ba\u8a8d\n\nscala> val t = sc.cassandraTable(\"test_from_spark\", \"fun\");\nt: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[3] at RDD at CassandraRDD.scala:15\n\nscala> t.count\nres6: Long = 10000\n\n\nscala> t.collect.slice(1, 10).foreach(println)\nCassandraRow{k: 9067, v: 90670}\nCassandraRow{k: 4830, v: 48300}\nCassandraRow{k: 2731, v: 27310}\nCassandraRow{k: 5056, v: 50560}\nCassandraRow{k: 6428, v: 64280}\nCassandraRow{k: 2713, v: 27130}\nCassandraRow{k: 769, v: 7690}\nCassandraRow{k: 9973, v: 99730}\nCassandraRow{k: 1863, v: 18630}\n\n\nscala> t.take(10).foreach(println)\nCassandraRow{k: 9640, v: 96400}\nCassandraRow{k: 9067, v: 90670}\nCassandraRow{k: 4830, v: 48300}\nCassandraRow{k: 2731, v: 27310}\nCassandraRow{k: 5056, v: 50560}\nCassandraRow{k: 6428, v: 64280}\nCassandraRow{k: 2713, v: 27130}\nCassandraRow{k: 769, v: 7690}\nCassandraRow{k: 9973, v: 99730}\nCassandraRow{k: 1863, v: 18630}\n\n# install\n\n```\ngit clone https://github.com/brianmhess/cassandra-loader.git\ncd cassandra-load\ngradle loader\n```\n\n# load\u52d5\u4f5c\u78ba\u8a8d\n\n```make_test_csv.sh\n\n#/bin/sh\nfor i in $(seq 1 100000) ; do\n   echo \"$i,$((i * 10))\"\ndone\n```\n\n```\nmake_test_csv.sh > test.csv\ncat test.csv\n>1,10\n>2,20\n>3,30\n>....\n```\n\n## bulk insert\n```\nbuild/cassandra-loader -f test.csv -schema \"test_from_spark.fun(k,v)\" -host 127.0.0.1\n```\n\n## cqlsh\u3067\u78ba\u8a8d\n```\ncqlsh\n\ncqlsh:test_from_spark> select * from fun limit 10;\n\n k    | v\n------+-------\n 4317 | 43170\n 3372 | 33720\n 1584 | 15840\n 7034 | 70340\n 9892 | 98920\n 9640 | 96400\n 9067 | 90670\n 4830 | 48300\n 2731 | 27310\n 5056 | 50560\n\n(10 rows)\n\n```\n\n## spark-shell\u4e0a\u3067\u78ba\u8a8d\n```\n\nscala> val t = sc.cassandraTable(\"test_from_spark\", \"fun\");\nt: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[3] at RDD at CassandraRDD.scala:15\n\nscala> t.count\nres6: Long = 10000\n\n\nscala> t.collect.slice(1, 10).foreach(println)\nCassandraRow{k: 9067, v: 90670}\nCassandraRow{k: 4830, v: 48300}\nCassandraRow{k: 2731, v: 27310}\nCassandraRow{k: 5056, v: 50560}\nCassandraRow{k: 6428, v: 64280}\nCassandraRow{k: 2713, v: 27130}\nCassandraRow{k: 769, v: 7690}\nCassandraRow{k: 9973, v: 99730}\nCassandraRow{k: 1863, v: 18630}\n\n\nscala> t.take(10).foreach(println)\nCassandraRow{k: 9640, v: 96400}\nCassandraRow{k: 9067, v: 90670}\nCassandraRow{k: 4830, v: 48300}\nCassandraRow{k: 2731, v: 27310}\nCassandraRow{k: 5056, v: 50560}\nCassandraRow{k: 6428, v: 64280}\nCassandraRow{k: 2713, v: 27130}\nCassandraRow{k: 769, v: 7690}\nCassandraRow{k: 9973, v: 99730}\nCassandraRow{k: 1863, v: 18630}\n```\n", "tags": ["Cassandra"]}