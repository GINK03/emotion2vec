{"context": " More than 1 year has passed since last update.Hi, this is Bython Chogo. I have to learn English so I try to post article both English and Japanese :(\nNow studying Machine Learning and practicing test scripting with Bayesian filtering. my plan is to estimate tag from web posted contents after learning several posts and tags. \nBayesian sample script can be got from Gihyo web page, I'll introduce later, before that today's topic and problem to talk is scraping contents from post. \nI found good slide to describe what I'd like to say however I've lost ... orz. Will add it later. \nRegarding the article, there is two way to scrape body contents. One is using characterized format of each contents. I don't need header or footer date for learning words because it may not useful for identifying the tag. \nAs a example, I try to scrape only article on Hatena Blog, the article is between the below tags. \n    <div class=entry-contents>\n    CONTENTS to SCRAPE!\n    </div>\n\nthis case, I wrote below code. \n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find(\"div\", {\"class\": \"entry-content\"})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\nLooks not cool.. but it works :( \nAlso I have to prepare all format I will scrape. This is very tired. So second way is to use learning method! But this looks difficult for me. \nTo be continued... \n\u3069\u3046\u3082\u6885\u6751\u9577\u5f8c\u3068\u7533\u3057\u307e\u3059\u3002\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\u82f1\u8a9e\u306e\u5b66\u7fd2\u3082\u517c\u306d\u3066\u3044\u308b\u306e\u3067\u82f1\u8a9e\u3068\u65e5\u672c\u8a9e\u3067\u66f8\u3044\u3066\u3044\u307e\u3059\u304c\u3001\u82f1\u8a9e\u306e\u4e0d\u7d30\u5de5\u3055\u306b\u306f\u76ee\u3092\u3064\u3080\u3063\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\u73fe\u5728\u6a5f\u68b0\u5b66\u7fd2\u306e\u304a\u52c9\u5f37\u3092\u3057\u3066\u304a\u308a\u307e\u3057\u3066\u3001\u305d\u306e\u5b9f\u8df5\u7d4c\u9a13\u306e\u4e00\u74b0\u3067\u30d9\u30a4\u30b8\u30a2\u30f3\u3092\u4f7f\u3063\u305f\u8a18\u4e8b\u306b\u5bfe\u3059\u308b\u81ea\u52d5\u30bf\u30b0\u4ed8\u3051\u30b7\u30b9\u30c6\u30e0\u3092\u88fd\u4f5c\u4e2d\u3067\u3059\u3002\u305f\u3060\u884c\u3046\u306b\u3042\u305f\u3063\u3066\u65b0\u305f\u306b\u899a\u3048\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u5343\u91cc\u306e\u9053\u3082\u4e00\u6b69\u304b\u3089\u3068\u3044\u3063\u305f\u3068\u3053\u308d\u3067\u3001\u3053\u3064\u3053\u3064\u3068\u304a\u3053\u306a\u3063\u3066\u3044\u308b\u6b21\u7b2c\u3067\u3054\u3056\u3044\u307e\u3059\u3002\n\u3067\u3001\u672c\u65e5\u306e\u304a\u984c\u306f\u5b66\u7fd2\u3084\u5224\u5b9a\u306b\u4f7f\u3046\u8a18\u4e8b\u306e\u629c\u51fa\u3057\u3001\u3044\u308f\u3086\u308bScraping\u3063\u3066\u3084\u3064\u3067\u3059\u3002\u3053\u3044\u3064\u3082\u3044\u307e\u30db\u30c3\u30c8\u306a\u30c6\u30fc\u30de\u3067\u3044\u308d\u3044\u308d\u306a\u3068\u3053\u308d\u3067\u8a18\u4e8b\u3092\u307f\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u5148\u65e5\u305d\u3046\u3084\u3063\u3066\u63a2\u3057\u305f\u826f\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u3046\u3063\u304b\u308a\u3057\u305f\u3053\u3068\u306b\u5931\u5ff5\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3042\u3068\u3067\u5f35\u308a\u306a\u304a\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3067\u3001\u305d\u306e\u8a18\u4e8b\u306e\u5185\u5bb9\u306f\u5bfe\u8c61\u30da\u30fc\u30b8\u306e\u30d8\u30c3\u30c0\u3068\u304b\u30d5\u30c3\u30bf\u3068\u304b\u3092\u7121\u8996\u3057\u3066\u3001\u8a18\u4e8b\u306e\u672c\u6587\u306e\u307f\u3092\u62bd\u51fa\u3059\u308b\u65b9\u6cd5\u3092\uff12\u3064\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3057\u305f\u3002\n\u4e00\u3064\u306f\u304b\u304f\u30b5\u30a4\u30c8\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5408\u308f\u305b\u3066\u3053\u3064\u3053\u3064\u3068\u8a18\u4e8b\u672c\u6587\u306e\u56f2\u307f\u30bf\u30b0\u3092\u767b\u9332\u3057\u3066\u629c\u304d\u51fa\u3059\u3053\u3068\u3002\u4f8b\u3048\u3070\u306f\u3066\u306a\u30d6\u30ed\u30b0\u306e\u5834\u5408\u306f\u3044\u304b\u306b\u306a\u308a\u307e\u3059\u3002\n    <div class=entry-contents>\n    CONTENTS to SCRAPE!\n    </div>\n\n\u3067\u3001\u3053\u3044\u3064\u304b\u3089\u4e2d\u8eab\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find(\"div\", {\"class\": \"entry-content\"})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\n\u305f\u3076\u3093\u3001\u30b3\u30fc\u30c9\u3082\u4e0d\u7d30\u5de5\u3060\u308d\u3046\u306a\u3068\u304a\u3082\u3044\u3064\u3064\u3053\u308c\u304c\u7cbe\u3044\u3063\u3071\u3044\u3067\u3059\u3002\n\u3067\u3001\u3053\u306e\u65b9\u6cd5\u3060\u3068\u5404\u30b5\u30a4\u30c8\u306e\u7279\u5fb4\u7684\u306a\u56f2\u307f\u3092\u767b\u9332\u3057\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3001\u305d\u308c\u304c\u9762\u5012\u306a\u5834\u5408\u306b\uff12\u3064\u76ee\u306e\u65b9\u6cd5\u306e\u5b66\u7fd2\u3092\u4f7f\u3046\u3001\u3068\u524d\u8ff0\u306e\u8a18\u4e8b\u306b\u306f\u304b\u304b\u308c\u3066\u3044\u305f\u3088\u3046\u306a\u6c17\u304c\u3057\u307e\u3059\u3002\u305f\u3060\u3001\u73fe\u6642\u70b9\u3067\u306e\u5b9f\u529b\u3067\u306f\u306a\u304b\u304b\u306a\u96e3\u3057\u3044\u3068\u3053\u308d\u3067\u3059\u3002\n\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u3064\u3044\u3066\u306f\u5b8c\u6210\u307e\u3067\u30b7\u30ea\u30fc\u30ba\u5316\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nHi, this is Bython Chogo. I have to learn English so I try to post article both English and Japanese :(\n\nNow studying Machine Learning and practicing test scripting with Bayesian filtering. my plan is to estimate tag from web posted contents after learning several posts and tags. \nBayesian sample script can be got from Gihyo web page, I'll introduce later, before that today's topic and problem to talk is scraping contents from post. \n\nI found good slide to describe what I'd like to say however I've lost ... orz. Will add it later. \nRegarding the article, there is two way to scrape body contents. One is using characterized format of each contents. I don't need header or footer date for learning words because it may not useful for identifying the tag. \n\nAs a example, I try to scrape only article on Hatena Blog, the article is between the below tags. \n\n        <div class=entry-contents>\n        CONTENTS to SCRAPE!\n        </div>\n\nthis case, I wrote below code. \n\n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find(\"div\", {\"class\": \"entry-content\"})\n        text = \"\"\n        for con in tag.contents:\n            p = re.compile(r'<.*?>')\n            text += p.sub('', con.encode('utf8'))\n\n\nLooks not cool.. but it works :( \nAlso I have to prepare all format I will scrape. This is very tired. So second way is to use learning method! But this looks difficult for me. \n\nTo be continued... \n\n\n\n\u3069\u3046\u3082\u6885\u6751\u9577\u5f8c\u3068\u7533\u3057\u307e\u3059\u3002\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\u82f1\u8a9e\u306e\u5b66\u7fd2\u3082\u517c\u306d\u3066\u3044\u308b\u306e\u3067\u82f1\u8a9e\u3068\u65e5\u672c\u8a9e\u3067\u66f8\u3044\u3066\u3044\u307e\u3059\u304c\u3001\u82f1\u8a9e\u306e\u4e0d\u7d30\u5de5\u3055\u306b\u306f\u76ee\u3092\u3064\u3080\u3063\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\u73fe\u5728\u6a5f\u68b0\u5b66\u7fd2\u306e\u304a\u52c9\u5f37\u3092\u3057\u3066\u304a\u308a\u307e\u3057\u3066\u3001\u305d\u306e\u5b9f\u8df5\u7d4c\u9a13\u306e\u4e00\u74b0\u3067\u30d9\u30a4\u30b8\u30a2\u30f3\u3092\u4f7f\u3063\u305f\u8a18\u4e8b\u306b\u5bfe\u3059\u308b\u81ea\u52d5\u30bf\u30b0\u4ed8\u3051\u30b7\u30b9\u30c6\u30e0\u3092\u88fd\u4f5c\u4e2d\u3067\u3059\u3002\u305f\u3060\u884c\u3046\u306b\u3042\u305f\u3063\u3066\u65b0\u305f\u306b\u899a\u3048\u308b\u3053\u3068\u304c\u591a\u304f\u3001\u5343\u91cc\u306e\u9053\u3082\u4e00\u6b69\u304b\u3089\u3068\u3044\u3063\u305f\u3068\u3053\u308d\u3067\u3001\u3053\u3064\u3053\u3064\u3068\u304a\u3053\u306a\u3063\u3066\u3044\u308b\u6b21\u7b2c\u3067\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u3067\u3001\u672c\u65e5\u306e\u304a\u984c\u306f\u5b66\u7fd2\u3084\u5224\u5b9a\u306b\u4f7f\u3046\u8a18\u4e8b\u306e\u629c\u51fa\u3057\u3001\u3044\u308f\u3086\u308bScraping\u3063\u3066\u3084\u3064\u3067\u3059\u3002\u3053\u3044\u3064\u3082\u3044\u307e\u30db\u30c3\u30c8\u306a\u30c6\u30fc\u30de\u3067\u3044\u308d\u3044\u308d\u306a\u3068\u3053\u308d\u3067\u8a18\u4e8b\u3092\u307f\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u5148\u65e5\u305d\u3046\u3084\u3063\u3066\u63a2\u3057\u305f\u826f\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u3046\u3063\u304b\u308a\u3057\u305f\u3053\u3068\u306b\u5931\u5ff5\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3042\u3068\u3067\u5f35\u308a\u306a\u304a\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3067\u3001\u305d\u306e\u8a18\u4e8b\u306e\u5185\u5bb9\u306f\u5bfe\u8c61\u30da\u30fc\u30b8\u306e\u30d8\u30c3\u30c0\u3068\u304b\u30d5\u30c3\u30bf\u3068\u304b\u3092\u7121\u8996\u3057\u3066\u3001\u8a18\u4e8b\u306e\u672c\u6587\u306e\u307f\u3092\u62bd\u51fa\u3059\u308b\u65b9\u6cd5\u3092\uff12\u3064\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3057\u305f\u3002\n\n\u4e00\u3064\u306f\u304b\u304f\u30b5\u30a4\u30c8\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5408\u308f\u305b\u3066\u3053\u3064\u3053\u3064\u3068\u8a18\u4e8b\u672c\u6587\u306e\u56f2\u307f\u30bf\u30b0\u3092\u767b\u9332\u3057\u3066\u629c\u304d\u51fa\u3059\u3053\u3068\u3002\u4f8b\u3048\u3070\u306f\u3066\u306a\u30d6\u30ed\u30b0\u306e\u5834\u5408\u306f\u3044\u304b\u306b\u306a\u308a\u307e\u3059\u3002\n\n\n\n        <div class=entry-contents>\n        CONTENTS to SCRAPE!\n        </div>\n\n\u3067\u3001\u3053\u3044\u3064\u304b\u3089\u4e2d\u8eab\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find(\"div\", {\"class\": \"entry-content\"})\n        text = \"\"\n        for con in tag.contents:\n            p = re.compile(r'<.*?>')\n            text += p.sub('', con.encode('utf8'))\n\n\n\u305f\u3076\u3093\u3001\u30b3\u30fc\u30c9\u3082\u4e0d\u7d30\u5de5\u3060\u308d\u3046\u306a\u3068\u304a\u3082\u3044\u3064\u3064\u3053\u308c\u304c\u7cbe\u3044\u3063\u3071\u3044\u3067\u3059\u3002\n\u3067\u3001\u3053\u306e\u65b9\u6cd5\u3060\u3068\u5404\u30b5\u30a4\u30c8\u306e\u7279\u5fb4\u7684\u306a\u56f2\u307f\u3092\u767b\u9332\u3057\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3001\u305d\u308c\u304c\u9762\u5012\u306a\u5834\u5408\u306b\uff12\u3064\u76ee\u306e\u65b9\u6cd5\u306e\u5b66\u7fd2\u3092\u4f7f\u3046\u3001\u3068\u524d\u8ff0\u306e\u8a18\u4e8b\u306b\u306f\u304b\u304b\u308c\u3066\u3044\u305f\u3088\u3046\u306a\u6c17\u304c\u3057\u307e\u3059\u3002\u305f\u3060\u3001\u73fe\u6642\u70b9\u3067\u306e\u5b9f\u529b\u3067\u306f\u306a\u304b\u304b\u306a\u96e3\u3057\u3044\u3068\u3053\u308d\u3067\u3059\u3002\n\n\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u3064\u3044\u3066\u306f\u5b8c\u6210\u307e\u3067\u30b7\u30ea\u30fc\u30ba\u5316\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n", "tags": ["MachineLearning", "Python", "scraping"]}