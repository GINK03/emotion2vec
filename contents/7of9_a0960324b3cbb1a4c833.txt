{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n\n\nTensorFlow\u3092\u4f7f\u3063\u305f100 input nodes, 100 output nodes\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u691c\u8a0e\u4e2d\u3002\n\"function approximation\"\u3092\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u4ee5\u4e0b\u3092\u898b\u3064\u3051\u305f\u3002\nWhat smooth approximations to the ReLu function are available in TensorFlow? @ Quora\n\u56de\u7b54\u3068\u3057\u3066Softplus\u3068\u3044\u3046\u3082\u306e\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\u8a73\u7d30\u306a\u56de\u7b54\u3092\u3057\u3066\u3044\u308bRahul Panicker\u6c0f\u306b\u3088\u308b\u3068 softmax\u3068\u95a2\u4fc2\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3042\u308b\u304c\u3001\u305d\u308c\u306fTensorFlow\u306a\u3069\u3067\u3088\u304f\u8033\u306b\u3059\u308bsoftmax\u95a2\u6570\u3068\u3082\u9055\u3046\u3088\u3046\u3060\u3002\n\nNote: Softmax, as described here is the classical softmax found in optimization literature (like relu, it is a convex function). This is different from (though related to) the softmax function available as a builtin cost function for classification in Tensorflow.\n\n\u56de\u7b54\u306b\u3088\u308b\u3068ReLU\u306ft\u3092\u7528\u3044\u305f\u8fd1\u4f3c\u5f0f\u3068\u306a\u308a\u3001t=1\u306e\u5834\u5408\u304csoftplus\u3002\u305d\u3057\u3066\u3001ReLU\u304csoftplus(t=1)\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u306e\u306f\u3001etxetxe^{tx}\u306et\u306fweight\u3067\u30011/t1/t1/t\u306et\u306fbias\u3067\u6271\u3048\u308b\u3068\u306e\u3053\u3068\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\nGNU bash, version 4.3.8(1)-release (x86_64-pc-linux-gnu)\n```\n\nTensorFlow\u3092\u4f7f\u3063\u305f100 input nodes, 100 output nodes\u306e\u5b66\u7fd2\u306b\u3064\u3044\u3066\u691c\u8a0e\u4e2d\u3002\n\n\"function approximation\"\u3092\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u4ee5\u4e0b\u3092\u898b\u3064\u3051\u305f\u3002\n[What smooth approximations to the ReLu function are available in TensorFlow?](https://www.quora.com/What-smooth-approximations-to-the-ReLu-function-are-available-in-TensorFlow) @ Quora\n\n\u56de\u7b54\u3068\u3057\u3066Softplus\u3068\u3044\u3046\u3082\u306e\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\n\u8a73\u7d30\u306a\u56de\u7b54\u3092\u3057\u3066\u3044\u308bRahul Panicker\u6c0f\u306b\u3088\u308b\u3068 softmax\u3068\u95a2\u4fc2\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3042\u308b\u304c\u3001\u305d\u308c\u306fTensorFlow\u306a\u3069\u3067\u3088\u304f\u8033\u306b\u3059\u308bsoftmax\u95a2\u6570\u3068\u3082\u9055\u3046\u3088\u3046\u3060\u3002\n\n> Note: Softmax, as described here is the classical softmax found in optimization literature (like relu, it is a convex function). This is different from (though related to) the softmax function available as a builtin cost function for classification in Tensorflow.\n\n\u56de\u7b54\u306b\u3088\u308b\u3068ReLU\u306ft\u3092\u7528\u3044\u305f\u8fd1\u4f3c\u5f0f\u3068\u306a\u308a\u3001t=1\u306e\u5834\u5408\u304csoftplus\u3002\u305d\u3057\u3066\u3001ReLU\u304csoftplus(t=1)\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u306e\u306f\u3001$e^{tx}$\u306et\u306fweight\u3067\u3001$1/t$\u306et\u306fbias\u3067\u6271\u3048\u308b\u3068\u306e\u3053\u3068\u3002\n\n\n\n", "tags": ["link", "TensorFlow", "borgWarp"]}