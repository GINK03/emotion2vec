{"context": "\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b - Chainer - Qiita\u3067\u53d6\u308a\u6271\u3063\u305f\u74b0\u5883\u3092OpenAI gym\u30e9\u30a4\u30af\u306b\u6271\u3048\u308b\u3088\u3046\u306b\u74b0\u5883\u3068AI\u3092\u5206\u96e2\u30fb\u6574\u5099\u3057\u307e\u3057\u305f\u3002\u5206\u96e2\u3057\u305f\u3068\u3044\u3063\u3066\u3082\u3001render\u306e\u3068\u3053\u308d\u3092\u5f53\u521dwxPython\u3068\u63cf\u753b\u30fb\u30ed\u30b8\u30c3\u30af\u4e00\u4f53\u3067\u4f5c\u308a\u3053\u3093\u3067\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u304a\u304b\u3057\u306a\u72b6\u614b\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u52d5\u304f\u306e\u3067\u826f\u3057\u3068\u3044\u3046\u6bb5\u968e\u3067\u3059\u3002\n\u3053\u306e\u5358\u773c\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u306fPOMDP\u306e\u4f8b\u3068\u3082\u3044\u3048\u307e\u3059\u304c\u3001\uff08\u6700\u521d\u306b\u30b3\u30fc\u30b9\u4e0a\u306b\u4e57\u305b\u308b\u524d\u63d0\u3067\uff09\u30ed\u30b8\u30c3\u30af\u30d9\u30fc\u30b9\u3067\u52d5\u304b\u3057\u3066\u307f\u3066\u3044\u308b\u4eba\u306f\u5c11\u306a\u304b\u3089\u305a\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\nPOMDP\u3063\u3066\u306a\u3093\u305e\u3068\u3044\u3046\u304b\u305f\u306f\u3001@okdshin\u3055\u3093\u304c\u6700\u8fd1\u89aa\u5207\u306a\u8aac\u660e\u3092\u66f8\u304b\u308c\u3066\u3044\u305f\u306e\u3067\u3001\u53c2\u8003\u306b\u3059\u308b\u3068\u3044\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u21d2 \u5916\u90e8\u30e1\u30e2\u30ea\uff08External Memory\uff09\u3092\u5229\u7528\u3057\u305f\u5f37\u5316\u5b66\u7fd2 - Qiita\n\u5148\u8ff0\u306e\u4ee5\u524d\u306e\u30c1\u30e3\u30ec\u30f3\u30b8\u3067\u306f\u3001\u904e\u53bb4\u30b9\u30c6\u30c3\u30d7\u5206\u306e\u30b9\u30c6\u30fc\u30c8\u3092\u72b6\u614b\u3068\u3057\u3066DQN\u306b\u304f\u308c\u3066\u3084\u3063\u3066\u3044\u307e\u3057\u305f\u3002\u3053\u306e\u307b\u304b\u3001\u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306e\u8981\u9818\u3067\u201d\u7dda\u201d\u306b\u5bfe\u3057\u3066\u3069\u306e\u304f\u3089\u3044\u306e\u4f4d\u7f6e\u306b\u81ea\u5206\u304c\u3044\u308b\u304b\u3092\u8868\u73fe\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5165\u308c\u3066\u3084\u3063\u3066\u3082\u5272\u3068\u3088\u304f\u306a\u308a\u307e\u3059\u3002\n\uff08\u307e\u3001\u4fe1\u5ff5\u307f\u305f\u3044\u306a\u3082\u306e\u3067\u3059\u306d\uff09\n\u3067\u3001OpenAI Gym\u306b\u3064\u3044\u3066\u306f@icoxfog417\u3055\u3093\u304c\u30cf\u30f3\u30ba\u30aa\u30f3\u30922016/11/16\u306b\u4f01\u753b\u3055\u308c\u3066\u3044\u308b\u305d\u3046\u3067\u3001\u4e8b\u524d\u6e96\u5099\u8cc7\u6599\u3068\u3057\u3066github\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\u306e\u304c\u5206\u304b\u308a\u3084\u3059\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u3002\nicoxfog417/techcircle_openai_handson: Tech-Circle OpenAI Handson Repository\n(Windows 7\u3060\u3068\u5c0e\u5165\u304c\u96e3\u3057\u3044\u307f\u305f\u3044\u3067\u3059\u3051\u3069\u306d\u3001\u672c\u4ef6\u306fWindwos7\u3001CPU ONLY\u3067\u3082\u52d5\u304d\u307e\u3059\uff09\n\uff08\u4ed6\u4eba\u306e\u890c\u3067\u76f8\u64b2\u53d6\u308a\u307e\u304f\u308a\uff09\n\n\u30ea\u30dd\u30b8\u30c8\u30ea\nGym_LineFollower: Simple Open AI gym like Environment\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3044\u305f\u3060\u3044\u3066\u3001\npython exapmle.py\n\n\u3067\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u52d5\u304d\u56de\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c1000\u30b9\u30c6\u30c3\u30d7\u3060\u3051\u8a08\u7b97\u3057\u307e\u3059\u3002\n\u3053\u3053\u306e\u4e2d\u3092\u66f8\u304d\u63db\u3048\u3066\u3001DQN\u3084\u305d\u306e\u307b\u304b\u306e\u65b9\u5f0f\u306eAI\u3067\u52d5\u304b\u305b\u307e\u3059\u3002\n\naction\u306f(\u5de6\u8f2a\u901f\u5ea6, \u53f3\u8f2a\u901f\u5ea6)\u3067\u3001\u305d\u308c\u305e\u308c[-1.0,1.0]\u306e\u5024\u57df\u3092\u53d6\u308c\u307e\u3059\u3002DQN\u306e\u3088\u3046\u306b\u96e2\u6563\u5024\u3092\u4f7f\u3044\u305f\u3044\u5834\u5408\u306f\u3001actionlist=[[1.0,1.0],[-1.0,1.0],...]\u306e\u3088\u3046\u306b\u3057\u305f\u6319\u53e5\u306b\u3001actionlist[0]\u3092\u6e21\u3059\u3088\u3046\u306a\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u304b\u306d\u3002\nobservation\u306f\u5358\u773c\u306e\u5149\u30bb\u30f3\u30b5\u3067\u4e2d\u8eab\u306f(\u691c\u77e5\u30fb\u4e0d\u691c\u77e5)\u3001\u3064\u307e\u308a(1 or 0)\u3067\u3059\u3002\ninfo\u306b\u306f\u30c7\u30d0\u30c3\u30b0\u306a\u3069\u3088\u3046\u306b\u3001\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u306e\u4f4d\u7f6e\u3084\u89d2\u5ea6\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u5fc5\u8981\u3067\u3057\u305f\u3089\u4f7f\u3063\u3066\u307f\u3066\u306d\uff01\nreward\u306f\u5f8c\u8ff0\u306e\u901a\u308a\u8003\u3048\u4e2d\uff01\n\n\nexample.py\n    def run(self):\n        observation = env.reset()\n        for t in range(1000):\n            env.render()\n            print(observation)\n            action = agent.act()\n            observation, reward, done, info = env.step(action)\n            if done:\n                print(\"Episode finished after {} timesteps\".format(t+1))\n                break\n            wx.Yield()\n\n        env.monitor.close()\n\n        print(\"Simulation completed\\nWaiting for closing window..\")\n\n\n\u30b5\u30f3\u30d7\u30eb\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f@ugo-nama\u3055\u3093\u306etorcs gym\u304b\u3089\u30d1\u30af\u30ea\u30d9\u30fc\u30b9\u3067\u3059\u3002\ngym_torcs/sample_agent.py\n\nsample_agent\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass Agent(object):\n    def __init__(self, dim_action):\n        self.dim_action = dim_action\n\n    def act(self):\n        return np.tanh(np.random.randn(self.dim_action)) # random action\n\n\n\n\u5de5\u4e8b\u4e2d\u306e\u3068\u3053\u308d/\u5bbf\u984c\uff08\u3053\u3053\u3092\u30e1\u30e2\u3059\u308b\u305f\u3081\u306e\u6295\u7a3f\u3068\u3082\u3044\u3048\u308b\uff09\n\n1. \u30b3\u30fc\u30b9\u304c\u6c7a\u3081\u6253\u3061\nAI\u306e\u7814\u7a76\u3068\u3044\u3046\u304b\u3089\u306b\u306f\u3044\u308d\u3093\u306a\u74b0\u5883\u3067\u8d70\u308c\u305f\u307b\u3046\u304c\u3044\u3044\u3067\u3059\u3088\u306d\u3002\u3067\u3059\u304c\u3001\u30aa\u30fc\u30d0\u30eb\u3067\u6c7a\u3081\u6253\u3061\u3067\u3059\u3002\n\n2. \u5831\u916c\u8a2d\u8a08\n\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b\u3067\u4f7f\u3063\u305f\u5831\u916c\u3068\u306f\u9055\u3046\u3082\u306e\u3092\u8003\u3048\u3066\u3044\u307e\u3059\u304c\u3001\u4eca\u306f\u4eee\u306e\u3082\u306e\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u73fe\u72b6\u306e\u96d1\u306a\u5831\u916c\n        if not done:\n            # \u751f\u304d\u6b8b\u3063\u3066\u3044\u308c\u3070\u30b3\u30fc\u30b9\u306b\u3044\u306a\u304f\u3066\u3082\u3054\u8912\u7f8e\n            reward = 1.0\n        elif self.steps_beyond_done is None:\n            # Robot just went out over the boundary\n            self.steps_beyond_done = 0\n            reward = 1.0\n        else:\n            if self.steps_beyond_done == 0:\n                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n            self.steps_beyond_done += 1\n            reward = 0.0\n\n\n\u4ee5\u524d\u306f\u7121\u7406\u306b\u30b3\u30fc\u30b9\u306b\u4e57\u305b\u308b\u305f\u3081\u306b\u8272\u3005\u30cf\u30c3\u30af\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u3053\u308c\u3082\u7f8e\u3057\u304f\u306a\u3044\u3067\u3059\u3002\n\n\u4ee5\u524d\u306e\u5999\u306b\u7d30\u304b\u3044\u602a\u3057\u3044\u5831\u916c\n            # Reward\n            # \u30b3\u30fc\u30b9\u306b\u3069\u308c\u3060\u3051\u6cbf\u3063\u3066\u3044\u308b\u304b\u306e\u3054\u8912\u7f8e\n            proximity_reward = 0.0\n            if self.Course.adLines([ag.pos_x, ag.pos_y], 5.0):\n                proximity_reward = 1.0\n            elif self.Course.adLines([ag.pos_x, ag.pos_y], 10.0):\n                proximity_reward = 0.5\n            # \u58c1\u306b\u3076\u3064\u304b\u3063\u305f\u3089\u7f70\u5247\n            if self.Box.adLines([ag.pos_x, ag.pos_y], 3.0):\n                proximity_reward -= 1.0\n\n            # \u76f4\u9032\u3092\u9078\u597d\u3059\u308b\u3088\u3046\u306b\u3054\u8912\u7f8e\n            forward_reward   = 0.0\n            if(action[0] == action[1] and proximity_reward > 0.75):  \n                forward_reward = 0.1 * proximity_reward\n\n            # \u30bb\u30f3\u30b5\u304c\u30b3\u30fc\u30b9\u3092\u691c\u77e5\u3057\u7d9a\u3051\u308b\u3088\u3046\u306a\u30a4\u30f3\u30bb\u30f3\u30c6\u30a3\u30d6           \n            eye_reward = 1.0 if ag.EYE.obj == 1 else 0.0\n\n            # \u3054\u8912\u7f8e\u306e\u5408\u8a08\n            reward = proximity_reward + forward_reward + eye_reward\n\n\n\n3. \u63cf\u753b\u3068\u30ed\u30b8\u30c3\u30af\u306e\u5206\u96e2\n\u56f0\u3063\u305f\u6319\u53e5\u306b\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u306b\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u304c\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u4f7f\u3063\u305f\u308a\u7f8e\u3057\u304f\u306a\u3044\u3002OpenAI gym\u306b\u5bc4\u305b\u308b\u304b\u3001ugonama\u3055\u3093\u307f\u305f\u3044\u306b\u63cf\u753b\u30b5\u30fc\u30d0\u65b9\u5f0f\u306b\u5bc4\u305b\u308b\u304b\u2026\u3002\n\u4ee5\u4e0a\n[\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b \\- Chainer \\- Qiita](http://qiita.com/chachay/items/555638e3079fce9d59c9)\u3067\u53d6\u308a\u6271\u3063\u305f\u74b0\u5883\u3092OpenAI gym\u30e9\u30a4\u30af\u306b\u6271\u3048\u308b\u3088\u3046\u306b\u74b0\u5883\u3068AI\u3092\u5206\u96e2\u30fb\u6574\u5099\u3057\u307e\u3057\u305f\u3002\u5206\u96e2\u3057\u305f\u3068\u3044\u3063\u3066\u3082\u3001render\u306e\u3068\u3053\u308d\u3092\u5f53\u521dwxPython\u3068\u63cf\u753b\u30fb\u30ed\u30b8\u30c3\u30af\u4e00\u4f53\u3067\u4f5c\u308a\u3053\u3093\u3067\u3057\u307e\u3063\u305f\u306e\u3067\u3001\u304a\u304b\u3057\u306a\u72b6\u614b\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u52d5\u304f\u306e\u3067\u826f\u3057\u3068\u3044\u3046\u6bb5\u968e\u3067\u3059\u3002\n\n\u3053\u306e\u5358\u773c\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u306f[POMDP](https://ja.wikipedia.org/wiki/%E9%83%A8%E5%88%86%E8%A6%B3%E6%B8%AC%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)\u306e\u4f8b\u3068\u3082\u3044\u3048\u307e\u3059\u304c\u3001\uff08\u6700\u521d\u306b\u30b3\u30fc\u30b9\u4e0a\u306b\u4e57\u305b\u308b\u524d\u63d0\u3067\uff09\u30ed\u30b8\u30c3\u30af\u30d9\u30fc\u30b9\u3067\u52d5\u304b\u3057\u3066\u307f\u3066\u3044\u308b\u4eba\u306f\u5c11\u306a\u304b\u3089\u305a\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\nPOMDP\u3063\u3066\u306a\u3093\u305e\u3068\u3044\u3046\u304b\u305f\u306f\u3001@okdshin\u3055\u3093\u304c\u6700\u8fd1\u89aa\u5207\u306a\u8aac\u660e\u3092\u66f8\u304b\u308c\u3066\u3044\u305f\u306e\u3067\u3001\u53c2\u8003\u306b\u3059\u308b\u3068\u3044\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u21d2 [\u5916\u90e8\u30e1\u30e2\u30ea\uff08External Memory\uff09\u3092\u5229\u7528\u3057\u305f\u5f37\u5316\u5b66\u7fd2 \\- Qiita](http://qiita.com/okdshin/items/c486da9c3e563147b0ff)\n\n\u5148\u8ff0\u306e\u4ee5\u524d\u306e\u30c1\u30e3\u30ec\u30f3\u30b8\u3067\u306f\u3001\u904e\u53bb4\u30b9\u30c6\u30c3\u30d7\u5206\u306e\u30b9\u30c6\u30fc\u30c8\u3092\u72b6\u614b\u3068\u3057\u3066DQN\u306b\u304f\u308c\u3066\u3084\u3063\u3066\u3044\u307e\u3057\u305f\u3002\u3053\u306e\u307b\u304b\u3001\u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u306e\u8981\u9818\u3067\u201d\u7dda\u201d\u306b\u5bfe\u3057\u3066\u3069\u306e\u304f\u3089\u3044\u306e\u4f4d\u7f6e\u306b\u81ea\u5206\u304c\u3044\u308b\u304b\u3092\u8868\u73fe\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5165\u308c\u3066\u3084\u3063\u3066\u3082\u5272\u3068\u3088\u304f\u306a\u308a\u307e\u3059\u3002\n\uff08\u307e\u3001\u4fe1\u5ff5\u307f\u305f\u3044\u306a\u3082\u306e\u3067\u3059\u306d\uff09\n\n\u3067\u3001OpenAI Gym\u306b\u3064\u3044\u3066\u306f@icoxfog417\u3055\u3093\u304c\u30cf\u30f3\u30ba\u30aa\u30f3\u30922016/11/16\u306b\u4f01\u753b\u3055\u308c\u3066\u3044\u308b\u305d\u3046\u3067\u3001\u4e8b\u524d\u6e96\u5099\u8cc7\u6599\u3068\u3057\u3066github\u306b\u4e0a\u304c\u3063\u3066\u3044\u308b\u306e\u304c\u5206\u304b\u308a\u3084\u3059\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u3002\n[icoxfog417/techcircle\\_openai\\_handson: Tech\\-Circle OpenAI Handson Repository](https://github.com/icoxfog417/techcircle_openai_handson)\n\n(Windows 7\u3060\u3068\u5c0e\u5165\u304c\u96e3\u3057\u3044\u307f\u305f\u3044\u3067\u3059\u3051\u3069\u306d\u3001\u672c\u4ef6\u306fWindwos7\u3001CPU ONLY\u3067\u3082\u52d5\u304d\u307e\u3059\uff09\n\n\uff08\u4ed6\u4eba\u306e\u890c\u3067\u76f8\u64b2\u53d6\u308a\u307e\u304f\u308a\uff09\n\n# \u30ea\u30dd\u30b8\u30c8\u30ea\n\n[Gym\\_LineFollower: Simple Open AI gym like Environment](https://github.com/Chachay/Gym_LineFollower)\n![Demo.png](https://qiita-image-store.s3.amazonaws.com/0/117379/a4499ead-c93c-c673-1568-4926b10ac7c1.png)\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3044\u305f\u3060\u3044\u3066\u3001\n\n```\npython exapmle.py\n```\n\n\u3067\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u52d5\u304d\u56de\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c1000\u30b9\u30c6\u30c3\u30d7\u3060\u3051\u8a08\u7b97\u3057\u307e\u3059\u3002\n\u3053\u3053\u306e\u4e2d\u3092\u66f8\u304d\u63db\u3048\u3066\u3001DQN\u3084\u305d\u306e\u307b\u304b\u306e\u65b9\u5f0f\u306eAI\u3067\u52d5\u304b\u305b\u307e\u3059\u3002\n\n* action\u306f(\u5de6\u8f2a\u901f\u5ea6, \u53f3\u8f2a\u901f\u5ea6)\u3067\u3001\u305d\u308c\u305e\u308c[-1.0,1.0]\u306e\u5024\u57df\u3092\u53d6\u308c\u307e\u3059\u3002DQN\u306e\u3088\u3046\u306b\u96e2\u6563\u5024\u3092\u4f7f\u3044\u305f\u3044\u5834\u5408\u306f\u3001actionlist=[[1.0,1.0],[-1.0,1.0],...]\u306e\u3088\u3046\u306b\u3057\u305f\u6319\u53e5\u306b\u3001actionlist[0]\u3092\u6e21\u3059\u3088\u3046\u306a\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u304b\u306d\u3002\n* observation\u306f\u5358\u773c\u306e\u5149\u30bb\u30f3\u30b5\u3067\u4e2d\u8eab\u306f(\u691c\u77e5\u30fb\u4e0d\u691c\u77e5)\u3001\u3064\u307e\u308a(1 or 0)\u3067\u3059\u3002\n* info\u306b\u306f\u30c7\u30d0\u30c3\u30b0\u306a\u3069\u3088\u3046\u306b\u3001\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u306e\u4f4d\u7f6e\u3084\u89d2\u5ea6\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u5fc5\u8981\u3067\u3057\u305f\u3089\u4f7f\u3063\u3066\u307f\u3066\u306d\uff01\n* reward\u306f\u5f8c\u8ff0\u306e\u901a\u308a\u8003\u3048\u4e2d\uff01\n\n```python:example.py\n    def run(self):\n        observation = env.reset()\n        for t in range(1000):\n            env.render()\n            print(observation)\n            action = agent.act()\n            observation, reward, done, info = env.step(action)\n            if done:\n                print(\"Episode finished after {} timesteps\".format(t+1))\n                break\n            wx.Yield()\n\n        env.monitor.close()\n        \n        print(\"Simulation completed\\nWaiting for closing window..\")\n```\n\n\u30b5\u30f3\u30d7\u30eb\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f@ugo-nama\u3055\u3093\u306etorcs gym\u304b\u3089\u30d1\u30af\u30ea\u30d9\u30fc\u30b9\u3067\u3059\u3002\n[gym\\_torcs/sample\\_agent\\.py](https://github.com/ugo-nama-kun/gym_torcs/blob/master/sample_agent.py)\n\n```python:sample_agent\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass Agent(object):\n    def __init__(self, dim_action):\n        self.dim_action = dim_action\n\n    def act(self):\n        return np.tanh(np.random.randn(self.dim_action)) # random action\n```\n\n# \u5de5\u4e8b\u4e2d\u306e\u3068\u3053\u308d/\u5bbf\u984c\uff08\u3053\u3053\u3092\u30e1\u30e2\u3059\u308b\u305f\u3081\u306e\u6295\u7a3f\u3068\u3082\u3044\u3048\u308b\uff09\n## 1. \u30b3\u30fc\u30b9\u304c\u6c7a\u3081\u6253\u3061\nAI\u306e\u7814\u7a76\u3068\u3044\u3046\u304b\u3089\u306b\u306f\u3044\u308d\u3093\u306a\u74b0\u5883\u3067\u8d70\u308c\u305f\u307b\u3046\u304c\u3044\u3044\u3067\u3059\u3088\u306d\u3002\u3067\u3059\u304c\u3001\u30aa\u30fc\u30d0\u30eb\u3067\u6c7a\u3081\u6253\u3061\u3067\u3059\u3002\n\n## 2. \u5831\u916c\u8a2d\u8a08\n[\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b](http://qiita.com/chachay/items/555638e3079fce9d59c9)\u3067\u4f7f\u3063\u305f\u5831\u916c\u3068\u306f\u9055\u3046\u3082\u306e\u3092\u8003\u3048\u3066\u3044\u307e\u3059\u304c\u3001\u4eca\u306f\u4eee\u306e\u3082\u306e\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\n```python:\u73fe\u72b6\u306e\u96d1\u306a\u5831\u916c\n        if not done:\n            # \u751f\u304d\u6b8b\u3063\u3066\u3044\u308c\u3070\u30b3\u30fc\u30b9\u306b\u3044\u306a\u304f\u3066\u3082\u3054\u8912\u7f8e\n            reward = 1.0\n        elif self.steps_beyond_done is None:\n            # Robot just went out over the boundary\n            self.steps_beyond_done = 0\n            reward = 1.0\n        else:\n            if self.steps_beyond_done == 0:\n                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n            self.steps_beyond_done += 1\n            reward = 0.0\n```\n\n\u4ee5\u524d\u306f\u7121\u7406\u306b\u30b3\u30fc\u30b9\u306b\u4e57\u305b\u308b\u305f\u3081\u306b\u8272\u3005\u30cf\u30c3\u30af\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u3053\u308c\u3082\u7f8e\u3057\u304f\u306a\u3044\u3067\u3059\u3002\n\n```python:\u4ee5\u524d\u306e\u5999\u306b\u7d30\u304b\u3044\u602a\u3057\u3044\u5831\u916c\n            # Reward\n            # \u30b3\u30fc\u30b9\u306b\u3069\u308c\u3060\u3051\u6cbf\u3063\u3066\u3044\u308b\u304b\u306e\u3054\u8912\u7f8e\n            proximity_reward = 0.0\n            if self.Course.adLines([ag.pos_x, ag.pos_y], 5.0):\n                proximity_reward = 1.0\n            elif self.Course.adLines([ag.pos_x, ag.pos_y], 10.0):\n                proximity_reward = 0.5\n            # \u58c1\u306b\u3076\u3064\u304b\u3063\u305f\u3089\u7f70\u5247\n            if self.Box.adLines([ag.pos_x, ag.pos_y], 3.0):\n                proximity_reward -= 1.0\n\n            # \u76f4\u9032\u3092\u9078\u597d\u3059\u308b\u3088\u3046\u306b\u3054\u8912\u7f8e\n            forward_reward   = 0.0\n            if(action[0] == action[1] and proximity_reward > 0.75):  \n                forward_reward = 0.1 * proximity_reward\n\n            # \u30bb\u30f3\u30b5\u304c\u30b3\u30fc\u30b9\u3092\u691c\u77e5\u3057\u7d9a\u3051\u308b\u3088\u3046\u306a\u30a4\u30f3\u30bb\u30f3\u30c6\u30a3\u30d6           \n            eye_reward = 1.0 if ag.EYE.obj == 1 else 0.0\n\n            # \u3054\u8912\u7f8e\u306e\u5408\u8a08\n            reward = proximity_reward + forward_reward + eye_reward\n```\n\n### 3. \u63cf\u753b\u3068\u30ed\u30b8\u30c3\u30af\u306e\u5206\u96e2\n\u56f0\u3063\u305f\u6319\u53e5\u306b\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u306b\u3057\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u304c\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u4f7f\u3063\u305f\u308a\u7f8e\u3057\u304f\u306a\u3044\u3002OpenAI gym\u306b\u5bc4\u305b\u308b\u304b\u3001ugonama\u3055\u3093\u307f\u305f\u3044\u306b\u63cf\u753b\u30b5\u30fc\u30d0\u65b9\u5f0f\u306b\u5bc4\u305b\u308b\u304b\u2026\u3002\n\n\u4ee5\u4e0a\n", "tags": ["\u5f37\u5316\u5b66\u7fd2", "POMDP"]}