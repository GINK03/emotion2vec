{"tags": ["Chainer", "Python", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2"], "context": "\n\n\u306f\u3058\u3081\u306b\n\u753b\u50cf\u8a8d\u8b58\u306a\u3069\u3067\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(Convolutional Neural Networks, CNN)\u3067\u3059\u304c\u3001\n\u6700\u8fd1\u306f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5206\u91ce\u3067\u3082\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059 [Kim, EMNLP2014]\u3002\n\u4eca\u56de\u306f\u3001Chainer\u3067CNN\u3092\u4f7f\u3063\u305f\u7c21\u5358\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u69cb\u7bc9\u3057\u3001\u6587\u66f8\u5206\u985e\u306e\u30bf\u30b9\u30af\u306b\u9069\u7528\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u30c7\u30fc\u30bf\u306e\u7f6e\u304d\u5834\n\n\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\n\nichiroex@github\n\n\n\u5b9f\u9a13\u306b\u4f7f\u7528\u3057\u305f\u30c7\u30fc\u30bf\n\n\n\n\u3053\u3061\u3089\u306b\u30c7\u30fc\u30bf\u304c\u7f6e\u3044\u3066\u3042\u308a\u307e\u3059\u3002\u300csentence polarity dataset v1.0 \u300d\u3092\u5229\u7528\u3057\u307e\u3057\u305f\u3002\n\u76f4\u63a5\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n\n\n\n\u4e8b\u524d\u6e96\u5099\n\nChainer, scikit-learn, gensim\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nword2vec\u306e\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb( GoogleNews-vectors-negative300.bin.gz)\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9. \n\n\n\u74b0\u5883\n\nPython 2.7\u7cfb\nChainer 1.6.2.1\n\n\n\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u4f5c\u6210\n\u4eca\u56de\u306f\u3001\u5165\u529b\u6587\u66f8\u3092word2vec\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3001\u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u6587\u66f8\u306b\u5bfe\u3057\u3066\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\n\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\u306fuitl.py\u306b\u5b9a\u7fa9\u3057\u3066\u3044\u308bdef load_data(fname)\u3067\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u30a4\u30e1\u30fc\u30b8\u3068\u3057\u3066\u306f\u3001\u5165\u529b\u3055\u308c\u305f\u4f55\u3089\u304b\u306e\u5358\u8a9e\u5217(\u6587\u66f8) x1x_1,x2x_2,x3x_3, .... , xnx_n \u306e\u5404\u5358\u8a9exix_i\u3092\n\u56fa\u5b9a\u6b21\u5143NN\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3001\u305d\u308c\u3089\u3092\u4e26\u3079\u305f2\u6b21\u5143\u306e\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n[\u4f8b]\n\n\u307e\u305f\u3001\u6587\u66f8\u3054\u3068\u306b\u6587\u9577\u304c\u7570\u306a\u308b\u305f\u3081\u3001\u5165\u529b\u6587\u66f8\u306e\u4e2d\u3067\u6700\u5927\u306e\u6587\u9577maxlenmaxlen\u306b\u5408\u308f\u305b\u308b\u305f\u3081\u306bpadding\u3092\u884c\u3044\u307e\u3059\u3002\n\u3064\u307e\u308a\u3001\u751f\u6210\u3055\u308c\u308b2\u6b21\u5143\u306e\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f\u3001N\u2217maxlenN * maxlen\u3068\u306a\u308a\u307e\u3059\u3002\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u4f7f\u7528\u3057\u305fGoogle\u304c\u516c\u958b\u3057\u3066\u3044\u308bword2vec\u306e\u30e2\u30c7\u30eb\u3067\u306f\u3001\u5404\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f300\u3067\u3059\u3002\n\nutil.py\ndef load_data(fname):\n    # \u5b66\u7fd2\u6e08\u307f\u306eword2vec\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\n    model =  word2vec.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\n    target = [] #\u30e9\u30d9\u30eb\n    source = [] #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n\n    #\u6587\u66f8\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\n    document_list = []\n    for l in open(fname, 'r').readlines():\n        sample = l.strip().split(' ',  1)\n        label = sample[0]\n        target.append(label) #\u30e9\u30d9\u30eb\n        document_list.append(sample[1].split()) #\u6587\u66f8\u3054\u3068\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\n\n    max_len = 0\n    rev_document_list = [] #\u672a\u77e5\u8a9e\u51e6\u7406\u5f8c\u306edocument list\n    for doc in document_list:\n        rev_doc = []\n        for word in doc:\n            try:\n                word_vec = np.array(model[word]) #\u672a\u77e5\u8a9e\u306e\u5834\u5408, KeyError\u304c\u8d77\u304d\u308b\n                rev_doc.append(word)\n            except KeyError:\n                rev_doc.append('<unk>') #\u672a\u77e5\u8a9e\n        rev_document_list.append(rev_doc)\n        #\u6587\u66f8\u306e\u6700\u5927\u9577\u3092\u6c42\u3081\u308b(padding\u7528)\n        if len(rev_doc) > max_len:\n            max_len = len(rev_doc)\n\n    #\u6587\u66f8\u9577\u3092padding\u306b\u3088\u308a\u5408\u308f\u305b\u308b\n    rev_document_list = padding(rev_document_list, max_len)\n\n    width = 0 #\u5404\u5358\u8a9e\u306e\u6b21\u5143\u6570\n    #\u6587\u66f8\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u5316\n    for doc in rev_document_list:\n        doc_vec = []\n        for word in doc:\n            try:\n                vec = model[word.decode('utf-8')]\n            except KeyError:\n                vec = model.seeded_vector(word)\n            doc_vec.extend(vec)\n            width = len(vec)\n        source.append(doc_vec)\n\n    dataset = {}\n    dataset['target'] = np.array(target)    \n    dataset['source'] = np.array(source)    \n\n    return dataset, max_len, width\n\n\n\n\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u306f\u3001\u7573\u307f\u8fbc\u307f\u5c64->\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64->\u5168\u7d50\u5408\u5c64\u3068\u3044\u3046\u69cb\u6210\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002\n\u9014\u4e2d\u3067\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3082\u304b\u307e\u305b\u3066\u3044\u307e\u3059\u3002\n\n\nnet.py\nclass SimpleCNN(Chain):\n\n    def __init__(self, input_channel, output_channel, filter_height, filter_width, mid_units, n_units, n_label):\n        super(SimpleCNN, self).__init__(\n            conv1 = L.Convolution2D(input_channel, output_channel, (filter_height, filter_width)),\n            l1    = L.Linear(mid_units, n_units),\n            l2    = L.Linear(n_units,  n_label),\n        )\n\n    #Classifier \u306b\u3088\u3063\u3066\u547c\u3070\u308c\u308b\n    def __call__(self, x):\n        h1 = F.max_pooling_2d(F.relu(self.conv1(x)), 3)\n        h2 = F.dropout(F.relu(self.l1(h1)))\n        y = self.l2(h2)\n        return y\n\n\n\n\n\u5b66\u7fd2\n\u5b66\u7fd2\u3067\u306f\u30011\u30a8\u30dd\u30c3\u30af\u6bce\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u305d\u308c\u305e\u308c\u3067\u6b63\u89e3\u7387\u3001\u30ed\u30b9\u3092\u8a08\u7b97\u3057\u3066\u8868\u793a\u3055\u305b\u3066\u3044\u307e\u3059\u3002\n\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u6587\u66f8\u5206\u985e\u3057\u305f\u6642\u3068\u307b\u307c\u540c\u3058\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u7573\u307f\u8fbc\u307f\u6642\u306e\u3001\u30d5\u30a3\u30eb\u30bf\u30b5\u30a4\u30ba\u306f3\u00d7300(\u5404\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570)\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\ntrain_cnn.py\n# Prepare dataset\ndataset, height, width = util.load_data(args.data)\nprint 'height:', height\nprint 'width:', width\n\ndataset['source'] = dataset['source'].astype(np.float32) #\u7279\u5fb4\u91cf\ndataset['target'] = dataset['target'].astype(np.int32) #\u30e9\u30d9\u30eb\n\nx_train, x_test, y_train, y_test = train_test_split(dataset['source'], dataset['target'], test_size=0.15)\nN_test = y_test.size         # test data size\nN = len(x_train)             # train data size\nin_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n\n# (nsample, channel, height, width) \u306e4\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u306b\u5909\u63db\ninput_channel = 1\nx_train = x_train.reshape(len(x_train), input_channel, height, width) \nx_test  = x_test.reshape(len(x_test), input_channel, height, width)\n\n# \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_units = args.nunits\nn_label = 2\nfilter_height = 3\noutput_channel = 50\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = L.Classifier( SimpleCNN(input_channel, output_channel, filter_height, width, 950, n_units, n_label))\n\n#GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\nif args.gpu > 0:\n    cuda.check_cuda_available()\n    cuda.get_device(args.gpu).use()\n    model.to_gpu()\n    xp = np if args.gpu <= 0 else cuda.cupy #args.gpu <= 0: use cpu, otherwise: use gpu\n\nbatchsize = args.batchsize\nn_epoch = args.epoch\n\n# Setup optimizer\noptimizer = optimizers.AdaGrad()\noptimizer.setup(model)\n\n# Learning loop\nfor epoch in six.moves.range(1, n_epoch + 1):\n\n    print 'epoch', epoch, '/', n_epoch\n\n    # training)\n    perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n    sum_train_loss     = 0.0\n    sum_train_accuracy = 0.0\n    for i in six.moves.range(0, N, batchsize):\n\n        #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n\n        optimizer.update(model, x, t)\n\n        sum_train_loss      += float(model.loss.data) * len(t.data)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n        sum_train_accuracy  += float(model.accuracy.data ) * len(t.data)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n\n    print('train mean loss={}, accuracy={}'.format(sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n\n    # evaluation\n    sum_test_loss     = 0.0\n    sum_test_accuracy = 0.0\n    for i in six.moves.range(0, N_test, batchsize):\n\n        # all test data\n        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n\n        loss = model(x, t)\n\n        sum_test_loss     += float(loss.data) * len(t.data)\n        sum_test_accuracy += float(model.accuracy.data)  * len(t.data)\n\n    print(' test mean loss={}, accuracy={}'.format(\n        sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n\n    if epoch > 10:\n        optimizer.lr *= 0.97\n        print 'learning rate: ', optimizer.lr\n\n    sys.stdout.flush()\n\n\n\n\u5b9f\u9a13\u7d50\u679c\n\u6700\u7d42\u7684\u306a\u6b63\u89e3\u7387\u306f\u3001accuracy=0.775624996424\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5206\u985e\u3057\u305f\u6642\u306f\u3001accuracy=0.716875001788\u3060\u3063\u305f\u306e\u3067\u3001\u304b\u306a\u308a\u6b63\u89e3\u7387\u304c\u826f\u304f\u306a\u308a\u307e\u3057\u305f\u3002\n\uff08\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u3068\u304d\u306f\u3001word2vec\u306f\u4f7f\u308f\u305a\u3001one hot\u306a\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u5b9f\u9a13\u306e\u6761\u4ef6\u306f\u7570\u306a\u308a\u307e\u3059\u3002\uff09\nheight: 59\nwidth: 300\nepoch 1 / 100\ntrain mean loss=0.68654858897, accuracy=0.584814038988\n test mean loss=0.673290403187, accuracy=0.674374999106\nepoch 2 / 100\ntrain mean loss=0.653146019086, accuracy=0.678733030628\n test mean loss=0.626838338375, accuracy=0.695624998212\nepoch 3 / 100\ntrain mean loss=0.604344114544, accuracy=0.717580840894\n test mean loss=0.582373640686, accuracy=0.713124997914\n\n...\n\nepoch 98 / 100\ntrain mean loss=0.399981137426, accuracy=0.826288489978\n test mean loss=0.460177404433, accuracy=0.775625003874\nlearning rate:  6.85350312961e-05\nepoch 99 / 100\ntrain mean loss=0.400466494895, accuracy=0.822536144887\n test mean loss=0.464013618231, accuracy=0.773749999702\nlearning rate:  6.64789803572e-05\nepoch 100 / 100\ntrain mean loss=0.399539747416, accuracy=0.824081227461\n test mean loss=0.466326575726, accuracy=0.775624996424\nlearning rate:  6.44846109465e-05\nsave the model\nsave the optimizer\n\n\n\u304a\u308f\u308a\u306b\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u3001\u6587\u66f8\u5206\u985e\uff08\u30dd\u30b8\u30cd\u30ac\u5206\u985e\uff09\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u3067\u3057\u305f\u304c\u3001\u305d\u308c\u306a\u308a\u306e\u7cbe\u5ea6\u306f\u3067\u308b\u3088\u3046\u3067\u3059\u3002\n\u6b21\u306f\u3001 Yoon Kim\u306e\u30e2\u30c7\u30eb\u3082chainer\u3067\u5b9f\u88c5\u3057\u305f\u306e\u3067\u3001\n\u8a18\u4e8b\u3092\u6295\u7a3f\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\nReferences\n\nYoon Kim, Convolutional Neural Networks for Sentence Classification EMNLP2014\n\n\n# \u306f\u3058\u3081\u306b\n\u753b\u50cf\u8a8d\u8b58\u306a\u3069\u3067\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(Convolutional Neural Networks, CNN)\u3067\u3059\u304c\u3001\n\u6700\u8fd1\u306f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u5206\u91ce\u3067\u3082\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059 [[Kim, EMNLP2014]](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf)\u3002\n\u4eca\u56de\u306f\u3001Chainer\u3067CNN\u3092\u4f7f\u3063\u305f\u7c21\u5358\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u69cb\u7bc9\u3057\u3001\u6587\u66f8\u5206\u985e\u306e\u30bf\u30b9\u30af\u306b\u9069\u7528\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n# \u30c7\u30fc\u30bf\u306e\u7f6e\u304d\u5834\n\n  - \u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n    - [ichiroex@github](https://github.com/ichiroex/chainer-cnn)\n  - \u5b9f\u9a13\u306b\u4f7f\u7528\u3057\u305f\u30c7\u30fc\u30bf\n    - [\u3053\u3061\u3089](https://www.cs.cornell.edu/people/pabo/movie-review-data/)\u306b\u30c7\u30fc\u30bf\u304c\u7f6e\u3044\u3066\u3042\u308a\u307e\u3059\u3002\u300csentence polarity dataset v1.0 \u300d\u3092\u5229\u7528\u3057\u307e\u3057\u305f\u3002\n    - [\u76f4\u63a5\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)\n\n\n# \u4e8b\u524d\u6e96\u5099\n - Chainer, scikit-learn, gensim\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n - word2vec\u306e\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb( GoogleNews-vectors-negative300.bin.gz)\u306e[\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9](https://code.google.com/archive/p/word2vec/). \n\n# \u74b0\u5883\n - Python 2.7\u7cfb\n - Chainer 1.6.2.1\n\n# \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u4f5c\u6210\n\u4eca\u56de\u306f\u3001\u5165\u529b\u6587\u66f8\u3092word2vec\u3092\u7528\u3044\u3066\u30d9\u30af\u30c8\u30eb\u5316\u3057\u3001\u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u6587\u66f8\u306b\u5bfe\u3057\u3066\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\n\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\u306f`uitl.py`\u306b\u5b9a\u7fa9\u3057\u3066\u3044\u308b`def load_data(fname)`\u3067\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u30a4\u30e1\u30fc\u30b8\u3068\u3057\u3066\u306f\u3001\u5165\u529b\u3055\u308c\u305f\u4f55\u3089\u304b\u306e\u5358\u8a9e\u5217(\u6587\u66f8) $x_1$,$x_2$,$x_3$, .... , $x_n$ \u306e\u5404\u5358\u8a9e$x_i$\u3092\n\u56fa\u5b9a\u6b21\u5143$N$\u306e\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3001\u305d\u308c\u3089\u3092\u4e26\u3079\u305f2\u6b21\u5143\u306e\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n[\u4f8b]\n![word_vector.png](https://qiita-image-store.s3.amazonaws.com/0/92685/f49016a7-5258-f2ca-0229-8c176bcd261c.png)\n\n\u307e\u305f\u3001\u6587\u66f8\u3054\u3068\u306b\u6587\u9577\u304c\u7570\u306a\u308b\u305f\u3081\u3001\u5165\u529b\u6587\u66f8\u306e\u4e2d\u3067\u6700\u5927\u306e\u6587\u9577$maxlen$\u306b\u5408\u308f\u305b\u308b\u305f\u3081\u306bpadding\u3092\u884c\u3044\u307e\u3059\u3002\n\u3064\u307e\u308a\u3001\u751f\u6210\u3055\u308c\u308b2\u6b21\u5143\u306e\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f\u3001$N * maxlen$\u3068\u306a\u308a\u307e\u3059\u3002\n\n\u3061\u306a\u307f\u306b\u3001\u4eca\u56de\u4f7f\u7528\u3057\u305fGoogle\u304c\u516c\u958b\u3057\u3066\u3044\u308bword2vec\u306e\u30e2\u30c7\u30eb\u3067\u306f\u3001\u5404\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f300\u3067\u3059\u3002\n\n```python:util.py\ndef load_data(fname):\n    # \u5b66\u7fd2\u6e08\u307f\u306eword2vec\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\n    model =  word2vec.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\n    target = [] #\u30e9\u30d9\u30eb\n    source = [] #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n\n    #\u6587\u66f8\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\n    document_list = []\n    for l in open(fname, 'r').readlines():\n        sample = l.strip().split(' ',  1)\n        label = sample[0]\n        target.append(label) #\u30e9\u30d9\u30eb\n        document_list.append(sample[1].split()) #\u6587\u66f8\u3054\u3068\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\n    \n    max_len = 0\n    rev_document_list = [] #\u672a\u77e5\u8a9e\u51e6\u7406\u5f8c\u306edocument list\n    for doc in document_list:\n        rev_doc = []\n        for word in doc:\n            try:\n                word_vec = np.array(model[word]) #\u672a\u77e5\u8a9e\u306e\u5834\u5408, KeyError\u304c\u8d77\u304d\u308b\n                rev_doc.append(word)\n            except KeyError:\n                rev_doc.append('<unk>') #\u672a\u77e5\u8a9e\n        rev_document_list.append(rev_doc)\n        #\u6587\u66f8\u306e\u6700\u5927\u9577\u3092\u6c42\u3081\u308b(padding\u7528)\n        if len(rev_doc) > max_len:\n            max_len = len(rev_doc)\n    \n    #\u6587\u66f8\u9577\u3092padding\u306b\u3088\u308a\u5408\u308f\u305b\u308b\n    rev_document_list = padding(rev_document_list, max_len)\n    \n    width = 0 #\u5404\u5358\u8a9e\u306e\u6b21\u5143\u6570\n    #\u6587\u66f8\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u5316\n    for doc in rev_document_list:\n        doc_vec = []\n        for word in doc:\n            try:\n                vec = model[word.decode('utf-8')]\n            except KeyError:\n                vec = model.seeded_vector(word)\n            doc_vec.extend(vec)\n            width = len(vec)\n        source.append(doc_vec)\n\n    dataset = {}\n    dataset['target'] = np.array(target)    \n    dataset['source'] = np.array(source)    \n\n    return dataset, max_len, width\n```\n\n# \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u306f\u3001\u7573\u307f\u8fbc\u307f\u5c64->\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64->\u5168\u7d50\u5408\u5c64\u3068\u3044\u3046\u69cb\u6210\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002\n\u9014\u4e2d\u3067\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3082\u304b\u307e\u305b\u3066\u3044\u307e\u3059\u3002\n\n![cnn_network_architecture.png](https://qiita-image-store.s3.amazonaws.com/0/92685/c5d6142e-8be6-c094-74a3-10c88b45c1be.png)\n\n\n\n```py:net.py\nclass SimpleCNN(Chain):\n    \n    def __init__(self, input_channel, output_channel, filter_height, filter_width, mid_units, n_units, n_label):\n        super(SimpleCNN, self).__init__(\n            conv1 = L.Convolution2D(input_channel, output_channel, (filter_height, filter_width)),\n            l1    = L.Linear(mid_units, n_units),\n            l2    = L.Linear(n_units,  n_label),\n        )\n    \n    #Classifier \u306b\u3088\u3063\u3066\u547c\u3070\u308c\u308b\n    def __call__(self, x):\n        h1 = F.max_pooling_2d(F.relu(self.conv1(x)), 3)\n        h2 = F.dropout(F.relu(self.l1(h1)))\n        y = self.l2(h2)\n        return y\n\n```\n\n# \u5b66\u7fd2\n\u5b66\u7fd2\u3067\u306f\u30011\u30a8\u30dd\u30c3\u30af\u6bce\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u305d\u308c\u305e\u308c\u3067\u6b63\u89e3\u7387\u3001\u30ed\u30b9\u3092\u8a08\u7b97\u3057\u3066\u8868\u793a\u3055\u305b\u3066\u3044\u307e\u3059\u3002\n[\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u6587\u66f8\u5206\u985e](http://qiita.com/ichiroex/items/9aa0bcada0b5bf6f9e1c)\u3057\u305f\u6642\u3068\u307b\u307c\u540c\u3058\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u7573\u307f\u8fbc\u307f\u6642\u306e\u3001\u30d5\u30a3\u30eb\u30bf\u30b5\u30a4\u30ba\u306f3\u00d7300(\u5404\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570)\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\n```python:train_cnn.py\n# Prepare dataset\ndataset, height, width = util.load_data(args.data)\nprint 'height:', height\nprint 'width:', width\n\ndataset['source'] = dataset['source'].astype(np.float32) #\u7279\u5fb4\u91cf\ndataset['target'] = dataset['target'].astype(np.int32) #\u30e9\u30d9\u30eb\n\nx_train, x_test, y_train, y_test = train_test_split(dataset['source'], dataset['target'], test_size=0.15)\nN_test = y_test.size         # test data size\nN = len(x_train)             # train data size\nin_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n\n# (nsample, channel, height, width) \u306e4\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u306b\u5909\u63db\ninput_channel = 1\nx_train = x_train.reshape(len(x_train), input_channel, height, width) \nx_test  = x_test.reshape(len(x_test), input_channel, height, width)\n\n# \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_units = args.nunits\nn_label = 2\nfilter_height = 3\noutput_channel = 50\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = L.Classifier( SimpleCNN(input_channel, output_channel, filter_height, width, 950, n_units, n_label))\n\n#GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\nif args.gpu > 0:\n    cuda.check_cuda_available()\n    cuda.get_device(args.gpu).use()\n    model.to_gpu()\n    xp = np if args.gpu <= 0 else cuda.cupy #args.gpu <= 0: use cpu, otherwise: use gpu\n\nbatchsize = args.batchsize\nn_epoch = args.epoch\n\n# Setup optimizer\noptimizer = optimizers.AdaGrad()\noptimizer.setup(model)\n\n# Learning loop\nfor epoch in six.moves.range(1, n_epoch + 1):\n\n    print 'epoch', epoch, '/', n_epoch\n    \n    # training)\n    perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n    sum_train_loss     = 0.0\n    sum_train_accuracy = 0.0\n    for i in six.moves.range(0, N, batchsize):\n\n        #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n        \n        optimizer.update(model, x, t)\n\n        sum_train_loss      += float(model.loss.data) * len(t.data)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n        sum_train_accuracy  += float(model.accuracy.data ) * len(t.data)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n\n    print('train mean loss={}, accuracy={}'.format(sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n\n    # evaluation\n    sum_test_loss     = 0.0\n    sum_test_accuracy = 0.0\n    for i in six.moves.range(0, N_test, batchsize):\n\n        # all test data\n        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n\n        loss = model(x, t)\n\n        sum_test_loss     += float(loss.data) * len(t.data)\n        sum_test_accuracy += float(model.accuracy.data)  * len(t.data)\n\n    print(' test mean loss={}, accuracy={}'.format(\n        sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n\n    if epoch > 10:\n        optimizer.lr *= 0.97\n        print 'learning rate: ', optimizer.lr\n\n    sys.stdout.flush()\n```\n\n\n\n# \u5b9f\u9a13\u7d50\u679c\n\u6700\u7d42\u7684\u306a\u6b63\u89e3\u7387\u306f\u3001`accuracy=0.775624996424`\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n[\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5206\u985e\u3057\u305f\u6642](http://qiita.com/ichiroex/items/9aa0bcada0b5bf6f9e1c)\u306f\u3001`accuracy=0.716875001788`\u3060\u3063\u305f\u306e\u3067\u3001\u304b\u306a\u308a\u6b63\u89e3\u7387\u304c\u826f\u304f\u306a\u308a\u307e\u3057\u305f\u3002\n\uff08\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u3068\u304d\u306f\u3001word2vec\u306f\u4f7f\u308f\u305a\u3001one hot\u306a\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u4f7f\u3063\u3066\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u5b9f\u9a13\u306e\u6761\u4ef6\u306f\u7570\u306a\u308a\u307e\u3059\u3002\uff09\n\n```\nheight: 59\nwidth: 300\nepoch 1 / 100\ntrain mean loss=0.68654858897, accuracy=0.584814038988\n test mean loss=0.673290403187, accuracy=0.674374999106\nepoch 2 / 100\ntrain mean loss=0.653146019086, accuracy=0.678733030628\n test mean loss=0.626838338375, accuracy=0.695624998212\nepoch 3 / 100\ntrain mean loss=0.604344114544, accuracy=0.717580840894\n test mean loss=0.582373640686, accuracy=0.713124997914\n\n...\n\nepoch 98 / 100\ntrain mean loss=0.399981137426, accuracy=0.826288489978\n test mean loss=0.460177404433, accuracy=0.775625003874\nlearning rate:  6.85350312961e-05\nepoch 99 / 100\ntrain mean loss=0.400466494895, accuracy=0.822536144887\n test mean loss=0.464013618231, accuracy=0.773749999702\nlearning rate:  6.64789803572e-05\nepoch 100 / 100\ntrain mean loss=0.399539747416, accuracy=0.824081227461\n test mean loss=0.466326575726, accuracy=0.775624996424\nlearning rate:  6.44846109465e-05\nsave the model\nsave the optimizer\n```\n\n# \u304a\u308f\u308a\u306b\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u3001\u6587\u66f8\u5206\u985e\uff08\u30dd\u30b8\u30cd\u30ac\u5206\u985e\uff09\u3092\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u3067\u3057\u305f\u304c\u3001\u305d\u308c\u306a\u308a\u306e\u7cbe\u5ea6\u306f\u3067\u308b\u3088\u3046\u3067\u3059\u3002\n\n\u6b21\u306f\u3001 [Yoon Kim](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf)\u306e\u30e2\u30c7\u30eb\u3082chainer\u3067\u5b9f\u88c5\u3057\u305f\u306e\u3067\u3001\n\u8a18\u4e8b\u3092\u6295\u7a3f\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n# References\n - [Yoon Kim, Convolutional Neural Networks for Sentence Classification EMNLP2014](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf)\n"}