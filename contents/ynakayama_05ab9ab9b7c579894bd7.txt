{"tags": ["scikit-learn", "MachineLearning", "tfidf"], "context": " More than 1 year has passed since last update.\u6628\u65e5\u89e6\u308c\u305f TF-IDF \u3092\u6c42\u3081\u308b\u30b3\u30fc\u30c9\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u6a5f\u68b0\u5b66\u7fd2\u306b\u3064\u3044\u3066\u306f\u4f8b\u306b\u3088\u3063\u3066 scikit-learn \u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3053\u306e\u3088\u3046\u306a\u65e2\u77e5\u306e\u8a08\u7b97\u306b\u3064\u3044\u3066\u306f\u81ea\u529b\u3067\u5b9f\u88c5\u3059\u308b\u3088\u308a\u5b8c\u6210\u5ea6\u306e\u9ad8\u3044\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5229\u7528\u3059\u308b\u3079\u304d\u3067\u3057\u3087\u3046\u3002\u3053\u308c\u306b\u3088\u308a\u8eca\u8f2a\u306e\u518d\u767a\u660e\u3092\u907f\u3051\u308b\u3001\u54c1\u8cea\u3092\u62c5\u4fdd\u3059\u308b\u3068\u3044\u3046\u72d9\u3044\u304c\u3042\u308a\u307e\u3059\u3002\n\u4e8b\u524d\u6e96\u5099\u3068\u3057\u3066\u3001\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u51e6\u7406\u5bfe\u8c61\u3068\u306a\u308b\u81ea\u7136\u8a00\u8a9e\u306e\u6587\u66f8\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002\nimport os\nimport MeCab\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nhome = os.path.expanduser('~')\ntarget_dir = os.path.join(home, 'docs')\ntoken_dict = {}\n\ndef tokenize(text):\n    \"\"\" MeCab \u3067\u5206\u304b\u3061\u66f8\u304d\u3057\u305f\u7d50\u679c\u3092\u30c8\u30fc\u30af\u30f3\u3068\u3057\u3066\u8fd4\u3059 \"\"\"\n    wakati = MeCab.Tagger(\"-O wakati\")\n    return wakati.parse(text)\n\n# \u3072\u3068\u3064\u3072\u3068\u3064\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306b\u5bfe\u3057\u3066\u8a9e\u5f59\u7fa4\u306e\u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u751f\u6210\u3059\u308b\nfor subdir, dirs, files in os.walk(target_dir):\n    for file in files:\n        file_path = os.path.join(subdir, file)\n        shakes = open(file_path, 'r')\n        text = shakes.read()\n        lowers = text.lower()\n        token_dict[file] = lowers\n\n# scikit-learn \u306e TF-IDF \u30d9\u30af\u30bf\u30e9\u30a4\u30b6\u30fc\u3092\u4f7f\u3046\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\ntfs = tfidf.fit_transform(token_dict.values())\n\nprint(token_dict)\nprint(tfs.toarray())\n\n\u7d22\u5f15\u6587\u5b57\u5217\u3092\u62bd\u51fa\u3057\u305f\u306e\u3061\u65e2\u5b58\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u7a81\u3063\u8fbc\u3081\u3070 TF-IDF \u304c\u6c42\u307e\u308a\u307e\u3059\u3002\n\n\u53c2\u8003\nTfidfVectorizer\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\u7279\u5fb4\u91cf(\u7d20\u6027)\u3092\u4f5c\u308b\u3068\u304d\u306e\u30e1\u30e2 + scikit-learn\u306b\u3061\u3087\u3063\u3068\u89e6\u308b\nhttp://sucrose.hatenablog.com/entry/2013/04/19/014258\n[\u6628\u65e5](http://qiita.com/ynakayama/items/300460aa718363abc85c)\u89e6\u308c\u305f TF-IDF \u3092\u6c42\u3081\u308b\u30b3\u30fc\u30c9\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u6a5f\u68b0\u5b66\u7fd2\u306b\u3064\u3044\u3066\u306f\u4f8b\u306b\u3088\u3063\u3066 scikit-learn \u3092\u4f7f\u3044\u307e\u3059\u3002\n\n\u3053\u306e\u3088\u3046\u306a\u65e2\u77e5\u306e\u8a08\u7b97\u306b\u3064\u3044\u3066\u306f\u81ea\u529b\u3067\u5b9f\u88c5\u3059\u308b\u3088\u308a\u5b8c\u6210\u5ea6\u306e\u9ad8\u3044\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5229\u7528\u3059\u308b\u3079\u304d\u3067\u3057\u3087\u3046\u3002\u3053\u308c\u306b\u3088\u308a\u8eca\u8f2a\u306e\u518d\u767a\u660e\u3092\u907f\u3051\u308b\u3001\u54c1\u8cea\u3092\u62c5\u4fdd\u3059\u308b\u3068\u3044\u3046\u72d9\u3044\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u4e8b\u524d\u6e96\u5099\u3068\u3057\u3066\u3001\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e docs \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u51e6\u7406\u5bfe\u8c61\u3068\u306a\u308b\u81ea\u7136\u8a00\u8a9e\u306e\u6587\u66f8\u3092\u683c\u7d0d\u3057\u307e\u3059\u3002\n\n\n```py3\nimport os\nimport MeCab\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nhome = os.path.expanduser('~')\ntarget_dir = os.path.join(home, 'docs')\ntoken_dict = {}\n\ndef tokenize(text):\n    \"\"\" MeCab \u3067\u5206\u304b\u3061\u66f8\u304d\u3057\u305f\u7d50\u679c\u3092\u30c8\u30fc\u30af\u30f3\u3068\u3057\u3066\u8fd4\u3059 \"\"\"\n    wakati = MeCab.Tagger(\"-O wakati\")\n    return wakati.parse(text)\n\n# \u3072\u3068\u3064\u3072\u3068\u3064\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306b\u5bfe\u3057\u3066\u8a9e\u5f59\u7fa4\u306e\u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u751f\u6210\u3059\u308b\nfor subdir, dirs, files in os.walk(target_dir):\n    for file in files:\n        file_path = os.path.join(subdir, file)\n        shakes = open(file_path, 'r')\n        text = shakes.read()\n        lowers = text.lower()\n        token_dict[file] = lowers\n\n# scikit-learn \u306e TF-IDF \u30d9\u30af\u30bf\u30e9\u30a4\u30b6\u30fc\u3092\u4f7f\u3046\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\ntfs = tfidf.fit_transform(token_dict.values())\n\nprint(token_dict)\nprint(tfs.toarray())\n```\n\n\u7d22\u5f15\u6587\u5b57\u5217\u3092\u62bd\u51fa\u3057\u305f\u306e\u3061\u65e2\u5b58\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u7a81\u3063\u8fbc\u3081\u3070 TF-IDF \u304c\u6c42\u307e\u308a\u307e\u3059\u3002\n\n## \u53c2\u8003\n\nTfidfVectorizer\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\n\u7279\u5fb4\u91cf(\u7d20\u6027)\u3092\u4f5c\u308b\u3068\u304d\u306e\u30e1\u30e2 + scikit-learn\u306b\u3061\u3087\u3063\u3068\u89e6\u308b\nhttp://sucrose.hatenablog.com/entry/2013/04/19/014258\n"}