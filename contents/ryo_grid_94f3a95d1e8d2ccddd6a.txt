{"context": "\u3069\u3046\u3082\u3001\u30aa\u30ea\u30a3\u7814\u7a76\u6240\uff08http://orylab.com/\uff09 \u306e ryo_grid \u3053\u3068\u795e\u6797\u3067\u3059\u3002\n\u4eca\u56de\u306fOpen AI Gym\u3067\u904a\u3093\u3067\u307f\u305f\u306e\u3067\u3001\u305d\u308c\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\u307f\u307e\u3059\u3002\n\nOpen AI Gym (https://gym.openai.com) \u3068\u306f\nAI\u958b\u767a\u8005\u304c\u81ea\u5206\u306eAI\u3092\u52d5\u304b\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u74b0\u5883\u3067\u3059\u3002\nPython\u5411\u3051\u306b\u306f\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002\nAI Gym\u306b\u3088\u308aAI\u958b\u767a\u8005\u3084AI\u7814\u7a76\u8005\u306fAI\u306e\u5b66\u7fd2\u30ed\u30b8\u30c3\u30af\u306e\u7814\u7a76\u958b\u767a\u306b\u5c02\u5ff5\u3067\u304d\u3001\u305d\u306e\u53ef\u8996\u5316\u3084\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\u624b\u9593\u304c\u7701\u3051\u307e\u3059\u3002\u307e\u305f\u3001\u540c\u3058\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u8907\u6570\u306e\u30e6\u30fc\u30b6\u304c\u7af6\u3044\u5408\u3063\u3066\u89e3\u3044\u3066\u3044\u308b\u306e\u3067\u3001\u53c2\u52a0\u8005\u5168\u4f53\u3067\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6027\u80fd\u5411\u4e0a\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002\n\u4e3b\u306a\u30bf\u30fc\u30b2\u30c3\u30c8\u306f\u5f37\u5316\u5b66\u7fd2\u306b\u3088\u3063\u3066\u69cb\u7bc9\u3055\u308c\u308bAI\u3067\u3059\u3002Deep Q-Learning Network(DQN)\u3092\u4ee3\u8868\u3068\u3059\u308b\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u3068\u547c\u3070\u308c\u308b\u624b\u6cd5\u304c\u5c11\u3057\u524d\u304b\u3089\u8a71\u984c\u3067\u3059\u304c\u3001\u305d\u308c\u3089\u3082\u5f37\u5316\u5b66\u7fd2\u306e\u4e00\u7a2e\u306a\u306e\u3067\u3001\u52d5\u304b\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\n\n\u6700\u8fd1\u8a71\u984c\u306b\u306a\u3063\u305f\u3001Google(Alphabet)\u5098\u4e0b\u306eDeepMind\u304c\u516c\u958b\u3057\u305fDeepMind Lab\u3068\u4f3c\u305f\u3088\u3046\u306a\u3082\u306e\u3067\u3001AI Gym\u306e\u65b9\u304c\u53e4\u682a\u3067\u3059\u3002\n\u30b0\u30fc\u30b0\u30eb\u306eDeepMind\u3001AI\u8a13\u7df4\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3092\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u5316\nhttp://japan.zdnet.com/article/35093248/\n\n\u8a66\u3057\u3066\u307f\u3088\u3046\n\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\npip install gym\n\n\u4ed6\u306b\u306f\u3001\u52d5\u753b\u306e\u751f\u6210\u306b\u5fc5\u8981\u306affmpeg\u304c\u8981\u6c42\u3055\u308c\u308b\u306e\u3067\u3001apt-get\u3084homebrew\u3067\u3088\u308d\u3057\u304f\u5165\u308c\u3066\u304a\u3044\u3066\u4e0b\u3055\u3044\u3002\n\u4e0b\u306f\u516c\u5f0f\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3001\n\ntest_gym.py\nimport gym\nenv = gym.make('CartPole-v0')\nenv.monitor.start('./cartpole-experiment')\nfor i_episode in range(20):\n    observation = env.reset()\n    for t in range(100):\n        env.render()\n        action = env.action_space.sample()\n        observation, reward, done, info = env.step(action)\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\n\nenv.monitor.close()\ngym.upload('./cartpole-experiment', api_key='YOUR_API_KEY')\n\n\n\u3053\u308c\u3092\u52d5\u304b\u3059\u3068\u3001\u52d5\u753b\u3084\u3089\u7d71\u8a08\u30c7\u30fc\u30bf\u304c\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3055\u308c\u3066\u3001\u4e0b\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u304c\u3067\u304d\u307e\u3059\u3002\n\u306a\u304a\u3001API_KEY\u306fGithub\u30a2\u30ab\u30a6\u30f3\u30c8\u3067AI gym\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u5165\u624b\u3067\u304d\u307e\u3059\u3002\nhttps://gym.openai.com/evaluations/eval_VqXE5bblQTCMQZKWmCJw\n\u5012\u7acb\u632f\u308a\u5b50\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3067\u3059\u3002\u4f55\u3082\u8003\u3048\u305a\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u30a2\u30af\u30b7\u30e7\u30f3(0:\u5de6\u79fb\u52d5, 1:\u53f3\u79fb\u52d5)\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u30a2\u30af\u30b7\u30e7\u30f3\u3068\u306f\u306a\u3093\u305e\u3084\uff1f\u3068\u3044\u3046\u65b9\u306f\u4e0b\u306e\u30ea\u30f3\u30af\u5148\u306a\u3069\u3067\u5f37\u5316\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8abf\u3079\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n\u30bc\u30ed\u304b\u3089Deep\u307e\u3067\u5b66\u3076\u5f37\u5316\u5b66\u7fd2\nhttp://qiita.com/icoxfog417/items/242439ecd1a477ece312\ngym\u306estep\u30e1\u30bd\u30c3\u30c9\u306f\u74b0\u5883\u60c5\u5831\u3001\u5831\u916c\u3001\u30a8\u30d4\u30bd\u30fc\u30c9\u304c\u7d42\u4e86\u6761\u4ef6\u306b\u9054\u3057\u305f\u304b\u3001\u306a\u3093\u304b\u53c2\u8003\u306e\u60c5\u5831\uff08\u3088\u304f\u308f\u304b\u3089\u306a\u3044\uff09\u306e4\u3064\u3092\u8fd4\u3057\u3066\u304d\u307e\u3059\u3002\u524d\u306e2\u3064\u306f\u5f37\u5316\u5b66\u7fd2\u3092\u56de\u3059\u3046\u3048\u3067\u306e\u4e2d\u5fc3\u3068\u306a\u308b\u60c5\u5831\u3067\u3001\u3053\u308c\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067AI\u306e\u5f37\u5316\u5b66\u7fd2\u304c\u53ef\u80fd\u3067\u3059\u3002\n\nDQN\u3067\u5012\u7acb\u632f\u308a\u5b50\n\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\ncartpole_dqn.py\n# coding: utf-8\nimport numpy as np\nimport time\n\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\nimport gym\n\nnp.random.seed(7)\n\n# \u904e\u53bb\u4f55\u30b3\u30de\u3092\u898b\u308b\u304b\nSTATE_NUM = 4\n\n# DQN\u5185\u90e8\u3067\u4f7f\u308f\u308c\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\nclass Q(Chain):\n    def __init__(self,state_num=STATE_NUM):\n        super(Q,self).__init__(\n             l1=L.Linear(state_num, 16),  # state\u304c\u30a4\u30f3\u30d7\u30c3\u30c8\n             l2=L.Linear(16, 32),\n             l3=L.Linear(32, 64),\n             l4=L.Linear(64, 256),\n             l5=L.Linear(256, 2), # \u51fa\u529b2\u30c1\u30e3\u30cd\u30eb(Qvalue)\u304c\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8            \n        )\n\n    def __call__(self,x,t):\n        return F.mean_squared_error(self.predict(x,train=True),t)\n\n    def  predict(self,x,train=False):\n        h1 = F.leaky_relu(self.l1(x))\n        h2 = F.leaky_relu(self.l2(h1))\n        h3 = F.leaky_relu(self.l3(h2))\n        h4 =  F.leaky_relu(self.l4(h3))\n        y = F.leaky_relu(self.l5(h4))\n        return y\n\n# DQN\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3057\u305f\u304c\u3063\u3066\u52d5\u4f5c\u3059\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\nclass DQNAgent():\n    def __init__(self, epsilon=0.99):\n        self.model = Q()\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.epsilon = epsilon # \u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u9078\u3076\u78ba\u7387\n        self.actions=[0,1] #\u3000\u884c\u52d5\u306e\u9078\u629e\u80a2\n        self.experienceMemory = [] # \u7d4c\u9a13\u30e1\u30e2\u30ea\n        self.memSize = 300*100  # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306e\u30b5\u30a4\u30ba(300\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0x100\u30a8\u30d4\u30bd\u30fc\u30c9)\n        self.experienceMemory_local=[] # \u7d4c\u9a13\u30e1\u30e2\u30ea\uff08\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\uff09\n        self.memPos = 0 #\u30e1\u30e2\u30ea\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        self.batch_num = 32 # \u5b66\u7fd2\u306b\u4f7f\u3046\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n        self.gamma = 0.9       # \u5272\u5f15\u7387\n        self.loss=0\n        self.total_reward_award=np.ones(100)*-1000 #100\u30a8\u30d4\u30bd\u30fc\u30c9\n\n    def get_action_value(self, seq):\n        # seq\u5f8c\u306e\u884c\u52d5\u4fa1\u5024\u3092\u8fd4\u3059\n        x = Variable(np.hstack([seq]).astype(np.float32).reshape((1,-1)))\n        return self.model.predict(x).data[0]\n\n    def get_greedy_action(self, seq):\n        action_index = np.argmax(self.get_action_value(seq))\n        return self.actions[action_index]\n\n    def reduce_epsilon(self):\n        self.epsilon-=1.0/100000\n\n    def get_epsilon(self):\n        return self.epsilon\n\n    def get_action(self,seq,train):\n        '''\n        \u30a2\u30af\u30b7\u30e7\u30f3\u3092\u8fd4\u3059\u3002\n        '''\n        action=0\n        if train==True and np.random.random()<self.epsilon:\n            # random\n            action = np.random.choice(self.actions)\n        else:\n            # greedy\n            action= self.get_greedy_action(seq)\n        return action\n\n    def experience_local(self,old_seq, action, reward, new_seq):\n        #\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u8a18\u61b6\n        self.experienceMemory_local.append( np.hstack([old_seq,action,reward,new_seq]) )\n\n    def experience_global(self,total_reward):\n        #\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8a18\u61b6\n        #\u30d9\u30b9\u30c8100\u306b\u5165\u308b\u7d4c\u9a13\u3092\u53d6\u308a\u8fbc\u3080\n        if np.min(self.total_reward_award)<total_reward:\n            i=np.argmin(self.total_reward_award)\n            self.total_reward_award[i]=total_reward\n\n            # GOOD EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        #\u4e00\u5b9a\u78ba\u7387\u3067\u512a\u79c0\u3067\u306a\u3044\u3082\u306e\u3082\u53d6\u308a\u8fbc\u3080\n        if np.random.random()<0.01:\n            # # NORMAL EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        self.experienceMemory_local=[]\n\n    def experience(self,x):\n        if len(self.experienceMemory)>self.memSize:\n            self.experienceMemory[int(self.memPos%self.memSize)]=x\n            self.memPos+=1\n        else:\n            self.experienceMemory.append( x )\n\n    def update_model(self,old_seq, action, reward, new_seq):\n        '''\n        \u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n        '''\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306b\u305f\u307e\u3063\u3066\u306a\u3044\u5834\u5408\u306f\u66f4\u65b0\u3057\u306a\u3044\n        if len(self.experienceMemory)<self.batch_num:\n            return\n\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n        memsize=len(self.experienceMemory)\n        batch_index = list(np.random.randint(0,memsize,(self.batch_num)))\n        batch =np.array( [self.experienceMemory[i] for i in batch_index ])\n        x = Variable(batch[:,0:STATE_NUM].reshape( (self.batch_num,-1)).astype(np.float32))\n        targets=self.model.predict(x).data.copy()\n\n        for i in range(self.batch_num):\n            #[ seq..., action, reward, seq_new]\n            a = batch[i,STATE_NUM]\n            r = batch[i, STATE_NUM+1]\n            ai=int((a+1)/2) #\u00b11 \u3092index(0,1)\u306b\u3002\n            new_seq= batch[i,(STATE_NUM+2):(STATE_NUM*2+2)]\n            targets[i,ai]=( r+ self.gamma * np.max(self.get_action_value(new_seq)))\n        t = Variable(np.array(targets).reshape((self.batch_num,-1)).astype(np.float32)) \n\n        # \u30cd\u30c3\u30c8\u306e\u66f4\u65b0\n        self.model.zerograds()\n        loss=self.model(x ,t)\n        self.loss = loss.data\n        loss.backward()\n        self.optimizer.update()\n\nclass pendulumEnvironment():\n    '''\n    \u632f\u308a\u5b50\u74b0\u5883\u3002\n    '''\n    def __init__(self):\n        self.env = gym.make('CartPole-v0')\n        self.env.monitor.start('./cartpole-experiment')\n\n    def reset(self):\n        self.env.reset()\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def monitor_close(self):\n        self.env.monitor.close()\n\n# \u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3002\nclass simulator:\n    def __init__(self, environment, agent):\n        self.agent = agent\n        self.env = environment\n\n        self.num_seq=STATE_NUM\n        self.reset_seq()\n        self.learning_rate=1.0\n        self.highscore=0\n        self.log=[]\n\n    def reset_seq(self):\n        self.seq=np.zeros(self.num_seq)\n\n    def push_seq(self, state):\n        self.seq[1:self.num_seq]=self.seq[0:self.num_seq-1]\n        self.seq[0]=state\n\n    def run(self, train=True):\n\n        self.env.reset()\n\n        self.reset_seq()\n        total_reward=0\n\n        for i in range(300):\n            # \u73fe\u5728\u306estate\u304b\u3089\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4fdd\u5b58\n            old_seq = self.seq.copy()\n\n            # \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u884c\u52d5\u3092\u6c7a\u3081\u308b\n            action = self.agent.get_action(old_seq,train)\n\n            # \u74b0\u5883\u306b\u884c\u52d5\u3092\u5165\u529b\u3059\u308b\n            observation, reward, done, info =  self.env.step(action)\n            total_reward +=reward\n\n            # \u7d50\u679c\u3092\u89b3\u6e2c\u3057\u3066state\u3068\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u66f4\u65b0\u3059\u308b\n            state = observation[2]\n            self.push_seq(state)\n            new_seq = self.seq.copy()\n\n            # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u8a18\u61b6\u3059\u308b\n            self.agent.experience_local(old_seq, action, reward, new_seq)\n\n            if done:\n                print(\"Episode finished after {} timesteps\".format(i+1))\n                break\n\n        # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u5185\u5bb9\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u79fb\u3059\n        self.agent.experience_global(total_reward)\n\n        if train:\n            # \u5b66\u7fd2\u7528\u30e1\u30e2\u30ea\u3092\u4f7f\u3063\u3066\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n            self.agent.update_model(old_seq, action, reward, new_seq)\n            self.agent.reduce_epsilon()\n\n        return total_reward\n\nif __name__ == '__main__':\n    agent=DQNAgent()\n    env=pendulumEnvironment()\n    sim=simulator(env,agent)\n\n    best_reword = 0\n    for i in range(300000):\n        total_reword = sim.run(train=True)\n        if best_reword < total_reword:\n            best_reword = total_reword\n\n        print(str(i) + \" \" + str(total_reword) + \" \" + str(best_reword))            \n        env.reset()\n\n        if best_reword > 195:\n            break\n\n    env.monitor_close()\n    gym.upload('./cartpole-experiment', api_key='YOUR_API_KEY')\n\n\n\u7d50\u679c\u306f\u3053\u3061\u3089\u3002\nhttps://gym.openai.com/evaluations/eval_zmiiqkPRgK10SOqMev3A#reproducibility\n\u5b9f\u88c5\u306b\u3042\u305f\u3063\u3066\u306f\u3001\u591a\u304f\u306e\u90e8\u5206\u3092\u4ee5\u4e0b\u306e\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u3092\u6d41\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u3044\u307e\u3059\u3002\n\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\u5012\u7acb\u632f\u5b50\u3067\u5b66\u3076 DQN (Deep Q Network)\nhttp://qiita.com/ashitani/items/bb393e24c20e83e54577\n\n2\u8db3\u6b69\u884c\u30ed\u30dc\u3067DQN\n\u30a2\u30af\u30b7\u30e7\u30f3\u306f(-1,0,1)\u3092\u5024\u306b\u53d6\u308b4\u6b21\u5143\u914d\u5217\u306a\u306e\u3067\u6ce8\u610f\u3002\n\nwalker_dqn.py\n# coding: utf-8\nimport numpy as np\nimport time\n\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\nimport gym\n\nnp.random.seed(7)\n\nSTATE_NUM = 24\n\n# DQN\u5185\u90e8\u3067\u4f7f\u308f\u308c\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\nclass Q(Chain):\n    def __init__(self,state_num=STATE_NUM):\n        super(Q,self).__init__(\n             l1=L.Linear(state_num, 16),  # state\u304c\u30a4\u30f3\u30d7\u30c3\u30c8\n             l2=L.Linear(16, 32),\n             l3=L.Linear(32, 64),\n             l4=L.Linear(64, 256),\n             l5=L.Linear(256, 3*3*3*3), # \u51fa\u529b2\u30c1\u30e3\u30cd\u30eb(Qvalue)\u304c\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8            \n        )\n\n    def __call__(self,x,t):\n        return F.mean_squared_error(self.predict(x,train=True),t)\n\n    def  predict(self,x,train=False):\n        h1 = F.leaky_relu(self.l1(x))\n        h2 = F.leaky_relu(self.l2(h1))\n        h3 = F.leaky_relu(self.l3(h2))\n        h4 =  F.leaky_relu(self.l4(h3))\n        y = F.leaky_relu(self.l5(h4))\n        return y\n\n# DQN\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3057\u305f\u304c\u3063\u3066\u52d5\u4f5c\u3059\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\nclass DQNAgent():\n    def __init__(self, epsilon=0.99):\n        self.model = Q()\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.epsilon = epsilon # \u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u9078\u3076\u78ba\u7387\n        self.actions=[-1,0,1] #\u3000\u884c\u52d5\u306e\u9078\u629e\u80a2\n        self.experienceMemory = [] # \u7d4c\u9a13\u30e1\u30e2\u30ea\n        self.memSize = 300*100  # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306e\u30b5\u30a4\u30ba(300\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0x100\u30a8\u30d4\u30bd\u30fc\u30c9)\n        self.experienceMemory_local=[] # \u7d4c\u9a13\u30e1\u30e2\u30ea\uff08\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\uff09\n        self.memPos = 0 #\u30e1\u30e2\u30ea\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        self.batch_num = 32 # \u5b66\u7fd2\u306b\u4f7f\u3046\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n        self.gamma = 0.9       # \u5272\u5f15\u7387\n        self.loss=0\n        self.total_reward_award=np.ones(100)*-1000 #100\u30a8\u30d4\u30bd\u30fc\u30c9\n\n    def index_to_list(self, index):\n        ret_arr = []\n        a = int(index / 27) - 1\n        rest = index - 27*int(index / 27)\n        ret_arr.append(a)\n        a = int(rest / 9) - 1\n        rest = rest - 9*int(rest / 9)\n        ret_arr.append(a)\n        a = int(rest / 3) - 1\n        rest = rest - 3*int(rest / 3)\n        ret_arr.append(a)\n        ret_arr.append(rest -1)\n\n        return ret_arr\n\n    def list_to_index(self, lst):\n        ret = 0\n\n        ret += (lst[0] + 1)*27\n        ret += (lst[1] + 1)*9\n        ret += (lst[2] + 1)*3\n        ret += (lst[3] + 1)\n\n        return ret\n\n    def get_action_value(self, seq):\n        # seq\u5f8c\u306e\u884c\u52d5\u4fa1\u5024\u3092\u8fd4\u3059\n        x = Variable(np.hstack([seq]).astype(np.float32).reshape((1,-1)))\n        return self.model.predict(x).data[0]\n\n    def get_greedy_action(self, seq):\n        action_index = np.argmax(self.get_action_value(seq))\n        return self.index_to_list(action_index)\n\n    def reduce_epsilon(self):\n        self.epsilon-=1.0/10000\n\n    def get_epsilon(self):\n        return self.epsilon\n\n    def get_action(self,seq,train):\n        '''\n        seq (env, old_env)\u306b\u5bfe\u3057\u3066\n        \u30a2\u30af\u30b7\u30e7\u30f3\u3092\u8fd4\u3059\u3002\n        '''\n        action=[]\n        if train==True and np.random.random()<self.epsilon:\n            # random\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n        else:\n            # greedy\n            action= self.get_greedy_action(seq)\n        return action\n\n    def experience_local(self,old_seq, action, reward, new_seq):\n        #\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u8a18\u61b6\n        self.experienceMemory_local.append( np.hstack([old_seq,action,reward,new_seq]) )\n\n    def experience_global(self,total_reward):\n        #\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8a18\u61b6\n        #\u30d9\u30b9\u30c8100\u306b\u5165\u308b\u7d4c\u9a13\u3092\u53d6\u308a\u8fbc\u3080\n        if np.min(self.total_reward_award)<total_reward:\n            i=np.argmin(self.total_reward_award)\n            self.total_reward_award[i]=total_reward\n\n            # GOOD EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        #\u4e00\u5b9a\u78ba\u7387\u3067\u512a\u79c0\u3067\u306a\u3044\u3082\u306e\u3082\u53d6\u308a\u8fbc\u3080\n        if np.random.random()<0.01:\n            # # NORMAL EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        self.experienceMemory_local=[]\n\n    def experience(self,x):\n        if len(self.experienceMemory)>self.memSize:\n            self.experienceMemory[int(self.memPos%self.memSize)]=x\n            self.memPos+=1\n        else:\n            self.experienceMemory.append( x )\n\n    def update_model(self,old_seq, action, reward, new_seq):\n        '''\n        \u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n        '''\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306b\u305f\u307e\u3063\u3066\u306a\u3044\u5834\u5408\u306f\u66f4\u65b0\u3057\u306a\u3044\n        if len(self.experienceMemory)<self.batch_num:\n            return\n\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n        memsize=len(self.experienceMemory)\n        batch_index = list(np.random.randint(0,memsize,(self.batch_num)))\n        batch =np.array( [self.experienceMemory[i] for i in batch_index ])\n        x = Variable(batch[:,0:STATE_NUM].reshape( (self.batch_num,-1)).astype(np.float32))\n        targets=self.model.predict(x).data.copy()\n\n        for i in range(self.batch_num):\n            #[ seq..., action, reward, seq_new]\n            a = batch[i,STATE_NUM]\n            r = batch[i, STATE_NUM+1]\n            ai=a\n            new_seq= batch[i,(STATE_NUM+2):(STATE_NUM*2+2)]\n            targets[i,ai]=( r+ self.gamma * np.max(self.get_action_value(new_seq)))\n        t = Variable(np.array(targets).reshape((self.batch_num,-1)).astype(np.float32)) \n\n        # \u30cd\u30c3\u30c8\u306e\u66f4\u65b0\n        self.model.zerograds()\n        loss=self.model(x ,t)\n        self.loss = loss.data\n        loss.backward()\n        self.optimizer.update()\n\nclass walkerEnvironment():\n    def __init__(self):\n        self.env = gym.make('BipedalWalkerHardcore-v2')\n        self.env.monitor.start('./walker-experiment')\n\n    def reset(self):\n        self.env.reset()\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def monitor_close(self):\n        self.env.monitor.close()\n\n# \u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3002\nclass simulator:\n    def __init__(self, environment, agent):\n        self.agent = agent\n        self.env = environment\n\n        self.num_seq=STATE_NUM\n        self.reset_seq()\n        self.learning_rate=1.0\n        self.highscore=0\n        self.log=[]\n\n    def reset_seq(self):\n        self.seq=np.zeros(self.num_seq)\n\n    def push_seq(self, state):\n        self.seq = state\n\n    def run(self, train=True):\n\n        self.env.reset()\n\n        self.reset_seq()\n        total_reward=0\n\n        for i in range(100000):\n            # \u73fe\u5728\u306estate\u304b\u3089\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4fdd\u5b58\n            old_seq = self.seq.copy()\n\n            # \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u884c\u52d5\u3092\u6c7a\u3081\u308b\n            action = self.agent.get_action(old_seq,train)\n\n            # \u74b0\u5883\u306b\u884c\u52d5\u3092\u5165\u529b\u3059\u308b\n            observation, reward, done, info =  self.env.step(action)\n            total_reward +=reward\n\n            # \u7d50\u679c\u3092\u89b3\u6e2c\u3057\u3066state\u3068\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u66f4\u65b0\u3059\u308b\n            state = observation\n            self.push_seq(state)\n            new_seq = self.seq.copy()\n\n            # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u8a18\u61b6\u3059\u308b\n            action_idx = self.agent.list_to_index(action)\n            self.agent.experience_local(old_seq, action_idx, reward, new_seq)\n\n            if done:\n                print(\"Episode finished after {} timesteps\".format(i+1))\n                break\n\n        # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u5185\u5bb9\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u79fb\u3059\n        self.agent.experience_global(total_reward)\n\n        if train:\n            # \u5b66\u7fd2\u7528\u30e1\u30e2\u30ea\u3092\u4f7f\u3063\u3066\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n            action_idx = self.agent.list_to_index(action)\n            self.agent.update_model(old_seq, action_idx, reward, new_seq)\n            self.agent.reduce_epsilon()\n\n        return total_reward\n\nif __name__ == '__main__':\n    agent=DQNAgent()\n    env=walkerEnvironment()\n    sim=simulator(env,agent)\n\n    best_reword = -200\n    for i in range(10000):\n        total_reword = sim.run(train=True)\n        if best_reword < total_reword:\n            best_reword = total_reword\n\n        print(str(i) + \" \" + str(total_reword) + \" \" + str(best_reword))            \n        env.reset()\n\n        if best_reword > 200:\n            break\n\n    env.monitor_close()\n    gym.upload('./walker-experiment', api_key='YOUR_API_KEY')\n\n\n\u7d50\u679c\u306f\u3053\u3061\u3089\u3002\nhttps://gym.openai.com/evaluations/eval_qaM0iTJSST2DDd5Br0HE6A#reproducibility\n\u751f\u307e\u308c\u305f\u3066\u306e\u5c0f\u9e7f\u306b\u3057\u304b\u306a\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\n10000\u30a8\u30d4\u30bd\u30fc\u30c9\u5b66\u7fd2\u3059\u308b\u904e\u7a0b\u3067\u5f97\u3089\u308c\u305f award (\u5831\u916c) \u306e\u63a8\u79fb\u3092\u898b\u308b\u3068\u3001\u7d42\u76e4\u306b\u826f\u3044\u30b9\u30b3\u30a2\u3092\u8a18\u9332\u3057\u3066\u3044\u308b\u69d8\u5b50\u304c\u898b\u3066\u53d6\u308c\u308b\u3002\n\u305f\u3060\u3001\u5168\u4f53\u7684\u306b\u306f\u4e0b\u304c\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u3082\u898b\u3048\u3066\u4e0d\u601d\u8b70\u3002\n\n\u305d\u3053\u3067\u3001100\u30a8\u30d4\u30bd\u30fc\u30c9\u3067\u79fb\u52d5\u5e73\u5747\u3092\u3068\u308b\u3068\u3001\u307e\u3042\u3061\u3083\u3093\u3068\u5b66\u7fd2\u3057\u3066\u308b\u3063\u307d\u3044\u3053\u3068\u304c\u8aad\u307f\u53d6\u308c\u308b\u3002\n\u3088\u304b\u3063\u305f\u3088\u304b\u3063\u305f\u3002\n\n\u306a\u304a\u3001\u4e0a\u8a18\u306eAI\u306f\u74b0\u5883\u306e\u60c5\u5831\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30b9\u3092\u610f\u8b58\u3059\u308b\u3053\u3068\u306a\u304f\u52d5\u4f5c\u3057\u3066\u3044\u307e\u3059\u3002\n\u3069\u3046\u3082\u3001\u826f\u304f\u3067\u304d\u305fAI\u3068\u3044\u3046\u306e\u306f\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u610f\u8b58\u3057\u306a\u304f\u3066\u3082\u826f\u3044\u306e\u3060\u3001\u3068\u5049\u3044\u4eba\u304b\u826f\u304f\u308f\u304b\u3089\u306a\u3044\u4eba\u304c\u8a00\u3063\u3066\u3044\u308b\u306e\u3092\u30cd\u30c3\u30c8\u3067\u898b\u307e\u3057\u305f\u3002\u306e\u3067\u3001\u305d\u308c\u306b\u5f93\u3063\u3066\u3044\u307e\u3059\uff08Gym\u5074\u306f\u74b0\u5883\u306e\u5404\u60c5\u5831\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u6559\u3048\u3066\u304f\u308c\u306a\u3044\u306e\u3067\u610f\u8b58\u3057\u3088\u3046\u304c\u306a\u3044\u3068\u3044\u3046\u8a71\u3082\u3042\u308b\uff09\u3002\n\u3061\u306a\u307f\u306b\u3001\u3088\u304f\u3067\u304d\u305f\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3060\u3068\u3053\u3093\u306a\u611f\u3058\u3067\u6b69\u304d\u307e\u3059\u3002\nhttps://gym.openai.com/evaluations/eval_Gxwojs5T5a8jayWaVwFA\n\u73fe\u5834\u304b\u3089\u306f\u4ee5\u4e0a\u3067\u3059\u3002\n\n(12/19\u8ffd\u8a18)\n\u4e8c\u8db3\u6b69\u884c\u30ed\u30dc\u3001\u8ee2\u304c\u3063\u3066\u305f\u30b3\u30fc\u30c9\u3092\u771f\u4f3c\u3057\u305f\u3089\u3001\u305d\u308c\u306a\u308a\u306b\u6b69\u304f\u3088\u3046\u306b\u3067\u3051\u305f\uff01\nhttps://gym.openai.com/evaluations/eval_wzmU0dtQCeoz2dWyhsD5Q\nNeuroevolution\u3068\u547c\u3070\u308c\u308b\u6280\u8853\u306e\u4e2d\u306e\u4e00\u624b\u6cd5\u3067\u3042\u308bNEAT(NeuroEvolution of Augmenting Topologies)\u3068\u3044\u3046\u3084\u3064\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\u3056\u3063\u304f\u308a\u8a00\u3046\u3068\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7d50\u5408\uff1f\u306e\u6700\u9069\u5316\u3092\u3057\u305f\u308a\u3059\u308b\u624b\u6cd5\u307f\u305f\u3044\u3067\u3059\u3002\n\n(1/10\u8ffd\u8a18)\n\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u306b\u3064\u3044\u3066\u66f8\u304d\u307e\u3057\u305f\nhttp://qiita.com/ryo_grid/items/af60750659d1d7ffeef9\n\u3069\u3046\u3082\u3001\u30aa\u30ea\u30a3\u7814\u7a76\u6240\uff08http://orylab.com/\uff09 \u306e ryo_grid \u3053\u3068\u795e\u6797\u3067\u3059\u3002\n\u4eca\u56de\u306fOpen AI Gym\u3067\u904a\u3093\u3067\u307f\u305f\u306e\u3067\u3001\u305d\u308c\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\u307f\u307e\u3059\u3002\n\n# Open AI Gym (https://gym.openai.com) \u3068\u306f\nAI\u958b\u767a\u8005\u304c\u81ea\u5206\u306eAI\u3092\u52d5\u304b\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u308b\u74b0\u5883\u3067\u3059\u3002\nPython\u5411\u3051\u306b\u306f\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002\nAI Gym\u306b\u3088\u308aAI\u958b\u767a\u8005\u3084AI\u7814\u7a76\u8005\u306fAI\u306e\u5b66\u7fd2\u30ed\u30b8\u30c3\u30af\u306e\u7814\u7a76\u958b\u767a\u306b\u5c02\u5ff5\u3067\u304d\u3001\u305d\u306e\u53ef\u8996\u5316\u3084\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\u624b\u9593\u304c\u7701\u3051\u307e\u3059\u3002\u307e\u305f\u3001\u540c\u3058\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u8907\u6570\u306e\u30e6\u30fc\u30b6\u304c\u7af6\u3044\u5408\u3063\u3066\u89e3\u3044\u3066\u3044\u308b\u306e\u3067\u3001\u53c2\u52a0\u8005\u5168\u4f53\u3067\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6027\u80fd\u5411\u4e0a\u304c\u671f\u5f85\u3055\u308c\u307e\u3059\u3002\n\u4e3b\u306a\u30bf\u30fc\u30b2\u30c3\u30c8\u306f\u5f37\u5316\u5b66\u7fd2\u306b\u3088\u3063\u3066\u69cb\u7bc9\u3055\u308c\u308bAI\u3067\u3059\u3002Deep Q-Learning Network(DQN)\u3092\u4ee3\u8868\u3068\u3059\u308b\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u3068\u547c\u3070\u308c\u308b\u624b\u6cd5\u304c\u5c11\u3057\u524d\u304b\u3089\u8a71\u984c\u3067\u3059\u304c\u3001\u305d\u308c\u3089\u3082\u5f37\u5316\u5b66\u7fd2\u306e\u4e00\u7a2e\u306a\u306e\u3067\u3001\u52d5\u304b\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\n\n![f2c61cc2ad9485d2971c591b2362162e.png](https://qiita-image-store.s3.amazonaws.com/0/12325/b78f6d6e-af56-38b7-998a-c01987c4d467.png)\n\n\n\u6700\u8fd1\u8a71\u984c\u306b\u306a\u3063\u305f\u3001Google(Alphabet)\u5098\u4e0b\u306eDeepMind\u304c\u516c\u958b\u3057\u305fDeepMind Lab\u3068\u4f3c\u305f\u3088\u3046\u306a\u3082\u306e\u3067\u3001AI Gym\u306e\u65b9\u304c\u53e4\u682a\u3067\u3059\u3002\n\n\u30b0\u30fc\u30b0\u30eb\u306eDeepMind\u3001AI\u8a13\u7df4\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3092\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u5316\nhttp://japan.zdnet.com/article/35093248/\n\n# \u8a66\u3057\u3066\u307f\u3088\u3046\n\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n> pip install gym\n\n\u4ed6\u306b\u306f\u3001\u52d5\u753b\u306e\u751f\u6210\u306b\u5fc5\u8981\u306affmpeg\u304c\u8981\u6c42\u3055\u308c\u308b\u306e\u3067\u3001apt-get\u3084homebrew\u3067\u3088\u308d\u3057\u304f\u5165\u308c\u3066\u304a\u3044\u3066\u4e0b\u3055\u3044\u3002\n\n\u4e0b\u306f\u516c\u5f0f\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3001\n\n```python:test_gym.py\nimport gym\nenv = gym.make('CartPole-v0')\nenv.monitor.start('./cartpole-experiment')\nfor i_episode in range(20):\n    observation = env.reset()\n    for t in range(100):\n        env.render()\n        action = env.action_space.sample()\n        observation, reward, done, info = env.step(action)\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\n            \nenv.monitor.close()\ngym.upload('./cartpole-experiment', api_key='YOUR_API_KEY')\n```\n\n\u3053\u308c\u3092\u52d5\u304b\u3059\u3068\u3001\u52d5\u753b\u3084\u3089\u7d71\u8a08\u30c7\u30fc\u30bf\u304c\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3055\u308c\u3066\u3001\u4e0b\u306e\u3088\u3046\u306a\u30da\u30fc\u30b8\u304c\u3067\u304d\u307e\u3059\u3002\n\u306a\u304a\u3001API_KEY\u306fGithub\u30a2\u30ab\u30a6\u30f3\u30c8\u3067AI gym\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3068\u5165\u624b\u3067\u304d\u307e\u3059\u3002\n\nhttps://gym.openai.com/evaluations/eval_VqXE5bblQTCMQZKWmCJw\n\n\u5012\u7acb\u632f\u308a\u5b50\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3067\u3059\u3002\u4f55\u3082\u8003\u3048\u305a\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u30a2\u30af\u30b7\u30e7\u30f3(0:\u5de6\u79fb\u52d5, 1:\u53f3\u79fb\u52d5)\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u30a2\u30af\u30b7\u30e7\u30f3\u3068\u306f\u306a\u3093\u305e\u3084\uff1f\u3068\u3044\u3046\u65b9\u306f\u4e0b\u306e\u30ea\u30f3\u30af\u5148\u306a\u3069\u3067\u5f37\u5316\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8abf\u3079\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u30bc\u30ed\u304b\u3089Deep\u307e\u3067\u5b66\u3076\u5f37\u5316\u5b66\u7fd2\nhttp://qiita.com/icoxfog417/items/242439ecd1a477ece312\n\ngym\u306estep\u30e1\u30bd\u30c3\u30c9\u306f\u74b0\u5883\u60c5\u5831\u3001\u5831\u916c\u3001\u30a8\u30d4\u30bd\u30fc\u30c9\u304c\u7d42\u4e86\u6761\u4ef6\u306b\u9054\u3057\u305f\u304b\u3001\u306a\u3093\u304b\u53c2\u8003\u306e\u60c5\u5831\uff08\u3088\u304f\u308f\u304b\u3089\u306a\u3044\uff09\u306e4\u3064\u3092\u8fd4\u3057\u3066\u304d\u307e\u3059\u3002\u524d\u306e2\u3064\u306f\u5f37\u5316\u5b66\u7fd2\u3092\u56de\u3059\u3046\u3048\u3067\u306e\u4e2d\u5fc3\u3068\u306a\u308b\u60c5\u5831\u3067\u3001\u3053\u308c\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067AI\u306e\u5f37\u5316\u5b66\u7fd2\u304c\u53ef\u80fd\u3067\u3059\u3002\n\n# DQN\u3067\u5012\u7acb\u632f\u308a\u5b50\n\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n```python:cartpole_dqn.py\n# coding: utf-8\nimport numpy as np\nimport time\n\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\nimport gym\n\nnp.random.seed(7)\n\n# \u904e\u53bb\u4f55\u30b3\u30de\u3092\u898b\u308b\u304b\nSTATE_NUM = 4\n\n# DQN\u5185\u90e8\u3067\u4f7f\u308f\u308c\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\nclass Q(Chain):\n    def __init__(self,state_num=STATE_NUM):\n        super(Q,self).__init__(\n             l1=L.Linear(state_num, 16),  # state\u304c\u30a4\u30f3\u30d7\u30c3\u30c8\n             l2=L.Linear(16, 32),\n             l3=L.Linear(32, 64),\n             l4=L.Linear(64, 256),\n             l5=L.Linear(256, 2), # \u51fa\u529b2\u30c1\u30e3\u30cd\u30eb(Qvalue)\u304c\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8            \n        )\n\n    def __call__(self,x,t):\n        return F.mean_squared_error(self.predict(x,train=True),t)\n\n    def  predict(self,x,train=False):\n        h1 = F.leaky_relu(self.l1(x))\n        h2 = F.leaky_relu(self.l2(h1))\n        h3 = F.leaky_relu(self.l3(h2))\n        h4 =  F.leaky_relu(self.l4(h3))\n        y = F.leaky_relu(self.l5(h4))\n        return y\n\n# DQN\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3057\u305f\u304c\u3063\u3066\u52d5\u4f5c\u3059\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\nclass DQNAgent():\n    def __init__(self, epsilon=0.99):\n        self.model = Q()\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.epsilon = epsilon # \u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u9078\u3076\u78ba\u7387\n        self.actions=[0,1] #\u3000\u884c\u52d5\u306e\u9078\u629e\u80a2\n        self.experienceMemory = [] # \u7d4c\u9a13\u30e1\u30e2\u30ea\n        self.memSize = 300*100  # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306e\u30b5\u30a4\u30ba(300\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0x100\u30a8\u30d4\u30bd\u30fc\u30c9)\n        self.experienceMemory_local=[] # \u7d4c\u9a13\u30e1\u30e2\u30ea\uff08\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\uff09\n        self.memPos = 0 #\u30e1\u30e2\u30ea\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        self.batch_num = 32 # \u5b66\u7fd2\u306b\u4f7f\u3046\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n        self.gamma = 0.9       # \u5272\u5f15\u7387\n        self.loss=0\n        self.total_reward_award=np.ones(100)*-1000 #100\u30a8\u30d4\u30bd\u30fc\u30c9\n\n    def get_action_value(self, seq):\n        # seq\u5f8c\u306e\u884c\u52d5\u4fa1\u5024\u3092\u8fd4\u3059\n        x = Variable(np.hstack([seq]).astype(np.float32).reshape((1,-1)))\n        return self.model.predict(x).data[0]\n\n    def get_greedy_action(self, seq):\n        action_index = np.argmax(self.get_action_value(seq))\n        return self.actions[action_index]\n\n    def reduce_epsilon(self):\n        self.epsilon-=1.0/100000\n\n    def get_epsilon(self):\n        return self.epsilon\n\n    def get_action(self,seq,train):\n        '''\n        \u30a2\u30af\u30b7\u30e7\u30f3\u3092\u8fd4\u3059\u3002\n        '''\n        action=0\n        if train==True and np.random.random()<self.epsilon:\n            # random\n            action = np.random.choice(self.actions)\n        else:\n            # greedy\n            action= self.get_greedy_action(seq)\n        return action\n\n    def experience_local(self,old_seq, action, reward, new_seq):\n        #\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u8a18\u61b6\n        self.experienceMemory_local.append( np.hstack([old_seq,action,reward,new_seq]) )\n\n    def experience_global(self,total_reward):\n        #\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8a18\u61b6\n        #\u30d9\u30b9\u30c8100\u306b\u5165\u308b\u7d4c\u9a13\u3092\u53d6\u308a\u8fbc\u3080\n        if np.min(self.total_reward_award)<total_reward:\n            i=np.argmin(self.total_reward_award)\n            self.total_reward_award[i]=total_reward\n\n            # GOOD EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        #\u4e00\u5b9a\u78ba\u7387\u3067\u512a\u79c0\u3067\u306a\u3044\u3082\u306e\u3082\u53d6\u308a\u8fbc\u3080\n        if np.random.random()<0.01:\n            # # NORMAL EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        self.experienceMemory_local=[]\n\n    def experience(self,x):\n        if len(self.experienceMemory)>self.memSize:\n            self.experienceMemory[int(self.memPos%self.memSize)]=x\n            self.memPos+=1\n        else:\n            self.experienceMemory.append( x )\n\n    def update_model(self,old_seq, action, reward, new_seq):\n        '''\n        \u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n        '''\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306b\u305f\u307e\u3063\u3066\u306a\u3044\u5834\u5408\u306f\u66f4\u65b0\u3057\u306a\u3044\n        if len(self.experienceMemory)<self.batch_num:\n            return\n\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n        memsize=len(self.experienceMemory)\n        batch_index = list(np.random.randint(0,memsize,(self.batch_num)))\n        batch =np.array( [self.experienceMemory[i] for i in batch_index ])\n        x = Variable(batch[:,0:STATE_NUM].reshape( (self.batch_num,-1)).astype(np.float32))\n        targets=self.model.predict(x).data.copy()\n\n        for i in range(self.batch_num):\n            #[ seq..., action, reward, seq_new]\n            a = batch[i,STATE_NUM]\n            r = batch[i, STATE_NUM+1]\n            ai=int((a+1)/2) #\u00b11 \u3092index(0,1)\u306b\u3002\n            new_seq= batch[i,(STATE_NUM+2):(STATE_NUM*2+2)]\n            targets[i,ai]=( r+ self.gamma * np.max(self.get_action_value(new_seq)))\n        t = Variable(np.array(targets).reshape((self.batch_num,-1)).astype(np.float32)) \n\n        # \u30cd\u30c3\u30c8\u306e\u66f4\u65b0\n        self.model.zerograds()\n        loss=self.model(x ,t)\n        self.loss = loss.data\n        loss.backward()\n        self.optimizer.update()\n\nclass pendulumEnvironment():\n    '''\n    \u632f\u308a\u5b50\u74b0\u5883\u3002\n    '''\n    def __init__(self):\n        self.env = gym.make('CartPole-v0')\n        self.env.monitor.start('./cartpole-experiment')\n\n    def reset(self):\n        self.env.reset()\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def monitor_close(self):\n        self.env.monitor.close()\n        \n# \u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3002\nclass simulator:\n    def __init__(self, environment, agent):\n        self.agent = agent\n        self.env = environment\n\n        self.num_seq=STATE_NUM\n        self.reset_seq()\n        self.learning_rate=1.0\n        self.highscore=0\n        self.log=[]\n\n    def reset_seq(self):\n        self.seq=np.zeros(self.num_seq)\n\n    def push_seq(self, state):\n        self.seq[1:self.num_seq]=self.seq[0:self.num_seq-1]\n        self.seq[0]=state\n\n    def run(self, train=True):\n\n        self.env.reset()\n\n        self.reset_seq()\n        total_reward=0\n\n        for i in range(300):\n            # \u73fe\u5728\u306estate\u304b\u3089\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4fdd\u5b58\n            old_seq = self.seq.copy()\n\n            # \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u884c\u52d5\u3092\u6c7a\u3081\u308b\n            action = self.agent.get_action(old_seq,train)\n\n            # \u74b0\u5883\u306b\u884c\u52d5\u3092\u5165\u529b\u3059\u308b\n            observation, reward, done, info =  self.env.step(action)\n            total_reward +=reward\n\n            # \u7d50\u679c\u3092\u89b3\u6e2c\u3057\u3066state\u3068\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u66f4\u65b0\u3059\u308b\n            state = observation[2]\n            self.push_seq(state)\n            new_seq = self.seq.copy()\n\n            # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u8a18\u61b6\u3059\u308b\n            self.agent.experience_local(old_seq, action, reward, new_seq)\n\n            if done:\n                print(\"Episode finished after {} timesteps\".format(i+1))\n                break\n        \n        # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u5185\u5bb9\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u79fb\u3059\n        self.agent.experience_global(total_reward)\n\n        if train:\n            # \u5b66\u7fd2\u7528\u30e1\u30e2\u30ea\u3092\u4f7f\u3063\u3066\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n            self.agent.update_model(old_seq, action, reward, new_seq)\n            self.agent.reduce_epsilon()\n\n        return total_reward\n\nif __name__ == '__main__':\n    agent=DQNAgent()\n    env=pendulumEnvironment()\n    sim=simulator(env,agent)\n\n    best_reword = 0\n    for i in range(300000):\n        total_reword = sim.run(train=True)\n        if best_reword < total_reword:\n            best_reword = total_reword\n\n        print(str(i) + \" \" + str(total_reword) + \" \" + str(best_reword))            \n        env.reset()\n\n        if best_reword > 195:\n            break\n\n    env.monitor_close()\n    gym.upload('./cartpole-experiment', api_key='YOUR_API_KEY')\n```\n\n\u7d50\u679c\u306f\u3053\u3061\u3089\u3002\nhttps://gym.openai.com/evaluations/eval_zmiiqkPRgK10SOqMev3A#reproducibility\n\n\u5b9f\u88c5\u306b\u3042\u305f\u3063\u3066\u306f\u3001\u591a\u304f\u306e\u90e8\u5206\u3092\u4ee5\u4e0b\u306e\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u3092\u6d41\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u3044\u307e\u3059\u3002\n\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u5012\u7acb\u632f\u5b50\u3067\u5b66\u3076 DQN (Deep Q Network)\nhttp://qiita.com/ashitani/items/bb393e24c20e83e54577\n\n# 2\u8db3\u6b69\u884c\u30ed\u30dc\u3067DQN\n\u30a2\u30af\u30b7\u30e7\u30f3\u306f(-1,0,1)\u3092\u5024\u306b\u53d6\u308b4\u6b21\u5143\u914d\u5217\u306a\u306e\u3067\u6ce8\u610f\u3002\n\n\n```python:walker_dqn.py\n# coding: utf-8\nimport numpy as np\nimport time\n\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\nimport gym\n\nnp.random.seed(7)\n\nSTATE_NUM = 24\n\n# DQN\u5185\u90e8\u3067\u4f7f\u308f\u308c\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\nclass Q(Chain):\n    def __init__(self,state_num=STATE_NUM):\n        super(Q,self).__init__(\n             l1=L.Linear(state_num, 16),  # state\u304c\u30a4\u30f3\u30d7\u30c3\u30c8\n             l2=L.Linear(16, 32),\n             l3=L.Linear(32, 64),\n             l4=L.Linear(64, 256),\n             l5=L.Linear(256, 3*3*3*3), # \u51fa\u529b2\u30c1\u30e3\u30cd\u30eb(Qvalue)\u304c\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8            \n        )\n\n    def __call__(self,x,t):\n        return F.mean_squared_error(self.predict(x,train=True),t)\n\n    def  predict(self,x,train=False):\n        h1 = F.leaky_relu(self.l1(x))\n        h2 = F.leaky_relu(self.l2(h1))\n        h3 = F.leaky_relu(self.l3(h2))\n        h4 =  F.leaky_relu(self.l4(h3))\n        y = F.leaky_relu(self.l5(h4))\n        return y\n\n# DQN\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3057\u305f\u304c\u3063\u3066\u52d5\u4f5c\u3059\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\nclass DQNAgent():\n    def __init__(self, epsilon=0.99):\n        self.model = Q()\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.epsilon = epsilon # \u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u9078\u3076\u78ba\u7387\n        self.actions=[-1,0,1] #\u3000\u884c\u52d5\u306e\u9078\u629e\u80a2\n        self.experienceMemory = [] # \u7d4c\u9a13\u30e1\u30e2\u30ea\n        self.memSize = 300*100  # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306e\u30b5\u30a4\u30ba(300\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0x100\u30a8\u30d4\u30bd\u30fc\u30c9)\n        self.experienceMemory_local=[] # \u7d4c\u9a13\u30e1\u30e2\u30ea\uff08\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\uff09\n        self.memPos = 0 #\u30e1\u30e2\u30ea\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        self.batch_num = 32 # \u5b66\u7fd2\u306b\u4f7f\u3046\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n        self.gamma = 0.9       # \u5272\u5f15\u7387\n        self.loss=0\n        self.total_reward_award=np.ones(100)*-1000 #100\u30a8\u30d4\u30bd\u30fc\u30c9\n\n    def index_to_list(self, index):\n        ret_arr = []\n        a = int(index / 27) - 1\n        rest = index - 27*int(index / 27)\n        ret_arr.append(a)\n        a = int(rest / 9) - 1\n        rest = rest - 9*int(rest / 9)\n        ret_arr.append(a)\n        a = int(rest / 3) - 1\n        rest = rest - 3*int(rest / 3)\n        ret_arr.append(a)\n        ret_arr.append(rest -1)\n        \n        return ret_arr\n\n    def list_to_index(self, lst):\n        ret = 0\n\n        ret += (lst[0] + 1)*27\n        ret += (lst[1] + 1)*9\n        ret += (lst[2] + 1)*3\n        ret += (lst[3] + 1)\n        \n        return ret\n    \n    def get_action_value(self, seq):\n        # seq\u5f8c\u306e\u884c\u52d5\u4fa1\u5024\u3092\u8fd4\u3059\n        x = Variable(np.hstack([seq]).astype(np.float32).reshape((1,-1)))\n        return self.model.predict(x).data[0]\n\n    def get_greedy_action(self, seq):\n        action_index = np.argmax(self.get_action_value(seq))\n        return self.index_to_list(action_index)\n\n    def reduce_epsilon(self):\n        self.epsilon-=1.0/10000\n\n    def get_epsilon(self):\n        return self.epsilon\n\n    def get_action(self,seq,train):\n        '''\n        seq (env, old_env)\u306b\u5bfe\u3057\u3066\n        \u30a2\u30af\u30b7\u30e7\u30f3\u3092\u8fd4\u3059\u3002\n        '''\n        action=[]\n        if train==True and np.random.random()<self.epsilon:\n            # random\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n            action.append(np.random.choice(self.actions))\n        else:\n            # greedy\n            action= self.get_greedy_action(seq)\n        return action\n\n    def experience_local(self,old_seq, action, reward, new_seq):\n        #\u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u8a18\u61b6\n        self.experienceMemory_local.append( np.hstack([old_seq,action,reward,new_seq]) )\n\n    def experience_global(self,total_reward):\n        #\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u8a18\u61b6\n        #\u30d9\u30b9\u30c8100\u306b\u5165\u308b\u7d4c\u9a13\u3092\u53d6\u308a\u8fbc\u3080\n        if np.min(self.total_reward_award)<total_reward:\n            i=np.argmin(self.total_reward_award)\n            self.total_reward_award[i]=total_reward\n\n            # GOOD EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        #\u4e00\u5b9a\u78ba\u7387\u3067\u512a\u79c0\u3067\u306a\u3044\u3082\u306e\u3082\u53d6\u308a\u8fbc\u3080\n        if np.random.random()<0.01:\n            # # NORMAL EXPERIENCE REPLAY\n            for x in self.experienceMemory_local:\n                self.experience( x )\n\n        self.experienceMemory_local=[]\n\n    def experience(self,x):\n        if len(self.experienceMemory)>self.memSize:\n            self.experienceMemory[int(self.memPos%self.memSize)]=x\n            self.memPos+=1\n        else:\n            self.experienceMemory.append( x )\n\n    def update_model(self,old_seq, action, reward, new_seq):\n        '''\n        \u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n        '''\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u306b\u305f\u307e\u3063\u3066\u306a\u3044\u5834\u5408\u306f\u66f4\u65b0\u3057\u306a\u3044\n        if len(self.experienceMemory)<self.batch_num:\n            return\n\n        # \u7d4c\u9a13\u30e1\u30e2\u30ea\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n        memsize=len(self.experienceMemory)\n        batch_index = list(np.random.randint(0,memsize,(self.batch_num)))\n        batch =np.array( [self.experienceMemory[i] for i in batch_index ])\n        x = Variable(batch[:,0:STATE_NUM].reshape( (self.batch_num,-1)).astype(np.float32))\n        targets=self.model.predict(x).data.copy()\n\n        for i in range(self.batch_num):\n            #[ seq..., action, reward, seq_new]\n            a = batch[i,STATE_NUM]\n            r = batch[i, STATE_NUM+1]\n            ai=a\n            new_seq= batch[i,(STATE_NUM+2):(STATE_NUM*2+2)]\n            targets[i,ai]=( r+ self.gamma * np.max(self.get_action_value(new_seq)))\n        t = Variable(np.array(targets).reshape((self.batch_num,-1)).astype(np.float32)) \n\n        # \u30cd\u30c3\u30c8\u306e\u66f4\u65b0\n        self.model.zerograds()\n        loss=self.model(x ,t)\n        self.loss = loss.data\n        loss.backward()\n        self.optimizer.update()\n\nclass walkerEnvironment():\n    def __init__(self):\n        self.env = gym.make('BipedalWalkerHardcore-v2')\n        self.env.monitor.start('./walker-experiment')\n\n    def reset(self):\n        self.env.reset()\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def monitor_close(self):\n        self.env.monitor.close()\n        \n# \u30b7\u30df\u30e5\u30ec\u30fc\u30bf\u3002\nclass simulator:\n    def __init__(self, environment, agent):\n        self.agent = agent\n        self.env = environment\n\n        self.num_seq=STATE_NUM\n        self.reset_seq()\n        self.learning_rate=1.0\n        self.highscore=0\n        self.log=[]\n\n    def reset_seq(self):\n        self.seq=np.zeros(self.num_seq)\n\n    def push_seq(self, state):\n        self.seq = state\n\n    def run(self, train=True):\n\n        self.env.reset()\n\n        self.reset_seq()\n        total_reward=0\n\n        for i in range(100000):\n            # \u73fe\u5728\u306estate\u304b\u3089\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4fdd\u5b58\n            old_seq = self.seq.copy()\n\n            # \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u884c\u52d5\u3092\u6c7a\u3081\u308b\n            action = self.agent.get_action(old_seq,train)\n\n            # \u74b0\u5883\u306b\u884c\u52d5\u3092\u5165\u529b\u3059\u308b\n            observation, reward, done, info =  self.env.step(action)\n            total_reward +=reward\n\n            # \u7d50\u679c\u3092\u89b3\u6e2c\u3057\u3066state\u3068\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u66f4\u65b0\u3059\u308b\n            state = observation\n            self.push_seq(state)\n            new_seq = self.seq.copy()\n\n            # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u8a18\u61b6\u3059\u308b\n            action_idx = self.agent.list_to_index(action)\n            self.agent.experience_local(old_seq, action_idx, reward, new_seq)\n\n            if done:\n                print(\"Episode finished after {} timesteps\".format(i+1))\n                break\n        \n        # \u30a8\u30d4\u30bd\u30fc\u30c9\u30ed\u30fc\u30ab\u30eb\u306a\u30e1\u30e2\u30ea\u5185\u5bb9\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30e1\u30e2\u30ea\u306b\u79fb\u3059\n        self.agent.experience_global(total_reward)\n\n        if train:\n            # \u5b66\u7fd2\u7528\u30e1\u30e2\u30ea\u3092\u4f7f\u3063\u3066\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3059\u308b\n            action_idx = self.agent.list_to_index(action)\n            self.agent.update_model(old_seq, action_idx, reward, new_seq)\n            self.agent.reduce_epsilon()\n\n        return total_reward\n\nif __name__ == '__main__':\n    agent=DQNAgent()\n    env=walkerEnvironment()\n    sim=simulator(env,agent)\n\n    best_reword = -200\n    for i in range(10000):\n        total_reword = sim.run(train=True)\n        if best_reword < total_reword:\n            best_reword = total_reword\n\n        print(str(i) + \" \" + str(total_reword) + \" \" + str(best_reword))            \n        env.reset()\n\n        if best_reword > 200:\n            break\n\n    env.monitor_close()\n    gym.upload('./walker-experiment', api_key='YOUR_API_KEY')\n```\n\n\u7d50\u679c\u306f\u3053\u3061\u3089\u3002\nhttps://gym.openai.com/evaluations/eval_qaM0iTJSST2DDd5Br0HE6A#reproducibility\n\n\u751f\u307e\u308c\u305f\u3066\u306e\u5c0f\u9e7f\u306b\u3057\u304b\u306a\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\n\n10000\u30a8\u30d4\u30bd\u30fc\u30c9\u5b66\u7fd2\u3059\u308b\u904e\u7a0b\u3067\u5f97\u3089\u308c\u305f award (\u5831\u916c) \u306e\u63a8\u79fb\u3092\u898b\u308b\u3068\u3001\u7d42\u76e4\u306b\u826f\u3044\u30b9\u30b3\u30a2\u3092\u8a18\u9332\u3057\u3066\u3044\u308b\u69d8\u5b50\u304c\u898b\u3066\u53d6\u308c\u308b\u3002\n\u305f\u3060\u3001\u5168\u4f53\u7684\u306b\u306f\u4e0b\u304c\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u3082\u898b\u3048\u3066\u4e0d\u601d\u8b70\u3002\n![bfc953f591940f2563d0b9dc54a8fb71.png](https://qiita-image-store.s3.amazonaws.com/0/12325/b947c010-f0bb-2145-c2b3-0e5b8e390980.png)\n\n\n\u305d\u3053\u3067\u3001100\u30a8\u30d4\u30bd\u30fc\u30c9\u3067\u79fb\u52d5\u5e73\u5747\u3092\u3068\u308b\u3068\u3001\u307e\u3042\u3061\u3083\u3093\u3068\u5b66\u7fd2\u3057\u3066\u308b\u3063\u307d\u3044\u3053\u3068\u304c\u8aad\u307f\u53d6\u308c\u308b\u3002\n\u3088\u304b\u3063\u305f\u3088\u304b\u3063\u305f\u3002\n![d16af146b0b7207a8f9519465baa2532.png](https://qiita-image-store.s3.amazonaws.com/0/12325/21a1ebf5-6e54-ed86-6746-409eaf981ca3.png)\n\n\n\u306a\u304a\u3001\u4e0a\u8a18\u306eAI\u306f\u74b0\u5883\u306e\u60c5\u5831\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30b9\u3092\u610f\u8b58\u3059\u308b\u3053\u3068\u306a\u304f\u52d5\u4f5c\u3057\u3066\u3044\u307e\u3059\u3002\n\u3069\u3046\u3082\u3001\u826f\u304f\u3067\u304d\u305fAI\u3068\u3044\u3046\u306e\u306f\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u610f\u8b58\u3057\u306a\u304f\u3066\u3082\u826f\u3044\u306e\u3060\u3001\u3068\u5049\u3044\u4eba\u304b\u826f\u304f\u308f\u304b\u3089\u306a\u3044\u4eba\u304c\u8a00\u3063\u3066\u3044\u308b\u306e\u3092\u30cd\u30c3\u30c8\u3067\u898b\u307e\u3057\u305f\u3002\u306e\u3067\u3001\u305d\u308c\u306b\u5f93\u3063\u3066\u3044\u307e\u3059\uff08Gym\u5074\u306f\u74b0\u5883\u306e\u5404\u60c5\u5831\u306e\u30bb\u30de\u30f3\u30c6\u30a3\u30af\u30b9\u3092\u6559\u3048\u3066\u304f\u308c\u306a\u3044\u306e\u3067\u610f\u8b58\u3057\u3088\u3046\u304c\u306a\u3044\u3068\u3044\u3046\u8a71\u3082\u3042\u308b\uff09\u3002\n\n\u3061\u306a\u307f\u306b\u3001\u3088\u304f\u3067\u304d\u305f\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3060\u3068\u3053\u3093\u306a\u611f\u3058\u3067\u6b69\u304d\u307e\u3059\u3002\nhttps://gym.openai.com/evaluations/eval_Gxwojs5T5a8jayWaVwFA\n\n\u73fe\u5834\u304b\u3089\u306f\u4ee5\u4e0a\u3067\u3059\u3002\n\n# (12/19\u8ffd\u8a18)\n\u4e8c\u8db3\u6b69\u884c\u30ed\u30dc\u3001\u8ee2\u304c\u3063\u3066\u305f\u30b3\u30fc\u30c9\u3092\u771f\u4f3c\u3057\u305f\u3089\u3001\u305d\u308c\u306a\u308a\u306b\u6b69\u304f\u3088\u3046\u306b\u3067\u3051\u305f\uff01\nhttps://gym.openai.com/evaluations/eval_wzmU0dtQCeoz2dWyhsD5Q\n\nNeuroevolution\u3068\u547c\u3070\u308c\u308b\u6280\u8853\u306e\u4e2d\u306e\u4e00\u624b\u6cd5\u3067\u3042\u308bNEAT(NeuroEvolution of Augmenting Topologies)\u3068\u3044\u3046\u3084\u3064\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\u3056\u3063\u304f\u308a\u8a00\u3046\u3068\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7d50\u5408\uff1f\u306e\u6700\u9069\u5316\u3092\u3057\u305f\u308a\u3059\u308b\u624b\u6cd5\u307f\u305f\u3044\u3067\u3059\u3002\n\n# (1/10\u8ffd\u8a18)\n\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u306b\u3064\u3044\u3066\u66f8\u304d\u307e\u3057\u305f\nhttp://qiita.com/ryo_grid/items/af60750659d1d7ffeef9\n\n", "tags": ["DeepLearning", "AI", "OpenAIGym", "\u6a5f\u68b0\u5b66\u7fd2", "DQN"]}