{"context": "\n\n\u306f\u3058\u3081\u306b\nPython\u3067\u306f\u3044\u304f\u3064\u304b\u7dda\u5f62\u56de\u5e30\u3092\u3059\u308b\u305f\u3081\u306b\u4f7f\u3048\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u3042\u308a\u307e\u3059\u3002\u500b\u4eba\u7684\u306b\u7dda\u5f62\u56de\u5e30\u3092\u3059\u308b\u5fc5\u8981\u306b\u305b\u307e\u3089\u308c\u3001\u305d\u306e\u305f\u3081\u306e\u65b9\u6cd5\u3092\u8abf\u3079\u305f\u306e\u3067\u30e1\u30e2\u3092\u517c\u306d\u3066\u30b7\u30a7\u30a2\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4f7f\u3063\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u4ee5\u4e0b\uff1a\n- statmodels \n- scikit-learn\n- PyMC3\n\n\u30c7\u30fc\u30bf\u6e96\u5099\n\u307e\u305a\u9069\u5f53\u306a\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u307e\u3059\u3002\u4eca\u56de\u306f\u4ee5\u4e0b\u306e\u5f0f\u306b\u30ce\u30a4\u30ba \u03f5\u03f5\\epsilon \u3092\u52a0\u3048\u305f\u4eba\u5de5\u30c7\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\u3002\ny=\u03b20+\u03b21x1+\u03b22x2+\u03f5y=\u03b20+\u03b21x1+\u03b22x2+\u03f5y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x2 + \\epsilon\n\u3053\u3053\u3067\u63a8\u5b9a\u3059\u308b\u306e\u306f\u3001\u03b20,\u03b21,\u03b22\u03b20,\u03b21,\u03b22\\beta_0, \\beta_1, \\beta_2\u306e\u5024\u3002\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generate random data\nbeta = [1.2, 0.5]\n# prep data\nx1 = np.random.random(size=1000)*5 \nx2 = np.random.random(size=1000)*10 \nx = np.transpose([x1, x2])\ny = np.dot(x, beta)  + np.random.normal(scale=0.5, size=1000)\n#data = dict(x=x, y=y)\ndata = dict(x1=x1, x2=x2, y=y)\ndf = pd.DataFrame(data)\n# visualisation\nplt.scatter(x1, y, color='b')\nplt.scatter(x2, y, color='orange')\n# 3D\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(x1, x2, y)\nplt.show()\n\n\n\n\n\nStatmodels\u3092\u4f7f\u3046\u5834\u5408\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)\nresults = sm.OLS(endog=y, exog=x).fit()\nresults.summary()\n\n\n\nOLS Regression Results\n\nDep. Variable: y   R-squared:             0.951\n\n\nModel: OLS   Adj. R-squared:        0.951\n\n\nMethod: Least Squares   F-statistic:           9745.\n\n\nDate: Fri, 10 Mar 2017   Prob (F-statistic):   0.00\n\n\nTime: 09:58:59   Log-Likelihood:      -724.14\n\n\nNo. Observations:   1000   AIC:                   1454.\n\n\nDf Residuals:    997   BIC:                   1469.\n\n\nDf Model:      2    \n\n\nCovariance Type: nonrobust    \n\n\n\n\n coef std err t P>|t| [95.0% Conf. Int.]\n\n\nconst     0.0499     0.042     1.181  0.238    -0.033     0.133\n\n\nx1     1.1823     0.011   107.081  0.000     1.161     1.204\n\n\nx2     0.4983     0.005    91.004  0.000     0.488     0.509\n\n\n\n\nOmnibus:  0.654   Durbin-Watson:         2.079\n\n\nProb(Omnibus):  0.721   Jarque-Bera (JB):      0.599\n\n\nSkew: -0.059   Prob(JB):              0.741\n\n\nKurtosis:  3.023   Cond. No.               17.2\n\n\n\nscikit-learn\u3092\u4f7f\u3046\u5834\u5408\nfrom sklearn import linear_model\n# compare different regressions\nlreg = linear_model.LinearRegression()\nlreg.fit(x, y)\nprint(\"Linear regression: \\t\", lreg.coef_)\nbreg = linear_model.BayesianRidge()\nbreg.fit(x, y)\nprint(\"Bayesian regression: \\t\", breg.coef_)\nereg = linear_model.ElasticNetCV()\nereg.fit(x, y)\nprint(\"Elastic net:  \\t\\t\", ereg.coef_)\nprint(\"true parameter values: \\t\", beta)\n\nLinear regression:   [ 1.18232244  0.49832431]\nBayesian regression:     [ 1.18214701  0.49830501]\nElastic net:         [ 1.17795855  0.49756084]\ntrue parameter values:   [1.2, 0.5]\n\n\nPyMC3\u3092\u4f7f\u3046\u5834\u5408\n\u4e0a\uff12\u3064\u306e\u5834\u5408\u3067\u306f\u6700\u5c24\u63a8\u5b9a\u3092\u4f7f\u3063\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001PyMC3\u3067\u306f\u30d9\u30a4\u30ba\u5b9a\u7406\u306b\u57fa\u3065\u3044\u3066\u30de\u30eb\u30b3\u30d5\u9023\u9396\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u6cd5\uff08MCMC)\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u78ba\u7387\u5206\u5e03\u3092\u6c42\u3081\u307e\u3059\u3002\nimport pymc3 as pm\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.Normal()\n    #pm.glm.glm('y ~ x', data, family=family)\n    pm.glm.glm('y ~ x1+x2', df, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    #step = pm.Metropolis()\n    trace_robust = pm.sample(25000, step)\n\nOptimization terminated successfully.\n         Current function value: 764.008811\n         Iterations: 19\n         Function evaluations: 30\n         Gradient evaluations: 30\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [00:32<00:00, 761.52it/s]\n\n# show results\npm.traceplot(trace_robust[2000:])\n#pm.summary(trace_robust[2000:])\nplt.show()\npm.df_summary(trace_robust[2000:])\n\n\n\u3000\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\n\n\n\n\nIntercept\n-0.022296\n0.040946\n0.000564\n-0.100767\n0.057460\n\n\nx1\n1.199371\n0.011235\n0.000126\n1.177191\n1.221067\n\n\nx2\n0.500502\n0.005346\n0.000057\n0.490258\n0.511003\n\n\nsd\n0.490271\n0.010949\n0.000072\n0.469599\n0.512102\n\n\n\n\n\n\u3082\u3046\u3061\u3087\u3063\u3068\u8907\u96d1\u306a\u5834\u5408\u3067\u3084\u3063\u3066\u307f\u308b\nPyMC3\u304c\u3069\u3053\u307e\u3067\u3067\u304d\u308b\u306e\u304b\u898b\u308b\u305f\u3081\u306b\u3001\u3082\u3046\u3061\u3087\u3063\u3068\u8907\u96d1\u306a\u5834\u5408\u3067\u3084\u3063\u3066\u307f\u307e\u3059\u3002\u5143\u306b\u306a\u308b\u5f0f\u306f\u4ee5\u4e0b\uff1a\ny=\u03b20+\u03b21x1+\u03b22x2+\u03b23x3+\u03b24x4+\u03b25x5+\u03f5y=\u03b20+\u03b21x1+\u03b22x2+\u03b23x3+\u03b24x4+\u03b25x5+\u03f5y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\epsilon\n# Generate random data\nbeta = [1.2, 0.5, 9.8, 0.2, 1.08]\n# prep data\nx1 = np.random.random(size=1000)*5 \nx2 = np.random.random(size=1000)*10 \nx3 = np.random.random(size=1000)\nx4 = np.random.normal(size=1000)*2 \nx5 = np.random.random(size=1000)*60 \nx = np.transpose([x1, x2, x3, x4, x5])\ny = np.dot(x, beta)  + np.random.normal(scale=0.5, size=1000)\n#data = dict(x=x, y=y)\ndata = dict(x1=x1, x2=x2, x3=x3, x4=x4, x5=x5, y=y)\ndf = pd.DataFrame(data)\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.Normal()\n    pm.glm.glm('y ~ x1+x2+x3+x4+x5', df, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    #step = pm.Metropolis()\n    trace_robust = pm.sample(25000, step)\n\n# show results\npm.traceplot(trace_robust[2000:])\nplt.show()\nprint(\"true parameter values are:\", beta)\npm.df_summary(trace_robust[2000:])\n\n\ntrue parameter values are: [1.2, 0.5, 9.8, 0.2, 1.08]\n\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\n\n\n\n\nIntercept\n0.041924\n0.059770\n0.000737\n-0.080421\n0.154130\n\n\nx1\n1.199973\n0.011466\n0.000106\n1.177061\n1.222395\n\n\nx2\n0.494488\n0.005656\n0.000053\n0.483624\n0.505661\n\n\nx3\n9.699889\n0.056527\n0.000484\n9.587273\n9.809352\n\n\nx4\n0.197271\n0.008424\n0.000052\n0.181196\n0.214320\n\n\nx5\n1.081120\n0.000922\n0.000008\n1.079339\n1.082917\n\n\nsd\n0.514316\n0.011438\n0.000067\n0.492296\n0.536947\n\n\n\n\n\n\u304a\u308f\u308a\u306b\n\u77ed\u3044\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u308b\u306e\u3067\u7279\u306b\u89e3\u8aac\u306f\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u3069\u308c\u3082\u3046\u307e\u304f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3067\u304d\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u3082\u3057\u4f55\u304b\u8cea\u554f\u304c\u3042\u308c\u3070\u9060\u616e\u306a\u304f\u30b3\u30e1\u30f3\u30c8\u304f\u3060\u3055\u3044\u3002\n\uff08\u8ffd\u8a18\uff09 \u51fa\u529b\u753b\u50cf\u304c\u9593\u9055\u3063\u3066\u3044\u305f\u306e\u3067\u4fee\u6b63\u3002\n# \u306f\u3058\u3081\u306b\n\nPython\u3067\u306f\u3044\u304f\u3064\u304b\u7dda\u5f62\u56de\u5e30\u3092\u3059\u308b\u305f\u3081\u306b\u4f7f\u3048\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u3042\u308a\u307e\u3059\u3002\u500b\u4eba\u7684\u306b\u7dda\u5f62\u56de\u5e30\u3092\u3059\u308b\u5fc5\u8981\u306b\u305b\u307e\u3089\u308c\u3001\u305d\u306e\u305f\u3081\u306e\u65b9\u6cd5\u3092\u8abf\u3079\u305f\u306e\u3067\u30e1\u30e2\u3092\u517c\u306d\u3066\u30b7\u30a7\u30a2\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4f7f\u3063\u305f\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u4ee5\u4e0b\uff1a\n- statmodels \n- scikit-learn\n- PyMC3\n\n\n## \u30c7\u30fc\u30bf\u6e96\u5099\n\n\u307e\u305a\u9069\u5f53\u306a\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u307e\u3059\u3002\u4eca\u56de\u306f\u4ee5\u4e0b\u306e\u5f0f\u306b\u30ce\u30a4\u30ba $\\epsilon$ \u3092\u52a0\u3048\u305f\u4eba\u5de5\u30c7\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x2 + \\epsilon$$\n\n\u3053\u3053\u3067\u63a8\u5b9a\u3059\u308b\u306e\u306f\u3001$\\beta_0, \\beta_1, \\beta_2$\u306e\u5024\u3002\n\n```python\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generate random data\nbeta = [1.2, 0.5]\n# prep data\nx1 = np.random.random(size=1000)*5 \nx2 = np.random.random(size=1000)*10 \nx = np.transpose([x1, x2])\ny = np.dot(x, beta)  + np.random.normal(scale=0.5, size=1000)\n#data = dict(x=x, y=y)\ndata = dict(x1=x1, x2=x2, y=y)\ndf = pd.DataFrame(data)\n# visualisation\nplt.scatter(x1, y, color='b')\nplt.scatter(x2, y, color='orange')\n# 3D\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(x1, x2, y)\nplt.show()\n\n```\n\n![output_2_0.png](https://qiita-image-store.s3.amazonaws.com/0/154129/a94c6ac7-7ec0-59c4-f111-9b4a124a5cde.png)\n![output_2_1.png](https://qiita-image-store.s3.amazonaws.com/0/154129/2deaa2f7-a70a-48e7-ac14-f3596fba87ef.png)\n\n\n\n## Statmodels\u3092\u4f7f\u3046\u5834\u5408\n\n```python\nimport statsmodels.api as sm\n\nx = sm.add_constant(x)\nresults = sm.OLS(endog=y, exog=x).fit()\nresults.summary()\n```\n\n\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9745.</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Fri, 10 Mar 2017</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>09:58:59</td>     <th>  Log-Likelihood:    </th> <td> -724.14</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   1454.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   997</td>      <th>  BIC:               </th> <td>   1469.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n</tr>\n<tr>\n  <th>const</th> <td>    0.0499</td> <td>    0.042</td> <td>    1.181</td> <td> 0.238</td> <td>   -0.033     0.133</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    1.1823</td> <td>    0.011</td> <td>  107.081</td> <td> 0.000</td> <td>    1.161     1.204</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.4983</td> <td>    0.005</td> <td>   91.004</td> <td> 0.000</td> <td>    0.488     0.509</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.654</td> <th>  Durbin-Watson:     </th> <td>   2.079</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.721</td> <th>  Jarque-Bera (JB):  </th> <td>   0.599</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.059</td> <th>  Prob(JB):          </th> <td>   0.741</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.023</td> <th>  Cond. No.          </th> <td>    17.2</td>\n</tr>\n</table>\n\n\n\n## scikit-learn\u3092\u4f7f\u3046\u5834\u5408\n\n\n```python\nfrom sklearn import linear_model\n# compare different regressions\nlreg = linear_model.LinearRegression()\nlreg.fit(x, y)\nprint(\"Linear regression: \\t\", lreg.coef_)\nbreg = linear_model.BayesianRidge()\nbreg.fit(x, y)\nprint(\"Bayesian regression: \\t\", breg.coef_)\nereg = linear_model.ElasticNetCV()\nereg.fit(x, y)\nprint(\"Elastic net:  \\t\\t\", ereg.coef_)\nprint(\"true parameter values: \\t\", beta)\n```\n\n    Linear regression: \t [ 1.18232244  0.49832431]\n    Bayesian regression: \t [ 1.18214701  0.49830501]\n    Elastic net:  \t\t [ 1.17795855  0.49756084]\n    true parameter values: \t [1.2, 0.5]\n\n\n## PyMC3\u3092\u4f7f\u3046\u5834\u5408\n\n\u4e0a\uff12\u3064\u306e\u5834\u5408\u3067\u306f\u6700\u5c24\u63a8\u5b9a\u3092\u4f7f\u3063\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001PyMC3\u3067\u306f\u30d9\u30a4\u30ba\u5b9a\u7406\u306b\u57fa\u3065\u3044\u3066\u30de\u30eb\u30b3\u30d5\u9023\u9396\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u6cd5\uff08MCMC)\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u78ba\u7387\u5206\u5e03\u3092\u6c42\u3081\u307e\u3059\u3002\n\n\n```python\nimport pymc3 as pm\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.Normal()\n    #pm.glm.glm('y ~ x', data, family=family)\n    pm.glm.glm('y ~ x1+x2', df, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    #step = pm.Metropolis()\n    trace_robust = pm.sample(25000, step)\n```\n\n    Optimization terminated successfully.\n             Current function value: 764.008811\n             Iterations: 19\n             Function evaluations: 30\n             Gradient evaluations: 30\n\n\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [00:32<00:00, 761.52it/s]\n\n\n```python\n# show results\npm.traceplot(trace_robust[2000:])\n#pm.summary(trace_robust[2000:])\nplt.show()\npm.df_summary(trace_robust[2000:])\n```\n![output_9_0.png](https://qiita-image-store.s3.amazonaws.com/0/154129/abe56f1c-aa48-a75b-1d72-d6024454b1be.png)\n\u3000\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>mc_error</th>\n      <th>hpd_2.5</th>\n      <th>hpd_97.5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Intercept</th>\n      <td>-0.022296</td>\n      <td>0.040946</td>\n      <td>0.000564</td>\n      <td>-0.100767</td>\n      <td>0.057460</td>\n    </tr>\n    <tr>\n      <th>x1</th>\n      <td>1.199371</td>\n      <td>0.011235</td>\n      <td>0.000126</td>\n      <td>1.177191</td>\n      <td>1.221067</td>\n    </tr>\n    <tr>\n      <th>x2</th>\n      <td>0.500502</td>\n      <td>0.005346</td>\n      <td>0.000057</td>\n      <td>0.490258</td>\n      <td>0.511003</td>\n    </tr>\n    <tr>\n      <th>sd</th>\n      <td>0.490271</td>\n      <td>0.010949</td>\n      <td>0.000072</td>\n      <td>0.469599</td>\n      <td>0.512102</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## \u3082\u3046\u3061\u3087\u3063\u3068\u8907\u96d1\u306a\u5834\u5408\u3067\u3084\u3063\u3066\u307f\u308b\n\nPyMC3\u304c\u3069\u3053\u307e\u3067\u3067\u304d\u308b\u306e\u304b\u898b\u308b\u305f\u3081\u306b\u3001\u3082\u3046\u3061\u3087\u3063\u3068\u8907\u96d1\u306a\u5834\u5408\u3067\u3084\u3063\u3066\u307f\u307e\u3059\u3002\u5143\u306b\u306a\u308b\u5f0f\u306f\u4ee5\u4e0b\uff1a\n\n$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\epsilon$\n\n\n\n```python\n# Generate random data\nbeta = [1.2, 0.5, 9.8, 0.2, 1.08]\n# prep data\nx1 = np.random.random(size=1000)*5 \nx2 = np.random.random(size=1000)*10 \nx3 = np.random.random(size=1000)\nx4 = np.random.normal(size=1000)*2 \nx5 = np.random.random(size=1000)*60 \nx = np.transpose([x1, x2, x3, x4, x5])\ny = np.dot(x, beta)  + np.random.normal(scale=0.5, size=1000)\n#data = dict(x=x, y=y)\ndata = dict(x1=x1, x2=x2, x3=x3, x4=x4, x5=x5, y=y)\ndf = pd.DataFrame(data)\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.Normal()\n    pm.glm.glm('y ~ x1+x2+x3+x4+x5', df, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    #step = pm.Metropolis()\n    trace_robust = pm.sample(25000, step)\n```\n\n\n```python\n# show results\npm.traceplot(trace_robust[2000:])\nplt.show()\nprint(\"true parameter values are:\", beta)\npm.df_summary(trace_robust[2000:])\n```\n![output_13_0.png](https://qiita-image-store.s3.amazonaws.com/0/154129/13c85d7b-5046-25a6-4a17-c80a437df00e.png)\n\n\n    true parameter values are: [1.2, 0.5, 9.8, 0.2, 1.08]\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>mc_error</th>\n      <th>hpd_2.5</th>\n      <th>hpd_97.5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Intercept</th>\n      <td>0.041924</td>\n      <td>0.059770</td>\n      <td>0.000737</td>\n      <td>-0.080421</td>\n      <td>0.154130</td>\n    </tr>\n    <tr>\n      <th>x1</th>\n      <td>1.199973</td>\n      <td>0.011466</td>\n      <td>0.000106</td>\n      <td>1.177061</td>\n      <td>1.222395</td>\n    </tr>\n    <tr>\n      <th>x2</th>\n      <td>0.494488</td>\n      <td>0.005656</td>\n      <td>0.000053</td>\n      <td>0.483624</td>\n      <td>0.505661</td>\n    </tr>\n    <tr>\n      <th>x3</th>\n      <td>9.699889</td>\n      <td>0.056527</td>\n      <td>0.000484</td>\n      <td>9.587273</td>\n      <td>9.809352</td>\n    </tr>\n    <tr>\n      <th>x4</th>\n      <td>0.197271</td>\n      <td>0.008424</td>\n      <td>0.000052</td>\n      <td>0.181196</td>\n      <td>0.214320</td>\n    </tr>\n    <tr>\n      <th>x5</th>\n      <td>1.081120</td>\n      <td>0.000922</td>\n      <td>0.000008</td>\n      <td>1.079339</td>\n      <td>1.082917</td>\n    </tr>\n    <tr>\n      <th>sd</th>\n      <td>0.514316</td>\n      <td>0.011438</td>\n      <td>0.000067</td>\n      <td>0.492296</td>\n      <td>0.536947</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n# \u304a\u308f\u308a\u306b\n\n\u77ed\u3044\u30b3\u30fc\u30c9\u3067\u66f8\u3051\u308b\u306e\u3067\u7279\u306b\u89e3\u8aac\u306f\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u3069\u308c\u3082\u3046\u307e\u304f\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a8\u5b9a\u3067\u304d\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u3082\u3057\u4f55\u304b\u8cea\u554f\u304c\u3042\u308c\u3070\u9060\u616e\u306a\u304f\u30b3\u30e1\u30f3\u30c8\u304f\u3060\u3055\u3044\u3002\n\n**\uff08\u8ffd\u8a18\uff09** \u51fa\u529b\u753b\u50cf\u304c\u9593\u9055\u3063\u3066\u3044\u305f\u306e\u3067\u4fee\u6b63\u3002\n\n", "tags": ["Python", "statmodels", "scikit-learn", "PyMC3"]}