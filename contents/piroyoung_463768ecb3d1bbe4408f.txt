{"context": " More than 1 year has passed since last update.CSV/TSV\u3092\u6b63\u3057\u3044\u578b\u3067\u30d1\u30fc\u30b9\u3059\u308b\u306e\u306f\u9aa8\u304c\u6298\u308c\u308b\uff0e\u305d\u306e\u6642\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3092\u30e1\u30e2\u4ee3\u308f\u308a\u306b\uff0e\n\u53c2\u8003\uff1ahttps://spark.apache.org/docs/latest/sql-programming-guide.html\n\n\u524d\u63d0\nSparkContext\u3084SQLContext\u306f\u30b7\u30f3\u30b0\u30eb\u30c8\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u6301\u3063\u3066\u304a\u304f.\npackage org.your.own.domain.utils\n\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkContext, SparkConf}\n\n/**\n * Created by piroyoung on 6/26/15.\n */\nobject SparkInfo {\n  val conf = new SparkConf()\n    .setAppName(\"PickTsv\")\n    .set(\"spark.storage.memoryFraction\", \"1\")\n    .set(\"spark.rdd.compress\", \"true\")\n    .set(\"spark.core.connection.ack.wait.timeout\", \"6000\")\n    .set(\"spark.akka.frameSize\", \"50\")\n    .set(\"spark.files.overwrite\", \"true\")\n\n  val sc = new SparkContext(conf)\n  val sqlc = new SQLContext(sc)\n}\n\n\n\u4ee5\u5f8cRDD\u3092\u6271\u3046\u969b\u306f\u3053\u306e\u30b7\u30f3\u30b0\u30eb\u30c8\u30f3\u3092import\u3057\u3066\u4f7f\u3046\uff0e\n\n\u8f9b\u3044\u4f8b\uff1aRow + Schema\u3067\u751f\u6210\norg.apache.spark.sql.Row\u3068\u3044\u3046\u30af\u30e9\u30b9\u304b\u3089\u751f\u6210\u3057\u305fRDD[Row]\u3068\uff0eSchema\u3092\u5408\u308f\u305b\u3066DataFrame\u3092\u751f\u6210\u3059\u308b\npackage org.your.own.domain\nimport org.your.own.domain.util._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\n\nval RowRDD: RDD[Row] = SparkInfo.sc.textFile(\"yourData.csv\")\n  .map(_.split(\",\"))\n  .map(p => Row(p(0), p(1))\n\n// !!\nval schema =\n  StructType(\n    \"key value\".split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n\nval df: DataFrame = SparkInfo.sqlc.createDataFrame(rowRDD, schema)\n\n\u3053\u3053\u3067\u6ce8\u610f\u304c\u3042\u3063\u3066\uff0c!!\u90e8\u5206\u3067\u5b9a\u7fa9\u3057\u305fSchema\u306f\u3059\u3079\u3066\u6587\u5b57\u5217\u578b\u3068\u3057\u3066\u5ba3\u8a00\u3057\u3066\u3044\u308b\u304c\uff0c\u30c7\u30fc\u30bf\u306b\u5408\u308f\u305b\u3066\u578b\u3092\u5ba3\u8a00\u3057\u305f\u308a\u30c7\u30fc\u30bf\u3092\u30ad\u30e3\u30b9\u30c8\u3059\u308b\u306e\u306f\u3068\u3066\u3082\u3064\u3089\u3044\uff0e\n\n\u30e9\u30af\u306a\u4f8b\uff1a case class\u306eRDD\u306f\u6697\u9ed9\u306bDF\u306b\u5909\u63db\u3067\u304d\u308b\uff0e\n\u305d\u308c\u3060\u3051\u3067\u306a\u304f\uff0cInt, Long, Double\u3092\u9069\u5207\u306a\u65b9\u3068\u3057\u3066\u53d6\u308a\u6271\u3063\u3066\u304f\u308c\u308b\uff0e\n\u3055\u3089\u306b\u306fjava.sql.Timestamp\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u306fSparkSQL\u306etimestamp\u578b\u306b\u5909\u63db\u3057\u3066\u304f\u308c\u308b\uff0e\n\nimport org.your.own.domain.utils._\nimport SparkInfo.sqlc.implicits._\nimport java.sql.Timestamp\n\ncase class MyRow(key: String, val: Int, date: Timestamp)\n\n// \u30b3\u30f3\u30d1\u30cb\u30aa\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\uff0c \u884c\u3092\u30d1\u30fc\u30b9\u3057\u3066Option\u3092\u8fd4\u3059\u30d5\u30a1\u30af\u30c8\u30ea\u30e1\u30bd\u30c3\u30c9\u3092\u66f8\u304f\uff0e\uff0e\nobject MyRow {\n  val fileName = \"yourOwnData.tsv\"\n  lazy val parsedRDD: RDD[Option[MyRow]] = SparkInfo.sc.textFile(filename).map(getRow)\n  def getRow(r: String): Option[MyRow] = {\n    val row = r.split(\"\\t\")\n    try {\n      Some(new MyRow(\n        key = row(0),\n        val = row(1).toInt,\n        date = Timestamp.valueOf(row(2))\n      ))\n    }\n    catch { e: Exception => None }\n  }\n}\n\nobject Parser {\n  def main(args: Array[String]): Unit = {\n\n    val df: DataFrame = MyRow.parsedRDD\n      .filter(_.isDefined) // None\u3067\u306a\u3051\u308c\u3070\n      .map(_.get) // MyRow\u3092\u53d6\u5f97\uff0c\u3053\u3053\u304c\u3044\u307e RDD[ -- case class -- ] \u3068\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u30dd\u30a4\u30f3\u30c8\n      .toDF // DF\u3078\u6697\u9ed9\u306e\u5909\u63db\n\n    // df\u306b\u95a2\u3059\u308b\u51e6\u7406\n\n  }\n}\n\n\n\u3068\u3053\u308c\u304c Spark1.4\u73fe\u5728\u306e\u30d1\u30fc\u30b5\u30fc\u306e\u50d5\u7684\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\uff0e\n\u3069\u3046\u305b\u307e\u3082\u306a\u304f\u3069\u3053\u304b\u306e3500\u4eba\u306e\u304a\u304b\u3052\u3067\u3053\u3093\u306a\u3053\u3068\u3057\u306a\u304f\u3066\u3082\u826f\u304f\u306a\u308b\u6c17\u304c\u3059\u308b\u3051\u3069\uff0e\uff0e\uff0e\u6b63\u76f4\u63a2\u308a\u63a2\u308a\u3084\u3063\u3066\u308b\u3093\u3067\u3082\u3063\u3068\u3044\u3044\u65b9\u6cd5\u3042\u3063\u305f\u3089\u3054\u6559\u6388\u304f\u3060\u3055\u3044\u307e\u305b\uff0e\uff0e\uff0e\nCSV/TSV\u3092\u6b63\u3057\u3044\u578b\u3067\u30d1\u30fc\u30b9\u3059\u308b\u306e\u306f\u9aa8\u304c\u6298\u308c\u308b\uff0e\u305d\u306e\u6642\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3092\u30e1\u30e2\u4ee3\u308f\u308a\u306b\uff0e\n\n\u53c2\u8003\uff1ahttps://spark.apache.org/docs/latest/sql-programming-guide.html\n\n## \u524d\u63d0\n\n`SparkContext`\u3084`SQLContext`\u306f\u30b7\u30f3\u30b0\u30eb\u30c8\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u6301\u3063\u3066\u304a\u304f.\n\n\n```scala\npackage org.your.own.domain.utils\n\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkContext, SparkConf}\n\n/**\n * Created by piroyoung on 6/26/15.\n */\nobject SparkInfo {\n  val conf = new SparkConf()\n    .setAppName(\"PickTsv\")\n    .set(\"spark.storage.memoryFraction\", \"1\")\n    .set(\"spark.rdd.compress\", \"true\")\n    .set(\"spark.core.connection.ack.wait.timeout\", \"6000\")\n    .set(\"spark.akka.frameSize\", \"50\")\n    .set(\"spark.files.overwrite\", \"true\")\n\n  val sc = new SparkContext(conf)\n  val sqlc = new SQLContext(sc)\n}\n\n```\n\n\u4ee5\u5f8c`RDD`\u3092\u6271\u3046\u969b\u306f\u3053\u306e\u30b7\u30f3\u30b0\u30eb\u30c8\u30f3\u3092import\u3057\u3066\u4f7f\u3046\uff0e\n\n\n## \u8f9b\u3044\u4f8b\uff1a`Row` + `Schema`\u3067\u751f\u6210\n\n`org.apache.spark.sql.Row`\u3068\u3044\u3046\u30af\u30e9\u30b9\u304b\u3089\u751f\u6210\u3057\u305f`RDD[Row]`\u3068\uff0eSchema\u3092\u5408\u308f\u305b\u3066DataFrame\u3092\u751f\u6210\u3059\u308b\n\n```scala\npackage org.your.own.domain\nimport org.your.own.domain.util._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\n\nval RowRDD: RDD[Row] = SparkInfo.sc.textFile(\"yourData.csv\")\n  .map(_.split(\",\"))\n  .map(p => Row(p(0), p(1))\n\n// !!\nval schema =\n  StructType(\n    \"key value\".split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n\nval df: DataFrame = SparkInfo.sqlc.createDataFrame(rowRDD, schema)\n```\n\n\u3053\u3053\u3067\u6ce8\u610f\u304c\u3042\u3063\u3066\uff0c`!!`\u90e8\u5206\u3067\u5b9a\u7fa9\u3057\u305fSchema\u306f\u3059\u3079\u3066\u6587\u5b57\u5217\u578b\u3068\u3057\u3066\u5ba3\u8a00\u3057\u3066\u3044\u308b\u304c\uff0c\u30c7\u30fc\u30bf\u306b\u5408\u308f\u305b\u3066\u578b\u3092\u5ba3\u8a00\u3057\u305f\u308a\u30c7\u30fc\u30bf\u3092\u30ad\u30e3\u30b9\u30c8\u3059\u308b\u306e\u306f\u3068\u3066\u3082\u3064\u3089\u3044\uff0e\n\n## \u30e9\u30af\u306a\u4f8b\uff1a `case class`\u306eRDD\u306f\u6697\u9ed9\u306bDF\u306b\u5909\u63db\u3067\u304d\u308b\uff0e\n\n\u305d\u308c\u3060\u3051\u3067\u306a\u304f\uff0c`Int`, `Long`, `Double`\u3092\u9069\u5207\u306a\u65b9\u3068\u3057\u3066\u53d6\u308a\u6271\u3063\u3066\u304f\u308c\u308b\uff0e\n\u3055\u3089\u306b\u306f`java.sql.Timestamp`\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u306fSparkSQL\u306etimestamp\u578b\u306b\u5909\u63db\u3057\u3066\u304f\u308c\u308b\uff0e\n\n```scala\n\nimport org.your.own.domain.utils._\nimport SparkInfo.sqlc.implicits._\nimport java.sql.Timestamp\n\ncase class MyRow(key: String, val: Int, date: Timestamp)\n\n// \u30b3\u30f3\u30d1\u30cb\u30aa\u30f3\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\uff0c \u884c\u3092\u30d1\u30fc\u30b9\u3057\u3066Option\u3092\u8fd4\u3059\u30d5\u30a1\u30af\u30c8\u30ea\u30e1\u30bd\u30c3\u30c9\u3092\u66f8\u304f\uff0e\uff0e\nobject MyRow {\n  val fileName = \"yourOwnData.tsv\"\n  lazy val parsedRDD: RDD[Option[MyRow]] = SparkInfo.sc.textFile(filename).map(getRow)\n  def getRow(r: String): Option[MyRow] = {\n    val row = r.split(\"\\t\")\n    try {\n      Some(new MyRow(\n        key = row(0),\n        val = row(1).toInt,\n        date = Timestamp.valueOf(row(2))\n      ))\n    }\n    catch { e: Exception => None }\n  }\n}\n\nobject Parser {\n  def main(args: Array[String]): Unit = {\n  \n    val df: DataFrame = MyRow.parsedRDD\n      .filter(_.isDefined) // None\u3067\u306a\u3051\u308c\u3070\n      .map(_.get) // MyRow\u3092\u53d6\u5f97\uff0c\u3053\u3053\u304c\u3044\u307e RDD[ -- case class -- ] \u3068\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u30dd\u30a4\u30f3\u30c8\n      .toDF // DF\u3078\u6697\u9ed9\u306e\u5909\u63db\n\n    // df\u306b\u95a2\u3059\u308b\u51e6\u7406\n  \n  }\n}\n\n```\n\n\u3068\u3053\u308c\u304c Spark1.4\u73fe\u5728\u306e\u30d1\u30fc\u30b5\u30fc\u306e\u50d5\u7684\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\uff0e\n\n\u3069\u3046\u305b\u307e\u3082\u306a\u304f\u3069\u3053\u304b\u306e3500\u4eba\u306e\u304a\u304b\u3052\u3067\u3053\u3093\u306a\u3053\u3068\u3057\u306a\u304f\u3066\u3082\u826f\u304f\u306a\u308b\u6c17\u304c\u3059\u308b\u3051\u3069\uff0e\uff0e\uff0e\u6b63\u76f4\u63a2\u308a\u63a2\u308a\u3084\u3063\u3066\u308b\u3093\u3067\u3082\u3063\u3068\u3044\u3044\u65b9\u6cd5\u3042\u3063\u305f\u3089\u3054\u6559\u6388\u304f\u3060\u3055\u3044\u307e\u305b\uff0e\uff0e\uff0e\n\n", "tags": ["Spark", "Scala", "Parse", "DataFrame", "Timestamp"]}