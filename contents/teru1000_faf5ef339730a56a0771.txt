{"tags": ["Spark", "sparksql", "Scala"], "context": " More than 1 year has passed since last update.\n\nspark 1.5.1 \u306eDataframe\u6ce8\u610f\u70b9\n\nDataframe\u3092toDF(\"entityId\",\"entityType\", \"targetEntity\", ...) \u3067\u751f\u6210\u3057\u305f\u3042\u3068filter or where \u95a2\u6570\u3067\u3001=== \u304c\u3046\u307e\u304f\u52d5\u4f5c\u3057\u306a\u3044\u30d0\u30b0\u3092\u56de\u907f\u3059\u308b\u65b9\u6cd5\nhttps://issues.apache.org/jira/browse/SPARK-10859\nv1.5.2\u4ee5\u964d\u306b\u306fFix\u3055\u308c\u308b\u4e88\u5b9a\u3067\u30011.5.2-RC2 \u3067\u306f\u4fee\u6b63\u6e08\u307f\u3002\n2015/11/09 v1.5.2\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\uff08http://spark.apache.org/news/spark-1-5-2-released.html\uff09\n\n\n\n\u9593\u9055\u3063\u305f\u7d50\u679c\u306b\u306a\u308b\u30b1\u30fc\u30b9\nval eventTableColumns = Seq[String](\n    \"entityType\"\n  , \"entityId\"\n  , \"targetEntityType\"\n  , \"targetEntityId\"\n  , \"properties\"\n  , \"eventTime\")\n\nval eventDF = sc.textFile(\"/tmp/events_s.csv\").map(_.split(\",\")).filter(_.size >= 6)\n  .map { e =>\n    (\n      e(5), e(0), \"item\", e(1), s\"\"\"{\"rating\": ${e(3).trim.toDouble}}\"\"\", e(3)\n    )\n  }.toDF(eventTableColumns:_*)\n\neventDF.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n\n\n\u56de\u907f\u7b56\u2460\n\n=== \u306e\u4ee3\u308f\u308a\u306b isin \u3092\u4f7f\u3046\n\neventDF.filter($\"entityType\" isin lit(\"user\")).select(\"entityId\").distinct.count\n\n\n\u56de\u907f\u7b56\u2461\n\ncase class \u3092\u4f7f\u3046\n\ncase class Event(entityType: String, entityId: String,\n                 targetEntityType: String, targetEntityId: String,\n                 properties: String, eventTime: String)\n\nval eventDF2 = sc.textFile(\"/tmp/events_s.csv\").map(_.split(\",\")).filter(_.size >= 6)\n  .map { e =>\n    Event(\n      e(5), e(0), \"item\", e(1), s\"\"\"{\"rating\": ${e(3).trim.toDouble}}\"\"\", e(3)\n    )\n  }.toDF()\n\neventDF2.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n\n\n\u56de\u907f\u7b56\u2462\n\nspark.sql.inMemoryColumnarStorage.partitionPruning \u3092 false \u306b\u3059\u308b\n\n    sqlContext.sql(\"SET spark.sql.inMemoryColumnarStorage.partitionPruning=false\")\n    eventDF.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n\n\n\u8a73\u3057\u3044\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u306f\u3053\u3061\u3089\nhttps://gist.github.com/schon/b49a616eeb0b78fb5f47\n# spark 1.5.1 \u306eDataframe\u6ce8\u610f\u70b9\n\n* Dataframe\u3092toDF(\"entityId\",\"entityType\", \"targetEntity\", ...) \u3067\u751f\u6210\u3057\u305f\u3042\u3068filter or where \u95a2\u6570\u3067\u3001=== \u304c\u3046\u307e\u304f\u52d5\u4f5c\u3057\u306a\u3044\u30d0\u30b0\u3092\u56de\u907f\u3059\u308b\u65b9\u6cd5\n* https://issues.apache.org/jira/browse/SPARK-10859\n* v1.5.2\u4ee5\u964d\u306b\u306fFix\u3055\u308c\u308b\u4e88\u5b9a\u3067\u30011.5.2-RC2 \u3067\u306f\u4fee\u6b63\u6e08\u307f\u3002\n* 2015/11/09 v1.5.2\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\uff08http://spark.apache.org/news/spark-1-5-2-released.html\uff09\n\n## \u9593\u9055\u3063\u305f\u7d50\u679c\u306b\u306a\u308b\u30b1\u30fc\u30b9\n\n```scala\nval eventTableColumns = Seq[String](\n    \"entityType\"\n  , \"entityId\"\n  , \"targetEntityType\"\n  , \"targetEntityId\"\n  , \"properties\"\n  , \"eventTime\")\n\nval eventDF = sc.textFile(\"/tmp/events_s.csv\").map(_.split(\",\")).filter(_.size >= 6)\n  .map { e =>\n    (\n      e(5), e(0), \"item\", e(1), s\"\"\"{\"rating\": ${e(3).trim.toDouble}}\"\"\", e(3)\n    )\n  }.toDF(eventTableColumns:_*)\n\neventDF.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n```\n\n## \u56de\u907f\u7b56\u2460\n* === \u306e\u4ee3\u308f\u308a\u306b isin \u3092\u4f7f\u3046\n\n```scala\neventDF.filter($\"entityType\" isin lit(\"user\")).select(\"entityId\").distinct.count\n```\n\n## \u56de\u907f\u7b56\u2461\n* case class \u3092\u4f7f\u3046\n\n```scala\ncase class Event(entityType: String, entityId: String,\n                 targetEntityType: String, targetEntityId: String,\n                 properties: String, eventTime: String)\n\nval eventDF2 = sc.textFile(\"/tmp/events_s.csv\").map(_.split(\",\")).filter(_.size >= 6)\n  .map { e =>\n    Event(\n      e(5), e(0), \"item\", e(1), s\"\"\"{\"rating\": ${e(3).trim.toDouble}}\"\"\", e(3)\n    )\n  }.toDF()\n\neventDF2.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n```\n\n\n## \u56de\u907f\u7b56\u2462\n* spark.sql.inMemoryColumnarStorage.partitionPruning \u3092 false \u306b\u3059\u308b\n\n```scala\n    sqlContext.sql(\"SET spark.sql.inMemoryColumnarStorage.partitionPruning=false\")\n    eventDF.filter($\"entityType\" === \"user\").select(\"entityId\").distinct.count\n```\n\n## \u8a73\u3057\u3044\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u306f\u3053\u3061\u3089\nhttps://gist.github.com/schon/b49a616eeb0b78fb5f47\n"}