{"context": "\n\n\u3053\u306e\u8a18\u4e8b\u306b\u3064\u3044\u3066\u3000\nDeepMind \u304c Nature \u306b\u6295\u7a3f\u3057\u305f\u8ad6\u6587 \nHybrid computing using a neural network with dynamic external memory\n\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b \"Differentiable Neural Computing (DNC)\" \u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3059\u3002\u30ed\u30b8\u30c3\u30af\u306e\u8aac\u660e\u304c\u30e1\u30a4\u30f3\u3067\u3059\u304c\u3001Python - Chainer \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\u3082\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\nDifferentiable Neural Computing (DNC)\u3068\u306f\nsequencial data \u3092 Neural Net \u3067\u51e6\u7406\u3057\u305f\u3044\u3068\u3044\u3046\u6b32\u6c42\u306f\u6614\u304b\u3089\u3042\u308b\u3088\u3046\u3067\u3001\u4e00\u756a\u30b9\u30bf\u30f3\u30c0\u30fc\u30c9\u306a\u3082\u306e\u306f Recurrent Neural Net (RNN) \u3067\u3059\u3002\u3057\u304b\u3057\u3001RNN \u306b\u306f\"\u52fe\u914d\u6d88\u5931\u554f\u984c\"\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308a\u307e\u3057\u3066\u3001\u305d\u308c\u3092\u514b\u670d\u3057\u305f\u30e2\u30c7\u30eb\u304c Long Short Term Memory (LSTM) \u3068\u547c\u3070\u308c\u3066\u3044\u307e\u3059\u3002\u4f55\u304c short \u3067\u4f55\u304c long \u306a\u306e\u304b\u3068\u3044\u3046\u3068\u3001\u5165\u529b\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001\u51fa\u529b\u3055\u308c\u308b\u307e\u3067 Neural Net \u306e\u5185\u90e8\u306b\u3068\u3069\u307e\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u3059\u3002\u3053\u308c\u306f\u3042\u308b\u610f\u5473\u3067 \"Memory\" \u306e\u3088\u3046\u306a\u3082\u306e\u3068\u8003\u3048\u3066\u3082\u3088\u3055\u305d\u3046\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u5165\u529b\u304b\u3089\u51fa\u529b\u307e\u3067\u306e\"\u77ed\u6642\u9593\"\u3057\u304b\u8a18\u61b6\u304c\u3067\u304d\u306a\u3044\u3002\u306a\u306e\u3067\u3001\"short term momory\" \u3068\u547c\u3070\u308c\u307e\u3059\u3002\u300c\u3053\u306e short term memory \u306e\u8a18\u61b6\u3067\u304d\u308b\u6642\u9593\u304c\u9577\u304f\u306a\u308a\u307e\u3057\u305f\u3088\u300d\u3068\u3044\u3046\u610f\u5473\u3067\u3001Long Short Term Memory \u3068\u540d\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u305d\u3046\u3059\u308b\u3068\u3001\u672c\u5f53\u306b long \u306a Meomory \u306f\u5b66\u7fd2\u6a5f\u5185\u90e8\u3067\u306f\u306a\u304f\u3066\u5916\u90e8\u306b\u3042\u3063\u3066\u3082\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3001\u305d\u3093\u306a\u6c17\u304c\u3057\u3066\u304d\u307e\u3059\u3002\u305d\u3046\u3044\u3046\u601d\u3044\u304b\u3089\u751f\u307e\u308c\u305f\u3001\u304b\u3069\u3046\u304b\u306f\u5206\u304b\u308a\u307e\u305b\u3093\u304c\u3001Differentiable Neural Computing (DNC) \u3068\u306f\u3001\u5b66\u7fd2\u6a5f\u306e\u5916\u90e8\u306b Memory \u3092\u7528\u610f\u3057\u3066\u3084\u3063\u3066\u3001Memory \u306e\u4f7f\u7528\u6cd5\u307e\u3067\u542b\u3081\u3066\u5b66\u7fd2\u3055\u305b\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\"Differentiable\"\u306e\u610f\u5473\u3067\u3059\u304c\u3001back propagation \u3067\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u601d\u3046\u3068\u3001\u5fae\u5206\u3092\u8a08\u7b97\u3067\u304d\u308b\u3053\u3068\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\u3067\u3059\u306e\u3067\u3001\u5916\u90e8 Memory \u306b\u5bfe\u3059\u308b\u64cd\u4f5c\u3082\u542b\u3081\u3066\"\u5fae\u5206\u8a08\u7b97\u304c\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\"\u3068\u3044\u3046\u610f\u5473\u3067\u3059\u3002\n\n\u30ed\u30b8\u30c3\u30af\u89e3\u8aac\n\nDNC \u306e\u5168\u4f53\u50cf\nDNC \u306e\u69cb\u6210\u8981\u7d20\u306f\u3001\u672c\u4f53\u3067\u3042\u308b Controller \u3068\u5916\u90e8 Memory \u3067\u3059\u3002\u8ad6\u6587\u306b\u3088\u308b\u3068\u3001\u5168\u4f53\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u306f\u4e0b\u56f3\u306e\u3088\u3046\u306b\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\nDNC \u306f sequencial data \u3092\u6271\u3044\u307e\u3059\u306e\u3067\u3001\u73fe\u5728\u306e time-step \u3092 $t$ \u3068\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u524d step \u306f $t-1$\u3001\u6b21 step \u306f $t+1$ \u3068\u306a\u308a\u3001\u56f3\u4e2d\u306e\u6dfb\u3048\u5b57 $t$ \u306f\u3001\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u304c\u3069\u306e time-step \u3067\u751f\u6210\u3055\u308c\u305f\u3082\u306e\u3067\u3042\u308b\u304b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\n\u305d\u308c\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u3092\u9806\u3092\u8ffd\u3063\u3066\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u56f3\u4e2d\u306b(1)~(4)\u306e\u756a\u53f7\u304c\u632f\u3063\u3066\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u3053\u306e\u9806\u3067\u8ffd\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n(1) : \"Data Set\" \u3088\u308a \u5165\u529b\u30c7\u30fc\u30bf $x_t$ \u304c\u5165\u529b\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306b\u3001\u524d step \u3067\u306e Memory \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{r}_{t-1}$ \u3068\u3092\u5408\u308f\u305b\u305f $\\boldsymbol{\\chi}_t=[\\boldsymbol{x}_t, \\boldsymbol{r}_{t-1}]$ \u3092 Controller \u3078\u306e\u5165\u529b\u3068\u3057\u307e\u3059\u3002\n(2) : Controller \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{h}_t$ \u304c\u5f97\u3089\u308c\u307e\u3059\u306e\u3067\u3001\u3053\u308c\u30922\u3064\u306e\u30eb\u30fc\u30c8\u306b\u632f\u308a\u5206\u3051\u307e\u3059\u3002\u3072\u3068\u3064\u306f\u3001\"Out Put\" \u306b\u5411\u3051\u3066\u306e $\\boldsymbol{v}_t$\u3001\u3082\u30461\u3064\u306f Memory \u3092\u5236\u5fa1\u3059\u308b\u305f\u3081\u306e \"interface vector\" $\\boldsymbol{\\xi}_t$ \u3067\u3059\u3002\u8ad6\u6587\u4e2d\u3067\u306f\u3001\u3053\u308c\u3089\u306f $\\boldsymbol{h}_t$ \u306e\u7dda\u5f62\u5909\u63db $\\boldsymbol{v}_t=W_y \\boldsymbol{h}_t$ \u3068\u3001$\\boldsymbol{\\xi}_t=W_{\\xi} \\boldsymbol{h}_t$ \u3068\u3057\u3066\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002\n(3) : \"interface vector\" $\\boldsymbol{\\xi}_t$ \u3092\u3082\u3068\u306b Memory \u3078\u306e\u30c7\u30fc\u30bf\u306e\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c Memory \u306e\u72b6\u614b\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u3082\u884c\u308f\u308c\u3001\"read vector\" $\\boldsymbol{r}_t$ \u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n(4) : \"read vector\" $\\boldsymbol{r}_t$ \u306f\u3001 \"Out Put\"\u3078\u306e\u51fa\u529b\u306b\u52a0\u7b97\u3055\u308c\u308b\u4e00\u65b9\u3067\u3001\u6b21 step \u3067\u306e Controller \u3078\u306e\u5165\u529b\u3078\u3068\u56de\u3055\u308c\u307e\u3059\u3002\n(5) : Controller \u304b\u3089\u306e\u51fa\u529b\u3068\u3001Memory\u304b\u3089\u306e\u51fa\u529b\u3092\u5408\u6210\u3057\"Out Put\" \u3078 $\\boldsymbol{y}_t = \\boldsymbol{v}_t + W_r \\boldsymbol{r}_t$ \u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\u4ee5\u4e0a\u3001(1)~(5) \u3092\u3082\u3063\u3066\u30011 step \u304c\u5b8c\u4e86\u3057\u307e\u3059\u3002\nController \u3068\u3057\u3066\u306f\u3001\u591a\u6b21\u5143\u306e\u5165\u529b\u3092\u53d7\u3051\u53d6\u308a\u3001\u591a\u6b21\u5143\u306e\u51fa\u529b\u3092\u8fd4\u3059\u5b66\u7fd2\u6a5f\u306a\u3089\u4f55\u3067\u3082\u3088\u3044\u306e\u3067\u3059\u304c\u3001(Recurrent) Neural Net \u3084 LSTM\u306a\u3069\u3092\u4f7f\u3046\u3053\u3068\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002(Recurrent) Neural Net \u3084 LSTM \u306e\u89e3\u8aac\u306f\u4ed6\u306b\u3086\u305a\u308b\u3068\u3057\u3066\u3001\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001DNC \u306e\u6700\u5927\u306e\u7279\u5fb4\u3067\u3042\u308b\u300c\u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u300d\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n\u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u306b\u3064\u3044\u3066\n\u305d\u308c\u3067\u306f\u3001\u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3057\u3087\u3046\u3002\u9762\u5012\u306a\u306e\u3067\u3001\u4ee5\u4e0b\u3067\u306f\u5358\u306b Memory \u3068\u547c\u3076\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\nMemory \u306e\u69cb\u9020\n\u307e\u305a\u306f\u3001Memory \u306e\u69cb\u9020\u306b\u3064\u3044\u3066\u628a\u63e1\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\nMemory \u3068\u3057\u3066\u306f $N$ \u00d7 $W$ \u306e\u884c\u5217\u3092\u7528\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001address \u304c $1$~$N$ \u307e\u3067\u632f\u3089\u308c\u3066\u3044\u3066\u3001\u5404 address \u306b $W$ \u6b21\u5143\u306e\u6570\u5024\u30d9\u30af\u30c8\u30eb\u3092\u683c\u7d0d\u3067\u304d\u308b slot \u304c\u3042\u308b\u3068\u8003\u3048\u307e\u3059\u3002Memory \u306e\u72b6\u614b\u306f\u523b\u3005\u3068\u66f4\u65b0\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001time-step $t$ \u3067\u306e Memory \u3092\u8868\u3059\u884c\u5217\u3092 $M_t$ \u3068\u66f8\u304f\u3053\u3068\u306b\u3057\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u884c\u5217\u306e\u6b21\u5143 $N$ \u00d7 $W$ \u306f\u56fa\u5b9a\u3055\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u3001\u5e38\u306b\u3001$N$ \u306f memory slot \u306e\u7dcf\u6570 \uff08address \u306e\u7dcf\u6570\uff09\u3001$W$ \u306f slot \u306e\u9577\u3055\uff08\u683c\u7d0d\u3059\u308b\u6570\u5024\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\uff09\u3068\u3057\u307e\u3059\u3002\n\ninterface vector \u306e\u8a73\u7d30\n\u300c\u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u300d\u3067\u3082\u8ff0\u3079\u305f\u901a\u308a\u3001Controller \u306b\u3088\u308b Memory \u306e\u64cd\u4f5c\u306f\n\"interface vector\" $\\boldsymbol{\\xi}_t$ \u3092\u901a\u3057\u3066\u884c\u308f\u308c\u307e\u3059\u3002\u4e00\u53e3\u3067 Memory \u306e\u64cd\u4f5c\u3068\u3044\u3063\u3066\u3082\u3001\u8aad\u307f\u8fbc\u307f/\u66f8\u304d\u8fbc\u307f/address\u306e\u6307\u5b9a\u306a\u3069\u3001\u8907\u6570\u306e\u30bf\u30b9\u30af\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u306e\u3067\u3001$\\boldsymbol{\\xi}_t$ \u306f\u6a5f\u80fd\u5225\u306b\u5404 component \u306b\u5206\u89e3\u3055\u308c\u307e\u3059\u3002\n\\begin{align}\n\\boldsymbol{\\xi}_t = \\Big[ \\boldsymbol{k}_t^{r, 1},...,\\boldsymbol{k}_t^{r, R}, \\; \\hat{\\beta}_t^{r, 1},...,\\hat{\\beta}_t^{r, R}, \\; \\boldsymbol{k}_t^w, \\; \\hat{\\beta_t}^w, \\hat{\\boldsymbol{e}}_t, \\; \\boldsymbol{\\nu}_t, \\; \\hat{f}_t^1,...,\\hat{f}_t^R, \\; \\hat{g}_t^a, \\; \\hat{g}_t^w, \\; \\hat{\\boldsymbol{\\pi}}_t^1,...,\\hat{\\boldsymbol{\\pi}}_t^R \\Big]\n\\end{align}\n\n'$\\hat{}$' \u8a18\u53f7\u304c\u3064\u3044\u3066\u3044\u308b\u3082\u306e\u306f\u3001\u3055\u3089\u306b scale \u5909\u63db\u3092\u304b\u3051\u307e\u3059\u3002\u7a2e\u985e\u304c\u591a\u304f\u3066\u3084\u3084\u3053\u3057\u3044\u306e\u3067\u3001\u3044\u3063\u305f\u3093\u307e\u3068\u3081\u307e\u3059\u3002( , )\u5185\u306e\u5024\u306f\u3001(\u6b21\u5143, \u500b\u6570)\u3067\u3059\u3002\n\u30fb\"read key\" $(W, R) \\; :\\; \\boldsymbol{k}_t^{r, 1},...,\\boldsymbol{k}_t^{r, R}$\n\u30fb\"read strength\" $(1, R) \\; : \\; \\beta_t^{r, 1},...,\\beta_t^{r, R} \\;\\;\\big(\\beta_t^{r, i}=\\text{oneplus}(\\hat{\\beta}_t^{r, i})\\big)$\n\u30fb\"write key\" $(W, 1) \\; : \\; \\boldsymbol{k}_t^w$\n\u30fb\"write strength\" $(1,1) \\; : \\; \\beta_t^w=\\text{oneplus}(\\hat{\\beta}_t^w)$\n\u30fb\"erase vector\" $(W, 1) \\; : \\; \\boldsymbol{e}_t=\\sigma(\\hat{\\boldsymbol{e}}_t)$\n\u30fb\"write vector\" $(W, 1) \\; : \\; \\boldsymbol{\\nu}_t$\n\u30fb\"free gate\" $(1, R) \\; : \\; f_t^1,...,f_t^R \\;\\; \\big(f_t^i=\\sigma(\\hat{f}_t^i)\\big)$ \n\u30fb\"allocation gate\" $(1, 1) \\; :\\; g^a_t=\\sigma(\\hat{g}^a_t)$\n\u30fb\"write gate\" $(1, 1) \\; :\\; g^w_t=\\sigma(\\hat{g}^w_t)$\n\u30fb\"read mode\" $(3, R) \\; : \\; \\boldsymbol{\\pi}_t^1,...,\\boldsymbol{\\pi}_t^R \\;\\; \\big(\\boldsymbol{\\pi}_t^i=\\text{softmax}(\\hat{\\boldsymbol{\\pi}}_t^i)\\big)$\n\u30b9\u30b1\u30fc\u30eb\u5909\u63db\u306b\u7528\u3044\u308b\u95a2\u6570\u306f\u3001\n\\begin{align}\n& \\text{oneplus}(x)\u3000= 1+\\text{log}(1+e^x) \\in [1, \\infty) \\\\\n& \\sigma(x) = \\frac{1}{1+e^{-x}} \\in [0, 1] \\\\\n& \\text{softmax}(\\boldsymbol{x}) = \\frac{e^\\boldsymbol{x}}{\\sum_ie^{x_i}} \\in [0, 1]\n\\end{align}\n\n\u3068\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002$\\text{oneplus}(x)$\u3001 $\\sigma(x)$\u3001$\\text{exp}(x)$ \u306e\u5f15\u6570\u306b\u30d9\u30af\u30c8\u30eb\u5024\u3092\u53d6\u308b\u5834\u5408\u306f\u3001\u6210\u5206\u3054\u3068\u306e\u4f5c\u7528\u3092\u610f\u5473\u3057\u307e\u3059\u306e\u3067\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u3053\u3053\u3067\u306f\u3001\u5b9a\u7fa9\u3092\u7f85\u5217\u3057\u305f\u3060\u3051\u306a\u306e\u3067\u3001\u4f55\u3082\u308f\u304b\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4ee5\u4e0b\u3001\u3053\u308c\u3089\u306e component \u3092\u3069\u306e\u3088\u3046\u306b\u4f7f\u3063\u3066 Memory \u3092\u5236\u5fa1\u3057\u3066\u3044\u304f\u304b\u3001\u9806\u306b\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\u5148\u306b\u9032\u3080\u524d\u306b\u30011\u70b9\u3060\u3051\u8aac\u660e\u3092\u52a0\u3048\u3066\u304a\u304d\u307e\u3059\u3002$R$ \u306f Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u306e\u56de\u6570\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u8ad6\u6587\u4e2d\u306e DNC \u3067\u306f\u30011 step \u306e\u3046\u3061\u3067 Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u3092\u8907\u6570\u56de\u884c\u3046\u8a2d\u5b9a\u3068\u306a\u3063\u3066\u3044\u3066\u3001\u305d\u306e\u56de\u6570\u304c $R$ \u3067\u3059\u3002\u5bfe\u3057\u3066\u3001\u66f8\u304d\u8fbc\u307f\u306f\u30011 step \u306b1\u56de\u306e\u307f\u306e\u8a2d\u5b9a\u3067\u3059\u3002\u307e\u305f\u3001interface vector \u306e\u6b21\u5143\u306f\u3001$W$ \u3068 $R$ \u3067\u6c7a\u307e\u3063\u3066\u3044\u3066\u3001$WR+3W+5R+3$ \u3068\u306a\u308b\u3053\u3068\u3082\u5206\u304b\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\nMemory \u306e\u8aad\u307f\u66f8\u304d\u624b\u9806\uff08\u6982\u8981\uff09\nMemory \u306e\u8aad\u307f\u66f8\u304d\u306b\u306f\u3001\u8aad\u307f\u66f8\u304d\u5bfe\u8c61\u3068\u306a\u308b Memory slot \u306e address \u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305f\u3060\u3057\u3001back propagation \u306b\u3088\u308b\u5b66\u7fd2\u304c\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306b\u306f\u3001\u5fae\u5206\u53ef\u80fd\u6027\u3001\u5c11\u306a\u304f\u3068\u3082\u6f14\u7b97\u306e\u9023\u7d9a\u6027\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\u3002\u3088\u3063\u3066\u3001\u7279\u5b9a\u306e\u3072\u3068\u3064\u306e address \u3060\u3051\u3092\u6307\u5b9a\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5e45\u3092\u6301\u305f\u305b\u3066\u3001\u3069\u306e address \u3092\u91cd\u70b9\u7684\u306b\u8aad\u307f\u8fbc\u3080/\u66f8\u304d\u8fbc\u3080\u304b\u306e\u91cd\u307f\u3065\u3051\u3092\u884c\u3044\u307e\u3059\u3002\u4ee5\u5f8c\u3001\u3053\u306e\u91cd\u307f\u3092\u3001\"read/write weighting\"\u3068\u547c\u3076\u3053\u3068\u306b\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3089\u3092\u3069\u3046\u6c42\u3081\u3066\u3044\u304f\u304b\u304c\u30dd\u30a4\u30f3\u30c8\u306b\u306a\u308a\u307e\u3059\u3002\n\"read/write weighting\" \u3092\u6c42\u3081\u3066\u3057\u307e\u3048\u3070\u3001\u8aad\u307f\u8fbc\u307f/\u66f8\u304d\u8fbc\u307f\u306e\u6f14\u7b97\u81ea\u4f53\u306f\u5358\u7d14\u3067\u3059\u3002\u8a73\u7d30\u306f\u5f8c\u306b\u56de\u3057\u307e\u3059\u304c\u3001\u6982\u8981\u3092\u3064\u304b\u3093\u3067\u304a\u304d\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u3001\u8aad\u307f\u8fbc\u307f\u306e\u5834\u5408\u3092\u8003\u3048\u3066\u307f\u307e\u3059\u3002\u4eca\u3001\"read weighting\" $\\boldsymbol{w}^r_t$ \u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002\u3053\u306e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f\u3001Memory slot \u306e\u7dcf\u6570\u3068\u540c\u3058 $N$ \u3067\u3059\u3002\u5404\u6210\u5206\u306f\u5bfe\u5fdc\u3059\u308b address \u306e Memory slot \u306b\u3042\u308b\u60c5\u5831\u3092\u3069\u308c\u3060\u3051\"\u5f37\u304f\"\u8aad\u307f\u8fbc\u3080\u304b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u8aad\u307f\u3060\u3055\u308c\u308b\u60c5\u5831\u306f\u3001Memory \u884c\u5217\u3068\u91cd\u307f\u30d9\u30af\u30c8\u30eb\u306e\u7a4d\u3068\u3057\u3066\u3001$M_t^T\\boldsymbol{w}^r_t$\u3068\u8868\u3055\u308c\u307e\u3059\u3002\u5148\u306b\u3082\u8ff0\u3079\u307e\u3057\u305f\u304c\u3001\u8ad6\u6587\u4e2d\u3067\u306f 1 step \u3067\u8aad\u307f\u51fa\u3057\u3092 $R$ \u56de\u884c\u3046\u8a2d\u5b9a\u306a\u306e\u3067\u3001\"read weighting\" \u3082 $R$ \u500b $\\{\\boldsymbol{w}_t^{r,i}\\}_{i=1,...,R}$ \u69cb\u6210\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u8aad\u307f\u51fa\u3057\u306b\u3088\u308b\u5897\u5e45\u306a\u3069\u9632\u3050\u305f\u3081\u3001\u81ea\u7136\u306a\u8981\u8acb\u3068\u3057\u3066\n\\begin{align}\n& 0 \\leqq \\boldsymbol{w}_t^{r,i}[n] \\leqq 1 \\;\\; (n=1,2,...,N)\\\\\n& \\sum_{n=1}^N \\boldsymbol{w}_t^{r,i}[n] \\leqq 1 \\;\\; (i=1,2,...,R)\n\\end{align}\n\n\u3092\u8ab2\u3059\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\u540c\u69d8\u306b\u3001\u66f8\u304d\u8fbc\u307f\u306e\u5834\u5408\u3082\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\"write weighting\" $\\boldsymbol{w_t^w}$ \u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u66f8\u304d\u8fbc\u307f\u305f\u3044\u60c5\u5831\u3092\u8868\u3059\u30d9\u30af\u30c8\u30eb $\\boldsymbol{\\nu}_t$ \u3082\u5f97\u3089\u308c\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002$\\boldsymbol{w_t^w}$\u306e\u6b21\u5143\u306f$N$\u3067\u3001\u5404\u6210\u5206\u306f\u5bfe\u5fdc\u3059\u308b Memory slot \u306b\u3069\u308c\u3060\u3051\"\u5f37\u304f\" $\\boldsymbol{\\nu}_t$ \u3092\u66f8\u304d\u8fbc\u3080\u304b\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001Memory \u884c\u5217 $M_{t-1}$ \u306b\u3001\u884c\u5217 $\\boldsymbol{w_t^w}\\boldsymbol{\\nu}_t^T$ \u3092\u52a0\u7b97\u3059\u308b\u3053\u3068\u3067\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3044 Memory \u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u5404\u6210\u5206\u306b\n\\begin{align}\n& 0 \\leqq \\boldsymbol{w}_t^w[n] \\leqq 1 \\;\\; (n=1,2,...,N)\\\\\n& \\sum_{n=1}^N \\boldsymbol{w}_t^w[n] \\leqq 1 \n\\end{align}\n\n\u3092\u8ab2\u3059\u70b9\u306f\u540c\u69d8\u3067\u3059\u3002\n\nMemory \u306e\u8aad\u307f\u66f8\u304d\u624b\u9806\uff08\u8a73\u7d30\uff09\n\u8ad6\u6587\u4e2d\u3067\u306f\u3001Memory \u3078\u306e\u8aad\u307f\u66f8\u304d\u81ea\u4f53\u3082\u542b\u3081\u3066\u4ee5\u4e0b\u306e4\u3064\u306e\u624b\u9806\u3092\u8e0f\u3093\u3067\u3044\u307e\u3059\u3002\n\u2460write weighting \u306e\u66f4\u65b0\n\u2461Memory \u3078\u306e\u66f8\u304d\u8fbc\u307f\n\u2462read weighting \u306e\u66f4\u65b0\n\u2463Memory \u304b\u3089\u306e\u8aad\u307f\u8fbc\u307f \n\u4ee5\u4e0b\u3001\u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u305d\u3063\u3066\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u5b9f\u88c5\u4f8b\u3067\u3082\u3001\u3053\u306e\u9806\u3067\u51e6\u7406\u3092\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5c1a\u3001\u524d time-step \u3067\u306e read/write weighting $\\{\\boldsymbol{w}_{t-1}^{r,i}\\}_{i=1,...,R}$ / $\\boldsymbol{w}_{t-1}^{w}$\u306f\u5f97\u3089\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\n\u2460write weighting\u306e\u66f4\u65b0\n\u66f8\u304d\u8fbc\u307f\u5148\u306e address \u306e\u9078\u629e\u306b\u306f2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u3001write weighting \u306f2\u3064\u306e\u8981\u7d20\u304b\u3089\u69cb\u6210\u3055\u308c\u307e\u3059\u3002\u3064\u307e\u308a\u3001(1) interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u305f \"key\" \u3092\u3082\u3068\u306b\u66f8\u304d\u8fbc\u307f\u5148 slot \u3092\u9078\u629e\u3001(2)\u524d time-step \u307e\u3067\u306e\u8aad\u307f\u51fa\u3057\u72b6\u6cc1\u3092\u3082\u3068\u306b\u4f7f\u7528\u6e08\u307f\u306e\u60c5\u5831\u304c\u6b8b\u3063\u3066\u3044\u308b slot \u3092\u9078\u629e\u3001\u306e2\u3064\u3067\u3059\u3002\n\u307e\u305a\u306f\u3001(1)\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\ninterface vector $\\boldsymbol{\\xi}_t$ \u4e2d\u306e \"write key\" $\\boldsymbol{k}_t^w$ \u3092\u5404 Memory slot \u306b\u4fdd\u6301\u3055\u308c\u3066\u3044\u308b\u60c5\u5831\u3068\u7167\u5408\u3057\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u307e\u305f\u3001weighting \u306e peak \u306e\u92ed\u3055\u3092\u8abf\u6574\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066 \"write strength\" $\\beta_t^w$ \u3082\u4f7f\u7528\u3057\u307e\u3059\u3002\n\u3053\u306e\u8a08\u7b97\u306f\u3001read weighting \u3092\u6c42\u3081\u308b\u3068\u3053\u308d\u3067\u3082\u5171\u901a\u306a\u306e\u3067\u3001$N$\u00d7$W$ \u884c\u5217 $M$\u3001$W$ \u6b21\u30d9\u30af\u30c8\u30eb $\\boldsymbol{k}$\u3001\u30b9\u30ab\u30e9\u30fc\u5024 $\\beta$ \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u6f14\u7b97\u3092\u5b9a\u7fa9\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\\begin{align}\n\\mathcal{C}(M, \\boldsymbol{k}, \\beta)[n] = \\frac{\\text{exp}\\big(\\mathcal{D}(\\boldsymbol{k}, M[n,:])\\beta \\big)}{\\sum_m\\text{exp}\\big(\\mathcal{D}(\\boldsymbol{k}, M[m,:])\\beta \\big)}\n\\end{align}\n\n\u3053\u3053\u3067\u3001$\\mathcal{D}$ \u306f2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u8ddd\u96e2\u3067\u3001\u3068\u308a\u304b\u305f\u306f\u3044\u308d\u3044\u308d\u8003\u3048\u3089\u308c\u307e\u3059\u304c\u3001\u8ad6\u6587\u306b\u5408\u308f\u305b\u3066\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\n\\begin{align}\n\\mathcal{D}(\\boldsymbol{u}, \\boldsymbol{v}) = \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{||\\boldsymbol{u}|| \\; ||\\boldsymbol{v}||}\n\\end{align}\n\n\u3068\u3057\u3066\u304a\u304d\u307e\u3059\u3002$\\beta \\rightarrow \\infty$ \u306e\u6975\u9650\u3067 $\\mathcal{C}$ \u306f\u92ed\u3044\u30d4\u30fc\u30af\u304c\u3072\u3068\u3064\u3060\u3051\u7acb\u3061\u307e\u3059\u3002\n\u3055\u3066\u3001\u3053\u3053\u3067\u5b9a\u7fa9\u3057\u305f\u6f14\u7b97\u3092\u7528\u3044\u3066\u3001(1)\u306e\u65b9\u6cd5\u306b\u3088\u308b write weighting \u306f\n\\begin{align}\n\\boldsymbol{c}_t^w=\\mathcal{C}(M_{t-1}, \\boldsymbol{k}_t^w, \\beta_t^w)\n\\end{align}\n\n\u3068\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002\n\u6b21\u306b(2)\u306e\u65b9\u6cd5\u306b\u3088\u308b\u8a08\u7b97\u3067\u3059\u3002\u591a\u5c11\u3084\u3084\u3053\u3057\u3044\u3067\u3059\u304c\u3001\u4ee5\u4e0b\u306e\u9806\u3067\u6c42\u3081\u3066\u3044\u304d\u307e\u3059\u3002\n(2)-1. \"retention vector\" $\\boldsymbol{\\psi}_t$ \u306e\u69cb\u6210\n\u524d time-step $t-1$ \u3067\u60c5\u5831\u3092\u8aad\u307f\u51fa\u3057\u305f Memory slot \u306f\u3001\u4f7f\u7528\u6e08\u307f slot \u3068\u3057\u3066\u66f8\u304d\u8fbc\u307f\u306b\u5229\u7528\u3057\u305f\u3044\u3068\u601d\u3046\u306e\u306f\u81ea\u7136\u3067\u3057\u3087\u3046\u3002\u3057\u304b\u3057\u3001\u4eca\u5f8c\u3082\u5fc5\u8981\u306a\u60c5\u5831\u304c\u4fdd\u6301\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u4e0a\u66f8\u304d\u3057\u3066\u306f\u3044\u3051\u307e\u305b\u3093\u3002\u3053\u306e\u3088\u3046\u306b\u3001\u524d time-step \u3067\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3063\u305f Memory slot \u3092\u672c\u5f53\u306b\u958b\u653e\u3057\u3066\u826f\u3044\u304b\u3069\u3046\u304b\u306e\u30d5\u30e9\u30b0\uff08\u5b9f\u969b\u306b\u306f0~1\u306e\u9023\u7d9a\u5024\u306a\u306e\u3067\u91cd\u307f\uff09\u304c \"free gate\" $f_t^i\\;(i=1,...,R)$ \u3067\u3059\u3002\u3059\u306a\u308f\u3061\u3001$f_t^i \\boldsymbol{w}_{t-1}^{r,i}$ \u3092\u6210\u5206\u5358\u4f4d\u3067\u8003\u3048\u3066\u3001\u300c$f_t^i w_{t-1}^{r,i}$ $\\simeq 1$ $\\Leftrightarrow f_t^i \\simeq 1$ and $w_{t-1}^{r,i}\\simeq 1$\u300d \u306a\u3089\u3070\u8aad\u307f\u8fbc\u307f\u6e08\u307f\u304b\u3064\u958b\u653eOK\u306a\u306e\u3067\u4e0a\u66f8\u304d\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u300c$f_t^i w_{t-1}^{r,i}$ $\\simeq 0$ $\\Leftrightarrow f_t^i \\simeq 0$ or $w_{t-1}^{r,i}\\simeq 0$\u300d \u306a\u3089\u3070 \u524d time-step \u3067\u8aad\u307f\u8fbc\u307e\u308c\u3066\u3044\u306a\u3044\u3001\u307e\u305f\u306f\u3001\u8aad\u307f\u8fbc\u307f\u6e08\u307f\u3060\u3063\u305f\u3068\u3057\u3066\u3082\u89e3\u653e\u30d5\u30e9\u30b0\u304c\u7acb\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u4e0a\u66f8\u304d\u4e0d\u53ef\u3068\u306a\u308a\u307e\u3059\u3002\u4eca\u3001\u5404\u56de\u306e\u8aad\u307f\u8fbc\u307f\u3054\u3068\u306b\u8003\u3048\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5168 $R$ \u56de\u3092\u3072\u3063\u304f\u308b\u3081\u3066\u306e\u4f7f\u7528\u4e2d\uff08\u4e0a\u66f8\u304d\u4e0d\u53ef\uff09\u30d5\u30e9\u30b0\u3067\u3042\u308b \"retention vector\" $\\boldsymbol{\\psi}_t$\u3092\n\\begin{align}\n\\boldsymbol{\\psi}_t = \\prod_{i=1}^R\\big(1-f_t^i\\boldsymbol{w}_{t-1}^{r, i}\\big)\n\\end{align}\n\n\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u7a4d\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u3068\u308a\u307e\u3059\u30021\u304b\u3089\u306e\u5dee\u3092\u6c42\u3081\u305f\u5f8c\u3067\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001$R$ \u56de\u306e\u3046\u30611\u5ea6\u3067\u3082\u4e0a\u66f8\u304d\u53ef\u3068\u5224\u65ad\u3055\u308c\u305f memory slot \u306f\u4e0a\u66f8\u304d\u53ef\u3068\u5224\u65ad\u3055\u308c\u307e\u3059\u3002\u4e00\u65b9\u3067\u3001\u4f7f\u7528\u4e2d\uff08\u4e0a\u66f8\u304d\u4e0d\u53ef\uff09 $\\psi_t \\simeq 1$ \u3068\u5224\u65ad\u3055\u308c\u308b\u306e\u306f\u3001$R$ \u56de\u5168\u3066\u306b\u3064\u3044\u3066\u4e0a\u66f8\u304d\u4e0d\u53ef\u3068\u5224\u65ad\u3055\u308c\u305f\u3068\u304d\u306b\u9650\u3089\u308c\u307e\u3059\u3002\n(2)-2. \"usage vector\" $\\boldsymbol{u}_t$ \u306e\u69cb\u6210\n(2)-1\u3067\u306f\u3001\u4f7f\u7528\u4e2d\u30d5\u30e9\u30b0\uff08\u6b63\u3057\u304f\u306f\u91cd\u307f\uff09\u3068\u3057\u3066\u306e $\\boldsymbol{\\psi}_t$ \u3092\u8003\u3048\u307e\u3057\u305f\u304c\u3001\u524d time-step \u3067\u306e\u8aad\u307f\u8fbc\u307f\u306b\u3064\u3044\u3066\u3057\u304b\u8003\u3048\u3066\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5b9f\u969b\u306b\u306f\u3001\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u5834\u5408\u306b\u306f\u4f7f\u7528\u4e2d\u306e\u5ea6\u5408\u3044\u304c\u9ad8\u307e\u308b\u306f\u305a\u3067\u3059\u3057\u3001\u76f4\u524d time-step \u3088\u308a\u524d\u306e time-step \u3067\u306e\u72b6\u6cc1\u3082\u8003\u3048\u308b\u3079\u304d\u3067\u3059\u3002\u3053\u308c\u3089\u3092\u8e0f\u307e\u3048\u3066\u3001\u5404 Memory slot \u306e\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044\u3092\u8868\u3059\u91cd\u307f \"memory usage vector\" $\\boldsymbol{u}_t$ \u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\\begin{align}\n\\boldsymbol{u}_t &= \\big(\\boldsymbol{u}_{t-1} + \\boldsymbol{w}_{t-1}^w - \\boldsymbol{u}_{t-1} \\circ \\boldsymbol{w}_{t-1}^w \\big) \\circ \\boldsymbol{\\psi}_t \\\\\n\\boldsymbol{u}_0 &= 0, \\;\\; \\boldsymbol{w}_0^w = 0\n\\end{align}\n\n$\\circ$\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u8868\u3057\u307e\u3059\u3002(...)\u5185\u7b2c3\u9805\u76ee\u306f $\\boldsymbol{u}_t$ \u306e\u5404\u6210\u5206\u304c $1$ \u3092\u8d85\u3048\u306a\u3044\u3088\u3046\u306b\u8abf\u6574\u3059\u308b\u305f\u3081\u306e\u88dc\u6b63\u3067\u3059\u3002$u_t = 1$ \u306b\u9054\u3059\u308b\u3068\u66f8\u304d\u8fbc\u307f\u306b\u3088\u308b\u91cd\u307f\u306e\u66f4\u65b0\u306f\u505c\u6b62\u3057\u307e\u3059\u3057\u3001\u3082\u3057 $u_t > 1$ \u3068\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3082\u66f4\u65b0\u306b\u3088\u308a\u5024\u3092\u6e1b\u5c11\u3055\u305b\u307e\u3059\u3002\n(2)-3. \"allocation weighting\" \u306e\u69cb\u6210\n\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044 $\\boldsymbol{u}_t$ \u306b\u57fa\u3065\u3044\u3066\u3001\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046 Memory slot \u306e address \u3092\u6c7a\u3081\u307e\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u3001\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044\u304c\u4f4e\u3044\u3068\u3053\u308d\u306b\u66f8\u304d\u8fbc\u3080\u8a33\u3067\u3059\u304c\u3001\u50be\u659c\u3092\u3064\u3051\u307e\u3059\u3002\u307e\u305a\u3001$\\boldsymbol{u}_t$ \u306e\u6210\u5206\u3092\u5024\u304c\u5c0f\u3055\u3044\u9806\u306b\u4e26\u3079\u305f\u6642\u306e index \u304b\u3089\u306a\u308b\u30d9\u30af\u30c8\u30eb\u3092  $\\boldsymbol{\\phi}_t$ \u3068\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\n\\begin{align}\n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[1]] \\leqq \\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[2]] \\leqq \n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[3]] \\leqq \\cdots \\leqq \n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[N]]\n\\end{align}\n\n\u3067\u3059\u3002\u3053\u308c\u3092\u7528\u3044\u3066\u3001\"allocation weighting\" $\\boldsymbol{a}_t$ \u3092\n\\begin{align}\n\\boldsymbol{a}_t[\\boldsymbol{\\phi}_t[n]] = \\big(1-\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[n]]\\big) \\prod_{m=1}^{n-1}\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[m]]\n\\end{align}\n\n\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u7a4d\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u3068\u308a\u307e\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u3001$1-\\boldsymbol{u}_t$ \u306a\u306e\u3067\u4f7f\u7528\u6e08\u307f\uff08\u66f8\u304d\u8fbc\u307f\u53ef\uff09\u306e\u5ea6\u5408\u3044\u3092\u8868\u3059\u91cd\u307f\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u4ee5\u4e0a\u3001(1)\u3068(2)\u306e\u7d50\u679c\u3092\u7d71\u5408\u3057\u3001write weighting $\\boldsymbol{w}_t^w$ \u3092\n\\begin{align}\n\\boldsymbol{w}_t^w = g_t^w\\big(g_t^a\\boldsymbol{a}_t+(1-g_t^a)\\boldsymbol{c}_t^w\\big)\n\\end{align}\n\n\u3068\u3057\u3066\u66f4\u65b0\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u3001$g_t^a$ \u3068 $g_t^w$ \u306f interface vector  $\\boldsymbol{\\xi}_t$ \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u305f\u3082\u306e\u3067\u3059\u3002\u305d\u308c\u305e\u308c\u3001\u300c\u4f7f\u7528\u6e08\u307f\u30d5\u30e9\u30b0 or \"key\"\u306e\u3069\u3061\u3089\u306b\u57fa\u3065\u3044\u3066\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046\u304b\u300d\u3001\u300c\u305d\u3082\u305d\u3082\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046\u304b\u300d\u3092\u5236\u5fa1\u3059\u308b gate \u3068\u3057\u3066\u50cd\u304d\u307e\u3059\u3002\n\n\u2461Memory \u3078\u306e\u66f8\u304d\u8fbc\u307f\nwrite weighting \u306e\u66f4\u65b0\u304c\u5b8c\u4e86\u3057\u307e\u3057\u305f\u306e\u3067\u3001Memory \u306b\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u66f8\u304d\u8fbc\u307f\u3068\u540c\u6642\u306b\u53e4\u3044\u60c5\u5831\u306e\u6d88\u53bb\u3082\u884c\u3044\u307e\u3059\u3002\u3069\u306e Memory slot \u3092\u5bfe\u8c61\u3068\u3059\u308b\u304b\u306e\u91cd\u307f\u3065\u3051\u306f\u3001\"write weighting\" $\\boldsymbol{w}_t^w$ \u3067\u884c\u3044\u307e\u3059\u3002\u66f8\u304d\u8fbc\u307e\u308c\u308b\u30c7\u30fc\u30bf\u306f \"write vector\" $\\boldsymbol{\\nu}_t$\u3001slot \u5185\u306e\u30c7\u30fc\u30bf\u3092\u3069\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\u3067\u6d88\u53bb\u3059\u308b\u304b\u306f\u3001erase vector $\\boldsymbol{e}_t$ \u3067\u4e0e\u3048\u3089\u308c\u3001\u3053\u306e2\u3064\u306f interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nMemory \u884c\u5217 $M_{t-1}$ \u306e\u6d88\u53bb\u30fb\u66f8\u304d\u8fbc\u307f\u306b\u3088\u308b\u66f4\u65b0\u306f\u4ee5\u4e0b\u306e\u5f0f\u306b\u5f93\u3063\u3066\u884c\u308f\u308c\u307e\u3059\u3002\n\\begin{align}\n& M_t[n,s] = M_{t-1}[n,s] \\big(1-\\boldsymbol{w}_t^w[n] \\boldsymbol{e}_t[s] \\big) + \\boldsymbol{w}_t^w[n] \\boldsymbol{\\nu}_t[s] \\\\\n& \\Leftrightarrow M_t = M_{t-1} \\circ \\big(1-\\boldsymbol{w}_t^w \\boldsymbol{e}_t^T \\big) + \\boldsymbol{w}_t^w \\boldsymbol{\\nu}_t^T \n\\end{align}\n\n\n\u2462read weighting\u306e\u66f4\u65b0\n\u8aad\u307f\u8fbc\u307f\u5148\u306e address \u306e\u9078\u629e\u306b\u30822\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\n(1) interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u305f \"key\" \u3092\u3082\u3068\u306b\u8aad\u307f\u8fbc\u307f\u5148 slot \u3092\u9078\u629e\u3001(2)\u66f8\u304d\u8fbc\u307f\u9806\u306b\u5f93\u3063\u3066 slot \u3092\u9078\u629e\u3001\u306e2\u3064\u3067\u3059\u3002\n(1)\u306b\u3064\u3044\u3066\u306f\u3001write weighting \u306e\u6642\u3068\u540c\u3058\u3088\u3046\u306b \"read key\" $\\{\\boldsymbol{k}_t^{r,i}\\}_{i=1,2,..,R}$ \u3068 \"read strength\" $\\{\\beta_t^{r,i}\\}_{i=1,2,..,R}$ \u3092\u7528\u3044\u3066\u3001\n\\begin{align}\n\\boldsymbol{c}_t^{r, i} = \\mathcal{C}\\big(M_t, \\boldsymbol{k}_t^{r,i}, \\beta_t^{r,i}\\big) \\;\\; (i=1,2,...,R)\n\\end{align}\n\n\u3068\u8a08\u7b97\u3057\u307e\u3059\u3002\n(2)\u306f\u3084\u3084\u9577\u3044\u3067\u3059\u304c\u3001\u9806\u306b\u8aac\u660e\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\u8aac\u660e\u306e\u4fbf\u5b9c\u4e0a(2)-2 \u2192(2)-1\u306e\u9806\u3067\u89e3\u8aac\u3057\u307e\u3059\u304c\u3001\u5b9f\u88c5\u3059\u308b\u969b\u306f(2)-1 \u2192(2)-2\u306e\u6d41\u308c\u3067\u3059\u3002\n(2)-2. \"precedence weighting\" \u306e\u69cb\u6210\nprecedence weighting $\\boldsymbol{p}_t$ \u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\\begin{align}\n& \\boldsymbol{p}_0 = 0 \\\\\n& \\boldsymbol{p}_t = \\Big(1-\\sum_{n=1}^N \\boldsymbol{w}_t^w[n]\\Big)\\boldsymbol{p}_{t-1} + \\boldsymbol{w}_t^w\n\\end{align}\n\n$\\boldsymbol{w}_t^w$ \u306f\u3059\u3067\u306b\u2460\u3067\u66f4\u65b0\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u2461\u3067\"\u5f37\u304f\"\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u5834\u5408\u306f\u3001$\\sum \\boldsymbol{w}_t^w \\simeq 1$ \u3068\u306a\u308b\u306e\u3067\u3001\u524d time-step \u306e\u60c5\u5831 $\\boldsymbol{p}_{t-1}$ \u306f\u6d88\u53bb\u3055\u308c\u3001\u73fe time-step \u3067\u3069\u3053\u306b\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u304b\u304c\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001\u6700\u5f8c\u306b\u884c\u308f\u308c\u305f\u66f8\u304d\u8fbc\u307f\u306e\u91cd\u307f\u304c\u4fdd\u6301\u3055\u308c\u7d9a\u3051\u307e\u3059\u3002\u307e\u305f\u3001$\\boldsymbol{p}_t[n] \\leq 1$\u3001$\\sum_n \\boldsymbol{p}_t[n] \\leq 1$ \u3067\u3042\u308b\u3053\u3068\u306f\u3059\u3050\u306b\u308f\u304b\u308a\u307e\u3059\u3002\n(2)-1. \"temporal link matrix\" \u306e\u69cb\u6210\nMemory slot \u3078\u306e\u66f8\u304d\u8fbc\u307f\u9806\u3092\u8868\u3059 $N$ \u00d7 $N$ \u884c\u5217 $L_t$ \u3092\u69cb\u6210\u3057\u307e\u3059\u3002\u6210\u5206\u5358\u4f4d\u3067\u307f\u305f\u3068\u304d\u306b $L_t[n, m] \\simeq 1$ \u3067\u3042\u308b\u3053\u3068\u306f \u300cMemory $M_t$ \u306b\u304a\u3044\u3066 slot 'n' \u306b\u3042\u308b\u60c5\u5831\u306f slot 'm' \u306b\u3042\u308b\u60c5\u5831 \u306e\u6b21\u306b\u66f8\u304d\u8fbc\u307e\u308c\u305f\u3082\u306e\u3067\u3042\u308b\u300d\u3068\u3044\u3046\u95a2\u4fc2\u304c\u8868\u73fe\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u66f4\u65b0\u5f0f\u3092\u4ee5\u4e0b\u3067\u4e0e\u3048\u307e\u3059\u3002\n\\begin{align}\n& L_0[n,m]=0 \\\\\n& L_t[n,n]=0 \\\\\n& L_t[n,m]=\\big(1-\\boldsymbol{w}_t^w[n]-\\boldsymbol{w}_t^w[m]\\big)L_{t-1}[n,m] + \\boldsymbol{w}_t^w[n]\\boldsymbol{p}_{t-1}[m]\n\\end{align}\n\n\u5bfe\u89d2\u6210\u5206\u306f\u610f\u5473\u3092\u306a\u3055\u306a\u3044\u305f\u3081 $0$ \u3067\u56fa\u5b9a\u3057\u307e\u3059\u3002$0 \\leqq L_t[n,m] \\leqq 1$ \u304a\u3088\u3073\u3001$\\sum_m L_t[n,m] \\leqq 1$ \u306f\u3059\u3050\u306b\u78ba\u304b\u3081\u3089\u308c\u307e\u3059\u3002$\\sum_n L_t[n,m] \\leqq 1$ \u306f\u78ba\u304b\u3081\u3088\u3046\u3068\u3057\u307e\u3057\u305f\u304c\u3001\u793a\u305b\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u793a\u305b\u305f\u65b9\u304c\u304a\u3089\u308c\u307e\u3057\u305f\u3089\u3001\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n(2)-3. \"forward/backward weighting\" \u306e\u69cb\u6210\nMemory \u3078\u306e\u66f8\u304d\u8fbc\u307f\u9806\u3092\u8003\u616e\u3057\u305f forward/backward weighting $\\{\\boldsymbol{f}_t^i\\}_{i=1,..,R}$ / $\\{\\boldsymbol{b}_t^i\\}_{i=1,..,R}$\u3092\u3084\u3084\u8352\u3063\u307d\u3044\u3067\u3059\u304c\n\\begin{align}\n&\\boldsymbol{f}_t^i[n]=\\sum_{m=1}^NL_t[n,m]\\boldsymbol{w}_{t-1}^{r,i}[m] \\; \\Leftrightarrow \\; \\boldsymbol{f}_t^i=L_t\\boldsymbol{w}_{t-1}^{r,i} \\\\\n&\\boldsymbol{b}_t^i[m]=\\sum_{n=1}^N\\boldsymbol{w}_{t-1}^{r,i}[n]L_t[n,m] \\; \\Leftrightarrow \\; \\boldsymbol{b}_t^i=L_t^T\\boldsymbol{w}_{t-1}^{r,i} \\\\\n\\end{align}\n\n\u3067\u69cb\u6210\u3057\u307e\u3059\u3002$\\sum_n L_t[n,m] \\leqq 1$ \u304c\u6210\u308a\u7acb\u3064\u306a\u3089\u3070\u3001$0\\leqq \\boldsymbol{f}_t^i[n] \\leqq 1$ \u304a\u3088\u3073 $\\sum_n\\boldsymbol{f}_t^i[n]\\leqq1$ \u304c\u6210\u7acb\u3057\u307e\u3059\u3002$\\boldsymbol{b}_t^i$ \u306b\u3064\u3044\u3066\u3082\u540c\u69d8\u3067\u3059\u3002\n\u4ee5\u4e0a\u3001(1)~(2)\u3092\u7d71\u5408\u3057\u3066\u3001read weighting $\\{\\boldsymbol{w}_t^{i,r}\\}_{i=1,...,R}$ \u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\n\\begin{align}\n\\boldsymbol{w}_t^{r,i}=\\boldsymbol{\\pi}_t^i[1]\\boldsymbol{b}_t^i + \\boldsymbol{\\pi}_t^i[2]\\boldsymbol{c}_t^i + \\boldsymbol{\\pi}_t^i[3]\\boldsymbol{f}_t^i \\;\\; (i=1,2,...,R)\n\\end{align}\n\n$\\{\\pi_t^i\\}_{i=1,...,R}$ \u306f interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u2463Memory\u304b\u3089\u306e\u8aad\u307f\u8fbc\u307f\nMemory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u306f\u7c21\u5358\u3067\u3059\u3002\u8aad\u307f\u51fa\u3057\u7d50\u679c\u306e \"read vector\" $\\{\\boldsymbol{r}_t^i\\}_{i=1,..,R}$ \u306f\n\\begin{align}\n\\boldsymbol{r}_t^i[s] = \\sum_{n=1}^N \\boldsymbol{w}_t^{r,i}[n]M_t[n,s] \\; \\Leftrightarrow \\; \\boldsymbol{r}_t^i = M_t^T\\boldsymbol{w}_t^{r,i} \\;\\; (i=1,2,...,R)\n\\end{align}\n\n\u3068\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002\n\u4ee5\u4e0a\u3092\u3082\u3063\u3066\u3001time-step $t$ \u306e Memory \u8aad\u307f\u66f8\u304d\u304c\u5b8c\u4e86\u3057\u307e\u3059\u3002\n\nPyhthon - Chainer \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\n\u4e0a\u3067\u8aac\u660e\u3057\u305f\u30ed\u30b8\u30c3\u30af\u306e python \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b3\u30fc\u30c9\u306f\u3001\nDNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5\n\u306b\u3042\u308b\u3082\u306e\u3067\u3059\u3002\u4e0a\u306e\u89e3\u8aac\u3068\u6bd4\u8f03\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u51e6\u7406\u3092\u95a2\u6570\u306b\u307e\u3068\u3081\u305f\u308a\u3001\u5909\u6570\u540d\u3092\u5909\u3048\u305f\u308a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u5185\u5bb9\u306f\u307b\u307c\u540c\u3058\u3082\u306e\u3067\u3059\u3002\n\u3072\u3068\u3064\u6ce8\u610f\u3057\u3066\u304a\u304d\u305f\u3044\u306e\u3067\u3059\u304c\u3001\u5168\u4f53\u50cf\u3067\u793a\u3057\u305f Controller \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{h}_t$ \u306f\u3001\u8ad6\u6587\u3067\u306f Controller \u306e\u6700\u7d42\u7684\u306a\u51fa\u529b\u3060\u3051\u3067\u306a\u304f\u3001hidden layer \u306e\u51fa\u529b\u3082\u3059\u3079\u3066\u7d71\u5408\u3057\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u307e\u3059\u3002\u3057\u304b\u3057\u7c21\u5358\u306e\u305f\u3081\u3001\u3053\u3053\u3067\u4f7f\u3046 Controller \u306f 2\u5c64\u7a0b\u5ea6\u306e\u7c21\u5358\u306a\u3082\u306e\u306b\u9650\u308a\u3001hidden layer \u306e\u51fa\u529b\u306f Controller \u306e\u5916\u306b\u306f\u53d6\u308a\u51fa\u3057\u307e\u305b\u3093\u3002\n\nDNC \u306e\u5b9f\u88c5\n\ndnc.py\nimport numpy as np\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import optimizers, Chain, Link, Variable\n\n\n# controller of DNC\nclass SimpleLSTM(Chain): \n\n    def __init__(self, d_in, d_hidden, d_out):\n        super(SimpleLSTM, self).__init__(\n            l1 = L.LSTM(d_in, d_hidden),\n            l2 = L.Linear(d_hidden, d_out),\n            )\n\n    def __call__(self, x):\n        return self.l2(self.l1(x))\n\n    def reset_state(self):\n        self.l1.reset_state()\n\n\nclass DNC(Chain):\n\n    def __init__(self, X, Y, N, W, R):\n        self.X = X # input dimension\n        self.Y = Y # output dimension\n        self.N = N # number of memory slot\n        self.W = W # dimension of one memory slot\n        self.R = R # number of read heads\n        self.d_ctr_in = W*R+X # input dimension into the controller\n        self.d_ctr_out = Y+W*R+3*W+5*R+3 # output dimension from the controller\n        self.d_ctr_hidden = self.d_ctr_out # dimension of hidden unit of the controller\n        self.d_interface = W*R+3*W+5*R+3 # dimension of interface vector\n\n        self.controller = SimpleLSTM(self.d_ctr_in, self.d_ctr_hidden, self.d_ctr_out)\n\n        super(DNC, self).__init__(\n            l_ctr = self.controller, \n            l_Wy = L.Linear(self.d_ctr_out, self.Y),\n            l_Wxi = L.Linear(self.d_ctr_out, self.d_interface), \n            l_Wr = L.Linear(self.R * self.W, self.Y), \n            )\n\n        self.reset_state()\n\n    def reset_state(self):\n        # initialize all the recurrent state\n        self.l_ctr.reset_state() # controller\n        self.u = Variable(np.zeros((self.N, 1)).astype(np.float32)) # usage vector (N, 1)\n        self.p = Variable(np.zeros((self.N, 1)).astype(np.float32)) # precedence weighting (N, 1)\n        self.L = Variable(np.zeros((self.N, self.N)).astype(np.float32)) # temporal memory linkage (N, N)                     \n        self.Mem = Variable(np.zeros((self.N, self.W)).astype(np.float32)) # memory (N, W)\n        self.r = Variable(np.zeros((1, self.R*self.W)).astype(np.float32)) # read vector (1, R * W)\n        self.wr = Variable(np.zeros((self.N, self.R)).astype(np.float32)) # read weighting (N, R)\n        self.ww = Variable(np.zeros((self.N, 1)).astype(np.float32)) # write weighting (N, 1)\n\n\n    # utility functions\n\n    def _cosine_similarity(self, u, v):\n        # cosine similarity as a distance of two vectors\n        # u, v: (1, -) Variable  -> (1, 1) Variable\n        denominator = F.sqrt(F.batch_l2_norm_squared(u) * F.batch_l2_norm_squared(v))\n        if (np.array_equal(denominator.data, np.array([0]))):\n            return F.matmul(u, F.transpose(v))\n        return F.matmul(u, F.transpose(v)) / F.reshape(denominator, (1, 1))\n\n\n    def _C(self, Mem, k, beta):\n        # similarity between rows of matrix Mem and vector k\n        # Mem:(N, W) Variable, k:(1, W) Variable, beta:(1, 1) Variable -> (N, 1) Variable\n        N, W = Mem.shape \n        ret_list = [0] * N\n        for i in range(N):\n            # calculate distance between i-th row of Mem and k\n            ret_list[i] = self._cosine_similarity(F.reshape(Mem[i,:], (1, W)), k) * beta \n        # concat horizontally because softmax operates along the direction of axis=1 \n        return F.transpose(F.softmax(F.concat(ret_list, 1))) \n\n\n    def _u2a(self, u):\n        # convert usage vector u to allocation weighting a\n        # u, a: (N, 1) Variable\n        N = u.shape[0]\n        phi = np.argsort(u.data.flatten()) # u.data[phi]: ascending\n        a_list = [0] * N    \n        cumprod = Variable(np.array([[1.0]]).astype(np.float32)) \n        for i in range(N): \n            a_list[phi[i]] = cumprod * (1.0 - F.reshape(u[phi[i]], (1, 1)))\n            cumprod *= F.reshape(u[phi[i]], (1, 1))\n        return F.concat(a_list, 0) \n\n\n    # operations of the DNC system\n\n    def _controller_io(self, x):\n        # input data from the Data Set : x (1, X) Variable  \n        # out-put from the controller h is split into two ways : v (1, Y), xi(1, W*R+3*W+5*R+3) Variable\n        chi = F.concat([x, self.r], 1) # total input to the controller \n        h = self.l_ctr(chi) # total out-put from the controller\n        self.v = self.l_Wy(h)\n        self.xi = self.l_Wxi(h)\n\n        # interface vector xi is split into several components\n        (self.kr, self.beta_r, self.kw, self.beta_w,\n         self.e, self.nu, self.f, self.ga, self.gw, self.pi\n         ) = F.split_axis(self.xi, np.cumsum(\n             [self.W*self.R, self.R, self.W, 1, self.W, self.W, self.R, 1, 1]), 1) # details of the interface vector\n\n        # rescale components\n        self.kr = F.reshape(self.kr, (self.R, self.W)) # read key (R, W)\n        self.beta_r = 1 + F.softplus(self.beta_r) # read strength (1, R)\n        # self.kw : write key (1, W)\n        self.beta_w = 1 + F.softplus(self.beta_w) # write strength (1, 1)\n        self.e = F.sigmoid(self.e) # erase vector (1, W)\n        # self.nu : write vector (1, W)\n        self.f = F.sigmoid(self.f) #  free gate (1, R)\n        self.ga = F.sigmoid(self.ga) # allcation gate (1, 1)\n        self.gw = F.sigmoid(self.gw) # write gate (1, 1)\n        self.pi = F.softmax(F.reshape(self.pi, (self.R, 3))) # read mode (R, 3)\n\n\n    def _up_date_write_weighting(self):\n        # calculate retention vector : psi (N, 1) \n        # here, read weighting : wr (N, R) must retain state one step former\n        psi_mat = 1 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), self.f) * self.wr # (N, R)\n        self.psi = Variable(np.ones((self.N, 1)).astype(np.float32)) \n        for i in range(self.R):\n            self.psi = self.psi * F.reshape(psi_mat[:,i],(self.N,1)) # (N, 1)\n        # up date usage vector : u (N, 1)\n        # here, write weighting : ww (N, 1) must retain state one step former\n        self.u = (self.u + self.ww - (self.u * self.ww)) * self.psi \n        # calculate allocation weighting : a (N, 1)\n        self.a = self._u2a(self.u) \n        # calculate write content weighting : cw (N, 1)\n        self.cw = self._C(self.Mem, self.kw, self.beta_w) \n        # up date write weighting : ww (N, 1)\n        self.ww = F.matmul(F.matmul(self.a, self.ga) + F.matmul(self.cw, 1.0 - self.ga), self.gw) \n\n\n    def _write_to_memory(self):\n        # erase vector : e (1, W) deletes information on the Memory : Mem (N, W) \n        # and write vector : nu (1, W) is written there\n        # write weighting : ww (N, 1) must be up-dated before this step\n        self.Mem = self.Mem * (np.ones((self.N, self.W)).astype(np.float32) - F.matmul(self.ww, self.e)) + F.matmul(self.ww, self.nu)\n\n\n    def _up_date_read_weighting(self):    \n        # up date temporal memory linkage : L (N, N)\n        ww_mat = F.matmul(self.ww, Variable(np.ones((1, self.N)).astype(np.float32))) # (N, N)\n        # here, precedence wighting : p (N, 1) must retain state one step former\n        self.L = (1.0 - ww_mat - F.transpose(ww_mat)) * self.L + F.matmul(self.ww, F.transpose(self.p)) # (N, N)\n        self.L = self.L * (np.ones((self.N, self.N)) - np.eye(self.N)) # constrain L[i,i] == 0   \n        # up date prcedence weighting : p (N, 1)\n        self.p = (1.0 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), F.reshape(F.sum(self.ww),(1, 1)))) * self.p + self.ww \n        # calculate forward weighting : fw (N, R)\n        # here, read wighting : wr (N, R) must retain state one step former\n        self.fw = F.matmul(self.L, self.wr) \n        # calculate backward weighting : bw (N, R)\n        self.bw = F.matmul(F.transpose(self.L), self.wr)\n        # calculate read content weighting : cr (N, R)\n        self.cr_list = [0] * self.R\n        for i in range(self.R):\n            self.cr_list[i] = self._C(self.Mem, F.reshape(self.kr[i,:], (1, self.W)), F.reshape(self.beta_r[0,i],(1, 1))) # (N, 1)\n        self.cr = F.concat(self.cr_list, 1) # (1, N * R)       \n        # compose up-dated read weighting : wr (N, R)\n        bcf_tensor = F.concat([\n                            F.reshape(F.transpose(self.bw), (self.R, self.N, 1)),\n                            F.reshape(F.transpose(self.cr), (self.R, self.N, 1)),\n                            F.reshape(F.transpose(self.fw), (self.R, self.N, 1))\n                            ], 2) # (R, N, 3)\n        self.pi = F.reshape(self.pi, (self.R, 3, 1)) # (R, 3, 1)\n        self.wr = F.transpose(F.reshape(F.batch_matmul(bcf_tensor, self.pi), (self.R, self.N))) # (N, R)\n\n\n    def _read_from_memory(self): \n        # read information from the memory : Mem (N, W) and compose read vector : r (W, R) to reshape (1, W * R)\n        # read weighting : wr (N, R) must be up-dated before this step\n        self.r = F.reshape(F.matmul(F.transpose(self.Mem), self.wr), (1, self.R * self.W))\n\n\n    def __call__(self, x):\n        self._controller_io(x) # input data is processed through the controller\n        self._up_date_write_weighting() \n        self._write_to_memory() # memory up-date\n        self._up_date_read_weighting()  \n        self._read_from_memory() # extract information from the memory\n        self.y = self.l_Wr(self.r) + self.v # compose total out put y : (1, Y)\n        return self.y\n\n\n\n\u4f7f\u7528\u4f8b\n\u4f7f\u7528\u4f8b\u3082\u8f09\u305b\u307e\u3059\u304c\u3001DNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5\u306b\u3042\u308b\u3082\u306e\u3068\u5185\u5bb9\u306f\u540c\u3058\u3067\u3059\u3002\n\u56fa\u5b9a\u9577\u306e one-hot \u30d9\u30af\u30c8\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306a\u500b\u6570\u5165\u529b\u3057\u3001\u5165\u529b\u5b8c\u4e86\u5f8c\u306b\u3001\u51fa\u529b\u3068\u3057\u3066\u5165\u529b\u30c7\u30fc\u30bf\u3092 echo \u3055\u305b\u307e\u3059\u3002echo \u3055\u305b\u305f\u30c7\u30fc\u30bf\u3068\u5165\u529b\u30c7\u30fc\u30bf\u306e2\u4e57\u8aa4\u5dee\u3092 loss \u3068\u3057\u3066\u30011\u30bb\u30c3\u30c8\u306e\u5165\u529b\u3054\u3068\u306b\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002\u5b66\u7fd2\u304c\u7d42\u4e86\u3057\u305f\u3089\u3001\u6b21\u30bb\u30c3\u30c8\u306e one-hot \u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\ndnc_echo_test.py\nimport dnc\n\nimport numpy as np\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import optimizers, Chain, Link, Variable\n\n\ndef onehot(x, n):\n    ret = np.zeros(n).astype(np.float32)\n    ret[x] = 1.0\n    return ret\n\n\nX = 5\nY = 5\nN = 10\nW = 10\nR = 2\n\n\nmodel = dnc.DNC(X, Y, N, W, R)\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n\nn_data = 10000 # number of input data\nloss = 0.0\nacc = 0.0\nacc_bool = []\n\n\nfor data_cnt in range(n_data):\n\n    loss_frac = np.zeros((1, 2))    \n\n    # prepare one pair of input and target data \n    # length of input data is randomly set\n    len_content = np.random.randint(3, 6) \n    # generate one input data as a sequence of randam integers\n    content = np.random.randint(0, X-1, len_content) \n    len_seq = len_content + len_content # the former is for input, the latter for the target\n    x_seq_list = [float('nan')] * len_seq # input sequence\n    t_seq_list = [float('nan')] * len_seq # target sequence\n\n    for i in range(len_seq):\n        # convert a format of input data\n        if (i < len_content):\n            x_seq_list[i] = onehot(content[i], X)\n        elif (i == len_content):\n            x_seq_list[i] = onehot(X-1, X)\n        else:\n            x_seq_list[i] = np.zeros(X).astype(np.float32)\n        # convert a format of output data\n        if (i >= len_content):\n            t_seq_list[i] = onehot(content[i - len_content], X)         \n\n    model.reset_state() # reset reccurent state per input data\n\n    # input data is fed as a sequence\n    for cnt in range(len_seq):\n        x = Variable(x_seq_list[cnt].reshape(1, X))\n        if (isinstance(t_seq_list[cnt], np.ndarray)):\n            t = Variable(t_seq_list[cnt].reshape(1, Y))\n        else:\n            t = []\n\n        y = model(x)\n\n        if (isinstance(t, chainer.Variable)):\n            loss += (y - t)**2\n            acc_bool.append(np.argmax(y.data)==np.argmax(t.data))                        \n            if (np.argmax(y.data)==np.argmax(t.data)): acc += 1\n\n        if (cnt+1==len_seq):\n            # training by back propagation\n            model.cleargrads()\n            loss.grad = np.ones(loss.shape, dtype=np.float32)\n            loss.backward()\n            optimizer.update()\n            loss.unchain_backward()\n            # print loss and accuracy\n            if data_cnt < 50 or data_cnt >= 9950:\n                print('(', data_cnt, ')', acc_bool, ' :: ', loss.data.sum()/loss.data.size/len_content, ' :: ', acc/len_content)\n            loss_frac += [loss.data.sum()/loss.data.size/len_seq, 1.]\n            loss = 0.0\n            acc = 0.0\n            acc_bool = []\n\n\n\ntest \u7d50\u679c\n10000\u56de\u7e70\u308a\u8fd4\u3057\u305f\u6642\u306e\u7d50\u679c\u3067\u3059\u3002\n[ ]\u5185\u306e bool \u5024\u306f\u3001\u51fa\u529b\u3079\u30af\u30c8\u30eb\u3067\u6700\u5927\u5024\u30921\u306b\u3001\u305d\u306e\u4ed6\u30920\u306b\u632f\u308a\u76f4\u3057\u305f\u3082\u306e\u304c\u3001\u5165\u529b\u5024\u3068\u4e00\u81f4\u3059\u308b\u304b\u3092 bool \u5024\u3067\u8868\u3057\u305f\u3082\u306e\u3067\u3059\u3002\n\u30fb\u6700\u521d\u306e20\u56de\u306e\u7d50\u679c\n( 0 ) [True, False, False]  ::  0.197543557485  ::  0.3333333333333333\n( 1 ) [False, False, False, False]  ::  0.209656882286  ::  0.0\n( 2 ) [True, False, False]  ::  0.172263367971  ::  0.3333333333333333\n( 3 ) [False, True, True]  ::  0.185363880793  ::  0.6666666666666666\n( 4 ) [True, True, True, True]  ::  0.157090616226  ::  1.0\n( 5 ) [False, False, False, False, False]  ::  0.191528530121  ::  0.0\n( 6 ) [True, False, False, False, False]  ::  0.175649337769  ::  0.2\n( 7 ) [False, False, False, True, True]  ::  0.173387451172  ::  0.4\n( 8 ) [True, False, True, True]  ::  0.150813746452  ::  0.75\n( 9 ) [False, True, False]  ::  0.163899072011  ::  0.3333333333333333\n( 10 ) [False, False, False, False, False]  ::  0.183468780518  ::  0.0\n( 11 ) [True, False, True, False]  ::  0.152743542194  ::  0.5\n( 12 ) [False, False, True, False]  ::  0.170574557781  ::  0.25\n( 13 ) [False, True, False, True, False]  ::  0.161617393494  ::  0.4\n( 14 ) [False, False, False, False]  ::  0.168220555782  ::  0.0\n( 15 ) [False, False, False]  ::  0.167814588547  ::  0.0\n( 16 ) [False, True, False, False]  ::  0.158575570583  ::  0.25\n( 17 ) [False, False, False, False]  ::  0.165678012371  ::  0.0\n( 18 ) [False, False, False]  ::  0.165241924922  ::  0.0\n( 19 ) [False, True, False]  ::  0.143808253606  ::  0.3333333333333333\n\n\u30fb\u6700\u5f8c20\u56de\u306e\u7d50\u679c\n( 9980 ) [True, True, True, True]  ::  0.000208107382059  ::  1.0\n( 9981 ) [True, True, True, True, True]  ::  0.000164349582046  ::  1.0\n( 9982 ) [True, True, True, True, True]  ::  0.000122650777921  ::  1.0\n( 9983 ) [True, True, True]  ::  0.000181751077374  ::  1.0\n( 9984 ) [True, True, True, True, True]  ::  0.000318505689502  ::  1.0\n( 9985 ) [True, True, True, True, True]  ::  0.00023639023304  ::  1.0\n( 9986 ) [True, True, True, True, True]  ::  0.000988183766603  ::  1.0\n( 9987 ) [True, True, True, True, True]  ::  0.000226851813495  ::  1.0\n( 9988 ) [True, True, True]  ::  0.000401457709571  ::  1.0\n( 9989 ) [True, True, True, True]  ::  0.000256504747085  ::  1.0\n( 9990 ) [True, True, True, True, True]  ::  0.000165695995092  ::  1.0\n( 9991 ) [True, True, True, True]  ::  0.000123940082267  ::  1.0\n( 9992 ) [True, True, True, True, True]  ::  0.000351718552411  ::  1.0\n( 9993 ) [True, True, True, True]  ::  0.000147357559763  ::  1.0\n( 9994 ) [True, True, True, True]  ::  0.000173216045368  ::  1.0\n( 9995 ) [True, True, True, True]  ::  0.000108330522198  ::  1.0\n( 9996 ) [True, True, True, True]  ::  0.00016659933608  ::  1.0\n( 9997 ) [True, True, True]  ::  0.000255667418242  ::  1.0\n( 9998 ) [True, True, True]  ::  0.000280433737983  ::  1.0\n( 9999 ) [True, True, True, True, True]  ::  0.000443447269499  ::  1.0\n\n\u5b8c\u74a7\u306b\u6b63\u89e3\u3057\u3066\u3044\u307e\u3059\u300220\u56de\u5206\u3057\u304b\u8f09\u305b\u3066\u3044\u307e\u305b\u3093\u304c\u3001\u6700\u5f8c\u306e100\u56de\u4e2d\u3067False\u306f0\u56de\u3067\u3057\u305f\u3002\u305f\u3060\u3057\u3001\u4ed6\u306e\u624b\u6cd5\u3068\u6bd4\u8f03\u307e\u3067\u306f\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001DNC \u3060\u304b\u3089\u3053\u305d\u306a\u306e\u304b\u306f\u4e0d\u660e\u3067\u3059\u3002\n\u4eca\u56de\u306f\u30ed\u30b8\u30c3\u30af\u306e\u7d39\u4ecb\u304c\u76ee\u7684\u3060\u3063\u305f\u306e\u3067\u3053\u3053\u307e\u3067\u3068\u3057\u307e\u3059\u3002\n\n\u53c2\u8003URL\n\u30fbChainer\u306e\u5b9f\u88c5\u4f8b\u306f\u3053\u306e\u30da\u30fc\u30b8\u306e\u3082\u306e\u3092\u4f7f\u308f\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\nDNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5\n\u30fb\u4eca\u56de\u306fGPU\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306a\u3069\u306f\u8003\u3048\u3066\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u4e0b\u306e\u30da\u30fc\u30b8\u304c\u53c2\u8003\u306b\u306a\u308b\u304b\u3068\u601d\u3044\u307e\u3059\n(Chainer) DNC(Differentiable Neural Computers)\u3067\u6587\u5b57\u5217\u306e\u5b66\u7fd2&\u751f\u6210\n\u30fbTensor Flow\u3067\u306e\u5b9f\u88c5\u3082\u3042\u308b\u3088\u3046\u3067\u3059\nhttps://github.com/Mostafa-Samir/DNC-tensorflow\n\u30fbLSTM\u306e\u89e3\u8aac\u306f\u3053\u3053\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\n\u308f\u304b\u308bLSTM \uff5e \u6700\u8fd1\u306e\u52d5\u5411\u3068\u5171\u306b\n### \u3053\u306e\u8a18\u4e8b\u306b\u3064\u3044\u3066\u3000\nDeepMind \u304c Nature \u306b\u6295\u7a3f\u3057\u305f\u8ad6\u6587 \n[Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz)\n\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b \"Differentiable Neural Computing (DNC)\" \u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3059\u3002\u30ed\u30b8\u30c3\u30af\u306e\u8aac\u660e\u304c\u30e1\u30a4\u30f3\u3067\u3059\u304c\u3001Python - Chainer \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\u3082\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n# Differentiable Neural Computing (DNC)\u3068\u306f\nsequencial data \u3092 Neural Net \u3067\u51e6\u7406\u3057\u305f\u3044\u3068\u3044\u3046\u6b32\u6c42\u306f\u6614\u304b\u3089\u3042\u308b\u3088\u3046\u3067\u3001\u4e00\u756a\u30b9\u30bf\u30f3\u30c0\u30fc\u30c9\u306a\u3082\u306e\u306f Recurrent Neural Net (RNN) \u3067\u3059\u3002\u3057\u304b\u3057\u3001RNN \u306b\u306f\"\u52fe\u914d\u6d88\u5931\u554f\u984c\"\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308a\u307e\u3057\u3066\u3001\u305d\u308c\u3092\u514b\u670d\u3057\u305f\u30e2\u30c7\u30eb\u304c Long Short Term Memory (LSTM) \u3068\u547c\u3070\u308c\u3066\u3044\u307e\u3059\u3002\u4f55\u304c short \u3067\u4f55\u304c long \u306a\u306e\u304b\u3068\u3044\u3046\u3068\u3001\u5165\u529b\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001\u51fa\u529b\u3055\u308c\u308b\u307e\u3067 Neural Net \u306e\u5185\u90e8\u306b\u3068\u3069\u307e\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u3059\u3002\u3053\u308c\u306f\u3042\u308b\u610f\u5473\u3067 \"Memory\" \u306e\u3088\u3046\u306a\u3082\u306e\u3068\u8003\u3048\u3066\u3082\u3088\u3055\u305d\u3046\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u5165\u529b\u304b\u3089\u51fa\u529b\u307e\u3067\u306e\"\u77ed\u6642\u9593\"\u3057\u304b\u8a18\u61b6\u304c\u3067\u304d\u306a\u3044\u3002\u306a\u306e\u3067\u3001\"short term momory\" \u3068\u547c\u3070\u308c\u307e\u3059\u3002\u300c\u3053\u306e short term memory \u306e\u8a18\u61b6\u3067\u304d\u308b\u6642\u9593\u304c\u9577\u304f\u306a\u308a\u307e\u3057\u305f\u3088\u300d\u3068\u3044\u3046\u610f\u5473\u3067\u3001Long Short Term Memory \u3068\u540d\u4ed8\u3051\u3089\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u305d\u3046\u3059\u308b\u3068\u3001\u672c\u5f53\u306b long \u306a Meomory \u306f\u5b66\u7fd2\u6a5f\u5185\u90e8\u3067\u306f\u306a\u304f\u3066\u5916\u90e8\u306b\u3042\u3063\u3066\u3082\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3001\u305d\u3093\u306a\u6c17\u304c\u3057\u3066\u304d\u307e\u3059\u3002\u305d\u3046\u3044\u3046\u601d\u3044\u304b\u3089\u751f\u307e\u308c\u305f\u3001\u304b\u3069\u3046\u304b\u306f\u5206\u304b\u308a\u307e\u305b\u3093\u304c\u3001Differentiable Neural Computing (DNC) \u3068\u306f\u3001\u5b66\u7fd2\u6a5f\u306e\u5916\u90e8\u306b Memory \u3092\u7528\u610f\u3057\u3066\u3084\u3063\u3066\u3001Memory \u306e\u4f7f\u7528\u6cd5\u307e\u3067\u542b\u3081\u3066\u5b66\u7fd2\u3055\u305b\u3066\u3057\u307e\u3046\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\"Differentiable\"\u306e\u610f\u5473\u3067\u3059\u304c\u3001back propagation \u3067\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u601d\u3046\u3068\u3001\u5fae\u5206\u3092\u8a08\u7b97\u3067\u304d\u308b\u3053\u3068\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\u3067\u3059\u306e\u3067\u3001\u5916\u90e8 Memory \u306b\u5bfe\u3059\u308b\u64cd\u4f5c\u3082\u542b\u3081\u3066\"\u5fae\u5206\u8a08\u7b97\u304c\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\"\u3068\u3044\u3046\u610f\u5473\u3067\u3059\u3002\n\n# \u30ed\u30b8\u30c3\u30af\u89e3\u8aac\n## DNC \u306e\u5168\u4f53\u50cf\nDNC \u306e\u69cb\u6210\u8981\u7d20\u306f\u3001\u672c\u4f53\u3067\u3042\u308b Controller \u3068\u5916\u90e8 Memory \u3067\u3059\u3002\u8ad6\u6587\u306b\u3088\u308b\u3068\u3001\u5168\u4f53\u3068\u3057\u3066\u306e\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u306f\u4e0b\u56f3\u306e\u3088\u3046\u306b\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n![DNC_concept.jpg](https://qiita-image-store.s3.amazonaws.com/0/132984/6569adeb-237c-5768-ff40-e723d34892f2.jpeg)\nDNC \u306f sequencial data \u3092\u6271\u3044\u307e\u3059\u306e\u3067\u3001\u73fe\u5728\u306e time-step \u3092 $t$ \u3068\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u524d step \u306f $t-1$\u3001\u6b21 step \u306f $t+1$ \u3068\u306a\u308a\u3001\u56f3\u4e2d\u306e\u6dfb\u3048\u5b57 $t$ \u306f\u3001\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u304c\u3069\u306e time-step \u3067\u751f\u6210\u3055\u308c\u305f\u3082\u306e\u3067\u3042\u308b\u304b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u305d\u308c\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u3092\u9806\u3092\u8ffd\u3063\u3066\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\u56f3\u4e2d\u306b(1)~(4)\u306e\u756a\u53f7\u304c\u632f\u3063\u3066\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u3053\u306e\u9806\u3067\u8ffd\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n(1) : \"Data Set\" \u3088\u308a \u5165\u529b\u30c7\u30fc\u30bf $x_t$ \u304c\u5165\u529b\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306b\u3001\u524d step \u3067\u306e Memory \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{r}\\_{t-1}$ \u3068\u3092\u5408\u308f\u305b\u305f $\\boldsymbol{\\chi}\\_t=[\\boldsymbol{x}\\_t, \\boldsymbol{r}\\_{t-1}]$ \u3092 Controller \u3078\u306e\u5165\u529b\u3068\u3057\u307e\u3059\u3002\n(2) : Controller \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{h}\\_t$ \u304c\u5f97\u3089\u308c\u307e\u3059\u306e\u3067\u3001\u3053\u308c\u30922\u3064\u306e\u30eb\u30fc\u30c8\u306b\u632f\u308a\u5206\u3051\u307e\u3059\u3002\u3072\u3068\u3064\u306f\u3001\"Out Put\" \u306b\u5411\u3051\u3066\u306e $\\boldsymbol{v}\\_t$\u3001\u3082\u30461\u3064\u306f Memory \u3092\u5236\u5fa1\u3059\u308b\u305f\u3081\u306e \"interface vector\" $\\boldsymbol{\\xi}\\_t$ \u3067\u3059\u3002\u8ad6\u6587\u4e2d\u3067\u306f\u3001\u3053\u308c\u3089\u306f $\\boldsymbol{h}\\_t$ \u306e\u7dda\u5f62\u5909\u63db $\\boldsymbol{v}\\_t=W\\_y \\boldsymbol{h}\\_t$ \u3068\u3001$\\boldsymbol{\\xi}\\_t=W\\_{\\xi} \\boldsymbol{h}\\_t$ \u3068\u3057\u3066\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002\n(3) : \"interface vector\" $\\boldsymbol{\\xi}\\_t$ \u3092\u3082\u3068\u306b Memory \u3078\u306e\u30c7\u30fc\u30bf\u306e\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c Memory \u306e\u72b6\u614b\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u3082\u884c\u308f\u308c\u3001\"read vector\" $\\boldsymbol{r}_t$ \u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n(4) : \"read vector\" $\\boldsymbol{r}\\_t$ \u306f\u3001 \"Out Put\"\u3078\u306e\u51fa\u529b\u306b\u52a0\u7b97\u3055\u308c\u308b\u4e00\u65b9\u3067\u3001\u6b21 step \u3067\u306e Controller \u3078\u306e\u5165\u529b\u3078\u3068\u56de\u3055\u308c\u307e\u3059\u3002\n(5) : Controller \u304b\u3089\u306e\u51fa\u529b\u3068\u3001Memory\u304b\u3089\u306e\u51fa\u529b\u3092\u5408\u6210\u3057\"Out Put\" \u3078 $\\boldsymbol{y}\\_t = \\boldsymbol{v}\\_t + W\\_r \\boldsymbol{r}\\_t$ \u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\u4ee5\u4e0a\u3001(1)~(5) \u3092\u3082\u3063\u3066\u30011 step \u304c\u5b8c\u4e86\u3057\u307e\u3059\u3002\n\nController \u3068\u3057\u3066\u306f\u3001\u591a\u6b21\u5143\u306e\u5165\u529b\u3092\u53d7\u3051\u53d6\u308a\u3001\u591a\u6b21\u5143\u306e\u51fa\u529b\u3092\u8fd4\u3059\u5b66\u7fd2\u6a5f\u306a\u3089\u4f55\u3067\u3082\u3088\u3044\u306e\u3067\u3059\u304c\u3001(Recurrent) Neural Net \u3084 LSTM\u306a\u3069\u3092\u4f7f\u3046\u3053\u3068\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002(Recurrent) Neural Net \u3084 LSTM \u306e\u89e3\u8aac\u306f\u4ed6\u306b\u3086\u305a\u308b\u3068\u3057\u3066\u3001\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001DNC \u306e\u6700\u5927\u306e\u7279\u5fb4\u3067\u3042\u308b\u300c\u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u300d\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n## \u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u306b\u3064\u3044\u3066\n\u305d\u308c\u3067\u306f\u3001\u5916\u90e8 Memory \u306e\u8aad\u307f\u66f8\u304d\u306b\u3064\u3044\u3066\u89e3\u8aac\u3057\u307e\u3057\u3087\u3046\u3002\u9762\u5012\u306a\u306e\u3067\u3001\u4ee5\u4e0b\u3067\u306f\u5358\u306b Memory \u3068\u547c\u3076\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n### Memory \u306e\u69cb\u9020\n\u307e\u305a\u306f\u3001Memory \u306e\u69cb\u9020\u306b\u3064\u3044\u3066\u628a\u63e1\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\nMemory \u3068\u3057\u3066\u306f $N$ \u00d7 $W$ \u306e\u884c\u5217\u3092\u7528\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001address \u304c $1$~$N$ \u307e\u3067\u632f\u3089\u308c\u3066\u3044\u3066\u3001\u5404 address \u306b $W$ \u6b21\u5143\u306e\u6570\u5024\u30d9\u30af\u30c8\u30eb\u3092\u683c\u7d0d\u3067\u304d\u308b slot \u304c\u3042\u308b\u3068\u8003\u3048\u307e\u3059\u3002Memory \u306e\u72b6\u614b\u306f\u523b\u3005\u3068\u66f4\u65b0\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001time-step $t$ \u3067\u306e Memory \u3092\u8868\u3059\u884c\u5217\u3092 $M\\_t$ \u3068\u66f8\u304f\u3053\u3068\u306b\u3057\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u884c\u5217\u306e\u6b21\u5143 $N$ \u00d7 $W$ \u306f\u56fa\u5b9a\u3055\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u3001\u5e38\u306b\u3001$N$ \u306f memory slot \u306e\u7dcf\u6570 \uff08address \u306e\u7dcf\u6570\uff09\u3001$W$ \u306f slot \u306e\u9577\u3055\uff08\u683c\u7d0d\u3059\u308b\u6570\u5024\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\uff09\u3068\u3057\u307e\u3059\u3002\n\n### interface vector \u306e\u8a73\u7d30\n\u300c\u5168\u4f53\u306e\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u300d\u3067\u3082\u8ff0\u3079\u305f\u901a\u308a\u3001Controller \u306b\u3088\u308b Memory \u306e\u64cd\u4f5c\u306f\n\"interface vector\" $\\boldsymbol{\\xi}\\_t$ \u3092\u901a\u3057\u3066\u884c\u308f\u308c\u307e\u3059\u3002\u4e00\u53e3\u3067 Memory \u306e\u64cd\u4f5c\u3068\u3044\u3063\u3066\u3082\u3001\u8aad\u307f\u8fbc\u307f/\u66f8\u304d\u8fbc\u307f/address\u306e\u6307\u5b9a\u306a\u3069\u3001\u8907\u6570\u306e\u30bf\u30b9\u30af\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u306e\u3067\u3001$\\boldsymbol{\\xi}\\_t$ \u306f\u6a5f\u80fd\u5225\u306b\u5404 component \u306b\u5206\u89e3\u3055\u308c\u307e\u3059\u3002\n\n``` math\n\\begin{align}\n\\boldsymbol{\\xi}_t = \\Big[ \\boldsymbol{k}_t^{r, 1},...,\\boldsymbol{k}_t^{r, R}, \\; \\hat{\\beta}_t^{r, 1},...,\\hat{\\beta}_t^{r, R}, \\; \\boldsymbol{k}_t^w, \\; \\hat{\\beta_t}^w, \\hat{\\boldsymbol{e}}_t, \\; \\boldsymbol{\\nu}_t, \\; \\hat{f}_t^1,...,\\hat{f}_t^R, \\; \\hat{g}_t^a, \\; \\hat{g}_t^w, \\; \\hat{\\boldsymbol{\\pi}}_t^1,...,\\hat{\\boldsymbol{\\pi}}_t^R \\Big]\n\\end{align}\n```\n'$\\hat{}$' \u8a18\u53f7\u304c\u3064\u3044\u3066\u3044\u308b\u3082\u306e\u306f\u3001\u3055\u3089\u306b scale \u5909\u63db\u3092\u304b\u3051\u307e\u3059\u3002\u7a2e\u985e\u304c\u591a\u304f\u3066\u3084\u3084\u3053\u3057\u3044\u306e\u3067\u3001\u3044\u3063\u305f\u3093\u307e\u3068\u3081\u307e\u3059\u3002( , )\u5185\u306e\u5024\u306f\u3001(\u6b21\u5143, \u500b\u6570)\u3067\u3059\u3002\n\u30fb\"read key\" $(W, R) \\; :\\; \\boldsymbol{k}\\_t^{r, 1},...,\\boldsymbol{k}\\_t^{r, R}$\n\u30fb\"read strength\" $(1, R) \\; : \\; \\beta\\_t^{r, 1},...,\\beta\\_t^{r, R} \\;\\;\\big(\\beta\\_t^{r, i}=\\text{oneplus}(\\hat{\\beta}\\_t^{r, i})\\big)$\n\u30fb\"write key\" $(W, 1) \\; : \\; \\boldsymbol{k}\\_t^w$\n\u30fb\"write strength\" $(1,1) \\; : \\; \\beta\\_t^w=\\text{oneplus}(\\hat{\\beta}\\_t^w)$\n\u30fb\"erase vector\" $(W, 1) \\; : \\; \\boldsymbol{e}\\_t=\\sigma(\\hat{\\boldsymbol{e}}_t)$\n\u30fb\"write vector\" $(W, 1) \\; : \\; \\boldsymbol{\\nu}\\_t$\n\u30fb\"free gate\" $(1, R) \\; : \\; f\\_t^1,...,f\\_t^R \\;\\; \\big(f\\_t^i=\\sigma(\\hat{f}\\_t^i)\\big)$ \n\u30fb\"allocation gate\" $(1, 1) \\; :\\; g^a\\_t=\\sigma(\\hat{g}^a\\_t)$\n\u30fb\"write gate\" $(1, 1) \\; :\\; g^w\\_t=\\sigma(\\hat{g}^w\\_t)$\n\u30fb\"read mode\" $(3, R) \\; : \\; \\boldsymbol{\\pi}\\_t^1,...,\\boldsymbol{\\pi}\\_t^R \\;\\; \\big(\\boldsymbol{\\pi}\\_t^i=\\text{softmax}(\\hat{\\boldsymbol{\\pi}}\\_t^i)\\big)$  \n\u30b9\u30b1\u30fc\u30eb\u5909\u63db\u306b\u7528\u3044\u308b\u95a2\u6570\u306f\u3001\n\n```math\n\\begin{align}\n& \\text{oneplus}(x)\u3000= 1+\\text{log}(1+e^x) \\in [1, \\infty) \\\\\n& \\sigma(x) = \\frac{1}{1+e^{-x}} \\in [0, 1] \\\\\n& \\text{softmax}(\\boldsymbol{x}) = \\frac{e^\\boldsymbol{x}}{\\sum_ie^{x_i}} \\in [0, 1]\n\\end{align}\n```\n\u3068\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u307e\u3059\u3002$\\text{oneplus}(x)$\u3001 $\\sigma(x)$\u3001$\\text{exp}(x)$ \u306e\u5f15\u6570\u306b\u30d9\u30af\u30c8\u30eb\u5024\u3092\u53d6\u308b\u5834\u5408\u306f\u3001\u6210\u5206\u3054\u3068\u306e\u4f5c\u7528\u3092\u610f\u5473\u3057\u307e\u3059\u306e\u3067\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u3053\u3053\u3067\u306f\u3001\u5b9a\u7fa9\u3092\u7f85\u5217\u3057\u305f\u3060\u3051\u306a\u306e\u3067\u3001\u4f55\u3082\u308f\u304b\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u4ee5\u4e0b\u3001\u3053\u308c\u3089\u306e component \u3092\u3069\u306e\u3088\u3046\u306b\u4f7f\u3063\u3066 Memory \u3092\u5236\u5fa1\u3057\u3066\u3044\u304f\u304b\u3001\u9806\u306b\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n\u5148\u306b\u9032\u3080\u524d\u306b\u30011\u70b9\u3060\u3051\u8aac\u660e\u3092\u52a0\u3048\u3066\u304a\u304d\u307e\u3059\u3002$R$ \u306f Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u306e\u56de\u6570\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u8ad6\u6587\u4e2d\u306e DNC \u3067\u306f\u30011 step \u306e\u3046\u3061\u3067 Memory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u3092\u8907\u6570\u56de\u884c\u3046\u8a2d\u5b9a\u3068\u306a\u3063\u3066\u3044\u3066\u3001\u305d\u306e\u56de\u6570\u304c $R$ \u3067\u3059\u3002\u5bfe\u3057\u3066\u3001\u66f8\u304d\u8fbc\u307f\u306f\u30011 step \u306b1\u56de\u306e\u307f\u306e\u8a2d\u5b9a\u3067\u3059\u3002\u307e\u305f\u3001interface vector \u306e\u6b21\u5143\u306f\u3001$W$ \u3068 $R$ \u3067\u6c7a\u307e\u3063\u3066\u3044\u3066\u3001$WR+3W+5R+3$ \u3068\u306a\u308b\u3053\u3068\u3082\u5206\u304b\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\n### Memory \u306e\u8aad\u307f\u66f8\u304d\u624b\u9806\uff08\u6982\u8981\uff09\nMemory \u306e\u8aad\u307f\u66f8\u304d\u306b\u306f\u3001\u8aad\u307f\u66f8\u304d\u5bfe\u8c61\u3068\u306a\u308b Memory slot \u306e address \u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305f\u3060\u3057\u3001back propagation \u306b\u3088\u308b\u5b66\u7fd2\u304c\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u306b\u306f\u3001\u5fae\u5206\u53ef\u80fd\u6027\u3001\u5c11\u306a\u304f\u3068\u3082\u6f14\u7b97\u306e\u9023\u7d9a\u6027\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\u3002\u3088\u3063\u3066\u3001\u7279\u5b9a\u306e\u3072\u3068\u3064\u306e address \u3060\u3051\u3092\u6307\u5b9a\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5e45\u3092\u6301\u305f\u305b\u3066\u3001\u3069\u306e address \u3092\u91cd\u70b9\u7684\u306b\u8aad\u307f\u8fbc\u3080/\u66f8\u304d\u8fbc\u3080\u304b\u306e\u91cd\u307f\u3065\u3051\u3092\u884c\u3044\u307e\u3059\u3002\u4ee5\u5f8c\u3001\u3053\u306e\u91cd\u307f\u3092\u3001\"read/write weighting\"\u3068\u547c\u3076\u3053\u3068\u306b\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3089\u3092\u3069\u3046\u6c42\u3081\u3066\u3044\u304f\u304b\u304c\u30dd\u30a4\u30f3\u30c8\u306b\u306a\u308a\u307e\u3059\u3002\n\n\"read/write weighting\" \u3092\u6c42\u3081\u3066\u3057\u307e\u3048\u3070\u3001\u8aad\u307f\u8fbc\u307f/\u66f8\u304d\u8fbc\u307f\u306e\u6f14\u7b97\u81ea\u4f53\u306f\u5358\u7d14\u3067\u3059\u3002\u8a73\u7d30\u306f\u5f8c\u306b\u56de\u3057\u307e\u3059\u304c\u3001\u6982\u8981\u3092\u3064\u304b\u3093\u3067\u304a\u304d\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u3001\u8aad\u307f\u8fbc\u307f\u306e\u5834\u5408\u3092\u8003\u3048\u3066\u307f\u307e\u3059\u3002\u4eca\u3001\"read weighting\" $\\boldsymbol{w}^r\\_t$ \u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002\u3053\u306e\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u306f\u3001Memory slot \u306e\u7dcf\u6570\u3068\u540c\u3058 $N$ \u3067\u3059\u3002\u5404\u6210\u5206\u306f\u5bfe\u5fdc\u3059\u308b address \u306e Memory slot \u306b\u3042\u308b\u60c5\u5831\u3092\u3069\u308c\u3060\u3051\"\u5f37\u304f\"\u8aad\u307f\u8fbc\u3080\u304b\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002\u3059\u306a\u308f\u3061\u3001\u8aad\u307f\u3060\u3055\u308c\u308b\u60c5\u5831\u306f\u3001Memory \u884c\u5217\u3068\u91cd\u307f\u30d9\u30af\u30c8\u30eb\u306e\u7a4d\u3068\u3057\u3066\u3001$M\\_t^T\\boldsymbol{w}^r\\_t$\u3068\u8868\u3055\u308c\u307e\u3059\u3002\u5148\u306b\u3082\u8ff0\u3079\u307e\u3057\u305f\u304c\u3001\u8ad6\u6587\u4e2d\u3067\u306f 1 step \u3067\u8aad\u307f\u51fa\u3057\u3092 $R$ \u56de\u884c\u3046\u8a2d\u5b9a\u306a\u306e\u3067\u3001\"read weighting\" \u3082 $R$ \u500b $\\\\{\\boldsymbol{w}\\_t^{r,i}\\\\}\\_{i=1,...,R}$ \u69cb\u6210\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u8aad\u307f\u51fa\u3057\u306b\u3088\u308b\u5897\u5e45\u306a\u3069\u9632\u3050\u305f\u3081\u3001\u81ea\u7136\u306a\u8981\u8acb\u3068\u3057\u3066\n\n```math\n\\begin{align}\n& 0 \\leqq \\boldsymbol{w}_t^{r,i}[n] \\leqq 1 \\;\\; (n=1,2,...,N)\\\\\n& \\sum_{n=1}^N \\boldsymbol{w}_t^{r,i}[n] \\leqq 1 \\;\\; (i=1,2,...,R)\n\\end{align}\n```\n\u3092\u8ab2\u3059\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\u540c\u69d8\u306b\u3001\u66f8\u304d\u8fbc\u307f\u306e\u5834\u5408\u3082\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\"write weighting\" $\\boldsymbol{w_t^w}$ \u304c\u5f97\u3089\u308c\u305f\u3068\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u66f8\u304d\u8fbc\u307f\u305f\u3044\u60c5\u5831\u3092\u8868\u3059\u30d9\u30af\u30c8\u30eb $\\boldsymbol{\\nu}\\_t$ \u3082\u5f97\u3089\u308c\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002$\\boldsymbol{w_t^w}$\u306e\u6b21\u5143\u306f$N$\u3067\u3001\u5404\u6210\u5206\u306f\u5bfe\u5fdc\u3059\u308b Memory slot \u306b\u3069\u308c\u3060\u3051\"\u5f37\u304f\" $\\boldsymbol{\\nu}\\_t$ \u3092\u66f8\u304d\u8fbc\u3080\u304b\u3092\u610f\u5473\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001Memory \u884c\u5217 $M\\_{t-1}$ \u306b\u3001\u884c\u5217 $\\boldsymbol{w_t^w}\\boldsymbol{\\nu}\\_t^T$ \u3092\u52a0\u7b97\u3059\u308b\u3053\u3068\u3067\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3044 Memory \u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u5404\u6210\u5206\u306b\n\n```math\n\\begin{align}\n& 0 \\leqq \\boldsymbol{w}_t^w[n] \\leqq 1 \\;\\; (n=1,2,...,N)\\\\\n& \\sum_{n=1}^N \\boldsymbol{w}_t^w[n] \\leqq 1 \n\\end{align}\n```\n\u3092\u8ab2\u3059\u70b9\u306f\u540c\u69d8\u3067\u3059\u3002\n\n### Memory \u306e\u8aad\u307f\u66f8\u304d\u624b\u9806\uff08\u8a73\u7d30\uff09\n\u8ad6\u6587\u4e2d\u3067\u306f\u3001Memory \u3078\u306e\u8aad\u307f\u66f8\u304d\u81ea\u4f53\u3082\u542b\u3081\u3066\u4ee5\u4e0b\u306e4\u3064\u306e\u624b\u9806\u3092\u8e0f\u3093\u3067\u3044\u307e\u3059\u3002\n\u2460write weighting \u306e\u66f4\u65b0\n\u2461Memory \u3078\u306e\u66f8\u304d\u8fbc\u307f\n\u2462read weighting \u306e\u66f4\u65b0\n\u2463Memory \u304b\u3089\u306e\u8aad\u307f\u8fbc\u307f \n\n\u4ee5\u4e0b\u3001\u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u305d\u3063\u3066\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u5b9f\u88c5\u4f8b\u3067\u3082\u3001\u3053\u306e\u9806\u3067\u51e6\u7406\u3092\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5c1a\u3001\u524d time-step \u3067\u306e read/write weighting $\\\\{\\boldsymbol{w}\\_{t-1}^{r,i}\\\\}\\_{i=1,...,R}$ / $\\boldsymbol{w}\\_{t-1}^{w}$\u306f\u5f97\u3089\u308c\u3066\u3044\u308b\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\n#### \u2460write weighting\u306e\u66f4\u65b0\n\u66f8\u304d\u8fbc\u307f\u5148\u306e address \u306e\u9078\u629e\u306b\u306f2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u3001write weighting \u306f2\u3064\u306e\u8981\u7d20\u304b\u3089\u69cb\u6210\u3055\u308c\u307e\u3059\u3002\u3064\u307e\u308a\u3001(1) interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u305f \"key\" \u3092\u3082\u3068\u306b\u66f8\u304d\u8fbc\u307f\u5148 slot \u3092\u9078\u629e\u3001(2)\u524d time-step \u307e\u3067\u306e\u8aad\u307f\u51fa\u3057\u72b6\u6cc1\u3092\u3082\u3068\u306b\u4f7f\u7528\u6e08\u307f\u306e\u60c5\u5831\u304c\u6b8b\u3063\u3066\u3044\u308b slot \u3092\u9078\u629e\u3001\u306e2\u3064\u3067\u3059\u3002\n\n\u307e\u305a\u306f\u3001(1)\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\ninterface vector $\\boldsymbol{\\xi}\\_t$ \u4e2d\u306e \"write key\" $\\boldsymbol{k}\\_t^w$ \u3092\u5404 Memory slot \u306b\u4fdd\u6301\u3055\u308c\u3066\u3044\u308b\u60c5\u5831\u3068\u7167\u5408\u3057\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u307e\u305f\u3001weighting \u306e peak \u306e\u92ed\u3055\u3092\u8abf\u6574\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066 \"write strength\" $\\beta_t^w$ \u3082\u4f7f\u7528\u3057\u307e\u3059\u3002\n\u3053\u306e\u8a08\u7b97\u306f\u3001read weighting \u3092\u6c42\u3081\u308b\u3068\u3053\u308d\u3067\u3082\u5171\u901a\u306a\u306e\u3067\u3001$N$\u00d7$W$ \u884c\u5217 $M$\u3001$W$ \u6b21\u30d9\u30af\u30c8\u30eb $\\boldsymbol{k}$\u3001\u30b9\u30ab\u30e9\u30fc\u5024 $\\beta$ \u306b\u5bfe\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u6f14\u7b97\u3092\u5b9a\u7fa9\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```math\n\\begin{align}\n\\mathcal{C}(M, \\boldsymbol{k}, \\beta)[n] = \\frac{\\text{exp}\\big(\\mathcal{D}(\\boldsymbol{k}, M[n,:])\\beta \\big)}{\\sum_m\\text{exp}\\big(\\mathcal{D}(\\boldsymbol{k}, M[m,:])\\beta \\big)}\n\\end{align}\n``` \n\u3053\u3053\u3067\u3001$\\mathcal{D}$ \u306f2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u8ddd\u96e2\u3067\u3001\u3068\u308a\u304b\u305f\u306f\u3044\u308d\u3044\u308d\u8003\u3048\u3089\u308c\u307e\u3059\u304c\u3001\u8ad6\u6587\u306b\u5408\u308f\u305b\u3066\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\n\n```math\n\\begin{align}\n\\mathcal{D}(\\boldsymbol{u}, \\boldsymbol{v}) = \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{v}}{||\\boldsymbol{u}|| \\; ||\\boldsymbol{v}||}\n\\end{align}\n```\n\u3068\u3057\u3066\u304a\u304d\u307e\u3059\u3002$\\beta \\rightarrow \\infty$ \u306e\u6975\u9650\u3067 $\\mathcal{C}$ \u306f\u92ed\u3044\u30d4\u30fc\u30af\u304c\u3072\u3068\u3064\u3060\u3051\u7acb\u3061\u307e\u3059\u3002\n\u3055\u3066\u3001\u3053\u3053\u3067\u5b9a\u7fa9\u3057\u305f\u6f14\u7b97\u3092\u7528\u3044\u3066\u3001(1)\u306e\u65b9\u6cd5\u306b\u3088\u308b write weighting \u306f\n\n```math\n\\begin{align}\n\\boldsymbol{c}_t^w=\\mathcal{C}(M_{t-1}, \\boldsymbol{k}_t^w, \\beta_t^w)\n\\end{align}\n```\n\u3068\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002\n\n\u6b21\u306b(2)\u306e\u65b9\u6cd5\u306b\u3088\u308b\u8a08\u7b97\u3067\u3059\u3002\u591a\u5c11\u3084\u3084\u3053\u3057\u3044\u3067\u3059\u304c\u3001\u4ee5\u4e0b\u306e\u9806\u3067\u6c42\u3081\u3066\u3044\u304d\u307e\u3059\u3002\n\n(2)-1. \"retention vector\" $\\boldsymbol{\\psi}\\_t$ \u306e\u69cb\u6210\n\u524d time-step $t-1$ \u3067\u60c5\u5831\u3092\u8aad\u307f\u51fa\u3057\u305f Memory slot \u306f\u3001\u4f7f\u7528\u6e08\u307f slot \u3068\u3057\u3066\u66f8\u304d\u8fbc\u307f\u306b\u5229\u7528\u3057\u305f\u3044\u3068\u601d\u3046\u306e\u306f\u81ea\u7136\u3067\u3057\u3087\u3046\u3002\u3057\u304b\u3057\u3001\u4eca\u5f8c\u3082\u5fc5\u8981\u306a\u60c5\u5831\u304c\u4fdd\u6301\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u4e0a\u66f8\u304d\u3057\u3066\u306f\u3044\u3051\u307e\u305b\u3093\u3002\u3053\u306e\u3088\u3046\u306b\u3001\u524d time-step \u3067\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3063\u305f Memory slot \u3092\u672c\u5f53\u306b\u958b\u653e\u3057\u3066\u826f\u3044\u304b\u3069\u3046\u304b\u306e\u30d5\u30e9\u30b0\uff08\u5b9f\u969b\u306b\u306f0~1\u306e\u9023\u7d9a\u5024\u306a\u306e\u3067\u91cd\u307f\uff09\u304c \"free gate\" $f\\_t^i\\;(i=1,...,R)$ \u3067\u3059\u3002\u3059\u306a\u308f\u3061\u3001$f\\_t^i \\boldsymbol{w}\\_{t-1}^{r,i}$ \u3092\u6210\u5206\u5358\u4f4d\u3067\u8003\u3048\u3066\u3001\u300c$f\\_t^i w\\_{t-1}^{r,i}$ $\\simeq 1$ $\\Leftrightarrow f\\_t^i \\simeq 1$ and $w\\_{t-1}^{r,i}\\simeq 1$\u300d \u306a\u3089\u3070\u8aad\u307f\u8fbc\u307f\u6e08\u307f\u304b\u3064\u958b\u653eOK\u306a\u306e\u3067\u4e0a\u66f8\u304d\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u300c$f\\_t^i w\\_{t-1}^{r,i}$ $\\simeq 0$ $\\Leftrightarrow f\\_t^i \\simeq 0$ or $w\\_{t-1}^{r,i}\\simeq 0$\u300d \u306a\u3089\u3070 \u524d time-step \u3067\u8aad\u307f\u8fbc\u307e\u308c\u3066\u3044\u306a\u3044\u3001\u307e\u305f\u306f\u3001\u8aad\u307f\u8fbc\u307f\u6e08\u307f\u3060\u3063\u305f\u3068\u3057\u3066\u3082\u89e3\u653e\u30d5\u30e9\u30b0\u304c\u7acb\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u4e0a\u66f8\u304d\u4e0d\u53ef\u3068\u306a\u308a\u307e\u3059\u3002\u4eca\u3001\u5404\u56de\u306e\u8aad\u307f\u8fbc\u307f\u3054\u3068\u306b\u8003\u3048\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u5168 $R$ \u56de\u3092\u3072\u3063\u304f\u308b\u3081\u3066\u306e\u4f7f\u7528\u4e2d\uff08\u4e0a\u66f8\u304d\u4e0d\u53ef\uff09\u30d5\u30e9\u30b0\u3067\u3042\u308b \"retention vector\" $\\boldsymbol{\\psi}\\_t$\u3092\n\n```math\n\\begin{align}\n\\boldsymbol{\\psi}_t = \\prod_{i=1}^R\\big(1-f_t^i\\boldsymbol{w}_{t-1}^{r, i}\\big)\n\\end{align}\n```\n\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u7a4d\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u3068\u308a\u307e\u3059\u30021\u304b\u3089\u306e\u5dee\u3092\u6c42\u3081\u305f\u5f8c\u3067\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001$R$ \u56de\u306e\u3046\u30611\u5ea6\u3067\u3082\u4e0a\u66f8\u304d\u53ef\u3068\u5224\u65ad\u3055\u308c\u305f memory slot \u306f\u4e0a\u66f8\u304d\u53ef\u3068\u5224\u65ad\u3055\u308c\u307e\u3059\u3002\u4e00\u65b9\u3067\u3001\u4f7f\u7528\u4e2d\uff08\u4e0a\u66f8\u304d\u4e0d\u53ef\uff09 $\\psi\\_t \\simeq 1$ \u3068\u5224\u65ad\u3055\u308c\u308b\u306e\u306f\u3001$R$ \u56de\u5168\u3066\u306b\u3064\u3044\u3066\u4e0a\u66f8\u304d\u4e0d\u53ef\u3068\u5224\u65ad\u3055\u308c\u305f\u3068\u304d\u306b\u9650\u3089\u308c\u307e\u3059\u3002\n\n(2)-2. \"usage vector\" $\\boldsymbol{u}\\_t$ \u306e\u69cb\u6210\n(2)-1\u3067\u306f\u3001\u4f7f\u7528\u4e2d\u30d5\u30e9\u30b0\uff08\u6b63\u3057\u304f\u306f\u91cd\u307f\uff09\u3068\u3057\u3066\u306e $\\boldsymbol{\\psi}\\_t$ \u3092\u8003\u3048\u307e\u3057\u305f\u304c\u3001\u524d time-step \u3067\u306e\u8aad\u307f\u8fbc\u307f\u306b\u3064\u3044\u3066\u3057\u304b\u8003\u3048\u3066\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u5b9f\u969b\u306b\u306f\u3001\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u5834\u5408\u306b\u306f\u4f7f\u7528\u4e2d\u306e\u5ea6\u5408\u3044\u304c\u9ad8\u307e\u308b\u306f\u305a\u3067\u3059\u3057\u3001\u76f4\u524d time-step \u3088\u308a\u524d\u306e time-step \u3067\u306e\u72b6\u6cc1\u3082\u8003\u3048\u308b\u3079\u304d\u3067\u3059\u3002\u3053\u308c\u3089\u3092\u8e0f\u307e\u3048\u3066\u3001\u5404 Memory slot \u306e\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044\u3092\u8868\u3059\u91cd\u307f \"memory usage vector\" $\\boldsymbol{u}_t$ \u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n```math\n\\begin{align}\n\\boldsymbol{u}_t &= \\big(\\boldsymbol{u}_{t-1} + \\boldsymbol{w}_{t-1}^w - \\boldsymbol{u}_{t-1} \\circ \\boldsymbol{w}_{t-1}^w \\big) \\circ \\boldsymbol{\\psi}_t \\\\\n\\boldsymbol{u}_0 &= 0, \\;\\; \\boldsymbol{w}_0^w = 0\n\\end{align}\n```\n$\\circ$\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u8868\u3057\u307e\u3059\u3002(...)\u5185\u7b2c3\u9805\u76ee\u306f $\\boldsymbol{u}\\_t$ \u306e\u5404\u6210\u5206\u304c $1$ \u3092\u8d85\u3048\u306a\u3044\u3088\u3046\u306b\u8abf\u6574\u3059\u308b\u305f\u3081\u306e\u88dc\u6b63\u3067\u3059\u3002$u\\_t = 1$ \u306b\u9054\u3059\u308b\u3068\u66f8\u304d\u8fbc\u307f\u306b\u3088\u308b\u91cd\u307f\u306e\u66f4\u65b0\u306f\u505c\u6b62\u3057\u307e\u3059\u3057\u3001\u3082\u3057 $u\\_t > 1$ \u3068\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3082\u66f4\u65b0\u306b\u3088\u308a\u5024\u3092\u6e1b\u5c11\u3055\u305b\u307e\u3059\u3002\n\n(2)-3. \"allocation weighting\" \u306e\u69cb\u6210\n\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044 $\\boldsymbol{u}\\_t$ \u306b\u57fa\u3065\u3044\u3066\u3001\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046 Memory slot \u306e address \u3092\u6c7a\u3081\u307e\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u3001\u4f7f\u7528\u4e2d\u5ea6\u5408\u3044\u304c\u4f4e\u3044\u3068\u3053\u308d\u306b\u66f8\u304d\u8fbc\u3080\u8a33\u3067\u3059\u304c\u3001\u50be\u659c\u3092\u3064\u3051\u307e\u3059\u3002\u307e\u305a\u3001$\\boldsymbol{u}\\_t$ \u306e\u6210\u5206\u3092\u5024\u304c\u5c0f\u3055\u3044\u9806\u306b\u4e26\u3079\u305f\u6642\u306e index \u304b\u3089\u306a\u308b\u30d9\u30af\u30c8\u30eb\u3092  $\\boldsymbol{\\phi}_t$ \u3068\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u3001\n\n```math\n\\begin{align}\n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[1]] \\leqq \\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[2]] \\leqq \n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[3]] \\leqq \\cdots \\leqq \n\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[N]]\n\\end{align}\n```\n\u3067\u3059\u3002\u3053\u308c\u3092\u7528\u3044\u3066\u3001\"allocation weighting\" $\\boldsymbol{a}\\_t$ \u3092\n\n```math\n\\begin{align}\n\\boldsymbol{a}_t[\\boldsymbol{\\phi}_t[n]] = \\big(1-\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[n]]\\big) \\prod_{m=1}^{n-1}\\boldsymbol{u}_t[\\boldsymbol{\\phi}_t[m]]\n\\end{align}\n```\n\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u7a4d\u306f\u6210\u5206\u3054\u3068\u306e\u7a4d\u3092\u3068\u308a\u307e\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u3001$1-\\boldsymbol{u}\\_t$ \u306a\u306e\u3067\u4f7f\u7528\u6e08\u307f\uff08\u66f8\u304d\u8fbc\u307f\u53ef\uff09\u306e\u5ea6\u5408\u3044\u3092\u8868\u3059\u91cd\u307f\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001(1)\u3068(2)\u306e\u7d50\u679c\u3092\u7d71\u5408\u3057\u3001write weighting $\\boldsymbol{w}\\_t^w$ \u3092\n\n```math\n\\begin{align}\n\\boldsymbol{w}_t^w = g_t^w\\big(g_t^a\\boldsymbol{a}_t+(1-g_t^a)\\boldsymbol{c}_t^w\\big)\n\\end{align}\n```\n\u3068\u3057\u3066\u66f4\u65b0\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u3001$g_t^a$ \u3068 $g_t^w$ \u306f interface vector  $\\boldsymbol{\\xi}_t$ \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u305f\u3082\u306e\u3067\u3059\u3002\u305d\u308c\u305e\u308c\u3001\u300c\u4f7f\u7528\u6e08\u307f\u30d5\u30e9\u30b0 or \"key\"\u306e\u3069\u3061\u3089\u306b\u57fa\u3065\u3044\u3066\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046\u304b\u300d\u3001\u300c\u305d\u3082\u305d\u3082\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3046\u304b\u300d\u3092\u5236\u5fa1\u3059\u308b gate \u3068\u3057\u3066\u50cd\u304d\u307e\u3059\u3002\n\n#### \u2461Memory \u3078\u306e\u66f8\u304d\u8fbc\u307f\nwrite weighting \u306e\u66f4\u65b0\u304c\u5b8c\u4e86\u3057\u307e\u3057\u305f\u306e\u3067\u3001Memory \u306b\u66f8\u304d\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\u305f\u3060\u3057\u3001\u66f8\u304d\u8fbc\u307f\u3068\u540c\u6642\u306b\u53e4\u3044\u60c5\u5831\u306e\u6d88\u53bb\u3082\u884c\u3044\u307e\u3059\u3002\u3069\u306e Memory slot \u3092\u5bfe\u8c61\u3068\u3059\u308b\u304b\u306e\u91cd\u307f\u3065\u3051\u306f\u3001\"write weighting\" $\\boldsymbol{w}\\_t^w$ \u3067\u884c\u3044\u307e\u3059\u3002\u66f8\u304d\u8fbc\u307e\u308c\u308b\u30c7\u30fc\u30bf\u306f \"write vector\" $\\boldsymbol{\\nu}\\_t$\u3001slot \u5185\u306e\u30c7\u30fc\u30bf\u3092\u3069\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\u3067\u6d88\u53bb\u3059\u308b\u304b\u306f\u3001erase vector $\\boldsymbol{e}\\_t$ \u3067\u4e0e\u3048\u3089\u308c\u3001\u3053\u306e2\u3064\u306f interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nMemory \u884c\u5217 $M_{t-1}$ \u306e\u6d88\u53bb\u30fb\u66f8\u304d\u8fbc\u307f\u306b\u3088\u308b\u66f4\u65b0\u306f\u4ee5\u4e0b\u306e\u5f0f\u306b\u5f93\u3063\u3066\u884c\u308f\u308c\u307e\u3059\u3002\n\n```math\n\\begin{align}\n& M_t[n,s] = M_{t-1}[n,s] \\big(1-\\boldsymbol{w}_t^w[n] \\boldsymbol{e}_t[s] \\big) + \\boldsymbol{w}_t^w[n] \\boldsymbol{\\nu}_t[s] \\\\\n& \\Leftrightarrow M_t = M_{t-1} \\circ \\big(1-\\boldsymbol{w}_t^w \\boldsymbol{e}_t^T \\big) + \\boldsymbol{w}_t^w \\boldsymbol{\\nu}_t^T \n\\end{align}\n``` \n\n#### \u2462read weighting\u306e\u66f4\u65b0\n\u8aad\u307f\u8fbc\u307f\u5148\u306e address \u306e\u9078\u629e\u306b\u30822\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\n(1) interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u305f \"key\" \u3092\u3082\u3068\u306b\u8aad\u307f\u8fbc\u307f\u5148 slot \u3092\u9078\u629e\u3001(2)\u66f8\u304d\u8fbc\u307f\u9806\u306b\u5f93\u3063\u3066 slot \u3092\u9078\u629e\u3001\u306e2\u3064\u3067\u3059\u3002\n\n(1)\u306b\u3064\u3044\u3066\u306f\u3001write weighting \u306e\u6642\u3068\u540c\u3058\u3088\u3046\u306b \"read key\" $\\\\{\\boldsymbol{k}\\_t^{r,i}\\\\}\\_{i=1,2,..,R}$ \u3068 \"read strength\" $\\\\{\\beta\\_t^{r,i}\\\\}\\_{i=1,2,..,R}$ \u3092\u7528\u3044\u3066\u3001\n\n```math\n\\begin{align}\n\\boldsymbol{c}_t^{r, i} = \\mathcal{C}\\big(M_t, \\boldsymbol{k}_t^{r,i}, \\beta_t^{r,i}\\big) \\;\\; (i=1,2,...,R)\n\\end{align}\n```\n\u3068\u8a08\u7b97\u3057\u307e\u3059\u3002\n\n(2)\u306f\u3084\u3084\u9577\u3044\u3067\u3059\u304c\u3001\u9806\u306b\u8aac\u660e\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\u8aac\u660e\u306e\u4fbf\u5b9c\u4e0a(2)-2 \u2192(2)-1\u306e\u9806\u3067\u89e3\u8aac\u3057\u307e\u3059\u304c\u3001\u5b9f\u88c5\u3059\u308b\u969b\u306f(2)-1 \u2192(2)-2\u306e\u6d41\u308c\u3067\u3059\u3002\n\n(2)-2. \"precedence weighting\" \u306e\u69cb\u6210\nprecedence weighting $\\boldsymbol{p}_t$ \u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n```math\n\\begin{align}\n& \\boldsymbol{p}_0 = 0 \\\\\n& \\boldsymbol{p}_t = \\Big(1-\\sum_{n=1}^N \\boldsymbol{w}_t^w[n]\\Big)\\boldsymbol{p}_{t-1} + \\boldsymbol{w}_t^w\n\\end{align}\n```\n$\\boldsymbol{w}\\_t^w$ \u306f\u3059\u3067\u306b\u2460\u3067\u66f4\u65b0\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u2461\u3067\"\u5f37\u304f\"\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u5834\u5408\u306f\u3001$\\sum \\boldsymbol{w}\\_t^w \\simeq 1$ \u3068\u306a\u308b\u306e\u3067\u3001\u524d time-step \u306e\u60c5\u5831 $\\boldsymbol{p}\\_{t-1}$ \u306f\u6d88\u53bb\u3055\u308c\u3001\u73fe time-step \u3067\u3069\u3053\u306b\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u305f\u304b\u304c\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\u66f8\u304d\u8fbc\u307f\u304c\u884c\u308f\u308c\u306a\u304b\u3063\u305f\u5834\u5408\u306f\u3001\u6700\u5f8c\u306b\u884c\u308f\u308c\u305f\u66f8\u304d\u8fbc\u307f\u306e\u91cd\u307f\u304c\u4fdd\u6301\u3055\u308c\u7d9a\u3051\u307e\u3059\u3002\u307e\u305f\u3001$\\boldsymbol{p}\\_t[n] \\leq 1$\u3001$\\sum\\_n \\boldsymbol{p}_t[n] \\leq 1$ \u3067\u3042\u308b\u3053\u3068\u306f\u3059\u3050\u306b\u308f\u304b\u308a\u307e\u3059\u3002\n\n(2)-1. \"temporal link matrix\" \u306e\u69cb\u6210\nMemory slot \u3078\u306e\u66f8\u304d\u8fbc\u307f\u9806\u3092\u8868\u3059 $N$ \u00d7 $N$ \u884c\u5217 $L_t$ \u3092\u69cb\u6210\u3057\u307e\u3059\u3002\u6210\u5206\u5358\u4f4d\u3067\u307f\u305f\u3068\u304d\u306b $L_t[n, m] \\simeq 1$ \u3067\u3042\u308b\u3053\u3068\u306f \u300cMemory $M_t$ \u306b\u304a\u3044\u3066 slot 'n' \u306b\u3042\u308b\u60c5\u5831\u306f slot 'm' \u306b\u3042\u308b\u60c5\u5831 \u306e\u6b21\u306b\u66f8\u304d\u8fbc\u307e\u308c\u305f\u3082\u306e\u3067\u3042\u308b\u300d\u3068\u3044\u3046\u95a2\u4fc2\u304c\u8868\u73fe\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u66f4\u65b0\u5f0f\u3092\u4ee5\u4e0b\u3067\u4e0e\u3048\u307e\u3059\u3002\n\n```math\n\\begin{align}\n& L_0[n,m]=0 \\\\\n& L_t[n,n]=0 \\\\\n& L_t[n,m]=\\big(1-\\boldsymbol{w}_t^w[n]-\\boldsymbol{w}_t^w[m]\\big)L_{t-1}[n,m] + \\boldsymbol{w}_t^w[n]\\boldsymbol{p}_{t-1}[m]\n\\end{align}\n```\n\u5bfe\u89d2\u6210\u5206\u306f\u610f\u5473\u3092\u306a\u3055\u306a\u3044\u305f\u3081 $0$ \u3067\u56fa\u5b9a\u3057\u307e\u3059\u3002$0 \\leqq L\\_t[n,m] \\leqq 1$ \u304a\u3088\u3073\u3001$\\sum_m L\\_t[n,m] \\leqq 1$ \u306f\u3059\u3050\u306b\u78ba\u304b\u3081\u3089\u308c\u307e\u3059\u3002$\\sum_n L\\_t[n,m] \\leqq 1$ \u306f\u78ba\u304b\u3081\u3088\u3046\u3068\u3057\u307e\u3057\u305f\u304c\u3001\u793a\u305b\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u793a\u305b\u305f\u65b9\u304c\u304a\u3089\u308c\u307e\u3057\u305f\u3089\u3001\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\n(2)-3. \"forward/backward weighting\" \u306e\u69cb\u6210\nMemory \u3078\u306e\u66f8\u304d\u8fbc\u307f\u9806\u3092\u8003\u616e\u3057\u305f forward/backward weighting $\\\\{\\boldsymbol{f}\\_t^i\\\\}\\_{i=1,..,R}$ / $\\\\{\\boldsymbol{b}\\_t^i\\\\}\\_{i=1,..,R}$\u3092\u3084\u3084\u8352\u3063\u307d\u3044\u3067\u3059\u304c\n\n```math\n\\begin{align}\n&\\boldsymbol{f}_t^i[n]=\\sum_{m=1}^NL_t[n,m]\\boldsymbol{w}_{t-1}^{r,i}[m] \\; \\Leftrightarrow \\; \\boldsymbol{f}_t^i=L_t\\boldsymbol{w}_{t-1}^{r,i} \\\\\n&\\boldsymbol{b}_t^i[m]=\\sum_{n=1}^N\\boldsymbol{w}_{t-1}^{r,i}[n]L_t[n,m] \\; \\Leftrightarrow \\; \\boldsymbol{b}_t^i=L_t^T\\boldsymbol{w}_{t-1}^{r,i} \\\\\n\\end{align}\n```\n\u3067\u69cb\u6210\u3057\u307e\u3059\u3002$\\sum_n L\\_t[n,m] \\leqq 1$ \u304c\u6210\u308a\u7acb\u3064\u306a\u3089\u3070\u3001$0\\leqq \\boldsymbol{f}\\_t^i[n] \\leqq 1$ \u304a\u3088\u3073 $\\sum_n\\boldsymbol{f}\\_t^i[n]\\leqq1$ \u304c\u6210\u7acb\u3057\u307e\u3059\u3002$\\boldsymbol{b}\\_t^i$ \u306b\u3064\u3044\u3066\u3082\u540c\u69d8\u3067\u3059\u3002\n\n\u4ee5\u4e0a\u3001(1)~(2)\u3092\u7d71\u5408\u3057\u3066\u3001read weighting $\\\\{\\boldsymbol{w}\\_t^{i,r}\\\\}\\_{i=1,...,R}$ \u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\n\n```math\n\\begin{align}\n\\boldsymbol{w}_t^{r,i}=\\boldsymbol{\\pi}_t^i[1]\\boldsymbol{b}_t^i + \\boldsymbol{\\pi}_t^i[2]\\boldsymbol{c}_t^i + \\boldsymbol{\\pi}_t^i[3]\\boldsymbol{f}_t^i \\;\\; (i=1,2,...,R)\n\\end{align}\n```\n$\\\\{\\pi_t^i\\\\}\\_{i=1,...,R}$ \u306f interface vector \u3092\u901a\u3057\u3066\u5165\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n#### \u2463Memory\u304b\u3089\u306e\u8aad\u307f\u8fbc\u307f \nMemory \u304b\u3089\u306e\u8aad\u307f\u51fa\u3057\u306f\u7c21\u5358\u3067\u3059\u3002\u8aad\u307f\u51fa\u3057\u7d50\u679c\u306e \"read vector\" $\\\\{\\boldsymbol{r}\\_t^i\\\\}\\_{i=1,..,R}$ \u306f\n\n```math\n\\begin{align}\n\\boldsymbol{r}_t^i[s] = \\sum_{n=1}^N \\boldsymbol{w}_t^{r,i}[n]M_t[n,s] \\; \\Leftrightarrow \\; \\boldsymbol{r}_t^i = M_t^T\\boldsymbol{w}_t^{r,i} \\;\\; (i=1,2,...,R)\n\\end{align}\n```\n\u3068\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3092\u3082\u3063\u3066\u3001time-step $t$ \u306e Memory \u8aad\u307f\u66f8\u304d\u304c\u5b8c\u4e86\u3057\u307e\u3059\u3002\n\n# Pyhthon - Chainer \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\n\u4e0a\u3067\u8aac\u660e\u3057\u305f\u30ed\u30b8\u30c3\u30af\u306e python \u306b\u3088\u308b\u5b9f\u88c5\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b3\u30fc\u30c9\u306f\u3001\n[DNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5](http://qiita.com/yos1up/items/599ff75c876f6f94d249)\n\u306b\u3042\u308b\u3082\u306e\u3067\u3059\u3002\u4e0a\u306e\u89e3\u8aac\u3068\u6bd4\u8f03\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u51e6\u7406\u3092\u95a2\u6570\u306b\u307e\u3068\u3081\u305f\u308a\u3001\u5909\u6570\u540d\u3092\u5909\u3048\u305f\u308a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u5185\u5bb9\u306f\u307b\u307c\u540c\u3058\u3082\u306e\u3067\u3059\u3002\n\n\u3072\u3068\u3064\u6ce8\u610f\u3057\u3066\u304a\u304d\u305f\u3044\u306e\u3067\u3059\u304c\u3001\u5168\u4f53\u50cf\u3067\u793a\u3057\u305f Controller \u304b\u3089\u306e\u51fa\u529b $\\boldsymbol{h}\\_t$ \u306f\u3001\u8ad6\u6587\u3067\u306f Controller \u306e\u6700\u7d42\u7684\u306a\u51fa\u529b\u3060\u3051\u3067\u306a\u304f\u3001hidden layer \u306e\u51fa\u529b\u3082\u3059\u3079\u3066\u7d71\u5408\u3057\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u307e\u3059\u3002\u3057\u304b\u3057\u7c21\u5358\u306e\u305f\u3081\u3001\u3053\u3053\u3067\u4f7f\u3046 Controller \u306f 2\u5c64\u7a0b\u5ea6\u306e\u7c21\u5358\u306a\u3082\u306e\u306b\u9650\u308a\u3001hidden layer \u306e\u51fa\u529b\u306f Controller \u306e\u5916\u306b\u306f\u53d6\u308a\u51fa\u3057\u307e\u305b\u3093\u3002\n\n## DNC \u306e\u5b9f\u88c5\n\n```Python3:dnc.py\nimport numpy as np\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import optimizers, Chain, Link, Variable\n\n\n# controller of DNC\nclass SimpleLSTM(Chain): \n    \n    def __init__(self, d_in, d_hidden, d_out):\n        super(SimpleLSTM, self).__init__(\n            l1 = L.LSTM(d_in, d_hidden),\n            l2 = L.Linear(d_hidden, d_out),\n            )\n        \n    def __call__(self, x):\n        return self.l2(self.l1(x))\n    \n    def reset_state(self):\n        self.l1.reset_state()\n        \n        \nclass DNC(Chain):\n    \n    def __init__(self, X, Y, N, W, R):\n        self.X = X # input dimension\n        self.Y = Y # output dimension\n        self.N = N # number of memory slot\n        self.W = W # dimension of one memory slot\n        self.R = R # number of read heads\n        self.d_ctr_in = W*R+X # input dimension into the controller\n        self.d_ctr_out = Y+W*R+3*W+5*R+3 # output dimension from the controller\n        self.d_ctr_hidden = self.d_ctr_out # dimension of hidden unit of the controller\n        self.d_interface = W*R+3*W+5*R+3 # dimension of interface vector\n        \n        self.controller = SimpleLSTM(self.d_ctr_in, self.d_ctr_hidden, self.d_ctr_out)\n\n        super(DNC, self).__init__(\n            l_ctr = self.controller, \n            l_Wy = L.Linear(self.d_ctr_out, self.Y),\n            l_Wxi = L.Linear(self.d_ctr_out, self.d_interface), \n            l_Wr = L.Linear(self.R * self.W, self.Y), \n            )\n        \n        self.reset_state()\n            \n    def reset_state(self):\n        # initialize all the recurrent state\n        self.l_ctr.reset_state() # controller\n        self.u = Variable(np.zeros((self.N, 1)).astype(np.float32)) # usage vector (N, 1)\n        self.p = Variable(np.zeros((self.N, 1)).astype(np.float32)) # precedence weighting (N, 1)\n        self.L = Variable(np.zeros((self.N, self.N)).astype(np.float32)) # temporal memory linkage (N, N)                     \n        self.Mem = Variable(np.zeros((self.N, self.W)).astype(np.float32)) # memory (N, W)\n        self.r = Variable(np.zeros((1, self.R*self.W)).astype(np.float32)) # read vector (1, R * W)\n        self.wr = Variable(np.zeros((self.N, self.R)).astype(np.float32)) # read weighting (N, R)\n        self.ww = Variable(np.zeros((self.N, 1)).astype(np.float32)) # write weighting (N, 1)\n\n        \n    # utility functions\n    \n    def _cosine_similarity(self, u, v):\n        # cosine similarity as a distance of two vectors\n        # u, v: (1, -) Variable  -> (1, 1) Variable\n        denominator = F.sqrt(F.batch_l2_norm_squared(u) * F.batch_l2_norm_squared(v))\n        if (np.array_equal(denominator.data, np.array([0]))):\n            return F.matmul(u, F.transpose(v))\n        return F.matmul(u, F.transpose(v)) / F.reshape(denominator, (1, 1))\n\n    \n    def _C(self, Mem, k, beta):\n        # similarity between rows of matrix Mem and vector k\n        # Mem:(N, W) Variable, k:(1, W) Variable, beta:(1, 1) Variable -> (N, 1) Variable\n        N, W = Mem.shape \n        ret_list = [0] * N\n        for i in range(N):\n            # calculate distance between i-th row of Mem and k\n            ret_list[i] = self._cosine_similarity(F.reshape(Mem[i,:], (1, W)), k) * beta \n        # concat horizontally because softmax operates along the direction of axis=1 \n        return F.transpose(F.softmax(F.concat(ret_list, 1))) \n\n    \n    def _u2a(self, u):\n        # convert usage vector u to allocation weighting a\n        # u, a: (N, 1) Variable\n        N = u.shape[0]\n        phi = np.argsort(u.data.flatten()) # u.data[phi]: ascending\n        a_list = [0] * N    \n        cumprod = Variable(np.array([[1.0]]).astype(np.float32)) \n        for i in range(N): \n            a_list[phi[i]] = cumprod * (1.0 - F.reshape(u[phi[i]], (1, 1)))\n            cumprod *= F.reshape(u[phi[i]], (1, 1))\n        return F.concat(a_list, 0) \n    \n    \n    # operations of the DNC system\n    \n    def _controller_io(self, x):\n        # input data from the Data Set : x (1, X) Variable  \n        # out-put from the controller h is split into two ways : v (1, Y), xi(1, W*R+3*W+5*R+3) Variable\n        chi = F.concat([x, self.r], 1) # total input to the controller \n        h = self.l_ctr(chi) # total out-put from the controller\n        self.v = self.l_Wy(h)\n        self.xi = self.l_Wxi(h)\n        \n        # interface vector xi is split into several components\n        (self.kr, self.beta_r, self.kw, self.beta_w,\n         self.e, self.nu, self.f, self.ga, self.gw, self.pi\n         ) = F.split_axis(self.xi, np.cumsum(\n             [self.W*self.R, self.R, self.W, 1, self.W, self.W, self.R, 1, 1]), 1) # details of the interface vector\n        \n        # rescale components\n        self.kr = F.reshape(self.kr, (self.R, self.W)) # read key (R, W)\n        self.beta_r = 1 + F.softplus(self.beta_r) # read strength (1, R)\n        # self.kw : write key (1, W)\n        self.beta_w = 1 + F.softplus(self.beta_w) # write strength (1, 1)\n        self.e = F.sigmoid(self.e) # erase vector (1, W)\n        # self.nu : write vector (1, W)\n        self.f = F.sigmoid(self.f) #  free gate (1, R)\n        self.ga = F.sigmoid(self.ga) # allcation gate (1, 1)\n        self.gw = F.sigmoid(self.gw) # write gate (1, 1)\n        self.pi = F.softmax(F.reshape(self.pi, (self.R, 3))) # read mode (R, 3)\n        \n        \n    def _up_date_write_weighting(self):\n        # calculate retention vector : psi (N, 1) \n        # here, read weighting : wr (N, R) must retain state one step former\n        psi_mat = 1 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), self.f) * self.wr # (N, R)\n        self.psi = Variable(np.ones((self.N, 1)).astype(np.float32)) \n        for i in range(self.R):\n            self.psi = self.psi * F.reshape(psi_mat[:,i],(self.N,1)) # (N, 1)\n        # up date usage vector : u (N, 1)\n        # here, write weighting : ww (N, 1) must retain state one step former\n        self.u = (self.u + self.ww - (self.u * self.ww)) * self.psi \n        # calculate allocation weighting : a (N, 1)\n        self.a = self._u2a(self.u) \n        # calculate write content weighting : cw (N, 1)\n        self.cw = self._C(self.Mem, self.kw, self.beta_w) \n        # up date write weighting : ww (N, 1)\n        self.ww = F.matmul(F.matmul(self.a, self.ga) + F.matmul(self.cw, 1.0 - self.ga), self.gw) \n\n        \n    def _write_to_memory(self):\n        # erase vector : e (1, W) deletes information on the Memory : Mem (N, W) \n        # and write vector : nu (1, W) is written there\n        # write weighting : ww (N, 1) must be up-dated before this step\n        self.Mem = self.Mem * (np.ones((self.N, self.W)).astype(np.float32) - F.matmul(self.ww, self.e)) + F.matmul(self.ww, self.nu)\n\n        \n    def _up_date_read_weighting(self):    \n        # up date temporal memory linkage : L (N, N)\n        ww_mat = F.matmul(self.ww, Variable(np.ones((1, self.N)).astype(np.float32))) # (N, N)\n        # here, precedence wighting : p (N, 1) must retain state one step former\n        self.L = (1.0 - ww_mat - F.transpose(ww_mat)) * self.L + F.matmul(self.ww, F.transpose(self.p)) # (N, N)\n        self.L = self.L * (np.ones((self.N, self.N)) - np.eye(self.N)) # constrain L[i,i] == 0   \n        # up date prcedence weighting : p (N, 1)\n        self.p = (1.0 - F.matmul(Variable(np.ones((self.N, 1)).astype(np.float32)), F.reshape(F.sum(self.ww),(1, 1)))) * self.p + self.ww \n        # calculate forward weighting : fw (N, R)\n        # here, read wighting : wr (N, R) must retain state one step former\n        self.fw = F.matmul(self.L, self.wr) \n        # calculate backward weighting : bw (N, R)\n        self.bw = F.matmul(F.transpose(self.L), self.wr)\n        # calculate read content weighting : cr (N, R)\n        self.cr_list = [0] * self.R\n        for i in range(self.R):\n            self.cr_list[i] = self._C(self.Mem, F.reshape(self.kr[i,:], (1, self.W)), F.reshape(self.beta_r[0,i],(1, 1))) # (N, 1)\n        self.cr = F.concat(self.cr_list, 1) # (1, N * R)       \n        # compose up-dated read weighting : wr (N, R)\n        bcf_tensor = F.concat([\n                            F.reshape(F.transpose(self.bw), (self.R, self.N, 1)),\n                            F.reshape(F.transpose(self.cr), (self.R, self.N, 1)),\n                            F.reshape(F.transpose(self.fw), (self.R, self.N, 1))\n                            ], 2) # (R, N, 3)\n        self.pi = F.reshape(self.pi, (self.R, 3, 1)) # (R, 3, 1)\n        self.wr = F.transpose(F.reshape(F.batch_matmul(bcf_tensor, self.pi), (self.R, self.N))) # (N, R)\n    \n    \n    def _read_from_memory(self): \n        # read information from the memory : Mem (N, W) and compose read vector : r (W, R) to reshape (1, W * R)\n        # read weighting : wr (N, R) must be up-dated before this step\n        self.r = F.reshape(F.matmul(F.transpose(self.Mem), self.wr), (1, self.R * self.W))\n\n        \n    def __call__(self, x):\n        self._controller_io(x) # input data is processed through the controller\n        self._up_date_write_weighting() \n        self._write_to_memory() # memory up-date\n        self._up_date_read_weighting()  \n        self._read_from_memory() # extract information from the memory\n        self.y = self.l_Wr(self.r) + self.v # compose total out put y : (1, Y)\n        return self.y\n```\n\n## \u4f7f\u7528\u4f8b\n\u4f7f\u7528\u4f8b\u3082\u8f09\u305b\u307e\u3059\u304c\u3001[DNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5](http://qiita.com/yos1up/items/599ff75c876f6f94d249)\u306b\u3042\u308b\u3082\u306e\u3068\u5185\u5bb9\u306f\u540c\u3058\u3067\u3059\u3002\n\u56fa\u5b9a\u9577\u306e one-hot \u30d9\u30af\u30c8\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306a\u500b\u6570\u5165\u529b\u3057\u3001\u5165\u529b\u5b8c\u4e86\u5f8c\u306b\u3001\u51fa\u529b\u3068\u3057\u3066\u5165\u529b\u30c7\u30fc\u30bf\u3092 echo \u3055\u305b\u307e\u3059\u3002echo \u3055\u305b\u305f\u30c7\u30fc\u30bf\u3068\u5165\u529b\u30c7\u30fc\u30bf\u306e2\u4e57\u8aa4\u5dee\u3092 loss \u3068\u3057\u3066\u30011\u30bb\u30c3\u30c8\u306e\u5165\u529b\u3054\u3068\u306b\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002\u5b66\u7fd2\u304c\u7d42\u4e86\u3057\u305f\u3089\u3001\u6b21\u30bb\u30c3\u30c8\u306e one-hot \u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\n```Python3:dnc_echo_test.py\nimport dnc\n\nimport numpy as np\nimport chainer\nfrom chainer import functions as F\nfrom chainer import links as L\nfrom chainer import optimizers, Chain, Link, Variable\n\n\ndef onehot(x, n):\n    ret = np.zeros(n).astype(np.float32)\n    ret[x] = 1.0\n    return ret\n\n\nX = 5\nY = 5\nN = 10\nW = 10\nR = 2\n\n\nmodel = dnc.DNC(X, Y, N, W, R)\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n\nn_data = 10000 # number of input data\nloss = 0.0\nacc = 0.0\nacc_bool = []\n\n\nfor data_cnt in range(n_data):\n    \n    loss_frac = np.zeros((1, 2))    \n    \n    # prepare one pair of input and target data \n    # length of input data is randomly set\n    len_content = np.random.randint(3, 6) \n    # generate one input data as a sequence of randam integers\n    content = np.random.randint(0, X-1, len_content) \n    len_seq = len_content + len_content # the former is for input, the latter for the target\n    x_seq_list = [float('nan')] * len_seq # input sequence\n    t_seq_list = [float('nan')] * len_seq # target sequence\n    \n    for i in range(len_seq):\n        # convert a format of input data\n        if (i < len_content):\n            x_seq_list[i] = onehot(content[i], X)\n        elif (i == len_content):\n            x_seq_list[i] = onehot(X-1, X)\n        else:\n            x_seq_list[i] = np.zeros(X).astype(np.float32)\n        # convert a format of output data\n        if (i >= len_content):\n            t_seq_list[i] = onehot(content[i - len_content], X)         \n    \n    model.reset_state() # reset reccurent state per input data\n    \n    # input data is fed as a sequence\n    for cnt in range(len_seq):\n        x = Variable(x_seq_list[cnt].reshape(1, X))\n        if (isinstance(t_seq_list[cnt], np.ndarray)):\n            t = Variable(t_seq_list[cnt].reshape(1, Y))\n        else:\n            t = []\n\n        y = model(x)\n        \n        if (isinstance(t, chainer.Variable)):\n            loss += (y - t)**2\n            acc_bool.append(np.argmax(y.data)==np.argmax(t.data))                        \n            if (np.argmax(y.data)==np.argmax(t.data)): acc += 1\n                \n        if (cnt+1==len_seq):\n            # training by back propagation\n            model.cleargrads()\n            loss.grad = np.ones(loss.shape, dtype=np.float32)\n            loss.backward()\n            optimizer.update()\n            loss.unchain_backward()\n            # print loss and accuracy\n            if data_cnt < 50 or data_cnt >= 9950:\n                print('(', data_cnt, ')', acc_bool, ' :: ', loss.data.sum()/loss.data.size/len_content, ' :: ', acc/len_content)\n            loss_frac += [loss.data.sum()/loss.data.size/len_seq, 1.]\n            loss = 0.0\n            acc = 0.0\n            acc_bool = []\n```\n\n## test \u7d50\u679c\n10000\u56de\u7e70\u308a\u8fd4\u3057\u305f\u6642\u306e\u7d50\u679c\u3067\u3059\u3002\n[ ]\u5185\u306e bool \u5024\u306f\u3001\u51fa\u529b\u3079\u30af\u30c8\u30eb\u3067\u6700\u5927\u5024\u30921\u306b\u3001\u305d\u306e\u4ed6\u30920\u306b\u632f\u308a\u76f4\u3057\u305f\u3082\u306e\u304c\u3001\u5165\u529b\u5024\u3068\u4e00\u81f4\u3059\u308b\u304b\u3092 bool \u5024\u3067\u8868\u3057\u305f\u3082\u306e\u3067\u3059\u3002\n\n\u30fb\u6700\u521d\u306e20\u56de\u306e\u7d50\u679c\n\n```\n( 0 ) [True, False, False]  ::  0.197543557485  ::  0.3333333333333333\n( 1 ) [False, False, False, False]  ::  0.209656882286  ::  0.0\n( 2 ) [True, False, False]  ::  0.172263367971  ::  0.3333333333333333\n( 3 ) [False, True, True]  ::  0.185363880793  ::  0.6666666666666666\n( 4 ) [True, True, True, True]  ::  0.157090616226  ::  1.0\n( 5 ) [False, False, False, False, False]  ::  0.191528530121  ::  0.0\n( 6 ) [True, False, False, False, False]  ::  0.175649337769  ::  0.2\n( 7 ) [False, False, False, True, True]  ::  0.173387451172  ::  0.4\n( 8 ) [True, False, True, True]  ::  0.150813746452  ::  0.75\n( 9 ) [False, True, False]  ::  0.163899072011  ::  0.3333333333333333\n( 10 ) [False, False, False, False, False]  ::  0.183468780518  ::  0.0\n( 11 ) [True, False, True, False]  ::  0.152743542194  ::  0.5\n( 12 ) [False, False, True, False]  ::  0.170574557781  ::  0.25\n( 13 ) [False, True, False, True, False]  ::  0.161617393494  ::  0.4\n( 14 ) [False, False, False, False]  ::  0.168220555782  ::  0.0\n( 15 ) [False, False, False]  ::  0.167814588547  ::  0.0\n( 16 ) [False, True, False, False]  ::  0.158575570583  ::  0.25\n( 17 ) [False, False, False, False]  ::  0.165678012371  ::  0.0\n( 18 ) [False, False, False]  ::  0.165241924922  ::  0.0\n( 19 ) [False, True, False]  ::  0.143808253606  ::  0.3333333333333333\n```\n\u30fb\u6700\u5f8c20\u56de\u306e\u7d50\u679c\n\n```\n( 9980 ) [True, True, True, True]  ::  0.000208107382059  ::  1.0\n( 9981 ) [True, True, True, True, True]  ::  0.000164349582046  ::  1.0\n( 9982 ) [True, True, True, True, True]  ::  0.000122650777921  ::  1.0\n( 9983 ) [True, True, True]  ::  0.000181751077374  ::  1.0\n( 9984 ) [True, True, True, True, True]  ::  0.000318505689502  ::  1.0\n( 9985 ) [True, True, True, True, True]  ::  0.00023639023304  ::  1.0\n( 9986 ) [True, True, True, True, True]  ::  0.000988183766603  ::  1.0\n( 9987 ) [True, True, True, True, True]  ::  0.000226851813495  ::  1.0\n( 9988 ) [True, True, True]  ::  0.000401457709571  ::  1.0\n( 9989 ) [True, True, True, True]  ::  0.000256504747085  ::  1.0\n( 9990 ) [True, True, True, True, True]  ::  0.000165695995092  ::  1.0\n( 9991 ) [True, True, True, True]  ::  0.000123940082267  ::  1.0\n( 9992 ) [True, True, True, True, True]  ::  0.000351718552411  ::  1.0\n( 9993 ) [True, True, True, True]  ::  0.000147357559763  ::  1.0\n( 9994 ) [True, True, True, True]  ::  0.000173216045368  ::  1.0\n( 9995 ) [True, True, True, True]  ::  0.000108330522198  ::  1.0\n( 9996 ) [True, True, True, True]  ::  0.00016659933608  ::  1.0\n( 9997 ) [True, True, True]  ::  0.000255667418242  ::  1.0\n( 9998 ) [True, True, True]  ::  0.000280433737983  ::  1.0\n( 9999 ) [True, True, True, True, True]  ::  0.000443447269499  ::  1.0\n```\n\u5b8c\u74a7\u306b\u6b63\u89e3\u3057\u3066\u3044\u307e\u3059\u300220\u56de\u5206\u3057\u304b\u8f09\u305b\u3066\u3044\u307e\u305b\u3093\u304c\u3001\u6700\u5f8c\u306e100\u56de\u4e2d\u3067False\u306f0\u56de\u3067\u3057\u305f\u3002\u305f\u3060\u3057\u3001\u4ed6\u306e\u624b\u6cd5\u3068\u6bd4\u8f03\u307e\u3067\u306f\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001DNC \u3060\u304b\u3089\u3053\u305d\u306a\u306e\u304b\u306f\u4e0d\u660e\u3067\u3059\u3002\n\n\u4eca\u56de\u306f\u30ed\u30b8\u30c3\u30af\u306e\u7d39\u4ecb\u304c\u76ee\u7684\u3060\u3063\u305f\u306e\u3067\u3053\u3053\u307e\u3067\u3068\u3057\u307e\u3059\u3002\n\n# \u53c2\u8003URL\n\u30fbChainer\u306e\u5b9f\u88c5\u4f8b\u306f\u3053\u306e\u30da\u30fc\u30b8\u306e\u3082\u306e\u3092\u4f7f\u308f\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\n[DNC (Differentiable Neural Computers) \u306e\u6982\u8981 + Chainer \u306b\u3088\u308b\u5b9f\u88c5](http://qiita.com/yos1up/items/599ff75c876f6f94d249)\n\n\u30fb\u4eca\u56de\u306fGPU\u3001\u30d0\u30c3\u30c1\u51e6\u7406\u306a\u3069\u306f\u8003\u3048\u3066\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u4e0b\u306e\u30da\u30fc\u30b8\u304c\u53c2\u8003\u306b\u306a\u308b\u304b\u3068\u601d\u3044\u307e\u3059\n[(Chainer) DNC(Differentiable Neural Computers)\u3067\u6587\u5b57\u5217\u306e\u5b66\u7fd2&\u751f\u6210](http://qiita.com/hatoo@github/items/784fe8f5deea64e0ff73#_reference-a9a61b5435da45c8758c)\n\n\u30fbTensor Flow\u3067\u306e\u5b9f\u88c5\u3082\u3042\u308b\u3088\u3046\u3067\u3059\nhttps://github.com/Mostafa-Samir/DNC-tensorflow\n\n\u30fbLSTM\u306e\u89e3\u8aac\u306f\u3053\u3053\u304c\u308f\u304b\u308a\u3084\u3059\u3044\u3067\u3059\n[\u308f\u304b\u308bLSTM \uff5e \u6700\u8fd1\u306e\u52d5\u5411\u3068\u5171\u306b](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)\n", "tags": ["Python", "Chainer", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af"]}