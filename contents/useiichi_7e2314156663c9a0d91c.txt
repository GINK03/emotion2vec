{"context": "\n\n\u306f\u3058\u3081\u306b\n\u3055\u304f\u3089\u306eVPS\u306bCoreOS\u3067kubernetes\u3092\u7acb\u3066\u308b\u3002\n\nCoreOS\uff08Beta:1235.2.0\uff09\nkubernetes\u306f1 master\u3001node\u306a\u3057\u306e\u69cb\u6210\netcd3\uff08\u99c6\u52d5\u306b\u306frkt\u3092\u4f7f\u7528\uff09\n\u30b3\u30f3\u30c6\u30ca\u74b0\u5883\u306fdocker\n\u30b3\u30f3\u30c6\u30ca\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306fflannel\n\u3055\u304f\u3089\u306eVPS\u306eVPS\u30d7\u30e9\u30f3\uff11\u3064\uff08\u30e1\u30e2\u30ea\uff1a1GB\uff09\nzram\nswap\n\n\n\u3055\u304f\u3089\u306eVPS\u306bCoreOS\u306eiso\u30a4\u30e1\u30fc\u30b8\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\nhttps://coreos.com/os/docs/latest/booting-with-iso.html\n\u306e\u300cDownload Stable ISO\u300d\u30dc\u30bf\u30f3\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002\n\u7ba1\u7406\u30b3\u30f3\u30bd\u30fc\u30eb\u3067SFTP\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u3092\u767a\u884c\u3057\u3066\u3001SFTP\u3067\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3002\n\nISO\u30a4\u30e1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\n\u3053\u3053\u3067\u6b21\u3092\u5165\u529b\u3002\n\n${MASTER_IP}\u3092publicly routable IP\u3067\u7f6e\u304d\u63db\u3048\u308b\nGateway\u3001DNS\u3092\u7f6e\u304d\u63db\u3048\u308b\n\n$ sudo vi /etc/systemd/network/static.network\n\n[Match]\nName=eth0\n[Network]\nAddress=${MASTER_IP}/23\nGateway=xxx.xxx.xxx.1\nDNS=\nDNS=\n\n$ sudo systemctl restart systemd-networkd\n$ sudo passwd core\n\n\u30ed\u30fc\u30ab\u30eb\u306ebash\u74b0\u5883\u306a\u3069\u304b\u3089\u4ee5\u4e0b\u3092\u5b9f\u884c\u3002\n$ ssh ${MASTER_IP} -l core\n\n$ vi cloud-config.yaml\n\n\u6b21\u306eyaml\u3092\u6d41\u3057\u8fbc\u3080\n\nssh-rsa\u3092\u7f6e\u304d\u63db\u3048\u308b\n${MASTER_IP}\u3092publicly routable IP\u3067\u7f6e\u304d\u63db\u3048\u308b\nGateway\u3001DNS\u3092\u7f6e\u304d\u63db\u3048\u308b\n\n#cloud-config\n\n# https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/deploy-master.md\n\n# /var/lib/coreos-install/user_data\n\nssh_authorized_keys:\n  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB\u30fb\u30fb\u30fbZiDsoTMHdHt0nswTkLhl1NAdEHBqt core@localhost\nwrite_files:\n  - path: /etc/systemd/timesyncd.conf\n    content: |\n      [Time]\n      NTP=ntp.nict.jp ntp.jst.mfeed.ad.jp\n  - path: /etc/zrm/zrm.sh\n    owner: root\n    permissions: 0755\n    content: |\n      #!/bin/bash\n      ### BEGIN INIT INFO\n      # Provides: zram\n      # Required-Start:\n      # Required-Stop:\n      # Default-Start: 2 3 4 5\n      # Default-Stop: 0 1 6\n      # Short-Description: Increased Performance In Linux With zRam (Virtual Swap Compressed in RAM)\n      # Description: Adapted from systemd scripts at https://github.com/mystilleef/FedoraZram\n      ### END INIT INFO\n      start() {\n          # get the number of CPUs\n          num_cpus=$(grep -c processor /proc/cpuinfo)\n          # if something goes wrong, assume we have 1\n          [ \"$num_cpus\" != 0 ] || num_cpus=1\n\n          # set decremented number of CPUs\n          decr_num_cpus=$((num_cpus - 1))\n\n          # get the amount of memory in the machine\n          mem_total_kb=$(grep MemTotal /proc/meminfo | grep -E --only-matching '[[:digit:]]+')\n\n          #we will only assign 50% of system memory to zram\n          mem_total_kb=$((mem_total_kb / 5))\n\n          mem_total=$((mem_total_kb * 1024))\n\n          # load dependency modules\n          modprobe zram num_devices=$num_cpus\n\n          # initialize the devices\n          for i in $(seq 0 $decr_num_cpus); do\n          echo $((mem_total / num_cpus)) > /sys/block/zram$i/disksize\n          done\n\n          # Creating swap filesystems\n          for i in $(seq 0 $decr_num_cpus); do\n          mkswap /dev/zram$i\n          done\n\n          # Switch the swaps on\n          for i in $(seq 0 $decr_num_cpus); do\n          swapon -p 100 /dev/zram$i\n          done\n      }\n      stop() {\n              for i in $(grep '^/dev/zram' /proc/swaps | awk '{ print $1 }'); do\n                      swapoff \"$i\"\n              done\n\n              if grep -q \"^zram \" /proc/modules; then\n                      sleep 1\n                      rmmod zram\n              fi\n      }\n      case \"$1\" in\n          start)\n              start\n              ;;\n          stop)\n              stop\n              ;;\n          restart)\n              stop\n              sleep 3\n              start\n              ;;\n      esac\n      wait\n\n  - path: /etc/flannel/options.env\n    owner: root\n    permissions: 0755\n    content: |\n      FLANNELD_IFACE=${MASTER_IP}\n      FLANNELD_ETCD_ENDPOINTS=http://${MASTER_IP}:2379\n\n  - path: /etc/kubernetes/manifests/kube-apiserver.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-apiserver\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-apiserver\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - apiserver\n          - --bind-address=0.0.0.0\n          - --etcd-servers=http://${MASTER_IP}:2379\n          - --allow-privileged=true\n          - --service-cluster-ip-range=10.3.0.0/24\n          - --secure-port=6443\n          - --advertise-address=${MASTER_IP}\n          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota\n          - --storage-backend=etcd3\n          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true\n          - --service-node-port-range=25-32767\n          ports:\n          - containerPort: 6443\n            hostPort: 6443\n            name: https\n          - containerPort: 8080\n            hostPort: 8080\n            name: local\n          volumeMounts:\n          - mountPath: /etc/kubernetes/ssl\n            name: ssl-certs-kubernetes\n            readOnly: true\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /etc/kubernetes/ssl\n          name: ssl-certs-kubernetes\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-proxy.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-proxy\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-proxy\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - proxy\n          - --master=http://127.0.0.1:8080\n          - --proxy-mode=iptables\n          securityContext:\n            privileged: true\n          volumeMounts:\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-controller-manager\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-controller-manager\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - controller-manager\n          - --master=http://127.0.0.1:8080\n          - --leader-elect=true\n          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --root-ca-file=/etc/kubernetes/ssl/ca.pem\n          livenessProbe:\n            httpGet:\n              host: 127.0.0.1\n              path: /healthz\n              port: 10252\n            initialDelaySeconds: 15\n            timeoutSeconds: 1\n          volumeMounts:\n          - mountPath: /etc/kubernetes/ssl\n            name: ssl-certs-kubernetes\n            readOnly: true\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /etc/kubernetes/ssl\n          name: ssl-certs-kubernetes\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-scheduler.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-scheduler\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-scheduler\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - scheduler\n          - --master=http://127.0.0.1:8080\n          - --leader-elect=true\n          livenessProbe:\n            httpGet:\n              host: 127.0.0.1\n              path: /healthz\n              port: 10251\n            initialDelaySeconds: 15\n            timeoutSeconds: 1\n\ncoreos:\n  units:\n    - name: settimezone.service\n      command: start\n      content: |\n        [Unit]\n        Description=Set the time zone\n\n        [Service]\n        ExecStart=/usr/bin/timedatectl set-timezone Asia/Tokyo\n        RemainAfterExit=yes\n        Type=oneshot\n    - name: zrm.service\n      command: start\n      content: |\n        [Unit]\n        Description=Manage swap spaces on zram, files and partitions.\n        After=local-fs.target\n\n        [Service]\n        RemainAfterExit=yes\n        ExecStart=/etc/zrm/zrm.sh start\n        ExecStop=/etc/zrm/zrm.sh  stop\n\n        [Install]\n        WantedBy=local-fs.target\n    - name: swap.service\n      command: start\n      content: |\n        [Unit]\n        Description=Turn on swap\n        Before=docker.service\n\n        [Service]\n        Type=oneshot\n        Environment=\"SWAPFILE=/swapfile\"\n        Environment=\"SWAPSIZE=2GiB\"\n        RemainAfterExit=true\n        ExecStartPre=/usr/bin/sh -c '/usr/bin/fallocate -l 2GiB /swapfile && chmod 0600 /swapfile && /usr/sbin/mkswap /swapfile'\n        ExecStartPre=/usr/sbin/losetup -f /swapfile\n        ExecStart=/usr/bin/sh -c \"/sbin/swapon /dev/loop0\"\n        ExecStop=/usr/bin/sh -c \"/sbin/swapoff /dev/loop0\"\n        ExecStopPost=/usr/bin/sh -c \"/usr/sbin/losetup -d /dev/loop0\"\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: static.network\n      content: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        Address=${MASTER_IP}/23\n        Gateway=xxx.xxx.xxx.1\n        DNS=\n        DNS=\n\n    - name: etcd3.service\n      command: start\n      content: |\n        [Unit]\n        Description=etcd3\n        Before=kubelet.service\n        [Service]\n        ExecStart=/usr/bin/rkt run --volume data-dir,kind=host,source=/var/lib/etcd,readOnly=false --net=host coreos.com/etcd:v3.0.13 -- \\\n          -name=infra0 \\\n          -advertise-client-urls=http://${MASTER_IP}:2379 \\\n          -listen-client-urls=http://${MASTER_IP}:2379,http://127.0.0.1:2379 \\\n          -initial-advertise-peer-urls=http://${MASTER_IP}:2380 \\\n          -listen-peer-urls=http://${MASTER_IP}:2380 \\\n          -initial-cluster-token=etcd-cluster-1 \\\n          -initial-cluster=infra0=http://${MASTER_IP}:2380 \\\n          -initial-cluster-state=new\n        [Install]\n        WantedBy=multi-user.target\n\n    - name: flanneld.service\n      command: start\n      drop-ins:\n      - name: 40-ExecStartPre-symlink.conf\n        content: |\n          [Service]\n          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env\n    - name: docker.service\n      command: start\n      drop-ins:\n      - name: 40-flannel.conf\n        content: |\n          [Unit]\n          Requires=flanneld.service\n          After=flanneld.service\n    - name: kubelet.service\n      command: start\n      content: |\n        [Service]\n        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests\n        ExecStart=/usr/bin/docker run \\\n          --net=host \\\n          --pid=host \\\n          --privileged \\\n          -v /:/rootfs:ro \\\n          -v /sys:/sys:rw \\\n          -v /var/run:/var/run:rw \\\n          -v /run:/run:rw \\\n          -v /var/lib/docker:/var/lib/docker:rw \\\n          -v /var/lib/kubelet:/var/lib/kubelet:slave \\\n          -v /var/log/containers:/var/log/containers:rw \\\n          -v /etc/kubernetes:/etc/kubernetes:rw \\\n          gcr.io/google_containers/hyperkube-amd64:v1.4.7 \\\n          /hyperkube kubelet \\\n          --allow-privileged \\\n          --api-servers=http://127.0.0.1:8080 \\\n          --network-plugin-dir=/etc/kubernetes/cni/net.d \\\n          --network-plugin= \\\n          --register-schedulable=true \\\n          --allow-privileged=true \\\n          --pod-manifest-path=/etc/kubernetes/manifests \\\n          --hostname-override=${MASTER_IP} \\\n          --cluster-dns=10.3.0.10 \\\n          --cluster-domain=cluster.local \\\n          --containerized \\\n          --v=2 \\\n          --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec/\n        Restart=always\n        RestartSec=10\n        [Install]\n        WantedBy=multi-user.target\n\n$ sudo coreos-install -d /dev/vda -C beta -V 1235.2.0 -c cloud-config.yaml\n$ sudo shutdown -h now\n\n\u3055\u304f\u3089\u306eVPS\u306e\u30b3\u30f3\u30bd\u30fc\u30eb\u304b\u3089vps\u3092\u8d77\u52d5\u3057\u3001\u6b21\u3092\u5b9f\u884c\u3002\n$ rm ~/.ssh/known_hosts\n$ ssh ${MASTER_IP} -l core\n\n\netcd3\u304cFailed\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n$ sudo rkt trust --prefix \"coreos.com/etcd\"\n\n\nKubernetes\u306e\u305f\u3081\u306eTLS Assets\u3092\u6e96\u5099\n\u6b21\u306eURL\u306e\u624b\u9806\u306b\u5f93\u3063\u3066TLS Assets\u3092\u751f\u6210\u3002\nhttps://coreos.com/kubernetes/docs/latest/openssl.html\n\u4ee5\u964d\u306e\u624b\u9806\u3067\u306f\u6b21\u306e\uff15\u3064\u306e\u30ad\u30fc\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u4eee\u5b9a\u3057\u307e\u3059\u3002\n\nca.pem\napiserver.pem\napiserver-key.pem\nadmin.pem\nadmin-key.pem\n\n\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308a\u3001\u751f\u6210\u3057\u305f\u30ad\u30fc\u3092\u6b21\u306e\u5834\u6240\u306b\u7f6e\u304d\u307e\u3059\u3002\n$ ssh ${MASTER_IP} -l core\n$ mkdir -p /etc/kubernetes/ssl\n\n\nFile: /etc/kubernetes/ssl/ca.pem\nFile: /etc/kubernetes/ssl/apiserver.pem\nFile: /etc/kubernetes/ssl/apiserver-key.pem\n\n\u30ad\u30fc\u306b\u9069\u5207\u306a\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u8a2d\u5b9a\n$ sudo chmod 600 /etc/kubernetes/ssl/*-key.pem\n$ sudo chown root:root /etc/kubernetes/ssl/*-key.pem\n\nvps\u3092\u518d\u8d77\u52d5\n$ sudo reboot\n\n$ ssh ${MASTER_IP} -l core\n$ curl -X PUT -d \"value={\\\"Network\\\":\\\"10.2.0.0/16\\\",\\\"Backend\\\":{\\\"Type\\\":\\\"vxlan\\\"}}\" \"http://10.1.10.2:2379/v2/keys/coreos.com/network/config\"\n\netcd3\u304c\u52d5\u4f5c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\n$ curl http://127.0.0.1:8080/version\n\n\n$ etcdctl cluster-health\n\n\nvps\u304b\u3089\u51fa\u308b\u3002\n$ exit\n\n\nkubectl\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\nkubectl\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\nlinux\u306e\u5834\u5408\u3001\u6b21\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\n$ curl -O https://storage.googleapis.com/kubernetes-release/release/v1.4.3/bin/linux/amd64/kubectl\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3089\u3001\u5b9f\u884c\u6a29\u9650\u3092\u78ba\u8a8d\u3057\u3001\u9069\u5207\u306a\u30d1\u30b9\u306b\u79fb\u52d5\u3059\u308b\n$ chmod +x kubectl\n$ mv kubectl /usr/local/bin/kubectl\n\nkubectl\u3092\u8a2d\u5b9a\u3059\u308b\u3002\n\nReplace ${MASTER_HOST} with the master node address or name used in previous steps\nReplace ${CA_CERT} with the absolute path to the ca.pem created in previous steps\nReplace ${ADMIN_KEY} with the absolute path to the admin-key.pem created in previous steps\nReplace ${ADMIN_CERT} with the absolute path to the admin.pem created in previous steps\n\n$ kubectl config set-cluster default-cluster --server=https://${MASTER_HOST} --certificate-authority=${CA_CERT}\n$ kubectl config set-credentials default-admin --certificate-authority=${CA_CERT} --client-key=${ADMIN_KEY} --client-certificate=${ADMIN_CERT}\n$ kubectl config set-context default-system --cluster=default-cluster --user=default-admin\n$ kubectl config use-context default-system\n\nkubectl\u306e\u8a2d\u5b9a\u3068\u63a5\u7d9a\u3092\u78ba\u8a8d\u3059\u308b\u3002\n$ kubectl get nodes\nNAME          STATUS          AGE\nX.X.X.X       Ready           1d\n\n\nDNS\u30a2\u30c9\u30aa\u30f3\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\n\nskydns.yaml\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*\n\n# Warning: This is a file generated from the base underscore template file: skydns-svc.yaml.base\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"KubeDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: 10.3.0.10\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n\n\n---\n\n\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*\n# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml\n# in sync with this file.\n\n# Warning: This is a file generated from the base underscore template file: skydns-rc.yaml.base\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  # replicas: not specified here:\n  # 1. In order to make Addon Manager do not reconcile this replicas parameter.\n  # 2. Default is 1.\n  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]'\n    spec:\n      containers:\n      - name: kubedns\n        image: gcr.io/google_containers/kubedns-amd64:1.9\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz-kubedns\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8081\n            scheme: HTTP\n          # we poll on pod startup for the Kubernetes master service and\n          # only setup the /readiness HTTP server once that's available.\n          initialDelaySeconds: 3\n          timeoutSeconds: 5\n        args:\n        - --domain=cluster.local.\n        - --dns-port=10053\n        - --config-map=kube-dns\n        # This should be set to v=2 only after the new image (cut from 1.5) has\n        # been released, otherwise we will flood the logs.\n        - --v=0\n        env:\n        - name: PROMETHEUS_PORT\n          value: \"10055\"\n        ports:\n        - containerPort: 10053\n          name: dns-local\n          protocol: UDP\n        - containerPort: 10053\n          name: dns-tcp-local\n          protocol: TCP\n        - containerPort: 10055\n          name: metrics\n          protocol: TCP\n      - name: dnsmasq\n        image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4\n        livenessProbe:\n          httpGet:\n            path: /healthz-dnsmasq\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --cache-size=1000\n        - --no-resolv\n        - --server=127.0.0.1#10053\n        - --log-facility=-\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details\n        resources:\n          requests:\n            cpu: 150m\n            memory: 10Mi\n      - name: dnsmasq-metrics\n        image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --v=2\n        - --logtostderr\n        ports:\n        - containerPort: 10054\n          name: metrics\n          protocol: TCP\n        resources:\n          requests:\n            memory: 10Mi\n      - name: healthz\n        image: gcr.io/google_containers/exechealthz-amd64:1.2\n        resources:\n          limits:\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            # Note that this container shouldn't really need 50Mi of memory. The\n            # limits are set higher than expected pending investigation on #29688.\n            # The extra memory was stolen from the kubedns container to keep the\n            # net memory requested by the pod constant.\n            memory: 50Mi\n        args:\n        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null\n        - --url=/healthz-dnsmasq\n        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null\n        - --url=/healthz-kubedns\n        - --port=8080\n        - --quiet\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n      dnsPolicy: Default  # Don't use cluster DNS.\n\n\n\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u3067DNS\u30a2\u30c9\u30aa\u30f3\u3092\u30b9\u30bf\u30fc\u30c8\u3055\u305b\u308b\u3002\n$ kubectl create -f skydns.yaml\n\n\u52d5\u3044\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3002\n$ kubectl get pods --namespace=kube-system | grep kube-dns\n\n\u306a\u304a\u3001\u6b21\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u3044\u308b\u5834\u5408\u306f\u3001\n$ E0927 08:06:37.575899       1 reflector.go:214] pkg/dns/dns.go:155: Failed to list *api.Endpoints: the server has asked for the client to provide credentials (get endpoints)\n$ 1 reflector.go:214] pkg/dns/dns.go:156: Failed to list *api.Service: the server has asked for the client to provide credentials (get services)\n$ 1 dns.go:173] Ignoring error while waiting for service default/kubernetes: the server has asked for the client to provide credentials (get services kubernetes). Sleeping 1s before retrying.\n\n\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306b\u3088\u308a\u3001\u89e3\u6c7a\u3067\u304d\u305f\u3002\nWhen I removed the token kubectl delete secrets/default-token-bkghp --namespace=kube-system it was automatically recreated and kube2sky started to work.\n\nPod\u304c\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\n\nbusybox.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n\n\n$ kubectl create -f busybox.yaml\n\n$ kubectl get pods busybox\n\n$ kubectl exec busybox -- nslookup kubernetes.default\n\n\n# \u306f\u3058\u3081\u306b\n\u3055\u304f\u3089\u306eVPS\u306bCoreOS\u3067kubernetes\u3092\u7acb\u3066\u308b\u3002\n\n- CoreOS\uff08Beta:1235.2.0\uff09\n- kubernetes\u306f1 master\u3001node\u306a\u3057\u306e\u69cb\u6210\n- etcd3\uff08\u99c6\u52d5\u306b\u306frkt\u3092\u4f7f\u7528\uff09\n- \u30b3\u30f3\u30c6\u30ca\u74b0\u5883\u306fdocker\n- \u30b3\u30f3\u30c6\u30ca\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306fflannel\n- \u3055\u304f\u3089\u306eVPS\u306eVPS\u30d7\u30e9\u30f3\uff11\u3064\uff08\u30e1\u30e2\u30ea\uff1a1GB\uff09\n- zram\n- swap\n\n# \u3055\u304f\u3089\u306eVPS\u306bCoreOS\u306eiso\u30a4\u30e1\u30fc\u30b8\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\nhttps://coreos.com/os/docs/latest/booting-with-iso.html\n\u306e\u300cDownload Stable ISO\u300d\u30dc\u30bf\u30f3\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002\n\u7ba1\u7406\u30b3\u30f3\u30bd\u30fc\u30eb\u3067SFTP\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u3092\u767a\u884c\u3057\u3066\u3001SFTP\u3067\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3002\n\n# ISO\u30a4\u30e1\u30fc\u30b8\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n![100_.png](https://qiita-image-store.s3.amazonaws.com/0/83147/e88599de-28e1-73dc-1131-71ad6929268a.png)\n\n![111__.png](https://qiita-image-store.s3.amazonaws.com/0/83147/895d7030-30ba-f745-862b-f8cd36fb8624.png)\n\n\u3053\u3053\u3067\u6b21\u3092\u5165\u529b\u3002\n\n- ${MASTER_IP}\u3092publicly routable IP\u3067\u7f6e\u304d\u63db\u3048\u308b\n- Gateway\u3001DNS\u3092\u7f6e\u304d\u63db\u3048\u308b\n\n```bash:\n$ sudo vi /etc/systemd/network/static.network\n```\n\n```text:\n[Match]\nName=eth0\n[Network]\nAddress=${MASTER_IP}/23\nGateway=xxx.xxx.xxx.1\nDNS=\nDNS=\n```\n\n```bash:\n$ sudo systemctl restart systemd-networkd\n$ sudo passwd core\n```\n\n\u30ed\u30fc\u30ab\u30eb\u306ebash\u74b0\u5883\u306a\u3069\u304b\u3089\u4ee5\u4e0b\u3092\u5b9f\u884c\u3002\n\n```bash:\n$ ssh ${MASTER_IP} -l core\n```\n```bash:\n$ vi cloud-config.yaml\n```\n\u6b21\u306eyaml\u3092\u6d41\u3057\u8fbc\u3080\n\n- ssh-rsa\u3092\u7f6e\u304d\u63db\u3048\u308b\n- ${MASTER_IP}\u3092publicly routable IP\u3067\u7f6e\u304d\u63db\u3048\u308b\n- Gateway\u3001DNS\u3092\u7f6e\u304d\u63db\u3048\u308b\n\n```yaml:\n#cloud-config\n\n# https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/deploy-master.md\n\n# /var/lib/coreos-install/user_data\n\nssh_authorized_keys:\n  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB\u30fb\u30fb\u30fbZiDsoTMHdHt0nswTkLhl1NAdEHBqt core@localhost\nwrite_files:\n  - path: /etc/systemd/timesyncd.conf\n    content: |\n      [Time]\n      NTP=ntp.nict.jp ntp.jst.mfeed.ad.jp\n  - path: /etc/zrm/zrm.sh\n    owner: root\n    permissions: 0755\n    content: |\n      #!/bin/bash\n      ### BEGIN INIT INFO\n      # Provides: zram\n      # Required-Start:\n      # Required-Stop:\n      # Default-Start: 2 3 4 5\n      # Default-Stop: 0 1 6\n      # Short-Description: Increased Performance In Linux With zRam (Virtual Swap Compressed in RAM)\n      # Description: Adapted from systemd scripts at https://github.com/mystilleef/FedoraZram\n      ### END INIT INFO\n      start() {\n          # get the number of CPUs\n          num_cpus=$(grep -c processor /proc/cpuinfo)\n          # if something goes wrong, assume we have 1\n          [ \"$num_cpus\" != 0 ] || num_cpus=1\n\n          # set decremented number of CPUs\n          decr_num_cpus=$((num_cpus - 1))\n\n          # get the amount of memory in the machine\n          mem_total_kb=$(grep MemTotal /proc/meminfo | grep -E --only-matching '[[:digit:]]+')\n\n          #we will only assign 50% of system memory to zram\n          mem_total_kb=$((mem_total_kb / 5))\n\n          mem_total=$((mem_total_kb * 1024))\n\n          # load dependency modules\n          modprobe zram num_devices=$num_cpus\n\n          # initialize the devices\n          for i in $(seq 0 $decr_num_cpus); do\n          echo $((mem_total / num_cpus)) > /sys/block/zram$i/disksize\n          done\n\n          # Creating swap filesystems\n          for i in $(seq 0 $decr_num_cpus); do\n          mkswap /dev/zram$i\n          done\n\n          # Switch the swaps on\n          for i in $(seq 0 $decr_num_cpus); do\n          swapon -p 100 /dev/zram$i\n          done\n      }\n      stop() {\n              for i in $(grep '^/dev/zram' /proc/swaps | awk '{ print $1 }'); do\n                      swapoff \"$i\"\n              done\n\n              if grep -q \"^zram \" /proc/modules; then\n                      sleep 1\n                      rmmod zram\n              fi\n      }\n      case \"$1\" in\n          start)\n              start\n              ;;\n          stop)\n              stop\n              ;;\n          restart)\n              stop\n              sleep 3\n              start\n              ;;\n      esac\n      wait\n\n  - path: /etc/flannel/options.env\n    owner: root\n    permissions: 0755\n    content: |\n      FLANNELD_IFACE=${MASTER_IP}\n      FLANNELD_ETCD_ENDPOINTS=http://${MASTER_IP}:2379\n\n  - path: /etc/kubernetes/manifests/kube-apiserver.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-apiserver\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-apiserver\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - apiserver\n          - --bind-address=0.0.0.0\n          - --etcd-servers=http://${MASTER_IP}:2379\n          - --allow-privileged=true\n          - --service-cluster-ip-range=10.3.0.0/24\n          - --secure-port=6443\n          - --advertise-address=${MASTER_IP}\n          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota\n          - --storage-backend=etcd3\n          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem\n          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --client-ca-file=/etc/kubernetes/ssl/ca.pem\n          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true\n          - --service-node-port-range=25-32767\n          ports:\n          - containerPort: 6443\n            hostPort: 6443\n            name: https\n          - containerPort: 8080\n            hostPort: 8080\n            name: local\n          volumeMounts:\n          - mountPath: /etc/kubernetes/ssl\n            name: ssl-certs-kubernetes\n            readOnly: true\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /etc/kubernetes/ssl\n          name: ssl-certs-kubernetes\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-proxy.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-proxy\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-proxy\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - proxy\n          - --master=http://127.0.0.1:8080\n          - --proxy-mode=iptables\n          securityContext:\n            privileged: true\n          volumeMounts:\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-controller-manager\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-controller-manager\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - controller-manager\n          - --master=http://127.0.0.1:8080\n          - --leader-elect=true\n          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem\n          - --root-ca-file=/etc/kubernetes/ssl/ca.pem\n          livenessProbe:\n            httpGet:\n              host: 127.0.0.1\n              path: /healthz\n              port: 10252\n            initialDelaySeconds: 15\n            timeoutSeconds: 1\n          volumeMounts:\n          - mountPath: /etc/kubernetes/ssl\n            name: ssl-certs-kubernetes\n            readOnly: true\n          - mountPath: /etc/ssl/certs\n            name: ssl-certs-host\n            readOnly: true\n        volumes:\n        - hostPath:\n            path: /etc/kubernetes/ssl\n          name: ssl-certs-kubernetes\n        - hostPath:\n            path: /usr/share/ca-certificates\n          name: ssl-certs-host\n\n  - path: /etc/kubernetes/manifests/kube-scheduler.yaml\n    owner: root\n    permissions: 0755\n    content: |\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: kube-scheduler\n        namespace: kube-system\n      spec:\n        hostNetwork: true\n        containers:\n        - name: kube-scheduler\n          image: gcr.io/google_containers/hyperkube-amd64:v1.4.7\n          command:\n          - /hyperkube\n          - scheduler\n          - --master=http://127.0.0.1:8080\n          - --leader-elect=true\n          livenessProbe:\n            httpGet:\n              host: 127.0.0.1\n              path: /healthz\n              port: 10251\n            initialDelaySeconds: 15\n            timeoutSeconds: 1\n\ncoreos:\n  units:\n    - name: settimezone.service\n      command: start\n      content: |\n        [Unit]\n        Description=Set the time zone\n\n        [Service]\n        ExecStart=/usr/bin/timedatectl set-timezone Asia/Tokyo\n        RemainAfterExit=yes\n        Type=oneshot\n    - name: zrm.service\n      command: start\n      content: |\n        [Unit]\n        Description=Manage swap spaces on zram, files and partitions.\n        After=local-fs.target\n\n        [Service]\n        RemainAfterExit=yes\n        ExecStart=/etc/zrm/zrm.sh start\n        ExecStop=/etc/zrm/zrm.sh  stop\n\n        [Install]\n        WantedBy=local-fs.target\n    - name: swap.service\n      command: start\n      content: |\n        [Unit]\n        Description=Turn on swap\n        Before=docker.service\n\n        [Service]\n        Type=oneshot\n        Environment=\"SWAPFILE=/swapfile\"\n        Environment=\"SWAPSIZE=2GiB\"\n        RemainAfterExit=true\n        ExecStartPre=/usr/bin/sh -c '/usr/bin/fallocate -l 2GiB /swapfile && chmod 0600 /swapfile && /usr/sbin/mkswap /swapfile'\n        ExecStartPre=/usr/sbin/losetup -f /swapfile\n        ExecStart=/usr/bin/sh -c \"/sbin/swapon /dev/loop0\"\n        ExecStop=/usr/bin/sh -c \"/sbin/swapoff /dev/loop0\"\n        ExecStopPost=/usr/bin/sh -c \"/usr/sbin/losetup -d /dev/loop0\"\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: static.network\n      content: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        Address=${MASTER_IP}/23\n        Gateway=xxx.xxx.xxx.1\n        DNS=\n        DNS=\n\n    - name: etcd3.service\n      command: start\n      content: |\n        [Unit]\n        Description=etcd3\n        Before=kubelet.service\n        [Service]\n        ExecStart=/usr/bin/rkt run --volume data-dir,kind=host,source=/var/lib/etcd,readOnly=false --net=host coreos.com/etcd:v3.0.13 -- \\\n          -name=infra0 \\\n          -advertise-client-urls=http://${MASTER_IP}:2379 \\\n          -listen-client-urls=http://${MASTER_IP}:2379,http://127.0.0.1:2379 \\\n          -initial-advertise-peer-urls=http://${MASTER_IP}:2380 \\\n          -listen-peer-urls=http://${MASTER_IP}:2380 \\\n          -initial-cluster-token=etcd-cluster-1 \\\n          -initial-cluster=infra0=http://${MASTER_IP}:2380 \\\n          -initial-cluster-state=new\n        [Install]\n        WantedBy=multi-user.target\n\n    - name: flanneld.service\n      command: start\n      drop-ins:\n      - name: 40-ExecStartPre-symlink.conf\n        content: |\n          [Service]\n          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env\n    - name: docker.service\n      command: start\n      drop-ins:\n      - name: 40-flannel.conf\n        content: |\n          [Unit]\n          Requires=flanneld.service\n          After=flanneld.service\n    - name: kubelet.service\n      command: start\n      content: |\n        [Service]\n        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests\n        ExecStart=/usr/bin/docker run \\\n          --net=host \\\n          --pid=host \\\n          --privileged \\\n          -v /:/rootfs:ro \\\n          -v /sys:/sys:rw \\\n          -v /var/run:/var/run:rw \\\n          -v /run:/run:rw \\\n          -v /var/lib/docker:/var/lib/docker:rw \\\n          -v /var/lib/kubelet:/var/lib/kubelet:slave \\\n          -v /var/log/containers:/var/log/containers:rw \\\n          -v /etc/kubernetes:/etc/kubernetes:rw \\\n          gcr.io/google_containers/hyperkube-amd64:v1.4.7 \\\n          /hyperkube kubelet \\\n          --allow-privileged \\\n          --api-servers=http://127.0.0.1:8080 \\\n          --network-plugin-dir=/etc/kubernetes/cni/net.d \\\n          --network-plugin= \\\n          --register-schedulable=true \\\n          --allow-privileged=true \\\n          --pod-manifest-path=/etc/kubernetes/manifests \\\n          --hostname-override=${MASTER_IP} \\\n          --cluster-dns=10.3.0.10 \\\n          --cluster-domain=cluster.local \\\n          --containerized \\\n          --v=2 \\\n          --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec/\n        Restart=always\n        RestartSec=10\n        [Install]\n        WantedBy=multi-user.target\n```\n\n```bash:\n$ sudo coreos-install -d /dev/vda -C beta -V 1235.2.0 -c cloud-config.yaml\n$ sudo shutdown -h now\n```\n\n\u3055\u304f\u3089\u306eVPS\u306e\u30b3\u30f3\u30bd\u30fc\u30eb\u304b\u3089vps\u3092\u8d77\u52d5\u3057\u3001\u6b21\u3092\u5b9f\u884c\u3002\n\n```bash:\n$ rm ~/.ssh/known_hosts\n$ ssh ${MASTER_IP} -l core\n```\n\n![245____.png](https://qiita-image-store.s3.amazonaws.com/0/83147/79dd42be-3ca0-6a8b-fc19-0d79775d4d3c.png)\n\netcd3\u304cFailed\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\n\n```bash:\n$ sudo rkt trust --prefix \"coreos.com/etcd\"\n```\n# Kubernetes\u306e\u305f\u3081\u306eTLS Assets\u3092\u6e96\u5099\n\u6b21\u306eURL\u306e\u624b\u9806\u306b\u5f93\u3063\u3066TLS Assets\u3092\u751f\u6210\u3002\nhttps://coreos.com/kubernetes/docs/latest/openssl.html\n\n\u4ee5\u964d\u306e\u624b\u9806\u3067\u306f\u6b21\u306e\uff15\u3064\u306e\u30ad\u30fc\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u4eee\u5b9a\u3057\u307e\u3059\u3002\n\n- ca.pem\n- apiserver.pem\n- apiserver-key.pem\n- admin.pem\n- admin-key.pem\n\n\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308a\u3001\u751f\u6210\u3057\u305f\u30ad\u30fc\u3092\u6b21\u306e\u5834\u6240\u306b\u7f6e\u304d\u307e\u3059\u3002\n\n```bash:\n$ ssh ${MASTER_IP} -l core\n$ mkdir -p /etc/kubernetes/ssl\n```\n\n- File: /etc/kubernetes/ssl/ca.pem\n- File: /etc/kubernetes/ssl/apiserver.pem\n- File: /etc/kubernetes/ssl/apiserver-key.pem\n\n\u30ad\u30fc\u306b\u9069\u5207\u306a\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u8a2d\u5b9a\n\n```bash:\n$ sudo chmod 600 /etc/kubernetes/ssl/*-key.pem\n$ sudo chown root:root /etc/kubernetes/ssl/*-key.pem\n```\n\nvps\u3092\u518d\u8d77\u52d5\n\n```bash:\n$ sudo reboot\n```\n\n```bash:\n$ ssh ${MASTER_IP} -l core\n$ curl -X PUT -d \"value={\\\"Network\\\":\\\"10.2.0.0/16\\\",\\\"Backend\\\":{\\\"Type\\\":\\\"vxlan\\\"}}\" \"http://10.1.10.2:2379/v2/keys/coreos.com/network/config\"\n```\n\netcd3\u304c\u52d5\u4f5c\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\n\n```bash:\n$ curl http://127.0.0.1:8080/version\n```\n\n![268____.png](https://qiita-image-store.s3.amazonaws.com/0/83147/c1d80f3b-8d44-c7a8-9d59-fe4d984acffc.png)\n\n```bash:\n$ etcdctl cluster-health\n```\n\n![279_________.png](https://qiita-image-store.s3.amazonaws.com/0/83147/139ff1ac-b433-097f-7887-270bf000da3b.png)\n\nvps\u304b\u3089\u51fa\u308b\u3002\n\n```bash:\n$ exit\n```\n\n# kubectl\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\n\nkubectl\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\nlinux\u306e\u5834\u5408\u3001\u6b21\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\n\n```bash:\n$ curl -O https://storage.googleapis.com/kubernetes-release/release/v1.4.3/bin/linux/amd64/kubectl\n```\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3089\u3001\u5b9f\u884c\u6a29\u9650\u3092\u78ba\u8a8d\u3057\u3001\u9069\u5207\u306a\u30d1\u30b9\u306b\u79fb\u52d5\u3059\u308b\n\n```bash:\n$ chmod +x kubectl\n$ mv kubectl /usr/local/bin/kubectl\n```\n\nkubectl\u3092\u8a2d\u5b9a\u3059\u308b\u3002\n\n- Replace ${MASTER_HOST} with the master node address or name used in previous steps\n- Replace ${CA_CERT} with the absolute path to the ca.pem created in previous steps\n- Replace ${ADMIN_KEY} with the absolute path to the admin-key.pem created in previous steps\n- Replace ${ADMIN_CERT} with the absolute path to the admin.pem created in previous steps\n\n```bash:\n$ kubectl config set-cluster default-cluster --server=https://${MASTER_HOST} --certificate-authority=${CA_CERT}\n$ kubectl config set-credentials default-admin --certificate-authority=${CA_CERT} --client-key=${ADMIN_KEY} --client-certificate=${ADMIN_CERT}\n$ kubectl config set-context default-system --cluster=default-cluster --user=default-admin\n$ kubectl config use-context default-system\n```\n\nkubectl\u306e\u8a2d\u5b9a\u3068\u63a5\u7d9a\u3092\u78ba\u8a8d\u3059\u308b\u3002\n\n```bash:\n$ kubectl get nodes\nNAME          STATUS          AGE\nX.X.X.X       Ready           1d\n```\n\n# DNS\u30a2\u30c9\u30aa\u30f3\u3092\u30c7\u30d7\u30ed\u30a4\u3059\u308b\n\n```yaml:skydns.yaml\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*\n\n# Warning: This is a file generated from the base underscore template file: skydns-svc.yaml.base\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"KubeDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: 10.3.0.10\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n\n\n---\n\n\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*\n# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml\n# in sync with this file.\n\n# Warning: This is a file generated from the base underscore template file: skydns-rc.yaml.base\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  # replicas: not specified here:\n  # 1. In order to make Addon Manager do not reconcile this replicas parameter.\n  # 2. Default is 1.\n  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]'\n    spec:\n      containers:\n      - name: kubedns\n        image: gcr.io/google_containers/kubedns-amd64:1.9\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz-kubedns\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8081\n            scheme: HTTP\n          # we poll on pod startup for the Kubernetes master service and\n          # only setup the /readiness HTTP server once that's available.\n          initialDelaySeconds: 3\n          timeoutSeconds: 5\n        args:\n        - --domain=cluster.local.\n        - --dns-port=10053\n        - --config-map=kube-dns\n        # This should be set to v=2 only after the new image (cut from 1.5) has\n        # been released, otherwise we will flood the logs.\n        - --v=0\n        env:\n        - name: PROMETHEUS_PORT\n          value: \"10055\"\n        ports:\n        - containerPort: 10053\n          name: dns-local\n          protocol: UDP\n        - containerPort: 10053\n          name: dns-tcp-local\n          protocol: TCP\n        - containerPort: 10055\n          name: metrics\n          protocol: TCP\n      - name: dnsmasq\n        image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4\n        livenessProbe:\n          httpGet:\n            path: /healthz-dnsmasq\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --cache-size=1000\n        - --no-resolv\n        - --server=127.0.0.1#10053\n        - --log-facility=-\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details\n        resources:\n          requests:\n            cpu: 150m\n            memory: 10Mi\n      - name: dnsmasq-metrics\n        image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --v=2\n        - --logtostderr\n        ports:\n        - containerPort: 10054\n          name: metrics\n          protocol: TCP\n        resources:\n          requests:\n            memory: 10Mi\n      - name: healthz\n        image: gcr.io/google_containers/exechealthz-amd64:1.2\n        resources:\n          limits:\n            memory: 50Mi\n          requests:\n            cpu: 10m\n            # Note that this container shouldn't really need 50Mi of memory. The\n            # limits are set higher than expected pending investigation on #29688.\n            # The extra memory was stolen from the kubedns container to keep the\n            # net memory requested by the pod constant.\n            memory: 50Mi\n        args:\n        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null\n        - --url=/healthz-dnsmasq\n        - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null\n        - --url=/healthz-kubedns\n        - --port=8080\n        - --quiet\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n      dnsPolicy: Default  # Don't use cluster DNS.\n```\n\n\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u3067DNS\u30a2\u30c9\u30aa\u30f3\u3092\u30b9\u30bf\u30fc\u30c8\u3055\u305b\u308b\u3002\n\n```bash:\n$ kubectl create -f skydns.yaml\n```\n\n\u52d5\u3044\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3002\n\n```bash:\n$ kubectl get pods --namespace=kube-system | grep kube-dns\n```\n\n\u306a\u304a\u3001\u6b21\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u3044\u308b\u5834\u5408\u306f\u3001\n\n```bash:\n$ E0927 08:06:37.575899       1 reflector.go:214] pkg/dns/dns.go:155: Failed to list *api.Endpoints: the server has asked for the client to provide credentials (get endpoints)\n$ 1 reflector.go:214] pkg/dns/dns.go:156: Failed to list *api.Service: the server has asked for the client to provide credentials (get services)\n$ 1 dns.go:173] Ignoring error while waiting for service default/kubernetes: the server has asked for the client to provide credentials (get services kubernetes). Sleeping 1s before retrying.\n```\n\n\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306b\u3088\u308a\u3001\u89e3\u6c7a\u3067\u304d\u305f\u3002\n\nWhen I removed the token kubectl delete secrets/default-token-bkghp --namespace=kube-system it was automatically recreated and kube2sky started to work.\n\n# Pod\u304c\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\n\n```yaml:busybox.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - image: busybox\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n```\n\n```bash:\n$ kubectl create -f busybox.yaml\n```\n\n```bash:\n$ kubectl get pods busybox\n```\n\n```bash:\n$ kubectl exec busybox -- nslookup kubernetes.default\n```\n\n![zzzzzzzzzzzzz222.png](https://qiita-image-store.s3.amazonaws.com/0/83147/9365c99d-7727-5139-0d67-e02930a38f3a.png)\n\n\n", "tags": ["CoreOS", "kubernetes", "\u3055\u304f\u3089\u306eVPS", "etcd3"]}