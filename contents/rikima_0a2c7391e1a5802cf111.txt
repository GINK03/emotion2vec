{"context": " More than 1 year has passed since last update.\u3092\u3053\u3053\u306b\u66f8\u3044\u3066\u3044\u304f\n\n\u5206\u6563\u3067\u3069\u3046\u3084\u3063\u3066gradient update\u3084\u3063\u3066\u3044\u308b\u304b\n\n\nworker\u306e\u7d50\u679c\u3092\u5e73\u5747\u3057\u3066\u3044\u308b\u3002\n\u3069\u3053\u3067\u3001broadcast\u3067\u914d\u3063\u3066, driver\u3067\u5e73\u5747\u3068\u3063\u3066\u3044\u308b\nshuffle\u3067\u5e73\u5747\u3057\u3066\u3044\u308b\uff1f\n\n\n\u5b66\u7fd2\u4fc2\u6570\u306f\u3069\u3093\u306a\u611f\u3058\uff1f\n\n\nAdaGrad\u3068\u304b\u306b\u66f4\u65b0\u3067\u304d\u305d\u3046\u304b\uff1f\n\n\nBSP\u3068\u306e\u9055\u3044\u306f\uff1f\nSSP\u306b\u3059\u308b\u306b\u306f\u4f55\u304c\u3044\u308b\uff1f\n\u975e\u52b9\u7387\u3063\u3066\u8a00\u308f\u308c\u3066\u3044\u308b\u3051\u3069\u3001\u3088\u3055\u3052\u306b\u3067\u304d\u308b\u304b\uff1f\n\u3044\u308d\u3044\u308dimprovement\u304c\u63d0\u6848\u3001\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3051\u3069\u3001\u305d\u308c\u3063\u3066\u3069\u3046\u306a\u306e\nLR\u3067\u306e SGD, L-BFGS\u306e\u6bd4\u8f03\n\n\n\u524d\u306e\u5b9f\u9a13\u7d50\u679c\u3060\u3068LR-SGD\u306f\u7cbe\u5ea6\u304c\u3067\u3066\u306a\u304b\u3063\u305f\u3002\u5341\u5206\u53cd\u5fa9\u3055\u305b\u3066\u53ce\u675f\u3059\u308b\u304b\u307f\u308b\u3002\u5206\u6563\u74b0\u5883\u3067\n\n\nAdaGrad, ADAM\u3092\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3057\u3066\u307f\u3066\u76ee\u7684\u95a2\u6570\u306e\u4e0b\u304c\u308a\u5177\u5408\u3092\u898b\u3066\u307f\u308b\u3002\u3059\u308b\u306a\u3089\u3001\u4eca\u9031\u672b\u3057\u304b\u306a\u3044\u3002\n\n\u306a\u306e\u3067\u3001\u66f8\u304f\u3053\u3068\u306f\n\n\u73fe\u72b6\u306eSGD\u5b9f\u88c5\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\njira\u30c1\u30b1\u30c3\u30c8\u306e\u95a2\u9023\u8a71\u984c\u306e\u7d39\u4ecb\n\n\n\u7279\u306bSGD\u306e\u52b9\u7387\u5316\u3001\u9ad8\u901f\u5316\u306e\u30c1\u30b1\u30c3\u30c8\n\n\n\nLR\u3067\u306eSGD, L-BFGS\u3068\u306e\u53ce\u675f\u7387\u306e\u6e2c\u5b9a\n\n\u30bd\u30fc\u30b9\u3044\u3058\u3063\u3066\u3001iter\u6bce\u306e\u76ee\u7684\u95a2\u6570\u5024\u3092\u51fa\u529b\u3055\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u3002\n\n\nAdaGrad, ADAM\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0 + \u5b9f\u9a13\n\n\u6700\u5f8c\u307e\u3067\u3044\u3051\u305f\u3089\u826f\u3044\u306a\u3002\u3002\u3002\njira tickect links\n\nhttps://issues.apache.org/jira/browse/SPARK-6346?filter=12332858\nhttps://issues.apache.org/jira/browse/SPARK-1270?filter=12332858\nhttps://issues.apache.org/jira/browse/SPARK-1359?filter=12332858\n\n\u8ad6\u6587 links \n\nhttp://learningsys.org/papers/LearningSys_2015_paper_17.pdf\n\n@DeveloperApi\nclass L1Updater extends Updater {\n  override def compute(\n      weightsOld: Vector,\n      gradient: Vector,\n      stepSize: Double,\n      iter: Int,\n      regParam: Double): (Vector, Double) = {\n    val thisIterStepSize = stepSize / math.sqrt(iter)\u3000\u203b\n    // Take gradient step\n    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n\nthisIterStepSize\u306f\u203b\n\nGradientDescent.scala\n\n/**\n     * For the first iteration, the regVal will be initialized as sum of weight squares\n     * if it's L2 updater; for L1 updater, the same logic is followed.\n     */\n    var regVal = updater.compute(\n      weights, Vectors.zeros(weights.size), 0, 1, regParam)._2\n\n    var converged = false // indicates whether converged based on convergenceTol\n    var i = 1\n    while (!converged && i <= numIterations) {\n      val bcWeights = data.context.broadcast(weights)\n      // Sample a subset (fraction miniBatchFraction) of the total data\n      // compute and sum up the subgradients on this subset (this is one map-reduce)\n      val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))\n            (c._1, c._2 + l, c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      if (miniBatchSize > 0) {\n        /**\n         * NOTE(Xinghao): lossSum is computed using the weights from the previous iteration\n         * and regVal is the regularization value computed in the previous iteration as well.\n         */\n        stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n        val update = updater.compute(\n          weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble),\n          stepSize, i, regParam)\n        weights = update._1\n        regVal = update._2\n\n        previousWeights = currentWeights\n        currentWeights = Some(weights)\n        if (previousWeights != None && currentWeights != None) {\n          converged = isConverged(previousWeights.get,\n            currentWeights.get, convergenceTol)\n        }\n      } else {\n        logWarning(s\"Iteration ($i/$numIterations). The size of sampled batch is zero\")\n      }\n      i += 1\n    }\n\n    logInfo(\"GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses %s\".format(\n      stochasticLossHistory.takeRight(10).mkString(\", \")))\n\n    (weights, stochasticLossHistory.toArray)\n\n  }\n\n\n\nstep size\u306fdefault 1.0,\nstepSize = stepSize / sqrt(iteration)\n\u3067\u6e1b\u8870\n\n/**\n   * Set the initial step size of SGD for the first step. Default 1.0.\n   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n   */\n  def setStepSize(step: Double): this.type = {\n    this.stepSize = step\n    this\n  }\n\nupdater\u306f\n\ngradient, weightvector\u3092\u5f15\u6570\u306b\u3068\u3063\u3066\u3001weightvector\u3092\u66f4\u65b0\u3057\u3066\u3001L1,L2\u306e\u6b63\u5247\u5316\u9805\u3092\u8a08\u7b97\u3059\u308b\u3002\n\ngradient\u306f\n\nexample\u3068current weightvector\u3092\u5f15\u6570\u306b\u3068\u3063\u3066gradient\u3092\u8a08\u7b97\u3057\u3066\u3001Loss\u3092\u8a08\u7b97\u3057\u3066\u8fd4\u3059\u3002\n\ntreeAggregation\u306f\n\npartition\u6bce\u306baggregate\u3059\u308b\u3002\n\u3055\u3089\u306bscale\u500b\u306epartion\u3092aggregate\u3057\u3066\u3001\u65b0\u3057\u3044 numPartition/scale\u306epartiallyAggregated partion\u3092\u4f5c\u308b\n\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3057 scale \uff0b numPartition /scale\u4ee5\u4e0b\u306b\u306a\u3063\u305f\u3089\u3001\u6700\u5f8c\u306b\u4e00\u56deaggregate\u3057\u3066\u3001\u7d50\u679c\u3092\u8fd4\u3059\u3002\n\u76ee\u7684\u306f\u3001\u4e00\u767a\u306eaggregation\u306freducer\u304c\uff11\u3064\u306b\u306a\u3063\u3066\u51e6\u7406\u304c1worker\u306b\u504f\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u306b\u3001tree\u69cb\u9020\u3067\u6bb5\u968e\u7684\u3001\u5206\u6563\u3067\u306baggregation\u3059\u308b\u3002\n\nstochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n\u3067loss\u306e\u5c65\u6b74\u3092\u8ffd\u52a0\u3002minBatchSize\u3067\u5272\u3063\u3066\u3044\u308b\u306e\u306f\u4e00\u5fdc\u5168\u4f53\u306e\u30ed\u30b9\u306b\u3059\u308b\u305f\u3081\uff1f\n\u53ce\u675f\u5224\u5b9a\u306f\n\nloss\u3067\u898b\u3066\u306a\u3044\nweighvector\u306e\u5909\u5316\u304cconvergenceTolelance\u4ee5\u4e0b\u3060\u3063\u305f\u3089\u53ce\u675f\u3057\u3066\u3044\u308b\u5224\u65ad\nrunMiniBatchSGD\u306f\u306fweightvector\u306eloss\u306e\u5909\u5316\u306e\u5c65\u6b74\u3092\u8fd4\u3059\u3002\n\u306a\u306e\u3067\u3001\u7d42\u308f\u3063\u305f\u3042\u3068\u306b\u5168loss\u306e\u5c65\u6b74\u3092\u8868\u793a\u3001\u30d7\u30ed\u30c3\u30c8\u3059\u308c\u3070\u53ce\u675f\u7387\u306f\u3060\u305b\u308b\u3002\n\u3053\u308c\u3092\u4f7f\u3063\u305f\u3001AdaGrad, ADAM\u3067\u53ce\u675f\u7387\u3092\u6bd4\u8f03\u3067\u304d\u308b\ntreeaggregate\u306f reduce\u3068\u540c\u3058\u306a\u306e\u3067\u3001\u6bce\u56de\u3001\u66f4\u65b0\u3057\u305fweightvector\u306fdriver\u306b\u5e30\u3063\u3066\u304d\u3066\u3001\u305d\u308c\u3092\u6bce\u56debroadecast\u3067\u914d\u3063\u3066\u3044\u308b\u3002\ndriver\u5074\u3067\u3001adagrad\u3067weightvector\u306e\u6210\u5206\u3092\u5909\u5316\u3055\u305b\u3066\u3001broadcast\u3067\u914d\u308b\u306e\u306f\u7c21\u5358\nadam\u3082\u540c\u3058\u3060\u308d\u3046\u3051\u3069\u3001\u5fc5\u8981\u306a\u60c5\u5831\u304c\u3061\u3087\u3063\u3068\u5897\u3048\u308b\u3002\n\n\u306a\u3093\u3067\u3042\u308b\u7a0b\u5ea6\u7c21\u6613\u3067\u5b9f\u88c5\u3067\u304d\u308b\u304c\u3001GradientDescent.scala\u3067\u306a\u304f\u3066\n\nAdaGrad.scala\u3068\u304bADAM.scala\u3068\u304b\u4f5c\u3063\u3066\u51e6\u7406\u3055\u305b\u308b\u306e\u304c\u826f\u3055\u305d\u3046\n\n\n\n\u3092\u3053\u3053\u306b\u66f8\u3044\u3066\u3044\u304f\n\n- \u5206\u6563\u3067\u3069\u3046\u3084\u3063\u3066gradient update\u3084\u3063\u3066\u3044\u308b\u304b\n - worker\u306e\u7d50\u679c\u3092\u5e73\u5747\u3057\u3066\u3044\u308b\u3002\n - \u3069\u3053\u3067\u3001broadcast\u3067\u914d\u3063\u3066, driver\u3067\u5e73\u5747\u3068\u3063\u3066\u3044\u308b\n - shuffle\u3067\u5e73\u5747\u3057\u3066\u3044\u308b\uff1f\n- \u5b66\u7fd2\u4fc2\u6570\u306f\u3069\u3093\u306a\u611f\u3058\uff1f\n - AdaGrad\u3068\u304b\u306b\u66f4\u65b0\u3067\u304d\u305d\u3046\u304b\uff1f\n- BSP\u3068\u306e\u9055\u3044\u306f\uff1f\n- SSP\u306b\u3059\u308b\u306b\u306f\u4f55\u304c\u3044\u308b\uff1f\n- \u975e\u52b9\u7387\u3063\u3066\u8a00\u308f\u308c\u3066\u3044\u308b\u3051\u3069\u3001\u3088\u3055\u3052\u306b\u3067\u304d\u308b\u304b\uff1f\n- \u3044\u308d\u3044\u308dimprovement\u304c\u63d0\u6848\u3001\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3051\u3069\u3001\u305d\u308c\u3063\u3066\u3069\u3046\u306a\u306e\n- LR\u3067\u306e SGD, L-BFGS\u306e\u6bd4\u8f03\n - \u524d\u306e\u5b9f\u9a13\u7d50\u679c\u3060\u3068LR-SGD\u306f\u7cbe\u5ea6\u304c\u3067\u3066\u306a\u304b\u3063\u305f\u3002\u5341\u5206\u53cd\u5fa9\u3055\u305b\u3066\u53ce\u675f\u3059\u308b\u304b\u307f\u308b\u3002\u5206\u6563\u74b0\u5883\u3067\n- AdaGrad, ADAM\u3092\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3057\u3066\u307f\u3066\u76ee\u7684\u95a2\u6570\u306e\u4e0b\u304c\u308a\u5177\u5408\u3092\u898b\u3066\u307f\u308b\u3002\u3059\u308b\u306a\u3089\u3001\u4eca\u9031\u672b\u3057\u304b\u306a\u3044\u3002\n\n\n\u306a\u306e\u3067\u3001\u66f8\u304f\u3053\u3068\u306f\n\n- \u73fe\u72b6\u306eSGD\u5b9f\u88c5\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\n- jira\u30c1\u30b1\u30c3\u30c8\u306e\u95a2\u9023\u8a71\u984c\u306e\u7d39\u4ecb\n - \u7279\u306bSGD\u306e\u52b9\u7387\u5316\u3001\u9ad8\u901f\u5316\u306e\u30c1\u30b1\u30c3\u30c8\n- LR\u3067\u306eSGD, L-BFGS\u3068\u306e\u53ce\u675f\u7387\u306e\u6e2c\u5b9a\n - \u30bd\u30fc\u30b9\u3044\u3058\u3063\u3066\u3001iter\u6bce\u306e\u76ee\u7684\u95a2\u6570\u5024\u3092\u51fa\u529b\u3055\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u3002\n\n- AdaGrad, ADAM\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0 + \u5b9f\u9a13\n\n\u6700\u5f8c\u307e\u3067\u3044\u3051\u305f\u3089\u826f\u3044\u306a\u3002\u3002\u3002\n\n\njira tickect links\n\n- https://issues.apache.org/jira/browse/SPARK-6346?filter=12332858\n\n- https://issues.apache.org/jira/browse/SPARK-1270?filter=12332858\n\n- https://issues.apache.org/jira/browse/SPARK-1359?filter=12332858\n\n\u8ad6\u6587 links \n\n- http://learningsys.org/papers/LearningSys_2015_paper_17.pdf\n\n```\n@DeveloperApi\nclass L1Updater extends Updater {\n  override def compute(\n      weightsOld: Vector,\n      gradient: Vector,\n      stepSize: Double,\n      iter: Int,\n      regParam: Double): (Vector, Double) = {\n    val thisIterStepSize = stepSize / math.sqrt(iter)\u3000\u203b\n    // Take gradient step\n    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n```\n\nthisIterStepSize\u306f\u203b\n\n```file:GradientDescent.scala\n\n/**\n     * For the first iteration, the regVal will be initialized as sum of weight squares\n     * if it's L2 updater; for L1 updater, the same logic is followed.\n     */\n    var regVal = updater.compute(\n      weights, Vectors.zeros(weights.size), 0, 1, regParam)._2\n\n    var converged = false // indicates whether converged based on convergenceTol\n    var i = 1\n    while (!converged && i <= numIterations) {\n      val bcWeights = data.context.broadcast(weights)\n      // Sample a subset (fraction miniBatchFraction) of the total data\n      // compute and sum up the subgradients on this subset (this is one map-reduce)\n      val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))\n            (c._1, c._2 + l, c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      if (miniBatchSize > 0) {\n        /**\n         * NOTE(Xinghao): lossSum is computed using the weights from the previous iteration\n         * and regVal is the regularization value computed in the previous iteration as well.\n         */\n        stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n        val update = updater.compute(\n          weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble),\n          stepSize, i, regParam)\n        weights = update._1\n        regVal = update._2\n\n        previousWeights = currentWeights\n        currentWeights = Some(weights)\n        if (previousWeights != None && currentWeights != None) {\n          converged = isConverged(previousWeights.get,\n            currentWeights.get, convergenceTol)\n        }\n      } else {\n        logWarning(s\"Iteration ($i/$numIterations). The size of sampled batch is zero\")\n      }\n      i += 1\n    }\n\n    logInfo(\"GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses %s\".format(\n      stochasticLossHistory.takeRight(10).mkString(\", \")))\n\n    (weights, stochasticLossHistory.toArray)\n\n  }\n\n```\n\nstep size\u306fdefault 1.0,\n\nstepSize = stepSize / sqrt(iteration)\n\n\u3067\u6e1b\u8870\n```\n/**\n   * Set the initial step size of SGD for the first step. Default 1.0.\n   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n   */\n  def setStepSize(step: Double): this.type = {\n    this.stepSize = step\n    this\n  }\n```\n\n\nupdater\u306f\n\n- gradient, weightvector\u3092\u5f15\u6570\u306b\u3068\u3063\u3066\u3001weightvector\u3092\u66f4\u65b0\u3057\u3066\u3001L1,L2\u306e\u6b63\u5247\u5316\u9805\u3092\u8a08\u7b97\u3059\u308b\u3002\n\ngradient\u306f\n\n- example\u3068current weightvector\u3092\u5f15\u6570\u306b\u3068\u3063\u3066gradient\u3092\u8a08\u7b97\u3057\u3066\u3001Loss\u3092\u8a08\u7b97\u3057\u3066\u8fd4\u3059\u3002\n\n\ntreeAggregation\u306f\n\n- partition\u6bce\u306baggregate\u3059\u308b\u3002\n- \u3055\u3089\u306bscale\u500b\u306epartion\u3092aggregate\u3057\u3066\u3001\u65b0\u3057\u3044 numPartition/scale\u306epartiallyAggregated partion\u3092\u4f5c\u308b\n- \u3053\u308c\u3092\u7e70\u308a\u8fd4\u3057 scale \uff0b numPartition /scale\u4ee5\u4e0b\u306b\u306a\u3063\u305f\u3089\u3001\u6700\u5f8c\u306b\u4e00\u56deaggregate\u3057\u3066\u3001\u7d50\u679c\u3092\u8fd4\u3059\u3002\n- \u76ee\u7684\u306f\u3001\u4e00\u767a\u306eaggregation\u306freducer\u304c\uff11\u3064\u306b\u306a\u3063\u3066\u51e6\u7406\u304c1worker\u306b\u504f\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u306b\u3001tree\u69cb\u9020\u3067\u6bb5\u968e\u7684\u3001\u5206\u6563\u3067\u306baggregation\u3059\u308b\u3002\n\n\nstochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n\n\u3067loss\u306e\u5c65\u6b74\u3092\u8ffd\u52a0\u3002minBatchSize\u3067\u5272\u3063\u3066\u3044\u308b\u306e\u306f\u4e00\u5fdc\u5168\u4f53\u306e\u30ed\u30b9\u306b\u3059\u308b\u305f\u3081\uff1f\n\n\u53ce\u675f\u5224\u5b9a\u306f\n\n- loss\u3067\u898b\u3066\u306a\u3044\n- weighvector\u306e\u5909\u5316\u304cconvergenceTolelance\u4ee5\u4e0b\u3060\u3063\u305f\u3089\u53ce\u675f\u3057\u3066\u3044\u308b\u5224\u65ad\n\n\n- runMiniBatchSGD\u306f\u306fweightvector\u306eloss\u306e\u5909\u5316\u306e\u5c65\u6b74\u3092\u8fd4\u3059\u3002\n- \u306a\u306e\u3067\u3001\u7d42\u308f\u3063\u305f\u3042\u3068\u306b\u5168loss\u306e\u5c65\u6b74\u3092\u8868\u793a\u3001\u30d7\u30ed\u30c3\u30c8\u3059\u308c\u3070\u53ce\u675f\u7387\u306f\u3060\u305b\u308b\u3002\n- \u3053\u308c\u3092\u4f7f\u3063\u305f\u3001AdaGrad, ADAM\u3067\u53ce\u675f\u7387\u3092\u6bd4\u8f03\u3067\u304d\u308b\n\n\n- treeaggregate\u306f reduce\u3068\u540c\u3058\u306a\u306e\u3067\u3001\u6bce\u56de\u3001\u66f4\u65b0\u3057\u305fweightvector\u306fdriver\u306b\u5e30\u3063\u3066\u304d\u3066\u3001\u305d\u308c\u3092\u6bce\u56debroadecast\u3067\u914d\u3063\u3066\u3044\u308b\u3002\n- driver\u5074\u3067\u3001adagrad\u3067weightvector\u306e\u6210\u5206\u3092\u5909\u5316\u3055\u305b\u3066\u3001broadcast\u3067\u914d\u308b\u306e\u306f\u7c21\u5358\n- adam\u3082\u540c\u3058\u3060\u308d\u3046\u3051\u3069\u3001\u5fc5\u8981\u306a\u60c5\u5831\u304c\u3061\u3087\u3063\u3068\u5897\u3048\u308b\u3002\n- \u306a\u3093\u3067\u3042\u308b\u7a0b\u5ea6\u7c21\u6613\u3067\u5b9f\u88c5\u3067\u304d\u308b\u304c\u3001GradientDescent.scala\u3067\u306a\u304f\u3066\n - AdaGrad.scala\u3068\u304bADAM.scala\u3068\u304b\u4f5c\u3063\u3066\u51e6\u7406\u3055\u305b\u308b\u306e\u304c\u826f\u3055\u305d\u3046\n", "tags": ["Spark", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2"]}