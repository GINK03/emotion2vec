{"context": "\n\n\u304d\u3063\u304b\u3051\nAutoencoder\uff08\u81ea\u5df1\u7b26\u53f7\u5316\u5668\uff09\u306f\u4ed6\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306b\u6bd4\u3079\u308b\u3068\u3084\u3084\u5730\u5473\u306a\u5b58\u5728\u3067\u3042\u308b\uff0e\u6587\u732e\u300c\u6df1\u5c64\u5b66\u7fd2\u300d\uff08\u5ca1\u8c37\u6c0f\u8457\uff0c\u8b1b\u8ac7\u793e\uff09\u3067\u306f\u7b2c\uff15\u7ae0\u306b\u767b\u5834\u3059\u308b\u304c\uff0c\n\n\u81ea\u5df1\u7b26\u53f7\u5316\u5668\u3068\u306f\uff0c\u76ee\u6a19\u51fa\u529b\u3092\u4f34\u308f\u306a\u3044\uff0c\u5165\u529b\u3060\u3051\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u305f\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306b\u3088\u308a\uff0c\u30c7\u30fc\u30bf\u3092\u3088\u304f\u8868\u3059\u7279\u5fb4\u3092\u7372\u5f97\u3057\uff0c\u3072\u3044\u3066\u306f\u30c7\u30fc\u30bf\u306e\u3088\u3044\u8868\u73fe\u65b9\u6cd5\u3092\u5f97\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3059\uff0e\u30c7\u30a3\u30fc\u30d7\u30cd\u30c3\u30c8\u306e\u4e8b\u524d\u5b66\u7fd2\uff0c\u3059\u306a\u308f\u3061\u305d\u306e\u91cd\u307f\u306e\u3088\u3044\u521d\u671f\u5024\u3092\u5f97\u308b\u76ee\u7684\u306b\u3082\u5229\u7528\u3055\u308c\u307e\u3059\uff0e\n\n\u3068\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\uff0e\u300c\u4e8b\u524d\u5b66\u7fd2\u300d\u3092\u5b9f\u65bd\u3059\u308b\u6a5f\u4f1a\u306f\u3042\u307e\u308a\u306a\u3055\u305d\u3046\uff0c\u3068\u3044\u3046\u3053\u3068\u3067\u3053\u306e\u7ae0\u306f\u76ee\u3092\u901a\u3059\u306b\u3068\u3069\u3081\uff0c\u6b21\u306e\u7ae0\uff0c\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8(CNN)\u3084\u518d\u5e30\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8(RNN)\u306e\u7ae0\u306b\u9032\u3080\u65b9\u3082\u591a\u3044\u3068\u601d\u308f\u308c\u308b\uff0e\n\u305f\u3060\u3088\u304f\u8abf\u3079\u308b\u3068\uff0c\u4e8b\u524d\u5b66\u7fd2\u306e\u4ed6\u306b\u3082Autoencoder\u306e\u4f7f\u7528\u76ee\u7684\u3068\u3057\u3066\u4ee5\u4e0b\u304c\u3042\u308b\u3088\u3046\u3060\uff0e\n\n\u30c7\u30fc\u30bf\u5727\u7e2e\uff0e\n\u30c7\u30fc\u30bf\u30ce\u30a4\u30ba\u9664\u53bb\uff0e\uff08Denoising Autoencoder\uff09\n\u753b\u50cf\u306e\u533a\u753b\u5206\u5272\uff08Semantic Segmentation - \u753b\u50cf\u306e\u4e2d\u8eab\u3092\u8a8d\u8b58\u3057\u305f\u4e0a\u3067\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\uff09\n\n\u73fe\u5728\uff0cKaggle\u3067\u306f\u533b\u7642\u753b\u50cf\u306b\u95a2\u3059\u308b\u30b3\u30f3\u30da(Ultrasound Nerve)\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u304c\uff0cAutoencoder\u3092\u4f7f\u3063\u305f\u624b\u6cd5\u304c\u672c\u547d\u306b\u306a\u308b\u3068\u30d5\u30a9\u30fc\u30e9\u30e0\u3067\u767a\u8a00\u3055\u308c\u3066\u3044\u305f\uff0e\u305d\u308c\u306a\u3089\u3070\u3068\u3044\u3046\u3053\u3068\u3067\uff0c\u52c9\u5f37\u306e\u305f\u3081\uff0cMNIST\uff08\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\uff09\u3092\u984c\u6750\u306bAutoencoder\u3092\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u305f\uff0e\n\nKeras\u306eAutoencoder\u306b\u95a2\u3059\u308b\u8a18\u4e8b\n\u30c4\u30fc\u30eb\u3068\u3057\u3066TensowFlow\u3092\u8003\u3048\u305f\u304c\uff0c\u6b8b\u5ff5\u306a\u304c\u3089TensorFlow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\uff0c\u7279\u306bTutorial\u306b\u306fAutoencoder\u306f\u306a\u3044\uff0e\u5225\u306eDeep Learning\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\uff0cKeras\u306b\u30d6\u30ed\u30b0\u8a18\u4e8b\u3068\u3057\u3066Autoencoder\u304c\u53d6\u308a\u4e0a\u3052\u3089\u308c\u3066\u304a\u308a\uff0c\u305d\u308c\u304c\u975e\u5e38\u306b\u53c2\u8003\u306b\u306a\u3063\u305f\uff0e\nBuilding Autoencoders in Keras : http://blog.keras.io/building-autoencoders-in-keras.html\n\u3053\u306e\u4e2d\u306b\u306f\uff0c\u3055\u307e\u3056\u307e\u306a\u7a2e\u985e\u306eAutoencoder\u306b\u3064\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u304c\u3055\u308c\u3066\u3044\u308b\uff0e\u307e\u305a\uff0c\u3053\u308c\u3092\u5199\u7d4c\u3057\u3066\u7406\u89e3\u3092\u6df1\u3081\uff0c\u6b21\u306b\uff0c\u3053\u306e\u4e2d\u304b\u3089\u6b21\u306e\u4e8c\u3064\u3092TensorFlow\u306b\u79fb\u690d\u3057\u3066\u307f\u305f\uff0e\n\n\u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306aAutoencoder\n\u7573\u8fbc\u307fAutoencoder\n\n\u7279\u306b\u300c\u7573\u8fbc\u307fAutoencoder\u300d\u306e\u65b9\u3067\u306f\uff0c\u901a\u5e38\u306e\u7573\u8fbc\u307f\u4e88\u6e2c\u30e2\u30c7\u30eb(CNN)\u3067\u884c\u3046\uff0c\u300c\u7573\u8fbc\u307f\u300d-> \u300c\u30d7\u30fc\u30ea\u30f3\u30b0\u300d\u306e\uff0c\uff08\u753b\u50cf\u306e\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u5897\u3084\u3057\u306a\u304c\u3089\uff09\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5c0f\u3055\u304f\u3059\u308b\u51e6\u7406\u306e\u9006\uff0c\uff08\u753b\u50cf\u306e\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u306a\u304c\u3089\uff09\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5927\u304d\u304f\u3057\u3066\u3044\u304f\u51e6\u7406\u304cAutoencoder\u3067\u5fc5\u8981\u3068\u306a\u308b\u306e\u3067\uff0c\u3053\u3053\u306e\u3068\u3053\u308d\u3067\u3044\u308d\u3044\u308d\u60a9\u3080\u3053\u3068\u3068\u306a\u3063\u305f\uff0e\n\uff08\u4ee5\u4e0b\uff0cTensorFlow\u306b\u3088\u308b\u5b9f\u88c5\u306e\u8a71\u306b\u306a\u308a\u307e\u3059\u304c\uff0c\u4e0a\u8a18\u306eKeras\u30d6\u30ed\u30b0\u8a18\u4e8b\u306b\u306fAutoencoder\u306b\u95a2\u3059\u308b\u5185\u5bb9\u306e\u307f\u306a\u3089\u305a\uff0cKeras\u3067TensorBoard\u3092\u4f7f\u3046\u65b9\u6cd5\u7b49\uff0c\u3068\u3066\u3082\u305f\u3081\u306b\u306a\u308b\u60c5\u5831\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u305c\u3072\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\uff09\n\uff08\u672c\u8a18\u4e8b\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u74b0\u5883\u306f\uff0cPython 2.7.11, Keras 1.0.5, TensorFlow 0.9.0rc0 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n\u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306aAutoencoder\nKeras\u306e\u30d6\u30ed\u30b0\u8a18\u4e8b\u3067\u3082\u6700\u521d\u306b\u767b\u5834\u3059\u308b\uff0e\u5165\u529b\u304b\u3089\u91cd\u307fw\uff0c\u30d0\u30a4\u30a2\u30b9b\u3092\u4f7f\u3063\u3066\u4e2d\u9593\u5c64\u3092\u751f\u6210\u3057\uff0c\u305d\u3053\u304b\u3089\u307e\u305f\u5225\u306e\u91cd\u307fw'\uff0c\u30d0\u30a4\u30a2\u30b9b'\u3092\u4f7f\u3063\u3066\u5165\u529b\u30c7\u30fc\u30bf\u3092\u518d\u751f\u6210\u3059\u308b\u3068\u3044\u3046\u30e2\u30c7\u30eb\uff0eKeras\u3067\u306e\u30e2\u30c7\u30eb\u90e8\u5206\u306e\u30b3\u30fc\u30c9\u306f\uff0c\u4ee5\u4e0b\u306e\u901a\u308a\uff0e\n# this is our input placeholder\ninput_img = Input(shape=(784,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(784, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input=input_img, output=decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input=input_img, output=encoded)\n\n# create a placeholder for an encoded (32-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n\nKeras\uff08\u7279\u306bSequential model API\uff09\u3067\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5165\u529b\u5074\u304b\u3089\u51fa\u529b\u5074\u306b\u5411\u3051\u3066\u30e2\u30c7\u30eb\u3092\u8a18\u8ff0\u3057\u3066\u3044\u304f\uff0e\u4e0a\u8a18\u306f\uff0c\u5165\u529b\u304b\u3089\u30d5\u30eb\u7d50\u5408\u5c64\uff08Dense())\u3067\u4e2d\u9593\u5024\u3092\u6c42\u3081\uff0c\u305d\u3053\u304b\u3089\u3059\u3050\u306b\u6298\u308a\u8fd4\u3057\u3066\u30d5\u30eb\u7d50\u5408\u5c64\u3092\u7d4c\u3066\u30c7\u30b3\u30fc\u30c9\u5024\u3092\u6c42\u3081\u3066\u3044\u308b\uff0e\n\u3053\u308c\u3092TensorFlow\u306b\u76f4\u3059\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\uff0e\n# Variables\nx = tf.placeholder(\"float\", [None, 784])\ny_ = tf.placeholder(\"float\", [None, 10])\n\nw_enc = tf.Variable(tf.random_normal([784, 625], mean=0.0, stddev=0.05))\nw_dec = tf.Variable(tf.random_normal([625, 784], mean=0.0, stddev=0.05))\n# w_dec = tf.transpose(w_enc) # if you use tied weights\nb_enc = tf.Variable(tf.zeros([625]))\nb_dec = tf.Variable(tf.zeros([784]))\n\n# Create the model\ndef model(X, w_e, b_e, w_d, b_d):\n    encoded = tf.sigmoid(tf.matmul(X, w_e) + b_e)\n    decoded = tf.sigmoid(tf.matmul(encoded, w_d) + b_d)\n\n    return encoded, decoded\n\nencoded, decoded = model(x, w_enc, b_enc, w_dec, b_dec)\n\n\u4e0a\u8a18\u306e\u901a\u308a\uff0c\u91cd\u307fw\u306e\u5f62\u306f\uff0cencode\u5074\u3068decode\u5074\u3067\u8ee2\u7f6e\u3055\u305b\u305f\u95a2\u4fc2\u3068\u306a\u308b\uff0e\u4e0a\u306e\u30b3\u30fc\u30c9\u3067\u306f\uff0cencode\u5074\u3068decode\u5074\u3067\u5225\u306e\u91cd\u307f\u3092\u8a2d\u5b9a\u3057\u305f\u304c\uff0c\u91cd\u307f\u5171\u6709\u3092\u884c\u3046\u5834\u5408\u306f\uff0ctf.transpose() \u3067\u5909\u6570\u3092\u8ee2\u7f6e\u3055\u305b\u308b\uff0e\uff08\u30b3\u30e1\u30f3\u30c8\u7b87\u6240\uff09\u91cd\u307f\u5171\u6709\u3092\u7528\u3044\u308b/\u7528\u3044\u306a\u3044\u306b\u95a2\u3057\u3066\u306f\uff0c\u660e\u78ba\u306a\u30eb\u30fc\u30eb\u306f\u306a\u3044\u3068\u306e\u3053\u3068\u306a\u306e\u3067\uff0c\u30b1\u30fc\u30b9\u3054\u3068\u306b\u4f7f\u3044\u5206\u3051\u308b\u3053\u3068\u306b\u306a\u308b\uff0e\n\u30e2\u30c7\u30eb\u304c\u30b7\u30f3\u30d7\u30eb\u306a\u306e\u3067\uff0c\u3059\u3050\u306b\u52d5\u4f5c\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u305f\uff0e\u8a08\u7b97\u7d50\u679c\u306f\uff0c\u6b21\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\nTraining...\n  step, loss =      0:  0.721\n  step, loss =   1000:  0.262\n  step, loss =   2000:  0.243\n  step, loss =   3000:  0.237\n  step, loss =   4000:  0.229\n  step, loss =   5000:  0.209\n  step, loss =   6000:  0.212\n  step, loss =   7000:  0.200\n  step, loss =   8000:  0.196\n  step, loss =   9000:  0.178\n  step, loss =  10000:  0.189\nloss (test) =  0.180014\n\nFig. \u5165\u529b\u753b\u50cf\u3068\u5fa9\u5143\u3055\u308c\u305f\u753b\u50cf\uff08\u30b7\u30f3\u30d7\u30ebAutoencoder\uff09\n\n\n\u7573\u8fbc\u307fAutoencoder\uff08Convolutional\u306aAutoencoder\uff09\n\u7573\u8fbc\u307fAutoencoder\u304c\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304b\uff0cKeras\u306e\u30d6\u30ed\u30b0\u304b\u3089\u5f15\u7528\u3059\u308b\uff0e\n\nSince our inputs are images, it makes sense to use convolutional neural networks (convnets) \nas encoders and decoders. In practical settings, autoencoders applied to images are always \nconvolutional autoencoders --they simply perform much better.\n\n\u753b\u50cf\u3092\u6271\u3046\u306b\u3042\u305f\u308aConvolution\u51e6\u7406\u306f\u6709\u52b9\u3067\uff0cAutoencoder\u3082\u5358\u7d14\u306b\u9ad8\u6a5f\u80fd\u306b\u306a\u308b\u3068\u306e\u3053\u3068\uff0e\u305d\u3053\u3067\u307e\u305aKeras\u3067\u306e\u5b9f\u88c5\u3092\u53c2\u8003\u306b\u3059\u308b\uff0e\ninput_img = Input(shape=(1, 28, 28))\n\nx = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(input_img)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nencoded = MaxPooling2D((2, 2), border_mode='same')(x)\n\n# at this point the representation is (8, 4, 4) i.e. 128-dimensional\n\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(16, 3, 3, activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Convolution2D(1, 3, 3, activation='sigmoid', border_mode='same')(x)\n\nautoencoder = Model(input_img, decoded)\n\n\n\u3055\u3059\u304c\u306bKeras\u306e\u30b3\u30fc\u30c9\u306f\u8aad\u307f\u3084\u3059\u3044\uff0e\n3\u7d44\u306e\u7573\u8fbc\u307f\u5c64\u3068MaxPooing\u5c64\u304b\u3089\u306a\u308bencode\u30d7\u30ed\u30bb\u30b9\u3068\uff0c\u305d\u306e\u5f8c\u6298\u308a\u8fd4\u3057\u3066\uff0c\u753b\u50cf\u3092\u5fa9\u5143\u3059\u308bdecode\u30d7\u30ed\u30bb\u30b9\u3067\u3053\u306eAutoencoder\u306f\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\uff0eendcode\u306e\u90e8\u5206\u306f\uff0c\u753b\u50cf\u5206\u985e\u306a\u3069\u3067\u7528\u3044\u308b\u901a\u5e38\u306eCNN\u5206\u985e\u5668\u306e\u3082\u306e\u3068\u540c\u3058\u3067\u3042\u308b\u304c\uff0cAutoencoder\u306b\u7279\u6709\u306a\u306e\u306f\u5f8c\u534a\u90e8\u5206\u3067\u3042\u308b\uff0eKeras\u30b3\u30fc\u30c9\u3067\u306f\uff0cConvolution2D()\u3068UpSampling2D()\u306e\u95a2\u6570\u304c\u7528\u3044\u3089\u308c\u3066\u3044\u308b\u304c\uff0c\u3053\u306eUpsampling2D()\u306b\u5bfe\u3057\u3066\uff0cTensorFlow\u306b\u540c\u69d8\u306e\u3082\u306e\u306f\u306a\u3044\uff0e\n\u3053\u308c\u306b\u4ee3\u308f\u308b\u306e\u304c \"tf.nn.conv2d_transpose()\" \u3067\u3042\u308b\uff0e\u3053\u308c\u3092\u4f7f\u3046\u306b\u3042\u305f\u308a\u6ce8\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u306e\u304c\uff0cKeras\u306e UpSampling2D() \u3092\uff11\u5bfe\uff11\u306b\u7f6e\u304d\u63db\u3048\u308b\u306e\u3067\u306f\u306a\u304f\uff0cConvolutional2D() + UpSampling2D() \u306e\u6a5f\u80fd\u3092 \"tf.nn.conv2d_transpose\" \u304c\u6301\u3063\u3066\u3044\u308b\u70b9\u3067\u3042\u308b\uff0e\u3053\u3053\u306b\u6ce8\u610f\u3057\uff0cTensorFlow\u3067\u5b9f\u88c5\u3057\u305f\u306e\u304c\u4ee5\u4e0b\u3067\u3042\u308b\uff0e\nfrom my_nn_lib import Convolution2D, MaxPooling2D\n\n# Conv 2-D transpose Layer (deconvolutoinal Layer)\nclass Conv2Dtranspose(object):\n    '''\n      constructor's args:\n          input      : input image (2D matrix)\n          output_siz : output image size\n          in_ch      : number of incoming image channel\n          out_ch     : number of outgoing image channel\n          patch_siz  : filter(patch) size\n    '''\n    def __init__(self, input, output_siz, in_ch, out_ch, patch_siz, activation='relu'):\n        self.input = input      \n        self.rows = output_siz[0]\n        self.cols = output_siz[1]\n        self.out_ch = out_ch\n        self.activation = activation\n\n        wshape = [patch_siz[0], patch_siz[1], out_ch, in_ch]    # note the arguments order\n\n        w_cvt = tf.Variable(tf.truncated_normal(wshape, stddev=0.1), \n                            trainable=True)\n        b_cvt = tf.Variable(tf.constant(0.1, shape=[out_ch]), \n                            trainable=True)\n        self.batsiz = tf.shape(input)[0]\n        self.w = w_cvt\n        self.b = b_cvt\n        self.params = [self.w, self.b]\n\n    def output(self):\n        shape4D = [self.batsiz, self.rows, self.cols, self.out_ch]      \n        linout = tf.nn.conv2d_transpose(self.input, self.w, output_shape=shape4D,\n                            strides=[1, 2, 2, 1], padding='SAME') + self.b\n        if self.activation == 'relu':\n            self.output = tf.nn.relu(linout)\n        elif self.activation == 'sigmoid':\n            self.output = tf.sigmoid(linout)\n        else:\n            self.output = linout\n\n        return self.output\n\ndef mk_nn_model(x, y_):\n    # Encoding phase\n    x_image = tf.reshape(x, [-1, 28, 28, 1])    \n    conv1 = Convolution2D(x_image, (28, 28), 1, 16, \n                          (3, 3), activation='relu')\n    conv1_out = conv1.output()\n\n    pool1 = MaxPooling2D(conv1_out)\n    pool1_out = pool1.output()\n\n    conv2 = Convolution2D(pool1_out, (14, 14), 16, 8, \n                          (3, 3), activation='relu')\n    conv2_out = conv2.output()\n\n    pool2 = MaxPooling2D(conv2_out)\n    pool2_out = pool2.output()\n\n    conv3 = Convolution2D(pool2_out, (7, 7), 8, 8, (3, 3), activation='relu')\n    conv3_out = conv3.output()\n\n    pool3 = MaxPooling2D(conv3_out)\n    pool3_out = pool3.output()\n    # at this point the representation is (8, 4, 4) i.e. 128-dimensional\n    # Decoding phase\n    conv_t1 = Conv2Dtranspose(pool3_out, (7, 7), 8, 8,\n                         (3, 3), activation='relu')\n    conv_t1_out = conv_t1.output()\n\n    conv_t2 = Conv2Dtranspose(conv_t1_out, (14, 14), 8, 8,\n                         (3, 3), activation='relu')\n    conv_t2_out = conv_t2.output()\n\n    conv_t3 = Conv2Dtranspose(conv_t2_out, (28, 28), 8, 16, \n                         (3, 3), activation='relu')\n    conv_t3_out = conv_t3.output()\n\n    conv_last = Convolution2D(conv_t3_out, (28, 28), 16, 1, (3, 3),\n                         activation='sigmoid')\n    decoded = conv_last.output()\n\n    decoded = tf.reshape(decoded, [-1, 784])\n    cross_entropy = -1. *x *tf.log(decoded) - (1. - x) *tf.log(1. - decoded)\n    loss = tf.reduce_mean(cross_entropy)\n\n    return loss, decoded\n\n\nTensorFlow\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u95a2\u4fc2API\u3092\u4f7f\u3044\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\uff0c\u30af\u30e9\u30b9\"Convolution2D\"\u3068\"MaxPooling2D\"\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\uff0e\uff08 \"tf.nn.conv2d()\"\u3068\"tf.nn.max_pool()\u306ewrapper\u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\u4eca\u56de\uff0c\u3053\u308c\u306b\u52a0\u3048\u3066\uff0ctf.nn.conv2d_transpose()\u3092\u5305\u3080\"Conv2Dtranspose\"\u30af\u30e9\u30b9\u3092\u5b9a\u7fa9\u3057\uff0c\u305d\u308c\u3092\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u95a2\u6570\u3067\u4f7f\u7528\u3057\u3066\u3044\u308b\uff0e\n\u3053\u308c\u3089\u306e\u30af\u30e9\u30b9\u4f7f\u7528\u306b\u3042\u305f\u308a\uff0c\uff08\u30af\u30e9\u30b9\u3067\u5f15\u6570\u3092\u5c11\u306a\u304f\u3057\u3088\u3046\u3068\u306f\u3057\u3066\u3044\u308b\u304c\uff0c\uff09\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u5f15\u6570\u304c\u591a\u304f\uff0c\u7279\u306bdimension\u95a2\u4fc2\u306f\uff0c\u9069\u5207\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u3052\u306a\u3044\u3068\u30a8\u30e9\u30fc\u306b\u3064\u306a\u304c\u308b\uff0e\uff08tf.nn.conv2d_transpose()\u306f\uff0ctf.nn.conv2d() \u3068\u540c\u3058\u3088\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3092\u3057\u3066\u9593\u9055\u3046\u30b1\u30fc\u30b9\u304c\u591a\u3044\u3088\u3046\u3067\uff0cstack overflow\u7b49\u3067Bug Fix\u306e\u30a2\u30c9\u30d0\u30a4\u30b9\u3092\u53d7\u3051\u3066\u3044\u308b\u8a18\u4e8b\u304c\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3057\u305f\uff0e\uff09\n\u3053\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u305f\u72b6\u6cc1\u306f\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\nTraining...\n  step, loss =      0:  0.616\n  step, loss =   1000:  0.158\n  step, loss =   2000:  0.132\n  step, loss =   3000:  0.125\n  step, loss =   4000:  0.116\n  step, loss =   5000:  0.113\n  step, loss =   6000:  0.108\n  step, loss =   7000:  0.116\n  step, loss =   8000:  0.114\n  step, loss =   9000:  0.109\n  step, loss =  10000:  0.108\nloss (test) =  0.106951\n\nFig. \u5165\u529b\u753b\u50cf\u3068\u5fa9\u5143\u3055\u308c\u305f\u753b\u50cf\uff08Convolutional Autoencoder\uff09\n\n\u4e0a\u306e\u300c\u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306a\u300d\u306e\u7d50\u679c\u753b\u50cf\u3068\u6bd4\u8f03\u3059\u308b\u3068\uff0c\u540c\u3058\u5b66\u7fd2\u56de\u6570\u3067\u3088\u308a\u660e\u77ad\u306a\u753b\u50cf\u304c\u5fa9\u5143\u3067\u304d\u3066\u3044\u308b\uff0e\n\u521d\u3081\u306fAutoencoder\u3092\u907f\u3051\u3066\u3044\u305f\u304c\uff0c\u3053\u3046\u3057\u3066\u5b9f\u969b\u52d5\u304b\u3057\u3066\u307f\u308b\u3068\u306a\u304b\u306a\u304b\u8208\u5473\u6df1\u3044\u52d5\u4f5c\u3092\u3059\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\uff0e\u307e\u305f\uff0c\u4eca\u56de\u306f\u52c9\u5f37\u306e\u305f\u3081\u306bTensorFlow\u3067\u306e\u5b9f\u88c5\u3092\u884c\u3063\u305f\u304c\uff0c\u6539\u3081\u3066Keras\u306e\uff08\u77ed\u3044\u30b3\u30fc\u30c9\u3067\u304b\u3051\u308b\uff09\u6709\u7528\u3055\u3092\u5b9f\u611f\u3067\u304d\u305f\uff0e\n\uff08\u672c\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u306f\uff0cGist : https://gist.github.com/tomokishii/7ddde510edb1c4273438ba0663b26fc6 \u306b\u8f09\u305b\u3066\u3044\u307e\u3059\uff0e\uff09\n\n\u53c2\u8003\u6587\u732e / web site\n\nBuilding Autoencoders in Keras - Keras Blog\nhttp://blog.keras.io/building-autoencoders-in-keras.html\n\nKeras\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 http://keras.io/\n\nTensorFlow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 https://www.tensorflow.org/versions/r0.9/api_docs/index.html\n\n\u6df1\u5c64\u5b66\u7fd2\uff08\u5ca1\u8c37\u6c0f\u8457\uff0c\u8b1b\u8ac7\u793e \u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba\uff09\nTensorFlow\u3067\u6a5f\u68b0\u5b66\u7fd2\u3068\u622f\u308c\u308b: AutoEncoder\u3092\u4f5c\u3063\u3066\u307f\u308b - Qiita \nhttp://qiita.com/mokemokechicken/items/8216aaad36709b6f0b5c\n\n\n\n\n## \u304d\u3063\u304b\u3051\n\nAutoencoder\uff08\u81ea\u5df1\u7b26\u53f7\u5316\u5668\uff09\u306f\u4ed6\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306b\u6bd4\u3079\u308b\u3068\u3084\u3084\u5730\u5473\u306a\u5b58\u5728\u3067\u3042\u308b\uff0e\u6587\u732e\u300c\u6df1\u5c64\u5b66\u7fd2\u300d\uff08\u5ca1\u8c37\u6c0f\u8457\uff0c\u8b1b\u8ac7\u793e\uff09\u3067\u306f\u7b2c\uff15\u7ae0\u306b\u767b\u5834\u3059\u308b\u304c\uff0c\n\n> \u81ea\u5df1\u7b26\u53f7\u5316\u5668\u3068\u306f\uff0c\u76ee\u6a19\u51fa\u529b\u3092\u4f34\u308f\u306a\u3044\uff0c\u5165\u529b\u3060\u3051\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u305f\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306b\u3088\u308a\uff0c\u30c7\u30fc\u30bf\u3092\u3088\u304f\u8868\u3059\u7279\u5fb4\u3092\u7372\u5f97\u3057\uff0c\u3072\u3044\u3066\u306f\u30c7\u30fc\u30bf\u306e\u3088\u3044\u8868\u73fe\u65b9\u6cd5\u3092\u5f97\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3059\uff0e\u30c7\u30a3\u30fc\u30d7\u30cd\u30c3\u30c8\u306e\u4e8b\u524d\u5b66\u7fd2\uff0c\u3059\u306a\u308f\u3061\u305d\u306e\u91cd\u307f\u306e\u3088\u3044\u521d\u671f\u5024\u3092\u5f97\u308b\u76ee\u7684\u306b\u3082\u5229\u7528\u3055\u308c\u307e\u3059\uff0e\n\n\u3068\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\uff0e\u300c\u4e8b\u524d\u5b66\u7fd2\u300d\u3092\u5b9f\u65bd\u3059\u308b\u6a5f\u4f1a\u306f\u3042\u307e\u308a\u306a\u3055\u305d\u3046\uff0c\u3068\u3044\u3046\u3053\u3068\u3067\u3053\u306e\u7ae0\u306f\u76ee\u3092\u901a\u3059\u306b\u3068\u3069\u3081\uff0c\u6b21\u306e\u7ae0\uff0c\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8(CNN)\u3084\u518d\u5e30\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8(RNN)\u306e\u7ae0\u306b\u9032\u3080\u65b9\u3082\u591a\u3044\u3068\u601d\u308f\u308c\u308b\uff0e\n\n\u305f\u3060\u3088\u304f\u8abf\u3079\u308b\u3068\uff0c\u4e8b\u524d\u5b66\u7fd2\u306e\u4ed6\u306b\u3082Autoencoder\u306e\u4f7f\u7528\u76ee\u7684\u3068\u3057\u3066\u4ee5\u4e0b\u304c\u3042\u308b\u3088\u3046\u3060\uff0e\n\n- \u30c7\u30fc\u30bf\u5727\u7e2e\uff0e\n- \u30c7\u30fc\u30bf\u30ce\u30a4\u30ba\u9664\u53bb\uff0e\uff08Denoising Autoencoder\uff09\n- \u753b\u50cf\u306e\u533a\u753b\u5206\u5272\uff08Semantic Segmentation - \u753b\u50cf\u306e\u4e2d\u8eab\u3092\u8a8d\u8b58\u3057\u305f\u4e0a\u3067\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\uff09\n\n\u73fe\u5728\uff0cKaggle\u3067\u306f\u533b\u7642\u753b\u50cf\u306b\u95a2\u3059\u308b\u30b3\u30f3\u30da(Ultrasound Nerve)\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u304c\uff0cAutoencoder\u3092\u4f7f\u3063\u305f\u624b\u6cd5\u304c\u672c\u547d\u306b\u306a\u308b\u3068\u30d5\u30a9\u30fc\u30e9\u30e0\u3067\u767a\u8a00\u3055\u308c\u3066\u3044\u305f\uff0e\u305d\u308c\u306a\u3089\u3070\u3068\u3044\u3046\u3053\u3068\u3067\uff0c\u52c9\u5f37\u306e\u305f\u3081\uff0cMNIST\uff08\u624b\u66f8\u304d\u6570\u5b57\u30c7\u30fc\u30bf\uff09\u3092\u984c\u6750\u306bAutoencoder\u3092\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u305f\uff0e\n\n## Keras\u306eAutoencoder\u306b\u95a2\u3059\u308b\u8a18\u4e8b\n\n\u30c4\u30fc\u30eb\u3068\u3057\u3066TensowFlow\u3092\u8003\u3048\u305f\u304c\uff0c\u6b8b\u5ff5\u306a\u304c\u3089TensorFlow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\uff0c\u7279\u306bTutorial\u306b\u306fAutoencoder\u306f\u306a\u3044\uff0e\u5225\u306eDeep Learning\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\uff0cKeras\u306b\u30d6\u30ed\u30b0\u8a18\u4e8b\u3068\u3057\u3066Autoencoder\u304c\u53d6\u308a\u4e0a\u3052\u3089\u308c\u3066\u304a\u308a\uff0c\u305d\u308c\u304c\u975e\u5e38\u306b\u53c2\u8003\u306b\u306a\u3063\u305f\uff0e\n\nBuilding Autoencoders in Keras : http://blog.keras.io/building-autoencoders-in-keras.html\n\n\u3053\u306e\u4e2d\u306b\u306f\uff0c\u3055\u307e\u3056\u307e\u306a\u7a2e\u985e\u306eAutoencoder\u306b\u3064\u3044\u3066\u5206\u304b\u308a\u3084\u3059\u304f\u8aac\u660e\u304c\u3055\u308c\u3066\u3044\u308b\uff0e\u307e\u305a\uff0c\u3053\u308c\u3092\u5199\u7d4c\u3057\u3066\u7406\u89e3\u3092\u6df1\u3081\uff0c\u6b21\u306b\uff0c\u3053\u306e\u4e2d\u304b\u3089\u6b21\u306e\u4e8c\u3064\u3092TensorFlow\u306b\u79fb\u690d\u3057\u3066\u307f\u305f\uff0e\n\n- \u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306aAutoencoder\n- \u7573\u8fbc\u307fAutoencoder\n\n\u7279\u306b\u300c\u7573\u8fbc\u307fAutoencoder\u300d\u306e\u65b9\u3067\u306f\uff0c\u901a\u5e38\u306e\u7573\u8fbc\u307f\u4e88\u6e2c\u30e2\u30c7\u30eb(CNN)\u3067\u884c\u3046\uff0c\u300c\u7573\u8fbc\u307f\u300d-> \u300c\u30d7\u30fc\u30ea\u30f3\u30b0\u300d\u306e\uff0c\uff08\u753b\u50cf\u306e\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u5897\u3084\u3057\u306a\u304c\u3089\uff09\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5c0f\u3055\u304f\u3059\u308b\u51e6\u7406\u306e\u9006\uff0c\uff08\u753b\u50cf\u306e\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u306a\u304c\u3089\uff09\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5927\u304d\u304f\u3057\u3066\u3044\u304f\u51e6\u7406\u304cAutoencoder\u3067\u5fc5\u8981\u3068\u306a\u308b\u306e\u3067\uff0c\u3053\u3053\u306e\u3068\u3053\u308d\u3067\u3044\u308d\u3044\u308d\u60a9\u3080\u3053\u3068\u3068\u306a\u3063\u305f\uff0e\n\n\uff08\u4ee5\u4e0b\uff0cTensorFlow\u306b\u3088\u308b\u5b9f\u88c5\u306e\u8a71\u306b\u306a\u308a\u307e\u3059\u304c\uff0c\u4e0a\u8a18\u306eKeras\u30d6\u30ed\u30b0\u8a18\u4e8b\u306b\u306fAutoencoder\u306b\u95a2\u3059\u308b\u5185\u5bb9\u306e\u307f\u306a\u3089\u305a\uff0cKeras\u3067TensorBoard\u3092\u4f7f\u3046\u65b9\u6cd5\u7b49\uff0c\u3068\u3066\u3082\u305f\u3081\u306b\u306a\u308b\u60c5\u5831\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u305c\u3072\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\uff09\n\uff08\u672c\u8a18\u4e8b\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u74b0\u5883\u306f\uff0cPython 2.7.11, Keras 1.0.5, TensorFlow 0.9.0rc0 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n## \u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306aAutoencoder\n\nKeras\u306e\u30d6\u30ed\u30b0\u8a18\u4e8b\u3067\u3082\u6700\u521d\u306b\u767b\u5834\u3059\u308b\uff0e\u5165\u529b\u304b\u3089\u91cd\u307f`w`\uff0c\u30d0\u30a4\u30a2\u30b9`b`\u3092\u4f7f\u3063\u3066\u4e2d\u9593\u5c64\u3092\u751f\u6210\u3057\uff0c\u305d\u3053\u304b\u3089\u307e\u305f\u5225\u306e\u91cd\u307f`w'`\uff0c\u30d0\u30a4\u30a2\u30b9`b'`\u3092\u4f7f\u3063\u3066\u5165\u529b\u30c7\u30fc\u30bf\u3092\u518d\u751f\u6210\u3059\u308b\u3068\u3044\u3046\u30e2\u30c7\u30eb\uff0eKeras\u3067\u306e\u30e2\u30c7\u30eb\u90e8\u5206\u306e\u30b3\u30fc\u30c9\u306f\uff0c\u4ee5\u4e0b\u306e\u901a\u308a\uff0e\n\n```py\n# this is our input placeholder\ninput_img = Input(shape=(784,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(784, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input=input_img, output=decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input=input_img, output=encoded)\n\n# create a placeholder for an encoded (32-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n```\n\nKeras\uff08\u7279\u306bSequential model API\uff09\u3067\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5165\u529b\u5074\u304b\u3089\u51fa\u529b\u5074\u306b\u5411\u3051\u3066\u30e2\u30c7\u30eb\u3092\u8a18\u8ff0\u3057\u3066\u3044\u304f\uff0e\u4e0a\u8a18\u306f\uff0c\u5165\u529b\u304b\u3089\u30d5\u30eb\u7d50\u5408\u5c64\uff08Dense())\u3067\u4e2d\u9593\u5024\u3092\u6c42\u3081\uff0c\u305d\u3053\u304b\u3089\u3059\u3050\u306b\u6298\u308a\u8fd4\u3057\u3066\u30d5\u30eb\u7d50\u5408\u5c64\u3092\u7d4c\u3066\u30c7\u30b3\u30fc\u30c9\u5024\u3092\u6c42\u3081\u3066\u3044\u308b\uff0e\n\n\u3053\u308c\u3092TensorFlow\u306b\u76f4\u3059\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\uff0e\n\n```py\n# Variables\nx = tf.placeholder(\"float\", [None, 784])\ny_ = tf.placeholder(\"float\", [None, 10])\n\nw_enc = tf.Variable(tf.random_normal([784, 625], mean=0.0, stddev=0.05))\nw_dec = tf.Variable(tf.random_normal([625, 784], mean=0.0, stddev=0.05))\n# w_dec = tf.transpose(w_enc) # if you use tied weights\nb_enc = tf.Variable(tf.zeros([625]))\nb_dec = tf.Variable(tf.zeros([784]))\n\n# Create the model\ndef model(X, w_e, b_e, w_d, b_d):\n    encoded = tf.sigmoid(tf.matmul(X, w_e) + b_e)\n    decoded = tf.sigmoid(tf.matmul(encoded, w_d) + b_d)\n    \n    return encoded, decoded\n\nencoded, decoded = model(x, w_enc, b_enc, w_dec, b_dec)\n```\n\n\u4e0a\u8a18\u306e\u901a\u308a\uff0c\u91cd\u307fw\u306e\u5f62\u306f\uff0cencode\u5074\u3068decode\u5074\u3067\u8ee2\u7f6e\u3055\u305b\u305f\u95a2\u4fc2\u3068\u306a\u308b\uff0e\u4e0a\u306e\u30b3\u30fc\u30c9\u3067\u306f\uff0cencode\u5074\u3068decode\u5074\u3067\u5225\u306e\u91cd\u307f\u3092\u8a2d\u5b9a\u3057\u305f\u304c\uff0c\u91cd\u307f\u5171\u6709\u3092\u884c\u3046\u5834\u5408\u306f\uff0ctf.transpose() \u3067\u5909\u6570\u3092\u8ee2\u7f6e\u3055\u305b\u308b\uff0e\uff08\u30b3\u30e1\u30f3\u30c8\u7b87\u6240\uff09\u91cd\u307f\u5171\u6709\u3092\u7528\u3044\u308b/\u7528\u3044\u306a\u3044\u306b\u95a2\u3057\u3066\u306f\uff0c\u660e\u78ba\u306a\u30eb\u30fc\u30eb\u306f\u306a\u3044\u3068\u306e\u3053\u3068\u306a\u306e\u3067\uff0c\u30b1\u30fc\u30b9\u3054\u3068\u306b\u4f7f\u3044\u5206\u3051\u308b\u3053\u3068\u306b\u306a\u308b\uff0e\n\n\u30e2\u30c7\u30eb\u304c\u30b7\u30f3\u30d7\u30eb\u306a\u306e\u3067\uff0c\u3059\u3050\u306b\u52d5\u4f5c\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u305f\uff0e\u8a08\u7b97\u7d50\u679c\u306f\uff0c\u6b21\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n\n```text\nTraining...\n  step, loss =      0:  0.721\n  step, loss =   1000:  0.262\n  step, loss =   2000:  0.243\n  step, loss =   3000:  0.237\n  step, loss =   4000:  0.229\n  step, loss =   5000:  0.209\n  step, loss =   6000:  0.212\n  step, loss =   7000:  0.200\n  step, loss =   8000:  0.196\n  step, loss =   9000:  0.178\n  step, loss =  10000:  0.189\nloss (test) =  0.180014\n```\n\n**Fig. \u5165\u529b\u753b\u50cf\u3068\u5fa9\u5143\u3055\u308c\u305f\u753b\u50cf\uff08\u30b7\u30f3\u30d7\u30ebAutoencoder\uff09**\n![mnist_ae1.png](https://qiita-image-store.s3.amazonaws.com/0/74152/cf24d861-3533-8cb6-aef0-580f4dbbe00f.png)\n\n\n## \u7573\u8fbc\u307fAutoencoder\uff08Convolutional\u306aAutoencoder\uff09\n\n\u7573\u8fbc\u307fAutoencoder\u304c\u3069\u306e\u3088\u3046\u306a\u3082\u306e\u304b\uff0cKeras\u306e\u30d6\u30ed\u30b0\u304b\u3089\u5f15\u7528\u3059\u308b\uff0e\n\n> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) \nas encoders and decoders. In practical settings, autoencoders applied to images are always \nconvolutional autoencoders --they simply perform much better.\n\n\u753b\u50cf\u3092\u6271\u3046\u306b\u3042\u305f\u308aConvolution\u51e6\u7406\u306f\u6709\u52b9\u3067\uff0cAutoencoder\u3082\u5358\u7d14\u306b\u9ad8\u6a5f\u80fd\u306b\u306a\u308b\u3068\u306e\u3053\u3068\uff0e\u305d\u3053\u3067\u307e\u305aKeras\u3067\u306e\u5b9f\u88c5\u3092\u53c2\u8003\u306b\u3059\u308b\uff0e\n\n```py\ninput_img = Input(shape=(1, 28, 28))\n\nx = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(input_img)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nencoded = MaxPooling2D((2, 2), border_mode='same')(x)\n\n# at this point the representation is (8, 4, 4) i.e. 128-dimensional\n\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(16, 3, 3, activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Convolution2D(1, 3, 3, activation='sigmoid', border_mode='same')(x)\n\nautoencoder = Model(input_img, decoded)\n\n```\n\n\u3055\u3059\u304c\u306bKeras\u306e\u30b3\u30fc\u30c9\u306f\u8aad\u307f\u3084\u3059\u3044\uff0e\n\n3\u7d44\u306e\u7573\u8fbc\u307f\u5c64\u3068MaxPooing\u5c64\u304b\u3089\u306a\u308bencode\u30d7\u30ed\u30bb\u30b9\u3068\uff0c\u305d\u306e\u5f8c\u6298\u308a\u8fd4\u3057\u3066\uff0c\u753b\u50cf\u3092\u5fa9\u5143\u3059\u308bdecode\u30d7\u30ed\u30bb\u30b9\u3067\u3053\u306eAutoencoder\u306f\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\uff0eendcode\u306e\u90e8\u5206\u306f\uff0c\u753b\u50cf\u5206\u985e\u306a\u3069\u3067\u7528\u3044\u308b\u901a\u5e38\u306eCNN\u5206\u985e\u5668\u306e\u3082\u306e\u3068\u540c\u3058\u3067\u3042\u308b\u304c\uff0cAutoencoder\u306b\u7279\u6709\u306a\u306e\u306f\u5f8c\u534a\u90e8\u5206\u3067\u3042\u308b\uff0eKeras\u30b3\u30fc\u30c9\u3067\u306f\uff0cConvolution2D()\u3068UpSampling2D()\u306e\u95a2\u6570\u304c\u7528\u3044\u3089\u308c\u3066\u3044\u308b\u304c\uff0c\u3053\u306eUpsampling2D()\u306b\u5bfe\u3057\u3066\uff0cTensorFlow\u306b\u540c\u69d8\u306e\u3082\u306e\u306f\u306a\u3044\uff0e\n\n\u3053\u308c\u306b\u4ee3\u308f\u308b\u306e\u304c \"tf.nn.conv2d_transpose()\" \u3067\u3042\u308b\uff0e\u3053\u308c\u3092\u4f7f\u3046\u306b\u3042\u305f\u308a\u6ce8\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u306e\u304c\uff0cKeras\u306e UpSampling2D() \u3092\uff11\u5bfe\uff11\u306b\u7f6e\u304d\u63db\u3048\u308b\u306e\u3067\u306f\u306a\u304f\uff0cConvolutional2D() + UpSampling2D() \u306e\u6a5f\u80fd\u3092 \"tf.nn.conv2d_transpose\" \u304c\u6301\u3063\u3066\u3044\u308b\u70b9\u3067\u3042\u308b\uff0e\u3053\u3053\u306b\u6ce8\u610f\u3057\uff0cTensorFlow\u3067\u5b9f\u88c5\u3057\u305f\u306e\u304c\u4ee5\u4e0b\u3067\u3042\u308b\uff0e\n\n```py\nfrom my_nn_lib import Convolution2D, MaxPooling2D\n\n# Conv 2-D transpose Layer (deconvolutoinal Layer)\nclass Conv2Dtranspose(object):\n    '''\n      constructor's args:\n          input      : input image (2D matrix)\n          output_siz : output image size\n          in_ch      : number of incoming image channel\n          out_ch     : number of outgoing image channel\n          patch_siz  : filter(patch) size\n    '''\n    def __init__(self, input, output_siz, in_ch, out_ch, patch_siz, activation='relu'):\n        self.input = input      \n        self.rows = output_siz[0]\n        self.cols = output_siz[1]\n        self.out_ch = out_ch\n        self.activation = activation\n        \n        wshape = [patch_siz[0], patch_siz[1], out_ch, in_ch]    # note the arguments order\n        \n        w_cvt = tf.Variable(tf.truncated_normal(wshape, stddev=0.1), \n                            trainable=True)\n        b_cvt = tf.Variable(tf.constant(0.1, shape=[out_ch]), \n                            trainable=True)\n        self.batsiz = tf.shape(input)[0]\n        self.w = w_cvt\n        self.b = b_cvt\n        self.params = [self.w, self.b]\n        \n    def output(self):\n        shape4D = [self.batsiz, self.rows, self.cols, self.out_ch]      \n        linout = tf.nn.conv2d_transpose(self.input, self.w, output_shape=shape4D,\n                            strides=[1, 2, 2, 1], padding='SAME') + self.b\n        if self.activation == 'relu':\n            self.output = tf.nn.relu(linout)\n        elif self.activation == 'sigmoid':\n            self.output = tf.sigmoid(linout)\n        else:\n            self.output = linout\n        \n        return self.output\n\ndef mk_nn_model(x, y_):\n    # Encoding phase\n    x_image = tf.reshape(x, [-1, 28, 28, 1])    \n    conv1 = Convolution2D(x_image, (28, 28), 1, 16, \n                          (3, 3), activation='relu')\n    conv1_out = conv1.output()\n    \n    pool1 = MaxPooling2D(conv1_out)\n    pool1_out = pool1.output()\n    \n    conv2 = Convolution2D(pool1_out, (14, 14), 16, 8, \n                          (3, 3), activation='relu')\n    conv2_out = conv2.output()\n    \n    pool2 = MaxPooling2D(conv2_out)\n    pool2_out = pool2.output()\n\n    conv3 = Convolution2D(pool2_out, (7, 7), 8, 8, (3, 3), activation='relu')\n    conv3_out = conv3.output()\n\n    pool3 = MaxPooling2D(conv3_out)\n    pool3_out = pool3.output()\n    # at this point the representation is (8, 4, 4) i.e. 128-dimensional\n    # Decoding phase\n    conv_t1 = Conv2Dtranspose(pool3_out, (7, 7), 8, 8,\n                         (3, 3), activation='relu')\n    conv_t1_out = conv_t1.output()\n\n    conv_t2 = Conv2Dtranspose(conv_t1_out, (14, 14), 8, 8,\n                         (3, 3), activation='relu')\n    conv_t2_out = conv_t2.output()\n\n    conv_t3 = Conv2Dtranspose(conv_t2_out, (28, 28), 8, 16, \n                         (3, 3), activation='relu')\n    conv_t3_out = conv_t3.output()\n\n    conv_last = Convolution2D(conv_t3_out, (28, 28), 16, 1, (3, 3),\n                         activation='sigmoid')\n    decoded = conv_last.output()\n\n    decoded = tf.reshape(decoded, [-1, 784])\n    cross_entropy = -1. *x *tf.log(decoded) - (1. - x) *tf.log(1. - decoded)\n    loss = tf.reduce_mean(cross_entropy)\n       \n    return loss, decoded\n\n```\n\nTensorFlow\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u95a2\u4fc2API\u3092\u4f7f\u3044\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\uff0c\u30af\u30e9\u30b9\"Convolution2D\"\u3068\"MaxPooling2D\"\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\uff0e\uff08 \"tf.nn.conv2d()\"\u3068\"tf.nn.max_pool()\u306ewrapper\u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n\u4eca\u56de\uff0c\u3053\u308c\u306b\u52a0\u3048\u3066\uff0ctf.nn.conv2d_transpose()\u3092\u5305\u3080\"Conv2Dtranspose\"\u30af\u30e9\u30b9\u3092\u5b9a\u7fa9\u3057\uff0c\u305d\u308c\u3092\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u95a2\u6570\u3067\u4f7f\u7528\u3057\u3066\u3044\u308b\uff0e\n\n\u3053\u308c\u3089\u306e\u30af\u30e9\u30b9\u4f7f\u7528\u306b\u3042\u305f\u308a\uff0c\uff08\u30af\u30e9\u30b9\u3067\u5f15\u6570\u3092\u5c11\u306a\u304f\u3057\u3088\u3046\u3068\u306f\u3057\u3066\u3044\u308b\u304c\uff0c\uff09\u6307\u5b9a\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u5f15\u6570\u304c\u591a\u304f\uff0c\u7279\u306bdimension\u95a2\u4fc2\u306f\uff0c\u9069\u5207\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u3052\u306a\u3044\u3068\u30a8\u30e9\u30fc\u306b\u3064\u306a\u304c\u308b\uff0e\uff08tf.nn.conv2d_transpose()\u306f\uff0ctf.nn.conv2d() \u3068\u540c\u3058\u3088\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3092\u3057\u3066\u9593\u9055\u3046\u30b1\u30fc\u30b9\u304c\u591a\u3044\u3088\u3046\u3067\uff0cstack overflow\u7b49\u3067Bug Fix\u306e\u30a2\u30c9\u30d0\u30a4\u30b9\u3092\u53d7\u3051\u3066\u3044\u308b\u8a18\u4e8b\u304c\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3057\u305f\uff0e\uff09\n\n\u3053\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u305f\u72b6\u6cc1\u306f\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n\n```text\nTraining...\n  step, loss =      0:  0.616\n  step, loss =   1000:  0.158\n  step, loss =   2000:  0.132\n  step, loss =   3000:  0.125\n  step, loss =   4000:  0.116\n  step, loss =   5000:  0.113\n  step, loss =   6000:  0.108\n  step, loss =   7000:  0.116\n  step, loss =   8000:  0.114\n  step, loss =   9000:  0.109\n  step, loss =  10000:  0.108\nloss (test) =  0.106951\n```\n\n**Fig. \u5165\u529b\u753b\u50cf\u3068\u5fa9\u5143\u3055\u308c\u305f\u753b\u50cf\uff08Convolutional Autoencoder\uff09**\n![mnist_ae2.png](https://qiita-image-store.s3.amazonaws.com/0/74152/e46afc6e-bfce-37dd-750a-ee4c74c94238.png)\n\n\u4e0a\u306e\u300c\u4e00\u756a\u30b7\u30f3\u30d7\u30eb\u306a\u300d\u306e\u7d50\u679c\u753b\u50cf\u3068\u6bd4\u8f03\u3059\u308b\u3068\uff0c\u540c\u3058\u5b66\u7fd2\u56de\u6570\u3067\u3088\u308a\u660e\u77ad\u306a\u753b\u50cf\u304c\u5fa9\u5143\u3067\u304d\u3066\u3044\u308b\uff0e\n\n\u521d\u3081\u306fAutoencoder\u3092\u907f\u3051\u3066\u3044\u305f\u304c\uff0c\u3053\u3046\u3057\u3066\u5b9f\u969b\u52d5\u304b\u3057\u3066\u307f\u308b\u3068\u306a\u304b\u306a\u304b\u8208\u5473\u6df1\u3044\u52d5\u4f5c\u3092\u3059\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\uff0e\u307e\u305f\uff0c\u4eca\u56de\u306f\u52c9\u5f37\u306e\u305f\u3081\u306bTensorFlow\u3067\u306e\u5b9f\u88c5\u3092\u884c\u3063\u305f\u304c\uff0c\u6539\u3081\u3066Keras\u306e\uff08\u77ed\u3044\u30b3\u30fc\u30c9\u3067\u304b\u3051\u308b\uff09\u6709\u7528\u3055\u3092\u5b9f\u611f\u3067\u304d\u305f\uff0e\n\n\uff08\u672c\u8a18\u4e8b\u306e\u30b3\u30fc\u30c9\u306f\uff0cGist : https://gist.github.com/tomokishii/7ddde510edb1c4273438ba0663b26fc6 \u306b\u8f09\u305b\u3066\u3044\u307e\u3059\uff0e\uff09\n\n## \u53c2\u8003\u6587\u732e / web site\n- Building Autoencoders in Keras - Keras Blog\n  http://blog.keras.io/building-autoencoders-in-keras.html\n- Keras\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 http://keras.io/\n- TensorFlow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 https://www.tensorflow.org/versions/r0.9/api_docs/index.html\n- \u6df1\u5c64\u5b66\u7fd2\uff08\u5ca1\u8c37\u6c0f\u8457\uff0c\u8b1b\u8ac7\u793e \u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba\uff09\n- TensorFlow\u3067\u6a5f\u68b0\u5b66\u7fd2\u3068\u622f\u308c\u308b: AutoEncoder\u3092\u4f5c\u3063\u3066\u307f\u308b - Qiita \n  http://qiita.com/mokemokechicken/items/8216aaad36709b6f0b5c\n", "tags": ["Python", "Keras", "TensorFlow", "Autoencoder"]}