{"context": " More than 1 year has passed since last update.Apache Spark 1.6\u304b\u3089\u65b0\u3057\u304f\u8ffd\u52a0\u3055\u308c\u305fDataset API\u3092\u8a66\u3057\u3066\u307f\u308b\u3002\n2015/12/14\u73fe\u5728\u307e\u3060\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3066\u306a\u3044\u304c\u3001\u5e74\u5185\u4e2d\u306b\u306f\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u306f\u305a\u3002\n\n\u80cc\u666f\n\n\nRDD\u306fLow Level API\u3067\u3001\u3068\u3057\u3066\u30d5\u30ec\u30ad\u30b7\u30d6\u30eb\u3060\u304c\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u3044\n\uff08Spark 1.3\u304b\u3089\u767b\u5834\u3057\u305f\uff09DataFrame\u306fHigh Level API\u3067\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u304c\u6700\u9069\u5316\u3057\u3066\u304f\u308c\u308b\u304c\u3001\u30d5\u30ec\u30ad\u30b7\u30d6\u30eb\u3055\u304c\u306a\u3044\u3002\u7279\u306bUDF\u306e\u4f7f\u3044\u52dd\u624b\u304c\u4e0d\u4fbf\u306a\u3068\u3053\u308d\u3084\u578b\u30c1\u30a7\u30c3\u30af\u306b\u5f31\u3044\n\n\nDataset API \u767b\u5834\n\n\u4e0a\u8a18\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306bSpark 1.6\u304b\u3089\u5b9f\u9a13\u7684\uff08Experimental\uff09\u306b\u767b\u5834\u3057\u305f\u306e\u304cDataset API\u3067\u3042\u308b\nRDD\u3068DataFrame\u306e\u826f\u3044\u3068\u3053\u308d\u3092\u4f75\u305b\u6301\u3064API\u3068\u3057\u3066\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u65e9\u304f\u3066\u4f7f\u3044\u52dd\u624b\u306e\u3088\u3044API\u3060\u3068\u8a00\u3048\u307e\u3059\u3002\n\n\n\u753b\u50cf\u306fhttp://technicaltidbit.blogspot.jp\u3088\u308a\nDataset API\u306e\u8981\u4ef6\u5b9a\u7fa9\u306f SPARK-9999 \u306b\u8a73\u3057\u304f\u66f8\u304b\u308c\u3066\u308b\u901a\u308a\u3001\u5927\u304d\u304f\u4e0b\u8a18\u306e\uff14\u3064\u306b\u306a\u308a\u307e\u3059\u3002\n\nFast\nTypesafe\nJava Compatible\nInteroperates with DataFrames\n\n\nLet's give it a try\n\nSpark 1.6.0 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\u89e3\u51cd\u5f8c\u306b\u3001/usr/local/spark-1.6.0 \u3078\u30b3\u30d4\u30fc\n\nSpark\u30bd\u30fc\u30b9\u306b\u540c\u80de\u3055\u308c\u3066\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u3063\u3066\u6bd4\u8f03\u3057\u3066\u307f\u308b\n/usr/local/spark-1.6.0/bin/spark-shell \u8d77\u52d5\n// \u5404\u81ea\u306e\u30d1\u30b9\u3092\u5408\u308f\u305b\u3066\u8a2d\u5b9a\nscala> val peopleFile = \"/usr/local/spark-1.6.0/examples/src/main/resources/people.json\"\n\n// JSON \u2192 DataFrame\nscala> val df = sqlContext.read.json(peopleFile)\ndf: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n\n// \u4e2d\u8eab\u78ba\u8a8d\nscala> df.printSchema\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\nscala> df.show\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n// Dataset\u3092\u4f7f\u3046\u305f\u3081\u3001case class \u5b9a\u7fa9\nscala> case class Person(age: Long, name: String)\n\n// DataFrame\u304b\u3089Dataset\u306b\u5909\u63db\nscala> val ds = df.as[Person]\nds: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n\n// DataFrame\u3067\u306e20\u6b73\u4ee5\u4e0a\u306e\u4eba\u3092\u53d6\u5f97\n// DataFrame\u306f\u884c\u5217\u8a08\u7b97\u306b\u7279\u5316\u3057\u305f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u306e\u3067\u3001\u30ab\u30e9\u30e0\u540d\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\nscala> df.where($\"age\" >= 20).show\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n// Dataset\u3067\u306e20\u6b73\u4ee5\u4e0a\u306e\u4eba\u3092\u53d6\u5f97\n// Dataset\u306fDataFrame\u306eRow\u3092JVM\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff08\u3053\u306e\u4f8b\u3067\u306fPerson\uff09\u3068\u3057\u3066\u6271\u3048\u308b\u305f\u3081\u3001UDF\u304c\u9069\u7528\u304c\u7c21\u5358\nscala> ds.filter(_.age >= 20).show\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n// Dataset \u2192\u3000DataFrame\nscala> val df2 = ds.toDF\ndf2: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n\n// Dataset \u2192\u3000RDD\nscala> val rdd = ds.rdd\nrdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[121] at rdd at <console>:33\n\n// \u5c11\u3057\u8907\u96d1\u306a\u51e6\u7406\uff08\u5e74\u4ee3\u5225\u4eba\u6570\u96c6\u8a08\uff09\nscala> import org.apache.spark.sql.types._\n\n// DataFrame\u306e\u5834\u5408\nscala> :paste\ndf.where($\"age\" > 0)\n  .groupBy((($\"age\" / 10) cast IntegerType) * 10 as \"decade\")\n  .agg(count($\"name\"))\n  .orderBy($\"decade\")\n  .show\n\n+------+-----------+\n|decade|count(name)|\n+------+-----------+\n|    10|          1|\n|    30|          1|\n+------+-----------+\n\n// Dataset\u306e\u5834\u5408\nscala> :paste\nds.filter(_.age > 0)\n  .groupBy(p => (p.age / 10) * 10)\n  .agg(count(\"name\"))\n   // orderBy\u304c\u306a\u3044\u3088\u3046\u306a\u306e\u3067\u3001DF\u3078\u5909\u63db\uff08\u3053\u308c\u306f\u3084\u3084\u4e0d\u4fbf\uff09\n  .toDF().withColumnRenamed(\"value\", \"decade\").orderBy(\"decade\") \n  .show\n\n+------+-----------+\n|decade|count(name)|\n+------+-----------+\n|    10|          1|\n|    30|          1|\n+------+-----------+\n\n\n\n\u4ed6\u306e\u4f8b\u306fdatabricks\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\nhttps://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/datasets.wordcount.scala.html\n\nDataset API\u4ee5\u5916\u306eSpark1.6.0\u306b\u3064\u3044\u3066\u306f\u3053\u3061\u3089\nhttps://docs.cloud.databricks.com/docs/spark/1.6/index.html#00%20Spark%201.6%20Preview.html\n\n\u307e\u3068\u3081\n\nDataset API\u306fRDD-like\u306b\u4f7f\u3048\u3066DataFrame\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u5229\u70b9\u3082\u6d3b\u304b\u305b\u308b\nDataFrame\u3084RDD\u3078\u7c21\u5358\u3082\u5909\u63db\u3067\u304d\u308b\n\u307e\u3060\u5b89\u5b9a\u7248\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u8db3\u308a\u306a\u3044\u30d5\u30a1\u30f3\u30af\u30b7\u30e7\u30f3\u3082\u3042\u308b\u3051\u3069\u3001\u4eca\u5f8c\u306b\u671f\u5f85\n\nApache Spark 1.6\u304b\u3089\u65b0\u3057\u304f\u8ffd\u52a0\u3055\u308c\u305f**Dataset API**\u3092\u8a66\u3057\u3066\u307f\u308b\u3002\n*2015/12/14\u73fe\u5728\u307e\u3060\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3066\u306a\u3044\u304c\u3001\u5e74\u5185\u4e2d\u306b\u306f\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u306f\u305a\u3002*\n\n# \u80cc\u666f\n* **RDD**\u306fLow Level API\u3067\u3001\u3068\u3057\u3066\u30d5\u30ec\u30ad\u30b7\u30d6\u30eb\u3060\u304c\u3001\u6700\u9069\u5316\u304c\u96e3\u3057\u3044\n* \uff08Spark 1.3\u304b\u3089\u767b\u5834\u3057\u305f\uff09**DataFrame**\u306fHigh Level API\u3067\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u304c\u6700\u9069\u5316\u3057\u3066\u304f\u308c\u308b\u304c\u3001\u30d5\u30ec\u30ad\u30b7\u30d6\u30eb\u3055\u304c\u306a\u3044\u3002\u7279\u306bUDF\u306e\u4f7f\u3044\u52dd\u624b\u304c\u4e0d\u4fbf\u306a\u3068\u3053\u308d\u3084\u578b\u30c1\u30a7\u30c3\u30af\u306b\u5f31\u3044\n\n# Dataset API \u767b\u5834\n* \u4e0a\u8a18\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306bSpark 1.6\u304b\u3089\u5b9f\u9a13\u7684\uff08Experimental\uff09\u306b\u767b\u5834\u3057\u305f\u306e\u304cDataset API\u3067\u3042\u308b\n* RDD\u3068DataFrame\u306e\u826f\u3044\u3068\u3053\u308d\u3092\u4f75\u305b\u6301\u3064API\u3068\u3057\u3066\u958b\u767a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u65e9\u304f\u3066\u4f7f\u3044\u52dd\u624b\u306e\u3088\u3044API\u3060\u3068\u8a00\u3048\u307e\u3059\u3002\n\n![SparkDatasets.png](https://qiita-image-store.s3.amazonaws.com/0/70901/fac2a162-7c1d-1ee4-c6b4-dd85e6992258.png)\n\n\u753b\u50cf\u306f[http://technicaltidbit.blogspot.jp](http://4.bp.blogspot.com/-nYIzYeAnNOw/VjEDtCS9NUI/AAAAAAAACO4/LL4XT7bCxog/s1600/SparkDatasets.png)\u3088\u308a\n\nDataset API\u306e\u8981\u4ef6\u5b9a\u7fa9\u306f [SPARK-9999](https://issues.apache.org/jira/browse/SPARK-9999) \u306b\u8a73\u3057\u304f\u66f8\u304b\u308c\u3066\u308b\u901a\u308a\u3001\u5927\u304d\u304f\u4e0b\u8a18\u306e\uff14\u3064\u306b\u306a\u308a\u307e\u3059\u3002\n> **Fast**\n> **Typesafe**\n> **Java Compatible**\n> **Interoperates with DataFrames**\n\n# Let's give it a try\n## Spark 1.6.0 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\u89e3\u51cd\u5f8c\u306b\u3001`/usr/local/spark-1.6.0` \u3078\u30b3\u30d4\u30fc\n\n## Spark\u30bd\u30fc\u30b9\u306b\u540c\u80de\u3055\u308c\u3066\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u3063\u3066\u6bd4\u8f03\u3057\u3066\u307f\u308b\n`/usr/local/spark-1.6.0/bin/spark-shell \u8d77\u52d5`\n\n```scala\n\b// \u5404\u81ea\u306e\u30d1\u30b9\u3092\u5408\u308f\u305b\u3066\u8a2d\u5b9a\nscala> val peopleFile = \"/usr/local/spark-1.6.0/examples/src/main/resources/people.json\"\n\n// JSON \u2192 DataFrame\nscala> val df = sqlContext.read.json(peopleFile)\ndf: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n\n// \u4e2d\u8eab\u78ba\u8a8d\nscala> df.printSchema\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\nscala> df.show\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n// Dataset\u3092\u4f7f\u3046\u305f\u3081\u3001case class \u5b9a\u7fa9\nscala> case class Person(age: Long, name: String)\n\n// DataFrame\u304b\u3089Dataset\u306b\u5909\u63db\nscala> val ds = df.as[Person]\nds: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n\n// DataFrame\u3067\u306e20\u6b73\u4ee5\u4e0a\u306e\u4eba\u3092\u53d6\u5f97\n// DataFrame\u306f\u884c\u5217\u8a08\u7b97\u306b\u7279\u5316\u3057\u305f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306a\u306e\u3067\u3001\u30ab\u30e9\u30e0\u540d\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\nscala> df.where($\"age\" >= 20).show\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n// Dataset\u3067\u306e20\u6b73\u4ee5\u4e0a\u306e\u4eba\u3092\u53d6\u5f97\n// Dataset\u306fDataFrame\u306eRow\u3092JVM\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff08\u3053\u306e\u4f8b\u3067\u306fPerson\uff09\u3068\u3057\u3066\u6271\u3048\u308b\u305f\u3081\u3001UDF\u304c\u9069\u7528\u304c\u7c21\u5358\nscala> ds.filter(_.age >= 20).show\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n// Dataset \u2192\u3000DataFrame\nscala> val df2 = ds.toDF\ndf2: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n\n// Dataset \u2192\u3000RDD\nscala> val rdd = ds.rdd\nrdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[121] at rdd at <console>:33\n\n// \u5c11\u3057\u8907\u96d1\u306a\u51e6\u7406\uff08\u5e74\u4ee3\u5225\u4eba\u6570\u96c6\u8a08\uff09\nscala> import org.apache.spark.sql.types._\n\n// DataFrame\u306e\u5834\u5408\nscala> :paste\ndf.where($\"age\" > 0)\n  .groupBy((($\"age\" / 10) cast IntegerType) * 10 as \"decade\")\n  .agg(count($\"name\"))\n  .orderBy($\"decade\")\n  .show\n\n+------+-----------+\n|decade|count(name)|\n+------+-----------+\n|    10|          1|\n|    30|          1|\n+------+-----------+\n\n// Dataset\u306e\u5834\u5408\nscala> :paste\nds.filter(_.age > 0)\n  .groupBy(p => (p.age / 10) * 10)\n  .agg(count(\"name\"))\n   // orderBy\u304c\u306a\u3044\u3088\u3046\u306a\u306e\u3067\u3001DF\u3078\u5909\u63db\uff08\u3053\u308c\u306f\u3084\u3084\u4e0d\u4fbf\uff09\n  .toDF().withColumnRenamed(\"value\", \"decade\").orderBy(\"decade\") \n  .show\n\n+------+-----------+\n|decade|count(name)|\n+------+-----------+\n|    10|          1|\n|    30|          1|\n+------+-----------+\n\n```\n\n## \u4ed6\u306e\u4f8b\u306fdatabricks\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\nhttps://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/datasets.wordcount.scala.html\n\n## Dataset API\u4ee5\u5916\u306eSpark1.6.0\u306b\u3064\u3044\u3066\u306f\u3053\u3061\u3089\nhttps://docs.cloud.databricks.com/docs/spark/1.6/index.html#00%20Spark%201.6%20Preview.html\n\n# \u307e\u3068\u3081\n* Dataset API\u306fRDD-like\u306b\u4f7f\u3048\u3066DataFrame\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u5229\u70b9\u3082\u6d3b\u304b\u305b\u308b\n* DataFrame\u3084RDD\u3078\u7c21\u5358\u3082\u5909\u63db\u3067\u304d\u308b\n* \u307e\u3060\u5b89\u5b9a\u7248\u3067\u306f\u306a\u3044\u305f\u3081\u3001\u8db3\u308a\u306a\u3044\u30d5\u30a1\u30f3\u30af\u30b7\u30e7\u30f3\u3082\u3042\u308b\u3051\u3069\u3001\u4eca\u5f8c\u306b\u671f\u5f85\n", "tags": ["Spark", "Scala"]}