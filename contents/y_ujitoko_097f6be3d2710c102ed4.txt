{"tags": ["\u6a5f\u68b0\u5b66\u7fd2", "\u6df1\u5c64\u5b66\u7fd2", "MachineLearning", "DeepLearning", "NeuralNetwork"], "context": "Stanford\u306eCS231n\u3068\u3044\u3046\u6388\u696d\u306e\u6559\u6750\u3092\u4f7f\u3063\u3066\u3001\u6a5f\u68b0\u5b66\u7fd2\u3092\u5b66\u3093\u3060\u3002\n\u81ea\u5206\u306e\u30e1\u30e2\u306e\u307e\u3068\u3081\u3002\n\uff08\u5199\u7d4c\u306b\u8fd1\u3044\u306e\u3067\u6ce8\u610f\uff09\n\u3061\u306a\u307f\u306b\u3053\u306eCS231n\u306f\u300c\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeepLearning\u300d\u3068\u3044\u3046\u672c\u306e\u5143\u30cd\u30bf\u306e\u3088\u3046\u3002\n\nModule 1: Neural Networks\n\n\nImage Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits\n\nL1/L2 distances, hyperparameter search, cross-validation\n\n\n\nLinear classification: Support Vector Machine, Softmax\n\nparameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo\n\n\n\nOptimization: Stochastic Gradient Descent\n\noptimization landscapes, local search, learning rate, analytic/numerical gradient\n\n\n\nBackpropagation, Intuitions\n\nchain rule interpretation, real-valued circuits, patterns in gradient flow\n\n\n\nNeural Networks Part 1: Setting up the Architecture\n\nmodel of a biological neuron, activation functions, neural net architecture, representational power\n\n\n\nNeural Networks Part 2: Setting up the Data and the Loss\n\npreprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions\n\n\n\nNeural Networks Part 3: Learning and Evaluation\n\ngradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods,   Adagrad/RMSprop, hyperparameter optimization, model ensembles\n\n\n\nPutting it together: Minimal Neural Network Case Study\n\nminimal 2D toy data example\n\n\n\n\nModule 2: Convolutional Neural Networks\uff08\u66f4\u65b0\u4e88\u5b9a\uff09\n\nConvolutional Neural Networks: Architectures, Convolution / Pooling Layers\n\n\nlayers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations\n\n\nUnderstanding and Visualizing Convolutional Neural Networks\n\n\ntSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons\n\n\nTransfer Learning and Fine-tuning Convolutional Neural Networks\n\n[Stanford\u306eCS231n](http://cs231n.stanford.edu/)\u3068\u3044\u3046\u6388\u696d\u306e\u6559\u6750\u3092\u4f7f\u3063\u3066\u3001\u6a5f\u68b0\u5b66\u7fd2\u3092\u5b66\u3093\u3060\u3002\n\u81ea\u5206\u306e\u30e1\u30e2\u306e\u307e\u3068\u3081\u3002\n\uff08\u5199\u7d4c\u306b\u8fd1\u3044\u306e\u3067\u6ce8\u610f\uff09\n\n\u3061\u306a\u307f\u306b\u3053\u306eCS231n\u306f\u300c\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeepLearning\u300d\u3068\u3044\u3046\u672c\u306e\u5143\u30cd\u30bf\u306e\u3088\u3046\u3002\n\n### Module 1: Neural Networks\n\n* [Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/02/181808)\n    * L1/L2 distances, hyperparameter search, cross-validation\n* [Linear classification: Support Vector Machine, Softmax](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/04/205622)\n    * parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo\n* [Optimization: Stochastic Gradient Descent](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/05/214805)\n    * optimization landscapes, local search, learning rate, analytic/numerical gradient\n* [Backpropagation, Intuitions](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/08/154237)\n    * chain rule interpretation, real-valued circuits, patterns in gradient flow\n* [Neural Networks Part 1: Setting up the Architecture](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/09/114740)\n    * model of a biological neuron, activation functions, neural net architecture, representational power\n* [Neural Networks Part 2: Setting up the Data and the Loss](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/09/221530)\n    * preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions\n* [Neural Networks Part 3: Learning and Evaluation](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/10/235433)\n    * gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods,   Adagrad/RMSprop, hyperparameter optimization, model ensembles\n* [Putting it together: Minimal Neural Network Case Study](http://yusuke-ujitoko.hatenablog.com/entry/2017/01/11/213849) \n    * minimal 2D toy data example\n\n### Module 2: Convolutional Neural Networks\uff08\u66f4\u65b0\u4e88\u5b9a\uff09\n* Convolutional Neural Networks: Architectures, Convolution / Pooling Layers\n    * layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations\n* Understanding and Visualizing Convolutional Neural Networks\n    * tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons\n* Transfer Learning and Fine-tuning Convolutional Neural Networks\n"}