{"context": "This post is for those who already understand the concept of stream processing and basic functionality of Kafka and Spark.\nAnd this post is also for people who are just dying to try Kafka and Spark right away. :)\n\nPurpose\n\nSetup a simple pipeline for stream processing on your local machine.\nIntegrate Spark consumer to the Kafka.\nImplement a word frequency processing pipeline.\n\nThe versions of components in this post will be:\n\nKafka: 0.10.1.0 with scala 2.11\nSpark streaming: 2.10\nSpark streaming Kafka: 2.10\n\n\nSetup\nStep 1 Install Kafka:\nwget http://ftp.meisei-u.ac.jp/mirror/apache/dist/kafka/0.10.1.0/kafka_2.10-0.10.1.0.tgz\ntar -xzf kafka_2.10-0.10.1.0.tgz\ncd kafka_2.10-0.10.1.0.tgz\n\nStep 2 Start a zookeeper for kafka\nbin/zookeeper-server-start.sh config/zookeeper.properties\n\nStep 3 Start kafka\nbin/kafka-server-start.sh config/server.properties\n\nStep 4 Create a topic test on Kafka\nbin/kafka-topics.sh \\\n--create \\\n--zookeeper localhost:2181 \\\n--replication-factor 1 \\\n--partitions 1 \\\n--topic test\n\nStep 5 Program a spark consumer for word frequency using scala:\nsbt file:\nname := \"spark-playground\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"2.1.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kafka-0-10\" % \"2.1.0\"\n\nmain file:\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\n\nobject AQuickExample extends App {\n  val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"AQuickExample\")\n\n  val kafkaParams = Map[String, Object](\n    \"bootstrap.servers\" -> \"localhost:9092\",\n    \"key.deserializer\" -> classOf[StringDeserializer],\n    \"value.deserializer\" -> classOf[StringDeserializer],\n    \"group.id\" -> \"spark-playground-group\",\n    \"auto.offset.reset\" -> \"latest\",\n    \"enable.auto.commit\" -> (false: java.lang.Boolean)\n  )\n  val ssc = new StreamingContext(conf, Seconds(1))\n\n\n  val inputStream = KafkaUtils.createDirectStream(ssc, PreferConsistent, Subscribe[String, String](Array(\"test\"), kafkaParams))\n  val processedStream = inputStream\n    .flatMap(record => record.value.split(\" \"))\n    .map(x => (x, 1))\n    .reduceByKey((x, y) => x + y)\n\n  processedStream.print()\n  ssc.start()\n  ssc.awaitTermination()\n\n}\n\nStep 6 Start the Spark consumer you wrote in step 5.\nsbt run\n\nStep 7 Start a console Kafka producer and fire some message to the Kafka using the topic test.\nStart the console producer:\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\nSend some message to the Kafka:\nhello world\n\nStep 8 Once you send the message, there will be something showing up in your Spark consumer terminal representing the word frequency:\n-------------------------------------------\nTime: 1485135467000 ms\n-------------------------------------------\n(hello,1)\n(world,1)\n\n...\n\n***This post is for those who already understand the concept of stream processing and basic functionality of Kafka and Spark.***\n***And this post is also for people who are just dying to try Kafka and Spark right away. :)***\n\n# Purpose\n\n- Setup a simple pipeline for stream processing on your local machine.\n- Integrate Spark consumer to the Kafka.\n- Implement a word frequency processing pipeline.\n\nThe versions of components in this post will be:\n\n- Kafka: 0.10.1.0 with scala 2.11\n- Spark streaming: 2.10\n- Spark streaming Kafka: 2.10\n\n# Setup\n\n***Step 1*** Install Kafka:\n\n```\nwget http://ftp.meisei-u.ac.jp/mirror/apache/dist/kafka/0.10.1.0/kafka_2.10-0.10.1.0.tgz\ntar -xzf kafka_2.10-0.10.1.0.tgz\ncd kafka_2.10-0.10.1.0.tgz\n```\n\n***Step 2*** Start a zookeeper for kafka\n\n```\nbin/zookeeper-server-start.sh config/zookeeper.properties\n```\n\n***Step 3*** Start kafka\n\n```\nbin/kafka-server-start.sh config/server.properties\n```\n\n***Step 4*** Create a topic *test* on Kafka\n\n```\nbin/kafka-topics.sh \\\n--create \\\n--zookeeper localhost:2181 \\\n--replication-factor 1 \\\n--partitions 1 \\\n--topic test\n```\n\n***Step 5*** Program a spark consumer for word frequency using scala:\n\nsbt file:\n\n```scala\nname := \"spark-playground\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.8\"\n\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"2.1.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kafka-0-10\" % \"2.1.0\"\n```\n\nmain file:\n\n```scala\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\n\nobject AQuickExample extends App {\n  val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"AQuickExample\")\n\n  val kafkaParams = Map[String, Object](\n    \"bootstrap.servers\" -> \"localhost:9092\",\n    \"key.deserializer\" -> classOf[StringDeserializer],\n    \"value.deserializer\" -> classOf[StringDeserializer],\n    \"group.id\" -> \"spark-playground-group\",\n    \"auto.offset.reset\" -> \"latest\",\n    \"enable.auto.commit\" -> (false: java.lang.Boolean)\n  )\n  val ssc = new StreamingContext(conf, Seconds(1))\n\n\n  val inputStream = KafkaUtils.createDirectStream(ssc, PreferConsistent, Subscribe[String, String](Array(\"test\"), kafkaParams))\n  val processedStream = inputStream\n    .flatMap(record => record.value.split(\" \"))\n    .map(x => (x, 1))\n    .reduceByKey((x, y) => x + y)\n\n  processedStream.print()\n  ssc.start()\n  ssc.awaitTermination()\n\n}\n```\n\n\n***Step 6*** Start the Spark consumer you wrote in step 5.\n\n```\nsbt run\n```\n\n***Step 7*** Start a console Kafka producer and fire some message to the Kafka using the topic *test*.\n\nStart the console producer:\n\n```\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n```\n\nSend some message to the Kafka:\n\n```\nhello world\n```\n\n***Step 8*** Once you send the message, there will be something showing up in your Spark consumer terminal representing the word frequency:\n\n```\n-------------------------------------------\nTime: 1485135467000 ms\n-------------------------------------------\n(hello,1)\n(world,1)\n\n...\n```\n", "tags": ["Kafka", "Spark", "StreamProcessing", "Scala"]}