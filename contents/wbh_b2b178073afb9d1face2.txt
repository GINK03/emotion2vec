{"tags": ["Python", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "Chainer", "\u611f\u60c5\u5206\u6790"], "context": "\n\n\u6982\u8981\n\u524d\u56de\u306fChainer\u306e\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u3067\u3042\u308b\u3001MLP(\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3)\u306e\u5b9f\u88c5\u3068\u3001CNN\u306e\u7573\u307f\u8fbc\u307f\u5c64\u304b\u3089\u5168\u7d50\u5408\u5c64\u306e\u30ce\u30fc\u30c9\u6570\u3092\u51fa\u3059\u305f\u3081\u306b\u5fc5\u8981\u306a\u8a08\u7b97\u5f0f\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306f\u3001\u5b9f\u969b\u306eTwitter\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u3093\u3067\u3001CNN\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8aac\u660e\u306f\u3053\u3053\u304c\u5206\u304b\u308a\u3084\u3059\u304b\u3063\u305f\u3067\u3059\u3002\n\n2\u6b21\u5143\u306e\u753b\u50cf\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u9069\u7528\u3057\u3001\u7279\u5fb4\u91cf\u3092\u5727\u7e2e\u3057\u307e\u3059\u3002\u305d\u306e\u5f8c\u3001\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3001\u3055\u3089\u306b\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002\n\u305f\u3076\u3093\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u306f\uff11\u7a2e\u985e\u3067\u306f\u306a\u304f\u3066\u3001\u51fa\u529b\u3057\u305f\u3044\u679a\u6570\u5206\u7570\u306a\u308b\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u9069\u7528\u3059\u308b\u5f62\u306b\u306a\u308b\u3093\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u76ee\u6a19\n\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3001\u3053\u306e\u8ad6\u6587\u3092\u307e\u306d\u3066\u307f\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002\n\n\u51e6\u7406\u306e\u6982\u8981\n\u51e6\u7406\u306e\u6982\u8981\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n\nTweet\u30c7\u30fc\u30bf\u306e\u8aad\u8fbc\nWord Embed(\u4eca\u56de\u306fWord2Vec\u3092\u4f7f\u7528)\n\u5b66\u7fd2\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u5206\u5272\nCNN\u306e\u5b9a\u7fa9\nCNN\u3067\u306e\u5b66\u7fd2\nCNN\u3067\u306e\u4e88\u6e2c\n\n\u306a\u304a\u3001CNN\u306e\u5b9a\u7fa9\u3068\u5b66\u7fd2\u306f\u8ad6\u6587\u306b\u8a18\u8f09\u306e\u4ee5\u4e0b\u306e\u65b9\u6cd5\u3092\u3068\u308a\u307e\u3059\u3002\n\n\u3053\u306e\u56f3\u306f\u30011\u679a\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u306b\u5bfe\u3059\u308b\u7573\u307f\u8fbc\u307f\u3068\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u3092\u63cf\u3044\u3066\u3044\u3066\u3001\u4e00\u756a\u5de6\u306e\u5927\u304d\u306a\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306fddd\u304cWord\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u3001sss\u304c\u30bb\u30f3\u30c6\u30f3\u30b9\u4e2d\u306e\u5358\u8a9e\u306e\u6570\u306b\u306a\u308a\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u30d5\u30a3\u30eb\u30bf\u30b5\u30a4\u30ba\u306f\u5bfe\u79f0\u884c\u5217\u3067\u306f\u306a\u304f\u3001d\u00d7md\u00d7md\u00d7m\u6b21\u5143\u306e\u975e\u5bfe\u79f0\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u306a\u308a\u307e\u3059\u3002\n\u305f\u3060\u3001\u8ad6\u6587\u3092\u8aad\u3093\u3067\u3082\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3059\u304c\u3001sss\u3063\u3066\u30bb\u30f3\u30c6\u30f3\u30b9\u3054\u3068\u306b\u9055\u3046\u306e\u3067\u3001\u305d\u306e\u3078\u3093\u3069\u3046\u3084\u3063\u3066\u308b\u306e\u304b\u306a\u3041\u3068\u601d\u3063\u3066\u3044\u305f\u3089\u3001\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3067\u306f\u3001\u5168Tweet\u306b\u73fe\u308c\u308b\u5404\u30bb\u30f3\u30c6\u30f3\u30b9\u306e\u6700\u5927\u5358\u8a9e\u6570\u3092\u3068\u3063\u3066\u3044\u307e\u3057\u305f\u306e\u3067\u3001\u3053\u308c\u3092\u307e\u306d\u307e\u3059\u3002\n\n\u30c7\u30fc\u30bf\u53d6\u5f97\nTweet\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\u30c7\u30fc\u30bf\u306f\u30011\u5217\u76ee\u304c[0,1]\u306e\u30d5\u30e9\u30b0\u306b\u306a\u3063\u3066\u304a\u308a\u30012\u5217\u76ee\u304c\u82f1\u8a9e\u3067\u306eTweet\u306b\u306a\u308a\u307e\u3059\u3002\n\nWord Embed\n2\u6b21\u5143\u306e\u753b\u50cf\u5f62\u5f0f\u3067\u30bb\u30f3\u30c6\u30f3\u30b9\u3092\u4fdd\u6301\u3057\u305f\u3044\u305f\u3081\u3001\u5206\u6563\u8868\u73fe\u306b\u76f4\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306fChainer\u4ed8\u5c5e\u306eEmbedID\u304c\u3046\u307e\u304f\u52d5\u304b\u306a\u304b\u3063\u305f\u306e\u3067\u3001gensim\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3042\u308bWord2Vec\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\u307e\u305a\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u304b\u3089Word\u306bID\u3092\u632f\u308a\u307e\u3059\u3002\n#! -*- coding:utf-8 -*-\n\ndef read(inp_file,num_sent=None):\n        f_in = open(inp_file, 'r')\n        lines = f_in.readlines()\n\n        words_map = {}\n        word_cnt = 0\n\n        k_wrd = 5 #\u5358\u8a9e\u30b3\u30f3\u30c6\u30af\u30b9\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n        y = []\n        x_wrd = []\n\n        if num_sent is None:\n            num_sent = len(lines)\n            max_sen_len = 0\n        else:\n            max_sen_len, num_sent = 0, num_sent\n\n\n        words_vocab_mat = []\n\n        token_list = []\n\n        for line in lines[:num_sent]:\n            words = line[:-1].split()\n            tokens = words[1:]\n            y.append(int(float(words[0])))\n            max_sen_len = max(max_sen_len,len(tokens))\n            for token in tokens:\n                if token not in words_map:\n                    words_map[token] = word_cnt\n                    token_list.append(token)\n                    word_cnt += 1\n            words_vocab_mat.append(tokens)\n\n        cnt = 0\n        for line in lines[:num_sent]:\n            words = line[:-1].split()\n            cnt += 1\n            tokens = words[1:]\n            word_mat = [-1] * (max_sen_len+k_wrd-1)\n\n            for i in xrange(len(tokens)):\n                word_mat[(k_wrd/2)+i] = words_map[tokens[i]]\n            x_wrd.append(word_mat)\n        max_sen_len += k_wrd-1\n\n        # num_sent: \u6587\u66f8\u306e\u6570\n        # word_cnt: \u5358\u8a9e\u306e\u7a2e\u985e\u6570\n        # max_sen_len: \u6587\u66f8\u306e\u6700\u5927\u306e\u9577\u3055\n        # x_wrd: \u5165\u529b\u3068\u306a\u308b\u5358\u8a9e\u306eid\u5217 \u884c\u6570 : \u30bb\u30f3\u30c6\u30f3\u30b9\u6570(num_sent) \u5217\u6570 : \u6587\u66f8\u306e\u6700\u5927\u306e\u9577\u3055(max_sen_len)\n        # k_wrd: window size\n        # words_map : key = word,value = id\n        # y: 1 or 0 (i.e., positive or negative)\n        # words_vocab_mat : sentence\u3092\u5206\u89e3\u3057\u305f\u3082\u306e\u3001\u884c\u6570\u306f\u30bb\u30f3\u30c6\u30f3\u30b9\u6570\u3001\u5217\u6570\u306f\u53ef\u5909\u3067\u5358\u8a9e\u6570\n        # token_list : token\u306e\u30ea\u30b9\u30c8\u3001index\u304cid\u306b\u5bfe\u5fdc\n        data = (num_sent, word_cnt, max_sen_len, k_wrd, x_wrd, y,words_map,words_vocab_mat,token_list)\n        return data\n\n(num_sent, word_cnt, max_sen_len, k_wrd, x_wrd, y,words_map,sentences,token_list) = load.read(\"data/tweets_clean.txt\",10000)\n\nx_wrd\u306f\u30bb\u30f3\u30c6\u30f3\u30b9\u6570\u00d7\u6700\u5927\u6587\u66f8\u9577\u306e\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3067\u3001\u5404\u8981\u7d20\u306f\u73fe\u308c\u305f\u5358\u8a9e\u306eID\u306b\u306a\u308a\u307e\u3059\u3002\n\u3042\u3068\u3067\u5fc5\u8981\u306a\u306e\u3067\u3001words_map\u3068token_list\u3068words_vocab_mat\u3082\u7528\u610f\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u6b21\u306bWord2Vec\u3092\u4f7f\u3063\u3066\u5404\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u3092\u5f97\u305f\u306e\u3061\u3001\u300c\u30bb\u30f3\u30c6\u30f3\u30b9\u753b\u50cf\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u300d(\u52dd\u624b\u306b\u3064\u3051\u305f)\u3092\u4f5c\u308a\u307e\u3059\u3002\n    \"\"\"Word2Vec\u3067\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u4f5c\u6210\u3059\u308b\"\"\"\n    word_dimension = 200\n    from gensim.models import Word2Vec\n    model_w2v = Word2Vec(sentences,seed=123,size=word_dimension,min_count=0,window=5)\n    sentence_image_matrix = np.zeros((len(sentences),1,word_dimension,max_sen_len)) #Convolution\u3059\u308b\u305f\u3081\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u753b\u50cf\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306e\u521d\u671f\u5316\n\n    \"\"\"x_wrd\u306b\u5bfe\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210\u3059\u308b\"\"\"\n    for i in range(0,len(x_wrd)):\n        tmp_id_list = x_wrd[i,:]\n        for j in range(0,len(tmp_id_list)):\n            \"\"\"1\u884c\u306b\u5bfe\u3057\u3066\u56de\u3059\"\"\"\n            id = tmp_id_list[j]\n            if id == -1:\n                \"\"\"\u60c5\u5831\u306a\u3057\"\"\"\n                sentence_image_matrix[i,0,:,j] = [0.] * word_dimension #0\u30d9\u30af\u30c8\u30eb\u3092\u5165\u308c\u308b\n            else:\n                target_word = token_list[id]\n                sentence_image_matrix[i,0,:,j] = model_w2v[target_word]\n\nsentence_image_matrix\u306f(\u30bb\u30f3\u30c6\u30f3\u30b9\u6570,1,\u30d9\u30af\u30c8\u30eb\u6b21\u5143=200,\u6700\u5927\u6587\u7ae0\u9577)\u306e\u5927\u304d\u3055\u306e4\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n\u5b66\u7fd2\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u5206\u5272\n\u521d\u3081\u3066\u77e5\u3063\u305f\u306e\u3067\u3059\u304c\u30014\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u306b\u5bfe\u3057\u3066\u3082sklearn\u306etrain_test_split\u304c\u4f7f\u3048\u307e\u3059\u3002\n\u305f\u3076\u3093\u3001\u7b2c1\u6b21\u5143\u3057\u304b\u898b\u3066\u3044\u306a\u3044\u304b\u3089\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n    \"\"\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\"\"\"\n    sentence_image_matrix = np.array(sentence_image_matrix,dtype=np.float32)\n    N = len(sentence_image_matrix)\n    t_n = 0.33\n    x_train,x_test,y_train,y_test = train_test_split(sentence_image_matrix,y,test_size=t_n,random_state=123)\n\n\nCNN\u306e\u5b9a\u7fa9\n\u554f\u984c\u306fCNN\u306e\u5b9a\u7fa9\u3067\u3059\u3002\n\u4eca\u56de\u8ad6\u6587\u3067\u306f\u3001\u975e\u5bfe\u79f0\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u4f7f\u3063\u3066\u304a\u308a\u3001\u3055\u3089\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u3082\u975e\u5bfe\u79f0\u3067\u3059\u306e\u3067\u3001\u305d\u306e\u8fba\u3092\u8003\u616e\u3057\u306a\u3044\u3068\u3044\u3051\u307e\u305b\u3093\u3002\n\u3059\u308b\u3068\u3053\u3093\u306a\u611f\u3058\u306b\u306a\u308a\u307e\u3059\u3002\nclass CNNFiltRow(ChainerClassifier):\n    \"\"\"\n    CNN\u306e\u884c\u65b9\u5411\u3092\u5168\u90e8\u30d5\u30a3\u30eb\u30bf\u30fc\u3068\u3057\u3066\u3001\u5217\u65b9\u5411\u306b\u52d5\u304b\u3059\u30d1\u30bf\u30fc\u30f3\n    \"\"\"\n\n    def _setup_network(self, **params):\n        self.input_dim = params[\"input_dim\"] #1\u679a\u306e\u753b\u50cf\u306e\u5217\u65b9\u5411\u306e\u6b21\u5143\n        self.in_channels = params[\"in_channels\"] #input channels : default = 1\n        self.out_channels = params[\"out_channels\"] #out_channels : \u4efb\u610f\n        self.row_dim = params[\"row_dim\"] #1\u679a\u306e\u753b\u50cf\u306e\u884c\u65b9\u5411\u306e\u6b21\u5143 = Filter\u306e\u884c\u6570\u306b\u306a\u308b\n        self.filt_clm = params[\"filt_clm\"] #Filter\u306e\u5217\u6570\n        self.pooling_row = params[\"pooling_row\"] if params.has_key(\"pooling_row\") else 1 #pooling\u306e\u884c\u6570 : default = 1\n        self.pooling_clm = params[\"pooling_clm\"] if params.has_key(\"pooling_clm\") else int(self.input_dim - 2 * math.floor(self.filt_clm/2.)) #Pooling\u306e\u5217\u6570 : default = math.floor((self.input_dim - 2 * math.floor(self.filt_clm/2.))\n        self.batch_size = params[\"batch_size\"] if params.has_key(\"batch_size\") else 100\n        self.hidden_dim = params[\"hidden_dim\"]\n        self.n_classes = params[\"n_classes\"]\n\n        self.conv1_out_dim = math.floor((self.input_dim - 2 * math.floor(self.filt_clm/2.))/self.pooling_clm)\n        network = FunctionSet(\n            conv1 = F.Convolution2D(self.in_channels,self.out_channels,(self.row_dim,self.filt_clm)), #Filter\u3092\u975e\u5bfe\u79f0\u306b\u3057\u305f\n            l1=F.Linear(self.conv1_out_dim*self.out_channels, self.hidden_dim),\n            l2=F.Linear(self.hidden_dim, self.hidden_dim),\n            l3=F.Linear(self.hidden_dim, self.n_classes),\n        )\n        return network\n\n    def forward(self, x, train=True):\n        h = F.max_pooling_2d(F.relu(self.network.conv1(x)), (self.pooling_row,self.pooling_clm))\n        h1 = F.dropout(F.relu(self.network.l1(h)),train=train)\n        h2 = F.dropout(F.relu(self.network.l2(h1)),train=train)\n        y = self.network.l3(h2)\n        return y\n\n    def output_func(self, h):\n        return F.softmax(h)\n\n    def loss_func(self, y, t):\n        return F.softmax_cross_entropy(y, t)\n\n    def fit(self, x_data, y_data):\n        batchsize = self.batch_size\n        N = len(y_data)\n        for loop in range(self.n_iter):\n            perm = np.random.permutation(N)\n            sum_accuracy = 0\n            sum_loss = 0\n            for i in six.moves.range(0, N, batchsize):\n                x_batch = x_data[perm[i:i + batchsize]]\n                y_batch = y_data[perm[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                self.optimizer.zero_grads()\n                yp = self.forward(x)\n                loss = self.loss_func(yp,y)\n                loss.backward()\n                self.optimizer.update()\n                sum_loss += loss.data * len(y_batch)\n                sum_accuracy += F.accuracy(yp,y).data * len(y_batch)\n            if self.report > 0 and loop % self.report == 0:\n                print('loop={}, train mean loss={} , train mean accuracy={}'.format(loop, sum_loss / N,sum_accuracy / N))\n\n        return self\n\n    def fit_test(self, x_data, y_data,x_test,y_test):\n        batchsize = self.batch_size\n        N = len(y_data)\n        Nt = len(y_test)\n        train_ac = []\n        test_ac = []\n        for loop in range(self.n_iter):\n            perm = np.random.permutation(N)\n            permt = np.random.permutation(Nt)\n            sum_accuracy = 0\n            sum_loss = 0\n\n            sum_accuracy_t = 0\n\n            \"\"\"\u5b66\u7fd2\u30d5\u30a7\u30fc\u30ba\"\"\"\n            for i in six.moves.range(0, N, batchsize):\n                x_batch = x_data[perm[i:i + batchsize]]\n                y_batch = y_data[perm[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                self.optimizer.zero_grads()\n                yp = self.forward(x)\n                loss = self.loss_func(yp,y)\n                loss.backward()\n                self.optimizer.update()\n                sum_loss += loss.data * len(y_batch)\n                sum_accuracy += F.accuracy(yp,y).data * len(y_batch)\n\n            \"\"\"\u30c6\u30b9\u30c8\u30d5\u30a7\u30fc\u30ba\"\"\"\n            for i in six.moves.range(0,Nt,batchsize):\n                x_batch = x_test[permt[i:i + batchsize]]\n                y_batch = y_test[permt[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                yp = self.forward(x,False)\n                sum_accuracy_t += F.accuracy(yp,y).data * len(y_batch)\n\n            if self.report > 0 and loop % self.report == 0:\n                print('loop={}, train mean loss={} , train mean accuracy={} , test mean accuracy={}'.format(loop, sum_loss / N,sum_accuracy / N,sum_accuracy_t / Nt))\n\n            train_ac.append(sum_accuracy / N)\n            test_ac.append(sum_accuracy_t / Nt)\n\n        return self,train_ac,test_ac\n\nChainerClassifier\u306b\u3064\u3044\u3066\u306f\u524d\u56de\u306e\u8a18\u4e8b\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\nCNN\u306e\u5b66\u7fd2\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u7cbe\u5ea6\u3082\u898b\u305f\u304b\u3063\u305f\u306e\u3067fit_test\u30e1\u30bd\u30c3\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n    \"\"\"CNN Filter Row\u306e\u5b66\u7fd2\"\"\"\n    n_iter = 200\n    report = 5\n    params = {\"input_dim\":max_sen_len,\"in_channels\":1,\"out_channels\":20,\"row_dim\":word_dimension,\"filt_clm\":3,\"batch_size\":100,\"hidden_dim\":300,\"n_classes\":2}\n    cnn = CNNFiltRow(n_iter=n_iter,report=report,**params)\n    cnn,train_ac,test_ac = cnn.fit_test(x_train,y_train,x_test,y_test)\n\n\u5b66\u7fd2\u306e\u969b\u306e\u7cbe\u5ea6\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u7cbe\u5ea6\u306e\u30d7\u30ed\u30c3\u30c8\u306f\u4ee5\u4e0b\u3002\nIter = 100\u304f\u3089\u3044\u304b\u3089\u904e\u5b66\u7fd2\u304c\u59cb\u307e\u308b\u611f\u3058\u306e\u3088\u3046\u3067\u3042\u308b\u3002\n\u305d\u308c\u3067\u3082\u6c4e\u5316\u6027\u80fd\u304c\u9ad8\u3044\u5370\u8c61\u3002\n\n\nCNN\u306e\u4e88\u6e2c\n\u6700\u7d42\u7684\u306b\u51fa\u6765\u4e0a\u304c\u3063\u305f\u30e2\u30c7\u30eb\u306b\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5165\u308c\u3066\u307f\u3066\u5404\u6307\u6a19\u3092\u51fa\u3057\u3066\u307f\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002\n[CNN]P AUC: 0.80 Pres: 0.66 Recl: 0.89 Fscr: 0.76\n\nFscore\u30670.76\u3067AUC\u30670.8\u3068\u7d50\u69cb\u826f\u3044\u611f\u3058\u306b\u306a\u3063\u305f\u3002\n\n\u4ed6\u306e\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\n\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u3057\u3066Random Forest\u3068MLP(\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3)\u3067\u3082\u540c\u3058\u3053\u3068\u3092\u3084\u3063\u3066\u307f\u308b\u3002\n\u5165\u529b\u30c7\u30fc\u30bf\u306f\u3053\u306e\u5834\u54082\u6b21\u5143\u3067\u306f\u306a\u3044\u306e\u3067\u3001MNIST\u306e\u3088\u3046\u306b1\u6b21\u5143\u306b\u76f4\u3057\u3066\u3044\u308b\u3002\n\u305d\u306e\u7d50\u679c\u3001\u540c\u69d8\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u5404\u7a2e\u6307\u6a19\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n[RF ]P AUC: 0.71 Pres: 0.65 Recl: 0.60 Fscr: 0.62\n[MLP]P AUC: 0.71 Pres: 0.64 Recl: 0.69 Fscr: 0.67\n[CNN]P AUC: 0.80 Pres: 0.66 Recl: 0.89 Fscr: 0.76\n\nCNN\u306e\u3053\u306e\u5727\u5012\u7684\u6027\u80fd\u5dee\u2026\n\n\u307e\u3068\u3081\n\n\u304a\u304a\u3080\u306dCNN\u306e\u6319\u52d5\u306f\u7406\u89e3\u3067\u304d\u305f\u3002\nBoW\u3067\u3084\u3063\u305f\u306e\u3082\u306e\u3082\u3042\u308b\u304c\u3001\u7cbe\u5ea6\u306f\u3053\u3053\u307e\u3067\u51fa\u306a\u304b\u3063\u305f\u3002\n\u9069\u5207\u306a\u5206\u6563\u8868\u73fe\u306f\u5fc5\u8981\u304b\u3068\u601d\u3046\u3002\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4eca\u56de\u7f6e\u304d\u3067\u3084\u3063\u305f\u304c\u3001\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u304d\u308b\u306a\u3089\u3084\u308a\u305f\u3044(\u8a08\u7b97\u6a5f\u30d1\u30ef\u30fc\u304c\u8db3\u308a\u306a\u3044)\n\u3042\u3068\u306f\u65e5\u672c\u8a9e\u3078\u306e\u62e1\u5f35\u304b\u3002\nLSTM\u3092\u4f7f\u3063\u305fRNN\u3082\u8a66\u3057\u3066\u307f\u305f\u3044\u3002\n\n\n# \u6982\u8981\n\n[\u524d\u56de](http://qiita.com/hironishi/items/e07fd287393cab7211e5)\u306fChainer\u306e\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\u3067\u3042\u308b\u3001MLP(\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3)\u306e\u5b9f\u88c5\u3068\u3001CNN\u306e\u7573\u307f\u8fbc\u307f\u5c64\u304b\u3089\u5168\u7d50\u5408\u5c64\u306e\u30ce\u30fc\u30c9\u6570\u3092\u51fa\u3059\u305f\u3081\u306b\u5fc5\u8981\u306a\u8a08\u7b97\u5f0f\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\n\n\u4eca\u56de\u306f\u3001\u5b9f\u969b\u306eTwitter\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u3093\u3067\u3001CNN\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n# \u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8aac\u660e\u306f[\u3053\u3053](http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/)\u304c\u5206\u304b\u308a\u3084\u3059\u304b\u3063\u305f\u3067\u3059\u3002\n\n![image](https://qiita-image-store.s3.amazonaws.com/0/54539/fe3ea3e0-72fc-e988-aaab-4cb75dee0bb0.png)\n\n2\u6b21\u5143\u306e\u753b\u50cf\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u9069\u7528\u3057\u3001\u7279\u5fb4\u91cf\u3092\u5727\u7e2e\u3057\u307e\u3059\u3002\u305d\u306e\u5f8c\u3001\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3001\u3055\u3089\u306b\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002\n\u305f\u3076\u3093\u3001\u30d5\u30a3\u30eb\u30bf\u30fc\u306f\uff11\u7a2e\u985e\u3067\u306f\u306a\u304f\u3066\u3001\u51fa\u529b\u3057\u305f\u3044\u679a\u6570\u5206\u7570\u306a\u308b\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u9069\u7528\u3059\u308b\u5f62\u306b\u306a\u308b\u3093\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\n# \u76ee\u6a19\n\n[\u3053\u3061\u3089\u306e\u8a18\u4e8b](http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6)\u3092\u53c2\u8003\u306b\u3001[\u3053\u306e\u8ad6\u6587](http://alt.qcri.org/semeval2015/cdrom/pdf/SemEval079.pdf)\u3092\u307e\u306d\u3066\u307f\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002\n\n# \u51e6\u7406\u306e\u6982\u8981\n\u51e6\u7406\u306e\u6982\u8981\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n1. [Tweet\u30c7\u30fc\u30bf](https://raw.githubusercontent.com/satwantrana/CharSCNN/master/tweets_clean.txt)\u306e\u8aad\u8fbc\n2. Word Embed(\u4eca\u56de\u306fWord2Vec\u3092\u4f7f\u7528)\n3. \u5b66\u7fd2\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u5206\u5272\n4. CNN\u306e\u5b9a\u7fa9\n5. CNN\u3067\u306e\u5b66\u7fd2\n6. CNN\u3067\u306e\u4e88\u6e2c\n\n\u306a\u304a\u3001CNN\u306e\u5b9a\u7fa9\u3068\u5b66\u7fd2\u306f\u8ad6\u6587\u306b\u8a18\u8f09\u306e\u4ee5\u4e0b\u306e\u65b9\u6cd5\u3092\u3068\u308a\u307e\u3059\u3002\n\n![image](https://qiita-image-store.s3.amazonaws.com/0/54539/aea3769a-387b-74f6-3324-7b1618b49a1c.png)\n\n\u3053\u306e\u56f3\u306f\u30011\u679a\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u306b\u5bfe\u3059\u308b\u7573\u307f\u8fbc\u307f\u3068\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u3092\u63cf\u3044\u3066\u3044\u3066\u3001\u4e00\u756a\u5de6\u306e\u5927\u304d\u306a\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306f$d$\u304cWord\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u3001$s$\u304c\u30bb\u30f3\u30c6\u30f3\u30b9\u4e2d\u306e\u5358\u8a9e\u306e\u6570\u306b\u306a\u308a\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u30d5\u30a3\u30eb\u30bf\u30b5\u30a4\u30ba\u306f\u5bfe\u79f0\u884c\u5217\u3067\u306f\u306a\u304f\u3001$d\u00d7m$\u6b21\u5143\u306e\u975e\u5bfe\u79f0\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u305f\u3060\u3001\u8ad6\u6587\u3092\u8aad\u3093\u3067\u3082\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3059\u304c\u3001$s$\u3063\u3066\u30bb\u30f3\u30c6\u30f3\u30b9\u3054\u3068\u306b\u9055\u3046\u306e\u3067\u3001\u305d\u306e\u3078\u3093\u3069\u3046\u3084\u3063\u3066\u308b\u306e\u304b\u306a\u3041\u3068\u601d\u3063\u3066\u3044\u305f\u3089\u3001[\u3053\u3061\u3089\u306e\u8a18\u4e8b](http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6)\u3067\u306f\u3001\u5168Tweet\u306b\u73fe\u308c\u308b\u5404\u30bb\u30f3\u30c6\u30f3\u30b9\u306e\u6700\u5927\u5358\u8a9e\u6570\u3092\u3068\u3063\u3066\u3044\u307e\u3057\u305f\u306e\u3067\u3001\u3053\u308c\u3092\u307e\u306d\u307e\u3059\u3002\n\n# \u30c7\u30fc\u30bf\u53d6\u5f97\n\n[Tweet\u30c7\u30fc\u30bf](https://raw.githubusercontent.com/satwantrana/CharSCNN/master/tweets_clean.txt)\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\u30c7\u30fc\u30bf\u306f\u30011\u5217\u76ee\u304c[0,1]\u306e\u30d5\u30e9\u30b0\u306b\u306a\u3063\u3066\u304a\u308a\u30012\u5217\u76ee\u304c\u82f1\u8a9e\u3067\u306eTweet\u306b\u306a\u308a\u307e\u3059\u3002\n\n# Word Embed\n\n2\u6b21\u5143\u306e\u753b\u50cf\u5f62\u5f0f\u3067\u30bb\u30f3\u30c6\u30f3\u30b9\u3092\u4fdd\u6301\u3057\u305f\u3044\u305f\u3081\u3001\u5206\u6563\u8868\u73fe\u306b\u76f4\u3057\u307e\u3059\u3002\n\u4eca\u56de\u306fChainer\u4ed8\u5c5e\u306eEmbedID\u304c\u3046\u307e\u304f\u52d5\u304b\u306a\u304b\u3063\u305f\u306e\u3067\u3001gensim\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3042\u308bWord2Vec\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u307e\u305a\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u304b\u3089Word\u306bID\u3092\u632f\u308a\u307e\u3059\u3002\n\n```python\n#! -*- coding:utf-8 -*-\n\ndef read(inp_file,num_sent=None):\n        f_in = open(inp_file, 'r')\n        lines = f_in.readlines()\n\n        words_map = {}\n        word_cnt = 0\n\n        k_wrd = 5 #\u5358\u8a9e\u30b3\u30f3\u30c6\u30af\u30b9\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n        y = []\n        x_wrd = []\n\n        if num_sent is None:\n            num_sent = len(lines)\n            max_sen_len = 0\n        else:\n            max_sen_len, num_sent = 0, num_sent\n\n\n        words_vocab_mat = []\n\n        token_list = []\n\n        for line in lines[:num_sent]:\n            words = line[:-1].split()\n            tokens = words[1:]\n            y.append(int(float(words[0])))\n            max_sen_len = max(max_sen_len,len(tokens))\n            for token in tokens:\n                if token not in words_map:\n                    words_map[token] = word_cnt\n                    token_list.append(token)\n                    word_cnt += 1\n            words_vocab_mat.append(tokens)\n\n        cnt = 0\n        for line in lines[:num_sent]:\n            words = line[:-1].split()\n            cnt += 1\n            tokens = words[1:]\n            word_mat = [-1] * (max_sen_len+k_wrd-1)\n\n            for i in xrange(len(tokens)):\n                word_mat[(k_wrd/2)+i] = words_map[tokens[i]]\n            x_wrd.append(word_mat)\n        max_sen_len += k_wrd-1\n\n        # num_sent: \u6587\u66f8\u306e\u6570\n        # word_cnt: \u5358\u8a9e\u306e\u7a2e\u985e\u6570\n        # max_sen_len: \u6587\u66f8\u306e\u6700\u5927\u306e\u9577\u3055\n        # x_wrd: \u5165\u529b\u3068\u306a\u308b\u5358\u8a9e\u306eid\u5217 \u884c\u6570 : \u30bb\u30f3\u30c6\u30f3\u30b9\u6570(num_sent) \u5217\u6570 : \u6587\u66f8\u306e\u6700\u5927\u306e\u9577\u3055(max_sen_len)\n        # k_wrd: window size\n        # words_map : key = word,value = id\n        # y: 1 or 0 (i.e., positive or negative)\n        # words_vocab_mat : sentence\u3092\u5206\u89e3\u3057\u305f\u3082\u306e\u3001\u884c\u6570\u306f\u30bb\u30f3\u30c6\u30f3\u30b9\u6570\u3001\u5217\u6570\u306f\u53ef\u5909\u3067\u5358\u8a9e\u6570\n        # token_list : token\u306e\u30ea\u30b9\u30c8\u3001index\u304cid\u306b\u5bfe\u5fdc\n        data = (num_sent, word_cnt, max_sen_len, k_wrd, x_wrd, y,words_map,words_vocab_mat,token_list)\n        return data\n\n(num_sent, word_cnt, max_sen_len, k_wrd, x_wrd, y,words_map,sentences,token_list) = load.read(\"data/tweets_clean.txt\",10000)\n```\n\nx_wrd\u306f\u30bb\u30f3\u30c6\u30f3\u30b9\u6570\u00d7\u6700\u5927\u6587\u66f8\u9577\u306e\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3067\u3001\u5404\u8981\u7d20\u306f\u73fe\u308c\u305f\u5358\u8a9e\u306eID\u306b\u306a\u308a\u307e\u3059\u3002\n\u3042\u3068\u3067\u5fc5\u8981\u306a\u306e\u3067\u3001words_map\u3068token_list\u3068words_vocab_mat\u3082\u7528\u610f\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u6b21\u306bWord2Vec\u3092\u4f7f\u3063\u3066\u5404\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u3092\u5f97\u305f\u306e\u3061\u3001\u300c\u30bb\u30f3\u30c6\u30f3\u30b9\u753b\u50cf\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u300d(\u52dd\u624b\u306b\u3064\u3051\u305f)\u3092\u4f5c\u308a\u307e\u3059\u3002\n\n```python\n    \"\"\"Word2Vec\u3067\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3092\u4f5c\u6210\u3059\u308b\"\"\"\n    word_dimension = 200\n    from gensim.models import Word2Vec\n    model_w2v = Word2Vec(sentences,seed=123,size=word_dimension,min_count=0,window=5)\n    sentence_image_matrix = np.zeros((len(sentences),1,word_dimension,max_sen_len)) #Convolution\u3059\u308b\u305f\u3081\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u753b\u50cf\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u306e\u521d\u671f\u5316\n\n    \"\"\"x_wrd\u306b\u5bfe\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210\u3059\u308b\"\"\"\n    for i in range(0,len(x_wrd)):\n        tmp_id_list = x_wrd[i,:]\n        for j in range(0,len(tmp_id_list)):\n            \"\"\"1\u884c\u306b\u5bfe\u3057\u3066\u56de\u3059\"\"\"\n            id = tmp_id_list[j]\n            if id == -1:\n                \"\"\"\u60c5\u5831\u306a\u3057\"\"\"\n                sentence_image_matrix[i,0,:,j] = [0.] * word_dimension #0\u30d9\u30af\u30c8\u30eb\u3092\u5165\u308c\u308b\n            else:\n                target_word = token_list[id]\n                sentence_image_matrix[i,0,:,j] = model_w2v[target_word]\n```\n\nsentence_image_matrix\u306f(\u30bb\u30f3\u30c6\u30f3\u30b9\u6570,1,\u30d9\u30af\u30c8\u30eb\u6b21\u5143=200,\u6700\u5927\u6587\u7ae0\u9577)\u306e\u5927\u304d\u3055\u306e4\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n# \u5b66\u7fd2\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u5206\u5272\n\n\u521d\u3081\u3066\u77e5\u3063\u305f\u306e\u3067\u3059\u304c\u30014\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u306b\u5bfe\u3057\u3066\u3082sklearn\u306etrain_test_split\u304c\u4f7f\u3048\u307e\u3059\u3002\n\u305f\u3076\u3093\u3001\u7b2c1\u6b21\u5143\u3057\u304b\u898b\u3066\u3044\u306a\u3044\u304b\u3089\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\n```python\n    \"\"\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\"\"\"\n    sentence_image_matrix = np.array(sentence_image_matrix,dtype=np.float32)\n    N = len(sentence_image_matrix)\n    t_n = 0.33\n    x_train,x_test,y_train,y_test = train_test_split(sentence_image_matrix,y,test_size=t_n,random_state=123)\n```\n\n# CNN\u306e\u5b9a\u7fa9\n\n\u554f\u984c\u306fCNN\u306e\u5b9a\u7fa9\u3067\u3059\u3002\n\u4eca\u56de\u8ad6\u6587\u3067\u306f\u3001\u975e\u5bfe\u79f0\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u4f7f\u3063\u3066\u304a\u308a\u3001\u3055\u3089\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u3082\u975e\u5bfe\u79f0\u3067\u3059\u306e\u3067\u3001\u305d\u306e\u8fba\u3092\u8003\u616e\u3057\u306a\u3044\u3068\u3044\u3051\u307e\u305b\u3093\u3002\n\n\u3059\u308b\u3068\u3053\u3093\u306a\u611f\u3058\u306b\u306a\u308a\u307e\u3059\u3002\n\n```python\nclass CNNFiltRow(ChainerClassifier):\n    \"\"\"\n    CNN\u306e\u884c\u65b9\u5411\u3092\u5168\u90e8\u30d5\u30a3\u30eb\u30bf\u30fc\u3068\u3057\u3066\u3001\u5217\u65b9\u5411\u306b\u52d5\u304b\u3059\u30d1\u30bf\u30fc\u30f3\n    \"\"\"\n\n    def _setup_network(self, **params):\n        self.input_dim = params[\"input_dim\"] #1\u679a\u306e\u753b\u50cf\u306e\u5217\u65b9\u5411\u306e\u6b21\u5143\n        self.in_channels = params[\"in_channels\"] #input channels : default = 1\n        self.out_channels = params[\"out_channels\"] #out_channels : \u4efb\u610f\n        self.row_dim = params[\"row_dim\"] #1\u679a\u306e\u753b\u50cf\u306e\u884c\u65b9\u5411\u306e\u6b21\u5143 = Filter\u306e\u884c\u6570\u306b\u306a\u308b\n        self.filt_clm = params[\"filt_clm\"] #Filter\u306e\u5217\u6570\n        self.pooling_row = params[\"pooling_row\"] if params.has_key(\"pooling_row\") else 1 #pooling\u306e\u884c\u6570 : default = 1\n        self.pooling_clm = params[\"pooling_clm\"] if params.has_key(\"pooling_clm\") else int(self.input_dim - 2 * math.floor(self.filt_clm/2.)) #Pooling\u306e\u5217\u6570 : default = math.floor((self.input_dim - 2 * math.floor(self.filt_clm/2.))\n        self.batch_size = params[\"batch_size\"] if params.has_key(\"batch_size\") else 100\n        self.hidden_dim = params[\"hidden_dim\"]\n        self.n_classes = params[\"n_classes\"]\n\n        self.conv1_out_dim = math.floor((self.input_dim - 2 * math.floor(self.filt_clm/2.))/self.pooling_clm)\n        network = FunctionSet(\n            conv1 = F.Convolution2D(self.in_channels,self.out_channels,(self.row_dim,self.filt_clm)), #Filter\u3092\u975e\u5bfe\u79f0\u306b\u3057\u305f\n            l1=F.Linear(self.conv1_out_dim*self.out_channels, self.hidden_dim),\n            l2=F.Linear(self.hidden_dim, self.hidden_dim),\n            l3=F.Linear(self.hidden_dim, self.n_classes),\n        )\n        return network\n\n    def forward(self, x, train=True):\n        h = F.max_pooling_2d(F.relu(self.network.conv1(x)), (self.pooling_row,self.pooling_clm))\n        h1 = F.dropout(F.relu(self.network.l1(h)),train=train)\n        h2 = F.dropout(F.relu(self.network.l2(h1)),train=train)\n        y = self.network.l3(h2)\n        return y\n\n    def output_func(self, h):\n        return F.softmax(h)\n\n    def loss_func(self, y, t):\n        return F.softmax_cross_entropy(y, t)\n\n    def fit(self, x_data, y_data):\n        batchsize = self.batch_size\n        N = len(y_data)\n        for loop in range(self.n_iter):\n            perm = np.random.permutation(N)\n            sum_accuracy = 0\n            sum_loss = 0\n            for i in six.moves.range(0, N, batchsize):\n                x_batch = x_data[perm[i:i + batchsize]]\n                y_batch = y_data[perm[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                self.optimizer.zero_grads()\n                yp = self.forward(x)\n                loss = self.loss_func(yp,y)\n                loss.backward()\n                self.optimizer.update()\n                sum_loss += loss.data * len(y_batch)\n                sum_accuracy += F.accuracy(yp,y).data * len(y_batch)\n            if self.report > 0 and loop % self.report == 0:\n                print('loop={}, train mean loss={} , train mean accuracy={}'.format(loop, sum_loss / N,sum_accuracy / N))\n\n        return self\n\n    def fit_test(self, x_data, y_data,x_test,y_test):\n        batchsize = self.batch_size\n        N = len(y_data)\n        Nt = len(y_test)\n        train_ac = []\n        test_ac = []\n        for loop in range(self.n_iter):\n            perm = np.random.permutation(N)\n            permt = np.random.permutation(Nt)\n            sum_accuracy = 0\n            sum_loss = 0\n\n            sum_accuracy_t = 0\n\n            \"\"\"\u5b66\u7fd2\u30d5\u30a7\u30fc\u30ba\"\"\"\n            for i in six.moves.range(0, N, batchsize):\n                x_batch = x_data[perm[i:i + batchsize]]\n                y_batch = y_data[perm[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                self.optimizer.zero_grads()\n                yp = self.forward(x)\n                loss = self.loss_func(yp,y)\n                loss.backward()\n                self.optimizer.update()\n                sum_loss += loss.data * len(y_batch)\n                sum_accuracy += F.accuracy(yp,y).data * len(y_batch)\n\n            \"\"\"\u30c6\u30b9\u30c8\u30d5\u30a7\u30fc\u30ba\"\"\"\n            for i in six.moves.range(0,Nt,batchsize):\n                x_batch = x_test[permt[i:i + batchsize]]\n                y_batch = y_test[permt[i:i + batchsize]]\n                x = Variable(x_batch)\n                y = Variable(y_batch)\n                yp = self.forward(x,False)\n                sum_accuracy_t += F.accuracy(yp,y).data * len(y_batch)\n\n            if self.report > 0 and loop % self.report == 0:\n                print('loop={}, train mean loss={} , train mean accuracy={} , test mean accuracy={}'.format(loop, sum_loss / N,sum_accuracy / N,sum_accuracy_t / Nt))\n\n            train_ac.append(sum_accuracy / N)\n            test_ac.append(sum_accuracy_t / Nt)\n\n        return self,train_ac,test_ac\n```\n\nChainerClassifier\u306b\u3064\u3044\u3066\u306f[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/hironishi/items/e07fd287393cab7211e5)\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\n# CNN\u306e\u5b66\u7fd2\n\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u7cbe\u5ea6\u3082\u898b\u305f\u304b\u3063\u305f\u306e\u3067fit_test\u30e1\u30bd\u30c3\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n```python\n    \"\"\"CNN Filter Row\u306e\u5b66\u7fd2\"\"\"\n    n_iter = 200\n    report = 5\n    params = {\"input_dim\":max_sen_len,\"in_channels\":1,\"out_channels\":20,\"row_dim\":word_dimension,\"filt_clm\":3,\"batch_size\":100,\"hidden_dim\":300,\"n_classes\":2}\n    cnn = CNNFiltRow(n_iter=n_iter,report=report,**params)\n    cnn,train_ac,test_ac = cnn.fit_test(x_train,y_train,x_test,y_test)\n```\n\n\u5b66\u7fd2\u306e\u969b\u306e\u7cbe\u5ea6\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u7cbe\u5ea6\u306e\u30d7\u30ed\u30c3\u30c8\u306f\u4ee5\u4e0b\u3002\nIter = 100\u304f\u3089\u3044\u304b\u3089\u904e\u5b66\u7fd2\u304c\u59cb\u307e\u308b\u611f\u3058\u306e\u3088\u3046\u3067\u3042\u308b\u3002\n\u305d\u308c\u3067\u3082\u6c4e\u5316\u6027\u80fd\u304c\u9ad8\u3044\u5370\u8c61\u3002\n\n![image](https://qiita-image-store.s3.amazonaws.com/0/54539/1b32ec20-0323-6a28-2489-6acfb8aef6c6.png)\n\n# CNN\u306e\u4e88\u6e2c\n\n\u6700\u7d42\u7684\u306b\u51fa\u6765\u4e0a\u304c\u3063\u305f\u30e2\u30c7\u30eb\u306b\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5165\u308c\u3066\u307f\u3066\u5404\u6307\u6a19\u3092\u51fa\u3057\u3066\u307f\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002\n\n```python\n[CNN]P AUC: 0.80 Pres: 0.66 Recl: 0.89 Fscr: 0.76\n```\n\nFscore\u30670.76\u3067AUC\u30670.8\u3068\u7d50\u69cb\u826f\u3044\u611f\u3058\u306b\u306a\u3063\u305f\u3002\n\n# \u4ed6\u306e\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\n\n\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u3057\u3066Random Forest\u3068MLP(\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3)\u3067\u3082\u540c\u3058\u3053\u3068\u3092\u3084\u3063\u3066\u307f\u308b\u3002\n\u5165\u529b\u30c7\u30fc\u30bf\u306f\u3053\u306e\u5834\u54082\u6b21\u5143\u3067\u306f\u306a\u3044\u306e\u3067\u3001MNIST\u306e\u3088\u3046\u306b1\u6b21\u5143\u306b\u76f4\u3057\u3066\u3044\u308b\u3002\n\n\u305d\u306e\u7d50\u679c\u3001\u540c\u69d8\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u5404\u7a2e\u6307\u6a19\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\u3002\n\n```python\n[RF ]P AUC: 0.71 Pres: 0.65 Recl: 0.60 Fscr: 0.62\n[MLP]P AUC: 0.71 Pres: 0.64 Recl: 0.69 Fscr: 0.67\n[CNN]P AUC: 0.80 Pres: 0.66 Recl: 0.89 Fscr: 0.76\n```\n\nCNN\u306e\u3053\u306e\u5727\u5012\u7684\u6027\u80fd\u5dee\u2026\n\n# \u307e\u3068\u3081\n\n* \u304a\u304a\u3080\u306dCNN\u306e\u6319\u52d5\u306f\u7406\u89e3\u3067\u304d\u305f\u3002\n* BoW\u3067\u3084\u3063\u305f\u306e\u3082\u306e\u3082\u3042\u308b\u304c\u3001\u7cbe\u5ea6\u306f\u3053\u3053\u307e\u3067\u51fa\u306a\u304b\u3063\u305f\u3002\n* \u9069\u5207\u306a\u5206\u6563\u8868\u73fe\u306f\u5fc5\u8981\u304b\u3068\u601d\u3046\u3002\n* \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u4eca\u56de\u7f6e\u304d\u3067\u3084\u3063\u305f\u304c\u3001\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u304d\u308b\u306a\u3089\u3084\u308a\u305f\u3044(\u8a08\u7b97\u6a5f\u30d1\u30ef\u30fc\u304c\u8db3\u308a\u306a\u3044)\n* \u3042\u3068\u306f\u65e5\u672c\u8a9e\u3078\u306e\u62e1\u5f35\u304b\u3002\n* LSTM\u3092\u4f7f\u3063\u305fRNN\u3082\u8a66\u3057\u3066\u307f\u305f\u3044\u3002\n"}