{"tags": ["Chainer", "Python", "DeepLearning"], "context": " More than 1 year has passed since last update.Deep Learning\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\uff11\u3064\u3067\u3042\u308bchainer\u3092\u4f7f\u3063\u3066\u52c9\u5f37\u304c\u3066\u3089sin\u95a2\u6570\u3092\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u3066\u307f\u305f\u3051\u3069\u30c0\u30e1\u3060\u3063\u305f\u3002chainer\u306eexample\u306b\u3042\u308bmnist\u3092\u3061\u3087\u3053\u3061\u3087\u3053\u66f8\u304d\u63db\u3048\u305f\u3060\u3051\u306e\u30b3\u30fc\u30c9\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u60aa\u3044\u306e\u304b\u3001\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u3067\u30d0\u30b0\u304c\u3042\u308b\u306e\u304b\u308f\u304b\u3089\u306a\u3044\u3002\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3059\u3054\u304f\u3046\u308c\u3057\u3044\u3002\n\u307e\u305a\u306f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304b\u3089\n\nmake_dataset.py\ndef make_dateset():\n    x_train = np.arange(0,3.14*50.0,0.05)\n    y_train = np.sin(x_train).astype(np.float32)\n    x_test  = np.arange(3.14*50.0,3.14 * 60.0,0.05)\n    y_test = np.sin(x_test).astype(np.float32)\n    return x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n\n\nx_train\u306b\u306fnp.array\u3067\uff10\u304b\u30893.14*50\u307e\u3067\u306e0.05\u523b\u307f\u3067\u6570\u5b57\u304c\u5165\u3063\u3066\u3044\u308b\nex) [0.0,0.05,0.10,......,157.0]\ny_train\u306b\u306fsin\u95a2\u6570\u306bx_train\u3092\u4ee3\u5165\u3057\u305f\u5024\u304c\u5165\u3063\u3066\u3044\u308b\u3002\nex) [0.0,0.47942553860420301,0.09983341......,]\nx_test,y_test\u3082\u540c\u3058\u3088\u3046\u306a\u611f\u3058\u3002\u7bc4\u56f2\u304c\u9055\u3046\u3060\u3051\n\nsin_test.py\nimport numpy as np\nimport six\nimport chainer\nfrom chainer import computational_graph as c\nfrom chainer import cuda\nimport chainer.functions as F\nfrom chainer import optimizers\nimport matplotlib.pyplot as plt\nimport csv\n\ndef make_dateset():\n    x_train = np.arange(0,3.14*50.0,0.05)\n    y_train = np.sin(x_train).astype(np.float32)\n    x_test  = np.arange(3.14*50.0,3.14 * 60.0,0.05)\n    y_test = np.sin(x_test).astype(np.float32)\n    return x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n\n\ndef forward(x_data,y_data,train = True):\n    x,t = chainer.Variable(x_data),chainer.Variable(y_data)\n    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n    h2 = F.dropout(F.relu(model.l2(h1)),  train=train)\n        h3 = F.dropout(F.relu(model.l3(h1)),  train=train)\n    y = model.l4(h3)\n    return F.mean_squared_error(y,t),y\n\n\nif __name__ == \"__main__\":\n\n    x_train,y_train,x_test,y_test = make_dateset()\n    x_train,y_train = x_train.reshape(len(x_train),1),y_train.reshape(len(y_train),1)\n    x_test,y_test = x_test.reshape(len(x_test),1),y_test.reshape(len(y_test),1)\n    y_t,y_p,ll = [],[],[]\n\n    xp = np\n\n    batchsize = 10\n    N = len(x_train)\n    N_test = len(x_test)\n    n_epoch = 100\n    n_units = 20\n    pred_y = []\n\n    model = chainer.FunctionSet(l1=F.Linear(1, n_units),\n                                l2=F.Linear(n_units, n_units),\n                                l3=F.Linear(n_units, u_units)),\n                                                                l4=F.Linear(n_units, 1))\n    optimizer = optimizers.Adam()\n    optimizer.setup(model.collect_parameters())\n\n\n\n    x_t,y_t,y_p = [],[],[]\n\n    for epoch in six.moves.range(1, n_epoch + 1):\n        print('epoch', epoch)\n\n        perm = np.random.permutation(N)\n        sum_loss = 0\n\n        for i in six.moves.range(0, N, batchsize):\n            x_batch = xp.asarray(x_train[perm[i:i + batchsize]])\n            y_batch = xp.asarray(y_train[perm[i:i + batchsize]])\n\n            optimizer.zero_grads()\n            loss,y = forward(x_batch, y_batch)\n            loss.backward()\n            optimizer.update()\n            sum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n        print \"train mean loss = \",sum_loss/N \n\n        sum_loss = 0\n        for i in six.moves.range(0, N_test, batchsize):\n            x_batch = xp.asarray(x_test[i:i + batchsize])\n            y_batch = xp.asarray(y_test[i:i + batchsize])\n            loss, y = forward(x_batch, y_batch, train=False)\n                        #\u30c7\u30d0\u30c3\u30af\u7528\n            #y_t.append(y_batch[0])\n            #y_p.append(y.data[0])\n            #x_t.append(x_batch[0])\n            sum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n        print \"test mean loss is \",sum_loss/N_test\n    #\u30c7\u30d0\u30c3\u30af\u7528\n    #f = open('sin_pre.csv','ab')\n    #csvWriter = csv.writer(f)\n    #csvWriter.writerow(y_p)\n    #f.close()\n\n    #f = open('sin_ans.csv','ab')\n    #csvWriter = csv.writer(f)\n    #csvWriter.writerow(y_t)\n    #f.close()\n\n\n\n\u6700\u521d\u306ex_train,y_train,x_test,y_test\u306fN*1\u884c\u5217\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3002(chainer\u306eVariable\u306b\u6e21\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081)\n\u3042\u3068\u306fmnist\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u307b\u3068\u3093\u3069\u540c\u3058\u3002\u9055\u3046\u6240\u306f\u56de\u5e30\u554f\u984c\u306a\u306e\u3067Forward()\u95a2\u6570\u3067mean_squared_error(\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570)\u3092\u4f7f\u3063\u3066loss\u3092\u6c42\u3081\u3066\u3044\u308b\u3002\u3042\u3068\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u304c1-20-20-1\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\n\u5b9f\u884c\u7d50\u679c(epoch10\u307e\u3067)\n('epoch', 1)\ntrain mean loss =  2553.66754833\ntest mean loss is  127.272548827\n('epoch', 2)\ntrain mean loss =  401.413729346\ntest mean loss is  5.86524515122\n('epoch', 3)\ntrain mean loss =  138.270190761\ntest mean loss is  4.34996299998\n('epoch', 4)\ntrain mean loss =  68.4881465446\ntest mean loss is  0.659433874475\n('epoch', 5)\ntrain mean loss =  38.2469408746\ntest mean loss is  0.640729590383\n('epoch', 6)\ntrain mean loss =  24.6955423482\ntest mean loss is  0.529370371471\n('epoch', 7)\ntrain mean loss =  16.3685227446\ntest mean loss is  0.505678843091\n('epoch', 8)\ntrain mean loss =  11.0349840385\ntest mean loss is  0.542997811425\n('epoch', 9)\ntrain mean loss =  7.98288726631\ntest mean loss is  0.509733980175\n('epoch', 10)\ntrain mean loss =  5.89249175341\ntest mean loss is  0.502585373718\n\n\n\u4e00\u5fdc\u5b66\u7fd2\u306f\u3057\u3066\u3044\u308b\u3063\u307d\u3044\u304cepoch\u304c20\u3092\u8d85\u3048\u305f\u3042\u305f\u308a\u304b\u3089test mean loss\u304c0.5\u4ed8\u8fd1\u3092\u3046\u308d\u3046\u308d\u3057\u3066\u3044\u3066\u3044\u3063\u3053\u3046\u306b\u4e0b\u304c\u308b\u6c17\u914d\u304c\u306a\u3044\u3002\u5c40\u6240\u89e3\u306b\u9665\u3063\u3066\u3057\u307e\u3063\u305f\u306e\u304b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u66f4\u65b0\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u304b\u3001\u5358\u7d14\u306b\u30b3\u30fc\u30c9\u306b\u30df\u30b9\u304c\u3042\u308b\u306e\u304b\u304c\u308f\u304b\u3089\u306a\u3044\u3002\n\n\u53c2\u8003\nchainer\n\nDeep Learning\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\uff11\u3064\u3067\u3042\u308b[chainer](http://chainer.org/)\u3092\u4f7f\u3063\u3066\u52c9\u5f37\u304c\u3066\u3089sin\u95a2\u6570\u3092\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u3066\u307f\u305f\u3051\u3069\u30c0\u30e1\u3060\u3063\u305f\u3002chainer\u306eexample\u306b\u3042\u308bmnist\u3092\u3061\u3087\u3053\u3061\u3087\u3053\u66f8\u304d\u63db\u3048\u305f\u3060\u3051\u306e\u30b3\u30fc\u30c9\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u60aa\u3044\u306e\u304b\u3001\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u3067\u30d0\u30b0\u304c\u3042\u308b\u306e\u304b\u308f\u304b\u3089\u306a\u3044\u3002\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3059\u3054\u304f\u3046\u308c\u3057\u3044\u3002\n\n\u307e\u305a\u306f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304b\u3089\n\n```lang:make_dataset.py\ndef make_dateset():\n\tx_train = np.arange(0,3.14*50.0,0.05)\n\ty_train = np.sin(x_train).astype(np.float32)\n\tx_test  = np.arange(3.14*50.0,3.14 * 60.0,0.05)\n\ty_test = np.sin(x_test).astype(np.float32)\n\treturn x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n```\n\nx_train\u306b\u306fnp.array\u3067\uff10\u304b\u30893.14*50\u307e\u3067\u306e0.05\u523b\u307f\u3067\u6570\u5b57\u304c\u5165\u3063\u3066\u3044\u308b\nex) [0.0,0.05,0.10,......,157.0]\n\ny_train\u306b\u306fsin\u95a2\u6570\u306bx_train\u3092\u4ee3\u5165\u3057\u305f\u5024\u304c\u5165\u3063\u3066\u3044\u308b\u3002\nex) [0.0,0.47942553860420301,0.09983341......,]\n\nx_test,y_test\u3082\u540c\u3058\u3088\u3046\u306a\u611f\u3058\u3002\u7bc4\u56f2\u304c\u9055\u3046\u3060\u3051\n\n```lang:sin_test.py\nimport numpy as np\nimport six\nimport chainer\nfrom chainer import computational_graph as c\nfrom chainer import cuda\nimport chainer.functions as F\nfrom chainer import optimizers\nimport matplotlib.pyplot as plt\nimport csv\n\ndef make_dateset():\n\tx_train = np.arange(0,3.14*50.0,0.05)\n\ty_train = np.sin(x_train).astype(np.float32)\n\tx_test  = np.arange(3.14*50.0,3.14 * 60.0,0.05)\n\ty_test = np.sin(x_test).astype(np.float32)\n\treturn x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n\n\t\ndef forward(x_data,y_data,train = True):\n\tx,t = chainer.Variable(x_data),chainer.Variable(y_data)\n\th1 = F.dropout(F.relu(model.l1(x)),  train=train)\n\th2 = F.dropout(F.relu(model.l2(h1)),  train=train)\n        h3 = F.dropout(F.relu(model.l3(h1)),  train=train)\n\ty = model.l4(h3)\n\treturn F.mean_squared_error(y,t),y\n\n\nif __name__ == \"__main__\":\n\t\n\tx_train,y_train,x_test,y_test = make_dateset()\n\tx_train,y_train = x_train.reshape(len(x_train),1),y_train.reshape(len(y_train),1)\n\tx_test,y_test = x_test.reshape(len(x_test),1),y_test.reshape(len(y_test),1)\n\ty_t,y_p,ll = [],[],[]\n\t\n\txp = np\n\n\tbatchsize = 10\n\tN = len(x_train)\n\tN_test = len(x_test)\n\tn_epoch = 100\n\tn_units = 20\n\tpred_y = []\n\n\tmodel = chainer.FunctionSet(l1=F.Linear(1, n_units),\n\t\t\t\t\t\t\t\tl2=F.Linear(n_units, n_units),\n\t\t\t\t\t\t\t\tl3=F.Linear(n_units, u_units)),\n                                                                l4=F.Linear(n_units, 1))\n\toptimizer = optimizers.Adam()\n\toptimizer.setup(model.collect_parameters())\n\n\n\n\tx_t,y_t,y_p = [],[],[]\n\n\tfor epoch in six.moves.range(1, n_epoch + 1):\n\t\tprint('epoch', epoch)\n\n\t\tperm = np.random.permutation(N)\n\t\tsum_loss = 0\n\n\t\tfor i in six.moves.range(0, N, batchsize):\n\t\t\tx_batch = xp.asarray(x_train[perm[i:i + batchsize]])\n\t\t\ty_batch = xp.asarray(y_train[perm[i:i + batchsize]])\n\n\t\t\toptimizer.zero_grads()\n\t\t\tloss,y = forward(x_batch, y_batch)\n\t\t\tloss.backward()\n\t\t\toptimizer.update()\n\t\t\tsum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n\t\tprint \"train mean loss = \",sum_loss/N \n\t\t\n\t\tsum_loss = 0\n\t\tfor i in six.moves.range(0, N_test, batchsize):\n\t\t\tx_batch = xp.asarray(x_test[i:i + batchsize])\n\t\t\ty_batch = xp.asarray(y_test[i:i + batchsize])\n\t\t\tloss, y = forward(x_batch, y_batch, train=False)\n                        #\u30c7\u30d0\u30c3\u30af\u7528\n\t\t\t#y_t.append(y_batch[0])\n\t\t\t#y_p.append(y.data[0])\n\t\t\t#x_t.append(x_batch[0])\n\t\t\tsum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n\t\tprint \"test mean loss is \",sum_loss/N_test\n    #\u30c7\u30d0\u30c3\u30af\u7528\n\t#f = open('sin_pre.csv','ab')\n\t#csvWriter = csv.writer(f)\n\t#csvWriter.writerow(y_p)\n\t#f.close()\n\n\t#f = open('sin_ans.csv','ab')\n\t#csvWriter = csv.writer(f)\n\t#csvWriter.writerow(y_t)\n\t#f.close()\n\n```\n\u6700\u521d\u306ex_train,y_train,x_test,y_test\u306fN*1\u884c\u5217\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3002(chainer\u306eVariable\u306b\u6e21\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081)\n\u3042\u3068\u306fmnist\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u307b\u3068\u3093\u3069\u540c\u3058\u3002\u9055\u3046\u6240\u306f\u56de\u5e30\u554f\u984c\u306a\u306e\u3067Forward()\u95a2\u6570\u3067mean_squared_error(\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570)\u3092\u4f7f\u3063\u3066loss\u3092\u6c42\u3081\u3066\u3044\u308b\u3002\u3042\u3068\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u304c1-20-20-1\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\n```lang:\u5b9f\u884c\u7d50\u679c(epoch10\u307e\u3067)\n('epoch', 1)\ntrain mean loss =  2553.66754833\ntest mean loss is  127.272548827\n('epoch', 2)\ntrain mean loss =  401.413729346\ntest mean loss is  5.86524515122\n('epoch', 3)\ntrain mean loss =  138.270190761\ntest mean loss is  4.34996299998\n('epoch', 4)\ntrain mean loss =  68.4881465446\ntest mean loss is  0.659433874475\n('epoch', 5)\ntrain mean loss =  38.2469408746\ntest mean loss is  0.640729590383\n('epoch', 6)\ntrain mean loss =  24.6955423482\ntest mean loss is  0.529370371471\n('epoch', 7)\ntrain mean loss =  16.3685227446\ntest mean loss is  0.505678843091\n('epoch', 8)\ntrain mean loss =  11.0349840385\ntest mean loss is  0.542997811425\n('epoch', 9)\ntrain mean loss =  7.98288726631\ntest mean loss is  0.509733980175\n('epoch', 10)\ntrain mean loss =  5.89249175341\ntest mean loss is  0.502585373718\n```\n\u4e00\u5fdc\u5b66\u7fd2\u306f\u3057\u3066\u3044\u308b\u3063\u307d\u3044\u304cepoch\u304c20\u3092\u8d85\u3048\u305f\u3042\u305f\u308a\u304b\u3089test mean loss\u304c0.5\u4ed8\u8fd1\u3092\u3046\u308d\u3046\u308d\u3057\u3066\u3044\u3066\u3044\u3063\u3053\u3046\u306b\u4e0b\u304c\u308b\u6c17\u914d\u304c\u306a\u3044\u3002\u5c40\u6240\u89e3\u306b\u9665\u3063\u3066\u3057\u307e\u3063\u305f\u306e\u304b\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u66f4\u65b0\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u304b\u3001\u5358\u7d14\u306b\u30b3\u30fc\u30c9\u306b\u30df\u30b9\u304c\u3042\u308b\u306e\u304b\u304c\u308f\u304b\u3089\u306a\u3044\u3002\n\n\n\n>\u53c2\u8003\n[chainer](http://chainer.org/)\n\n"}