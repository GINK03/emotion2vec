{"context": "\n\n\u3010 \u8ad6\u6587 \u3011\n\n\nManaal Faruqui et.al, Retrofitting Word Vectors to Semantic Lexicons\n\n\n\u3010 \u5b9f\u88c5\u30b3\u30fc\u30c9 \u3011\n\n\n\uff08 GitHub \uff09mfaruqui/retrofitting\n\n\n\nGitHub \u304b\u3089 git clone\n\n\nTerminal\nLast login: Mon Aug 15 09:50:36 on ttys000\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ cd Desktop/\nHirofumiYashima-no-MacBook:Desktop hirofumiyashima$ ls | grep retro\nretrofit_word2vec\nHirofumiYashima-no-MacBook:Desktop hirofumiyashima$ cd retrofit_word2vec/\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ git clone https://github.com/mfaruqui/retrofitting.git\nCloning into 'retrofitting'...\nremote: Counting objects: 37, done.\nremote: Total 37 (delta 0), reused 0 (delta 0), pack-reused 37\nUnpacking objects: 100% (37/37), done.\nChecking connectivity... done.\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nclone\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u6210 \u3092 \u78ba\u8a8d\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls retrofitting/\nLICENSE     README.md   lexicons    retrofit.py sample_vec.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head sample_vec.txt\nhead: sample_vec.txt: No such file or directory\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\n\u5916\u90e8\u77e5\u8b58DB\u3092\u53c2\u7167\u3059\u308b\u3053\u3068\u3067\u3001\u88dc\u6b63\u3092\u884c\u3046\u5bfe\u8c61\u306b\u3059\u308b \u4e8b\u524d\u306b\u901a\u5e38\uff08Mikolov\u8ad6\u6587\u30d9\u30fc\u30b9\uff09\u306e word2vec\u304b\u3089\u751f\u6210\u3055\u305b\u305f\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306e \u30b5\u30f3\u30d7\u30eb\u30fb\u30d5\u30a1\u30a4\u30eb \u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/sample_vec.txt\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nverplank 0.061966 0.013867 0.097894 0.069291 0.025128 -0.029515 -0.0068756 -0.21864 -0.025601 -0.047496 -0.091767 0.081062 0.11705 0.025643 -0.051421 -0.074378 0.012586 -0.026915 0.19103 -0.15763 0.051448 0.13455 -0.040023 0.2077 0.017204 0.22573 0.083228 -0.0079535 -0.11535 0.17306 0.014453 -0.19401 -0.1076 -0.16378 0.25349 -0.18004 -0.044328 -0.16348 -0.091957 -0.36885 -0.01324 0.0078103 -0.00681 -0.021193 -0.11642 0.23982 -0.23426 0.43564\nclottes -0.0089251 -0.061586 0.1221 0.035306 -0.061313 -0.04685 0.11966 0.084184 0.057614 0.16206 0.10281 -0.22645 -0.015185 0.050584 -0.04209 -0.026132 0.011169 0.23204 0.076184 0.069735 -0.11726 0.050913 -0.087998 0.15215 0.082915 -0.055324 -0.10663 -0.084158 -0.059272 0.37413 0.25528 -0.12261 -0.2272 -0.028977 0.021512 0.073214 -0.30931 0.29448 0.066778 -0.092153 -0.14165 0.11589 -0.11014 0.040938 -0.33708 -0.030592 0.22528 0.16051\nnunnery 0.277 0.019486 -0.23815 -0.10068 -0.17544 -0.2148 -0.1853 -0.010177 -0.019739 -0.03499 -0.0099115 -0.099382 -0.11386 0.090788 0.21348 0.19557 -0.03373 0.17034 -0.21515 -0.044315 0.014785 -0.211 0.036734 0.12371 -0.30186 0.22997 -0.23255 0.0052285 0.029958 0.079806 -0.02739 -0.076451 0.053968 -0.1083 -0.067588 0.026335 -0.13879 0.027829 -0.026704 0.29506 0.018326 0.14407 -0.26815 -0.13034 -0.015692 0.18495 -0.045091 -0.04577\nclottey 0.13007 -0.091402 0.0037005 0.014496 -0.067745 -0.032454 0.31061 0.08075 0.031257 0.13992 -0.099519 -0.072196 0.15347 -0.10603 -0.1111 0.014984 0.0068156 -0.096376 0.020761 -0.12781 -0.041597 0.28228 0.041162 0.075579 0.041916 0.28711 0.024134 0.34505 -0.071057 0.21699 0.021115 -0.029354 -0.054196 0.013538 0.17918 -0.092146 -0.099105 -0.045168 -0.16692 -0.40238 -0.27625 0.028427 0.050746 -0.021625 -0.12425 -0.18158 0.058694 0.18238\nsowell 0.030762 -0.091813 0.021076 0.12843 0.0179 -0.077895 0.026529 -0.062911 0.063693 -0.020966 -0.014219 0.014767 0.13628 0.1027 -0.04707 0.10664 -0.039 -0.048278 -0.16216 0.010273 -0.03461 -0.042256 -0.15101 -0.16981 0.094874 0.21788 0.019252 0.084203 0.12758 -0.11146 -0.069592 0.019972 -0.12341 -0.24046 -0.26306 -0.24068 0.23155 0.2028 -0.010772 0.21385 -0.29686 -0.25456 0.23879 -0.15923 -0.13032 0.28009 0.23929 0.05916\nwoods 0.049942 0.042324 0.018938 0.064624 -0.064808 -0.086171 -0.075199 -0.19779 -0.055387 -0.23949 -0.0056237 0.1199 0.070734 0.091679 -0.18294 -0.11322 0.032094 -0.072929 0.066702 -0.070065 0.015355 0.030946 -0.26707 0.027615 -0.060169 0.22764 0.11389 0.030353 -0.062781 0.084065 -0.012644 -0.075017 -0.03808 -0.20063 0.068258 -0.025851 0.044887 -0.058646 -0.10062 -0.33411 -0.25018 -0.099802 0.13244 0.012939 -0.13918 0.24713 -0.39212 0.35557\nclotted -0.15325 0.17469 -0.14858 0.16705 -0.061657 0.38397 0.015206 0.11725 0.096851 -0.10926 0.20226 0.12276 -0.084128 -0.10981 0.11 -0.11522 -0.23358 -0.17534 0.23471 0.1212 -0.14923 0.15468 0.12293 0.15476 0.0097238 0.065649 -0.28607 -0.070528 0.041749 -0.075993 0.11935 -0.014533 -0.22663 -0.067245 0.060851 -0.043102 -0.025318 0.24947 0.091302 0.035894 -0.056044 -0.1219 0.2178 0.096062 0.03495 -0.060335 -0.066316 -0.1939\nspiders -0.098084 0.17303 -0.1513 0.23683 0.046248 -0.090944 -0.068375 0.074935 -0.026971 -0.2084 0.052636 -0.027759 0.23191 0.15999 -0.38536 0.05138 0.32286 -0.024479 0.082912 -0.2383 0.14478 0.070837 -0.16746 -0.13519 -0.078819 0.043042 0.11614 0.052073 0.11514 -0.023325 0.25228 0.015687 0.054137 -0.1127 -0.18158 0.01556 -0.020509 -0.2665 0.18425 0.03688 0.040863 0.10596 0.011119 0.012524 0.20098 -0.093732 -0.067517 0.13145\nmdbs -0.16749 0.076818 -0.0078256 -0.23028 0.10919 -0.0029132 0.10671 0.022756 -0.0051267 -0.086064 -0.27925 0.057727 -0.079849 -0.0799 -0.061325 0.043617 -0.20629 -0.21419 -0.13687 -0.24101 -0.090371 0.081844 0.13152 0.14611 -0.18709 -0.30011 -0.080421 -0.11626 -0.048492 0.022221 0.22303 0.060831 0.004612 -0.092795 0.056692 0.075079 0.12904 -0.02208 -0.088094 0.23042 -0.1211 0.25484 0.095224 -0.27334 0.15141 0.2108 -0.074673 -0.17334\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head -1 retrofitting/sample_vec.txt\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -1 retrofitting/sample_vec.txt\nishida 0.23975 -0.010016 0.12995 -0.042012 0.05867 0.11263 0.22874 -0.083031 -0.15666 0.031165 -0.037073 0.032879 -0.060202 0.11307 -0.019522 0.14007 0.082405 0.35134 -0.18307 0.11955 -0.18926 -0.0361 0.017629 0.098925 0.011091 0.13668 -0.26085 -0.02082 -0.15929 -0.091311 0.27706 -0.12584 -0.11666 -0.2786 0.11303 0.032773 -0.12723 -0.23614 -0.16537 0.073717 0.10836 -0.046751 -0.28947 -0.096606 0.13055 0.10114 -0.027045 -0.018152\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/sample_vec.txt\n    1000 retrofitting/sample_vec.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls retrofitting/lexicons/\nframenet.txt        ppdb-xl.txt     wordnet-synonyms+.txt   wordnet-synonyms.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/lexicons/framenet.txt \nsource\nsprinkle splatter pump drip spray splash spatter squirt\nsplatter sprinkle pump drip spray splash spatter squirt\npump sprinkle splatter drip spray splash spatter squirt\ndrip sprinkle splatter pump spray splash spatter squirt\nspray sprinkle splatter drip pump splash spatter squirt\nsplash sprinkle splatter pump drip spray spatter squirt\nspatter sprinkle splatter pump drip spray splash squirt\nsquirt sprinkle splatter pump drip spray splash spatter\nrun-through rehearse run practice rehearsal mock drill exercise\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\n\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08Framenet\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail retrofitting/lexicons/framenet.txt \nrenowned famous notoriety renown legendary stature notorious epic fame infamous reputation\nfamous renowned notoriety renown legendary stature notorious epic fame infamous reputation\nnotoriety renowned famous renown legendary stature notorious epic fame infamous reputation\nreputation renowned famous notoriety renown legendary stature notorious epic fame infamous\nlegendary renowned famous notoriety renown stature notorious epic fame infamous reputation\nstature renowned famous notoriety renown legendary notorious epic fame infamous reputation\nnotorious renowned famous notoriety renown legendary stature epic fame infamous reputation\nepic renowned famous notoriety renown legendary stature notorious fame infamous reputation\nfame renowned famous notoriety renown legendary stature notorious epic infamous reputation\ninfamous renowned famous notoriety renown legendary stature notorious epic fame reputation\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/framenet.txt \n   10822 retrofitting/lexicons/framenet.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\n\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08ppdb\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d\n\n\nPPDB(PlantPromoterDB)\n\n\nPPDB: The Paraphrase Database\n\uff08 Web\u30d6\u30e9\u30a6\u30b6 \u691c\u7d22\u753b\u9762 \uff09paraphrase.org\nJuri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, PPDB: The Paraphrase Database\n\n\n\u3061\u306a\u307f\u306b\u3001\u4ee5\u4e0b\u306e\u65e5\u672c\u8a9e\u7248\u306ePPDB\u3082\u5b58\u5728\u3059\u308b\u3088\u3046\u3067\u3059\u3002\n\nPPDB : Japanese - \u65e5\u672c\u8a9e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\n\n\nThe Paraphrase Database : Japanese\u306f\u65e5\u82f1\u5bfe\u8a33\u30b3\u30fc\u30d1\u30b9\u304b\u3089\u5b66\u7fd2\u3055\u308c\u305f\u65e5\u672c\u8a9e\u306e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u96c6\u3067\u3059\u3002\n\n\n\u6c34\u4e0a \u96c5\u535a \u4ed6 \u300c\u65e5\u672c\u8a9e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u69cb\u7bc9\u3068\u8a00\u8a9e\u7684\u500b\u4eba\u6027\u5909\u63db\u3078\u306e\u5fdc\u7528\u300d\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a \u7b2c20\u56de\u5e74\u6b21\u5927\u4f1a \u767a\u8868\u8ad6\u6587\u96c6 (2014\u5e743\u6708)\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/ppdb-xl.txt \n  102902 retrofitting/lexicons/ppdb-xl.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$head retrofitting/lexicons/ppdb-xl.txt \nfawn crawl\nreallocations re-allocations reassignments redeployments diversions overrides redistributions\nheavily-fortified strongly-fortified\n360.00 360\n1,800 1800 1.8 1.800 1-800\n1,802 1802\nsonja sonia\ngag muzzle joke reflex\nwoods bretton wood timber forest bois drink lumber forests antlers\nclotted coagulated\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head -1 retrofitting/lexicons/ppdb-xl.txt \nfawn crawl\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -1 retrofitting/lexicons/ppdb-xl.txt \n'arch\u00e9ologie arch\u00e9ologie\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -3 retrofitting/lexicons/ppdb-xl.txt \nexpands extends broadens widens enlarges stretches expansion increases expand\n38kb 41kb\n'arch\u00e9ologie arch\u00e9ologie\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/ppdb-xl.txt \n  102902 retrofitting/lexicons/ppdb-xl.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \n\n\n\n\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08WordNet\u4e2d\u306e\u985e\u4f3c\u8a9e\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/wordnet-synonyms.txt\n  148730 retrofitting/lexicons/wordnet-synonyms.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/lexicons/wordnet-synonyms.txt\nfawn toady truckle bootlick kowtow kotow suck_up crawl creep cringe cower grovel dun greyish_brown grayish_brown\nunsupportable \nnunnery \nperoxide_blonde peroxide_blond\nThoreauvian \nlong_wave \nPlasticine \ncircuitry \nclotted clogged\nxerophthalmus xerophthalmia xeroma conjunctivitis_arida\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail retrofitting/lexicons/wordnet-synonyms.txt\ndry_unit dry_measure\nspendable expendable\napolemia \nfighting_chair \ndegustation tasting savoring savouring relishing\nCosta_Rica Republic_of_Costa_Rica\njawbone shmooze shmoose schmooze schmoose lower_jaw mandible mandibula mandibular_bone submaxilla lower_jawbone jowl\nfamily_Iridaceae Iridaceae iris_family\nclaim_agent adjuster adjustor claims_adjuster claims_adjustor\nformularize formularise\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ cd retrofitting/\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls\nLICENSE     README.md   lexicons    retrofit.py sample_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ tree\n.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 lexicons\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 framenet.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ppdb-xl.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wordnet-synonyms+.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wordnet-synonyms.txt\n\u251c\u2500\u2500 retrofit.py\n\u2514\u2500\u2500 sample_vec.txt\n\n1 directory, 8 files\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\n\nretrofit.py \u5b9f\u884c\n\n\nword2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb sample_vec.txt\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09\u306e retrofitting \u306b\u3088\u308b ppdb-xl\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u77e5\u8b58DB\u53c2\u7167\u3057\u3066\u306e\u30d9\u30af\u30c8\u30eb\u88dc\u6b63\u5b9f\u884c\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ time python retrofit.py -i sample_vec.txt -l lexicons/ppdb-xl.txt -n 10 -o out_vec.txt\nVectors read from: sample_vec.txt \n\nWriting down the vectors in out_vec.txt\n\nreal    0m3.728s\nuser    0m2.809s\nsys 0m0.283s\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\u88dc\u6b63\u5f8c\u306e\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09out_vec.txt\u3000\u304c \u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3002\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls\nLICENSE     README.md   lexicons    out_vec.txt retrofit.py sample_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls | grep out\nout_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\n\u884c\u6570\u306f\u88dc\u6b63\u524d\u3068\u540c\u30581,000\u884c\uff08\u30c8\u30fc\u30af\u30f3\u6570\uff1a1,000\u4ef6\uff09\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ wc -l out_vec.txt \n    1000 out_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head out_vec.txt \nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \nverplank 0.0620 0.0139 0.0979 0.0693 0.0251 -0.0295 -0.0069 -0.2186 -0.0256 -0.0475 -0.0918 0.0811 0.1171 0.0256 -0.0514 -0.0744 0.0126 -0.0269 0.1910 -0.1576 0.0514 0.1346 -0.0400 0.2077 0.0172 0.2257 0.0832 -0.0080 -0.1154 0.1731 0.0145 -0.1940 -0.1076 -0.1638 0.2535 -0.1800 -0.0443 -0.1635 -0.0920 -0.3689 -0.0132 0.0078 -0.0068 -0.0212 -0.1164 0.2398 -0.2343 0.4356 \nclottes -0.0089 -0.0616 0.1221 0.0353 -0.0613 -0.0468 0.1197 0.0842 0.0576 0.1621 0.1028 -0.2264 -0.0152 0.0506 -0.0421 -0.0261 0.0112 0.2320 0.0762 0.0697 -0.1173 0.0509 -0.0880 0.1521 0.0829 -0.0553 -0.1066 -0.0842 -0.0593 0.3741 0.2553 -0.1226 -0.2272 -0.0290 0.0215 0.0732 -0.3093 0.2945 0.0668 -0.0922 -0.1416 0.1159 -0.1101 0.0409 -0.3371 -0.0306 0.2253 0.1605 \nchaim 0.1786 -0.1085 0.1778 0.1119 0.0524 -0.0223 -0.2343 0.1261 0.0106 0.1048 -0.0081 -0.1374 0.0759 0.0962 0.1196 -0.0828 -0.2241 0.2000 0.1038 -0.0327 -0.0238 0.1589 0.0838 0.0316 0.0914 -0.3076 0.1447 -0.2133 -0.0091 0.1464 0.0682 0.2652 -0.2158 0.2633 -0.1456 -0.1633 -0.0712 0.2052 0.1588 0.0926 -0.0655 0.2765 -0.0931 0.0765 0.0526 0.1291 -0.1030 -0.0121 \nchain -0.0781 0.1797 -0.0714 -0.1388 -0.0100 0.1631 -0.0590 -0.0122 0.0131 0.0095 -0.2766 0.1228 -0.0379 -0.0177 0.0000 -0.1840 -0.0271 0.1516 0.1337 0.3704 -0.0302 0.0059 -0.3412 -0.0947 -0.0698 -0.0115 0.0201 -0.0483 -0.1166 0.0771 0.0418 -0.0391 -0.0921 -0.0928 -0.2170 -0.0096 -0.0162 0.0405 -0.1827 -0.0036 -0.0354 -0.1323 -0.3062 0.0959 -0.1465 -0.2642 -0.3611 -0.0275 \nwhoever 0.0086 -0.2238 0.0260 0.0636 -0.0396 0.1064 -0.0173 0.0194 0.0407 -0.1259 -0.0866 0.0319 0.0718 0.0426 -0.1504 -0.1081 0.2821 -0.0492 -0.0694 -0.1264 0.0719 0.0192 -0.2853 0.1130 -0.3556 0.2950 -0.0640 -0.0270 0.1557 0.1020 -0.2832 0.0109 -0.0710 0.1190 0.0856 -0.1717 0.0285 0.1827 -0.1751 -0.1333 0.0364 0.1084 0.0708 0.0303 0.1840 -0.0210 -0.3455 0.1309 \nnunnery 0.2770 0.0195 -0.2382 -0.1007 -0.1754 -0.2148 -0.1853 -0.0102 -0.0197 -0.0350 -0.0099 -0.0994 -0.1139 0.0908 0.2135 0.1956 -0.0337 0.1703 -0.2152 -0.0443 0.0148 -0.2110 0.0367 0.1237 -0.3019 0.2300 -0.2326 0.0052 0.0300 0.0798 -0.0274 -0.0765 0.0540 -0.1083 -0.0676 0.0263 -0.1388 0.0278 -0.0267 0.2951 0.0183 0.1441 -0.2682 -0.1303 -0.0157 0.1850 -0.0451 -0.0458 \nclottey 0.1301 -0.0914 0.0037 0.0145 -0.0677 -0.0325 0.3106 0.0808 0.0313 0.1399 -0.0995 -0.0722 0.1535 -0.1060 -0.1111 0.0150 0.0068 -0.0964 0.0208 -0.1278 -0.0416 0.2823 0.0412 0.0756 0.0419 0.2871 0.0241 0.3451 -0.0711 0.2170 0.0211 -0.0294 -0.0542 0.0135 0.1792 -0.0921 -0.0991 -0.0452 -0.1669 -0.4024 -0.2763 0.0284 0.0507 -0.0216 -0.1243 -0.1816 0.0587 0.1824 \nchaib 0.1786 -0.0779 -0.0624 -0.0319 0.3098 -0.0052 0.1528 -0.0334 0.0106 0.1000 0.2047 0.0676 0.0295 -0.1867 -0.1391 0.0521 0.0349 0.0707 -0.1828 0.1060 -0.0937 -0.1508 -0.0852 0.0648 0.0471 -0.0340 -0.0727 0.1469 0.0657 0.3754 -0.1520 -0.1103 0.0072 -0.1809 0.0472 0.1635 -0.1598 -0.3670 -0.0037 -0.1924 0.3299 -0.0481 -0.0669 -0.0296 -0.0810 -0.0157 0.1708 0.0786 \nsowell 0.0308 -0.0918 0.0211 0.1284 0.0179 -0.0779 0.0265 -0.0629 0.0637 -0.0210 -0.0142 0.0148 0.1363 0.1027 -0.0471 0.1066 -0.0390 -0.0483 -0.1622 0.0103 -0.0346 -0.0423 -0.1510 -0.1698 0.0949 0.2179 0.0193 0.0842 0.1276 -0.1115 -0.0696 0.0200 -0.1234 -0.2405 -0.2631 -0.2407 0.2315 0.2028 -0.0108 0.2138 -0.2969 -0.2546 0.2388 -0.1592 -0.1303 0.2801 0.2393 0.0592 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head -1 out_vec.txt \nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\nTerminal\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ tail -5 out_vec.txt \npovey 0.1855 -0.0944 0.0717 0.0137 0.3037 0.1172 0.1695 -0.2207 -0.2558 -0.2055 0.2382 0.0017 -0.1664 0.0394 -0.1281 0.3221 -0.0165 0.1347 0.0769 0.0703 -0.0993 -0.2320 -0.1133 -0.0598 -0.0214 0.1051 0.1176 -0.1253 -0.0214 0.1301 -0.1389 -0.2233 -0.1817 0.0542 -0.1167 0.0473 0.0593 -0.1360 0.1194 0.0694 0.1798 -0.1673 -0.1586 -0.1303 0.0200 0.0181 0.0473 -0.1375 \nyoyogi 0.0426 0.0003 -0.1796 -0.2146 -0.0519 -0.0270 0.0178 0.0793 0.0096 0.0285 0.2679 0.0690 -0.2021 -0.0247 0.3348 -0.0425 0.2201 -0.0012 -0.0638 -0.0919 -0.0230 -0.0751 -0.1723 0.0395 -0.0372 0.1007 0.2209 -0.0106 0.0665 -0.0620 -0.1566 -0.1946 0.0124 -0.3048 0.1834 0.0431 -0.2530 0.0037 -0.1254 0.0387 -0.0598 0.2520 -0.0033 -0.1544 -0.0200 0.3011 0.1653 -0.1687 \npover 0.0101 0.0160 -0.0599 -0.0762 -0.0088 -0.0031 -0.0703 -0.0178 -0.0647 0.0384 0.0263 0.1612 -0.2271 0.2278 0.0696 0.0904 -0.0471 0.1998 -0.1184 -0.0872 -0.2935 0.0010 0.0704 -0.1177 0.1419 0.1858 -0.2007 0.1042 -0.0548 -0.0882 0.0791 0.0569 0.1203 0.0790 0.1630 0.0540 -0.0414 -0.0824 -0.0857 0.3505 -0.1964 -0.1329 0.0667 -0.1782 -0.1268 0.4447 -0.1017 0.1892 \nmattek-sands 0.0976 0.0061 0.0038 -0.0612 -0.0434 -0.0615 0.1116 0.0557 0.0742 -0.0170 -0.1538 0.0007 -0.0159 -0.0057 0.1259 -0.0917 0.1878 -0.0632 -0.1555 0.0847 -0.0801 0.1519 -0.0296 0.2367 0.3086 0.2105 -0.1150 0.1443 0.2199 0.0822 -0.1501 0.0707 0.0278 -0.2357 0.3132 -0.2210 0.0888 -0.0694 -0.0745 -0.1735 -0.2447 0.0386 0.0663 -0.1861 0.1029 0.0464 -0.3628 0.0425 \nsteelhead -0.0988 0.2772 -0.2653 0.1573 -0.0628 -0.0763 0.0049 0.1066 0.0501 -0.3517 0.1783 0.0748 0.3490 0.1317 -0.2024 0.0931 0.0975 -0.0248 0.1177 -0.1384 0.0562 -0.0417 0.1516 -0.2274 -0.1648 -0.0852 0.0422 0.2284 -0.1228 0.1553 0.0434 -0.0383 0.0109 0.1200 -0.0206 0.0638 -0.1681 0.2414 0.0267 0.0465 0.0655 0.1238 0.2046 0.0211 0.0609 -0.1298 0.0099 0.0043 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\n\nretrofitting\u88dc\u6b63\u5f8c\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u3092 \u4f7f\u3063\u3066\u3001word_similarity, word_analogy\u30bf\u30b9\u30af \u3092 gensim \u3067 \u6f14\u7b97\u3057\u3066\u307f\u308b\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 2.7.11 |Anaconda 4.0.0 (x86_64)| (default, Dec  6 2015, 18:57:58) \n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n>>>\n\n\n\n\u5b9f\u884c\u30a8\u30e9\u30fc\n\n\ngensim.models.Word2Vec.load_word2vec_format \u30e1\u30bd\u30c3\u30c9 \u3067 \u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb \u3092 \u751f\u6210\u3067\u304d\u306a\u3044\n\n\nPython 2.7\n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"out_vec.txt\", binary=False)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/hirofumiyashima/.pyenv/versions/anaconda-4.0.0/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 1086, in load_word2vec_format\n    vocab_size, vector_size = map(int, header.split())  # throws for invalid file format\nValueError: invalid literal for int() with base 10: 'biennials'\n>>> \n\n\n\n\u5b9f\u884c\u30a8\u30e9\u30fc\n\n\ngensim.models.Word2Vec.load_word2vec_format \u30e1\u30bd\u30c3\u30c9 \u3067 \u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb \u3092 \u751f\u6210\u3067\u304d\u306a\u3044\n\n\nPython 2.7\n>>> model = gensim.models.Word2Vec.load_word2vec_format('out_vec.txt', binary=False)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/hirofumiyashima/.pyenv/versions/anaconda-4.0.0/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 1086, in load_word2vec_format\n    vocab_size, vector_size = map(int, header.split())  # throws for invalid file format\nValueError: invalid literal for int() with base 10: 'biennials'\n>>> \n\n\n\ngensim Word2Vec\u30e2\u30b8\u30e5\u30fc\u30eb \u304c\u8aad\u307f\u8fbc\u307f\u53ef\u80fd\u306a \u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u306f\u3001\uff11\u884c\u76ee\u306b\u30b3\u30fc\u30d1\u30b9\u4ef6\u6570\uff08\u30d5\u30a1\u30a4\u30eb\u884c\u6570\uff09\u3068\u7279\u5fb4\u7a7a\u9593\u306e\u6b21\u5143\u306e\u6570\uff08\u30d5\u30a1\u30a4\u30eb\u5217\u6570\uff09\u3092\u8a18\u8ff0\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u306a\u69d8\u3067\u3059\u3002\n\n\n\u4ee5\u4e0b\u306e\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u306b\u306a\u3089\u3063\u3066\u3001\uff11\u884c\u76ee \u3092 \u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n\n kanjirz50\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/23\uff09\u300cGloVe\u304c\u51fa\u529b\u3057\u305f\u30e2\u30c7\u30eb(txt)\u3092gensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u6574\u5f62\u3059\u308b\u3002\u300d\n\n\n\nGloVe\u3067\u5b66\u7fd2\u3059\u308b\u3068\u3001\u6a19\u6e96\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u306fvector.bin\u3068vector.txt\u304c\u51fa\u529b\u3055\u308c\u308b\u3002\n\u3053\u308c\u3089\u306fgensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u306a\u3044\u306e\u3067\u3001\u8aad\u307f\u8fbc\u3081\u308b\u5f62\u5f0f\u306b\u6574\u5f62\u3059\u308b\u3002\nword2vec\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n1\u884c\u76ee\uff1a\u5358\u8a9e\u7a2e\u985e\u6570 \u6b21\u5143\u6570\n2\u884c\u76ee\u4ee5\u964d\uff1a\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 \u2026\n\u4e0a\u8a18\u306e\u3088\u3046\u306a\u30b7\u30f3\u30d7\u30eb\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u306a\u3063\u3066\u3044\u308b\u3002\nGloVe\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 \u2026\n\u4e0a\u8a18\u306e\u3088\u3046\u306b\u3001word2vec\u306e1\u884c\u76ee\u304c\u6b20\u3051\u3066\u3044\u308b\u3002\n\u3064\u307e\u308a\u5358\u8a9e\u7a2e\u985e\u6570\u3068\u6b21\u5143\u6570\u30921\u884c\u76ee\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u5f62\u5f0f\u304c\u5909\u63db\u3067\u304d\u308b\u3002\n\n\u4eca\u56de\u4f7f\u3046\u306e\u306f\u3001(word2vec\u3088\u308a\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u826f\u3044\u3068\u3055\u308c\u308b\uff09GloVe\u304b\u3089\u51fa\u529b\u3057\u305f\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u30d5\u30a1\u30a4\u30eb\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09\u3067\u306f\u306a\u304c\u3001\u6700\u5f8c\u306e\uff12\u884c\u306e\u90fd\u5408\u306f\u540c\u3058\u3002\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u501f\u7528\u3057\u3066\u3001retrofirt.py \u304b\u3089\u51fa\u529b\u3055\u308c\u305f \u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3000\u30d5\u30a1\u30a4\u30eb\u306b\uff11\u884c\u76ee\u3092\u8ffd\u52a0\u3059\u308b\u3002\n\nglove2word2vec_format.py\uff08Python3\u74b0\u5883\uff09\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nprint\u30e1\u30bd\u30c3\u30c9\u306eend\u30aa\u30d7\u30b7\u30e7\u30f3\uff08\u5404\u884c\u306e\u6b21\u306b\u6539\u884c\u633f\u5165\u3057\u306a\u3044\uff09\u306fPython3\u304b\u3089\u4f7f\u7528\u3067\u304d\u308b\u306e\u3067\u3001glove2word2vec_format.py\u3000\u306f\u3001Python 3\u74b0\u5883\u3067\u4f7f\u7528\u3059\u308b\u3002\n\n\nTermina\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ cat glove2word2vec_format.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n\n\n\nPython3\u74b0\u5883 \u3067  glove2word2vec_format.py \u3092 \u5b9f\u884c\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python glove2word2vec_format.py out_vec.txt > gensim_vector.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n\n\uff11\u884c\u76ee\u306b\u3001\u4ee5\u4e0b\u304c\u633f\u5165\u3055\u308c\u3066\u3044\u308b\u3002\n\n1000 48\n\n\nTerminal\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$head gensim_vector.txt \n1000 48\nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \n\n\uff08 \u4e2d\u7565 \uff09\n\nchaib 0.1786 -0.0779 -0.0624 -0.0319 0.3098 -0.0052 0.1528 -0.0334 0.0106 0.1000 0.2047 0.0676 0.0295 -0.1867 -0.1391 0.0521 0.0349 0.0707 -0.1828 0.1060 -0.0937 -0.1508 -0.0852 0.0648 0.0471 -0.0340 -0.0727 0.1469 0.0657 0.3754 -0.1520 -0.1103 0.0072 -0.1809 0.0472 0.1635 -0.1598 -0.3670 -0.0037 -0.1924 0.3299 -0.0481 -0.0669 -0.0296 -0.0810 -0.0157 0.1708 0.0786 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n\n\n\n\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u30d5\u30a1\u30a4\u30eb \u3092 \u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb\u30fb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u751f\u6210 \u306b \u6210\u529f\n\n\nPython 3.5.2\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"gensim_vector.txt\", binary=False)\n>>> print(model)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>>\n\n\n\nmost_similar\u30e1\u30bd\u30c3\u30c9\n\n>>> model.most_similar(\"wrong\")\n[('trivial', 0.5341627597808838), ('extent', 0.48358750343322754), ('whoever', 0.4811513125896454), ('optimist', 0.46935686469078064), ('tempted', 0.4658152759075165), ('quibble', 0.46456658840179443), ('unequivocally', 0.44248801469802856), ('fix', 0.43918511271476746), ('jerk', 0.42578360438346863), ('fit', 0.41329652070999146)]\n>>> \n>>> model.most_similar(\"welcomed\")\n[('welcomes', 0.9832462072372437), ('spoken', 0.6331268548965454), ('secretary-general', 0.6301525831222534), ('congratulations', 0.542494535446167), ('thanking', 0.5062479972839355), ('doubts', 0.5043333768844604), ('exclaimed', 0.47787442803382874), ('chair', 0.41937389969825745), ('drafted', 0.4132911264896393), ('non-militarized', 0.4084121584892273)]\n>>> \n>>> model.most_similar(\"yahoo\")\n[('dish.o', 0.7728222012519836), ('vonage', 0.7293688654899597), ('telia', 0.639286458492279), ('co-marketing', 0.5994674563407898), ('olympus', 0.5876779556274414), ('regus', 0.582847535610199), ('nasdaq-listed', 0.5645484328269958), ('finnish-german', 0.5628683567047119), ('lossmaking', 0.5567153692245483), ('publicly-traded', 0.5565172433853149)]\n>>> \n>>> model.most_similar(\"military-based\")\n[('president-for-life', 0.5531608462333679), ('abacha', 0.5091353058815002), ('despised', 0.5032479166984558), ('kremlin', 0.4599403142929077), ('month-long', 0.43890973925590515), ('shinawatra', 0.4219447374343872), ('regularize', 0.42181631922721863), ('old-guard', 0.4092077612876892), ('vice-like', 0.38102418184280396), ('lifeline', 0.38001883029937744)]\n>>> \n>>> model.most_similar(\"kids\")\n[('obese', 0.4557153284549713), ('spaying', 0.4185415208339691), ('olds', 0.4133707880973816), ('wrong', 0.37881478667259216), ('needed', 0.3663429021835327), ('fit', 0.360312283039093), ('mentors', 0.3448786735534668), ('potty-training', 0.34364503622055054), ('fix', 0.3323163688182831), ('screaming', 0.3309297561645508)]\n>>> \n>>> model.most_similar(\"olympus\")\n[('tookey', 0.6202044486999512), ('yahoo', 0.5876779556274414), ('pggm', 0.580516517162323), ('irg', 0.5800860524177551), ('sacyr', 0.5752755403518677), ('jih', 0.5725178122520447), ('fiduciary', 0.5634198188781738), ('corporate', 0.5586938858032227), ('publicly-traded', 0.5569629073143005), ('nasdaq-listed', 0.5287851095199585)]\n>>>\n\n\nsimilarity\u30e1\u30bd\u30c3\u30c9\n\n>>> model.similarity(\"olympus\", \"yahoo\")\n0.58767794860053679\n>>>\n\n\nmost_similar\u30e1\u30bd\u30c3\u30c9\uff08posiutive\u5358\u8a9e \u3068  nefgative\u5358\u8a9e \u53cc\u65b9 \u3092 \u6307\u5b9a\uff09\n\n>>> model.most_similar(positive=[\"olympus\"], negative=[\"yahoo\"])\n[('ofsted', 0.44535747170448303), ('fiduciary', 0.426706999540329), ('post-accident', 0.4187975525856018), ('preuss', 0.3981691002845764), ('ex-fbi', 0.38794970512390137), ('super-safe', 0.3831671476364136), ('comically', 0.374136358499527), ('riskiness', 0.36748120188713074), ('shirked', 0.3594794273376465), ('stapel', 0.35505157709121704)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"olympus\"])\n[('techland', 0.558988094329834), ('ivillage.com', 0.5493491291999817), ('whosay', 0.5184122920036316), ('net-a-porter.com', 0.49817216396331787), ('icloud', 0.49218812584877014), ('thalia', 0.488607794046402), ('usenet', 0.48386359214782715), ('stahler', 0.42786073684692383), ('vonage', 0.421731173992157), ('webpage', 0.41963058710098267)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"kids\"], negative=[\"olympus\"])\n[('ivillage.com', 0.42363297939300537), ('olds', 0.4227023124694824), ('techland', 0.39514291286468506), ('surfing', 0.39229321479797363), ('thalia', 0.39018523693084717), ('junhasavasdikul', 0.3871711492538452), ('usenet', 0.36667299270629883), ('obese', 0.3630061745643616), ('perchance', 0.3595729470252991), ('luba', 0.35373014211654663)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"olympus\", \"kids\"])\n[('jarrin', 0.4498675465583801), ('net-a-porter.com', 0.42687320709228516), ('counter-sued', 0.42193296551704407), ('first-rounder', 0.3740149736404419), ('techland', 0.35563167929649353), ('dnp', 0.3551561236381531), ('whosay', 0.3550400137901306), ('clottes', 0.3525778651237488), ('wintonlos', 0.34782421588897705), ('stahler', 0.3322279453277588)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"kids\"])\n[('dish.o', 0.5999091863632202), ('vonage', 0.5247481465339661), ('telia', 0.5069398283958435), ('co-marketing', 0.5059320330619812), ('nasdaq-listed', 0.49054503440856934), ('publicly-traded', 0.4846886098384857), ('regus', 0.4727519154548645), ('lossmaking', 0.4521976709365845), ('counter-sued', 0.45045703649520874), ('olympus', 0.43984055519104004)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"kids\"], negative=[])\n[('tech', 0.5492722392082214), ('vonage', 0.5070191621780396), ('woods', 0.5068948864936829), ('dish.o', 0.49435973167419434), ('telia', 0.39850202202796936), ('techland', 0.39513349533081055), ('irc', 0.3942038416862488), ('olympus', 0.3918909430503845), ('finnish-german', 0.3896961212158203), ('corporate', 0.37088701128959656)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"olympus\"], negative=[])\n[('dish.o', 0.6819705963134766), ('publicly-traded', 0.6248651146888733), ('nasdaq-listed', 0.6135591864585876), ('vonage', 0.6036995053291321), ('lossmaking', 0.601861298084259), ('regus', 0.5796098709106445), ('sacyr', 0.5676641464233398), ('corporate', 0.5568501949310303), ('tookey', 0.5475372076034546), ('co-marketing', 0.5441089272499084)]\n>>> \n>>> model.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"]\n... )\n[('cherbourg', 0.3693275451660156), ('signal-callers', 0.36640459299087524), ('hamel', 0.3584540784358978), ('lucero', 0.3427450954914093), ('quinnell', 0.34220728278160095), ('wintonlos', 0.3380362391471863), ('kavaler', 0.32803821563720703), ('mcdizzy', 0.32609716057777405), ('hamnett', 0.32079148292541504), ('borle', 0.3176853060722351)]\n>>> \n>>> model.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.3693275451660156), ('signal-callers', 0.36640459299087524), ('hamel', 0.3584540784358978), ('lucero', 0.3427450954914093), ('quinnell', 0.34220728278160095), ('wintonlos', 0.3380362391471863), ('kavaler', 0.32803821563720703), ('mcdizzy', 0.32609716057777405), ('hamnett', 0.32079148292541504), ('borle', 0.3176853060722351)]\n>>> \n>>> quit()\n\n\n\n\u3010 \u6bd4\u8f03 \u3011retrofit.py \u524d\u5f8c\n\nretrofit.py \u3067 \u88dc\u6b63\u3059\u308b\u524d\u306e\uff08\u666e\u901a\u306eword2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\uff09\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u3067 most_similar\u30e1\u30bd\u30c3\u30c9\u7b49\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c__\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv versions\n  system\n  2.7\n  3.5.0\n  3.5.1\n  3.5.1/envs/py351tensorflow\n* 3.5.2 (set by /Users/hirofumiyashima/Desktop/retrofit_word2vec/retrofitting/.python-version)\n  anaconda-4.0.0\n  py351tensorflow\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python glove2word2vec_format.py sample_vec.txt > gensim_sample_vector.txt\n\n\n1\u884c\u76ee\u306b\u3001\u4ee5\u4e0b\u304c\u633f\u5165\u3055\u308c\u3066\u3044\u308b\n\n\n1000 48\n\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head gensim_sample_vector.txt \n1000 48\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nverplank 0.061966 0.013867 0.097894 0.069291 0.025128 -0.029515 -0.0068756 -0.21864 -0.025601 -0.047496 -0.091767 0.081062 0.11705 0.025643 -0.051421 -0.074378 0.012586 -0.026915 0.19103 -0.15763 0.051448 0.13455 -0.040023 0.2077 0.017204 0.22573 0.083228 -0.0079535 -0.11535 0.17306 0.014453 -0.19401 -0.1076 -0.16378 0.25349 -0.18004 -0.044328 -0.16348 -0.091957 -0.36885 -0.01324 0.0078103 -0.00681 -0.021193 -0.11642 0.23982 -0.23426 0.43564\nclottes -0.0089251 -0.061586 0.1221 0.035306 -0.061313 -0.04685 0.11966 0.084184 0.057614 0.16206 0.10281 -0.22645 -0.015185 0.050584 -0.04209 -0.026132 0.011169 0.23204 0.076184 0.069735 -0.11726 0.050913 -0.087998 0.15215 0.082915 -0.055324 -0.10663 -0.084158 -0.059272 0.37413 0.25528 -0.12261 -0.2272 -0.028977 0.021512 0.073214 -0.30931 0.29448 0.066778 -0.092153 -0.14165 0.11589 -0.11014 0.040938 -0.33708 -0.030592 0.22528 0.16051\nnunnery 0.277 0.019486 -0.23815 -0.10068 -0.17544 -0.2148 -0.1853 -0.010177 -0.019739 -0.03499 -0.0099115 -0.099382 -0.11386 0.090788 0.21348 0.19557 -0.03373 0.17034 -0.21515 -0.044315 0.014785 -0.211 0.036734 0.12371 -0.30186 0.22997 -0.23255 0.0052285 0.029958 0.079806 -0.02739 -0.076451 0.053968 -0.1083 -0.067588 0.026335 -0.13879 0.027829 -0.026704 0.29506 0.018326 0.14407 -0.26815 -0.13034 -0.015692 0.18495 -0.045091 -0.04577\n\n\uff08 \u4e2d\u7565 \uff09\npiders -0.098084 0.17303 -0.1513 0.23683 0.046248 -0.090944 -0.068375 0.074935 -0.026971 -0.2084 0.052636 -0.027759 0.23191 0.15999 -0.38536 0.05138 0.32286 -0.024479 0.082912 -0.2383 0.14478 0.070837 -0.16746 -0.13519 -0.078819 0.043042 0.11614 0.052073 0.11514 -0.023325 0.25228 0.015687 0.054137 -0.1127 -0.18158 0.01556 -0.020509 -0.2665 0.18425 0.03688 0.040863 0.10596 0.011119 0.012524 0.20098 -0.093732 -0.067517 0.13145\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model_sample_vec = gensim.models.Word2Vec.load_word2vec_format(\"gensim_sample_vector.txt\", binary=False)\n>>> print(model_sample_vec)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> model_sample_vec.most_similar(\"wrong\")\n[('trivial', 0.5341147780418396), ('nature', 0.5161055326461792), ('extent', 0.4835756719112396), ('whoever', 0.4812297821044922), ('optimist', 0.46935874223709106), ('tempted', 0.46578240394592285), ('quibble', 0.4645503759384155), ('unequivocally', 0.44251543283462524), ('fix', 0.43913888931274414), ('jerk', 0.4257252812385559)]\n>>> \n>>> model_sample_vec.most_similar(\"welcomed\")\n[('welcomes', 0.8587126731872559), ('spoken', 0.6633444428443909), ('secretary-general', 0.6413360238075256), ('doubts', 0.5253970623016357), ('ire', 0.517067015171051), ('thanking', 0.4925107955932617), ('congratulations', 0.4785553514957428), ('exclaimed', 0.46664339303970337), ('wednesday', 0.44500139355659485), ('chair', 0.43357059359550476)]\n>>> \n>>> model_sample_vec.most_similar(\"yahoo\")\n[('dish.o', 0.7727800607681274), ('vonage', 0.7294098734855652), ('telia', 0.6392509341239929), ('co-marketing', 0.5994648337364197), ('olympus', 0.5876917243003845), ('regus', 0.5828214883804321), ('nasdaq-listed', 0.5644622445106506), ('finnish-german', 0.5628864765167236), ('lossmaking', 0.5566997528076172), ('publicly-traded', 0.5564723014831543)]\n>>> \n>>> model_sample_vec.most_similar(\"military-based\")\n[('president-for-life', 0.5531935691833496), ('abacha', 0.5091578364372253), ('despised', 0.5032562017440796), ('kremlin', 0.45989224314689636), ('month-long', 0.4389973282814026), ('shinawatra', 0.4218860864639282), ('regularize', 0.4218169152736664), ('old-guard', 0.40915483236312866), ('vice-like', 0.38100743293762207), ('lifeline', 0.38002198934555054)]\n>>> \n>>> model_sample_vec.most_similar(\"kids\")\n[('obese', 0.4557613432407379), ('spaying', 0.41861096024513245), ('olds', 0.41338664293289185), ('wrong', 0.37878769636154175), ('needed', 0.3663170635700226), ('fit', 0.3603125214576721), ('mentors', 0.3448629379272461), ('potty-training', 0.3436267077922821), ('fix', 0.3324095904827118), ('screaming', 0.3309009373188019)]\n>>> \n>>> model_sample_vec.most_similar(\"olympus\")\n[('tookey', 0.6201847791671753), ('yahoo', 0.5876917839050293), ('pggm', 0.580501139163971), ('irg', 0.5801130533218384), ('sacyr', 0.5752725601196289), ('jih', 0.5724684000015259), ('fiduciary', 0.56339430809021), ('corporate', 0.5586907863616943), ('publicly-traded', 0.5569700598716736), ('nasdaq-listed', 0.5287832021713257)]\n>>> \n>>> model_sample_vec.similarity(\"olympus\", \"yahoo\")\n0.58769171764588612\n>>> \n>>> model_sample_vec.most_similar(positive=[\"olympus\"], negative=[\"yahoo\"])\n[('ofsted', 0.4454064667224884), ('fiduciary', 0.4266740083694458), ('post-accident', 0.41873520612716675), ('preuss', 0.39813536405563354), ('ex-fbi', 0.3879525065422058), ('super-safe', 0.3832082152366638), ('comically', 0.3741567134857178), ('riskiness', 0.36750614643096924), ('shirked', 0.35946837067604065), ('stapel', 0.35496634244918823)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"olympus\"])\n[('techland', 0.5590645670890808), ('ivillage.com', 0.5493117570877075), ('whosay', 0.5184593796730042), ('net-a-porter.com', 0.4981782138347626), ('icloud', 0.4922236204147339), ('thalia', 0.48856472969055176), ('usenet', 0.48386064171791077), ('stahler', 0.4279034733772278), ('vonage', 0.42177340388298035), ('webpage', 0.4197327792644501)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"kids\"], negative=[\"olympus\"])\n[('ivillage.com', 0.4235672354698181), ('olds', 0.42271482944488525), ('techland', 0.3952169418334961), ('surfing', 0.39227139949798584), ('thalia', 0.3901672065258026), ('junhasavasdikul', 0.38726335763931274), ('usenet', 0.36668533086776733), ('obese', 0.36304527521133423), ('perchance', 0.35954704880714417), ('luba', 0.35366585850715637)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"olympus\", \"kids\"])\n[('jarrin', 0.4498426914215088), ('net-a-porter.com', 0.42685195803642273), ('counter-sued', 0.4218709170818329), ('first-rounder', 0.37408456206321716), ('techland', 0.35565197467803955), ('dnp', 0.35518622398376465), ('whosay', 0.3550260663032532), ('clottes', 0.35258710384368896), ('wintonlos', 0.3478633761405945), ('stahler', 0.33223098516464233)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"kids\"])\n[('dish.o', 0.5998775362968445), ('vonage', 0.5247606039047241), ('telia', 0.5068938732147217), ('co-marketing', 0.5059046745300293), ('nasdaq-listed', 0.4904913306236267), ('publicly-traded', 0.48468953371047974), ('regus', 0.4727250933647156), ('lossmaking', 0.45219260454177856), ('counter-sued', 0.45042723417282104), ('olympus', 0.43985074758529663)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"kids\"], negative=[])\n[('tech', 0.5492537617683411), ('vonage', 0.5070642232894897), ('woods', 0.5069363117218018), ('dish.o', 0.4943317770957947), ('telia', 0.39849722385406494), ('techland', 0.3951759338378906), ('irc', 0.3942807614803314), ('olympus', 0.3919001519680023), ('finnish-german', 0.38974136114120483), ('corporate', 0.3707897365093231)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"olympus\"], negative=[])\n[('dish.o', 0.6819364428520203), ('publicly-traded', 0.6248412132263184), ('nasdaq-listed', 0.613507091999054), ('vonage', 0.6037248969078064), ('lossmaking', 0.6018809080123901), ('regus', 0.5795953869819641), ('sacyr', 0.5676511526107788), ('corporate', 0.5568079352378845), ('tookey', 0.5475448369979858), ('co-marketing', 0.5441123843193054)]\n>>> \n>>> model_sample_vec.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.36930254101753235), ('signal-callers', 0.3664279580116272), ('hamel', 0.35849088430404663), ('lucero', 0.34278666973114014), ('quinnell', 0.3422016501426697), ('wintonlos', 0.33804047107696533), ('kavaler', 0.3280620574951172), ('mcdizzy', 0.326118528842926), ('hamnett', 0.32082003355026245), ('borle', 0.3177386522293091)]\n>>> \n>>> model_sample_vec.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.36930254101753235), ('signal-callers', 0.3664279580116272), ('hamel', 0.35849088430404663), ('lucero', 0.34278666973114014), ('quinnell', 0.3422016501426697), ('wintonlos', 0.33804047107696533), ('kavaler', 0.3280620574951172), ('mcdizzy', 0.326118528842926), ('hamnett', 0.32082003355026245), ('borle', 0.3177386522293091)]\n>>>\n>>> quit()\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n\n\nPython 2.7\u3067\u5b9f\u884c\u3057\u305f\u5834\u5408\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 2.7\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 2.7 (r27:82500, Jul 23 2016, 15:46:16) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"gensim_sample_vector.txt\", binary=False)\n>>> print(model)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>> \n>>> model.most_similar(\"wrong\")\n[(u'trivial', 0.5341147780418396), (u'nature', 0.5161055326461792), (u'extent', 0.4835756719112396), (u'whoever', 0.4812297821044922), (u'optimist', 0.46935874223709106), (u'tempted', 0.46578240394592285), (u'quibble', 0.4645503759384155), (u'unequivocally', 0.44251543283462524), (u'fix', 0.43913888931274414), (u'jerk', 0.4257252812385559)]\n>>> \n>>> model.most_similar(\"welcomed\")\n[(u'welcomes', 0.8587126731872559), (u'spoken', 0.6633444428443909), (u'secretary-general', 0.6413360238075256), (u'doubts', 0.5253970623016357), (u'ire', 0.517067015171051), (u'thanking', 0.4925107955932617), (u'congratulations', 0.4785553514957428), (u'exclaimed', 0.46664339303970337), (u'wednesday', 0.44500139355659485), (u'chair', 0.43357059359550476)]\n>>> \n>>> quit()\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n\n\n\n\u3010 \u53c2\u8003 \u3011\n\nLIFE WITH PYTHON \uff082013/12/23\uff09\u300cPython Tips\uff1a\u6539\u884c\u306a\u3057\u3067\u6587\u5b57\u5217\u3092\u51fa\u529b\u3057\u305f\u3044\u300d\ngensim, models.word2vec \u2013 Deep learning with word2vec\n\nInitialize a model with e.g.:\n>>> model = Word2Vec(sentences, size=100, window=5,min_count=5, workers=4)\n\nPersist a model to disk with:\n>>> model.save(fname)\n>>> model = Word2Vec.load(fname)  # you can continue training with the loaded model!\n\nThe model can also be instantiated from an existing file on disk in the word2vec C format:\n>>> model = Word2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False)  # C text format\n>>> model = Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)  # C binary format\n\nYou can perform various syntactic/semantic NLP word tasks with the model. Some of them are already built-in:\n>>> model.most_similar(positive=['woman', 'king'], negative=['man'])\n[('queen', 0.50882536), ...]\n\n>>> model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n'cereal'\n\n>>> model.similarity('woman', 'man')\n0.73723527\n\n>>> model['computer']  # raw numpy vector of a word\narray([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)\n\n\n###__\u3010 \u8ad6\u6587 \u3011__\n\n* [Manaal Faruqui _et.al, Retrofitting Word Vectors to Semantic Lexicons_](https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf)\n\n###__\u3010 \u5b9f\u88c5\u30b3\u30fc\u30c9 \u3011__\n\n* [\uff08 GitHub \uff09mfaruqui/retrofitting](https://github.com/mfaruqui/retrofitting)\n\n___\n\n##__GitHub \u304b\u3089 git clone__\n\n```{bash:Terminal}\nLast login: Mon Aug 15 09:50:36 on ttys000\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ cd Desktop/\nHirofumiYashima-no-MacBook:Desktop hirofumiyashima$ ls | grep retro\nretrofit_word2vec\nHirofumiYashima-no-MacBook:Desktop hirofumiyashima$ cd retrofit_word2vec/\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ git clone https://github.com/mfaruqui/retrofitting.git\nCloning into 'retrofitting'...\nremote: Counting objects: 37, done.\nremote: Total 37 (delta 0), reused 0 (delta 0), pack-reused 37\nUnpacking objects: 100% (37/37), done.\nChecking connectivity... done.\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n##__clone\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u6210 \u3092 \u78ba\u8a8d__\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls retrofitting/\nLICENSE\t\tREADME.md\tlexicons\tretrofit.py\tsample_vec.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head sample_vec.txt\nhead: sample_vec.txt: No such file or directory\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n###__\u5916\u90e8\u77e5\u8b58DB\u3092\u53c2\u7167\u3059\u308b\u3053\u3068\u3067\u3001\u88dc\u6b63\u3092\u884c\u3046\u5bfe\u8c61\u306b\u3059\u308b \u4e8b\u524d\u306b\u901a\u5e38\uff08Mikolov\u8ad6\u6587\u30d9\u30fc\u30b9\uff09\u306e word2vec\u304b\u3089\u751f\u6210\u3055\u305b\u305f\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306e \u30b5\u30f3\u30d7\u30eb\u30fb\u30d5\u30a1\u30a4\u30eb \u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/sample_vec.txt\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nverplank 0.061966 0.013867 0.097894 0.069291 0.025128 -0.029515 -0.0068756 -0.21864 -0.025601 -0.047496 -0.091767 0.081062 0.11705 0.025643 -0.051421 -0.074378 0.012586 -0.026915 0.19103 -0.15763 0.051448 0.13455 -0.040023 0.2077 0.017204 0.22573 0.083228 -0.0079535 -0.11535 0.17306 0.014453 -0.19401 -0.1076 -0.16378 0.25349 -0.18004 -0.044328 -0.16348 -0.091957 -0.36885 -0.01324 0.0078103 -0.00681 -0.021193 -0.11642 0.23982 -0.23426 0.43564\nclottes -0.0089251 -0.061586 0.1221 0.035306 -0.061313 -0.04685 0.11966 0.084184 0.057614 0.16206 0.10281 -0.22645 -0.015185 0.050584 -0.04209 -0.026132 0.011169 0.23204 0.076184 0.069735 -0.11726 0.050913 -0.087998 0.15215 0.082915 -0.055324 -0.10663 -0.084158 -0.059272 0.37413 0.25528 -0.12261 -0.2272 -0.028977 0.021512 0.073214 -0.30931 0.29448 0.066778 -0.092153 -0.14165 0.11589 -0.11014 0.040938 -0.33708 -0.030592 0.22528 0.16051\nnunnery 0.277 0.019486 -0.23815 -0.10068 -0.17544 -0.2148 -0.1853 -0.010177 -0.019739 -0.03499 -0.0099115 -0.099382 -0.11386 0.090788 0.21348 0.19557 -0.03373 0.17034 -0.21515 -0.044315 0.014785 -0.211 0.036734 0.12371 -0.30186 0.22997 -0.23255 0.0052285 0.029958 0.079806 -0.02739 -0.076451 0.053968 -0.1083 -0.067588 0.026335 -0.13879 0.027829 -0.026704 0.29506 0.018326 0.14407 -0.26815 -0.13034 -0.015692 0.18495 -0.045091 -0.04577\nclottey 0.13007 -0.091402 0.0037005 0.014496 -0.067745 -0.032454 0.31061 0.08075 0.031257 0.13992 -0.099519 -0.072196 0.15347 -0.10603 -0.1111 0.014984 0.0068156 -0.096376 0.020761 -0.12781 -0.041597 0.28228 0.041162 0.075579 0.041916 0.28711 0.024134 0.34505 -0.071057 0.21699 0.021115 -0.029354 -0.054196 0.013538 0.17918 -0.092146 -0.099105 -0.045168 -0.16692 -0.40238 -0.27625 0.028427 0.050746 -0.021625 -0.12425 -0.18158 0.058694 0.18238\nsowell 0.030762 -0.091813 0.021076 0.12843 0.0179 -0.077895 0.026529 -0.062911 0.063693 -0.020966 -0.014219 0.014767 0.13628 0.1027 -0.04707 0.10664 -0.039 -0.048278 -0.16216 0.010273 -0.03461 -0.042256 -0.15101 -0.16981 0.094874 0.21788 0.019252 0.084203 0.12758 -0.11146 -0.069592 0.019972 -0.12341 -0.24046 -0.26306 -0.24068 0.23155 0.2028 -0.010772 0.21385 -0.29686 -0.25456 0.23879 -0.15923 -0.13032 0.28009 0.23929 0.05916\nwoods 0.049942 0.042324 0.018938 0.064624 -0.064808 -0.086171 -0.075199 -0.19779 -0.055387 -0.23949 -0.0056237 0.1199 0.070734 0.091679 -0.18294 -0.11322 0.032094 -0.072929 0.066702 -0.070065 0.015355 0.030946 -0.26707 0.027615 -0.060169 0.22764 0.11389 0.030353 -0.062781 0.084065 -0.012644 -0.075017 -0.03808 -0.20063 0.068258 -0.025851 0.044887 -0.058646 -0.10062 -0.33411 -0.25018 -0.099802 0.13244 0.012939 -0.13918 0.24713 -0.39212 0.35557\nclotted -0.15325 0.17469 -0.14858 0.16705 -0.061657 0.38397 0.015206 0.11725 0.096851 -0.10926 0.20226 0.12276 -0.084128 -0.10981 0.11 -0.11522 -0.23358 -0.17534 0.23471 0.1212 -0.14923 0.15468 0.12293 0.15476 0.0097238 0.065649 -0.28607 -0.070528 0.041749 -0.075993 0.11935 -0.014533 -0.22663 -0.067245 0.060851 -0.043102 -0.025318 0.24947 0.091302 0.035894 -0.056044 -0.1219 0.2178 0.096062 0.03495 -0.060335 -0.066316 -0.1939\nspiders -0.098084 0.17303 -0.1513 0.23683 0.046248 -0.090944 -0.068375 0.074935 -0.026971 -0.2084 0.052636 -0.027759 0.23191 0.15999 -0.38536 0.05138 0.32286 -0.024479 0.082912 -0.2383 0.14478 0.070837 -0.16746 -0.13519 -0.078819 0.043042 0.11614 0.052073 0.11514 -0.023325 0.25228 0.015687 0.054137 -0.1127 -0.18158 0.01556 -0.020509 -0.2665 0.18425 0.03688 0.040863 0.10596 0.011119 0.012524 0.20098 -0.093732 -0.067517 0.13145\nmdbs -0.16749 0.076818 -0.0078256 -0.23028 0.10919 -0.0029132 0.10671 0.022756 -0.0051267 -0.086064 -0.27925 0.057727 -0.079849 -0.0799 -0.061325 0.043617 -0.20629 -0.21419 -0.13687 -0.24101 -0.090371 0.081844 0.13152 0.14611 -0.18709 -0.30011 -0.080421 -0.11626 -0.048492 0.022221 0.22303 0.060831 0.004612 -0.092795 0.056692 0.075079 0.12904 -0.02208 -0.088094 0.23042 -0.1211 0.25484 0.095224 -0.27334 0.15141 0.2108 -0.074673 -0.17334\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head -1 retrofitting/sample_vec.txt\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -1 retrofitting/sample_vec.txt\nishida 0.23975 -0.010016 0.12995 -0.042012 0.05867 0.11263 0.22874 -0.083031 -0.15666 0.031165 -0.037073 0.032879 -0.060202 0.11307 -0.019522 0.14007 0.082405 0.35134 -0.18307 0.11955 -0.18926 -0.0361 0.017629 0.098925 0.011091 0.13668 -0.26085 -0.02082 -0.15929 -0.091311 0.27706 -0.12584 -0.11666 -0.2786 0.11303 0.032773 -0.12723 -0.23614 -0.16537 0.073717 0.10836 -0.046751 -0.28947 -0.096606 0.13055 0.10114 -0.027045 -0.018152\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/sample_vec.txt\n    1000 retrofitting/sample_vec.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls retrofitting/lexicons/\nframenet.txt\t\tppdb-xl.txt\t\twordnet-synonyms+.txt\twordnet-synonyms.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/lexicons/framenet.txt \nsource\nsprinkle splatter pump drip spray splash spatter squirt\nsplatter sprinkle pump drip spray splash spatter squirt\npump sprinkle splatter drip spray splash spatter squirt\ndrip sprinkle splatter pump spray splash spatter squirt\nspray sprinkle splatter drip pump splash spatter squirt\nsplash sprinkle splatter pump drip spray spatter squirt\nspatter sprinkle splatter pump drip spray splash squirt\nsquirt sprinkle splatter pump drip spray splash spatter\nrun-through rehearse run practice rehearsal mock drill exercise\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n###__\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08Framenet\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail retrofitting/lexicons/framenet.txt \nrenowned famous notoriety renown legendary stature notorious epic fame infamous reputation\nfamous renowned notoriety renown legendary stature notorious epic fame infamous reputation\nnotoriety renowned famous renown legendary stature notorious epic fame infamous reputation\nreputation renowned famous notoriety renown legendary stature notorious epic fame infamous\nlegendary renowned famous notoriety renown stature notorious epic fame infamous reputation\nstature renowned famous notoriety renown legendary notorious epic fame infamous reputation\nnotorious renowned famous notoriety renown legendary stature epic fame infamous reputation\nepic renowned famous notoriety renown legendary stature notorious fame infamous reputation\nfame renowned famous notoriety renown legendary stature notorious epic infamous reputation\ninfamous renowned famous notoriety renown legendary stature notorious epic fame reputation\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/framenet.txt \n   10822 retrofitting/lexicons/framenet.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n###__\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08ppdb\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d__\n\n####__PPDB(PlantPromoterDB)__\n\n* [PPDB: The Paraphrase Database](http://www.cis.upenn.edu/~ccb/ppdb/)\n\n* [\uff08 Web\u30d6\u30e9\u30a6\u30b6 \u691c\u7d22\u753b\u9762 \uff09paraphrase.org](http://paraphrase.org/#/)\n* [Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, _PPDB: The Paraphrase Database_](http://www.aclweb.org/anthology/N13-1092.pdf)\n\n___\n\n__\u3061\u306a\u307f\u306b\u3001\u4ee5\u4e0b\u306e\u65e5\u672c\u8a9e\u7248\u306ePPDB\u3082\u5b58\u5728\u3059\u308b\u3088\u3046\u3067\u3059\u3002__\n\n* [PPDB : Japanese - \u65e5\u672c\u8a9e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9](http://isw3.naist.jp/~masahiro-mi/jppdb/)\n\n> The Paraphrase Database : Japanese\u306f\u65e5\u82f1\u5bfe\u8a33\u30b3\u30fc\u30d1\u30b9\u304b\u3089\u5b66\u7fd2\u3055\u308c\u305f\u65e5\u672c\u8a9e\u306e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u96c6\u3067\u3059\u3002\n\n* [\u6c34\u4e0a \u96c5\u535a \u4ed6 \u300c\u65e5\u672c\u8a9e\u8a00\u3044\u63db\u3048\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u69cb\u7bc9\u3068\u8a00\u8a9e\u7684\u500b\u4eba\u6027\u5909\u63db\u3078\u306e\u5fdc\u7528\u300d\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a \u7b2c20\u56de\u5e74\u6b21\u5927\u4f1a \u767a\u8868\u8ad6\u6587\u96c6 (2014\u5e743\u6708)](http://www.anlp.jp/proceedings/annual_meeting/2014/pdf_dir/D5-2.pdf)\n\n___\n\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/ppdb-xl.txt \n  102902 retrofitting/lexicons/ppdb-xl.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$head retrofitting/lexicons/ppdb-xl.txt \nfawn crawl\nreallocations re-allocations reassignments redeployments diversions overrides redistributions\nheavily-fortified strongly-fortified\n360.00 360\n1,800 1800 1.8 1.800 1-800\n1,802 1802\nsonja sonia\ngag muzzle joke reflex\nwoods bretton wood timber forest bois drink lumber forests antlers\nclotted coagulated\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head -1 retrofitting/lexicons/ppdb-xl.txt \nfawn crawl\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -1 retrofitting/lexicons/ppdb-xl.txt \n'arch\u00e9ologie arch\u00e9ologie\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail -3 retrofitting/lexicons/ppdb-xl.txt \nexpands extends broadens widens enlarges stretches expansion increases expand\n38kb 41kb\n'arch\u00e9ologie arch\u00e9ologie\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/ppdb-xl.txt \n  102902 retrofitting/lexicons/ppdb-xl.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ \n```\n\n###__\u8a9e\u5f59\u9593\u306e\u610f\u5473\u95a2\u4fc2 \u77e5\u8b58DB\u30d5\u30a1\u30a4\u30eb\uff08WordNet\u4e2d\u306e\u985e\u4f3c\u8a9e\uff09\u306e \u30c7\u30fc\u30bf\u5f62\u5f0f \u3092 \u78ba\u8a8d__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ wc -l retrofitting/lexicons/wordnet-synonyms.txt\n  148730 retrofitting/lexicons/wordnet-synonyms.txt\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ head retrofitting/lexicons/wordnet-synonyms.txt\nfawn toady truckle bootlick kowtow kotow suck_up crawl creep cringe cower grovel dun greyish_brown grayish_brown\nunsupportable \nnunnery \nperoxide_blonde peroxide_blond\nThoreauvian \nlong_wave \nPlasticine \ncircuitry \nclotted clogged\nxerophthalmus xerophthalmia xeroma conjunctivitis_arida\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ tail retrofitting/lexicons/wordnet-synonyms.txt\ndry_unit dry_measure\nspendable expendable\napolemia \nfighting_chair \ndegustation tasting savoring savouring relishing\nCosta_Rica Republic_of_Costa_Rica\njawbone shmooze shmoose schmooze schmoose lower_jaw mandible mandibula mandibular_bone submaxilla lower_jawbone jowl\nfamily_Iridaceae Iridaceae iris_family\nclaim_agent adjuster adjustor claims_adjuster claims_adjustor\nformularize formularise\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ ls\nretrofitting\nHirofumiYashima-no-MacBook:retrofit_word2vec hirofumiyashima$ cd retrofitting/\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls\nLICENSE\t\tREADME.md\tlexicons\tretrofit.py\tsample_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ tree\n.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 lexicons\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 framenet.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ppdb-xl.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wordnet-synonyms+.txt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wordnet-synonyms.txt\n\u251c\u2500\u2500 retrofit.py\n\u2514\u2500\u2500 sample_vec.txt\n\n1 directory, 8 files\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n___\n\n###__retrofit.py \u5b9f\u884c__\n\n####__word2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb sample_vec.txt\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09\u306e retrofitting \u306b\u3088\u308b ppdb-xl\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u77e5\u8b58DB\u53c2\u7167\u3057\u3066\u306e\u30d9\u30af\u30c8\u30eb\u88dc\u6b63\u5b9f\u884c__\n \n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ time python retrofit.py -i sample_vec.txt -l lexicons/ppdb-xl.txt -n 10 -o out_vec.txt\nVectors read from: sample_vec.txt \n\nWriting down the vectors in out_vec.txt\n\nreal\t0m3.728s\nuser\t0m2.809s\nsys\t0m0.283s\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n__\u88dc\u6b63\u5f8c\u306e\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09out_vec.txt\u3000\u304c \u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3002__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls\nLICENSE\t\tREADME.md\tlexicons\tout_vec.txt\tretrofit.py\tsample_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ ls | grep out\nout_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n* \u884c\u6570\u306f\u88dc\u6b63\u524d\u3068\u540c\u30581,000\u884c\uff08\u30c8\u30fc\u30af\u30f3\u6570\uff1a1,000\u4ef6\uff09\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ wc -l out_vec.txt \n    1000 out_vec.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head out_vec.txt \nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \nverplank 0.0620 0.0139 0.0979 0.0693 0.0251 -0.0295 -0.0069 -0.2186 -0.0256 -0.0475 -0.0918 0.0811 0.1171 0.0256 -0.0514 -0.0744 0.0126 -0.0269 0.1910 -0.1576 0.0514 0.1346 -0.0400 0.2077 0.0172 0.2257 0.0832 -0.0080 -0.1154 0.1731 0.0145 -0.1940 -0.1076 -0.1638 0.2535 -0.1800 -0.0443 -0.1635 -0.0920 -0.3689 -0.0132 0.0078 -0.0068 -0.0212 -0.1164 0.2398 -0.2343 0.4356 \nclottes -0.0089 -0.0616 0.1221 0.0353 -0.0613 -0.0468 0.1197 0.0842 0.0576 0.1621 0.1028 -0.2264 -0.0152 0.0506 -0.0421 -0.0261 0.0112 0.2320 0.0762 0.0697 -0.1173 0.0509 -0.0880 0.1521 0.0829 -0.0553 -0.1066 -0.0842 -0.0593 0.3741 0.2553 -0.1226 -0.2272 -0.0290 0.0215 0.0732 -0.3093 0.2945 0.0668 -0.0922 -0.1416 0.1159 -0.1101 0.0409 -0.3371 -0.0306 0.2253 0.1605 \nchaim 0.1786 -0.1085 0.1778 0.1119 0.0524 -0.0223 -0.2343 0.1261 0.0106 0.1048 -0.0081 -0.1374 0.0759 0.0962 0.1196 -0.0828 -0.2241 0.2000 0.1038 -0.0327 -0.0238 0.1589 0.0838 0.0316 0.0914 -0.3076 0.1447 -0.2133 -0.0091 0.1464 0.0682 0.2652 -0.2158 0.2633 -0.1456 -0.1633 -0.0712 0.2052 0.1588 0.0926 -0.0655 0.2765 -0.0931 0.0765 0.0526 0.1291 -0.1030 -0.0121 \nchain -0.0781 0.1797 -0.0714 -0.1388 -0.0100 0.1631 -0.0590 -0.0122 0.0131 0.0095 -0.2766 0.1228 -0.0379 -0.0177 0.0000 -0.1840 -0.0271 0.1516 0.1337 0.3704 -0.0302 0.0059 -0.3412 -0.0947 -0.0698 -0.0115 0.0201 -0.0483 -0.1166 0.0771 0.0418 -0.0391 -0.0921 -0.0928 -0.2170 -0.0096 -0.0162 0.0405 -0.1827 -0.0036 -0.0354 -0.1323 -0.3062 0.0959 -0.1465 -0.2642 -0.3611 -0.0275 \nwhoever 0.0086 -0.2238 0.0260 0.0636 -0.0396 0.1064 -0.0173 0.0194 0.0407 -0.1259 -0.0866 0.0319 0.0718 0.0426 -0.1504 -0.1081 0.2821 -0.0492 -0.0694 -0.1264 0.0719 0.0192 -0.2853 0.1130 -0.3556 0.2950 -0.0640 -0.0270 0.1557 0.1020 -0.2832 0.0109 -0.0710 0.1190 0.0856 -0.1717 0.0285 0.1827 -0.1751 -0.1333 0.0364 0.1084 0.0708 0.0303 0.1840 -0.0210 -0.3455 0.1309 \nnunnery 0.2770 0.0195 -0.2382 -0.1007 -0.1754 -0.2148 -0.1853 -0.0102 -0.0197 -0.0350 -0.0099 -0.0994 -0.1139 0.0908 0.2135 0.1956 -0.0337 0.1703 -0.2152 -0.0443 0.0148 -0.2110 0.0367 0.1237 -0.3019 0.2300 -0.2326 0.0052 0.0300 0.0798 -0.0274 -0.0765 0.0540 -0.1083 -0.0676 0.0263 -0.1388 0.0278 -0.0267 0.2951 0.0183 0.1441 -0.2682 -0.1303 -0.0157 0.1850 -0.0451 -0.0458 \nclottey 0.1301 -0.0914 0.0037 0.0145 -0.0677 -0.0325 0.3106 0.0808 0.0313 0.1399 -0.0995 -0.0722 0.1535 -0.1060 -0.1111 0.0150 0.0068 -0.0964 0.0208 -0.1278 -0.0416 0.2823 0.0412 0.0756 0.0419 0.2871 0.0241 0.3451 -0.0711 0.2170 0.0211 -0.0294 -0.0542 0.0135 0.1792 -0.0921 -0.0991 -0.0452 -0.1669 -0.4024 -0.2763 0.0284 0.0507 -0.0216 -0.1243 -0.1816 0.0587 0.1824 \nchaib 0.1786 -0.0779 -0.0624 -0.0319 0.3098 -0.0052 0.1528 -0.0334 0.0106 0.1000 0.2047 0.0676 0.0295 -0.1867 -0.1391 0.0521 0.0349 0.0707 -0.1828 0.1060 -0.0937 -0.1508 -0.0852 0.0648 0.0471 -0.0340 -0.0727 0.1469 0.0657 0.3754 -0.1520 -0.1103 0.0072 -0.1809 0.0472 0.1635 -0.1598 -0.3670 -0.0037 -0.1924 0.3299 -0.0481 -0.0669 -0.0296 -0.0810 -0.0157 0.1708 0.0786 \nsowell 0.0308 -0.0918 0.0211 0.1284 0.0179 -0.0779 0.0265 -0.0629 0.0637 -0.0210 -0.0142 0.0148 0.1363 0.1027 -0.0471 0.1066 -0.0390 -0.0483 -0.1622 0.0103 -0.0346 -0.0423 -0.1510 -0.1698 0.0949 0.2179 0.0193 0.0842 0.1276 -0.1115 -0.0696 0.0200 -0.1234 -0.2405 -0.2631 -0.2407 0.2315 0.2028 -0.0108 0.2138 -0.2969 -0.2546 0.2388 -0.1592 -0.1303 0.2801 0.2393 0.0592 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head -1 out_vec.txt \nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n```{bash:Terminal}\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ tail -5 out_vec.txt \npovey 0.1855 -0.0944 0.0717 0.0137 0.3037 0.1172 0.1695 -0.2207 -0.2558 -0.2055 0.2382 0.0017 -0.1664 0.0394 -0.1281 0.3221 -0.0165 0.1347 0.0769 0.0703 -0.0993 -0.2320 -0.1133 -0.0598 -0.0214 0.1051 0.1176 -0.1253 -0.0214 0.1301 -0.1389 -0.2233 -0.1817 0.0542 -0.1167 0.0473 0.0593 -0.1360 0.1194 0.0694 0.1798 -0.1673 -0.1586 -0.1303 0.0200 0.0181 0.0473 -0.1375 \nyoyogi 0.0426 0.0003 -0.1796 -0.2146 -0.0519 -0.0270 0.0178 0.0793 0.0096 0.0285 0.2679 0.0690 -0.2021 -0.0247 0.3348 -0.0425 0.2201 -0.0012 -0.0638 -0.0919 -0.0230 -0.0751 -0.1723 0.0395 -0.0372 0.1007 0.2209 -0.0106 0.0665 -0.0620 -0.1566 -0.1946 0.0124 -0.3048 0.1834 0.0431 -0.2530 0.0037 -0.1254 0.0387 -0.0598 0.2520 -0.0033 -0.1544 -0.0200 0.3011 0.1653 -0.1687 \npover 0.0101 0.0160 -0.0599 -0.0762 -0.0088 -0.0031 -0.0703 -0.0178 -0.0647 0.0384 0.0263 0.1612 -0.2271 0.2278 0.0696 0.0904 -0.0471 0.1998 -0.1184 -0.0872 -0.2935 0.0010 0.0704 -0.1177 0.1419 0.1858 -0.2007 0.1042 -0.0548 -0.0882 0.0791 0.0569 0.1203 0.0790 0.1630 0.0540 -0.0414 -0.0824 -0.0857 0.3505 -0.1964 -0.1329 0.0667 -0.1782 -0.1268 0.4447 -0.1017 0.1892 \nmattek-sands 0.0976 0.0061 0.0038 -0.0612 -0.0434 -0.0615 0.1116 0.0557 0.0742 -0.0170 -0.1538 0.0007 -0.0159 -0.0057 0.1259 -0.0917 0.1878 -0.0632 -0.1555 0.0847 -0.0801 0.1519 -0.0296 0.2367 0.3086 0.2105 -0.1150 0.1443 0.2199 0.0822 -0.1501 0.0707 0.0278 -0.2357 0.3132 -0.2210 0.0888 -0.0694 -0.0745 -0.1735 -0.2447 0.0386 0.0663 -0.1861 0.1029 0.0464 -0.3628 0.0425 \nsteelhead -0.0988 0.2772 -0.2653 0.1573 -0.0628 -0.0763 0.0049 0.1066 0.0501 -0.3517 0.1783 0.0748 0.3490 0.1317 -0.2024 0.0931 0.0975 -0.0248 0.1177 -0.1384 0.0562 -0.0417 0.1516 -0.2274 -0.1648 -0.0852 0.0422 0.2284 -0.1228 0.1553 0.0434 -0.0383 0.0109 0.1200 -0.0206 0.0638 -0.1681 0.2414 0.0267 0.0465 0.0655 0.1238 0.2046 0.0211 0.0609 -0.1298 0.0099 0.0043 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n##__retrofitting\u88dc\u6b63\u5f8c\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u3092 \u4f7f\u3063\u3066\u3001word_similarity, word_analogy\u30bf\u30b9\u30af \u3092 gensim \u3067 \u6f14\u7b97\u3057\u3066\u307f\u308b__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 2.7.11 |Anaconda 4.0.0 (x86_64)| (default, Dec  6 2015, 18:57:58) \n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n>>>\n```\n\n####__\u5b9f\u884c\u30a8\u30e9\u30fc__\n\n* gensim.models.Word2Vec.load_word2vec_format \u30e1\u30bd\u30c3\u30c9 \u3067 \u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb \u3092 \u751f\u6210\u3067\u304d\u306a\u3044\n\n```{Python:Python 2.7}\n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"out_vec.txt\", binary=False)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/hirofumiyashima/.pyenv/versions/anaconda-4.0.0/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 1086, in load_word2vec_format\n    vocab_size, vector_size = map(int, header.split())  # throws for invalid file format\nValueError: invalid literal for int() with base 10: 'biennials'\n>>> \n```\n\n###__\u5b9f\u884c\u30a8\u30e9\u30fc__\n\n* gensim.models.Word2Vec.load_word2vec_format \u30e1\u30bd\u30c3\u30c9 \u3067 \u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb \u3092 \u751f\u6210\u3067\u304d\u306a\u3044\n\n\n```{Python:Python 2.7}\n>>> model = gensim.models.Word2Vec.load_word2vec_format('out_vec.txt', binary=False)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/hirofumiyashima/.pyenv/versions/anaconda-4.0.0/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 1086, in load_word2vec_format\n    vocab_size, vector_size = map(int, header.split())  # throws for invalid file format\nValueError: invalid literal for int() with base 10: 'biennials'\n>>> \n```\n\n####__gensim Word2Vec\u30e2\u30b8\u30e5\u30fc\u30eb \u304c\u8aad\u307f\u8fbc\u307f\u53ef\u80fd\u306a \u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u306f\u3001\uff11\u884c\u76ee\u306b\u30b3\u30fc\u30d1\u30b9\u4ef6\u6570\uff08\u30d5\u30a1\u30a4\u30eb\u884c\u6570\uff09\u3068\u7279\u5fb4\u7a7a\u9593\u306e\u6b21\u5143\u306e\u6570\uff08\u30d5\u30a1\u30a4\u30eb\u5217\u6570\uff09\u3092\u8a18\u8ff0\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u306a\u69d8\u3067\u3059\u3002__\n####__\u4ee5\u4e0b\u306e\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u306b\u306a\u3089\u3063\u3066\u3001\uff11\u884c\u76ee \u3092 \u8ffd\u52a0\u3057\u307e\u3059\u3002__\n\n*  [kanjirz50\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/23\uff09\u300cGloVe\u304c\u51fa\u529b\u3057\u305f\u30e2\u30c7\u30eb(txt)\u3092gensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u6574\u5f62\u3059\u308b\u3002\u300d](http://qiita.com/kanjirz50/items/9d1c79d64dde46604395)\n\n>GloVe\u3067\u5b66\u7fd2\u3059\u308b\u3068\u3001\u6a19\u6e96\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u306fvector.bin\u3068vector.txt\u304c\u51fa\u529b\u3055\u308c\u308b\u3002\n>\n>\u3053\u308c\u3089\u306fgensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u306a\u3044\u306e\u3067\u3001\u8aad\u307f\u8fbc\u3081\u308b\u5f62\u5f0f\u306b\u6574\u5f62\u3059\u308b\u3002\n>\n>__word2vec\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8__\n>\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n>1\u884c\u76ee\uff1a\u5358\u8a9e\u7a2e\u985e\u6570 \u6b21\u5143\u6570\n>2\u884c\u76ee\u4ee5\u964d\uff1a\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 \u2026\n>\u4e0a\u8a18\u306e\u3088\u3046\u306a\u30b7\u30f3\u30d7\u30eb\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u306a\u3063\u3066\u3044\u308b\u3002\n>\n>__GloVe\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8__\n>\n>\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n>\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 \u2026\n>\n>__\u4e0a\u8a18\u306e\u3088\u3046\u306b\u3001word2vec\u306e1\u884c\u76ee\u304c\u6b20\u3051\u3066\u3044\u308b\u3002__\n>__\u3064\u307e\u308a\u5358\u8a9e\u7a2e\u985e\u6570\u3068\u6b21\u5143\u6570\u30921\u884c\u76ee\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u5f62\u5f0f\u304c\u5909\u63db\u3067\u304d\u308b\u3002__\n\n\n__\u4eca\u56de\u4f7f\u3046\u306e\u306f\u3001(word2vec\u3088\u308a\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u826f\u3044\u3068\u3055\u308c\u308b\uff09GloVe\u304b\u3089\u51fa\u529b\u3057\u305f\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u30d5\u30a1\u30a4\u30eb\uff08\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\uff09\u3067\u306f\u306a\u304c\u3001\u6700\u5f8c\u306e\uff12\u884c\u306e\u90fd\u5408\u306f\u540c\u3058\u3002__\n\n__\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u501f\u7528\u3057\u3066\u3001retrofirt.py \u304b\u3089\u51fa\u529b\u3055\u308c\u305f \u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3000\u30d5\u30a1\u30a4\u30eb\u306b\uff11\u884c\u76ee\u3092\u8ffd\u52a0\u3059\u308b\u3002__\n\n\n```{Python3:glove2word2vec_format.py\uff08Python3\u74b0\u5883\uff09}\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n* print\u30e1\u30bd\u30c3\u30c9\u306eend\u30aa\u30d7\u30b7\u30e7\u30f3\uff08\u5404\u884c\u306e\u6b21\u306b\u6539\u884c\u633f\u5165\u3057\u306a\u3044\uff09\u306fPython3\u304b\u3089\u4f7f\u7528\u3067\u304d\u308b\u306e\u3067\u3001glove2word2vec_format.py\u3000\u306f\u3001Python 3\u74b0\u5883\u3067\u4f7f\u7528\u3059\u308b\u3002\n\n```{bash:Termina}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ cat glove2word2vec_format.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n```\n\n####__Python3\u74b0\u5883 \u3067  glove2word2vec_format.py \u3092 \u5b9f\u884c__\n\n\n```\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python glove2word2vec_format.py out_vec.txt > gensim_vector.txt\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$\n```\n\n\uff11\u884c\u76ee\u306b\u3001\u4ee5\u4e0b\u304c\u633f\u5165\u3055\u308c\u3066\u3044\u308b\u3002\n\n> 1000 48\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$head gensim_vector.txt \n1000 48\nbiennials -0.1431 0.1085 -0.0324 0.0916 -0.0297 0.1170 -0.1558 0.2403 -0.0858 -0.0393 0.0259 -0.0140 0.0168 0.0091 -0.2026 -0.0319 -0.1212 0.0816 0.0081 0.1562 -0.3269 0.1711 -0.1709 -0.1913 0.1786 -0.2245 0.2271 -0.1087 0.0259 0.1972 0.2117 -0.2224 0.0563 -0.0978 0.1419 0.0855 -0.0385 0.2349 0.1352 -0.1268 -0.1391 0.2790 0.0026 -0.1749 -0.0635 0.0276 0.1356 -0.0094 \n\n\uff08 \u4e2d\u7565 \uff09\n\nchaib 0.1786 -0.0779 -0.0624 -0.0319 0.3098 -0.0052 0.1528 -0.0334 0.0106 0.1000 0.2047 0.0676 0.0295 -0.1867 -0.1391 0.0521 0.0349 0.0707 -0.1828 0.1060 -0.0937 -0.1508 -0.0852 0.0648 0.0471 -0.0340 -0.0727 0.1469 0.0657 0.3754 -0.1520 -0.1103 0.0072 -0.1809 0.0472 0.1635 -0.1598 -0.3670 -0.0037 -0.1924 0.3299 -0.0481 -0.0669 -0.0296 -0.0810 -0.0157 0.1708 0.0786 \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n```\n\n####__\u975e\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u30d5\u30a1\u30a4\u30eb \u3092 \u8aad\u307f\u8fbc\u3093\u3067\u3001word2vec\u30e2\u30c7\u30eb\u30fb\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u751f\u6210 \u306b \u6210\u529f__\n\n```{Python:Python 3.5.2}\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"gensim_vector.txt\", binary=False)\n>>> print(model)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>>\n```\n\n####__most_similar\u30e1\u30bd\u30c3\u30c9__\n\n```\n>>> model.most_similar(\"wrong\")\n[('trivial', 0.5341627597808838), ('extent', 0.48358750343322754), ('whoever', 0.4811513125896454), ('optimist', 0.46935686469078064), ('tempted', 0.4658152759075165), ('quibble', 0.46456658840179443), ('unequivocally', 0.44248801469802856), ('fix', 0.43918511271476746), ('jerk', 0.42578360438346863), ('fit', 0.41329652070999146)]\n>>> \n>>> model.most_similar(\"welcomed\")\n[('welcomes', 0.9832462072372437), ('spoken', 0.6331268548965454), ('secretary-general', 0.6301525831222534), ('congratulations', 0.542494535446167), ('thanking', 0.5062479972839355), ('doubts', 0.5043333768844604), ('exclaimed', 0.47787442803382874), ('chair', 0.41937389969825745), ('drafted', 0.4132911264896393), ('non-militarized', 0.4084121584892273)]\n>>> \n>>> model.most_similar(\"yahoo\")\n[('dish.o', 0.7728222012519836), ('vonage', 0.7293688654899597), ('telia', 0.639286458492279), ('co-marketing', 0.5994674563407898), ('olympus', 0.5876779556274414), ('regus', 0.582847535610199), ('nasdaq-listed', 0.5645484328269958), ('finnish-german', 0.5628683567047119), ('lossmaking', 0.5567153692245483), ('publicly-traded', 0.5565172433853149)]\n>>> \n>>> model.most_similar(\"military-based\")\n[('president-for-life', 0.5531608462333679), ('abacha', 0.5091353058815002), ('despised', 0.5032479166984558), ('kremlin', 0.4599403142929077), ('month-long', 0.43890973925590515), ('shinawatra', 0.4219447374343872), ('regularize', 0.42181631922721863), ('old-guard', 0.4092077612876892), ('vice-like', 0.38102418184280396), ('lifeline', 0.38001883029937744)]\n>>> \n>>> model.most_similar(\"kids\")\n[('obese', 0.4557153284549713), ('spaying', 0.4185415208339691), ('olds', 0.4133707880973816), ('wrong', 0.37881478667259216), ('needed', 0.3663429021835327), ('fit', 0.360312283039093), ('mentors', 0.3448786735534668), ('potty-training', 0.34364503622055054), ('fix', 0.3323163688182831), ('screaming', 0.3309297561645508)]\n>>> \n>>> model.most_similar(\"olympus\")\n[('tookey', 0.6202044486999512), ('yahoo', 0.5876779556274414), ('pggm', 0.580516517162323), ('irg', 0.5800860524177551), ('sacyr', 0.5752755403518677), ('jih', 0.5725178122520447), ('fiduciary', 0.5634198188781738), ('corporate', 0.5586938858032227), ('publicly-traded', 0.5569629073143005), ('nasdaq-listed', 0.5287851095199585)]\n>>>\n```\n\n####__similarity\u30e1\u30bd\u30c3\u30c9__\n\n```\n>>> model.similarity(\"olympus\", \"yahoo\")\n0.58767794860053679\n>>>\n```\n\n####__most_similar\u30e1\u30bd\u30c3\u30c9\uff08posiutive\u5358\u8a9e \u3068  nefgative\u5358\u8a9e \u53cc\u65b9 \u3092 \u6307\u5b9a\uff09__\n\n```\n>>> model.most_similar(positive=[\"olympus\"], negative=[\"yahoo\"])\n[('ofsted', 0.44535747170448303), ('fiduciary', 0.426706999540329), ('post-accident', 0.4187975525856018), ('preuss', 0.3981691002845764), ('ex-fbi', 0.38794970512390137), ('super-safe', 0.3831671476364136), ('comically', 0.374136358499527), ('riskiness', 0.36748120188713074), ('shirked', 0.3594794273376465), ('stapel', 0.35505157709121704)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"olympus\"])\n[('techland', 0.558988094329834), ('ivillage.com', 0.5493491291999817), ('whosay', 0.5184122920036316), ('net-a-porter.com', 0.49817216396331787), ('icloud', 0.49218812584877014), ('thalia', 0.488607794046402), ('usenet', 0.48386359214782715), ('stahler', 0.42786073684692383), ('vonage', 0.421731173992157), ('webpage', 0.41963058710098267)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"kids\"], negative=[\"olympus\"])\n[('ivillage.com', 0.42363297939300537), ('olds', 0.4227023124694824), ('techland', 0.39514291286468506), ('surfing', 0.39229321479797363), ('thalia', 0.39018523693084717), ('junhasavasdikul', 0.3871711492538452), ('usenet', 0.36667299270629883), ('obese', 0.3630061745643616), ('perchance', 0.3595729470252991), ('luba', 0.35373014211654663)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"olympus\", \"kids\"])\n[('jarrin', 0.4498675465583801), ('net-a-porter.com', 0.42687320709228516), ('counter-sued', 0.42193296551704407), ('first-rounder', 0.3740149736404419), ('techland', 0.35563167929649353), ('dnp', 0.3551561236381531), ('whosay', 0.3550400137901306), ('clottes', 0.3525778651237488), ('wintonlos', 0.34782421588897705), ('stahler', 0.3322279453277588)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\"], negative=[\"kids\"])\n[('dish.o', 0.5999091863632202), ('vonage', 0.5247481465339661), ('telia', 0.5069398283958435), ('co-marketing', 0.5059320330619812), ('nasdaq-listed', 0.49054503440856934), ('publicly-traded', 0.4846886098384857), ('regus', 0.4727519154548645), ('lossmaking', 0.4521976709365845), ('counter-sued', 0.45045703649520874), ('olympus', 0.43984055519104004)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"kids\"], negative=[])\n[('tech', 0.5492722392082214), ('vonage', 0.5070191621780396), ('woods', 0.5068948864936829), ('dish.o', 0.49435973167419434), ('telia', 0.39850202202796936), ('techland', 0.39513349533081055), ('irc', 0.3942038416862488), ('olympus', 0.3918909430503845), ('finnish-german', 0.3896961212158203), ('corporate', 0.37088701128959656)]\n>>> \n>>> model.most_similar(positive=[\"yahoo\", \"olympus\"], negative=[])\n[('dish.o', 0.6819705963134766), ('publicly-traded', 0.6248651146888733), ('nasdaq-listed', 0.6135591864585876), ('vonage', 0.6036995053291321), ('lossmaking', 0.601861298084259), ('regus', 0.5796098709106445), ('sacyr', 0.5676641464233398), ('corporate', 0.5568501949310303), ('tookey', 0.5475372076034546), ('co-marketing', 0.5441089272499084)]\n>>> \n>>> model.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"]\n... )\n[('cherbourg', 0.3693275451660156), ('signal-callers', 0.36640459299087524), ('hamel', 0.3584540784358978), ('lucero', 0.3427450954914093), ('quinnell', 0.34220728278160095), ('wintonlos', 0.3380362391471863), ('kavaler', 0.32803821563720703), ('mcdizzy', 0.32609716057777405), ('hamnett', 0.32079148292541504), ('borle', 0.3176853060722351)]\n>>> \n>>> model.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.3693275451660156), ('signal-callers', 0.36640459299087524), ('hamel', 0.3584540784358978), ('lucero', 0.3427450954914093), ('quinnell', 0.34220728278160095), ('wintonlos', 0.3380362391471863), ('kavaler', 0.32803821563720703), ('mcdizzy', 0.32609716057777405), ('hamnett', 0.32079148292541504), ('borle', 0.3176853060722351)]\n>>> \n>>> quit()\n```\n\n\n___\n\n##__\u3010 \u6bd4\u8f03 \u3011retrofit.py \u524d\u5f8c__\n\nretrofit.py \u3067 \u88dc\u6b63\u3059\u308b\u524d\u306e\uff08\u666e\u901a\u306eword2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\uff09\u5358\u8a9e \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u3067 most_similar\u30e1\u30bd\u30c3\u30c9\u7b49\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c__\n\n```\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv versions\n  system\n  2.7\n  3.5.0\n  3.5.1\n  3.5.1/envs/py351tensorflow\n* 3.5.2 (set by /Users/hirofumiyashima/Desktop/retrofit_word2vec/retrofitting/.python-version)\n  anaconda-4.0.0\n  py351tensorflow\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python glove2word2vec_format.py sample_vec.txt > gensim_sample_vector.txt\n```\n\n####__1\u884c\u76ee\u306b\u3001\u4ee5\u4e0b\u304c\u633f\u5165\u3055\u308c\u3066\u3044\u308b__\n\n>```\n> 1000 48\n>```\n\n\n```\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ head gensim_sample_vector.txt \n1000 48\nbiennials -0.14307 0.10847 -0.032377 0.091573 -0.029721 0.11704 -0.15579 0.24033 -0.085831 -0.039297 0.025851 -0.014047 0.016754 0.0091426 -0.20261 -0.031946 -0.12119 0.081614 0.0080966 0.15618 -0.3269 0.17114 -0.17094 -0.19134 0.17856 -0.22448 0.2271 -0.10867 0.025868 0.19715 0.2117 -0.22244 0.056319 -0.097754 0.14187 0.085534 -0.038548 0.23494 0.13518 -0.1268 -0.13909 0.27896 0.0025857 -0.17491 -0.063471 0.027577 0.13556 -0.0094128\nverplank 0.061966 0.013867 0.097894 0.069291 0.025128 -0.029515 -0.0068756 -0.21864 -0.025601 -0.047496 -0.091767 0.081062 0.11705 0.025643 -0.051421 -0.074378 0.012586 -0.026915 0.19103 -0.15763 0.051448 0.13455 -0.040023 0.2077 0.017204 0.22573 0.083228 -0.0079535 -0.11535 0.17306 0.014453 -0.19401 -0.1076 -0.16378 0.25349 -0.18004 -0.044328 -0.16348 -0.091957 -0.36885 -0.01324 0.0078103 -0.00681 -0.021193 -0.11642 0.23982 -0.23426 0.43564\nclottes -0.0089251 -0.061586 0.1221 0.035306 -0.061313 -0.04685 0.11966 0.084184 0.057614 0.16206 0.10281 -0.22645 -0.015185 0.050584 -0.04209 -0.026132 0.011169 0.23204 0.076184 0.069735 -0.11726 0.050913 -0.087998 0.15215 0.082915 -0.055324 -0.10663 -0.084158 -0.059272 0.37413 0.25528 -0.12261 -0.2272 -0.028977 0.021512 0.073214 -0.30931 0.29448 0.066778 -0.092153 -0.14165 0.11589 -0.11014 0.040938 -0.33708 -0.030592 0.22528 0.16051\nnunnery 0.277 0.019486 -0.23815 -0.10068 -0.17544 -0.2148 -0.1853 -0.010177 -0.019739 -0.03499 -0.0099115 -0.099382 -0.11386 0.090788 0.21348 0.19557 -0.03373 0.17034 -0.21515 -0.044315 0.014785 -0.211 0.036734 0.12371 -0.30186 0.22997 -0.23255 0.0052285 0.029958 0.079806 -0.02739 -0.076451 0.053968 -0.1083 -0.067588 0.026335 -0.13879 0.027829 -0.026704 0.29506 0.018326 0.14407 -0.26815 -0.13034 -0.015692 0.18495 -0.045091 -0.04577\n```\n\n\uff08 \u4e2d\u7565 \uff09\n\n```\npiders -0.098084 0.17303 -0.1513 0.23683 0.046248 -0.090944 -0.068375 0.074935 -0.026971 -0.2084 0.052636 -0.027759 0.23191 0.15999 -0.38536 0.05138 0.32286 -0.024479 0.082912 -0.2383 0.14478 0.070837 -0.16746 -0.13519 -0.078819 0.043042 0.11614 0.052073 0.11514 -0.023325 0.25228 0.015687 0.054137 -0.1127 -0.18158 0.01556 -0.020509 -0.2665 0.18425 0.03688 0.040863 0.10596 0.011119 0.012524 0.20098 -0.093732 -0.067517 0.13145\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n```\n\n\n```\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model_sample_vec = gensim.models.Word2Vec.load_word2vec_format(\"gensim_sample_vector.txt\", binary=False)\n>>> print(model_sample_vec)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> model_sample_vec.most_similar(\"wrong\")\n[('trivial', 0.5341147780418396), ('nature', 0.5161055326461792), ('extent', 0.4835756719112396), ('whoever', 0.4812297821044922), ('optimist', 0.46935874223709106), ('tempted', 0.46578240394592285), ('quibble', 0.4645503759384155), ('unequivocally', 0.44251543283462524), ('fix', 0.43913888931274414), ('jerk', 0.4257252812385559)]\n>>> \n>>> model_sample_vec.most_similar(\"welcomed\")\n[('welcomes', 0.8587126731872559), ('spoken', 0.6633444428443909), ('secretary-general', 0.6413360238075256), ('doubts', 0.5253970623016357), ('ire', 0.517067015171051), ('thanking', 0.4925107955932617), ('congratulations', 0.4785553514957428), ('exclaimed', 0.46664339303970337), ('wednesday', 0.44500139355659485), ('chair', 0.43357059359550476)]\n>>> \n>>> model_sample_vec.most_similar(\"yahoo\")\n[('dish.o', 0.7727800607681274), ('vonage', 0.7294098734855652), ('telia', 0.6392509341239929), ('co-marketing', 0.5994648337364197), ('olympus', 0.5876917243003845), ('regus', 0.5828214883804321), ('nasdaq-listed', 0.5644622445106506), ('finnish-german', 0.5628864765167236), ('lossmaking', 0.5566997528076172), ('publicly-traded', 0.5564723014831543)]\n>>> \n>>> model_sample_vec.most_similar(\"military-based\")\n[('president-for-life', 0.5531935691833496), ('abacha', 0.5091578364372253), ('despised', 0.5032562017440796), ('kremlin', 0.45989224314689636), ('month-long', 0.4389973282814026), ('shinawatra', 0.4218860864639282), ('regularize', 0.4218169152736664), ('old-guard', 0.40915483236312866), ('vice-like', 0.38100743293762207), ('lifeline', 0.38002198934555054)]\n>>> \n>>> model_sample_vec.most_similar(\"kids\")\n[('obese', 0.4557613432407379), ('spaying', 0.41861096024513245), ('olds', 0.41338664293289185), ('wrong', 0.37878769636154175), ('needed', 0.3663170635700226), ('fit', 0.3603125214576721), ('mentors', 0.3448629379272461), ('potty-training', 0.3436267077922821), ('fix', 0.3324095904827118), ('screaming', 0.3309009373188019)]\n>>> \n>>> model_sample_vec.most_similar(\"olympus\")\n[('tookey', 0.6201847791671753), ('yahoo', 0.5876917839050293), ('pggm', 0.580501139163971), ('irg', 0.5801130533218384), ('sacyr', 0.5752725601196289), ('jih', 0.5724684000015259), ('fiduciary', 0.56339430809021), ('corporate', 0.5586907863616943), ('publicly-traded', 0.5569700598716736), ('nasdaq-listed', 0.5287832021713257)]\n>>> \n>>> model_sample_vec.similarity(\"olympus\", \"yahoo\")\n0.58769171764588612\n>>> \n>>> model_sample_vec.most_similar(positive=[\"olympus\"], negative=[\"yahoo\"])\n[('ofsted', 0.4454064667224884), ('fiduciary', 0.4266740083694458), ('post-accident', 0.41873520612716675), ('preuss', 0.39813536405563354), ('ex-fbi', 0.3879525065422058), ('super-safe', 0.3832082152366638), ('comically', 0.3741567134857178), ('riskiness', 0.36750614643096924), ('shirked', 0.35946837067604065), ('stapel', 0.35496634244918823)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"olympus\"])\n[('techland', 0.5590645670890808), ('ivillage.com', 0.5493117570877075), ('whosay', 0.5184593796730042), ('net-a-porter.com', 0.4981782138347626), ('icloud', 0.4922236204147339), ('thalia', 0.48856472969055176), ('usenet', 0.48386064171791077), ('stahler', 0.4279034733772278), ('vonage', 0.42177340388298035), ('webpage', 0.4197327792644501)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"kids\"], negative=[\"olympus\"])\n[('ivillage.com', 0.4235672354698181), ('olds', 0.42271482944488525), ('techland', 0.3952169418334961), ('surfing', 0.39227139949798584), ('thalia', 0.3901672065258026), ('junhasavasdikul', 0.38726335763931274), ('usenet', 0.36668533086776733), ('obese', 0.36304527521133423), ('perchance', 0.35954704880714417), ('luba', 0.35366585850715637)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"olympus\", \"kids\"])\n[('jarrin', 0.4498426914215088), ('net-a-porter.com', 0.42685195803642273), ('counter-sued', 0.4218709170818329), ('first-rounder', 0.37408456206321716), ('techland', 0.35565197467803955), ('dnp', 0.35518622398376465), ('whosay', 0.3550260663032532), ('clottes', 0.35258710384368896), ('wintonlos', 0.3478633761405945), ('stahler', 0.33223098516464233)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\"], negative=[\"kids\"])\n[('dish.o', 0.5998775362968445), ('vonage', 0.5247606039047241), ('telia', 0.5068938732147217), ('co-marketing', 0.5059046745300293), ('nasdaq-listed', 0.4904913306236267), ('publicly-traded', 0.48468953371047974), ('regus', 0.4727250933647156), ('lossmaking', 0.45219260454177856), ('counter-sued', 0.45042723417282104), ('olympus', 0.43985074758529663)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"kids\"], negative=[])\n[('tech', 0.5492537617683411), ('vonage', 0.5070642232894897), ('woods', 0.5069363117218018), ('dish.o', 0.4943317770957947), ('telia', 0.39849722385406494), ('techland', 0.3951759338378906), ('irc', 0.3942807614803314), ('olympus', 0.3919001519680023), ('finnish-german', 0.38974136114120483), ('corporate', 0.3707897365093231)]\n>>> \n>>> model_sample_vec.most_similar(positive=[\"yahoo\", \"olympus\"], negative=[])\n[('dish.o', 0.6819364428520203), ('publicly-traded', 0.6248412132263184), ('nasdaq-listed', 0.613507091999054), ('vonage', 0.6037248969078064), ('lossmaking', 0.6018809080123901), ('regus', 0.5795953869819641), ('sacyr', 0.5676511526107788), ('corporate', 0.5568079352378845), ('tookey', 0.5475448369979858), ('co-marketing', 0.5441123843193054)]\n>>> \n>>> model_sample_vec.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.36930254101753235), ('signal-callers', 0.3664279580116272), ('hamel', 0.35849088430404663), ('lucero', 0.34278666973114014), ('quinnell', 0.3422016501426697), ('wintonlos', 0.33804047107696533), ('kavaler', 0.3280620574951172), ('mcdizzy', 0.326118528842926), ('hamnett', 0.32082003355026245), ('borle', 0.3177386522293091)]\n>>> \n>>> model_sample_vec.most_similar(positive=[], negative=[\"yahoo\", \"olympus\"])\n[('cherbourg', 0.36930254101753235), ('signal-callers', 0.3664279580116272), ('hamel', 0.35849088430404663), ('lucero', 0.34278666973114014), ('quinnell', 0.3422016501426697), ('wintonlos', 0.33804047107696533), ('kavaler', 0.3280620574951172), ('mcdizzy', 0.326118528842926), ('hamnett', 0.32082003355026245), ('borle', 0.3177386522293091)]\n>>>\n>>> quit()\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n```\n\n\n###__Python 2.7\u3067\u5b9f\u884c\u3057\u305f\u5834\u5408__\n\n\n```\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ pyenv local 2.7\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ python\nPython 2.7 (r27:82500, Jul 23 2016, 15:46:16) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import gensim\n>>> \n>>> model = gensim.models.Word2Vec.load_word2vec_format(\"gensim_sample_vector.txt\", binary=False)\n>>> print(model)\nWord2Vec(vocab=1000, size=48, alpha=0.025)\n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>> \n>>> model.most_similar(\"wrong\")\n[(u'trivial', 0.5341147780418396), (u'nature', 0.5161055326461792), (u'extent', 0.4835756719112396), (u'whoever', 0.4812297821044922), (u'optimist', 0.46935874223709106), (u'tempted', 0.46578240394592285), (u'quibble', 0.4645503759384155), (u'unequivocally', 0.44251543283462524), (u'fix', 0.43913888931274414), (u'jerk', 0.4257252812385559)]\n>>> \n>>> model.most_similar(\"welcomed\")\n[(u'welcomes', 0.8587126731872559), (u'spoken', 0.6633444428443909), (u'secretary-general', 0.6413360238075256), (u'doubts', 0.5253970623016357), (u'ire', 0.517067015171051), (u'thanking', 0.4925107955932617), (u'congratulations', 0.4785553514957428), (u'exclaimed', 0.46664339303970337), (u'wednesday', 0.44500139355659485), (u'chair', 0.43357059359550476)]\n>>> \n>>> quit()\n\nHirofumiYashima-no-MacBook:retrofitting hirofumiyashima$ \n```\n\n___\n\n##__\u3010 \u53c2\u8003 \u3011__\n\n\n\n[LIFE WITH PYTHON \uff082013/12/23\uff09\u300cPython Tips\uff1a\u6539\u884c\u306a\u3057\u3067\u6587\u5b57\u5217\u3092\u51fa\u529b\u3057\u305f\u3044\u300d](http://www.lifewithpython.com/2013/12/python-print-without-.html)\n\n[gensim, _models.word2vec \u2013 Deep learning with word2vec_](https://radimrehurek.com/gensim/models/word2vec.html)\n\n>__Initialize a model with e.g.:__\n>\n>```\n>>>> model = Word2Vec(sentences, size=100, window=5,min_count=5, workers=4)\n>```\n>\n>__Persist a model to disk with:__\n>\n>```\n>>>> model.save(fname)\n>>>> model = Word2Vec.load(fname)  # you can continue training with the loaded model!\n>```\n>\n>__The model can also be instantiated from an existing file on disk in the word2vec C format:__\n>\n>```\n>>>> model = Word2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False)  # C text format\n>>>> model = Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)  # C binary format\n>```\n>\n>__You can perform various syntactic/semantic NLP word tasks with the model. Some of them are already built-in:__\n>\n>```\n>>>> model.most_similar(positive=['woman', 'king'], negative=['man'])\n[('queen', 0.50882536), ...]\n>\n>>>> model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n'cereal'\n>\n>>>> model.similarity('woman', 'man')\n0.73723527\n>\n>>>> model['computer']  # raw numpy vector of a word\narray([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)\n>```\n", "tags": ["word2vec", "DeepLearning", "NLP", "ontology", "lod"]}