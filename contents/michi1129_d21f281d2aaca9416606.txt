{"context": " More than 1 year has passed since last update.\n\n\u3084\u308b\u3053\u3068\nmac\u306bvirtualbox\u3068vagrant\u3092\u5165\u308c\u3066ubuntu\u3092\u52d5\u304b\u3059\u3001\u305d\u306e\u4e0a\u3067hadoop\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\n\u672c\u8aad\u3093\u3060\u308a\u30b5\u30a4\u30c8\u898b\u305f\u308a\u3057\u3066\u3066\u3082\u3001\u66f8\u3044\u3066\u3042\u308b\u3053\u3068\u304c\u30d0\u30e9\u30d0\u30e9\u3067\u3069\u308c\u3092\u5199\u7d4c\u3057\u3066\u3044\u3044\u306e\u304b\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u5099\u5fd8\u9332\u3068\u3057\u3066\u6b8b\u3057\u3066\u304a\u304f\u3002\n\n\u74b0\u5883\nMac OS X 10.11.3\nVagrant 1.8.1\n- ubuntu 14.04\nHadoop 2.7.2\n\n\u59cb\u3081\u308b\u524d\u306b\n\u5f8c\u5148\u8003\u3048\u305a\u306b\u884c\u304f\u3068hadoop\u3063\u3066\u5358\u8a9e\u304c\u5c71\u307b\u3069\u51fa\u3066\u304d\u3066\u3053\u3093\u304c\u3089\u304c\u308b\u306e\u3067\u3001\u5224\u5225\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002\u30e6\u30fc\u30b6\u30fc\u540d\u3068\u304b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u3068\u304b\u3002\n\nhdtest1\ntestdir2\nhduser3\n\n\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u5165\u529b\u3082\u3001\u3069\u306e\u74b0\u5883\u3067\u3001\u8ab0\u3092\u64cd\u4f5c\u3057\u3066\u3044\u308b\u306e\u304b\u3001\u6df7\u540c\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u308b\n\n\u5b9f\u8df5\n\n\u6e96\u5099\nvagrant\u3067ubuntu\u3092\u7528\u610f\u3059\u308b\u3002\u4e8b\u524d\u306bhadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304a\u304f\u3002\nmac$ mkdir hdtest1\nmac$ mv hadoo-2.7.2.tar.gz hdtest1/\nmac$ cd hdtest1\nmac$ vagrant init ubuntu/trusty64\nmac$ vagrant up\nmac$ vagrant ssh\n\nvagrant$ vi .profile\n\n\u3068\u308a\u3042\u3048\u305a\u3084\u3063\u3066\u304a\u304f\n\n.profile\n# \u672c\u984c\u3068\u95a2\u4fc2\u306a\u3044\u3051\u3069\u30e1\u30e2\nexport HISTSIZE=10000\nexport HISTFILESIZE=10000\nexport HISTCONTROL=ignoredups\nexport HISTIGNORE=ls:ll:lla:pwd:history\n\n\n# \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5b9f\u884c\nvagrant$ sudo apt-get autoremove -y\nvagrant$ sudo apt-get update -y\nvagrant$ sudo apt-get upgrade -y\nvagrant$ sudo apt-get install -y default-jdk\n\n\n\u30b9\u30bf\u30f3\u30c9\u30a2\u30ed\u30f3\u30e2\u30fc\u30c9\u3067\u52d5\u304b\u3059\nStandalone Operation\n# mac\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30de\u30a6\u30f3\u30c8\u3067\u304d\u3066\u3044\u308b\u3082\u306e\u3068\u3059\u308b\nvagrant$ cp /vagrant/hadoop-2.7.2.tar.gz .\nvagrant$ tar zxvf hadoop-2.7.2.tar.gz\nvagrant$ cd hadoop-2.7.2\n\n# \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f5c\u3063\u3066\u3001\u30b5\u30f3\u30d7\u30eb\u306e\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u3066\u3044\u308b\u3060\u3051\nvagrant$ mkdir testdir2\nvagrant$ mkdir testdir2/input\nvagrant$ cp etc/hadoop/*.xml testdir2/input\n\n# \u5b9f\u884c\nvagrant$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep testdir2/input testdir2/output 'dfs[a-z.]+'\nError: JAVA_HOME is not set and could not be found.\n\n# JAVA_HOME\u304c\u306a\u3044\u3068\u8a00\u308f\u308c\u305f\n# \u30ea\u30f3\u30af\u3067\u306f\u3046\u307e\u304f\u884c\u304b\u306a\u304b\u3063\u305f\u6c17\u304c\u3059\u308b\u306e\u3067\u672c\u4f53\u3092\u63a2\u3059\nvagrant$ ls /usr/lib/jvm/\ndefault-java  java-1.7.0-openjdk-amd64  java-7-openjdk-amd64\n\n# JAVA_HOME\u3092\u8a2d\u5b9a\u3057\u3066\u5b9f\u884c\nvagrant$ JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep testdir2/input testdir2/output 'dfs[a-z.]+'\n\nvagrant$ ls testdir2/output/\npart-r-00000  _SUCCESS\n\nvagrant$ cat testdir2/output/part-r-00000 \n1   dfsadmin\n\n\u52d5\u3044\u305f\n\n\u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u3067\u52d5\u304b\u3059\nPseudo-Distributed Operation\n\u3044\u308d\u3044\u308d\u6e96\u5099\n# \u5c02\u7528\u306e\u30e6\u30fc\u30b6\u3092\u8ffd\u52a0\u3059\u308b\n# useradd\u3067\u306a\u304f\u3066adduser\u3059\u308b\nvagrant$ adduser hduser3\n\n# hadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u3092\u5909\u3048\u3066\u3001\u5148\u306e\u30e6\u30fc\u30b6\u306e\u6240\u6709\u3068\u3059\u308b\nvagrant$ cd ~\nvagrant$ sudo mv hadoop-2.7.2 /usr/local/\nvagrant$ sudo chown -R hduser3:hduser3 /usr/local/hadoop-2.7.2/\n\n# hduser3\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b\nvagrant$ ssh hduser3@localhost\n\n# \u521d\u56de\u306fknown_hosts\u306b\u8ffd\u52a0\u3059\u308b\u306e\u3092\u805e\u304b\u308c\u308b\n# \u30d1\u30b9\u30ef\u30fc\u30c9\u5165\u529b\u304c\u9762\u5012\u306a\u3089\u9375\u8a8d\u8a3c\u306b\u3059\u308b\u304c\u7701\u7565\n\n# \u5148\u306bvagrant\u30e6\u30fc\u30b6\u30fc\u3067\u3082\u3084\u3063\u305f.profile\u306e\u8ffd\u52a0\u3092\u3057\u305f\u308a\u3057\u305f\n\n# \u81ea\u5206\u7528\u306e\u9375\u8a8d\u8a3c\u3092\u8a2d\u5b9a\u3059\u308b\nhduser3$ ssh-keygen -t rsa -P ''\nhduser3$ cat .ssh/id_rsa.pub >> .ssh/authorized_keys\nhduser3$ chmod 0600 .ssh/authorized_keys\n\nhduser3$ cd /usr/local/hadoop-2.7.2\n\n\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3059\u308b\u3002core-site.xml\u306f\u6ce8\u610f\u3002\u30b5\u30a4\u30c8\u3068\u304b\u672c\u3068\u304b\u3068\u7570\u306a\u308b\u306e\u3067\u516c\u5f0f\u30b5\u30a4\u30c8\u3092\u53c2\u7167\u3002\n\netc/hadoop/core-site.xml\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://localhost:9000</value>\n  </property>\n</configuration>\n\n\n\netc/hadoop/hdfs-site.xml\n<configuration>\n  <property>\n    <name>dfs.replication</name>\n    <value>1</value>\n  </property>\n</configuration>\n\n\nhdfs\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\nhduser3$ bin/hdfs namenode -format\nError: JAVA_HOME is not set and could not be found.\n\n# \u307e\u305fJAVA_HOME\u304c\u306a\u3044\u3068\u8a00\u308f\u308c\u305f\n\n\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3059\u308b\n\netc/hadoop/hadoop-env.sh\n# The java implementation to use.\n#export JAVA_HOME=${JAVA_HOME}\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n\n\n\u518d\u5ea6\u5b9f\u884c\nhduser3$ bin/hdfs namenode -format\nhduser3$ sbin/start-dfs.sh \nhduser3$ jps\n12244 Jps\n11936 DataNode\n12135 SecondaryNameNode\n11771 NameNode\n\nhduser3$ bin/hdfs dfs -mkdir /user\nhduser3$ bin/hdfs dfs -mkdir /user/hduser3\nhduser3$ bin/hdfs dfs -put etc/hadoop input\n\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n...\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000e0ebb000, 104861696, 0) failed; error='Cannot allocate memory' (errno=12)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (malloc) failed to allocate 104861696 bytes for committing reserved memory.\n# An error report file with more information is saved as:\n# /usr/local/hadoop-2.7.2/hs_err_pid12520.log\n\n\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u3044\u3089\u3057\u3044\u306e\u3067\u3001\u30e1\u30e2\u30ea\u3092\u5897\u3084\u3057\u3066\u304f\u308b\nhduser3$ exit\nvagrant$ exit\nmac$ vi Vagrantfile\n\n\nVagrantfile\n  # config.vm.provider \"virtualbox\" do |vb|\n  #   # Display the VirtualBox GUI when booting the machine\n  #   vb.gui = true\n  #\n  #   # Customize the amount of memory on the VM:\n  #   vb.memory = \"1024\"\n  # end\n  config.vm.provider \"virtualbox\" do |vb|\n  #   # Customize the amount of memory on the VM:\n    vb.memory = \"2048\"\n  end\n\n\n\u518d\u5ea6\u5b9f\u884c\nmac$ vagrant ssh\nvagrant$ ssh hduser3@localhost\n\nhduser3$ cd /usr/local/hadoop-2.7.2\nhduser3$ sbin/start-dfs.sh\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n16/02/16 15:01:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n16/02/16 15:01:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\njava.net.ConnectException: Call From vagrant-ubuntu-trusty-64/10.0.2.15 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n...\n\nhduser3$ jps\n1835 DataNode\n2029 SecondaryNameNode\n2168 Jps\n\n\u3053\u3051\u305f\u3002\u4f55\u304b\u8db3\u308a\u306a\u3044\u3002\n\u4e00\u65e6hdfs\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664\u3057\u3066\u3084\u308a\u76f4\u3059\u3002\nhduser3$ sbin/stop-dfs.sh\nhduser3$ rm -rf /tmp/hadoop-hduser3/\nhduser3$ bin/hdfs namenode -format\nhduser3$ sbin/start-dfs.sh\nhduser3$ jps\n3080 DataNode\n3285 SecondaryNameNode\n3394 Jps\n2909 NameNode\n\nhduser3$ bin/hdfs dfs -mkdir /user\nhduser3$ bin/hdfs dfs -mkdir /user/hduser3\nhduser3$ bin/hdfs dfs -put etc/hadoop input\n\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n\nhduser3$ bin/hdfs dfs -ls /user/hduser3\nFound 2 items\ndrwxr-xr-x   - hduser3 supergroup          0 2016-02-16 15:05 /user/hduser3/input\ndrwxr-xr-x   - hduser3 supergroup          0 2016-02-16 15:06 /user/hduser3/output\n\nhduser3$ bin/hdfs dfs -get output output\nhduser3$ ls output/\npart-r-00000  _SUCCESS\n\nhduser3$ cat output/part-r-00000 \n4   dfs.class\n4   dfs.audit.logger\n3   dfs.server.namenode.\n2   dfs.period\n2   dfs.audit.log.maxfilesize\n2   dfs.audit.log.maxbackupindex\n1   dfsmetrics.log\n1   dfsadmin\n1   dfs.servers\n1   dfs.replication\n1   dfs.file\n\n\u52d5\u3044\u305f\u3002\n\n\u5099\u8003\n\n\u4eee\u60f3\u30de\u30b7\u30f3\u306e\u30e1\u30e2\u30ea\u5272\u5f53\u3092\u5148\u306b\u3084\u3063\u3066\u304a\u304f\u3068\u3001\u9014\u4e2d\u306e\u624b\u623b\u308a\u304c\u306a\u3044\nadduser\u3058\u3083\u306a\u304f\u3066useradd\u306e\u5834\u5408\u3001\u30b3\u30de\u30f3\u30c9\u306b\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u306a\u3044\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u306a\u304b\u3063\u305f\u308a\u3059\u308b(debian\u7cfb\u306e\u5834\u5408)\n\u4f5c\u696d\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5916\u306b\u3059\u308b\u306a\u3089\u3001hadoop\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d1\u30b9\u3092\u901a\u3057\u305f\u308a\u3059\u308b\nstart-dfs.sh\u3059\u308b\u3068 http://localhost:50070/ \u3067web\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3082\u53c2\u7167\u3067\u304d\u308b\u304c\u3001Vagrantfile\u3067port forward\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\n\n\n\u53c2\u8003\u30b5\u30a4\u30c8\nHadoop\nMapReduce Tutorial\n# \u3084\u308b\u3053\u3068\nmac\u306bvirtualbox\u3068vagrant\u3092\u5165\u308c\u3066ubuntu\u3092\u52d5\u304b\u3059\u3001\u305d\u306e\u4e0a\u3067hadoop\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u3002\n\u672c\u8aad\u3093\u3060\u308a\u30b5\u30a4\u30c8\u898b\u305f\u308a\u3057\u3066\u3066\u3082\u3001\u66f8\u3044\u3066\u3042\u308b\u3053\u3068\u304c\u30d0\u30e9\u30d0\u30e9\u3067\u3069\u308c\u3092\u5199\u7d4c\u3057\u3066\u3044\u3044\u306e\u304b\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u5099\u5fd8\u9332\u3068\u3057\u3066\u6b8b\u3057\u3066\u304a\u304f\u3002\n\n# \u74b0\u5883\nMac OS X 10.11.3\nVagrant 1.8.1\n- ubuntu 14.04\nHadoop 2.7.2\n\n# \u59cb\u3081\u308b\u524d\u306b\n\u5f8c\u5148\u8003\u3048\u305a\u306b\u884c\u304f\u3068hadoop\u3063\u3066\u5358\u8a9e\u304c\u5c71\u307b\u3069\u51fa\u3066\u304d\u3066\u3053\u3093\u304c\u3089\u304c\u308b\u306e\u3067\u3001\u5224\u5225\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002\u30e6\u30fc\u30b6\u30fc\u540d\u3068\u304b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u3068\u304b\u3002\n\n* hdtest1\n* testdir2\n* hduser3\n\n\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u5165\u529b\u3082\u3001\u3069\u306e\u74b0\u5883\u3067\u3001\u8ab0\u3092\u64cd\u4f5c\u3057\u3066\u3044\u308b\u306e\u304b\u3001\u6df7\u540c\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u308b\n\n# \u5b9f\u8df5\n\n## \u6e96\u5099\nvagrant\u3067ubuntu\u3092\u7528\u610f\u3059\u308b\u3002\u4e8b\u524d\u306bhadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304a\u304f\u3002\n\n```\nmac$ mkdir hdtest1\nmac$ mv hadoo-2.7.2.tar.gz hdtest1/\nmac$ cd hdtest1\nmac$ vagrant init ubuntu/trusty64\nmac$ vagrant up\nmac$ vagrant ssh\n\nvagrant$ vi .profile\n```\n\n\u3068\u308a\u3042\u3048\u305a\u3084\u3063\u3066\u304a\u304f\n\n```bash:.profile\n# \u672c\u984c\u3068\u95a2\u4fc2\u306a\u3044\u3051\u3069\u30e1\u30e2\nexport HISTSIZE=10000\nexport HISTFILESIZE=10000\nexport HISTCONTROL=ignoredups\nexport HISTIGNORE=ls:ll:lla:pwd:history\n```\n\n```\n# \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5b9f\u884c\nvagrant$ sudo apt-get autoremove -y\nvagrant$ sudo apt-get update -y\nvagrant$ sudo apt-get upgrade -y\nvagrant$ sudo apt-get install -y default-jdk\n```\n\n## \u30b9\u30bf\u30f3\u30c9\u30a2\u30ed\u30f3\u30e2\u30fc\u30c9\u3067\u52d5\u304b\u3059\n\n[Standalone Operation](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation)\n\n```\n# mac\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30de\u30a6\u30f3\u30c8\u3067\u304d\u3066\u3044\u308b\u3082\u306e\u3068\u3059\u308b\nvagrant$ cp /vagrant/hadoop-2.7.2.tar.gz .\nvagrant$ tar zxvf hadoop-2.7.2.tar.gz\nvagrant$ cd hadoop-2.7.2\n\n# \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f5c\u3063\u3066\u3001\u30b5\u30f3\u30d7\u30eb\u306e\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u3066\u3044\u308b\u3060\u3051\nvagrant$ mkdir testdir2\nvagrant$ mkdir testdir2/input\nvagrant$ cp etc/hadoop/*.xml testdir2/input\n\n# \u5b9f\u884c\nvagrant$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep testdir2/input testdir2/output 'dfs[a-z.]+'\nError: JAVA_HOME is not set and could not be found.\n\n# JAVA_HOME\u304c\u306a\u3044\u3068\u8a00\u308f\u308c\u305f\n# \u30ea\u30f3\u30af\u3067\u306f\u3046\u307e\u304f\u884c\u304b\u306a\u304b\u3063\u305f\u6c17\u304c\u3059\u308b\u306e\u3067\u672c\u4f53\u3092\u63a2\u3059\nvagrant$ ls /usr/lib/jvm/\ndefault-java  java-1.7.0-openjdk-amd64  java-7-openjdk-amd64\n\n# JAVA_HOME\u3092\u8a2d\u5b9a\u3057\u3066\u5b9f\u884c\nvagrant$ JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep testdir2/input testdir2/output 'dfs[a-z.]+'\n\nvagrant$ ls testdir2/output/\npart-r-00000  _SUCCESS\n\nvagrant$ cat testdir2/output/part-r-00000 \n1\tdfsadmin\n```\n\n\u52d5\u3044\u305f\n\n## \u64ec\u4f3c\u5206\u6563\u30e2\u30fc\u30c9\u3067\u52d5\u304b\u3059\n\n[Pseudo-Distributed Operation](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)\n\n\u3044\u308d\u3044\u308d\u6e96\u5099\n\n```\n# \u5c02\u7528\u306e\u30e6\u30fc\u30b6\u3092\u8ffd\u52a0\u3059\u308b\n# useradd\u3067\u306a\u304f\u3066adduser\u3059\u308b\nvagrant$ adduser hduser3\n\n# hadoop\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5148\u3092\u5909\u3048\u3066\u3001\u5148\u306e\u30e6\u30fc\u30b6\u306e\u6240\u6709\u3068\u3059\u308b\nvagrant$ cd ~\nvagrant$ sudo mv hadoop-2.7.2 /usr/local/\nvagrant$ sudo chown -R hduser3:hduser3 /usr/local/hadoop-2.7.2/\n\n# hduser3\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b\nvagrant$ ssh hduser3@localhost\n\n# \u521d\u56de\u306fknown_hosts\u306b\u8ffd\u52a0\u3059\u308b\u306e\u3092\u805e\u304b\u308c\u308b\n# \u30d1\u30b9\u30ef\u30fc\u30c9\u5165\u529b\u304c\u9762\u5012\u306a\u3089\u9375\u8a8d\u8a3c\u306b\u3059\u308b\u304c\u7701\u7565\n\n# \u5148\u306bvagrant\u30e6\u30fc\u30b6\u30fc\u3067\u3082\u3084\u3063\u305f.profile\u306e\u8ffd\u52a0\u3092\u3057\u305f\u308a\u3057\u305f\n\n# \u81ea\u5206\u7528\u306e\u9375\u8a8d\u8a3c\u3092\u8a2d\u5b9a\u3059\u308b\nhduser3$ ssh-keygen -t rsa -P ''\nhduser3$ cat .ssh/id_rsa.pub >> .ssh/authorized_keys\nhduser3$ chmod 0600 .ssh/authorized_keys\n\nhduser3$ cd /usr/local/hadoop-2.7.2\n```\n\n\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3059\u308b\u3002core-site.xml\u306f\u6ce8\u610f\u3002\u30b5\u30a4\u30c8\u3068\u304b\u672c\u3068\u304b\u3068\u7570\u306a\u308b\u306e\u3067\u516c\u5f0f\u30b5\u30a4\u30c8\u3092\u53c2\u7167\u3002\n\n```xml:etc/hadoop/core-site.xml\n<configuration>\n  <property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://localhost:9000</value>\n  </property>\n</configuration>\n```\n\n```xml:etc/hadoop/hdfs-site.xml\n<configuration>\n  <property>\n    <name>dfs.replication</name>\n    <value>1</value>\n  </property>\n</configuration>\n```\n\nhdfs\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n\n```\nhduser3$ bin/hdfs namenode -format\nError: JAVA_HOME is not set and could not be found.\n\n# \u307e\u305fJAVA_HOME\u304c\u306a\u3044\u3068\u8a00\u308f\u308c\u305f\n```\n\n\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3059\u308b\n\n```bash:etc/hadoop/hadoop-env.sh\n# The java implementation to use.\n#export JAVA_HOME=${JAVA_HOME}\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n```\n\n\u518d\u5ea6\u5b9f\u884c\n\n```\nhduser3$ bin/hdfs namenode -format\nhduser3$ sbin/start-dfs.sh \nhduser3$ jps\n12244 Jps\n11936 DataNode\n12135 SecondaryNameNode\n11771 NameNode\n\nhduser3$ bin/hdfs dfs -mkdir /user\nhduser3$ bin/hdfs dfs -mkdir /user/hduser3\nhduser3$ bin/hdfs dfs -put etc/hadoop input\n\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n...\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000e0ebb000, 104861696, 0) failed; error='Cannot allocate memory' (errno=12)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (malloc) failed to allocate 104861696 bytes for committing reserved memory.\n# An error report file with more information is saved as:\n# /usr/local/hadoop-2.7.2/hs_err_pid12520.log\n```\n\n\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u3044\u3089\u3057\u3044\u306e\u3067\u3001\u30e1\u30e2\u30ea\u3092\u5897\u3084\u3057\u3066\u304f\u308b\n\n```\nhduser3$ exit\nvagrant$ exit\nmac$ vi Vagrantfile\n```\n\n```ruby:Vagrantfile\n  # config.vm.provider \"virtualbox\" do |vb|\n  #   # Display the VirtualBox GUI when booting the machine\n  #   vb.gui = true\n  #\n  #   # Customize the amount of memory on the VM:\n  #   vb.memory = \"1024\"\n  # end\n  config.vm.provider \"virtualbox\" do |vb|\n  #   # Customize the amount of memory on the VM:\n    vb.memory = \"2048\"\n  end\n```\n\n\u518d\u5ea6\u5b9f\u884c\n\n```\nmac$ vagrant ssh\nvagrant$ ssh hduser3@localhost\n\nhduser3$ cd /usr/local/hadoop-2.7.2\nhduser3$ sbin/start-dfs.sh\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n16/02/16 15:01:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n16/02/16 15:01:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\njava.net.ConnectException: Call From vagrant-ubuntu-trusty-64/10.0.2.15 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n...\n\nhduser3$ jps\n1835 DataNode\n2029 SecondaryNameNode\n2168 Jps\n```\n\n\u3053\u3051\u305f\u3002\u4f55\u304b\u8db3\u308a\u306a\u3044\u3002  \n\u4e00\u65e6hdfs\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664\u3057\u3066\u3084\u308a\u76f4\u3059\u3002\n\n```\nhduser3$ sbin/stop-dfs.sh\nhduser3$ rm -rf /tmp/hadoop-hduser3/\nhduser3$ bin/hdfs namenode -format\nhduser3$ sbin/start-dfs.sh\nhduser3$ jps\n3080 DataNode\n3285 SecondaryNameNode\n3394 Jps\n2909 NameNode\n\nhduser3$ bin/hdfs dfs -mkdir /user\nhduser3$ bin/hdfs dfs -mkdir /user/hduser3\nhduser3$ bin/hdfs dfs -put etc/hadoop input\n\nhduser3$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n\nhduser3$ bin/hdfs dfs -ls /user/hduser3\nFound 2 items\ndrwxr-xr-x   - hduser3 supergroup          0 2016-02-16 15:05 /user/hduser3/input\ndrwxr-xr-x   - hduser3 supergroup          0 2016-02-16 15:06 /user/hduser3/output\n\nhduser3$ bin/hdfs dfs -get output output\nhduser3$ ls output/\npart-r-00000  _SUCCESS\n\nhduser3$ cat output/part-r-00000 \n4\tdfs.class\n4\tdfs.audit.logger\n3\tdfs.server.namenode.\n2\tdfs.period\n2\tdfs.audit.log.maxfilesize\n2\tdfs.audit.log.maxbackupindex\n1\tdfsmetrics.log\n1\tdfsadmin\n1\tdfs.servers\n1\tdfs.replication\n1\tdfs.file\n```\n\n\u52d5\u3044\u305f\u3002\n\n## \u5099\u8003\n* \u4eee\u60f3\u30de\u30b7\u30f3\u306e\u30e1\u30e2\u30ea\u5272\u5f53\u3092\u5148\u306b\u3084\u3063\u3066\u304a\u304f\u3068\u3001\u9014\u4e2d\u306e\u624b\u623b\u308a\u304c\u306a\u3044\n* adduser\u3058\u3083\u306a\u304f\u3066useradd\u306e\u5834\u5408\u3001\u30b3\u30de\u30f3\u30c9\u306b\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u3064\u3051\u306a\u3044\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u306a\u304b\u3063\u305f\u308a\u3059\u308b(debian\u7cfb\u306e\u5834\u5408)\n* \u4f5c\u696d\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5916\u306b\u3059\u308b\u306a\u3089\u3001hadoop\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d1\u30b9\u3092\u901a\u3057\u305f\u308a\u3059\u308b\n* start-dfs.sh\u3059\u308b\u3068 http://localhost:50070/ \u3067web\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3082\u53c2\u7167\u3067\u304d\u308b\u304c\u3001Vagrantfile\u3067port forward\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\n\n## \u53c2\u8003\u30b5\u30a4\u30c8\n[Hadoop](http://hadoop.apache.org/)\n\n[MapReduce Tutorial](http://hadoop.apache.org/docs/r2.7.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)\n", "tags": ["hadoop"]}