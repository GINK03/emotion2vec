{"context": "100\u672c\u30ce\u30c3\u30af\u306e\u300c\u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790 (40\u301c49)\u300d\u3092\u89e3\u3044\u3066\u307f\u307e\u3057\u305f\u3002\u7b2c4\u7ae0 \u5f62\u614b\u7d20\u89e3\u6790 (30\u301c39)\u306e\u7d9a\u304d\u3067\u3059\u3002\n\n\u74b0\u5883\n\nOS X El Capitan Version 10.11.4 \nPython 3.5.1\n\n\n\u53c2\u8003\u306b\u3057\u305f\u30da\u30fc\u30b8\nCaboCha\u3001pydot\u3001Graphviz\u306e\u5c0e\u5165\u3084\u3001\u300c49. \u540d\u8a5e\u9593\u306e\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u62bd\u51fa\u300d\u3067\u7d50\u69cb\u30cf\u30de\u308a\u307e\u3057\u305f\u304c\u3001\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u9802\u304d\u306a\u3093\u3068\u304b\u89e3\u6c7a\u3057\u307e\u3057\u305f\u3002\n\n\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u524d\u534a)\n\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u5f8c\u534a)\n\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015\u5e74\u7248 (46\uff5e49)\nCaboCha\u516c\u5f0f\u30b5\u30a4\u30c8\nCaboCha\u3067\u59cb\u3081\u308b\u4fc2\u308a\u53d7\u3051\u89e3\u6790\nGraphviz\u3068dot\u8a00\u8a9e\u3067\u30b0\u30e9\u30d5\u3092\u63cf\u304f\u65b9\u6cd5\u306e\u307e\u3068\u3081\ngraphviz\u3092\u4f7f\u3063\u3066Python3\u3067\u6728\u69cb\u9020\u3092\u63cf\u304f\nAttributeError: module 'pydot' has no attribute 'graph_from_dot_data' in spyder\n\n\n\u6e96\u5099\n\n\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\nimport CaboCha\nimport pydotplus\nimport subprocess\n\n\n\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u6587\u7ae0\u306e\u4fdd\u5b58\n\n\u590f\u76ee\u6f31\u77f3\u306e\u5c0f\u8aac\u300e\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u300f\u306e\u6587\u7ae0\uff08neko.txt\uff09\u3092CaboCha\u3092\u4f7f\u3063\u3066\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u3057\uff0c\u305d\u306e\u7d50\u679c\u3092neko.txt.cabocha\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u305b\u3088\uff0e\n\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u3044\u3066\uff0c\u4ee5\u4e0b\u306e\u554f\u306b\u5bfe\u5fdc\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u88c5\u305b\u3088\uff0e\n\ndef make_analyzed_file(input_file_name: str, output_file_name: str) -> None:\n    \"\"\"\n    \u30d7\u30ec\u30fc\u30f3\u306a\u65e5\u672c\u8a9e\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b.\n    (\u7a7a\u767d\u306f\u524a\u9664\u3057\u307e\u3059.)\n    :param input_file_name \u30d7\u30ec\u30fc\u30f3\u306a\u65e5\u672c\u8a9e\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :param output_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    c = CaboCha.Parser()\n    with open(input_file_name, encoding='utf-8') as input_file:\n        with open(output_file_name, mode='w', encoding='utf-8') as output_file:\n            for line in input_file:\n                tree = c.parse(line.lstrip())\n                output_file.write(tree.toString(CaboCha.FORMAT_LATTICE))\n\n\nmake_analyzed_file('neko.txt', 'neko.txt.cabocha')\n\n\n40. \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\uff08\u5f62\u614b\u7d20\uff09\n\n\u5f62\u614b\u7d20\u3092\u8868\u3059\u30af\u30e9\u30b9Morph\u3092\u5b9f\u88c5\u305b\u3088\uff0e\u3053\u306e\u30af\u30e9\u30b9\u306f\u8868\u5c64\u5f62\uff08surface\uff09\uff0c\u57fa\u672c\u5f62\uff08base\uff09\uff0c\u54c1\u8a5e\uff08pos\uff09\uff0c\u54c1\u8a5e\u7d30\u5206\u985e1\uff08pos1\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\u3053\u3068\u3068\u3059\u308b\uff0e\n\u3055\u3089\u306b\uff0cCaboCha\u306e\u89e3\u6790\u7d50\u679c\uff08neko.txt.cabocha\uff09\u3092\u8aad\u307f\u8fbc\u307f\uff0c\u5404\u6587\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\uff0c3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\u305b\u3088\uff0e\n\nclass Morph:\n    \"\"\"\n    1\u3064\u306e\u5f62\u614b\u7d20\u3092\u8868\u3059\u30af\u30e9\u30b9\n    \"\"\"\n\n    def __init__(self, surface, base, pos, pos1):\n        \"\"\"\n        \u30e1\u30f3\u30d0\u5909\u6570\u3068\u3057\u3066\u8868\u5c64\u5f62\uff08surface\uff09\uff0c\u57fa\u672c\u5f62\uff08base\uff09\uff0c\u54c1\u8a5e\uff08pos\uff09\uff0c\u54c1\u8a5e\u7d30\u5206\u985e1\uff08pos1\uff09\u3092\u6301\u3064.\n        \"\"\"\n        self.surface = surface\n        self.base = base\n        self.pos = pos\n        self.pos1 = pos1\n\n    def is_end_of_sentence(self) -> bool: return self.pos1 == '\u53e5\u70b9'\n\n    def __str__(self) -> str: return 'surface: {}, base: {}, pos: {}, pos1: {}'.format(self.surface, self.base, self.pos, self.pos1)\n\n\ndef make_morph_list(analyzed_file_name: str) -> list:\n    \"\"\"\n    \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u5404\u6587\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\n    :param analyzed_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :return list \u4e00\u3064\u306e\u6587\u7ae0\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    sentences = []\n    sentence = []\n    with open(analyzed_file_name, encoding='utf-8') as input_file:\n        for line in input_file:\n            line_list = line.split()\n            if (line_list[0] == '*') | (line_list[0] == 'EOS'):\n                pass\n            else:\n                line_list = line_list[0].split(',') + line_list[1].split(',')\n                # \u3053\u306e\u6642\u70b9\u3067line_list\u306f\u3053\u3093\u306a\u611f\u3058\n                # ['\u59cb\u3081', '\u540d\u8a5e', '\u526f\u8a5e\u53ef\u80fd', '*', '*', '*', '*', '\u59cb\u3081', '\u30cf\u30b8\u30e1', '\u30cf\u30b8\u30e1']\n                _morph = Morph(surface=line_list[0], base=line_list[7], pos=line_list[1], pos1=line_list[2])\n\n                sentence.append(_morph)\n\n                if _morph.is_end_of_sentence():\n                    sentences.append(sentence)\n                    sentence = []\n\n    return sentences\n\n\nmorphed_sentences = make_morph_list('neko.txt.cabocha')\n\n# 3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\nfor morph in morphed_sentences[2]:\n    print(str(morph))\n\n\n41. \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\uff08\u6587\u7bc0\u30fb\u4fc2\u308a\u53d7\u3051\uff09\n\n40\u306b\u52a0\u3048\u3066\uff0c\u6587\u7bc0\u3092\u8868\u3059\u30af\u30e9\u30b9Chunk\u3092\u5b9f\u88c5\u305b\u3088\uff0e\n\u3053\u306e\u30af\u30e9\u30b9\u306f\u5f62\u614b\u7d20\uff08Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09\u306e\u30ea\u30b9\u30c8\uff08morphs\uff09\uff0c\u4fc2\u308a\u5148\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\uff08dst\uff09\uff0c\u4fc2\u308a\u5143\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u306e\u30ea\u30b9\u30c8\uff08srcs\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\u3053\u3068\u3068\u3059\u308b\uff0e\n\u3055\u3089\u306b\uff0c\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u306eCaboCha\u306e\u89e3\u6790\u7d50\u679c\u3092\u8aad\u307f\u8fbc\u307f\uff0c\uff11\u6587\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\uff0c8\u6587\u76ee\u306e\u6587\u7bc0\u306e\u6587\u5b57\u5217\u3068\u4fc2\u308a\u5148\u3092\u8868\u793a\u305b\u3088\uff0e\n\u7b2c5\u7ae0\u306e\u6b8b\u308a\u306e\u554f\u984c\u3067\u306f\uff0c\u3053\u3053\u3067\u4f5c\u3063\u305f\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6d3b\u7528\u305b\u3088\uff0e\n\nChunk\u30af\u30e9\u30b9\u306b\u305f\u304f\u3055\u3093\u30e1\u30bd\u30c3\u30c9\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u5fc5\u8981\u306a\u306e\u306f__init__\u3068__str__\u306e\u307f\u3067\u3059\u3002\u305d\u306e\u4ed6\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u3001\u5f8c\u7d9a\u306e\u554f\u3092\u89e3\u3044\u3066\u3044\u304f\u306b\u3042\u305f\u3063\u3066\u90fd\u5ea6\u8ffd\u52a0\u3057\u3066\u3044\u3063\u305f\u3082\u306e\u3067\u3059\u3002\nclass Chunk:\n    def __init__(self, morphs: list, dst: str, srcs: str) -> None:\n        \"\"\"\n        \u5f62\u614b\u7d20\uff08Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09\u306e\u30ea\u30b9\u30c8\uff08morphs\uff09\uff0c\u4fc2\u308a\u5148\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\uff08dst\uff09\uff0c\u4fc2\u308a\u5143\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u306e\u30ea\u30b9\u30c8\uff08srcs\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\n        \"\"\"\n        self.morphs = morphs\n        self.dst = int(dst.strip(\"D\"))\n        self.srcs = int(srcs)\n\n    # \u4ee5\u4e0b\u306f\u5f8c\u3005\u4f7f\u3046\u30e1\u30bd\u30c3\u30c9\u3067\u3059.\n    def join_morphs(self) -> str:\n        return ''.join([_morph.surface for _morph in self.morphs if _morph.pos != '\u8a18\u53f7'])\n\n    def has_noun(self) -> bool:\n        return any([_morph.pos == '\u540d\u8a5e' for _morph in self.morphs])\n\n    def has_verb(self) -> bool:\n        return any([_morph.pos == '\u52d5\u8a5e' for _morph in self.morphs])\n\n    def has_particle(self) -> bool:\n        return any([_morph.pos == '\u52a9\u8a5e' for _morph in self.morphs])\n\n    def has_sahen_connection_noun_plus_wo(self) -> bool:\n        \"\"\"\n        \u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092\uff08\u52a9\u8a5e\uff09\u300d\u3092\u542b\u3080\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059.\n        \"\"\"\n        for idx, _morph in enumerate(self.morphs):\n            if _morph.pos == '\u540d\u8a5e' and _morph.pos1 == '\u30b5\u5909\u63a5\u7d9a' and len(self.morphs[idx:]) > 1 and \\\n                            self.morphs[idx + 1].pos == '\u52a9\u8a5e' and self.morphs[idx + 1].base == '\u3092':\n                return True\n\n        return False\n\n    def first_verb(self) -> Morph:\n        return [_morph for _morph in self.morphs if _morph.pos == '\u52d5\u8a5e'][0]\n\n    def last_particle(self) -> list:\n        return [_morph for _morph in self.morphs if _morph.pos == '\u52a9\u8a5e'][-1]\n\n    def pair(self, sentence: list) -> str:\n        return self.join_morphs() + '\\t' + sentence[self.dst].join_morphs()\n\n    def replace_noun(self, alt: str) -> None:\n        \"\"\"\n        \u540d\u8a5e\u306e\u8868\u8c61\u3092\u7f6e\u63db\u3059\u308b.\n        \"\"\"\n        for _morph in self.morphs:\n            if _morph.pos == '\u540d\u8a5e':\n                _morph.surface = alt\n\n    def __str__(self) -> str:\n        return 'srcs: {}, dst: {}, morphs: ({})'.format(self.srcs, self.dst, ' / '.join([str(_morph) for _morph in self.morphs]))\n\n\ndef make_chunk_list(analyzed_file_name: str) -> list:\n    \"\"\"\n    \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u5404\u6587\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\n    :param analyzed_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :return list \u4e00\u3064\u306e\u6587\u7ae0\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    sentences = []\n    sentence = []\n    _chunk = None\n    with open(analyzed_file_name, encoding='utf-8') as input_file:\n        for line in input_file:\n            line_list = line.split()\n            if line_list[0] == '*':\n                if _chunk is not None:\n                    sentence.append(_chunk)\n                _chunk = Chunk(morphs=[], dst=line_list[2], srcs=line_list[1])\n            elif line_list[0] == 'EOS':  # End of sentence\n                if _chunk is not None:\n                    sentence.append(_chunk)\n                if len(sentence) > 0:\n                    sentences.append(sentence)\n                _chunk = None\n                sentence = []\n            else:\n                line_list = line_list[0].split(',') + line_list[1].split(',')\n                # \u3053\u306e\u6642\u70b9\u3067line_list\u306f\u3053\u3093\u306a\u611f\u3058\n                # ['\u59cb\u3081', '\u540d\u8a5e', '\u526f\u8a5e\u53ef\u80fd', '*', '*', '*', '*', '\u59cb\u3081', '\u30cf\u30b8\u30e1', '\u30cf\u30b8\u30e1']\n                _morph = Morph(surface=line_list[0], base=line_list[7], pos=line_list[1], pos1=line_list[2])\n                _chunk.morphs.append(_morph)\n\n    return sentences\n\n\nchunked_sentences = make_chunk_list('neko.txt.cabocha')\n\n# 3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\nfor chunk in chunked_sentences[2]:\n    print(str(chunk))\n\n\n42. \u4fc2\u308a\u5143\u3068\u4fc2\u308a\u5148\u306e\u6587\u7bc0\u306e\u8868\u793a\n\n\u4fc2\u308a\u5143\u306e\u6587\u7bc0\u3068\u4fc2\u308a\u5148\u306e\u6587\u7bc0\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u3059\u3079\u3066\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u53e5\u8aad\u70b9\u306a\u3069\u306e\u8a18\u53f7\u306f\u51fa\u529b\u3057\u306a\u3044\u3088\u3046\u306b\u305b\u3088\uff0e\n\n44\u3067\u4f7f\u3044\u3084\u3059\u3044\u3088\u3046\u306b\u6587\u7ae0\u3054\u3068\u306b\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002\ndef is_valid_chunk(_chunk, sentence):\n    return _chunk.join_morphs() != '' and _chunk.dst > -1 and sentence[_chunk.dst].join_morphs() != ''\n\n\npaired_sentences = [[chunk.pair(sentence) for chunk in sentence if is_valid_chunk(chunk, sentence)] for sentence in chunked_sentences if len(sentence) > 1]\nprint(paired_sentences[0:100])\n\n\n43. \u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u304c\u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u4fc2\u308b\u3082\u306e\u3092\u62bd\u51fa\n\n\u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u304c\uff0c\u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u4fc2\u308b\u3068\u304d\uff0c\u3053\u308c\u3089\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u53e5\u8aad\u70b9\u306a\u3069\u306e\u8a18\u53f7\u306f\u51fa\u529b\u3057\u306a\u3044\u3088\u3046\u306b\u305b\u3088\uff0e\n\nChunk\u30af\u30e9\u30b9\u306b\u8272\u3005\u4fbf\u5229\u30e1\u30bd\u30c3\u30c9\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u306e\u3067\u697d\u3067\u3059\u306d\u3002\nfor sentence in chunked_sentences:\n    for chunk in sentence:\n        if chunk.has_noun() and chunk.dst > -1 and sentence[chunk.dst].has_verb():\n            print(chunk.pair(sentence))\n\n\n44. \u4fc2\u308a\u53d7\u3051\u6728\u306e\u53ef\u8996\u5316\n\n\u4e0e\u3048\u3089\u308c\u305f\u6587\u306e\u4fc2\u308a\u53d7\u3051\u6728\u3092\u6709\u5411\u30b0\u30e9\u30d5\u3068\u3057\u3066\u53ef\u8996\u5316\u305b\u3088\uff0e\n\u53ef\u8996\u5316\u306b\u306f\uff0c\u4fc2\u308a\u53d7\u3051\u6728\u3092DOT\u8a00\u8a9e\u306b\u5909\u63db\u3057\uff0cGraphviz\u3092\u7528\u3044\u308b\u3068\u3088\u3044\uff0e\u307e\u305f\uff0cPython\u304b\u3089\u6709\u5411\u30b0\u30e9\u30d5\u3092\u76f4\u63a5\u7684\u306b\u53ef\u8996\u5316\u3059\u308b\u306b\u306f\uff0cpydot\u3092\u4f7f\u3046\u3068\u3088\u3044\uff0e\n\ndef sentence_to_dot(idx: int, sentence: list) -> str:\n    head = \"digraph sentence{} \".format(idx)\n    body_head = \"{ graph [rankdir = LR]; \"\n    body_list = ['\"{}\"->\"{}\"; '.format(*chunk_pair.split()) for chunk_pair in sentence]\n\n    return head + body_head + ''.join(body_list) + '}'\n\n\ndef sentences_to_dots(sentences: list) -> list:\n    _dots = []\n    for idx, sentence in enumerate(sentences):\n        _dots.append(sentence_to_dot(idx, sentence))\n    return _dots\n\n\ndef save_graph(dot: str, file_name: str) -> None:\n    g = pydotplus.graph_from_dot_data(dot)\n    g.write_jpeg(file_name, prog='dot')\n\n\ndots = sentences_to_dots(paired_sentences)\nfor idx in range(101, 104):\n    save_graph(dots[idx], 'graph{}.jpg'.format(idx))\n\n\n\u3010\u30b5\u30f3\u30d7\u30eb\u3011101\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n\n\n\u3010\u30b5\u30f3\u30d7\u30eb\u3011102\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n\n\n\u3010\u30b5\u30f3\u30d7\u30eb\u3011103\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n\n\u203b\u3061\u306a\u307f\u306b\u300c\u986b\u3048\u308b\u300d\u306f\u300c\u3075\u308b\u3048\u308b\u300d\u3068\u8aad\u3080\u305d\u3046\u3067\u3059\u3002\n\n45. \u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u62bd\u51fa\n\n\u4eca\u56de\u7528\u3044\u3066\u3044\u308b\u6587\u7ae0\u3092\u30b3\u30fc\u30d1\u30b9\u3068\u898b\u306a\u3057\uff0c\u65e5\u672c\u8a9e\u306e\u8ff0\u8a9e\u304c\u53d6\u308a\u3046\u308b\u683c\u3092\u8abf\u67fb\u3057\u305f\u3044\uff0e\n\u52d5\u8a5e\u3092\u8ff0\u8a9e\uff0c\u52d5\u8a5e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u306e\u52a9\u8a5e\u3092\u683c\u3068\u8003\u3048\uff0c\u8ff0\u8a9e\u3068\u683c\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n\u305f\u3060\u3057\uff0c\u51fa\u529b\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u305b\u3088\uff0e\n\n\u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u304a\u3044\u3066\uff0c\u6700\u5de6\u306e\u52d5\u8a5e\u306e\u57fa\u672c\u5f62\u3092\u8ff0\u8a9e\u3068\u3059\u308b\n\u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\u3092\u683c\u3068\u3059\u308b\n\u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\uff08\u6587\u7bc0\uff09\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u3059\u3079\u3066\u306e\u52a9\u8a5e\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u8f9e\u66f8\u9806\u306b\u4e26\u3079\u308b\n\n\u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u4f8b\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u3092\u8003\u3048\u308b\uff0e\n\u3053\u306e\u6587\u306f\u300c\u59cb\u3081\u308b\u300d\u3068\u300c\u898b\u308b\u300d\u306e\uff12\u3064\u306e\u52d5\u8a5e\u3092\u542b\u307f\uff0c\u300c\u59cb\u3081\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u3053\u3053\u3067\u300d\uff0c\u300c\u898b\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u543e\u8f29\u306f\u300d\u3068\u300c\u3082\u306e\u3092\u300d\u3068\u89e3\u6790\u3055\u308c\u305f\u5834\u5408\u306f\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u306b\u306a\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\u59cb\u3081\u308b  \u3067\n\u898b\u308b    \u306f \u3092\n\n\u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\uff0c\u4ee5\u4e0b\u306e\u4e8b\u9805\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\u305b\u3088\uff0e\n\n\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b\n\u300c\u3059\u308b\u300d\u300c\u898b\u308b\u300d\u300c\u4e0e\u3048\u308b\u300d\u3068\u3044\u3046\u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\uff08\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e26\u3079\u3088\uff09\n\n\ndef case_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.(\u300c\u683c\u300d\u306f\u82f1\u8a9e\u3067\"Case\"\u3068\u3044\u3046\u3089\u3057\u3044.)\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u4e0e\u3048\u308b', ['\u306b', '\u3092']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _case_pattern = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n\n            if len(particles) > 0:\n                _case_pattern.append([_chunk.first_verb().base, sorted(particles)])\n\n    return _case_pattern\n\n\ndef save_case_patterns(_case_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _case_patterns \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u4e0e\u3048\u308b', ['\u306b', '\u3092']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for _case in _case_patterns:\n            output_file.write('{}\\t{}\\n'.format(_case[0], ' '.join(_case[1])))\n\n\nsave_case_patterns(case_patterns(chunked_sentences), 'case_patterns.txt')\n\n\ndef print_case_pattern_ranking(_grep_str: str) -> None:\n    \"\"\"\n    \u30b3\u30fc\u30d1\u30b9(case_pattern.txt)\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e0a\u4f4d20\u4ef6\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u3092\u8868\u793a\u3059\u308b.\n    `cat case_patterns.txt | grep '^\u3059\u308b\\t' | sort | uniq -c | sort -r | head -20`\u306e\u3088\u3046\u306aUnix\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066print\u3057\u3066\u3044\u308b.\n    grep\u306e\u90e8\u5206\u306f\u5f15\u6570`_grep_str`\u306b\u5fdc\u3058\u3066\u4ed8\u52a0\u3055\u308c\u308b.\n    :param _grep_str \u691c\u7d22\u6761\u4ef6\u3068\u306a\u308b\u52d5\u8a5e\n    \"\"\"\n    _grep_str = '' if _grep_str == '' else '| grep \\'^{}\\t\\''.format(_grep_str)\n    print(subprocess.run('cat case_patterns.txt {} | sort | uniq -c | sort -r | head -10'.format(_grep_str), shell=True))\n\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b\uff08\u4e0a\u4f4d10\u4ef6\uff09\n# \u300c\u3059\u308b\u300d\u300c\u898b\u308b\u300d\u300c\u4e0e\u3048\u308b\u300d\u3068\u3044\u3046\u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\uff08\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e0a\u4f4d10\u4ef6\uff09\nfor grep_str in ['', '\u3059\u308b', '\u898b\u308b', '\u4e0e\u3048\u308b']:\n    print_case_pattern_ranking(grep_str)\n\n\n46. \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u60c5\u5831\u306e\u62bd\u51fa\n\n45\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6539\u5909\u3057\uff0c\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306b\u7d9a\u3051\u3066\u9805\uff08\u8ff0\u8a9e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u305d\u306e\u3082\u306e\uff09\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n45\u306e\u4ed5\u69d8\u306b\u52a0\u3048\u3066\uff0c\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u305b\u3088\uff0e\n\n\u9805\u306f\u8ff0\u8a9e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u306e\u5358\u8a9e\u5217\u3068\u3059\u308b\uff08\u672b\u5c3e\u306e\u52a9\u8a5e\u3092\u53d6\u308a\u9664\u304f\u5fc5\u8981\u306f\u306a\u3044\uff09\n\u8ff0\u8a9e\u306b\u4fc2\u308b\u6587\u7bc0\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u52a9\u8a5e\u3068\u540c\u4e00\u306e\u57fa\u6e96\u30fb\u9806\u5e8f\u3067\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u4e26\u3079\u308b\n\n\u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u4f8b\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u3092\u8003\u3048\u308b\uff0e\n\u3053\u306e\u6587\u306f\u300c\u59cb\u3081\u308b\u300d\u3068\u300c\u898b\u308b\u300d\u306e\uff12\u3064\u306e\u52d5\u8a5e\u3092\u542b\u307f\uff0c\u300c\u59cb\u3081\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u3053\u3053\u3067\u300d\uff0c\u300c\u898b\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u543e\u8f29\u306f\u300d\u3068\u300c\u3082\u306e\u3092\u300d\u3068\u89e3\u6790\u3055\u308c\u305f\u5834\u5408\u306f\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u306b\u306a\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\u59cb\u3081\u308b  \u3067      \u3053\u3053\u3067\n\u898b\u308b    \u306f \u3092   \u543e\u8f29\u306f \u3082\u306e\u3092\n\n\ndef sorted_double_list(key_list: list, value_list: list) -> tuple:\n    \"\"\"\n    2\u3064\u306e\u30ea\u30b9\u30c8\u3092\u5f15\u6570\u306b\u53d6\u308a\u3001\u4e00\u65b9\u306e\u30ea\u30b9\u30c8\u3092\u30ad\u30fc\u3001\u3082\u3046\u4e00\u65b9\u306e\u30ea\u30b9\u30c8\u3092\u5024\u3068\u3057\u3066dict\u5316\u3057\u30ad\u30fc\u3067\u30bd\u30fc\u30c8\u3057\u3066\u304b\u3089\u3001\u5143\u901a\u308a\u306b2\u3064\u306e\u30ea\u30b9\u30c8\u306b\u5206\u89e3\u3057\u3066\u30bf\u30d7\u30eb\u3068\u3057\u3066\u8fd4\u3059.\n    :param key_list \u30bd\u30fc\u30c8\u3059\u308b\u3068\u304d\u306e\u30ad\u30fc\u3068\u306a\u308b\u30ea\u30b9\u30c8\n    :param value_list key\u306b\u5f93\u3063\u3066\u30bd\u30fc\u30c8\u3055\u308c\u308b\u30ea\u30b9\u30c8\n    :return key_list\u3067\u30bd\u30fc\u30c8\u6e08\u307f\u306e2\u3064\u306e\u30ea\u30b9\u30c8\u306e\u30bf\u30d7\u30eb\n    \"\"\"\n    double_list = list(zip(key_list, value_list))\n    double_list = dict(double_list)\n    double_list = sorted(double_list.items())\n    return [pair[0] for pair in double_list], [pair[1] for pair in double_list]\n\n\ndef case_frame_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _case_frame_patterns = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            clauses = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n\n            if len(particles) > 0:\n                _case_frame_patterns.append([_chunk.first_verb().base, *sorted_double_list(particles, clauses)])\n\n    return _case_frame_patterns\n\n\ndef save_case_frame_patterns(_case_frame_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _case_frame_patterns \u683c\u30d5\u30ec\u30fc\u30e0(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for case in _case_frame_patterns:\n            output_file.write('{}\\t{}\\t{}\\n'.format(case[0], ' '.join(case[1]), ' '.join(case[2])))\n\n\nsave_case_frame_patterns(case_frame_patterns(chunked_sentences), 'case_frame_patterns.txt')\n\n\n47. \u6a5f\u80fd\u52d5\u8a5e\u69cb\u6587\u306e\u30de\u30a4\u30cb\u30f3\u30b0\n\n\u52d5\u8a5e\u306e\u30f2\u683c\u306b\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e\u304c\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306e\u307f\u306b\u7740\u76ee\u3057\u305f\u3044\uff0e46\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u6539\u5909\u305b\u3088\uff0e\n\n\u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092\uff08\u52a9\u8a5e\uff09\u300d\u3067\u69cb\u6210\u3055\u308c\u308b\u6587\u7bc0\u304c\u52d5\u8a5e\u306b\u4fc2\u308b\u5834\u5408\u306e\u307f\u3092\u5bfe\u8c61\u3068\u3059\u308b\n\u8ff0\u8a9e\u306f\u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\u306e\u57fa\u672c\u5f62\u300d\u3068\u3057\uff0c\u6587\u7bc0\u4e2d\u306b\u8907\u6570\u306e\u52d5\u8a5e\u304c\u3042\u308b\u3068\u304d\u306f\uff0c\u6700\u5de6\u306e\u52d5\u8a5e\u3092\u7528\u3044\u308b\n\u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\uff08\u6587\u7bc0\uff09\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u3059\u3079\u3066\u306e\u52a9\u8a5e\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u8f9e\u66f8\u9806\u306b\u4e26\u3079\u308b\n\u8ff0\u8a9e\u306b\u4fc2\u308b\u6587\u7bc0\u304c\u8907\u6570\u3042\u308b\u5834\u5408\u306f\uff0c\u3059\u3079\u3066\u306e\u9805\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u4e26\u3079\u308b\uff08\u52a9\u8a5e\u306e\u4e26\u3073\u9806\u3068\u63c3\u3048\u3088\uff09\n\n\u4f8b\u3048\u3070\u300c\u5225\u6bb5\u304f\u308b\u306b\u3082\u53ca\u3070\u3093\u3055\u3068\u3001\u4e3b\u4eba\u306f\u624b\u7d19\u306b\u8fd4\u4e8b\u3092\u3059\u308b\u3002\u300d\u3068\u3044\u3046\u6587\u304b\u3089\uff0c\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\u8fd4\u4e8b\u3092\u3059\u308b      \u3068 \u306b \u306f        \u53ca\u3070\u3093\u3055\u3068 \u624b\u7d19\u306b \u4e3b\u4eba\u306f\n\n\u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\uff0c\u4ee5\u4e0b\u306e\u4e8b\u9805\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\u305b\u3088\uff0e\n\n\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\uff08\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\uff09\n\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u52a9\u8a5e\u30d1\u30bf\u30fc\u30f3\n\n\ndef sahen_case_frame_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _sahen_case_frame_patterns = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            sahen_connection_noun = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and c.has_sahen_connection_noun_plus_wo()]\n            clauses = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and not c.has_sahen_connection_noun_plus_wo() and c.has_particle()]\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and not c.has_sahen_connection_noun_plus_wo() and c.has_particle()]\n\n            if len(sahen_connection_noun) > 0 and len(particles) > 0:\n                _sahen_case_frame_patterns.append([sahen_connection_noun[0] + _chunk.first_verb().base, *sorted_double_list(particles, clauses)])\n\n    return _sahen_case_frame_patterns\n\n\ndef save_sahen_case_frame_patterns(_sahen_case_frame_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _sahen_case_frame_patterns \u683c\u30d5\u30ec\u30fc\u30e0(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for case in _sahen_case_frame_patterns:\n            output_file.write('{}\\t{}\\t{}\\n'.format(case[0], ' '.join(case[1]), ' '.join(case[2])))\n\n\nsave_sahen_case_frame_patterns(sahen_case_frame_patterns(chunked_sentences), 'sahen_case_frame_patterns.txt')\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\uff08\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\uff09\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\nprint(subprocess.run('cat sahen_case_frame_patterns.txt | cut -f 1 | sort | uniq -c | sort -r | head -10', shell=True))\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u52a9\u8a5e\u30d1\u30bf\u30fc\u30f3\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\nprint(subprocess.run('cat sahen_case_frame_patterns.txt | cut -f 1,2 | sort | uniq -c | sort -r | head -10', shell=True))\n\n\n48. \u540d\u8a5e\u304b\u3089\u6839\u3078\u306e\u30d1\u30b9\u306e\u62bd\u51fa\n\n\u6587\u4e2d\u306e\u3059\u3079\u3066\u306e\u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u5bfe\u3057\uff0c\u305d\u306e\u6587\u7bc0\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u30d1\u30b9\u3092\u62bd\u51fa\u305b\u3088\uff0e \u305f\u3060\u3057\uff0c\u69cb\u6587\u6728\u4e0a\u306e\u30d1\u30b9\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3082\u306e\u3068\u3059\u308b\uff0e\n\n\u5404\u6587\u7bc0\u306f\uff08\u8868\u5c64\u5f62\u306e\uff09\u5f62\u614b\u7d20\u5217\u3067\u8868\u73fe\u3059\u308b\n\u30d1\u30b9\u306e\u958b\u59cb\u6587\u7bc0\u304b\u3089\u7d42\u4e86\u6587\u7bc0\u306b\u81f3\u308b\u307e\u3067\uff0c\u5404\u6587\u7bc0\u306e\u8868\u73fe\u3092\"->\"\u3067\u9023\u7d50\u3059\u308b\n\n\u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u304b\u3089\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\u543e\u8f29\u306f -> \u898b\u305f\n\u3053\u3053\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 -> \u898b\u305f\n\u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 -> \u898b\u305f\n\u3082\u306e\u3092 -> \u898b\u305f\n\n\n\u518d\u5e30\u7684\u306b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3068\u30b9\u30c3\u30ad\u30ea\u66f8\u3051\u307e\u3059\u306d\u3002\ndef path_to_root(_chunk: Chunk, _sentence: list) -> list:\n    \"\"\"\n    \u5f15\u6570\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u305f\u6587\u7bc0(`_chunk`)\u304croot\u306e\u5834\u5408\u306f\u3001\u305d\u306e\u6587\u7bc0\u3092\u8fd4\u3057\u307e\u3059.\n    \u5f15\u6570\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u305f\u6587\u7bc0(`_chunk`)\u304croot\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u6587\u7bc0\u3068\u305d\u306e\u6587\u7bc0\u304c\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u304b\u3089root\u307e\u3067\u306e\u30d1\u30b9\u3092list\u3068\u3057\u3066\u8fd4\u3057\u307e\u3059.\n    :param _chunk root\u3078\u306e\u8d77\u70b9\u3068\u306a\u308b\u6587\u7bc0\n    :param _sentence \u5206\u6790\u5bfe\u8c61\u306e\u6587\u7ae0\n    :return list _chunk\u304b\u3089root\u307e\u3067\u306e\u30d1\u30b9\n    \"\"\"\n    if _chunk.dst == -1:\n        return [_chunk]\n    else:\n        return [_chunk] + path_to_root(_sentence[_chunk.dst], _sentence)\n\n\ndef join_chunks_by_arrow(_chunks: list) -> str:\n    return ' -> '.join([c.join_morphs() for c in _chunks])\n\n\n# \u6700\u521d10\u6587\u3060\u3051\u51fa\u529b\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\nfor sentence in chunked_sentences[0:10]:\n    for chunk in sentence:\n        if chunk.has_noun():\n            print(join_chunks_by_arrow(path_to_root(chunk, sentence)))\n\n\n49. \u540d\u8a5e\u9593\u306e\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u62bd\u51fa\n\n\u6587\u4e2d\u306e\u3059\u3079\u3066\u306e\u540d\u8a5e\u53e5\u306e\u30da\u30a2\u3092\u7d50\u3076\u6700\u77ed\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u3092\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u540d\u8a5e\u53e5\u30da\u30a2\u306e\u6587\u7bc0\u756a\u53f7\u304ci\u3068j\uff08i<j\uff09\u306e\u3068\u304d\uff0c\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3082\u306e\u3068\u3059\u308b\uff0e\n\n\u554f\u984c48\u3068\u540c\u69d8\u306b\uff0c\u30d1\u30b9\u306f\u958b\u59cb\u6587\u7bc0\u304b\u3089\u7d42\u4e86\u6587\u7bc0\u306b\u81f3\u308b\u307e\u3067\u306e\u5404\u6587\u7bc0\u306e\u8868\u73fe\uff08\u8868\u5c64\u5f62\u306e\u5f62\u614b\u7d20\u5217\uff09\u3092\"->\"\u3067\u9023\u7d50\u3057\u3066\u8868\u73fe\u3059\u308b\n\u6587\u7bc0i\u3068j\u306b\u542b\u307e\u308c\u308b\u540d\u8a5e\u53e5\u306f\u305d\u308c\u305e\u308c\uff0cX\u3068Y\u306b\u7f6e\u63db\u3059\u308b\n\n\u307e\u305f\uff0c\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u5f62\u72b6\u306f\uff0c\u4ee5\u4e0b\u306e2\u901a\u308a\u304c\u8003\u3048\u3089\u308c\u308b\uff0e\n\n\u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u6587\u7bc0j\u304c\u5b58\u5728\u3059\u308b\u5834\u5408: \u6587\u7bc0i\u304b\u3089\u6587\u7bc0j\u306e\u30d1\u30b9\u3092\u8868\u793a\n\u4e0a\u8a18\u4ee5\u5916\u3067\uff0c\u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408: \u6587\u7bc0i\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u306e\u30d1\u30b9\u3068\u6587\u7bc0j\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u307e\u3067\u306e\u30d1\u30b9\uff0c\u6587\u7bc0k\u306e\u5185\u5bb9\u3092\"|\"\u3067\u9023\u7d50\u3057\u3066\u8868\u793a\n\n\u4f8b\u3048\u3070\uff0c\u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u3002\u300d\u3068\u3044\u3046\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u304b\u3089\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\nX\u306f | Y\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 | \u898b\u305f\nX\u306f | Y\u3068\u3044\u3046 -> \u3082\u306e\u3092 | \u898b\u305f\nX\u306f | Y\u3092 | \u898b\u305f\nX\u3067 -> \u59cb\u3081\u3066 -> Y\nX\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> Y\nX\u3068\u3044\u3046 -> Y\n\n\n\u554f\u984c\u6587\u3092\u8aad\u3093\u3067\u3082\u4f55\u304c\u3084\u308a\u305f\u3044\u306e\u304b\u5168\u304f\u5206\u304b\u3089\u306a\u304b\u3063\u305f\u3051\u3069\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u5f8c\u534a)\u3084\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015\u5e74\u7248 (46\uff5e49)\u3092\u8aad\u3093\u3067\u7406\u89e3\u3057\u305f\u3068\u3053\u308d\u306b\u3088\u308b\u3068\u3001\u3053\u3046\u3044\u3046\u3053\u3068\u3089\u3057\u3044.\n\u5206\u304b\u3089\u306a\u3044\u306a\u308a\u306b\u3001\u554f\u984c\u3092\u5206\u89e3\u3057\u306a\u304c\u3089\u3068\u308a\u3042\u3048\u305a\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u59cb\u3081\u3066\u307f\u308b\u3068\u5f90\u3005\u306b\u5206\u304b\u3063\u3066\u304f\u308b\u3082\u3093\u3067\u3059\u306d. \u3069\u306e\u3088\u3046\u306b\u554f\u984c\u3092\u5206\u89e3\u3057\u305f\u304b\u306b\u3064\u3044\u3066\u306f\u3001\u4e0b\u8a18\u30b3\u30fc\u30c9\u306b\u5c11\u3057\u30b3\u30e1\u30f3\u30c8\u3092\u591a\u3081\u306b\u66f8\u3044\u3066\u8aac\u660e\u3057\u3066\u3044\u308b\u306e\u3067\u53c2\u8003\u306b\u306a\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\ndef noun_pairs(_sentence: list):\n    \"\"\"\n    \u5f15\u6570\u3068\u3057\u3066\u6e21\u3055\u308c\u305f\u6587\u7ae0\u304c\u6301\u3064\u5168\u3066\u306e\u540d\u8a5e\u7bc0\u304b\u3089\u4f5c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u5168\u3066\u306e\u30da\u30a2\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059.\n    \"\"\"\n    from itertools import combinations\n    _noun_chunks = [_chunk for _chunk in _sentence if _chunk.has_noun()]\n    return list(combinations(_noun_chunks, 2))\n\n\ndef common_chunk(path_i: list, path_j: list) -> Chunk:\n    \"\"\"\n    \u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408\u3001\u6587\u7bc0k\u3092\u8fd4\u3059.\n    \"\"\"\n    _chunk_k = None\n    path_i = list(reversed(path_i))\n    path_j = list(reversed(path_j))\n    for idx, (c_i, c_j) in enumerate(zip(path_i, path_j)):\n        if c_i.srcs != c_j.srcs:\n            _chunk_k = path_i[idx - 1]\n            break\n\n    return _chunk_k\n\n\nfor sentence in chunked_sentences:\n    # \u540d\u8a5e\u53e5\u30da\u30a2\u306e\u30ea\u30b9\u30c8\n    n_pairs = noun_pairs(sentence)\n    if len(n_pairs) == 0:\n        continue\n\n    for n_pair in n_pairs:\n        chunk_i, chunk_j = n_pair\n\n        # \u6587\u7bc0i\u3068j\u306b\u542b\u307e\u308c\u308b\u540d\u8a5e\u53e5\u306f\u305d\u308c\u305e\u308c\uff0cX\u3068Y\u306b\u7f6e\u63db\u3059\u308b\n        chunk_i.replace_noun('X')\n        chunk_j.replace_noun('Y')\n\n        # \u6587\u7bc0i\u3068j\u304b\u3089root\u3078\u306e\u30d1\u30b9(Chunk\u578b\u306elist)\n        path_chunk_i_to_root = path_to_root(chunk_i, sentence)\n        path_chunk_j_to_root = path_to_root(chunk_j, sentence)\n\n        if chunk_j in path_chunk_i_to_root:\n            # \u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u6587\u7bc0j\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\n\n            # \u6587\u7bc0j\u306e\u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_j = path_chunk_i_to_root.index(chunk_j)\n\n            # \u6587\u7bc0i\u304b\u3089\u6587\u7bc0j\u306e\u30d1\u30b9\u3092\u8868\u793a\n            print(join_chunks_by_arrow(path_chunk_i_to_root[0: idx_j + 1]))\n        else:\n            # \u4e0a\u8a18\u4ee5\u5916\u3067\uff0c\u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408\n\n            # \u6587\u7bc0k\u3092\u53d6\u5f97\n            chunk_k = common_chunk(path_chunk_i_to_root, path_chunk_j_to_root)\n            if chunk_k is None:\n                continue\n\n            # \u6587\u7bc0k\u306e\u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_k_i = path_chunk_i_to_root.index(chunk_k)\n\n            # \u6587\u7bc0k\u306e\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_k_j = path_chunk_j_to_root.index(chunk_k)\n\n            # \u6587\u7bc0i\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u306e\u30d1\u30b9\u3068\u6587\u7bc0j\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u307e\u3067\u306e\u30d1\u30b9\uff0c\u6587\u7bc0k\u306e\u5185\u5bb9\u3092\"|\"\u3067\u9023\u7d50\u3057\u3066\u8868\u793a\n            print(' | '.join([join_chunks_by_arrow(path_chunk_i_to_root[0: idx_k_i]),\n                              join_chunks_by_arrow(path_chunk_j_to_root[0: idx_k_j]),\n                              chunk_k.join_morphs()]))\n\n[100\u672c\u30ce\u30c3\u30af](http://www.cl.ecei.tohoku.ac.jp/nlp100/)\u306e\u300c\u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790 (40\u301c49)\u300d\u3092\u89e3\u3044\u3066\u307f\u307e\u3057\u305f\u3002[\u7b2c4\u7ae0 \u5f62\u614b\u7d20\u89e3\u6790 (30\u301c39)](http://qiita.com/mas178/items/eb33cf9a3bb79102ff5c)\u306e\u7d9a\u304d\u3067\u3059\u3002\n\n### \u74b0\u5883\n\n- OS X El Capitan Version 10.11.4 \n- Python 3.5.1\n\n### \u53c2\u8003\u306b\u3057\u305f\u30da\u30fc\u30b8\n\nCaboCha\u3001pydot\u3001Graphviz\u306e\u5c0e\u5165\u3084\u3001\u300c49. \u540d\u8a5e\u9593\u306e\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u62bd\u51fa\u300d\u3067\u7d50\u69cb\u30cf\u30de\u308a\u307e\u3057\u305f\u304c\u3001\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u9802\u304d\u306a\u3093\u3068\u304b\u89e3\u6c7a\u3057\u307e\u3057\u305f\u3002\n\n- [\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u524d\u534a)](http://qiita.com/tdrk/items/f80c5d957de47f068ec1)\n- [\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u5f8c\u534a)](http://qiita.com/tdrk/items/e6d3847f4284304ca9b8)\n- [\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015\u5e74\u7248 (46\uff5e49)](http://kenichia.hatenablog.com/entry/2016/02/11/221513)\n- [CaboCha\u516c\u5f0f\u30b5\u30a4\u30c8](https://taku910.github.io/cabocha/)\n- [CaboCha\u3067\u59cb\u3081\u308b\u4fc2\u308a\u53d7\u3051\u89e3\u6790](http://qiita.com/nezuq/items/f481f07fc0576b38e81d)\n- [Graphviz\u3068dot\u8a00\u8a9e\u3067\u30b0\u30e9\u30d5\u3092\u63cf\u304f\u65b9\u6cd5\u306e\u307e\u3068\u3081](http://qiita.com/rubytomato@github/items/51779135bc4b77c8c20d)\n- [graphviz\u3092\u4f7f\u3063\u3066Python3\u3067\u6728\u69cb\u9020\u3092\u63cf\u304f](http://qiita.com/shimo_t/items/b761973805f2cf0b2967)\n- [AttributeError: module 'pydot' has no attribute 'graph_from_dot_data' in spyder]\n  (http://stackoverflow.com/questions/35285142/attributeerror-module-pydot-has-no-attribute-graph-from-dot-data-in-spyder)\n\n\n# \u6e96\u5099\n\n### \u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\n\n```py3\nimport CaboCha\nimport pydotplus\nimport subprocess\n```\n\n### \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u6587\u7ae0\u306e\u4fdd\u5b58\n\n> \u590f\u76ee\u6f31\u77f3\u306e\u5c0f\u8aac\u300e\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u300f\u306e\u6587\u7ae0\uff08neko.txt\uff09\u3092CaboCha\u3092\u4f7f\u3063\u3066\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u3057\uff0c\u305d\u306e\u7d50\u679c\u3092neko.txt.cabocha\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u305b\u3088\uff0e\n> \u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u3044\u3066\uff0c\u4ee5\u4e0b\u306e\u554f\u306b\u5bfe\u5fdc\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u88c5\u305b\u3088\uff0e\n\n```py3\ndef make_analyzed_file(input_file_name: str, output_file_name: str) -> None:\n    \"\"\"\n    \u30d7\u30ec\u30fc\u30f3\u306a\u65e5\u672c\u8a9e\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b.\n    (\u7a7a\u767d\u306f\u524a\u9664\u3057\u307e\u3059.)\n    :param input_file_name \u30d7\u30ec\u30fc\u30f3\u306a\u65e5\u672c\u8a9e\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :param output_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    c = CaboCha.Parser()\n    with open(input_file_name, encoding='utf-8') as input_file:\n        with open(output_file_name, mode='w', encoding='utf-8') as output_file:\n            for line in input_file:\n                tree = c.parse(line.lstrip())\n                output_file.write(tree.toString(CaboCha.FORMAT_LATTICE))\n\n\nmake_analyzed_file('neko.txt', 'neko.txt.cabocha')\n```\n\n# 40. \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\uff08\u5f62\u614b\u7d20\uff09\n\n> \u5f62\u614b\u7d20\u3092\u8868\u3059\u30af\u30e9\u30b9Morph\u3092\u5b9f\u88c5\u305b\u3088\uff0e\u3053\u306e\u30af\u30e9\u30b9\u306f\u8868\u5c64\u5f62\uff08surface\uff09\uff0c\u57fa\u672c\u5f62\uff08base\uff09\uff0c\u54c1\u8a5e\uff08pos\uff09\uff0c\u54c1\u8a5e\u7d30\u5206\u985e1\uff08pos1\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\u3053\u3068\u3068\u3059\u308b\uff0e\n> \u3055\u3089\u306b\uff0cCaboCha\u306e\u89e3\u6790\u7d50\u679c\uff08neko.txt.cabocha\uff09\u3092\u8aad\u307f\u8fbc\u307f\uff0c\u5404\u6587\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\uff0c3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\u305b\u3088\uff0e\n\n```py3\nclass Morph:\n    \"\"\"\n    1\u3064\u306e\u5f62\u614b\u7d20\u3092\u8868\u3059\u30af\u30e9\u30b9\n    \"\"\"\n\n    def __init__(self, surface, base, pos, pos1):\n        \"\"\"\n        \u30e1\u30f3\u30d0\u5909\u6570\u3068\u3057\u3066\u8868\u5c64\u5f62\uff08surface\uff09\uff0c\u57fa\u672c\u5f62\uff08base\uff09\uff0c\u54c1\u8a5e\uff08pos\uff09\uff0c\u54c1\u8a5e\u7d30\u5206\u985e1\uff08pos1\uff09\u3092\u6301\u3064.\n        \"\"\"\n        self.surface = surface\n        self.base = base\n        self.pos = pos\n        self.pos1 = pos1\n\n    def is_end_of_sentence(self) -> bool: return self.pos1 == '\u53e5\u70b9'\n\n    def __str__(self) -> str: return 'surface: {}, base: {}, pos: {}, pos1: {}'.format(self.surface, self.base, self.pos, self.pos1)\n\n\ndef make_morph_list(analyzed_file_name: str) -> list:\n    \"\"\"\n    \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u5404\u6587\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\n    :param analyzed_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :return list \u4e00\u3064\u306e\u6587\u7ae0\u3092Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    sentences = []\n    sentence = []\n    with open(analyzed_file_name, encoding='utf-8') as input_file:\n        for line in input_file:\n            line_list = line.split()\n            if (line_list[0] == '*') | (line_list[0] == 'EOS'):\n                pass\n            else:\n                line_list = line_list[0].split(',') + line_list[1].split(',')\n                # \u3053\u306e\u6642\u70b9\u3067line_list\u306f\u3053\u3093\u306a\u611f\u3058\n                # ['\u59cb\u3081', '\u540d\u8a5e', '\u526f\u8a5e\u53ef\u80fd', '*', '*', '*', '*', '\u59cb\u3081', '\u30cf\u30b8\u30e1', '\u30cf\u30b8\u30e1']\n                _morph = Morph(surface=line_list[0], base=line_list[7], pos=line_list[1], pos1=line_list[2])\n\n                sentence.append(_morph)\n\n                if _morph.is_end_of_sentence():\n                    sentences.append(sentence)\n                    sentence = []\n\n    return sentences\n\n\nmorphed_sentences = make_morph_list('neko.txt.cabocha')\n\n# 3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\nfor morph in morphed_sentences[2]:\n    print(str(morph))\n```\n\n# 41. \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\uff08\u6587\u7bc0\u30fb\u4fc2\u308a\u53d7\u3051\uff09\n\n> 40\u306b\u52a0\u3048\u3066\uff0c\u6587\u7bc0\u3092\u8868\u3059\u30af\u30e9\u30b9Chunk\u3092\u5b9f\u88c5\u305b\u3088\uff0e\n> \u3053\u306e\u30af\u30e9\u30b9\u306f\u5f62\u614b\u7d20\uff08Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09\u306e\u30ea\u30b9\u30c8\uff08morphs\uff09\uff0c\u4fc2\u308a\u5148\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\uff08dst\uff09\uff0c\u4fc2\u308a\u5143\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u306e\u30ea\u30b9\u30c8\uff08srcs\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\u3053\u3068\u3068\u3059\u308b\uff0e\n> \u3055\u3089\u306b\uff0c\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u306eCaboCha\u306e\u89e3\u6790\u7d50\u679c\u3092\u8aad\u307f\u8fbc\u307f\uff0c\uff11\u6587\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\uff0c8\u6587\u76ee\u306e\u6587\u7bc0\u306e\u6587\u5b57\u5217\u3068\u4fc2\u308a\u5148\u3092\u8868\u793a\u305b\u3088\uff0e\n> \u7b2c5\u7ae0\u306e\u6b8b\u308a\u306e\u554f\u984c\u3067\u306f\uff0c\u3053\u3053\u3067\u4f5c\u3063\u305f\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6d3b\u7528\u305b\u3088\uff0e\n\nChunk\u30af\u30e9\u30b9\u306b\u305f\u304f\u3055\u3093\u30e1\u30bd\u30c3\u30c9\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u5fc5\u8981\u306a\u306e\u306f`__init__`\u3068`__str__`\u306e\u307f\u3067\u3059\u3002\u305d\u306e\u4ed6\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u3001\u5f8c\u7d9a\u306e\u554f\u3092\u89e3\u3044\u3066\u3044\u304f\u306b\u3042\u305f\u3063\u3066\u90fd\u5ea6\u8ffd\u52a0\u3057\u3066\u3044\u3063\u305f\u3082\u306e\u3067\u3059\u3002\n\n```py3\nclass Chunk:\n    def __init__(self, morphs: list, dst: str, srcs: str) -> None:\n        \"\"\"\n        \u5f62\u614b\u7d20\uff08Morph\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09\u306e\u30ea\u30b9\u30c8\uff08morphs\uff09\uff0c\u4fc2\u308a\u5148\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\uff08dst\uff09\uff0c\u4fc2\u308a\u5143\u6587\u7bc0\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u306e\u30ea\u30b9\u30c8\uff08srcs\uff09\u3092\u30e1\u30f3\u30d0\u5909\u6570\u306b\u6301\u3064\n        \"\"\"\n        self.morphs = morphs\n        self.dst = int(dst.strip(\"D\"))\n        self.srcs = int(srcs)\n\n    # \u4ee5\u4e0b\u306f\u5f8c\u3005\u4f7f\u3046\u30e1\u30bd\u30c3\u30c9\u3067\u3059.\n    def join_morphs(self) -> str:\n        return ''.join([_morph.surface for _morph in self.morphs if _morph.pos != '\u8a18\u53f7'])\n\n    def has_noun(self) -> bool:\n        return any([_morph.pos == '\u540d\u8a5e' for _morph in self.morphs])\n\n    def has_verb(self) -> bool:\n        return any([_morph.pos == '\u52d5\u8a5e' for _morph in self.morphs])\n\n    def has_particle(self) -> bool:\n        return any([_morph.pos == '\u52a9\u8a5e' for _morph in self.morphs])\n\n    def has_sahen_connection_noun_plus_wo(self) -> bool:\n        \"\"\"\n        \u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092\uff08\u52a9\u8a5e\uff09\u300d\u3092\u542b\u3080\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059.\n        \"\"\"\n        for idx, _morph in enumerate(self.morphs):\n            if _morph.pos == '\u540d\u8a5e' and _morph.pos1 == '\u30b5\u5909\u63a5\u7d9a' and len(self.morphs[idx:]) > 1 and \\\n                            self.morphs[idx + 1].pos == '\u52a9\u8a5e' and self.morphs[idx + 1].base == '\u3092':\n                return True\n\n        return False\n\n    def first_verb(self) -> Morph:\n        return [_morph for _morph in self.morphs if _morph.pos == '\u52d5\u8a5e'][0]\n\n    def last_particle(self) -> list:\n        return [_morph for _morph in self.morphs if _morph.pos == '\u52a9\u8a5e'][-1]\n\n    def pair(self, sentence: list) -> str:\n        return self.join_morphs() + '\\t' + sentence[self.dst].join_morphs()\n\n    def replace_noun(self, alt: str) -> None:\n        \"\"\"\n        \u540d\u8a5e\u306e\u8868\u8c61\u3092\u7f6e\u63db\u3059\u308b.\n        \"\"\"\n        for _morph in self.morphs:\n            if _morph.pos == '\u540d\u8a5e':\n                _morph.surface = alt\n\n    def __str__(self) -> str:\n        return 'srcs: {}, dst: {}, morphs: ({})'.format(self.srcs, self.dst, ' / '.join([str(_morph) for _morph in self.morphs]))\n\n\ndef make_chunk_list(analyzed_file_name: str) -> list:\n    \"\"\"\n    \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u5404\u6587\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\n    :param analyzed_file_name \u4fc2\u308a\u53d7\u3051\u89e3\u6790\u6e08\u307f\u306e\u6587\u7ae0\u30d5\u30a1\u30a4\u30eb\u540d\n    :return list \u4e00\u3064\u306e\u6587\u7ae0\u3092Chunk\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    sentences = []\n    sentence = []\n    _chunk = None\n    with open(analyzed_file_name, encoding='utf-8') as input_file:\n        for line in input_file:\n            line_list = line.split()\n            if line_list[0] == '*':\n                if _chunk is not None:\n                    sentence.append(_chunk)\n                _chunk = Chunk(morphs=[], dst=line_list[2], srcs=line_list[1])\n            elif line_list[0] == 'EOS':  # End of sentence\n                if _chunk is not None:\n                    sentence.append(_chunk)\n                if len(sentence) > 0:\n                    sentences.append(sentence)\n                _chunk = None\n                sentence = []\n            else:\n                line_list = line_list[0].split(',') + line_list[1].split(',')\n                # \u3053\u306e\u6642\u70b9\u3067line_list\u306f\u3053\u3093\u306a\u611f\u3058\n                # ['\u59cb\u3081', '\u540d\u8a5e', '\u526f\u8a5e\u53ef\u80fd', '*', '*', '*', '*', '\u59cb\u3081', '\u30cf\u30b8\u30e1', '\u30cf\u30b8\u30e1']\n                _morph = Morph(surface=line_list[0], base=line_list[7], pos=line_list[1], pos1=line_list[2])\n                _chunk.morphs.append(_morph)\n\n    return sentences\n\n\nchunked_sentences = make_chunk_list('neko.txt.cabocha')\n\n# 3\u6587\u76ee\u306e\u5f62\u614b\u7d20\u5217\u3092\u8868\u793a\nfor chunk in chunked_sentences[2]:\n    print(str(chunk))\n```\n\n# 42. \u4fc2\u308a\u5143\u3068\u4fc2\u308a\u5148\u306e\u6587\u7bc0\u306e\u8868\u793a\n> \u4fc2\u308a\u5143\u306e\u6587\u7bc0\u3068\u4fc2\u308a\u5148\u306e\u6587\u7bc0\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u3059\u3079\u3066\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u53e5\u8aad\u70b9\u306a\u3069\u306e\u8a18\u53f7\u306f\u51fa\u529b\u3057\u306a\u3044\u3088\u3046\u306b\u305b\u3088\uff0e\n\n44\u3067\u4f7f\u3044\u3084\u3059\u3044\u3088\u3046\u306b\u6587\u7ae0\u3054\u3068\u306b\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002\n\n```py3\ndef is_valid_chunk(_chunk, sentence):\n    return _chunk.join_morphs() != '' and _chunk.dst > -1 and sentence[_chunk.dst].join_morphs() != ''\n\n\npaired_sentences = [[chunk.pair(sentence) for chunk in sentence if is_valid_chunk(chunk, sentence)] for sentence in chunked_sentences if len(sentence) > 1]\nprint(paired_sentences[0:100])\n```\n\n# 43. \u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u304c\u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u4fc2\u308b\u3082\u306e\u3092\u62bd\u51fa\n\n> \u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u304c\uff0c\u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u4fc2\u308b\u3068\u304d\uff0c\u3053\u308c\u3089\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u53e5\u8aad\u70b9\u306a\u3069\u306e\u8a18\u53f7\u306f\u51fa\u529b\u3057\u306a\u3044\u3088\u3046\u306b\u305b\u3088\uff0e\n\nChunk\u30af\u30e9\u30b9\u306b\u8272\u3005\u4fbf\u5229\u30e1\u30bd\u30c3\u30c9\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\u306e\u3067\u697d\u3067\u3059\u306d\u3002\n\n```py3\nfor sentence in chunked_sentences:\n    for chunk in sentence:\n        if chunk.has_noun() and chunk.dst > -1 and sentence[chunk.dst].has_verb():\n            print(chunk.pair(sentence))\n```\n\n# 44. \u4fc2\u308a\u53d7\u3051\u6728\u306e\u53ef\u8996\u5316\n> \u4e0e\u3048\u3089\u308c\u305f\u6587\u306e\u4fc2\u308a\u53d7\u3051\u6728\u3092\u6709\u5411\u30b0\u30e9\u30d5\u3068\u3057\u3066\u53ef\u8996\u5316\u305b\u3088\uff0e\n> \u53ef\u8996\u5316\u306b\u306f\uff0c\u4fc2\u308a\u53d7\u3051\u6728\u3092DOT\u8a00\u8a9e\u306b\u5909\u63db\u3057\uff0cGraphviz\u3092\u7528\u3044\u308b\u3068\u3088\u3044\uff0e\u307e\u305f\uff0cPython\u304b\u3089\u6709\u5411\u30b0\u30e9\u30d5\u3092\u76f4\u63a5\u7684\u306b\u53ef\u8996\u5316\u3059\u308b\u306b\u306f\uff0cpydot\u3092\u4f7f\u3046\u3068\u3088\u3044\uff0e\n\n```py3\ndef sentence_to_dot(idx: int, sentence: list) -> str:\n    head = \"digraph sentence{} \".format(idx)\n    body_head = \"{ graph [rankdir = LR]; \"\n    body_list = ['\"{}\"->\"{}\"; '.format(*chunk_pair.split()) for chunk_pair in sentence]\n\n    return head + body_head + ''.join(body_list) + '}'\n\n\ndef sentences_to_dots(sentences: list) -> list:\n    _dots = []\n    for idx, sentence in enumerate(sentences):\n        _dots.append(sentence_to_dot(idx, sentence))\n    return _dots\n\n\ndef save_graph(dot: str, file_name: str) -> None:\n    g = pydotplus.graph_from_dot_data(dot)\n    g.write_jpeg(file_name, prog='dot')\n\n\ndots = sentences_to_dots(paired_sentences)\nfor idx in range(101, 104):\n    save_graph(dots[idx], 'graph{}.jpg'.format(idx))\n```\n\n\n##### \u3010\u30b5\u30f3\u30d7\u30eb\u3011101\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n![graph101.jpg](https://qiita-image-store.s3.amazonaws.com/0/36429/b46a8496-e0a0-45a9-afa4-35fda2c25da1.jpeg)\n##### \u3010\u30b5\u30f3\u30d7\u30eb\u3011102\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n![graph102.jpg](https://qiita-image-store.s3.amazonaws.com/0/36429/5843d6c0-d290-80c7-0af0-355ccc475158.jpeg)\n##### \u3010\u30b5\u30f3\u30d7\u30eb\u3011103\u756a\u76ee\u306e\u6587\u7ae0\u306e\u4fc2\u308a\u53d7\u3051\u6728\n![graph103.jpg](https://qiita-image-store.s3.amazonaws.com/0/36429/6f2402e8-a207-2968-77fa-af4e606a0bb1.jpeg)\n\u203b\u3061\u306a\u307f\u306b\u300c\u986b\u3048\u308b\u300d\u306f\u300c\u3075\u308b\u3048\u308b\u300d\u3068\u8aad\u3080\u305d\u3046\u3067\u3059\u3002\n\n# 45. \u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u62bd\u51fa\n\n> \u4eca\u56de\u7528\u3044\u3066\u3044\u308b\u6587\u7ae0\u3092\u30b3\u30fc\u30d1\u30b9\u3068\u898b\u306a\u3057\uff0c\u65e5\u672c\u8a9e\u306e\u8ff0\u8a9e\u304c\u53d6\u308a\u3046\u308b\u683c\u3092\u8abf\u67fb\u3057\u305f\u3044\uff0e\n> \u52d5\u8a5e\u3092\u8ff0\u8a9e\uff0c\u52d5\u8a5e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u306e\u52a9\u8a5e\u3092\u683c\u3068\u8003\u3048\uff0c\u8ff0\u8a9e\u3068\u683c\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n> \u305f\u3060\u3057\uff0c\u51fa\u529b\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u305b\u3088\uff0e\n\n> - \u52d5\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u304a\u3044\u3066\uff0c\u6700\u5de6\u306e\u52d5\u8a5e\u306e\u57fa\u672c\u5f62\u3092\u8ff0\u8a9e\u3068\u3059\u308b\n> - \u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\u3092\u683c\u3068\u3059\u308b\n> - \u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\uff08\u6587\u7bc0\uff09\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u3059\u3079\u3066\u306e\u52a9\u8a5e\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u8f9e\u66f8\u9806\u306b\u4e26\u3079\u308b\n\n> \u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u4f8b\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u3092\u8003\u3048\u308b\uff0e\n> \u3053\u306e\u6587\u306f\u300c\u59cb\u3081\u308b\u300d\u3068\u300c\u898b\u308b\u300d\u306e\uff12\u3064\u306e\u52d5\u8a5e\u3092\u542b\u307f\uff0c\u300c\u59cb\u3081\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u3053\u3053\u3067\u300d\uff0c\u300c\u898b\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u543e\u8f29\u306f\u300d\u3068\u300c\u3082\u306e\u3092\u300d\u3068\u89e3\u6790\u3055\u308c\u305f\u5834\u5408\u306f\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u306b\u306a\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\n> ```\n> \u59cb\u3081\u308b  \u3067\n> \u898b\u308b    \u306f \u3092\n> ```\n\n> \u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\uff0c\u4ee5\u4e0b\u306e\u4e8b\u9805\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\u305b\u3088\uff0e\n\n> - \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b\n> - \u300c\u3059\u308b\u300d\u300c\u898b\u308b\u300d\u300c\u4e0e\u3048\u308b\u300d\u3068\u3044\u3046\u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\uff08\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e26\u3079\u3088\uff09\n\n```py3\ndef case_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.(\u300c\u683c\u300d\u306f\u82f1\u8a9e\u3067\"Case\"\u3068\u3044\u3046\u3089\u3057\u3044.)\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u4e0e\u3048\u308b', ['\u306b', '\u3092']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _case_pattern = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n\n            if len(particles) > 0:\n                _case_pattern.append([_chunk.first_verb().base, sorted(particles)])\n\n    return _case_pattern\n\n\ndef save_case_patterns(_case_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _case_patterns \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u4e0e\u3048\u308b', ['\u306b', '\u3092']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for _case in _case_patterns:\n            output_file.write('{}\\t{}\\n'.format(_case[0], ' '.join(_case[1])))\n\n\nsave_case_patterns(case_patterns(chunked_sentences), 'case_patterns.txt')\n\n\ndef print_case_pattern_ranking(_grep_str: str) -> None:\n    \"\"\"\n    \u30b3\u30fc\u30d1\u30b9(case_pattern.txt)\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e0a\u4f4d20\u4ef6\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u3092\u8868\u793a\u3059\u308b.\n    `cat case_patterns.txt | grep '^\u3059\u308b\\t' | sort | uniq -c | sort -r | head -20`\u306e\u3088\u3046\u306aUnix\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066print\u3057\u3066\u3044\u308b.\n    grep\u306e\u90e8\u5206\u306f\u5f15\u6570`_grep_str`\u306b\u5fdc\u3058\u3066\u4ed8\u52a0\u3055\u308c\u308b.\n    :param _grep_str \u691c\u7d22\u6761\u4ef6\u3068\u306a\u308b\u52d5\u8a5e\n    \"\"\"\n    _grep_str = '' if _grep_str == '' else '| grep \\'^{}\\t\\''.format(_grep_str)\n    print(subprocess.run('cat case_patterns.txt {} | sort | uniq -c | sort -r | head -10'.format(_grep_str), shell=True))\n\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b\uff08\u4e0a\u4f4d10\u4ef6\uff09\n# \u300c\u3059\u308b\u300d\u300c\u898b\u308b\u300d\u300c\u4e0e\u3048\u308b\u300d\u3068\u3044\u3046\u52d5\u8a5e\u306e\u683c\u30d1\u30bf\u30fc\u30f3\uff08\u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u51fa\u73fe\u983b\u5ea6\u306e\u9ad8\u3044\u9806\u306b\u4e0a\u4f4d10\u4ef6\uff09\nfor grep_str in ['', '\u3059\u308b', '\u898b\u308b', '\u4e0e\u3048\u308b']:\n    print_case_pattern_ranking(grep_str)\n```\n\n# 46. \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u60c5\u5831\u306e\u62bd\u51fa\n\n> 45\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6539\u5909\u3057\uff0c\u8ff0\u8a9e\u3068\u683c\u30d1\u30bf\u30fc\u30f3\u306b\u7d9a\u3051\u3066\u9805\uff08\u8ff0\u8a9e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u305d\u306e\u3082\u306e\uff09\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n> 45\u306e\u4ed5\u69d8\u306b\u52a0\u3048\u3066\uff0c\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u305b\u3088\uff0e\n\n> - \u9805\u306f\u8ff0\u8a9e\u306b\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u306e\u5358\u8a9e\u5217\u3068\u3059\u308b\uff08\u672b\u5c3e\u306e\u52a9\u8a5e\u3092\u53d6\u308a\u9664\u304f\u5fc5\u8981\u306f\u306a\u3044\uff09\n> - \u8ff0\u8a9e\u306b\u4fc2\u308b\u6587\u7bc0\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u52a9\u8a5e\u3068\u540c\u4e00\u306e\u57fa\u6e96\u30fb\u9806\u5e8f\u3067\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u4e26\u3079\u308b\n\n> \u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u4f8b\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u3092\u8003\u3048\u308b\uff0e\n> \u3053\u306e\u6587\u306f\u300c\u59cb\u3081\u308b\u300d\u3068\u300c\u898b\u308b\u300d\u306e\uff12\u3064\u306e\u52d5\u8a5e\u3092\u542b\u307f\uff0c\u300c\u59cb\u3081\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u3053\u3053\u3067\u300d\uff0c\u300c\u898b\u308b\u300d\u306b\u4fc2\u308b\u6587\u7bc0\u306f\u300c\u543e\u8f29\u306f\u300d\u3068\u300c\u3082\u306e\u3092\u300d\u3068\u89e3\u6790\u3055\u308c\u305f\u5834\u5408\u306f\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u306b\u306a\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\n> ```\n> \u59cb\u3081\u308b  \u3067      \u3053\u3053\u3067\n> \u898b\u308b    \u306f \u3092   \u543e\u8f29\u306f \u3082\u306e\u3092\n> ```\n\n```py3\ndef sorted_double_list(key_list: list, value_list: list) -> tuple:\n    \"\"\"\n    2\u3064\u306e\u30ea\u30b9\u30c8\u3092\u5f15\u6570\u306b\u53d6\u308a\u3001\u4e00\u65b9\u306e\u30ea\u30b9\u30c8\u3092\u30ad\u30fc\u3001\u3082\u3046\u4e00\u65b9\u306e\u30ea\u30b9\u30c8\u3092\u5024\u3068\u3057\u3066dict\u5316\u3057\u30ad\u30fc\u3067\u30bd\u30fc\u30c8\u3057\u3066\u304b\u3089\u3001\u5143\u901a\u308a\u306b2\u3064\u306e\u30ea\u30b9\u30c8\u306b\u5206\u89e3\u3057\u3066\u30bf\u30d7\u30eb\u3068\u3057\u3066\u8fd4\u3059.\n    :param key_list \u30bd\u30fc\u30c8\u3059\u308b\u3068\u304d\u306e\u30ad\u30fc\u3068\u306a\u308b\u30ea\u30b9\u30c8\n    :param value_list key\u306b\u5f93\u3063\u3066\u30bd\u30fc\u30c8\u3055\u308c\u308b\u30ea\u30b9\u30c8\n    :return key_list\u3067\u30bd\u30fc\u30c8\u6e08\u307f\u306e2\u3064\u306e\u30ea\u30b9\u30c8\u306e\u30bf\u30d7\u30eb\n    \"\"\"\n    double_list = list(zip(key_list, value_list))\n    double_list = dict(double_list)\n    double_list = sorted(double_list.items())\n    return [pair[0] for pair in double_list], [pair[1] for pair in double_list]\n\n\ndef case_frame_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _case_frame_patterns = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            clauses = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and c.has_particle()]\n\n            if len(particles) > 0:\n                _case_frame_patterns.append([_chunk.first_verb().base, *sorted_double_list(particles, clauses)])\n\n    return _case_frame_patterns\n\n\ndef save_case_frame_patterns(_case_frame_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _case_frame_patterns \u683c\u30d5\u30ec\u30fc\u30e0(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for case in _case_frame_patterns:\n            output_file.write('{}\\t{}\\t{}\\n'.format(case[0], ' '.join(case[1]), ' '.join(case[2])))\n\n\nsave_case_frame_patterns(case_frame_patterns(chunked_sentences), 'case_frame_patterns.txt')\n```\n\n# 47. \u6a5f\u80fd\u52d5\u8a5e\u69cb\u6587\u306e\u30de\u30a4\u30cb\u30f3\u30b0\n\n> \u52d5\u8a5e\u306e\u30f2\u683c\u306b\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e\u304c\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306e\u307f\u306b\u7740\u76ee\u3057\u305f\u3044\uff0e46\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3088\u3046\u306b\u6539\u5909\u305b\u3088\uff0e\n\n> - \u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092\uff08\u52a9\u8a5e\uff09\u300d\u3067\u69cb\u6210\u3055\u308c\u308b\u6587\u7bc0\u304c\u52d5\u8a5e\u306b\u4fc2\u308b\u5834\u5408\u306e\u307f\u3092\u5bfe\u8c61\u3068\u3059\u308b\n> - \u8ff0\u8a9e\u306f\u300c\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\u306e\u57fa\u672c\u5f62\u300d\u3068\u3057\uff0c\u6587\u7bc0\u4e2d\u306b\u8907\u6570\u306e\u52d5\u8a5e\u304c\u3042\u308b\u3068\u304d\u306f\uff0c\u6700\u5de6\u306e\u52d5\u8a5e\u3092\u7528\u3044\u308b\n> - \u8ff0\u8a9e\u306b\u4fc2\u308b\u52a9\u8a5e\uff08\u6587\u7bc0\uff09\u304c\u8907\u6570\u3042\u308b\u3068\u304d\u306f\uff0c\u3059\u3079\u3066\u306e\u52a9\u8a5e\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u8f9e\u66f8\u9806\u306b\u4e26\u3079\u308b\n> - \u8ff0\u8a9e\u306b\u4fc2\u308b\u6587\u7bc0\u304c\u8907\u6570\u3042\u308b\u5834\u5408\u306f\uff0c\u3059\u3079\u3066\u306e\u9805\u3092\u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u4e26\u3079\u308b\uff08\u52a9\u8a5e\u306e\u4e26\u3073\u9806\u3068\u63c3\u3048\u3088\uff09\n\n> \u4f8b\u3048\u3070\u300c\u5225\u6bb5\u304f\u308b\u306b\u3082\u53ca\u3070\u3093\u3055\u3068\u3001\u4e3b\u4eba\u306f\u624b\u7d19\u306b\u8fd4\u4e8b\u3092\u3059\u308b\u3002\u300d\u3068\u3044\u3046\u6587\u304b\u3089\uff0c\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\n> ```\n> \u8fd4\u4e8b\u3092\u3059\u308b      \u3068 \u306b \u306f        \u53ca\u3070\u3093\u3055\u3068 \u624b\u7d19\u306b \u4e3b\u4eba\u306f\n> ```\n\n> \u3053\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\uff0c\u4ee5\u4e0b\u306e\u4e8b\u9805\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\u305b\u3088\uff0e\n\n> - \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\uff08\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\uff09\n> - \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u52a9\u8a5e\u30d1\u30bf\u30fc\u30f3\n\n```py3\ndef sahen_case_frame_patterns(_chunked_sentences: list) -> list:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u30d5\u30ec\u30fc\u30e0\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u307e\u3059.\n    :param _chunked_sentences \u30c1\u30e3\u30f3\u30af\u5316\u3055\u308c\u305f\u5f62\u614b\u7d20\u3092\u6587\u7ae0\u3054\u3068\u306b\u30ea\u30b9\u30c8\u5316\u3057\u305f\u3082\u306e\u306e\u30ea\u30b9\u30c8\n    :return \u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    _sahen_case_frame_patterns = []\n    for sentence in _chunked_sentences:\n        for _chunk in sentence:\n            if not _chunk.has_verb():\n                continue\n\n            sahen_connection_noun = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and c.has_sahen_connection_noun_plus_wo()]\n            clauses = [c.join_morphs() for c in sentence if c.dst == _chunk.srcs and not c.has_sahen_connection_noun_plus_wo() and c.has_particle()]\n            particles = [c.last_particle().base for c in sentence if c.dst == _chunk.srcs and not c.has_sahen_connection_noun_plus_wo() and c.has_particle()]\n\n            if len(sahen_connection_noun) > 0 and len(particles) > 0:\n                _sahen_case_frame_patterns.append([sahen_connection_noun[0] + _chunk.first_verb().base, *sorted_double_list(particles, clauses)])\n\n    return _sahen_case_frame_patterns\n\n\ndef save_sahen_case_frame_patterns(_sahen_case_frame_patterns: list, file_name: str) -> None:\n    \"\"\"\n    \u52d5\u8a5e\u306e\u683c\u306e\u30d1\u30bf\u30fc\u30f3(\u52d5\u8a5e\u3068\u52a9\u8a5e\u306e\u7d44\u307f\u5408\u308f\u305b)\u306e\u30ea\u30b9\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u307e\u3059.\n    :param _sahen_case_frame_patterns \u683c\u30d5\u30ec\u30fc\u30e0(\u4f8b\u3048\u3070['\u3059\u308b', ['\u3066', '\u306f'], ['\u6ce3\u3044\u3066', '\u3044\u305f\u4e8b\u3060\u3051\u306f']])\u306e\u30ea\u30b9\u30c8\n    :param file_name \u4fdd\u5b58\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d\n    \"\"\"\n    with open(file_name, mode='w', encoding='utf-8') as output_file:\n        for case in _sahen_case_frame_patterns:\n            output_file.write('{}\\t{}\\t{}\\n'.format(case[0], ' '.join(case[1]), ' '.join(case[2])))\n\n\nsave_sahen_case_frame_patterns(sahen_case_frame_patterns(chunked_sentences), 'sahen_case_frame_patterns.txt')\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\uff08\u30b5\u5909\u63a5\u7d9a\u540d\u8a5e+\u3092+\u52d5\u8a5e\uff09\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\nprint(subprocess.run('cat sahen_case_frame_patterns.txt | cut -f 1 | sort | uniq -c | sort -r | head -10', shell=True))\n\n# \u30b3\u30fc\u30d1\u30b9\u4e2d\u3067\u983b\u51fa\u3059\u308b\u8ff0\u8a9e\u3068\u52a9\u8a5e\u30d1\u30bf\u30fc\u30f3\u3092UNIX\u30b3\u30de\u30f3\u30c9\u3092\u7528\u3044\u3066\u78ba\u8a8d\nprint(subprocess.run('cat sahen_case_frame_patterns.txt | cut -f 1,2 | sort | uniq -c | sort -r | head -10', shell=True))\n```\n\n# 48. \u540d\u8a5e\u304b\u3089\u6839\u3078\u306e\u30d1\u30b9\u306e\u62bd\u51fa\n\n> \u6587\u4e2d\u306e\u3059\u3079\u3066\u306e\u540d\u8a5e\u3092\u542b\u3080\u6587\u7bc0\u306b\u5bfe\u3057\uff0c\u305d\u306e\u6587\u7bc0\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u30d1\u30b9\u3092\u62bd\u51fa\u305b\u3088\uff0e \u305f\u3060\u3057\uff0c\u69cb\u6587\u6728\u4e0a\u306e\u30d1\u30b9\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3082\u306e\u3068\u3059\u308b\uff0e\n\n> - \u5404\u6587\u7bc0\u306f\uff08\u8868\u5c64\u5f62\u306e\uff09\u5f62\u614b\u7d20\u5217\u3067\u8868\u73fe\u3059\u308b\n> - \u30d1\u30b9\u306e\u958b\u59cb\u6587\u7bc0\u304b\u3089\u7d42\u4e86\u6587\u7bc0\u306b\u81f3\u308b\u307e\u3067\uff0c\u5404\u6587\u7bc0\u306e\u8868\u73fe\u3092\"->\"\u3067\u9023\u7d50\u3059\u308b\n\n> \u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u300d\u3068\u3044\u3046\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u304b\u3089\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\n> ```\n> \u543e\u8f29\u306f -> \u898b\u305f\n> \u3053\u3053\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 -> \u898b\u305f\n> \u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 -> \u898b\u305f\n> \u3082\u306e\u3092 -> \u898b\u305f\n> ```\n\n\u518d\u5e30\u7684\u306b\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\u3068\u30b9\u30c3\u30ad\u30ea\u66f8\u3051\u307e\u3059\u306d\u3002\n\n```py3\ndef path_to_root(_chunk: Chunk, _sentence: list) -> list:\n    \"\"\"\n    \u5f15\u6570\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u305f\u6587\u7bc0(`_chunk`)\u304croot\u306e\u5834\u5408\u306f\u3001\u305d\u306e\u6587\u7bc0\u3092\u8fd4\u3057\u307e\u3059.\n    \u5f15\u6570\u3068\u3057\u3066\u4e0e\u3048\u3089\u308c\u305f\u6587\u7bc0(`_chunk`)\u304croot\u3067\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u6587\u7bc0\u3068\u305d\u306e\u6587\u7bc0\u304c\u4fc2\u3063\u3066\u3044\u308b\u6587\u7bc0\u304b\u3089root\u307e\u3067\u306e\u30d1\u30b9\u3092list\u3068\u3057\u3066\u8fd4\u3057\u307e\u3059.\n    :param _chunk root\u3078\u306e\u8d77\u70b9\u3068\u306a\u308b\u6587\u7bc0\n    :param _sentence \u5206\u6790\u5bfe\u8c61\u306e\u6587\u7ae0\n    :return list _chunk\u304b\u3089root\u307e\u3067\u306e\u30d1\u30b9\n    \"\"\"\n    if _chunk.dst == -1:\n        return [_chunk]\n    else:\n        return [_chunk] + path_to_root(_sentence[_chunk.dst], _sentence)\n\n\ndef join_chunks_by_arrow(_chunks: list) -> str:\n    return ' -> '.join([c.join_morphs() for c in _chunks])\n\n\n# \u6700\u521d10\u6587\u3060\u3051\u51fa\u529b\u3057\u3066\u52d5\u4f5c\u78ba\u8a8d\nfor sentence in chunked_sentences[0:10]:\n    for chunk in sentence:\n        if chunk.has_noun():\n            print(join_chunks_by_arrow(path_to_root(chunk, sentence)))\n```\n\n# 49. \u540d\u8a5e\u9593\u306e\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u62bd\u51fa\n\n> \u6587\u4e2d\u306e\u3059\u3079\u3066\u306e\u540d\u8a5e\u53e5\u306e\u30da\u30a2\u3092\u7d50\u3076\u6700\u77ed\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u3092\u62bd\u51fa\u305b\u3088\uff0e\u305f\u3060\u3057\uff0c\u540d\u8a5e\u53e5\u30da\u30a2\u306e\u6587\u7bc0\u756a\u53f7\u304ci\u3068j\uff08i<j\uff09\u306e\u3068\u304d\uff0c\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306f\u4ee5\u4e0b\u306e\u4ed5\u69d8\u3092\u6e80\u305f\u3059\u3082\u306e\u3068\u3059\u308b\uff0e\n\n> - \u554f\u984c48\u3068\u540c\u69d8\u306b\uff0c\u30d1\u30b9\u306f\u958b\u59cb\u6587\u7bc0\u304b\u3089\u7d42\u4e86\u6587\u7bc0\u306b\u81f3\u308b\u307e\u3067\u306e\u5404\u6587\u7bc0\u306e\u8868\u73fe\uff08\u8868\u5c64\u5f62\u306e\u5f62\u614b\u7d20\u5217\uff09\u3092\"->\"\u3067\u9023\u7d50\u3057\u3066\u8868\u73fe\u3059\u308b\n> - \u6587\u7bc0i\u3068j\u306b\u542b\u307e\u308c\u308b\u540d\u8a5e\u53e5\u306f\u305d\u308c\u305e\u308c\uff0cX\u3068Y\u306b\u7f6e\u63db\u3059\u308b\n\n> \u307e\u305f\uff0c\u4fc2\u308a\u53d7\u3051\u30d1\u30b9\u306e\u5f62\u72b6\u306f\uff0c\u4ee5\u4e0b\u306e2\u901a\u308a\u304c\u8003\u3048\u3089\u308c\u308b\uff0e\n\n> - \u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u6587\u7bc0j\u304c\u5b58\u5728\u3059\u308b\u5834\u5408: \u6587\u7bc0i\u304b\u3089\u6587\u7bc0j\u306e\u30d1\u30b9\u3092\u8868\u793a\n> - \u4e0a\u8a18\u4ee5\u5916\u3067\uff0c\u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408: \u6587\u7bc0i\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u306e\u30d1\u30b9\u3068\u6587\u7bc0j\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u307e\u3067\u306e\u30d1\u30b9\uff0c\u6587\u7bc0k\u306e\u5185\u5bb9\u3092\"|\"\u3067\u9023\u7d50\u3057\u3066\u8868\u793a\n\n> \u4f8b\u3048\u3070\uff0c\u300c\u543e\u8f29\u306f\u3053\u3053\u3067\u59cb\u3081\u3066\u4eba\u9593\u3068\u3044\u3046\u3082\u306e\u3092\u898b\u305f\u3002\u300d\u3068\u3044\u3046\u6587\uff08neko.txt.cabocha\u306e8\u6587\u76ee\uff09\u304b\u3089\uff0c\u6b21\u306e\u3088\u3046\u306a\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u306f\u305a\u3067\u3042\u308b\uff0e\n\n> ```\n> X\u306f | Y\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> \u3082\u306e\u3092 | \u898b\u305f\n> X\u306f | Y\u3068\u3044\u3046 -> \u3082\u306e\u3092 | \u898b\u305f\n> X\u306f | Y\u3092 | \u898b\u305f\n> X\u3067 -> \u59cb\u3081\u3066 -> Y\n> X\u3067 -> \u59cb\u3081\u3066 -> \u4eba\u9593\u3068\u3044\u3046 -> Y\n> X\u3068\u3044\u3046 -> Y\n> ```\n\n\u554f\u984c\u6587\u3092\u8aad\u3093\u3067\u3082\u4f55\u304c\u3084\u308a\u305f\u3044\u306e\u304b\u5168\u304f\u5206\u304b\u3089\u306a\u304b\u3063\u305f\u3051\u3069\u3001[\u81ea\u7136\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af \u7b2c5\u7ae0 \u4fc2\u308a\u53d7\u3051\u89e3\u6790(\u5f8c\u534a)](http://qiita.com/tdrk/items/e6d3847f4284304ca9b8)\u3084[\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015\u5e74\u7248 (46\uff5e49)](http://kenichia.hatenablog.com/entry/2016/02/11/221513)\u3092\u8aad\u3093\u3067\u7406\u89e3\u3057\u305f\u3068\u3053\u308d\u306b\u3088\u308b\u3068\u3001\u3053\u3046\u3044\u3046\u3053\u3068\u3089\u3057\u3044.\n\n\u5206\u304b\u3089\u306a\u3044\u306a\u308a\u306b\u3001\u554f\u984c\u3092\u5206\u89e3\u3057\u306a\u304c\u3089\u3068\u308a\u3042\u3048\u305a\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u59cb\u3081\u3066\u307f\u308b\u3068\u5f90\u3005\u306b\u5206\u304b\u3063\u3066\u304f\u308b\u3082\u3093\u3067\u3059\u306d. \u3069\u306e\u3088\u3046\u306b\u554f\u984c\u3092\u5206\u89e3\u3057\u305f\u304b\u306b\u3064\u3044\u3066\u306f\u3001\u4e0b\u8a18\u30b3\u30fc\u30c9\u306b\u5c11\u3057\u30b3\u30e1\u30f3\u30c8\u3092\u591a\u3081\u306b\u66f8\u3044\u3066\u8aac\u660e\u3057\u3066\u3044\u308b\u306e\u3067\u53c2\u8003\u306b\u306a\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n```py3\ndef noun_pairs(_sentence: list):\n    \"\"\"\n    \u5f15\u6570\u3068\u3057\u3066\u6e21\u3055\u308c\u305f\u6587\u7ae0\u304c\u6301\u3064\u5168\u3066\u306e\u540d\u8a5e\u7bc0\u304b\u3089\u4f5c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u5168\u3066\u306e\u30da\u30a2\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059.\n    \"\"\"\n    from itertools import combinations\n    _noun_chunks = [_chunk for _chunk in _sentence if _chunk.has_noun()]\n    return list(combinations(_noun_chunks, 2))\n\n\ndef common_chunk(path_i: list, path_j: list) -> Chunk:\n    \"\"\"\n    \u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408\u3001\u6587\u7bc0k\u3092\u8fd4\u3059.\n    \"\"\"\n    _chunk_k = None\n    path_i = list(reversed(path_i))\n    path_j = list(reversed(path_j))\n    for idx, (c_i, c_j) in enumerate(zip(path_i, path_j)):\n        if c_i.srcs != c_j.srcs:\n            _chunk_k = path_i[idx - 1]\n            break\n\n    return _chunk_k\n\n\nfor sentence in chunked_sentences:\n    # \u540d\u8a5e\u53e5\u30da\u30a2\u306e\u30ea\u30b9\u30c8\n    n_pairs = noun_pairs(sentence)\n    if len(n_pairs) == 0:\n        continue\n\n    for n_pair in n_pairs:\n        chunk_i, chunk_j = n_pair\n\n        # \u6587\u7bc0i\u3068j\u306b\u542b\u307e\u308c\u308b\u540d\u8a5e\u53e5\u306f\u305d\u308c\u305e\u308c\uff0cX\u3068Y\u306b\u7f6e\u63db\u3059\u308b\n        chunk_i.replace_noun('X')\n        chunk_j.replace_noun('Y')\n\n        # \u6587\u7bc0i\u3068j\u304b\u3089root\u3078\u306e\u30d1\u30b9(Chunk\u578b\u306elist)\n        path_chunk_i_to_root = path_to_root(chunk_i, sentence)\n        path_chunk_j_to_root = path_to_root(chunk_j, sentence)\n\n        if chunk_j in path_chunk_i_to_root:\n            # \u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u6587\u7bc0j\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\n\n            # \u6587\u7bc0j\u306e\u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_j = path_chunk_i_to_root.index(chunk_j)\n\n            # \u6587\u7bc0i\u304b\u3089\u6587\u7bc0j\u306e\u30d1\u30b9\u3092\u8868\u793a\n            print(join_chunks_by_arrow(path_chunk_i_to_root[0: idx_j + 1]))\n        else:\n            # \u4e0a\u8a18\u4ee5\u5916\u3067\uff0c\u6587\u7bc0i\u3068\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u3067\u5171\u901a\u306e\u6587\u7bc0k\u3067\u4ea4\u308f\u308b\u5834\u5408\n\n            # \u6587\u7bc0k\u3092\u53d6\u5f97\n            chunk_k = common_chunk(path_chunk_i_to_root, path_chunk_j_to_root)\n            if chunk_k is None:\n                continue\n\n            # \u6587\u7bc0k\u306e\u6587\u7bc0i\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_k_i = path_chunk_i_to_root.index(chunk_k)\n\n            # \u6587\u7bc0k\u306e\u6587\u7bc0j\u304b\u3089\u69cb\u6587\u6728\u306e\u6839\u306b\u81f3\u308b\u7d4c\u8def\u4e0a\u306b\u304a\u3051\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n            idx_k_j = path_chunk_j_to_root.index(chunk_k)\n\n            # \u6587\u7bc0i\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u306e\u30d1\u30b9\u3068\u6587\u7bc0j\u304b\u3089\u6587\u7bc0k\u306b\u81f3\u308b\u76f4\u524d\u307e\u3067\u306e\u30d1\u30b9\uff0c\u6587\u7bc0k\u306e\u5185\u5bb9\u3092\"|\"\u3067\u9023\u7d50\u3057\u3066\u8868\u793a\n            print(' | '.join([join_chunks_by_arrow(path_chunk_i_to_root[0: idx_k_i]),\n                              join_chunks_by_arrow(path_chunk_j_to_root[0: idx_k_j]),\n                              chunk_k.join_morphs()]))\n```\n\n", "tags": ["Python3.5.1", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "NLP", "\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af"]}