{"context": "\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000spark\u3092\u4f7f\u3046\u3068\u51e6\u7406\u304c\u901f\u3044\n\u3068\u5c0f\u8033\u306b\u631f\u3093\u3060\u306e\u3067\u4e00\u5ea6\u52d5\u304b\u3057\u3066\u307f\u305f\u3002\n\n\u521d\u671f\u8a2d\u5b9a\nHomebrew \u3092\u4f7f\u3063\u3066\u4e0b\u8a18\u306e\u901a\u308a\u5c0e\u5165\u3057\u307e\u3057\u305f\u3002\nbrew install apache-spark\n\n\n\u30c7\u30fc\u30bf\u53d6\u5f97\n\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u304b\u3089CSV\u5f62\u5f0f\u306e\u90f5\u4fbf\u756a\u53f7\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u305f\u3002\nzipcloud\n\n\u8d77\u52d5\u65b9\u6cd5\nHomebrew\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3001\u4e0b\u8a18\u306eapache-spark\u306e\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u3078\u79fb\u52d5\u3059\u308b\u3002\n\ncd /usr/local/Cellar/apache-spark/1.5.2/bin/\n\n\u3069\u3046\u3084\u3089spark\u306fscala,java,python,R\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u81ea\u5206\u306fpython\u3092\u4f7f\u3044\u305f\u304b\u3063\u305f\u305f\u3081\npyspark\n\n\u3067\u8d77\u52d5\u3059\u308b\u3002\n\"spark\"\u306e\u30de\u30fc\u30af\u304c\u898b\u3048\u305f\u3089OK\u3002\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n      /_/\n\nUsing Python version 2.7.11 (default, Dec 26 2015 17:47:53)\nSparkContext available as sc, HiveContext available as sqlContext.\n>>>\n\n\n\u5b9f\u88c5\u5185\u5bb9\n\u5168\u56fd\u306e\u90f5\u4fbf\u756a\u53f7\u306e\u30c7\u30fc\u30bf\u306f\u5168\u90e8\u3067\u7d0412\u4e07\u4ef6\u3042\u308b\u3002\n\u5b9f\u88c5\u3059\u308b\u524d\u306b\u3001\u5927\u91cf\u306b\u30c7\u30fc\u30bf\u3092\u51e6\u7406\u3059\u308b\u3053\u3068\u3092\u60f3\u5b9a\u3057\u3066\u30c7\u30fc\u30bf\u309288\u56de\u30b3\u30d4\u30da\u3057\u3001\u7d041,200\u4e07\u4ef6\u306b\u500d\u52a0\u3057\u3066CSV\u30d5\u30a1\u30a4\u30eb\u3092\u65b0\u305f\u306b\u4f5c\u6210\u3057\u305f\u3002\n\u305d\u3057\u3066\u3001\n- \u90f5\u4fbf\u756a\u53f7\u306b\"7\"\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\n- \u5e02\u753a\u6751\u540d\u306b\u52d5\u7269\u306e\u540d\u524d\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\n\u3092AND\u6761\u4ef6\u3067\u691c\u7d22\u3059\u308b\u3088\u3046\u306b\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002\n\nsparkSample.py\n# -*- coding: utf-8 -*-\nimport time\nfrom pyspark import SparkContext\n\ndef main():\n    #\u691c\u7d22\u3057\u305f\u3044\u6f22\u5b57\n    queryList = [\"\u9e7f\",\"\u9ce5\",\"\u718a\",\"\u733f\",\"\u72ac\"]\n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30bf\u30fc\u30c8\n    start = time.time()\n\n    #\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    sc = SparkContext('local', 'Simple App')\n    logData = sc.textFile('KEN_ALL_OVER_TEN_MILLION.CSV')\n\n    #\u5404\u30af\u30a8\u30ea\u6bce\u306b\u60c5\u5831\u3092\u62bd\u51fa\n    for item in queryList:\n        #split\u3067\u30ea\u30b9\u30c8\u5316\n        lines = logData.map(lambda x: x.split(','))\n        #\u90f5\u4fbf\u756a\u53f7\u304c7\u3092\u542b\u3080\u3082\u306e\u3092\u62bd\u51fa\n        numberPicks = lines.filter(lambda s: unicode('7', 'utf-8') in s[2])\n        #\u5e02\u753a\u6751\u540d\u306b\u5bfe\u8c61\u306e\u6f22\u5b57\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\u3092\u62bd\u51fa\n        namePicks = lines.filter(lambda s: unicode(item, 'utf-8') in s[7])\n        #\u30ea\u30b9\u30c8\u306b\u683c\u7d0d\n        desList = namePicks.collect()\n\n        #\u30ed\u30b0\u51fa\u529b\n        for line in desList:\n            s = u\"\"\n            for i, unit in enumerate(line):\n                if i != 0:\n                    s = s + u', '\n                s = s + unit\n            print s.encode('utf-8')\n\n        #\u30d2\u30c3\u30c8\u6570\u51fa\u529b\n        outlog = \"query:\" + item.decode('utf-8') + u\", count:\" + \\\n            unicode(str(len(desList)), 'utf-8') + \", Time:{0}\".format(time.time() - start) + u\"[sec]\"\n        print outlog.encode('utf-8')    \n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30c8\u30c3\u30d7\n    finish_time = time.time() - start\n    print u\"Time[total]:{0}\".format(finish_time) + u\"[sec]\"\n\n    #\u7d42\u4e86\u51e6\u7406\n    sc.stop()\n\nif __name__ == '__main__':\n    main()\n\n\n\n\u8a55\u4fa1\u3092\u3059\u308b\u306b\u3042\u305f\u308a\u3001spark\u3092\u4f7f\u308f\u306a\u3044\u5834\u5408\u306e\u30b3\u30fc\u30c9\u3082\u66f8\u3044\u3066\u307f\u305f\u3002\n\nplain.py\n# -*- coding: utf-8 -*-\nimport time\n\ndef pickAnimal(recordList, qList, start):\n    #\u5404\u30af\u30a8\u30ea\u6bce\u306b\u60c5\u5831\u3092\u62bd\u51fa\n    for q in qList:\n        count = 0\n        for record in recordList:\n            sepRecord = record.split(\",\")\n            if len(sepRecord) == 15:\n                #\u90f5\u4fbf\u756a\u53f7\u304c7\u3092\u542b\u3080\u3082\u306e\u3092\u62bd\u51fa\n                #\u5e02\u753a\u6751\u540d\u306b\u5bfe\u8c61\u306e\u6f22\u5b57\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\u3092\u62bd\u51fa\n                if -1 < sepRecord[2].find(\"7\") and -1 < sepRecord[7].find(q):\n                    count = count + 1\n                    #\u30ed\u30b0\u51fa\u529b\n                    print record\n                    #\u30d2\u30c3\u30c8\u6570\u51fa\u529b\n        print \"query:\" + q + \", count:\" + str(count) + \", Time:{0}\".format(time.time() - start) + \"[sec]\"\n\ndef main():\n\n    sepRecordList = []\n    #\u691c\u7d22\u3057\u305f\u3044\u6f22\u5b57\n    queryList = [\"\u9e7f\",\"\u9ce5\",\"\u718a\",\"\u733f\",\"\u72ac\"]\n\n    #\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    srcpath = \"KEN_ALL_OVER_TEN_MILLION.CSV\"\n    srcIN = open(srcpath, 'r')\n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30bf\u30fc\u30c8\n    start = time.time()\n    for line in srcIN:\n        sepRecordList.append(line)\n\n    pickAnimal(sepRecordList, queryList, start)\n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30c8\u30c3\u30d7\n    finish_time = time.time() - start\n    print \"Time:{0}\".format(finish_time) + \"[sec]\"\n\n    #\u7d42\u4e86\u51e6\u7406\n    srcIN.close()\n\nif __name__ == '__main__':\n    main()\n\n\n\n\u8a08\u6e2c\u7d50\u679c\n$pyspark sparkSample.py\n~(\u4e2d\u7565)~\nTime[total]:645.52906394[sec]\n\n$python plain.py\n~(\u4e2d\u7565)~\nTime:112.966698885[sec]\n\n\u3075\u3001\u666e\u901a\u306b\u5b9f\u88c5\u3057\u305f\u65b9\u304c\u7d04\uff16\u500d\u901f\u3044\u3002\u3002\u3002\n\n\u7de8\u96c6\u5f8c\u8a18\n\u901f\u3055\u3092\u5b9f\u611f\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u5206\u6563\u51e6\u7406\u306e\u74b0\u5883\u3092\u6574\u3048\u308b\u304b\u6a5f\u68b0\u5b66\u7fd2\u3067\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u3092\u8a66\u884c\u3092\u7e70\u308a\u8fd4\u3055\u306a\u3044\u3068\u3069\u3046\u3084\u3089\u30c0\u30e1\u307f\u305f\u3044\u3067\u3059\u3002\nspark\u306e\u5a01\u529b\u3092\u808c\u3067\u4f53\u9a13\u3067\u304d\u308b\u307e\u3067\u304c\u6b21\u306e\u76ee\u6a19\u306a\u6c17\u304c\u3057\u307e\u3057\u305f\u3002\n\n\u53c2\u8003\n\nSpark Programming Guid\n(https://spark.apache.org/docs/1.2.0/programming-guide.html)\nSparkContext\u30e1\u30e2\n(http://www.ne.jp/asahi/hishidama/home/tech/scala/spark/SparkContext.html)\nApache Spark \u3092Python\u3067\u64cd\u4f5c\u3059\u308b(pyspark)\n(http://symfoware.blog68.fc2.com/blog-entry-1188.html)\n\n\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000**spark\u3092\u4f7f\u3046\u3068\u51e6\u7406\u304c\u901f\u3044**\n\u3068\u5c0f\u8033\u306b\u631f\u3093\u3060\u306e\u3067\u4e00\u5ea6\u52d5\u304b\u3057\u3066\u307f\u305f\u3002\n\n#\u521d\u671f\u8a2d\u5b9a\nHomebrew \u3092\u4f7f\u3063\u3066\u4e0b\u8a18\u306e\u901a\u308a\u5c0e\u5165\u3057\u307e\u3057\u305f\u3002\n\n```\nbrew install apache-spark\n```\n\n#\u30c7\u30fc\u30bf\u53d6\u5f97\n\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u304b\u3089CSV\u5f62\u5f0f\u306e\u90f5\u4fbf\u756a\u53f7\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u305f\u3002\n[zipcloud](http://zipcloud.ibsnet.co.jp/)\n\n#\u8d77\u52d5\u65b9\u6cd5\nHomebrew\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3001\u4e0b\u8a18\u306eapache-spark\u306e\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u3078\u79fb\u52d5\u3059\u308b\u3002\n```\ncd /usr/local/Cellar/apache-spark/1.5.2/bin/\n```\n\n\u3069\u3046\u3084\u3089spark\u306fscala,java,python,R\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u81ea\u5206\u306fpython\u3092\u4f7f\u3044\u305f\u304b\u3063\u305f\u305f\u3081\n\n```\npyspark\n```\n\n\u3067\u8d77\u52d5\u3059\u308b\u3002\n\"spark\"\u306e\u30de\u30fc\u30af\u304c\u898b\u3048\u305f\u3089OK\u3002\n\n```\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n      /_/\n\nUsing Python version 2.7.11 (default, Dec 26 2015 17:47:53)\nSparkContext available as sc, HiveContext available as sqlContext.\n>>>\n```\n\n#\u5b9f\u88c5\u5185\u5bb9\n\u5168\u56fd\u306e\u90f5\u4fbf\u756a\u53f7\u306e\u30c7\u30fc\u30bf\u306f\u5168\u90e8\u3067\u7d0412\u4e07\u4ef6\u3042\u308b\u3002\n\u5b9f\u88c5\u3059\u308b\u524d\u306b\u3001\u5927\u91cf\u306b\u30c7\u30fc\u30bf\u3092\u51e6\u7406\u3059\u308b\u3053\u3068\u3092\u60f3\u5b9a\u3057\u3066\u30c7\u30fc\u30bf\u309288\u56de\u30b3\u30d4\u30da\u3057\u3001\u7d041,200\u4e07\u4ef6\u306b\u500d\u52a0\u3057\u3066CSV\u30d5\u30a1\u30a4\u30eb\u3092\u65b0\u305f\u306b\u4f5c\u6210\u3057\u305f\u3002\n\u305d\u3057\u3066\u3001\n- **\u90f5\u4fbf\u756a\u53f7\u306b\"7\"\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e**\n- **\u5e02\u753a\u6751\u540d\u306b\u52d5\u7269\u306e\u540d\u524d\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e**\n\u3092AND\u6761\u4ef6\u3067\u691c\u7d22\u3059\u308b\u3088\u3046\u306b\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002\n\n```python:sparkSample.py\n# -*- coding: utf-8 -*-\nimport time\nfrom pyspark import SparkContext\n\ndef main():\n    #\u691c\u7d22\u3057\u305f\u3044\u6f22\u5b57\n    queryList = [\"\u9e7f\",\"\u9ce5\",\"\u718a\",\"\u733f\",\"\u72ac\"]\n    \n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30bf\u30fc\u30c8\n    start = time.time()\n    \n    #\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    sc = SparkContext('local', 'Simple App')\n    logData = sc.textFile('KEN_ALL_OVER_TEN_MILLION.CSV')\n    \n    #\u5404\u30af\u30a8\u30ea\u6bce\u306b\u60c5\u5831\u3092\u62bd\u51fa\n    for item in queryList:\n        #split\u3067\u30ea\u30b9\u30c8\u5316\n        lines = logData.map(lambda x: x.split(','))\n        #\u90f5\u4fbf\u756a\u53f7\u304c7\u3092\u542b\u3080\u3082\u306e\u3092\u62bd\u51fa\n        numberPicks = lines.filter(lambda s: unicode('7', 'utf-8') in s[2])\n        #\u5e02\u753a\u6751\u540d\u306b\u5bfe\u8c61\u306e\u6f22\u5b57\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\u3092\u62bd\u51fa\n        namePicks = lines.filter(lambda s: unicode(item, 'utf-8') in s[7])\n        #\u30ea\u30b9\u30c8\u306b\u683c\u7d0d\n        desList = namePicks.collect()\n\n        #\u30ed\u30b0\u51fa\u529b\n        for line in desList:\n            s = u\"\"\n            for i, unit in enumerate(line):\n                if i != 0:\n                    s = s + u', '\n                s = s + unit\n            print s.encode('utf-8')\n\n        #\u30d2\u30c3\u30c8\u6570\u51fa\u529b\n        outlog = \"query:\" + item.decode('utf-8') + u\", count:\" + \\\n            unicode(str(len(desList)), 'utf-8') + \", Time:{0}\".format(time.time() - start) + u\"[sec]\"\n        print outlog.encode('utf-8')    \n    \n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30c8\u30c3\u30d7\n    finish_time = time.time() - start\n    print u\"Time[total]:{0}\".format(finish_time) + u\"[sec]\"\n\n    #\u7d42\u4e86\u51e6\u7406\n    sc.stop()\n\nif __name__ == '__main__':\n    main()\n\n```\n\n\u8a55\u4fa1\u3092\u3059\u308b\u306b\u3042\u305f\u308a\u3001spark\u3092\u4f7f\u308f\u306a\u3044\u5834\u5408\u306e\u30b3\u30fc\u30c9\u3082\u66f8\u3044\u3066\u307f\u305f\u3002\n\n```python:plain.py\n# -*- coding: utf-8 -*-\nimport time\n\ndef pickAnimal(recordList, qList, start):\n    #\u5404\u30af\u30a8\u30ea\u6bce\u306b\u60c5\u5831\u3092\u62bd\u51fa\n\tfor q in qList:\n\t\tcount = 0\n\t\tfor record in recordList:\n\t\t\tsepRecord = record.split(\",\")\n\t\t\tif len(sepRecord) == 15:\n                #\u90f5\u4fbf\u756a\u53f7\u304c7\u3092\u542b\u3080\u3082\u306e\u3092\u62bd\u51fa\n                #\u5e02\u753a\u6751\u540d\u306b\u5bfe\u8c61\u306e\u6f22\u5b57\u304c\u542b\u3093\u3067\u3044\u308b\u3082\u306e\u3092\u62bd\u51fa\n\t\t\t\tif -1 < sepRecord[2].find(\"7\") and -1 < sepRecord[7].find(q):\n\t\t\t\t\tcount = count + 1\n                    #\u30ed\u30b0\u51fa\u529b\n\t\t\t\t\tprint record\n                    #\u30d2\u30c3\u30c8\u6570\u51fa\u529b\n\t\tprint \"query:\" + q + \", count:\" + str(count) + \", Time:{0}\".format(time.time() - start) + \"[sec]\"\n\ndef main():\n\n\tsepRecordList = []\n    #\u691c\u7d22\u3057\u305f\u3044\u6f22\u5b57\n\tqueryList = [\"\u9e7f\",\"\u9ce5\",\"\u718a\",\"\u733f\",\"\u72ac\"]\n\n    #\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n\tsrcpath = \"KEN_ALL_OVER_TEN_MILLION.CSV\"\n\tsrcIN = open(srcpath, 'r')\n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30bf\u30fc\u30c8\n\tstart = time.time()\n\tfor line in srcIN:\n\t\tsepRecordList.append(line)\n\n\tpickAnimal(sepRecordList, queryList, start)\n\n    #\u6642\u9593\u8a08\u6e2c\u30b9\u30c8\u30c3\u30d7\n\tfinish_time = time.time() - start\n\tprint \"Time:{0}\".format(finish_time) + \"[sec]\"\n\n    #\u7d42\u4e86\u51e6\u7406\n\tsrcIN.close()\n\nif __name__ == '__main__':\n    main()\n```\n\n#\u8a08\u6e2c\u7d50\u679c\n\n```spark\u3042\u308a\n$pyspark sparkSample.py\n~(\u4e2d\u7565)~\nTime[total]:645.52906394[sec]\n```\n\n```spark\u306a\u3057\n$python plain.py\n~(\u4e2d\u7565)~\nTime:112.966698885[sec]\n```\n\n\u3075\u3001\u666e\u901a\u306b\u5b9f\u88c5\u3057\u305f\u65b9\u304c\u7d04\uff16\u500d\u901f\u3044\u3002\u3002\u3002\n\n#\u7de8\u96c6\u5f8c\u8a18\n\u901f\u3055\u3092\u5b9f\u611f\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u5206\u6563\u51e6\u7406\u306e\u74b0\u5883\u3092\u6574\u3048\u308b\u304b\u6a5f\u68b0\u5b66\u7fd2\u3067\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u3092\u8a66\u884c\u3092\u7e70\u308a\u8fd4\u3055\u306a\u3044\u3068\u3069\u3046\u3084\u3089\u30c0\u30e1\u307f\u305f\u3044\u3067\u3059\u3002\nspark\u306e\u5a01\u529b\u3092\u808c\u3067\u4f53\u9a13\u3067\u304d\u308b\u307e\u3067\u304c\u6b21\u306e\u76ee\u6a19\u306a\u6c17\u304c\u3057\u307e\u3057\u305f\u3002\n\n#\u53c2\u8003\n- Spark Programming Guid\n(https://spark.apache.org/docs/1.2.0/programming-guide.html)\n\n- SparkContext\u30e1\u30e2\n(http://www.ne.jp/asahi/hishidama/home/tech/scala/spark/SparkContext.html)\n\n- Apache Spark \u3092Python\u3067\u64cd\u4f5c\u3059\u308b(pyspark)\n(http://symfoware.blog68.fc2.com/blog-entry-1188.html)\n\n", "tags": ["Spark", "Python"]}