{"context": "\n\nWHY\n\u6df1\u5c64\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8208\u5473\u304c\u3042\u308b\u65b9\u304c\u591a\u3044\u3068\u601d\u3046\u306e\u3067\u5bfe\u8a71\u306b\u304a\u3051\u308b\u6df1\u5c64\u5b66\u7fd2\u306e\u5b9f\u88c5\u306b\u3064\u3044\u3066\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\u96d1\u8ac7\u5fdc\u7b54\u304cChainer\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u305d\u306e\u90e8\u5206\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u3066\u8aac\u660e\u3057\u307e\u3059\u3002\u305f\u3060\u3057\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u53e4\u3044\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\n\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u3044\u308b\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.5.1\u3067\u3059\n\u9593\u9055\u3044\u304c\u3042\u308b\u90e8\u5206\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u6df1\u304f\u7406\u89e3\u3057\u305f\u3044\u90e8\u5206\u304c\u3042\u3063\u305f\u306e\u3067\u4e00\u90e8Chainer\u306e\u30b3\u30fc\u30c9\u3092\u8ffd\u3063\u3066\u3044\u307e\u3059\u3002\u9593\u9055\u3044\u304c\u3042\u308c\u3070\u5927\u5909\u304a\u624b\u6570\u3067\u3059\u304c\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\n\nPyCon 2016\u3067\u767a\u8868\u3057\u305f\u5185\u5bb9\u306f\u3069\u3061\u3089\u304b\u3068\u3044\u3046\u3068\u30b3\u30f3\u30bb\u30d7\u30c8\u3084\u6982\u8981\u30d9\u30fc\u30b9\u3092\u4f1d\u3048\u3066\u9802\u3051\u306a\u306e\u3067\u5b9f\u969b\u306b\u5b9f\u88c5\u3057\u3066\u3044\u308b\u30b3\u30fc\u30c9\u306e\u8aac\u660e\u304c\u306a\u3044\u306e\u3067\u3001\u81ea\u8eab\u306e\u632f\u308a\u8fd4\u308a\u3068\u3044\u3046\u610f\u5473\u3067\u3082\u3042\u3063\u305f\u65b9\u304c\u826f\u3044\n\u305d\u3053\u3067\u30b3\u30fc\u30c9\u306e\u8aac\u660e\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u3082\u3063\u3068\u7406\u89e3\u3057\u3066\u4f7f\u3063\u3066\u3082\u3089\u3048\u308b\u4eba\u304c\u5897\u3048\u3066\u6b32\u3057\u3044\u3068\u601d\u3044\u3001\u3053\u306e\u8a18\u4e8b\u3092\u66f8\u304d\u307e\u3057\u305f\u3002(\u3067\u304d\u308c\u3070github\u306e\u30b9\u30bf\u30fc\u304c\u5897\u3048\u308b\u3068\u5b09\u3057\u3044)\n\n\nDocker Hub\nhttps://hub.docker.com/r/masayaresearch/dialogue/\ngithub\nhttps://github.com/SnowMasaya/Chainer-Slack-Twitter-Dialogue\n\u4ed6\u306b\u3082\u8cea\u554f\u5fdc\u7b54\u3001\u8a71\u984c\u5206\u985e\u3001\u30c7\u30fc\u30bf\u53d6\u5f97\u306e\u4e26\u5217\u5316\u306a\u3069\u591a\u5c90\u306b\u6e21\u3063\u3066\u3044\u308b\u306e\u3067\u305d\u306e\u90e8\u5206\u3082\u8981\u671b\u304c\u3042\u308c\u3070\u66f8\u304d\u307e\u3059\u3002\n\nWHAT\n\n\u96d1\u8ac7\u5fdc\u7b54\n\u5206\u985e\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u4e2d\u3067\u3082\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3068\u306f\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6a5f\u68b0\u7ffb\u8a33\u306e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u30b7\u30fc\u30b1\u30f3\u30b9 to \u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u30e2\u30c7\u30eb\u3067\u306f\u9577\u6587\u306e\u5165\u529b\u306b\u304a\u3044\u3066\u4e00\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306b\u96c6\u7d04\u3055\u308c\u308b\u3068\u6700\u521d\u306e\u5358\u8a9e\u306e\u91cd\u8981\u6027\u304c\u5fae\u5206\u306e\u96c6\u7a4d\u306b\u3088\u3063\u3066\u8584\u308c\u3066\u304f\u308b\u554f\u984c\u304c\u3042\u308a\u307e\u3057\u305f\u3002\u82f1\u8a9e\u306e\u5834\u5408\u306f\u7279\u306b\u6700\u521d\u306e\u5358\u8a9e\u306e\u91cd\u8981\u6027\u304c\u5897\u3057\u3066\u304d\u307e\u3059\u3002\n\u305d\u308c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u3001\u5f93\u6765\u3067\u306f\u9006\u65b9\u5411\u306e\u5165\u529b\u3092\u5165\u308c\u3066\u7ffb\u8a33\u7cbe\u5ea6\u3092\u4e0a\u3052\u3066\u3044\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u65e5\u672c\u8a9e\u3084\u4e2d\u56fd\u8a9e\u306e\u5834\u5408\u306f\u9006\u306b\u6700\u5f8c\u306e\u5358\u8a9e\u304c\u91cd\u8981\u306b\u306a\u308b\u305f\u3081\u672c\u8cea\u7684\u306a\u89e3\u6c7a\u306b\u306a\u308a\u307e\u305b\u3093\u3002\n\u305d\u3053\u3067\u5165\u529b\u306e\u30a8\u30f3\u30b3\u30fc\u30c9\u3068\u30c7\u30b3\u30fc\u30c9\u3092\u5206\u3051\u3066\u884c\u308f\u305a\u306b\u30c7\u30b3\u30fc\u30c9\u3068\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u96a0\u308c\u5c64\u3068\u30a8\u30f3\u30b3\u30fc\u30c9\u306e\u5165\u529b\u3092\u52a0\u91cd\u5e73\u5747\u3057\u3066\u5404\u30c7\u30b3\u30fc\u30c9\u3054\u3068\u306e\u51fa\u529b\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3068\u3057\u3066\u63d0\u6848\u3055\u308c\u305f\u306e\u304c\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3067\u3059\u3002\u3082\u3068\u3082\u3068\u306f\u753b\u50cf\u306e\u5206\u91ce\u3067\u6210\u679c\u3092\u51fa\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u73fe\u5728\u306f\u6a5f\u68b0\u7ffb\u8a33\u3084\u6587\u7ae0\u8981\u7d04\u306e\u30bf\u30b9\u30af\u3067\u6210\u679c\u3092\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\n\u30a4\u30e1\u30fc\u30b8\n\n\n\"\u3082\"\u3092\u4e88\u6e2c\u3059\u308b\u306b\u306f\u201d\u50d5\u201d\u3068\u5165\u529b\uff08\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\uff09\u304c\u5f97\u3089\u308c\u305f\u6642\u306e\u4e8b\u5f8c\u78ba\u7387\u306b\u306a\u308a\u307e\u3059\u3002\n\u4e8b\u5f8c\u78ba\u7387\u306f\u4e00\u3064\u524d\u306e\u5358\u8a9e\uff08\u50d5\uff09\u3068\u96a0\u308c\u5c64\u306e\u72b6\u614b\u3068\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb(\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\")\u306e\u30b9\u30b3\u30a2\u306b\u306a\u308a\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306f\u4eca\u306f\u7121\u8996\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3042\u3068\u3067\u89e3\u8aac\u3092\u3057\u307e\u3059\u3002\n\u95a2\u6570g\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u304c\u4e00\u822c\u7684\u3067\u3059\n\n\u4e0a\u56f3\u306e\u3088\u3046\u306b\u4e8b\u524d\u306e\u51fa\u529b\u3092\u8003\u616e\u3057\u3066\u3001\u73fe\u5728\u306e\u72b6\u614b\u3068\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u4e88\u6e2c\u3059\u308b\u5834\u5408\u306b\u4f7f\u7528\u3059\u308b\u6570\u5f0f\u306f\u4e0b\u8a18\u3067\u3059\u3002\np(y_i|y_1,...y_{i_1}, \\vec{x}) = g(y_{i-1}, s_i, c_i)\np(yi|y1,...yi1,\u2192x)=g(yi\u22121,si,ci)p(yi|y1,...yi1,x\u20d7\u00a0)=g(yi\u22121,si,ci){p(y_i|y_1,...y_{i_1}, \\vec{x}) = g(y_{i-1}, s_i, c_i)\n}\n\u3053\u3053\u3067\u96a0\u308c\u5c64\u306e\u6642\u523bt\u306e\u72b6\u614b\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3067\u304d\u308b\u3002\uff08\"\u3082\"\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306e\u72b6\u614b\uff09\n\u3053\u308c\u306f\u4e00\u3064\u524d\u306e\u5358\u8a9e\"\u50d5\"\u3068\u4e00\u3064\u524d\u306e\u72b6\u614b\u3068\u4e00\u3064\u524d\u306e\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306b\u3088\u3063\u3066\u6c7a\u307e\u308a\u307e\u3059\u3002\n\u95a2\u6570f\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u304c\u4e00\u822c\u7684\ns_i=f(s_{i-1}, y_{i-1},c_i)\nsi=f(si\u22121,yi\u22121,ci){s_i=f(s_{i-1}, y_{i-1},c_i)\n}\n\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\uff08\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\u201d\uff09\u306e\u96a0\u308c\u5c64\u3068\u91cd\u307faa\u306e\u7dcf\u548c\u3067\u6c7a\u307e\u308a\u307e\u3059\u3002\nc_{i} = \\sum^{T_x}_{j=1}\\alpha_{ij}h_{j}\nci=Tx\u2211j=1\u03b1ijhj{c_{i} = \\sum^{T_x}_{j=1}\\alpha_{ij}h_{j}\n}\n\u3067\u306f\u5148\u307b\u3069\u5b9a\u7fa9\u3057\u305f\u91cd\u307f\u3092\u3069\u306e\u3088\u3046\u306b\u6c42\u3081\u308b\u304b\u3067\u3059\u304c\u3001e\u3068\u3044\u3046\u96a0\u308c\u5c64h\u3068\u51fa\u529b\u5074\u306e\u4e00\u3064\u524d\u306e\u72b6\u614bs(\"\u306f\u201d\u306e\u5834\u5408\u306f\"\u50d5\")\u304b\u3089\u5f97\u3089\u308c\u308b\u91cd\u307f\u3092\u30b9\u30b3\u30a2\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5f62\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\u306eh\u304c\u7279\u6b8a\u306a\u5f62\u306b\u306a\u308b\u305f\u3081\u3067\u3059\u3002\u3053\u306e\u70b9\u306f\u5f8c\u8ff0\u3057\u307e\u3059\u3002\ne\u3068\u3044\u3046\u30b9\u30b3\u30a2\u306f\u78ba\u7387\u306e\u305f\u3081\u5c0f\u3055\u3044\u5024\u306b\u306a\u308a\u307e\u3059\u3002\u305d\u308c\u3092exp\u95a2\u6570\u306b\u3088\u3063\u3066\u5927\u304d\u3044\u5024\u306b\u3057\u3066\u5165\u529b\u90e8\u5168\u3066\u3067\u5272\u308a\u7b97\u3092\u3057\u3066\u5165\u529b\u3068\u51fa\u529b\u306e\u30da\u30a2\u306b\u30de\u30c3\u30c1\u3057\u305f\u91cd\u307f\u3092\u7b97\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\n\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})} \\\\\ne_{ij} = a(s_{i-1}, h_j)\n\u03b1ij=exp(eij)\u2211Txk=1exp(eik)eij=a(si\u22121,hj){\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})} \\\\\ne_{ij} = a(s_{i-1}, h_j)\n}\n\u3067\u306f\u96a0\u308c\u5c64h\u306f\u3069\u306e\u3088\u3046\u306a\u70b9\u304c\u7279\u6b8a\u306a\u306e\u3067\u3057\u3087\u3046\u304b\u3002\n\u5b9f\u306f\u30d5\u30a9\u30ef\u30fc\u30c9\u3068\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3092\u5408\u308f\u305b\u3066\u3044\u308b\u70b9\u304c\u901a\u5e38\u306e\u30b7\u30fc\u30b1\u30f3\u30b9 to \u30b7\u30fc\u30b1\u30f3\u30b9\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002\n\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30d5\u30a9\u30ef\u30fc\u30c9\u3068\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3092\u5b9a\u7fa9\u3057\u3066\u3001\u305d\u308c\u3092\u30b3\u30f3\u30ab\u30c1\u3057\u3066\u8868\u3057\u307e\u3059\u3002\u3053\u308c\u304c\u30a8\u30f3\u30b3\u30fc\u30c9\u5165\u529b\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\u306e\u96a0\u308c\u5c64\u306b\u306a\u308a\u307e\u3059\u3002\n(\\vec{h_1},...\\vec{h_{T_x}})\\\\\n(\\overleftarrow{h_1},...\\overleftarrow{h_{T_x}})\\\\\nh_j = [\\vec{h_j^T};\\overleftarrow{h_j^T}]^T\n(\u2192h1,...\u2192hTx)(\u2190h1,...\u2190hTx)hj=[\u2192hTj;\u2190hTj]T{(\\vec{h_1},...\\vec{h_{T_x}})\\\\\n(\\overleftarrow{h_1},...\\overleftarrow{h_{T_x}})\\\\\nh_j = [\\vec{h_j^T};\\overleftarrow{h_j^T}]^T\n}\n\u3067\u306f\u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u30d9\u30fc\u30b9\u3067\u3053\u306e\u6570\u5f0f\u3092\u3069\u306e\u3088\u3046\u306b\u5b9f\u73fe\u3057\u3066\u3044\u308b\u304b\u8ffd\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\nsrc_embed.py\n\n\n\u8a00\u8a9e\u306e\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u79fb\u3059\u90e8\u5206\u3067\u3059\u3002 \n\n\nattention_encoder.py\n\n\n\u5165\u529b\u5074\u8a00\u8a9e\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u79fb\u3055\u308c\u305f\u60c5\u5831\u3092\u4f1d\u642c\u3059\u308b\u90e8\u5206\u3067\u3059\u3002\uff08\u5bfe\u8a71\u3067\u8a00\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u90e8\u5206\u306b\u3042\u305f\u308a\u307e\u3059\uff09\n\n\nattention.py\n\n\n\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u4f5c\u6210\u3059\u308b\u90e8\u5206\n\n\n\nattention_decoder.py\n\n\u51fa\u529b\u8a00\u8a9e\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u8a00\u8a9e\u306e\u51fa\u529b\u3001\u96a0\u308c\u5c64\u306e\u4f1d\u642c\u307e\u3067\u884c\u3046\u3002\n\n\n\nattention_dialogue.py\n\n\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\n\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n\u91cd\u307f\u306e\u521d\u671f\u5316\n\u91cd\u307f\u306e\u57cb\u3081\u8fbc\u307f\n\u30a8\u30f3\u30b3\u30fc\u30c9\u51e6\u7406\n\u30c7\u30b3\u30fc\u30c9\u51e6\u7406 \n\n\n\n\u4e0a\u8a18\u306e5\u3064\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u3002\n\nHOW\n\nsrc_embed.py\n\n\u5165\u529b\u8a00\u8a9e\u306e\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3080\u90e8\u5206\u304b\u3089\u89e3\u8aac\u3057\u307e\u3059\u3002\n\u5165\u529b\u8a00\u8a9e\u306e\u8a9e\u5f59\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u6570\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\u5165\u529b\u8a00\u8a9e\u306e\u8a9e\u5f59\u306f\u5bfe\u8a71\u306e\u5834\u5408\u306f\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u306b\u306a\u308a\u307e\u3059\u3002\n    def __init__(self, vocab_size, embed_size):\n        super(SrcEmbed, self).__init__(\n            weight_xi=links.EmbedID(vocab_size, embed_size),\n        )\n\n\u5177\u4f53\u7684\u306a\u51e6\u7406\u306e\u4e2d\u8eab\u306b\u306a\u308a\u307e\u3059\u3002\nW (~chainer.Variable)\u306fchainer.Variable\u306e\u57cb\u3081\u8fbc\u307f\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\n\u5e73\u57470\u3001\u5206\u65631.0\u306e\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\u3055\u308c\u308b\u521d\u671f\u91cd\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n    def __init__(self, in_size, out_size, initialW=None, ignore_label=None):\n        super(EmbedID, self).__init__(W=(in_size, out_size))\n        if initialW is None:\n            initialW = initializers.Normal(1.0)\n        initializers.init_weight(self.W.data, initialW)\n        self.ignore_label = ignore_label\n\n\u5177\u4f53\u7684\u306b\u6b63\u898f\u5206\u5e03\u304b\u3089\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\nxp\u306e\u90e8\u5206\u306fgpu\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306a\u306e\u3067numpy.random.normal\u3067\u306a\u304fxp.random.normal\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u53c2\u8003\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html\nclass Normal(initializer.Initializer):\n    def __init__(self, scale=0.05, dtype=None):\n        self.scale = scale\n        super(Normal, self).__init__(dtype)\n\n    def __call__(self, array):\n        xp = cuda.get_array_module(array)\n        array[...] = xp.random.normal(\n             loc=0.0, scale=self.scale, size=array.shape)\n\n\u3053\u3053\u3067\u8fd4\u5374\u3059\u308b\u521d\u671f\u306e\u91cd\u307f\u306f\u4e0b\u8a18\u3067\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\ninitializer\u306e\u30c7\u30fc\u30bf\u306fnumpy.ndarray\u306e\u30c7\u30fc\u30bf\u304b\u30af\u30e9\u30b9\u3082\u3057\u304f\u306fcupy.ndarray\u306e\u30af\u30e9\u30b9\u3067\u8a2d\u5b9a\u3055\u308c\u307e\u3059\u3002\ndef init_weight(weights, initializer, scale=1.0):\n\n    if initializer is None:\n        initializer = HeNormal(1 / numpy.sqrt(2))\n    elif numpy.isscalar(initializer):\n        initializer = Constant(initializer)\n    elif isinstance(initializer, numpy.ndarray):\n        initializer = Constant(initializer)\n\n    assert callable(initializer)\n    initializer(weights)\n    weights *= scale\n\ninitializer\u304cNone\u4ee5\u5916\u306e\u5834\u5408\u306bgpu\u5f62\u5f0f\u306earray\u304b\u901a\u5e38\u306earray\u304b\u3092\u8fd4\u5374\u3057\u3066\u3044\u307e\u3059\u3002\n\nclass Constant(initializer.Initializer):\n\n    def __init__(self, fill_value, dtype=None):\n        self.fill_value = fill_value\n        super(Constant, self).__init__(dtype)\n\n    def __call__(self, array):\n        if self.dtype is not None:\n            assert array.dtype == self.dtype\n        xp = cuda.get_array_module(array)\n        array[...] = xp.asarray(self.fill_value)\n\n\u5177\u4f53\u7684\u306b\u5224\u5b9a\u3057\u3066\u8fd4\u5374\u3057\u3066\u3044\u308b\u90e8\u5206\u306f\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\ndef get_array_module(*args):\n    if available:\n        return cupy.get_array_module(*args)\n    else:\n        return numpy\n\n__call__\u95a2\u6570\u306b\u3088\u3063\u3066src_embed\u3092\u547c\u3073\u51fa\u3057\u3066\u5165\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u57cb\u3081\u8fbc\u3093\u3067\u3044\u307e\u3059\u3002\nfunctions.tanh\u3067\u53cc\u6975\u95a2\u6570\u3092\u7528\u3044\u3066\u5fae\u5206\u53ef\u80fd\u306a\u7a7a\u9593\u306b\u5199\u50cf\u3057\u3066\u3044\u307e\u3059\u3002\u5fae\u5206\u53ef\u80fd\u306a\u7a7a\u9593\u3067\u3042\u308c\u3070\u8aa4\u5dee\u9006\u4f1d\u642c\u306b\u3088\u308a\u5b66\u7fd2\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n    def __call__(self, source):\n        return functions.tanh(self.weight_xi(source))\n\n\nattention_encoder.py\n\n\u5148\u307b\u3069\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3055\u308c\u305f\u5165\u529b\u5c64\u3092\u96a0\u308c\u5c64\u306b\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\u306a\u305c4\u500d\u304b\n\u5165\u529b\u30b2\u30fc\u30c8\n\u5fd8\u5374\u30b2\u30fc\u30c8\n\u51fa\u529b\u30b2\u30fc\u30c8\n\u4ee5\u524d\u306e\u5165\u529b\u3092\u8003\u616e\u3059\u308b\u30b2\u30fc\u30c8\n\u4e0a\u8a18\u306e4\u3064\u3092\u8003\u616e\u3057\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002\n\u306a\u305c\u305d\u308c\u304c\u5fc5\u8981\u304b\u306f\u4ed6\u306e\u8cc7\u6599\u3067\u3082\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u8a73\u7d30\u306f\u8ff0\u3079\u307e\u305b\u3093\u304c\u3001\u3053\u306e\u5de5\u592b\u306b\u3088\u3063\u3066\u904e\u5b66\u7fd2\u3092\u9632\u3044\u3067\u3044\u307e\u3059\u3002\n    def __init__(self, embed_size, hidden_size):\n        super(AttentionEncoder, self).__init__(\n            source_to_hidden=links.Linear(embed_size, 4 * hidden_size),\n            hidden_to_hidden=links.Linear(hidden_size, 4 * hidden_size),\n        )\n\n\u5177\u4f53\u7684\u306alinks.Liner\u306e\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u91cd\u307f\u306e\u521d\u671f\u5316\n\u91cd\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u4e0e\n\u30d0\u30a4\u30a2\u30b9\u306e\u521d\u671f\u5316\n\u30d0\u30a4\u30a2\u30b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u4e0e\n\n    def __init__(self, in_size, out_size, wscale=1, bias=0, nobias=False,\n                 initialW=None, initial_bias=None):\n        super(Linear, self).__init__()\n\n        self.initialW = initialW\n        self.wscale = wscale\n\n        self.out_size = out_size\n        self._W_initializer = initializers._get_initializer(initialW, math.sqrt(wscale))\n\n        if in_size is None:\n            self.add_uninitialized_param('W')\n        else:\n            self._initialize_params(in_size)\n\n        if nobias:\n            self.b = None\n        else:\n            if initial_bias is None:\n                initial_bias = bias\n            bias_initializer = initializers._get_initializer(initial_bias)\n            self.add_param('b', out_size, initializer=bias_initializer)\n\n    def _initialize_params(self, in_size):\n        self.add_param('W', (self.out_size, in_size), initializer=self._W_initializer)\n\n\u5177\u4f53\u7684\u306a\u521d\u671f\u5316\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\nscale\u304c\u30c7\u30d5\u30a1\u30eb\u30c8\u30671\u306a\u306e\u3067\u305d\u308c\u3092\u304b\u3051\u3066Array\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\u5148\u307b\u3069\u51fa\u3066\u304d\u305fConstant\u3067\u521d\u671f\u5024\u3092\u56fa\u5b9a\u5024\u3067\u521d\u671f\u5316\u3057\u3066scale\u3092\u304b\u3051\u3066\u3044\u307e\u3059\u3002\nclass _ScaledInitializer(initializer.Initializer):\n\n    def __init__(self, initializer, scale=1.0):\n        self.initializer = initializer\n        self.scale = scale\n        dtype = getattr(initializer, 'dtype', None)\n        super(Identity, self).__init__(dtype)\n\n    def __call__(self, array):\n        self.initializer(array)\n        array *= self.scale\n\n\ndef _get_initializer(initializer, scale=1.0):\n    if initializer is None:\n        return HeNormal(scale / numpy.sqrt(2))\n    if numpy.isscalar(initializer):\n        return Constant(initializer * scale)\n    if isinstance(initializer, numpy.ndarray):\n        return Constant(initializer * scale)\n\n    assert callable(initializer)\n    if scale == 1.0:\n        return initializer\n    return _ScaledInitializer(initializer, scale)\n\n\u73fe\u5728\u306e\u72b6\u614b\u3068\u524d\u56de\u306e\u96a0\u308c\u5c64\u306e\u5024\u3001\u5165\u529b\u5c64\u306e\u5024\u3092\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\n    def __call__(self, source, current, hidden):\n        return functions.lstm(current, self.source_to_hidden(source) + self.hidden_to_hidden(hidden))\n\n\u4e0a\u8a18\u306elstm\u3067forward\u306e\u51e6\u7406\u306e\u969b\u306b\u547c\u3070\u308c\u3066\u3044\u308b\u51e6\u7406\u304c\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\n\u30d5\u30a1\u30a4\u30eb\u306fchainer/functions/activation/lstm.py\u306b\u306a\u308a\u307e\u3059\u3002\n\u5165\u529b\u3092lstm\u306e4\u3064\u306e\u30b2\u30fc\u30c8\u306b\u5206\u3051\u3066\u3044\u307e\u3059\u3002\nlen(x):\u884c\u306e\u9577\u3055\u306e\u53d6\u5f97\nx.shape[1]\uff1a\u5217\u306e\u9577\u3055\u306e\u53d6\u5f97\nx.shape[2:]:3\u6b21\u5143\u4ee5\u4e0a\u306e\u30c7\u30fc\u30bf\u306e\u5834\u5408\u306b\u4f7f\u7528\n    def _extract_gates(x):\n        r = x.reshape((len(x), x.shape[1] // 4, 4) + x.shape[2:])\n        return [r[:, :, i] for i in six.moves.range(4)]\n\ncpu\u306e\u51e6\u7406\n\n\u5165\u529b\u3068\u72b6\u614b\u3092\u53d6\u5f97\nlstm\u306b\u57fa\u3065\u3044\u30664\u3064\u306e\u60c5\u5831\u3092\u53d6\u5f97\nlstm\u306e\u5024\u306f\u8ad6\u6587\u306b\u6e96\u62e0\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u4ee5\u5916\u306f\u5165\u529b\u304c\u306f\u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\u3067\u5b9f\u73fe\u53ef\u80fd\n\u6b21\u306e\u72b6\u614b\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u521d\u671f\u5316\n\u5165\u529b\u3068\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u7a4d\u3068\u5fd8\u5374\u3068\u904e\u53bb\u306e\u72b6\u614b\u306e\u7a4d\u306e\u8a71\u306b\u3088\u3063\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u3001\u6b21\u306e\u72b6\u614b\u3092\u4ed8\u4e0e\n\u96a0\u308c\u5c64\u306e\u5024\u306f\u51fa\u529b\u3068\u6b21\u306e\u72b6\u614b\u306e\u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\u3067\u4e0e\u3048\u308b\n\ngpu\u306e\u51e6\u7406\u306f\u540c\u4e00\u305f\u3060\u3057\u3001C++\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u4e0b\u8a18\u306e\u5b9a\u7fa9\u3092\u4f7f\u7528\u3057\u3066\u8aad\u3093\u3067\u3044\u308b\u3002python\u4e0a\u3067\u5b9a\u7fa9\u3057\u3066\u3044\u308blstm\u3067\u540c\u4e00\u3060\u304cC++\u3067\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u3002\n_preamble = '''\ntemplate <typename T> __device__ T sigmoid(T x) {\n    const T half = 0.5;\n    return tanh(x * half) * half + half;\n}\ntemplate <typename T> __device__ T grad_sigmoid(T y) { return y * (1 - y); }\ntemplate <typename T> __device__ T grad_tanh(T y) { return 1 - y * y; }\n#define COMMON_ROUTINE \\\n    T aa = tanh(a); \\\n    T ai = sigmoid(i_); \\\n    T af = sigmoid(f); \\\n    T ao = sigmoid(o);\n'''\n\n\n    def forward(self, inputs):\n        c_prev, x = inputs\n        a, i, f, o = _extract_gates(x)\n        batch = len(x)\n\n        if isinstance(x, numpy.ndarray):\n            self.a = numpy.tanh(a)\n            self.i = _sigmoid(i)\n            self.f = _sigmoid(f)\n            self.o = _sigmoid(o)\n\n            c_next = numpy.empty_like(c_prev)\n            c_next[:batch] = self.a * self.i + self.f * c_prev[:batch]\n            h = self.o * numpy.tanh(c_next[:batch])\n        else:\n            c_next = cuda.cupy.empty_like(c_prev)\n            h = cuda.cupy.empty_like(c_next[:batch])\n            cuda.elementwise(\n                'T c_prev, T a, T i_, T f, T o', 'T c, T h',\n                '''\n                    COMMON_ROUTINE;\n                    c = aa * ai + af * c_prev;\n                    h = ao * tanh(c);\n                ''',\n                'lstm_fwd', preamble=_preamble)(\n                    c_prev[:batch], a, i, f, o, c_next[:batch], h)\n\n        c_next[batch:] = c_prev[batch:]\n        self.c = c_next[:batch]\n        return c_next, h\n\ngpu\u306e\u51e6\u7406\u306f\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\ncuda\u306e\u4e2d\u8eab\u3092\u547c\u3076\u305f\u3081\u306e\u51e6\u7406\u304c\u4e0b\u8a18\u3002\ncupy\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3002\ncupy\u306b\u3064\u3044\u3066\nhttp://docs.chainer.org/en/stable/cupy-reference/overview.html\n\u4e0b\u8a18\u3067\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u3092\u4f5c\u6210\u3057\u3001cuda\u306e\u30e1\u30e2\u30ea\u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3057\u3066\u7d50\u679c\u3092cuda\u306e\u30c7\u30d0\u30a4\u30b9\u3068\u9023\u643a\u3057\u307e\u3059\u3002\ngpu\u306e\u30e1\u30e2\u30ea\u7a7a\u9593\u3067\u8a08\u7b97\u3057\u305f\u5024\u3092\u9023\u643a\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u7406\u7531\u306f\u4e0b\u8a18\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\nhttp://www.nvidia.com/docs/io/116711/sc11-cuda-c-basics.pdf\n@memoize(for_each_device=True)\ndef elementwise(in_params, out_params, operation, name, **kwargs):\n    check_cuda_available()\n    return cupy.ElementwiseKernel(\n        in_params, out_params, operation, name, **kwargs)\n\nbackword\u306e\u969b\u306f\u4e0b\u8a18\u306e\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\nchainer\u306f\u3053\u3053\u3089\u8fba\u306e\u51e6\u7406\u3092\u96a0\u3057\u3066\u304f\u308c\u308b\u306e\u3067\u52a9\u304b\u308a\u307e\u3059\u3002\nforward\u306e\u51e6\u7406\u3068\u540c\u3058\u3067\u3059\u304c\u3001\u9055\u3044\u306f\u5165\u529b\u3060\u3051\u3067\u306a\u304f\u52fe\u914d\u306e\u51fa\u529b\u3082\u4f7f\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\ngc_prev[:batch]\u3067\u96a0\u308c\u5c64\u3068\u51fa\u529b\u5c64\u306e\u7a4d\u306b\u52fe\u914d\u3092\u8db3\u3057\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u66f4\u65b0\u3057\u3066\u3044\u307e\u3059\u3002\n_grad_tanh\u3068_grad_sigmoid\u3067\u52fe\u914d\u3092\u8a08\u7b97\u3057\u66f4\u65b0\u3057\u3066\u3044\u308b\u3002\n            co = numpy.tanh(self.c)\n            gc_prev = numpy.empty_like(c_prev)\n            # multiply f later\n            gc_prev[:batch] = gh * self.o * _grad_tanh(co) + gc_update\n            gc = gc_prev[:batch]\n            ga[:] = gc * self.i * _grad_tanh(self.a)\n            gi[:] = gc * self.a * _grad_sigmoid(self.i)\n            gf[:] = gc * c_prev[:batch] * _grad_sigmoid(self.f)\n            go[:] = gh * co * _grad_sigmoid(self.o)\n            gc_prev[:batch] *= self.f  # multiply f here\n            gc_prev[batch:] = gc_rest\n\ngpu\u306e\u51e6\u7406\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\ncpu\u306e\u51e6\u7f6e\u3068\u540c\u4e00\u3067\u3059\u304cC++\u3092\u901a\u3059\u305f\u3081\u306bcuda.elementwise\u3092\u4f7f\u7528\u3057\u3066\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002\n            a, i, f, o = _extract_gates(x)\n            gc_prev = xp.empty_like(c_prev)\n            cuda.elementwise(\n                'T c_prev, T c, T gc, T gh, T a, T i_, T f, T o',\n                'T gc_prev, T ga, T gi, T gf, T go',\n                '''\n                    COMMON_ROUTINE;\n                    T co = tanh(c);\n                    T temp = gh * ao * grad_tanh(co) + gc;\n                    ga = temp * ai * grad_tanh(aa);\n                    gi = temp * aa * grad_sigmoid(ai);\n                    gf = temp * c_prev * grad_sigmoid(af);\n                    go = gh * co * grad_sigmoid(ao);\n                    gc_prev = temp * af;\n                ''',\n                'lstm_bwd', preamble=_preamble)(\n                    c_prev[:batch], self.c, gc_update, gh, a, i, f, o,\n                    gc_prev[:batch], ga, gi, gf, go)\n            gc_prev[batch:] = gc_rest\n\n\n\nattention.py\n\n\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u4fdd\u6301\u3059\u308b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\n\nannotion_weight\u304c\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u90e8\u5206\u306e\u91cd\u307f\n\nback_weight\u304c\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u90e8\u5206\u306e\u91cd\u307f\u3001\n\npw\u304c\u73fe\u5728\u306e\u5c64\u306e\u91cd\u307f\n\nweight_exponential\u304c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067exp\u95a2\u6570\u3092\u51e6\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a\n\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__(\n            annotion_weight=links.Linear(hidden_size, hidden_size),\n            back_weight=links.Linear(hidden_size, hidden_size),\n            pw=links.Linear(hidden_size, hidden_size),\n            weight_exponential=links.Linear(hidden_size, 1),\n        )\n        self.hidden_size = hidden_size\n\nannotion_list\u304c\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\nback_word_list\u304c\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\np\u304c\u73fe\u5728\u306e\u5c64\u306e\u91cd\u307f\n\n    def __call__(self, annotion_list, back_word_list, p):\n\n\u30d0\u30c3\u30c1\u51e6\u7406\u7528\u306e\u521d\u671f\u5316\n\n        batch_size = p.data.shape[0]\n        exponential_list = []\n        sum_exponential = XP.fzeros((batch_size, 1))\n\nforward\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\u3068back_word\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\u3001\u73fe\u5728\u306e\u5c64\u306e\u72b6\u614b\u3092\u7dcf\u5408\u3057\u305f\u91cd\u307f\u3092\u4f5c\u6210\n\u4e0b\u8a18\u306b\u76f8\u5f53\ne_{ij} = a(s_{i-1}, h_j)\neij=a(si\u22121,hj){e_{ij} = a(s_{i-1}, h_j)\n}\n\u305d\u3053\u3067\u5f97\u3089\u308c\u305f\u5024\u3092exp\u95a2\u6570\u3067\u51e6\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3001\u5404\u5024\u3092\u30ea\u30b9\u30c8\u5316\n\u5408\u8a08\u5024\u3082\u7b97\u51fa\u3057\u307e\u3059\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x}\\exp(e_{ik})} \\\\\n\u03b1ij=exp(eij)\u2211Txk=1exp(eik){\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x}\\exp(e_{ik})} \\\\\n}\n\u4e21\u65b9\u5411\u304b\u3089\u51e6\u7406\u3059\u308b\u305f\u3081\u3001\u524d\u65b9\u5411\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u30ea\u30b9\u30c8\u3068\u5f8c\u308d\u65b9\u5411\u304b\u3089\u306e\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u73fe\u5728\u306e\u91cd\u307f\u3092\u542b\u3081\u3066\u91cd\u307f\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\nexp\u95a2\u6570\u306e\u91cd\u307f\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3002\nexp\u95a2\u6570\u306e\u7dcf\u548c\u3092\u8a08\u7b97\n\n        for annotion, back_word in zip(annotion_list, back_word_list):\n            weight = functions.tanh(self.annotion_weight(annotion) + self.back_weight(back_word) + self.pw(p))\n            exponential = functions.exp(self.weight_exponential(weight))\n            exponential_list.append(exponential)\n            sum_exponential += exponential\n\n\u521d\u671f\u5316\u3092\u884c\u3044\u3001\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u91cd\u307f\u3092\u8a08\u7b97\u3057\u3066\u304a\u304d\u3001\u305d\u306e\u91cd\u307f\u3092\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3067\u884c\u5217\u8a08\u7b97\u3057\u305f\u5024\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u7528\u610f\u3057\u3066\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002functions.batch_matmul\u3067\u884c\u5217\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\na\u304c\u5de6\u884c\u5217\nb\u304c\u53f3\u884c\u5217\ntransa\u304c\u3042\u308b\u3068\u304d\u306f\u5de6\u306e\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u3046\u3002\ntransb\u304c\u3042\u308b\u3068\u304d\u306f\u53f3\u306e\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u3046\ndef batch_matmul(a, b, transa=False, transb=False):\n    return BatchMatMul(transa=transa, transb=transb)(a, b)\n\n\u5b9f\u969b\u306e\u884c\u5217\u8a08\u7b97\u306e\u4e2d\u8eab\n- \u884c\u5217\u3092\u8a08\u7b97\u3067\u304d\u308b\u5f62\u306b\u5909\u63db\n\u3067\u306f\u884c\u5217\u3092\u8981\u7d20\u3054\u3068\u306b\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u5909\u63db\u3057\u3066\u3044\u308b\n\na = a.reshape(a.shape[:2] + (-1,))\n\n\u4e0b\u8a18\u306e\u3088\u3046\u306a\u884c\u5217\u304c\u3042\u3063\u305f\u6642\u306b\narray([[1, 2, 3],\n       [4, 5, 6],\n       [3, 4, 5]])\n\n\u305d\u308c\u3092\u4e0b\u8a18\u306e\u3088\u3046\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3002\narray([[[1],\n        [2],\n        [3]],\n\n       [[4],\n        [5],\n        [6]],\n\n       [[3],\n        [4],\n        [5]]])\n\n\n\u3082\u3057\u8ee2\u7f6e\u304c\u5fc5\u8981\u3067\u3042\u308c\u3070\u51e6\u7406\u3092\u884c\u3046\u3002\n\u7b54\u3048\u3068\u3057\u3066\u8fd4\u3059\u884c\u5217\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u521d\u671f\u5316\nnumpy\u3067\u3042\u308c\u3070\u884c\u5217\u306e\u8981\u7d20\u3054\u3068\u306b\u8a08\u7b97\u3057gpu\u7528\u306ecupy\u3067\u3042\u308c\u3070matmul\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u8a08\u7b97\u3092\u3059\u308b\u3002matmul\u306f\u30b9\u30ab\u30e9\u30fc\u8a08\u7b97\u3092\u8a31\u53ef\u3057\u3066\u304a\u3089\u305a\u3001\u884c\u5217\u3092\u30b9\u30bf\u30c3\u30af\u306b\u7a4d\u3093\u3067\u51e6\u7406\u3092\u884c\u3046\u3002\n\ndef _batch_matmul(a, b, transa=False, transb=False, transout=False):\n    a = a.reshape(a.shape[:2] + (-1,))\n    b = b.reshape(b.shape[:2] + (-1,))\n    trans_axis = (0, 2, 1)\n    if transout:\n        transa, transb = not transb, not transa\n        a, b = b, a\n    if transa:\n        a = a.transpose(trans_axis)\n    if transb:\n        b = b.transpose(trans_axis)\n    xp = cuda.get_array_module(a)\n    if xp is numpy:\n        ret = numpy.empty(a.shape[:2] + b.shape[2:], dtype=a.dtype)\n        for i in six.moves.range(len(a)):\n            ret[i] = numpy.dot(a[i], b[i])\n        return ret\n    return xp.matmul(a, b)\n\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u884c\u5217\u306e\u30b5\u30a4\u30ba\u5206\u3001\u30bc\u30ed\u884c\u5217\u3067\u521d\u671f\u5316\u3092\u3057\u3066\u3001annotion\u3068back_word\u3067\u8a08\u7b97\u3057\u305f\u7dcf\u548c\u3092\u8fd4\u3057\u307e\u3059\u3002 \n\n        ZEROS = XP.fzeros((batch_size, self.hidden_size))\n        annotion_value = ZEROS\n        back_word_value = ZEROS\n        # Calculate the Convolution Value each annotion and back word\n        for annotion, back_word, exponential in zip(annotion_list, back_word_list, exponential_list):\n            exponential /= sum_exponential\n            annotion_value += functions.reshape(functions.batch_matmul(annotion, exponential), (batch_size, self.hidden_size))\n            back_word_value += functions.reshape(functions.batch_matmul(back_word, exponential), (batch_size, self.hidden_size))\n        return annotion_value, back_word_value\n\n\nattention_decoder.py\n\n\u51fa\u529b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u5bfe\u8a71\u306e\u5834\u5408\u306f\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u306a\u308a\u307e\u3059\u3002\n\u5165\u529b\u3088\u308a\u8907\u96d1\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\nembed_vocab\uff1a\u51fa\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3059\u308b\u90e8\u5206\nembed_hidden\uff1a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5024\u3092LSTM\u306b\u4f1d\u642c\u3059\u308b\u90e8\u5206\nhidden_hidden\uff1a\u96a0\u308c\u5c64\u306e\u4f1d\u642c\u90e8\u5206\nannotation_hidden\uff1a\u30d5\u30a9\u30ef\u30fc\u30c9\u578b\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\nback_word_hidden\uff1a\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u578b\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\nhidden_embed\uff1a\u96a0\u308c\u5c64\u304b\u3089\u51fa\u529b\u5c64\uff08\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u5f53\u305f\u308b\uff09\u3078\u306e\u4f1d\u642c\nembded_target\uff1a\u51fa\u529b\u5c64\u304b\u3089\u30b7\u30b9\u30c6\u30e0\u306e\u51fa\u529b\uff08\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u5f53\u305f\u308b\uff09\u3078\u306e\u4f1d\u642c\n        super(AttentionDecoder, self).__init__(\n            embed_vocab=links.EmbedID(vocab_size, embed_size),\n            embed_hidden=links.Linear(embed_size, 4 * hidden_size),\n            hidden_hidden=links.Linear(hidden_size, 4 * hidden_size),\n            annotation_hidden=links.Linear(embed_size, 4 * hidden_size),\n            back_word_hidden=links.Linear(hidden_size, 4 * hidden_size),\n            hidden_embed=links.Linear(hidden_size, embed_size),\n            embded_target=links.Linear(embed_size, vocab_size),\n        )\n\n\n\u51fa\u529b\u5358\u8a9e\u3092\u96a0\u308c\u5c64\u3078\u5199\u50cf\u3057\u3066\u5fae\u5206\u53ef\u80fd\u306a\u53cc\u6975\u95a2\u6570\u3092\u4f7f\u7528\n\u51fa\u529b\u5358\u8a9e\u306e\u96a0\u308c\u5c64\u3001\u96a0\u308c\u5c64\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u7dcf\u548c\u3092lsm\u306b\u4e0e\u3048\u3066\u3001\u72b6\u614b\u3068\u96a0\u308c\u5c64\u3092\u4e88\u6e2c\n\u51fa\u529b\u306e\u305f\u3081\u306e\u96a0\u308c\u5c64\u3092\u5148\u307b\u3069\u4e88\u6e2c\u3057\u305f\u96a0\u308c\u5c64\u3092\u4f7f\u3063\u3066\u5fae\u5206\u53ef\u80fd\u306a\u53cc\u6975\u95a2\u6570\u3067\u4e88\u6e2c\u3059\u308b\n\u51fa\u529b\u306e\u305f\u3081\u306e\u96a0\u308c\u5c64\u3092\u4f7f\u7528\u3057\u3066\u51fa\u529b\u5358\u8a9e\u3092\u4e88\u6e2c\u3001\u73fe\u5728\u306e\u72b6\u614b\u3001\u96a0\u308c\u5c64\u3092\u8fd4\u3059\n        embed = functions.tanh(self.embed_vocab(target))\n        current, hidden = functions.lstm(current, self.embed_hidden(embed) + self.hidden_hidden(hidden) +\n                                         self.annotation_hidden(annotation) + self.back_word_hidden(back_word))\n        embed_hidden = functions.tanh(self.hidden_embed(hidden))\n        return self.embded_target(embed_hidden), current, hidden\n\n\nattention_dialogue.py\n\u5177\u4f53\u7684\u306a\u5bfe\u8a71\u7528\u306e\u51e6\u7406\u3092\u884c\u3046\u90e8\u5206\u3067\u3059\u3002\n\u5148\u307b\u3069\u8aac\u660e\u3057\u305f4\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\nemb\u3067\u5165\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3057\u307e\u3059\u3002\nforward_encode\uff1a\u30d5\u30a9\u30ef\u30fc\u30c9\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u3066\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u7528\u306b\u7528\u610f\u3057\u307e\u3059\u3002\nback_encdode\uff1a\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u3066\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u7528\u306b\u7528\u610f\u3057\u307e\u3059\u3002\nattention\uff1a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u306b\u7528\u610f\ndec\uff1a\u51fa\u529b\u7528\u306e\u5358\u8a9e\u306e\u305f\u3081\u306b\u7528\u610f\n\u8a9e\u5f59\u306e\u30b5\u30a4\u30ba\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3059\u308b\u305f\u3081\u306e\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3001XP\u3067gpu\u3092\u4f7f\u7528\u3059\u308b\u304b\u3069\u3046\u304b\u6c7a\u3081\u3066\u3044\u307e\u3059\u3002\n\n        super(AttentionDialogue, self).__init__(\n            emb=SrcEmbed(vocab_size, embed_size),\n            forward_encode=AttentionEncoder(embed_size, hidden_size),\n            back_encdode=AttentionEncoder(embed_size, hidden_size),\n            attention=Attention(hidden_size),\n            dec=AttentionDecoder(vocab_size, embed_size, hidden_size),\n        )\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.XP = XP\n\n\u521d\u671f\u5316\u3092\u3057\u3066\u52fe\u914d\u3092\u30bc\u30ed\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n    def reset(self):\n        self.zerograds()\n        self.source_list = []\n\n\u5165\u529b\u8a00\u8a9e\uff08\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\uff09\u3092\u5358\u8a9e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\u3002\n\n    def embed(self, source):\n        self.source_list.append(self.emb(source))\n\nencode\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u90e8\u5206\u3067\u3059\u3002\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u53d6\u5f97\u306e\u305f\u3081\u306b\u5165\u529b\u8a00\u8a9e\u306e1\u6b21\u5143\u90e8\u5206\u306e\u307f\u53d6\u3063\u3066\u3044\u307e\u3059\u3002\n\u56f3\n\u521d\u671f\u5316\u3092\u3057\u3066\u3044\u307e\u3059\u304cgpu\u3068cpu\u3067\u521d\u671f\u5316\u306e\u5024\u304c\u7570\u306a\u308b\u306e\u3067self.XP.fzeros\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u4f5c\u6210\u306e\u305f\u3081\u306b\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002\n\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3082\u540c\u69d8\u306e\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n    def encode(self):\n        batch_size = self.source_list[0].data.shape[0]\n        ZEROS = self.XP.fzeros((batch_size, self.hidden_size))\n        context = ZEROS\n        annotion = ZEROS\n        annotion_list = []\n        # Get the annotion list\n        for source in self.source_list:\n            context, annotion = self.forward_encode(source, context, annotion)\n            annotion_list.append(annotion)\n        context = ZEROS\n        back_word = ZEROS\n        back_word_list = []\n        # Get the back word list\n        for source in reversed(self.source_list):\n            context, back_word = self.back_encdode(source, context, back_word)\n            back_word_list.insert(0, back_word)\n        self.annotion_list = annotion_list\n        self.back_word_list = back_word_list\n        self.context = ZEROS\n        self.hidden = ZEROS\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3001\u96a0\u308c\u5c64\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u5c64\u306b\u5165\u308c\u3066\u305d\u308c\u305e\u308c\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30ef\u30fc\u30c9\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08dec\u306b\u3088\u3063\u3066\u5f97\u3089\u308c\u308b\uff09\u3001\u96a0\u308c\u5c64\u306e\u5024\u3001\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u5024\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u5024\u3067\u51fa\u529b\u3057\u305f\u5358\u8a9e\u3092\u8fd4\u3057\u3066\u3044\u308b\u3002\n\n    def decode(self, target_word):\n        annotion_value, back_word_value = self.attention(self.annotion_list, self.back_word_list, self.hidden)\n        target_word, self.context, self.hidden = self.dec(target_word, self.context, self.hidden, annotion_value, back_word_value)\n        return target_word\n\n\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n\u8a9e\u5f59\u30b5\u30a4\u30ba\u3001\u6f5c\u5728\u5c64\u306e\u5199\u50cf\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3092\u4fdd\u5b58\u3057\u3066\u3044\u308b\u3002\n\n    def save_spec(self, filename):\n        with open(filename, 'w') as fp:\n            print(self.vocab_size, file=fp)\n            print(self.embed_size, file=fp)\n            print(self.hidden_size, file=fp)\n\n\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u90e8\u5206\u3002\u3053\u3053\u3067\u8aad\u307f\u8fbc\u3093\u3060\u5024\u3092\u53d6\u5f97\u3057\u3066\u30e2\u30c7\u30eb\u306b\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n    def load_spec(filename, XP):\n        with open(filename) as fp:\n            vocab_size = int(next(fp))\n            embed_size = int(next(fp))\n            hidden_size = int(next(fp))\n        return AttentionDialogue(vocab_size, embed_size, hidden_size, XP)\n\n\nEncoderDecoderModelAttention.py\n\u3053\u306e\u90e8\u5206\u3067\u5b9f\u969b\u5148\u307b\u3069\u8aac\u660e\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\n\u5404\u7a2e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n    def __init__(self, parameter_dict):\n        self.parameter_dict       = parameter_dict\n        self.source               = parameter_dict[\"source\"]\n        self.target               = parameter_dict[\"target\"]\n        self.test_source          = parameter_dict[\"test_source\"]\n        self.test_target          = parameter_dict[\"test_target\"]\n        self.vocab                = parameter_dict[\"vocab\"]\n        self.embed                = parameter_dict[\"embed\"]\n        self.hidden               = parameter_dict[\"hidden\"]\n        self.epoch                = parameter_dict[\"epoch\"]\n        self.minibatch            = parameter_dict[\"minibatch\"]\n        self.generation_limit     = parameter_dict[\"generation_limit\"]\n        self.word2vec = parameter_dict[\"word2vec\"]\n        self.word2vecFlag = parameter_dict[\"word2vecFlag\"]\n        self.model = parameter_dict[\"model\"]\n        self.attention_dialogue   = parameter_dict[\"attention_dialogue\"]\n        XP.set_library(False, 0)\n        self.XP = XP\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u51e6\u7406\u306e\u5b9f\u88c5\u3067\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\u3068\u30bd\u30fc\u30b9\u306e\u30b5\u30a4\u30ba\u3092\u53d6\u5f97\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002\n\n    def forward_implement(self, src_batch, trg_batch, src_vocab, trg_vocab, attention, is_training, generation_limit):\n        batch_size = len(src_batch)\n        src_len = len(src_batch[0])\n        trg_len = len(trg_batch[0]) if trg_batch else 0\n        src_stoi = src_vocab.stoi\n        trg_stoi = trg_vocab.stoi\n        trg_itos = trg_vocab.itos\n        attention.reset()\n\n\u5165\u529b\u8a00\u8a9e\u3092\u9006\u65b9\u5411\u304b\u3089\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\u9006\u65b9\u5411\u304b\u3089\u5165\u529b\u3059\u308b\u3068\u6a5f\u68b0\u7ffb\u8a33\u7d50\u679c\u304c\u4e0a\u304c\u3063\u3066\u3044\u308b\u306e\u3067\u5bfe\u8a71\u3082\u540c\u69d8\u306e\u5f62\u306b\u3057\u3066\u3044\u307e\u3059\u304c\u52b9\u679c\u306f\u306a\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n        x = self.XP.iarray([src_stoi('</s>') for _ in range(batch_size)])\n        attention.embed(x)\n        for l in reversed(range(src_len)):\n            x = self.XP.iarray([src_stoi(src_batch[k][l]) for k in range(batch_size)])\n            attention.embed(x)\n\n        attention.encode()\n\n\u53d6\u5f97\u3057\u305f\u3044\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u8a00\u8a9e\u5217\u3092<s>\u3067\u521d\u671f\u5316\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n        t = self.XP.iarray([trg_stoi('<s>') for _ in range(batch_size)])\n        hyp_batch = [[] for _ in range(batch_size)]\n\n\u5b66\u7fd2\u90e8\u5206\u3067\u3059\u3002\n\u8a00\u8a9e\u60c5\u5831\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u3057\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044\u306e\u3067stoi\u3067\u8a00\u8a9e\u304b\u3089\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\uff08\u3053\u306e\u5834\u5408\u306f\u5bfe\u8a71\u306e\u51fa\u529b\uff09\u3092\u51fa\u3057\u3066\u3001\u6b63\u89e3\u30c7\u30fc\u30bf\u3068\u6bd4\u8f03\u3057\u3066\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\n\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306f\u78ba\u7387\u5206\u5e03\u9593\u306e\u8ddd\u96e2\u3092\u51fa\u3057\u3066\u3044\u308b\u306e\u3067\u3053\u306e\u8a08\u7b97\u306e\u30ed\u30b9\u304c\u5c11\u306a\u3044\u307b\u3069\u51fa\u529b\u7d50\u679c\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u306b\u8fd1\u3065\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002\n\u4eee\u8aac\u5019\u88dc\u3068\u7b97\u51fa\u3057\u305f\u640d\u5931\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n\n        if is_training:\n            loss = self.XP.fzeros(())\n            for l in range(trg_len):\n                y = attention.decode(t)\n                t = self.XP.iarray([trg_stoi(trg_batch[k][l]) for k in range(batch_size)])\n                loss += functions.softmax_cross_entropy(y, t)\n                output = cuda.to_cpu(y.data.argmax(1))\n                for k in range(batch_size):\n                    hyp_batch[k].append(trg_itos(output[k]))\n            return hyp_batch, loss\n\n\u3053\u3053\u306f\u30c6\u30b9\u30c8\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306f\u7121\u9650\u306b\u5019\u88dc\u751f\u6210\u304c\u3067\u304d\u3001\u7279\u306blstm\u306e\u30e2\u30c7\u30eb\u306e\u5834\u5408\u306f\u904e\u53bb\u306e\u72b6\u614b\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u3001\u7121\u9650\u306e\u30eb\u30fc\u30d7\u306b\u5165\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u5236\u9650\u3092\u304b\u3051\u3066\u3044\u307e\u3059\u3002\n\u521d\u671f\u5316\u3057\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5358\u8a9e\u5217\u3092\u5229\u7528\u3057\u3066\u51fa\u529b\u3057\u307e\u3059\u3002\n\u51fa\u529b\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u6700\u5927\u5024\u3092\u51fa\u529b\u3057\u3066t\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u51fa\u529b\u3055\u308c\u305f\u5019\u88dc\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u304b\u3089\u8a00\u8a9e\u60c5\u5831\u306b\u5909\u63db\u3057\u3066\u3044\u307e\u3059\u3002\n\u3059\u3079\u3066\u306e\u5019\u88dc\u304c</s>\u306e\u7d42\u4e86\u8a18\u53f7\u3067\u7d42\u308f\u3063\u3066\u3044\u308b\u5834\u5408\u306b\u51e6\u7406\u3092break\u3059\u308b\u3002\n\n        else:\n            while len(hyp_batch[0]) < generation_limit:\n                y = attention.decode(t)\n                output = cuda.to_cpu(y.data.argmax(1))\n                t = self.XP.iarray(output)\n                for k in range(batch_size):\n                    hyp_batch[k].append(trg_itos(output[k]))\n                if all(hyp_batch[k][-1] == '</s>' for k in range(batch_size)):\n                    break\n\n        return hyp_batch\n\n\u5b66\u7fd2\u5168\u4f53\u306e\u51e6\u7406\u3067\u3059\u3002\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u521d\u671f\u5316\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\nself.vocab\u306f\u5168\u4f53\u306e\u8a9e\u5f59\u3067gens.word_list\u3067\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n        src_vocab = Vocabulary.new(gens.word_list(self.source), self.vocab)\n        trg_vocab = Vocabulary.new(gens.word_list(self.target), self.vocab)\n\nVocabulary.new()\u3067\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u8a9e\u5f59\u60c5\u5831\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\ngens.word_list(self.source)\u3067\u4e0b\u8a18\u306e\u3088\u3046\u306agenerator\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002self.source\u3067\u4e0e\u3048\u308b\u306e\u306f\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308a\u307e\u3059\u3002\ndef word_list(filename):\n    with open(filename) as fp:\n        for l in fp:\n            yield l.split()\n\n\u8a9e\u5f59\u60c5\u5831\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u5909\u63db\u3059\u308b\u51e6\u7406\u306f\u4e0b\u8a18\u306e\u90e8\u5206\u3067\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n<unk>\u306f\u672a\u77e5\u8a9e\u30670\u3001<s>\u306f\u63a5\u982d\u8a9e\u30671\u3001</s>\u306f\u672b\u5c3e\u306e\u8a9e\u53e5\u30672\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\u3042\u3089\u304b\u3058\u3081\u306b\u3053\u308c\u3089\u306b\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u305f\u3081+3\u3092\u3057\u3066\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u4e88\u7d04\u8a9e\u3088\u308a\u5f8c\u306b\u306a\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n    @staticmethod\n    def new(list_generator, size):\n        self = Vocabulary()\n        self.__size = size\n\n        word_freq = defaultdict(lambda: 0)\n        for words in list_generator:\n            for word in words:\n                word_freq[word] += 1\n\n        self.__stoi = defaultdict(lambda: 0)\n        self.__stoi['<unk>'] = 0\n        self.__stoi['<s>'] = 1\n        self.__stoi['</s>'] = 2\n        self.__itos = [''] * self.__size\n        self.__itos[0] = '<unk>'\n        self.__itos[1] = '<s>'\n        self.__itos[2] = '</s>'\n\n        for i, (k, v) in zip(range(self.__size - 3), sorted(word_freq.items(), key=lambda x: -x[1])):\n            self.__stoi[k] = i + 3\n            self.__itos[i + 3] = k\n\n        return self\n\n\n\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3067\u3059\u3002\n\u8a9e\u5f59\u3068\u57cb\u3081\u8fbc\u307f\u5c64\u3068\u96a0\u308c\u5c64\u3068XP\u3092\u4e0e\u3048\u307e\u3059\u3002\nXP\u306fcpu\u8a08\u7b97\u3068gpu\u8a08\u7b97\u3092\u884c\u3046\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\n        trace('making model ...')\n        self.attention_dialogue = AttentionDialogue(self.vocab, self.embed, self.hidden, self.XP)\n\n\u8ee2\u79fb\u5b66\u7fd2\u306e\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u3061\u3089\u3067word2vec\u3067\u4f5c\u6210\u3057\u305f\u91cd\u307f\u3092\u8ee2\u79fb\u3055\u305b\u3066\u3044\u307e\u3059\u3002\nword2vec\u3067\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u306e\u540d\u524dweight_xi\u306a\u306e\u3067\u5165\u529b\u767a\u8a71\u306f\u7d71\u4e00\u3057\u3066\u3044\u307e\u3059\u304c\u51fa\u529b\u767a\u8a71\u306e\u90e8\u5206\u306fembded_target\u3067\u7570\u306a\u308b\u305f\u3081\u4e0b\u8a18\u306e\u51e6\u7406\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n[0]\u306e\u90e8\u5206\u306f\u91cd\u307f\u306e\u540d\u524d\n[1]\u306e\u90e8\u5206\u306f\u5024\u306b\u306a\u308a\u307e\u3059\u3002\n                if dst[\"embded_target\"] and child.name == \"weight_xi\" and self.word2vecFlag:\n                    for a, b in zip(child.namedparams(), dst[\"embded_target\"].namedparams()):\n                        b[1].data = a[1].data\n\n\u91cd\u307f\u306e\u30b3\u30d4\u30fc\u90e8\u5206\u3067\u3059\u3002\n\u5143\u3068\u306a\u308b\u90e8\u5206\u306e\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u56de\u3057\u3001\u6761\u4ef6\u304c\u4e00\u81f4\u3059\u308b\u5834\u5408\u306f\u91cd\u307f\u306e\u30b3\u30d4\u30fc\u3092\u3057\u307e\u3059\u3002\n\u6761\u4ef6\uff11\uff1a\u91cd\u307f\u306b\u547d\u540d\u3055\u308c\u305f\u540d\u524d\u306b\u4e00\u81f4\u3059\u308b\u3082\u306e\u304c\u3042\u308b\u3053\u3068\n\u6761\u4ef6\uff12\uff1a\u91cd\u307f\u306e\u578b\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\n\u6761\u4ef6\uff13\uff1alink.Link\u306e\u90e8\u5206\u3064\u307e\u308a\u30e2\u30c7\u30eb\u90e8\u5206\u307e\u3067\u5230\u9054\u3057\u3066\u3044\u308b\u3053\u3068\n\u6761\u4ef6\uff14\uff1a\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u306e\u884c\u5217\u306e\u9577\u3055\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\n\n    def copy_model(self, src, dst, dec_flag=False):\n        print(\"start copy\")\n        for child in src.children():\n            if dec_flag:\n                if dst[\"embded_target\"] and child.name == \"weight_xi\" and self.word2vecFlag:\n                    for a, b in zip(child.namedparams(), dst[\"embded_target\"].namedparams()):\n                        b[1].data = a[1].data\n                    print('Copy weight_jy')\n            if child.name not in dst.__dict__: continue\n            dst_child = dst[child.name]\n            if type(child) != type(dst_child): continue\n            if isinstance(child, link.Chain):\n                self.copy_model(child, dst_child)\n            if isinstance(child, link.Link):\n                match = True\n                for a, b in zip(child.namedparams(), dst_child.namedparams()):\n                    if a[0] != b[0]:\n                        match = False\n                        break\n                    if a[1].data.shape != b[1].data.shape:\n                        match = False\n                        break\n                if not match:\n                    print('Ignore %s because of parameter mismatch' % child.name)\n                    continue\n                for a, b in zip(child.namedparams(), dst_child.namedparams()):\n                    b[1].data = a[1].data\n                print('Copy %s' % child.name)\n\n\n        if self.word2vecFlag:\n            self.copy_model(self.word2vec, self.attention_dialogue.emb)\n            self.copy_model(self.word2vec, self.attention_dialogue.dec, dec_flag=True)\n\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n            gen1 = gens.word_list(self.source)\n            gen2 = gens.word_list(self.target)\n            gen3 = gens.batch(gens.sorted_parallel(gen1, gen2, 100 * self.minibatch), self.minibatch)\n\n\u4e21\u8005\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u4e0b\u8a18\u3067\u30bf\u30d7\u30eb\u5f62\u5f0f\u3067\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u307e\u3059\u3002\ndef batch(generator, batch_size):\n    batch = []\n    is_tuple = False\n    for l in generator:\n        is_tuple = isinstance(l, tuple)\n        batch.append(l)\n        if len(batch) == batch_size:\n            yield tuple(list(x) for x in zip(*batch)) if is_tuple else batch\n            batch = []\n    if batch:\n        yield tuple(list(x) for x in zip(*batch)) if is_tuple else batch\n\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u3066\u30bd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\n\ndef sorted_parallel(generator1, generator2, pooling, order=1):\n    gen1 = batch(generator1, pooling)\n    gen2 = batch(generator2, pooling)\n    for batch1, batch2 in zip(gen1, gen2):\n        #yield from sorted(zip(batch1, batch2), key=lambda x: len(x[1]))\n        for x in sorted(zip(batch1, batch2), key=lambda x: len(x[order])):\n            yield x\n\n\u6700\u9069\u5316\u306b\u306fAdagrad\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u5358\u7d14\u306b\u66f4\u65b0\u56de\u6570\u304c\u7d2f\u7a4d\u3059\u308b\u307b\u3069\u66f4\u65b0\u5e45\u304c\u5c0f\u3055\u304f\u306a\u308b\u624b\u6cd5\u3067\u3059\u3002\nr \u2190 r + g^2_{\\vec{w}}\\\\\nw \u2190 w - \\frac{\\alpha}{r + }g^2_{\\vec{w}}\nr\u2190r+g2\u2192ww\u2190w\u2212\u03b1r+g2\u2192w{r \u2190 r + g^2_{\\vec{w}}\\\\\nw \u2190 w - \\frac{\\alpha}{r + }g^2_{\\vec{w}}\n}\noptimizer.GradientClipping(5)\u3067L2\u6b63\u5247\u5316\u3092\u4f7f\u7528\u3057\u3066\u52fe\u914d\u304c\u4e00\u5b9a\u306e\u7bc4\u56f2\u5185\u306b\u306a\u308b\u3088\u3046\u306b\u6291\u3048\u3066\u3044\u307e\u3059\u3002\n\n            opt = optimizers.AdaGrad(lr = 0.01)\n            opt.setup(self.attention_dialogue)\n            opt.add_hook(optimizer.GradientClipping(5))\n\n\u4e0b\u8a18\u3067\u5165\u529b\u30e6\u30fc\u30b6\u30fc\u767a\u8a71\u3068\u5bfe\u5fdc\u30e6\u30fc\u30b6\u30fc\u767a\u8a71\u3092fill_batch\u306b\u3088\u308a*\u3067\u7a74\u57cb\u3081\u3057\u3066\u6df1\u5c64\u5b66\u7fd2\u53ef\u80fd\u306a\u5f62\u306b\u3057\u307e\u3059\u3002\ndef fill_batch(batch, token='</s>'):\n    max_len = max(len(x) for x in batch)\n    return [x + [token] * (max_len - len(x) + 1) for x in batch]\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u51e6\u7406\u3067\u5f97\u3089\u308c\u305f\u640d\u5931\u3092\u4f7f\u3063\u3066\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u51e6\u7406\u3092\u884c\u3044\u3001\u91cd\u307f\u306e\u66f4\u65b0\u3092\u884c\u3046\u3002\n\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u51e6\u7406\u306f\u6d3b\u6027\u5316\u95a2\u6570\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3002\n\u66f4\u65b0\u90e8\u5206\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\u30c7\u30fc\u30bf\u3092gpu\u3067\u51e6\u7406\u3059\u308b\u304bcpu\u3067\u51e6\u7406\u3059\u308b\u304b\u5909\u3048\u3066\ntuple\u3001dict\u3001\u305d\u308c\u4ee5\u5916\u3067\u30c7\u30fc\u30bf\u306e\u4e0e\u3048\u65b9\u3092\u5909\u3048\u3066\u3001\u640d\u5931\u95a2\u6570\u306b\u3088\u308b\u6700\u9069\u5316\u3092\u5909\u3048\u3066\u3044\u308b\u3002\n\n    def update_core(self):\n        batch = self._iterators['main'].next()\n        in_arrays = self.converter(batch, self.device)\n\n        optimizer = self._optimizers['main']\n        loss_func = self.loss_func or optimizer.target\n\n        if isinstance(in_arrays, tuple):\n            in_vars = tuple(variable.Variable(x) for x in in_arrays)\n            optimizer.update(loss_func, *in_vars)\n        elif isinstance(in_arrays, dict):\n            in_vars = {key: variable.Variable(x)\n                       for key, x in six.iteritems(in_arrays)}\n            optimizer.update(loss_func, **in_vars)\n        else:\n            in_var = variable.Variable(in_arrays)\n            optimizer.update(loss_func, in_var)\n\n            for src_batch, trg_batch in gen3:\n                src_batch = fill_batch(src_batch)\n                trg_batch = fill_batch(trg_batch)\n                K = len(src_batch)\n                hyp_batch, loss = self.forward_implement(src_batch, trg_batch, src_vocab, trg_vocab, self.attention_dialogue, True, 0)\n                loss.backward()\n                opt.update()\n\n\n\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\nsave\u3068save_spec\u306fchainer\u306e\u6a19\u6e96\u3067\u306f\u5b58\u5728\u305b\u305a\u3001\u8a00\u8a9e\u306b\u95a2\u3059\u308b\u60c5\u5831\u306e\u4fdd\u5b58\u306e\u305f\u3081\u3001\u5225\u306b\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\nsave\u306f\u767a\u8a71\u30c7\u30fc\u30bf\u306e\u60c5\u5831\u3092\u4fdd\u5b58\nsave_spec\u306f\u8a9e\u5f59\u306e\u30b5\u30a4\u30ba\u3084\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u4fdd\u5b58\nsave_hdf5\u306fhdf5\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n\n        trace('saving model ...')\n        prefix = self.model\n        model_path = APP_ROOT + \"/model/\" + prefix\n        src_vocab.save(model_path + '.srcvocab')\n        trg_vocab.save(model_path + '.trgvocab')\n        self.attention_dialogue.save_spec(model_path + '.spec')\n        serializers.save_hdf5(model_path + '.weights', self.attention_dialogue)\n\n\n\u30c6\u30b9\u30c8\u306e\u90e8\u5206\u3067\u3059\u3002\n\u5b66\u7fd2\u306e\u969b\u306b\u51fa\u529b\u3055\u308c\u305f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5165\u529b\u767a\u8a71\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u5185\u5bb9\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\n    def test(self):\n        trace('loading model ...')\n        prefix = self.model\n        model_path = APP_ROOT + \"/model/\" + prefix\n        src_vocab = Vocabulary.load(model_path + '.srcvocab')\n        trg_vocab = Vocabulary.load(model_path + '.trgvocab')\n        self.attention_dialogue = AttentionDialogue.load_spec(model_path + '.spec', self.XP)\n        serializers.load_hdf5(model_path + '.weights', self.attention_dialogue)\n\n        trace('generating translation ...')\n        generated = 0\n\n        with open(self.test_target, 'w') as fp:\n            for src_batch in gens.batch(gens.word_list(self.source), self.minibatch):\n                src_batch = fill_batch(src_batch)\n                K = len(src_batch)\n\n                trace('sample %8d - %8d ...' % (generated + 1, generated + K))\n                hyp_batch = self.forward_implement(src_batch, None, src_vocab, trg_vocab, self.attention_dialogue, False, self.generation_limit)\n\n                source_cuont = 0\n                for hyp in hyp_batch:\n                    hyp.append('</s>')\n                    hyp = hyp[:hyp.index('</s>')]\n                    print(\"src : \" + \"\".join(src_batch[source_cuont]).replace(\"</s>\", \"\"))\n                    print('hyp : ' +''.join(hyp))\n                    print(' '.join(hyp), file=fp)\n                    source_cuont = source_cuont + 1\n\n                generated += K\n\n        trace('finished.')\n\n\n\u307e\u3068\u3081\nPyCon 2016\u3067\u767a\u8868\u3057\u305f\u5185\u5bb9\u3067\u3059\u304c\u3001\u3053\u308c\u3067\u3082\u4e00\u90e8\u3060\u3068\u601d\u3046\u3068\u4ed6\u306e\u90e8\u5206\u306e\u8aac\u660e\u3082\u5408\u308f\u305b\u308b\u3068\u9053\u306e\u308a\u304c\u9577\u305d\u3046\u3067\u3059\u3002\n\u73fe\u72b6\u3067\u5358\u7d14\u306a\u6df1\u5c64\u5b66\u7fd2\u3067\u5bfe\u5fdc\u3067\u304d\u308b\u7bc4\u56f2\u306f\u9650\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u8907\u5408\u7684\u306a\u6280\u8853\u3092\u4f7f\u3063\u3066\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002\n\u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u5bfe\u8a71\u7528\u306e\u30e2\u30c7\u30eb\u304c\u591a\u6570\u51fa\u3066\u304d\u3066\u3044\u308b\u72b6\u6cc1\u306a\u306e\u3067\u8a55\u4fa1\u6307\u6a19\u3092\u78ba\u5b9a\u3057\u3066\u3001\u6df1\u5c64\u5b66\u7fd2\u306e\u30e2\u30c7\u30eb\u3092\u5909\u66f4\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u6027\u80fd\u5411\u4e0a\u306b\u7e4b\u304c\u3063\u3066\u3044\u304f\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u53c2\u8003\nAttention and Memory in Deep Learning and NLP\n# WHY\n\n\u6df1\u5c64\u5b66\u7fd2\u306b\u3064\u3044\u3066\u8208\u5473\u304c\u3042\u308b\u65b9\u304c\u591a\u3044\u3068\u601d\u3046\u306e\u3067\u5bfe\u8a71\u306b\u304a\u3051\u308b\u6df1\u5c64\u5b66\u7fd2\u306e\u5b9f\u88c5\u306b\u3064\u3044\u3066\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\n\u96d1\u8ac7\u5fdc\u7b54\u304cChainer\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u305d\u306e\u90e8\u5206\u306b\u30d5\u30a9\u30fc\u30ab\u30b9\u3057\u3066\u8aac\u660e\u3057\u307e\u3059\u3002\u305f\u3060\u3057\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u53e4\u3044\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n\u52d5\u4f5c\u78ba\u8a8d\u3057\u3066\u3044\u308b\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.5.1\u3067\u3059\n\n\u9593\u9055\u3044\u304c\u3042\u308b\u90e8\u5206\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u6df1\u304f\u7406\u89e3\u3057\u305f\u3044\u90e8\u5206\u304c\u3042\u3063\u305f\u306e\u3067\u4e00\u90e8Chainer\u306e\u30b3\u30fc\u30c9\u3092\u8ffd\u3063\u3066\u3044\u307e\u3059\u3002\u9593\u9055\u3044\u304c\u3042\u308c\u3070\u5927\u5909\u304a\u624b\u6570\u3067\u3059\u304c\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\n\n- PyCon 2016\u3067\u767a\u8868\u3057\u305f\u5185\u5bb9\u306f\u3069\u3061\u3089\u304b\u3068\u3044\u3046\u3068\u30b3\u30f3\u30bb\u30d7\u30c8\u3084\u6982\u8981\u30d9\u30fc\u30b9\u3092\u4f1d\u3048\u3066\u9802\u3051\u306a\u306e\u3067\u5b9f\u969b\u306b\u5b9f\u88c5\u3057\u3066\u3044\u308b\u30b3\u30fc\u30c9\u306e\u8aac\u660e\u304c\u306a\u3044\u306e\u3067\u3001\u81ea\u8eab\u306e\u632f\u308a\u8fd4\u308a\u3068\u3044\u3046\u610f\u5473\u3067\u3082\u3042\u3063\u305f\u65b9\u304c\u826f\u3044\n\n- \u305d\u3053\u3067\u30b3\u30fc\u30c9\u306e\u8aac\u660e\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u3082\u3063\u3068\u7406\u89e3\u3057\u3066\u4f7f\u3063\u3066\u3082\u3089\u3048\u308b\u4eba\u304c\u5897\u3048\u3066\u6b32\u3057\u3044\u3068\u601d\u3044\u3001\u3053\u306e\u8a18\u4e8b\u3092\u66f8\u304d\u307e\u3057\u305f\u3002(\u3067\u304d\u308c\u3070github\u306e\u30b9\u30bf\u30fc\u304c\u5897\u3048\u308b\u3068\u5b09\u3057\u3044)\n\n<img width=\"973\" alt=\"Screen Shot 2016-12-26 at 8.04.52 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/0a19e462-b5be-c76f-0536-07d06ec8a3a8.png\">\n\nDocker Hub\n\nhttps://hub.docker.com/r/masayaresearch/dialogue/\n\ngithub\n\nhttps://github.com/SnowMasaya/Chainer-Slack-Twitter-Dialogue\n\n\u4ed6\u306b\u3082\u8cea\u554f\u5fdc\u7b54\u3001\u8a71\u984c\u5206\u985e\u3001\u30c7\u30fc\u30bf\u53d6\u5f97\u306e\u4e26\u5217\u5316\u306a\u3069\u591a\u5c90\u306b\u6e21\u3063\u3066\u3044\u308b\u306e\u3067\u305d\u306e\u90e8\u5206\u3082\u8981\u671b\u304c\u3042\u308c\u3070\u66f8\u304d\u307e\u3059\u3002\n\n# WHAT\n\n## \u96d1\u8ac7\u5fdc\u7b54\n \n\u5206\u985e\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u6df1\u5c64\u5b66\u7fd2\u306e\u4e2d\u3067\u3082\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3068\u306f\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6a5f\u68b0\u7ffb\u8a33\u306e\u30bf\u30b9\u30af\u306b\u304a\u3044\u3066\u30b7\u30fc\u30b1\u30f3\u30b9 to \u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u30e2\u30c7\u30eb\u3067\u306f\u9577\u6587\u306e\u5165\u529b\u306b\u304a\u3044\u3066\u4e00\u3064\u306e\u30d9\u30af\u30c8\u30eb\u306b\u96c6\u7d04\u3055\u308c\u308b\u3068\u6700\u521d\u306e\u5358\u8a9e\u306e\u91cd\u8981\u6027\u304c\u5fae\u5206\u306e\u96c6\u7a4d\u306b\u3088\u3063\u3066\u8584\u308c\u3066\u304f\u308b\u554f\u984c\u304c\u3042\u308a\u307e\u3057\u305f\u3002\u82f1\u8a9e\u306e\u5834\u5408\u306f\u7279\u306b\u6700\u521d\u306e\u5358\u8a9e\u306e\u91cd\u8981\u6027\u304c\u5897\u3057\u3066\u304d\u307e\u3059\u3002\n\n\u305d\u308c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u3001\u5f93\u6765\u3067\u306f\u9006\u65b9\u5411\u306e\u5165\u529b\u3092\u5165\u308c\u3066\u7ffb\u8a33\u7cbe\u5ea6\u3092\u4e0a\u3052\u3066\u3044\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u65e5\u672c\u8a9e\u3084\u4e2d\u56fd\u8a9e\u306e\u5834\u5408\u306f\u9006\u306b\u6700\u5f8c\u306e\u5358\u8a9e\u304c\u91cd\u8981\u306b\u306a\u308b\u305f\u3081\u672c\u8cea\u7684\u306a\u89e3\u6c7a\u306b\u306a\u308a\u307e\u305b\u3093\u3002\n\n\u305d\u3053\u3067\u5165\u529b\u306e\u30a8\u30f3\u30b3\u30fc\u30c9\u3068\u30c7\u30b3\u30fc\u30c9\u3092\u5206\u3051\u3066\u884c\u308f\u305a\u306b\u30c7\u30b3\u30fc\u30c9\u3068\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u96a0\u308c\u5c64\u3068\u30a8\u30f3\u30b3\u30fc\u30c9\u306e\u5165\u529b\u3092\u52a0\u91cd\u5e73\u5747\u3057\u3066\u5404\u30c7\u30b3\u30fc\u30c9\u3054\u3068\u306e\u51fa\u529b\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3068\u3057\u3066\u63d0\u6848\u3055\u308c\u305f\u306e\u304c\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u3067\u3059\u3002\u3082\u3068\u3082\u3068\u306f\u753b\u50cf\u306e\u5206\u91ce\u3067\u6210\u679c\u3092\u51fa\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u73fe\u5728\u306f\u6a5f\u68b0\u7ffb\u8a33\u3084\u6587\u7ae0\u8981\u7d04\u306e\u30bf\u30b9\u30af\u3067\u6210\u679c\u3092\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n\u30a4\u30e1\u30fc\u30b8\n\n\n<img width=\"500\" alt=\"image.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/6e79415d-298a-af1c-25d1-c1dfe39faec9.png\">\n\n<img width=\"500\" alt=\"image.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/e3fa7777-2f1f-ad65-f63d-3baf1d42cdf6.png\">\n\n\n\"\u3082\"\u3092\u4e88\u6e2c\u3059\u308b\u306b\u306f\u201d\u50d5\u201d\u3068\u5165\u529b\uff08\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\uff09\u304c\u5f97\u3089\u308c\u305f\u6642\u306e\u4e8b\u5f8c\u78ba\u7387\u306b\u306a\u308a\u307e\u3059\u3002\n\u4e8b\u5f8c\u78ba\u7387\u306f\u4e00\u3064\u524d\u306e\u5358\u8a9e\uff08\u50d5\uff09\u3068\u96a0\u308c\u5c64\u306e\u72b6\u614b\u3068\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb(\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\")\u306e\u30b9\u30b3\u30a2\u306b\u306a\u308a\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306f\u4eca\u306f\u7121\u8996\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3042\u3068\u3067\u89e3\u8aac\u3092\u3057\u307e\u3059\u3002\n\u95a2\u6570g\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u304c\u4e00\u822c\u7684\u3067\u3059\n\n<img width=\"216\" alt=\"Screen Shot 2016-12-26 at 9.38.00 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/6ae30f7d-7c53-6a1f-40df-83836fcf306f.png\">\n\n\u4e0a\u56f3\u306e\u3088\u3046\u306b\u4e8b\u524d\u306e\u51fa\u529b\u3092\u8003\u616e\u3057\u3066\u3001\u73fe\u5728\u306e\u72b6\u614b\u3068\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u4e88\u6e2c\u3059\u308b\u5834\u5408\u306b\u4f7f\u7528\u3059\u308b\u6570\u5f0f\u306f\u4e0b\u8a18\u3067\u3059\u3002\n\n```math\np(y_i|y_1,...y_{i_1}, \\vec{x}) = g(y_{i-1}, s_i, c_i)\n```\n\n\u3053\u3053\u3067\u96a0\u308c\u5c64\u306e\u6642\u523bt\u306e\u72b6\u614b\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3067\u304d\u308b\u3002\uff08\"\u3082\"\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306e\u72b6\u614b\uff09\n\u3053\u308c\u306f\u4e00\u3064\u524d\u306e\u5358\u8a9e\"\u50d5\"\u3068\u4e00\u3064\u524d\u306e\u72b6\u614b\u3068\u4e00\u3064\u524d\u306e\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306b\u3088\u3063\u3066\u6c7a\u307e\u308a\u307e\u3059\u3002\n\u95a2\u6570f\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u304c\u4e00\u822c\u7684\n\n```math\ns_i=f(s_{i-1}, y_{i-1},c_i)\n```\n\n\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\uff08\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\u201d\uff09\u306e\u96a0\u308c\u5c64\u3068\u91cd\u307f$a$\u306e\u7dcf\u548c\u3067\u6c7a\u307e\u308a\u307e\u3059\u3002\n\n```math\nc_{i} = \\sum^{T_x}_{j=1}\\alpha_{ij}h_{j}\n```\n\n\u3067\u306f\u5148\u307b\u3069\u5b9a\u7fa9\u3057\u305f\u91cd\u307f\u3092\u3069\u306e\u3088\u3046\u306b\u6c42\u3081\u308b\u304b\u3067\u3059\u304c\u3001e\u3068\u3044\u3046\u96a0\u308c\u5c64h\u3068\u51fa\u529b\u5074\u306e\u4e00\u3064\u524d\u306e\u72b6\u614bs(\"\u306f\u201d\u306e\u5834\u5408\u306f\"\u50d5\")\u304b\u3089\u5f97\u3089\u308c\u308b\u91cd\u307f\u3092\u30b9\u30b3\u30a2\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u3088\u3046\u306a\u5f62\u306f\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\u306eh\u304c\u7279\u6b8a\u306a\u5f62\u306b\u306a\u308b\u305f\u3081\u3067\u3059\u3002\u3053\u306e\u70b9\u306f\u5f8c\u8ff0\u3057\u307e\u3059\u3002\ne\u3068\u3044\u3046\u30b9\u30b3\u30a2\u306f\u78ba\u7387\u306e\u305f\u3081\u5c0f\u3055\u3044\u5024\u306b\u306a\u308a\u307e\u3059\u3002\u305d\u308c\u3092exp\u95a2\u6570\u306b\u3088\u3063\u3066\u5927\u304d\u3044\u5024\u306b\u3057\u3066\u5165\u529b\u90e8\u5168\u3066\u3067\u5272\u308a\u7b97\u3092\u3057\u3066\u5165\u529b\u3068\u51fa\u529b\u306e\u30da\u30a2\u306b\u30de\u30c3\u30c1\u3057\u305f\u91cd\u307f\u3092\u7b97\u51fa\u3057\u3066\u3044\u307e\u3059\u3002\n\n```math\n\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})} \\\\\ne_{ij} = a(s_{i-1}, h_j)\n```\n\n\u3067\u306f\u96a0\u308c\u5c64h\u306f\u3069\u306e\u3088\u3046\u306a\u70b9\u304c\u7279\u6b8a\u306a\u306e\u3067\u3057\u3087\u3046\u304b\u3002\n\u5b9f\u306f\u30d5\u30a9\u30ef\u30fc\u30c9\u3068\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3092\u5408\u308f\u305b\u3066\u3044\u308b\u70b9\u304c\u901a\u5e38\u306e\u30b7\u30fc\u30b1\u30f3\u30b9 to \u30b7\u30fc\u30b1\u30f3\u30b9\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002\n\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30d5\u30a9\u30ef\u30fc\u30c9\u3068\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3092\u5b9a\u7fa9\u3057\u3066\u3001\u305d\u308c\u3092\u30b3\u30f3\u30ab\u30c1\u3057\u3066\u8868\u3057\u307e\u3059\u3002\u3053\u308c\u304c\u30a8\u30f3\u30b3\u30fc\u30c9\u5165\u529b\"\u79c1 \u306f \u30a8\u30f3\u30b8\u30cb\u30a2 \u3060\"\u306e\u96a0\u308c\u5c64\u306b\u306a\u308a\u307e\u3059\u3002\n\n```math\n(\\vec{h_1},...\\vec{h_{T_x}})\\\\\n(\\overleftarrow{h_1},...\\overleftarrow{h_{T_x}})\\\\\nh_j = [\\vec{h_j^T};\\overleftarrow{h_j^T}]^T\n```\n\n\u3067\u306f\u5b9f\u969b\u306b\u30b3\u30fc\u30c9\u30d9\u30fc\u30b9\u3067\u3053\u306e\u6570\u5f0f\u3092\u3069\u306e\u3088\u3046\u306b\u5b9f\u73fe\u3057\u3066\u3044\u308b\u304b\u8ffd\u3063\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\n- src_embed.py\n - \u8a00\u8a9e\u306e\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u79fb\u3059\u90e8\u5206\u3067\u3059\u3002 \n- attention_encoder.py\n - \u5165\u529b\u5074\u8a00\u8a9e\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u79fb\u3055\u308c\u305f\u60c5\u5831\u3092\u4f1d\u642c\u3059\u308b\u90e8\u5206\u3067\u3059\u3002\uff08\u5bfe\u8a71\u3067\u8a00\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u90e8\u5206\u306b\u3042\u305f\u308a\u307e\u3059\uff09\n- attention.py\n - \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u4f5c\u6210\u3059\u308b\u90e8\u5206\n- attention_decoder.py\n - \u51fa\u529b\u8a00\u8a9e\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u8a00\u8a9e\u306e\u51fa\u529b\u3001\u96a0\u308c\u5c64\u306e\u4f1d\u642c\u307e\u3067\u884c\u3046\u3002\n\n- attention_dialogue.py\n  - \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\n  - \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n  - \u91cd\u307f\u306e\u521d\u671f\u5316\n  - \u91cd\u307f\u306e\u57cb\u3081\u8fbc\u307f\n  - \u30a8\u30f3\u30b3\u30fc\u30c9\u51e6\u7406\n  - \u30c7\u30b3\u30fc\u30c9\u51e6\u7406 \n\n\u4e0a\u8a18\u306e5\u3064\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u3002\n\n# HOW\n\n## src_embed.py\n\n<img width=\"216\" alt=\"Screen Shot 2016-12-26 at 9.38.00 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/1e1c53f2-052d-1d02-4cda-a3b33cbe8bef.png\">\n\n\u5165\u529b\u8a00\u8a9e\u306e\u60c5\u5831\u3092\u57cb\u3081\u8fbc\u3080\u90e8\u5206\u304b\u3089\u89e3\u8aac\u3057\u307e\u3059\u3002\n\u5165\u529b\u8a00\u8a9e\u306e\u8a9e\u5f59\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u6570\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\n\u5165\u529b\u8a00\u8a9e\u306e\u8a9e\u5f59\u306f\u5bfe\u8a71\u306e\u5834\u5408\u306f\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\n    def __init__(self, vocab_size, embed_size):\n        super(SrcEmbed, self).__init__(\n            weight_xi=links.EmbedID(vocab_size, embed_size),\n        )\n```\n\n\u5177\u4f53\u7684\u306a\u51e6\u7406\u306e\u4e2d\u8eab\u306b\u306a\u308a\u307e\u3059\u3002\n`W (~chainer.Variable)`\u306fchainer.Variable\u306e\u57cb\u3081\u8fbc\u307f\u884c\u5217\u306b\u306a\u308a\u307e\u3059\u3002\n\u5e73\u57470\u3001\u5206\u65631.0\u306e\u6b63\u898f\u5206\u5e03\u304b\u3089\u751f\u6210\u3055\u308c\u308b\u521d\u671f\u91cd\u307f\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```py\n    def __init__(self, in_size, out_size, initialW=None, ignore_label=None):\n        super(EmbedID, self).__init__(W=(in_size, out_size))\n        if initialW is None:\n            initialW = initializers.Normal(1.0)\n        initializers.init_weight(self.W.data, initialW)\n        self.ignore_label = ignore_label\n```\n\n\u5177\u4f53\u7684\u306b\u6b63\u898f\u5206\u5e03\u304b\u3089\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u308b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n`xp`\u306e\u90e8\u5206\u306f`gpu`\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306a\u306e\u3067`numpy.random.normal`\u3067\u306a\u304f`xp.random.normal`\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u53c2\u8003\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html\n\n```py\nclass Normal(initializer.Initializer):\n    def __init__(self, scale=0.05, dtype=None):\n        self.scale = scale\n        super(Normal, self).__init__(dtype)\n\n    def __call__(self, array):\n        xp = cuda.get_array_module(array)\n        array[...] = xp.random.normal(\n             loc=0.0, scale=self.scale, size=array.shape)\n```\n\n\u3053\u3053\u3067\u8fd4\u5374\u3059\u308b\u521d\u671f\u306e\u91cd\u307f\u306f\u4e0b\u8a18\u3067\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n`initializer`\u306e\u30c7\u30fc\u30bf\u306f`numpy.ndarray`\u306e\u30c7\u30fc\u30bf\u304b\u30af\u30e9\u30b9\u3082\u3057\u304f\u306f`cupy.ndarray`\u306e\u30af\u30e9\u30b9\u3067\u8a2d\u5b9a\u3055\u308c\u307e\u3059\u3002\n\n\n```py\ndef init_weight(weights, initializer, scale=1.0):\n\n    if initializer is None:\n        initializer = HeNormal(1 / numpy.sqrt(2))\n    elif numpy.isscalar(initializer):\n        initializer = Constant(initializer)\n    elif isinstance(initializer, numpy.ndarray):\n        initializer = Constant(initializer)\n\n    assert callable(initializer)\n    initializer(weights)\n    weights *= scale\n```\n\n`initializer`\u304c`None`\u4ee5\u5916\u306e\u5834\u5408\u306bgpu\u5f62\u5f0f\u306earray\u304b\u901a\u5e38\u306earray\u304b\u3092\u8fd4\u5374\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\nclass Constant(initializer.Initializer):\n\n    def __init__(self, fill_value, dtype=None):\n        self.fill_value = fill_value\n        super(Constant, self).__init__(dtype)\n\n    def __call__(self, array):\n        if self.dtype is not None:\n            assert array.dtype == self.dtype\n        xp = cuda.get_array_module(array)\n        array[...] = xp.asarray(self.fill_value)\n```\n\n\u5177\u4f53\u7684\u306b\u5224\u5b9a\u3057\u3066\u8fd4\u5374\u3057\u3066\u3044\u308b\u90e8\u5206\u306f\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\ndef get_array_module(*args):\n    if available:\n        return cupy.get_array_module(*args)\n    else:\n        return numpy\n```\n\n\n`__call__`\u95a2\u6570\u306b\u3088\u3063\u3066src_embed\u3092\u547c\u3073\u51fa\u3057\u3066\u5165\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u57cb\u3081\u8fbc\u3093\u3067\u3044\u307e\u3059\u3002\n`functions.tanh`\u3067\u53cc\u6975\u95a2\u6570\u3092\u7528\u3044\u3066\u5fae\u5206\u53ef\u80fd\u306a\u7a7a\u9593\u306b\u5199\u50cf\u3057\u3066\u3044\u307e\u3059\u3002\u5fae\u5206\u53ef\u80fd\u306a\u7a7a\u9593\u3067\u3042\u308c\u3070\u8aa4\u5dee\u9006\u4f1d\u642c\u306b\u3088\u308a\u5b66\u7fd2\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\n    def __call__(self, source):\n        return functions.tanh(self.weight_xi(source))\n```\n\n## attention_encoder.py\n\n<img width=\"216\" alt=\"Screen Shot 2016-12-26 at 9.38.00 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/3e93ba75-b699-0a87-b9fb-d9738034690d.png\">\n\n\u5148\u307b\u3069\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3055\u308c\u305f\u5165\u529b\u5c64\u3092\u96a0\u308c\u5c64\u306b\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\u306a\u305c4\u500d\u304b\n\n\u5165\u529b\u30b2\u30fc\u30c8\n\u5fd8\u5374\u30b2\u30fc\u30c8\n\u51fa\u529b\u30b2\u30fc\u30c8\n\u4ee5\u524d\u306e\u5165\u529b\u3092\u8003\u616e\u3059\u308b\u30b2\u30fc\u30c8\n\n\u4e0a\u8a18\u306e4\u3064\u3092\u8003\u616e\u3057\u3066\u3044\u308b\u305f\u3081\u3067\u3059\u3002\n\u306a\u305c\u305d\u308c\u304c\u5fc5\u8981\u304b\u306f\u4ed6\u306e\u8cc7\u6599\u3067\u3082\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u8a73\u7d30\u306f\u8ff0\u3079\u307e\u305b\u3093\u304c\u3001\u3053\u306e\u5de5\u592b\u306b\u3088\u3063\u3066\u904e\u5b66\u7fd2\u3092\u9632\u3044\u3067\u3044\u307e\u3059\u3002\n\n```py\n    def __init__(self, embed_size, hidden_size):\n        super(AttentionEncoder, self).__init__(\n            source_to_hidden=links.Linear(embed_size, 4 * hidden_size),\n            hidden_to_hidden=links.Linear(hidden_size, 4 * hidden_size),\n        )\n```\n\n\u5177\u4f53\u7684\u306alinks.Liner\u306e\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\n\n- \u91cd\u307f\u306e\u521d\u671f\u5316\n- \u91cd\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u4e0e\n- \u30d0\u30a4\u30a2\u30b9\u306e\u521d\u671f\u5316\n- \u30d0\u30a4\u30a2\u30b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u4e0e\n\n```py\n    def __init__(self, in_size, out_size, wscale=1, bias=0, nobias=False,\n                 initialW=None, initial_bias=None):\n        super(Linear, self).__init__()\n\n        self.initialW = initialW\n        self.wscale = wscale\n\n        self.out_size = out_size\n        self._W_initializer = initializers._get_initializer(initialW, math.sqrt(wscale))\n\n        if in_size is None:\n            self.add_uninitialized_param('W')\n        else:\n            self._initialize_params(in_size)\n\n        if nobias:\n            self.b = None\n        else:\n            if initial_bias is None:\n                initial_bias = bias\n            bias_initializer = initializers._get_initializer(initial_bias)\n            self.add_param('b', out_size, initializer=bias_initializer)\n\n    def _initialize_params(self, in_size):\n        self.add_param('W', (self.out_size, in_size), initializer=self._W_initializer)\n```\n\n\u5177\u4f53\u7684\u306a\u521d\u671f\u5316\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\nscale\u304c\u30c7\u30d5\u30a1\u30eb\u30c8\u30671\u306a\u306e\u3067\u305d\u308c\u3092\u304b\u3051\u3066Array\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\u5148\u307b\u3069\u51fa\u3066\u304d\u305f`Constant`\u3067\u521d\u671f\u5024\u3092\u56fa\u5b9a\u5024\u3067\u521d\u671f\u5316\u3057\u3066scale\u3092\u304b\u3051\u3066\u3044\u307e\u3059\u3002\n\n```py\nclass _ScaledInitializer(initializer.Initializer):\n\n    def __init__(self, initializer, scale=1.0):\n        self.initializer = initializer\n        self.scale = scale\n        dtype = getattr(initializer, 'dtype', None)\n        super(Identity, self).__init__(dtype)\n\n    def __call__(self, array):\n        self.initializer(array)\n        array *= self.scale\n\n\ndef _get_initializer(initializer, scale=1.0):\n    if initializer is None:\n        return HeNormal(scale / numpy.sqrt(2))\n    if numpy.isscalar(initializer):\n        return Constant(initializer * scale)\n    if isinstance(initializer, numpy.ndarray):\n        return Constant(initializer * scale)\n\n    assert callable(initializer)\n    if scale == 1.0:\n        return initializer\n    return _ScaledInitializer(initializer, scale)\n```\n\n\u73fe\u5728\u306e\u72b6\u614b\u3068\u524d\u56de\u306e\u96a0\u308c\u5c64\u306e\u5024\u3001\u5165\u529b\u5c64\u306e\u5024\u3092\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n    def __call__(self, source, current, hidden):\n        return functions.lstm(current, self.source_to_hidden(source) + self.hidden_to_hidden(hidden))\n```\n\n\u4e0a\u8a18\u306elstm\u3067forward\u306e\u51e6\u7406\u306e\u969b\u306b\u547c\u3070\u308c\u3066\u3044\u308b\u51e6\u7406\u304c\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\n\u30d5\u30a1\u30a4\u30eb\u306f`chainer/functions/activation/lstm.py`\u306b\u306a\u308a\u307e\u3059\u3002\n\u5165\u529b\u3092lstm\u306e4\u3064\u306e\u30b2\u30fc\u30c8\u306b\u5206\u3051\u3066\u3044\u307e\u3059\u3002\n`len(x)`:\u884c\u306e\u9577\u3055\u306e\u53d6\u5f97\n`x.shape[1]`\uff1a\u5217\u306e\u9577\u3055\u306e\u53d6\u5f97\n`x.shape[2:]`:3\u6b21\u5143\u4ee5\u4e0a\u306e\u30c7\u30fc\u30bf\u306e\u5834\u5408\u306b\u4f7f\u7528\n\n```py\n    def _extract_gates(x):\n        r = x.reshape((len(x), x.shape[1] // 4, 4) + x.shape[2:])\n        return [r[:, :, i] for i in six.moves.range(4)]\n```\n\ncpu\u306e\u51e6\u7406\n\n- \u5165\u529b\u3068\u72b6\u614b\u3092\u53d6\u5f97\n- lstm\u306b\u57fa\u3065\u3044\u30664\u3064\u306e\u60c5\u5831\u3092\u53d6\u5f97\n- lstm\u306e\u5024\u306f\u8ad6\u6587\u306b\u6e96\u62e0\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u4ee5\u5916\u306f\u5165\u529b\u304c\u306f\u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\u3067\u5b9f\u73fe\u53ef\u80fd\n- \u6b21\u306e\u72b6\u614b\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u521d\u671f\u5316\n- \u5165\u529b\u3068\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u7a4d\u3068\u5fd8\u5374\u3068\u904e\u53bb\u306e\u72b6\u614b\u306e\u7a4d\u306e\u8a71\u306b\u3088\u3063\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u3001\u6b21\u306e\u72b6\u614b\u3092\u4ed8\u4e0e\n- \u96a0\u308c\u5c64\u306e\u5024\u306f\u51fa\u529b\u3068\u6b21\u306e\u72b6\u614b\u306e\u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\u3067\u4e0e\u3048\u308b\n\ngpu\u306e\u51e6\u7406\u306f\u540c\u4e00\u305f\u3060\u3057\u3001C++\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u4e0b\u8a18\u306e\u5b9a\u7fa9\u3092\u4f7f\u7528\u3057\u3066\u8aad\u3093\u3067\u3044\u308b\u3002python\u4e0a\u3067\u5b9a\u7fa9\u3057\u3066\u3044\u308blstm\u3067\u540c\u4e00\u3060\u304cC++\u3067\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u3002\n\n```\n_preamble = '''\ntemplate <typename T> __device__ T sigmoid(T x) {\n    const T half = 0.5;\n    return tanh(x * half) * half + half;\n}\ntemplate <typename T> __device__ T grad_sigmoid(T y) { return y * (1 - y); }\ntemplate <typename T> __device__ T grad_tanh(T y) { return 1 - y * y; }\n#define COMMON_ROUTINE \\\n    T aa = tanh(a); \\\n    T ai = sigmoid(i_); \\\n    T af = sigmoid(f); \\\n    T ao = sigmoid(o);\n'''\n\n```\n\n```py\n    def forward(self, inputs):\n        c_prev, x = inputs\n        a, i, f, o = _extract_gates(x)\n        batch = len(x)\n\n        if isinstance(x, numpy.ndarray):\n            self.a = numpy.tanh(a)\n            self.i = _sigmoid(i)\n            self.f = _sigmoid(f)\n            self.o = _sigmoid(o)\n\n            c_next = numpy.empty_like(c_prev)\n            c_next[:batch] = self.a * self.i + self.f * c_prev[:batch]\n            h = self.o * numpy.tanh(c_next[:batch])\n        else:\n            c_next = cuda.cupy.empty_like(c_prev)\n            h = cuda.cupy.empty_like(c_next[:batch])\n            cuda.elementwise(\n                'T c_prev, T a, T i_, T f, T o', 'T c, T h',\n                '''\n                    COMMON_ROUTINE;\n                    c = aa * ai + af * c_prev;\n                    h = ao * tanh(c);\n                ''',\n                'lstm_fwd', preamble=_preamble)(\n                    c_prev[:batch], a, i, f, o, c_next[:batch], h)\n\n        c_next[batch:] = c_prev[batch:]\n        self.c = c_next[:batch]\n        return c_next, h\n```\n\ngpu\u306e\u51e6\u7406\u306f\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\ncuda\u306e\u4e2d\u8eab\u3092\u547c\u3076\u305f\u3081\u306e\u51e6\u7406\u304c\u4e0b\u8a18\u3002\ncupy\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3002\ncupy\u306b\u3064\u3044\u3066\n\nhttp://docs.chainer.org/en/stable/cupy-reference/overview.html\n\n\u4e0b\u8a18\u3067\u30ab\u30fc\u30cd\u30eb\u95a2\u6570\u3092\u4f5c\u6210\u3057\u3001cuda\u306e\u30e1\u30e2\u30ea\u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3057\u3066\u7d50\u679c\u3092cuda\u306e\u30c7\u30d0\u30a4\u30b9\u3068\u9023\u643a\u3057\u307e\u3059\u3002\ngpu\u306e\u30e1\u30e2\u30ea\u7a7a\u9593\u3067\u8a08\u7b97\u3057\u305f\u5024\u3092\u9023\u643a\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u7406\u7531\u306f\u4e0b\u8a18\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\n\nhttp://www.nvidia.com/docs/io/116711/sc11-cuda-c-basics.pdf\n\n```py\n@memoize(for_each_device=True)\ndef elementwise(in_params, out_params, operation, name, **kwargs):\n    check_cuda_available()\n    return cupy.ElementwiseKernel(\n        in_params, out_params, operation, name, **kwargs)\n```\n\nbackword\u306e\u969b\u306f\u4e0b\u8a18\u306e\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\nchainer\u306f\u3053\u3053\u3089\u8fba\u306e\u51e6\u7406\u3092\u96a0\u3057\u3066\u304f\u308c\u308b\u306e\u3067\u52a9\u304b\u308a\u307e\u3059\u3002\nforward\u306e\u51e6\u7406\u3068\u540c\u3058\u3067\u3059\u304c\u3001\u9055\u3044\u306f\u5165\u529b\u3060\u3051\u3067\u306a\u304f\u52fe\u914d\u306e\u51fa\u529b\u3082\u4f7f\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n`gc_prev[:batch]`\u3067\u96a0\u308c\u5c64\u3068\u51fa\u529b\u5c64\u306e\u7a4d\u306b\u52fe\u914d\u3092\u8db3\u3057\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u66f4\u65b0\u3057\u3066\u3044\u307e\u3059\u3002\n`_grad_tanh`\u3068`_grad_sigmoid`\u3067\u52fe\u914d\u3092\u8a08\u7b97\u3057\u66f4\u65b0\u3057\u3066\u3044\u308b\u3002\n\n\n```py\n            co = numpy.tanh(self.c)\n            gc_prev = numpy.empty_like(c_prev)\n            # multiply f later\n            gc_prev[:batch] = gh * self.o * _grad_tanh(co) + gc_update\n            gc = gc_prev[:batch]\n            ga[:] = gc * self.i * _grad_tanh(self.a)\n            gi[:] = gc * self.a * _grad_sigmoid(self.i)\n            gf[:] = gc * c_prev[:batch] * _grad_sigmoid(self.f)\n            go[:] = gh * co * _grad_sigmoid(self.o)\n            gc_prev[:batch] *= self.f  # multiply f here\n            gc_prev[batch:] = gc_rest\n```\n\ngpu\u306e\u51e6\u7406\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\ncpu\u306e\u51e6\u7f6e\u3068\u540c\u4e00\u3067\u3059\u304cC++\u3092\u901a\u3059\u305f\u3081\u306b`cuda.elementwise`\u3092\u4f7f\u7528\u3057\u3066\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n            a, i, f, o = _extract_gates(x)\n            gc_prev = xp.empty_like(c_prev)\n            cuda.elementwise(\n                'T c_prev, T c, T gc, T gh, T a, T i_, T f, T o',\n                'T gc_prev, T ga, T gi, T gf, T go',\n                '''\n                    COMMON_ROUTINE;\n                    T co = tanh(c);\n                    T temp = gh * ao * grad_tanh(co) + gc;\n                    ga = temp * ai * grad_tanh(aa);\n                    gi = temp * aa * grad_sigmoid(ai);\n                    gf = temp * c_prev * grad_sigmoid(af);\n                    go = gh * co * grad_sigmoid(ao);\n                    gc_prev = temp * af;\n                ''',\n                'lstm_bwd', preamble=_preamble)(\n                    c_prev[:batch], self.c, gc_update, gh, a, i, f, o,\n                    gc_prev[:batch], ga, gi, gf, go)\n            gc_prev[batch:] = gc_rest\n\n```\n\n## attention.py\n\n<img width=\"216\" alt=\"Screen Shot 2016-12-26 at 9.38.00 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/ecbeb4fb-d012-d9ff-3608-50492224fbd7.png\">\n\n\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u4fdd\u6301\u3059\u308b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\n- `annotion_weight`\u304c\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u90e8\u5206\u306e\u91cd\u307f\n- `back_weight`\u304c\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u90e8\u5206\u306e\u91cd\u307f\u3001\n- `pw`\u304c\u73fe\u5728\u306e\u5c64\u306e\u91cd\u307f\n- `weight_exponential`\u304c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067exp\u95a2\u6570\u3092\u51e6\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a\n\n```py\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__(\n            annotion_weight=links.Linear(hidden_size, hidden_size),\n            back_weight=links.Linear(hidden_size, hidden_size),\n            pw=links.Linear(hidden_size, hidden_size),\n            weight_exponential=links.Linear(hidden_size, 1),\n        )\n        self.hidden_size = hidden_size\n```\n\n\n\n`annotion_list`\u304c\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\n`back_word_list`\u304c\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\n`p`\u304c\u73fe\u5728\u306e\u5c64\u306e\u91cd\u307f\n\n\n```py\n\n    def __call__(self, annotion_list, back_word_list, p):\n```\n\n\u30d0\u30c3\u30c1\u51e6\u7406\u7528\u306e\u521d\u671f\u5316\n\n```py\n\n        batch_size = p.data.shape[0]\n        exponential_list = []\n        sum_exponential = XP.fzeros((batch_size, 1))\n```\n\nforward\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\u3068back_word\u306e\u5358\u8a9e\u30ea\u30b9\u30c8\u3001\u73fe\u5728\u306e\u5c64\u306e\u72b6\u614b\u3092\u7dcf\u5408\u3057\u305f\u91cd\u307f\u3092\u4f5c\u6210\n\u4e0b\u8a18\u306b\u76f8\u5f53\n\n```math\ne_{ij} = a(s_{i-1}, h_j)\n```\n\n\u305d\u3053\u3067\u5f97\u3089\u308c\u305f\u5024\u3092exp\u95a2\u6570\u3067\u51e6\u7406\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3001\u5404\u5024\u3092\u30ea\u30b9\u30c8\u5316\n\u5408\u8a08\u5024\u3082\u7b97\u51fa\u3057\u307e\u3059\n\n\n```math\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x}\\exp(e_{ik})} \\\\\n```\n\n\u4e21\u65b9\u5411\u304b\u3089\u51e6\u7406\u3059\u308b\u305f\u3081\u3001\u524d\u65b9\u5411\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u30ea\u30b9\u30c8\u3068\u5f8c\u308d\u65b9\u5411\u304b\u3089\u306e\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u73fe\u5728\u306e\u91cd\u307f\u3092\u542b\u3081\u3066\u91cd\u307f\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\nexp\u95a2\u6570\u306e\u91cd\u307f\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3002\nexp\u95a2\u6570\u306e\u7dcf\u548c\u3092\u8a08\u7b97\n\n```py\n\n        for annotion, back_word in zip(annotion_list, back_word_list):\n            weight = functions.tanh(self.annotion_weight(annotion) + self.back_weight(back_word) + self.pw(p))\n            exponential = functions.exp(self.weight_exponential(weight))\n            exponential_list.append(exponential)\n            sum_exponential += exponential\n```\n\n\u521d\u671f\u5316\u3092\u884c\u3044\u3001\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u91cd\u307f\u3092\u8a08\u7b97\u3057\u3066\u304a\u304d\u3001\u305d\u306e\u91cd\u307f\u3092\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3067\u884c\u5217\u8a08\u7b97\u3057\u305f\u5024\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u7528\u610f\u3057\u3066\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002`functions.batch_matmul`\u3067\u884c\u5217\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\n`a`\u304c\u5de6\u884c\u5217\n`b`\u304c\u53f3\u884c\u5217\n`transa`\u304c\u3042\u308b\u3068\u304d\u306f\u5de6\u306e\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u3046\u3002\n`transb`\u304c\u3042\u308b\u3068\u304d\u306f\u53f3\u306e\u884c\u5217\u306e\u8ee2\u7f6e\u3092\u884c\u3046\n\n```py3\ndef batch_matmul(a, b, transa=False, transb=False):\n    return BatchMatMul(transa=transa, transb=transb)(a, b)\n```\n\n\u5b9f\u969b\u306e\u884c\u5217\u8a08\u7b97\u306e\u4e2d\u8eab\n- \u884c\u5217\u3092\u8a08\u7b97\u3067\u304d\u308b\u5f62\u306b\u5909\u63db\n\u3067\u306f\u884c\u5217\u3092\u8981\u7d20\u3054\u3068\u306b\u8a08\u7b97\u3067\u304d\u308b\u3088\u3046\u306b\u5909\u63db\u3057\u3066\u3044\u308b\n```\na = a.reshape(a.shape[:2] + (-1,))\n```\n\n\u4e0b\u8a18\u306e\u3088\u3046\u306a\u884c\u5217\u304c\u3042\u3063\u305f\u6642\u306b\n\n```\narray([[1, 2, 3],\n       [4, 5, 6],\n       [3, 4, 5]])\n```\n\n\u305d\u308c\u3092\u4e0b\u8a18\u306e\u3088\u3046\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3002\n\n```\narray([[[1],\n        [2],\n        [3]],\n\n       [[4],\n        [5],\n        [6]],\n\n       [[3],\n        [4],\n        [5]]])\n```\n\n- \u3082\u3057\u8ee2\u7f6e\u304c\u5fc5\u8981\u3067\u3042\u308c\u3070\u51e6\u7406\u3092\u884c\u3046\u3002\n- \u7b54\u3048\u3068\u3057\u3066\u8fd4\u3059\u884c\u5217\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u521d\u671f\u5316\n- numpy\u3067\u3042\u308c\u3070\u884c\u5217\u306e\u8981\u7d20\u3054\u3068\u306b\u8a08\u7b97\u3057gpu\u7528\u306ecupy\u3067\u3042\u308c\u3070`matmul`\u3092\u4f7f\u7528\u3057\u3066\u884c\u5217\u8a08\u7b97\u3092\u3059\u308b\u3002`matmul`\u306f\u30b9\u30ab\u30e9\u30fc\u8a08\u7b97\u3092\u8a31\u53ef\u3057\u3066\u304a\u3089\u305a\u3001\u884c\u5217\u3092\u30b9\u30bf\u30c3\u30af\u306b\u7a4d\u3093\u3067\u51e6\u7406\u3092\u884c\u3046\u3002\n\n```py\ndef _batch_matmul(a, b, transa=False, transb=False, transout=False):\n    a = a.reshape(a.shape[:2] + (-1,))\n    b = b.reshape(b.shape[:2] + (-1,))\n    trans_axis = (0, 2, 1)\n    if transout:\n        transa, transb = not transb, not transa\n        a, b = b, a\n    if transa:\n        a = a.transpose(trans_axis)\n    if transb:\n        b = b.transpose(trans_axis)\n    xp = cuda.get_array_module(a)\n    if xp is numpy:\n        ret = numpy.empty(a.shape[:2] + b.shape[2:], dtype=a.dtype)\n        for i in six.moves.range(len(a)):\n            ret[i] = numpy.dot(a[i], b[i])\n        return ret\n    return xp.matmul(a, b)\n```\n\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u884c\u5217\u306e\u30b5\u30a4\u30ba\u5206\u3001\u30bc\u30ed\u884c\u5217\u3067\u521d\u671f\u5316\u3092\u3057\u3066\u3001`annotion`\u3068`back_word`\u3067\u8a08\u7b97\u3057\u305f\u7dcf\u548c\u3092\u8fd4\u3057\u307e\u3059\u3002 \n\n```py\n\n        ZEROS = XP.fzeros((batch_size, self.hidden_size))\n        annotion_value = ZEROS\n        back_word_value = ZEROS\n        # Calculate the Convolution Value each annotion and back word\n        for annotion, back_word, exponential in zip(annotion_list, back_word_list, exponential_list):\n            exponential /= sum_exponential\n            annotion_value += functions.reshape(functions.batch_matmul(annotion, exponential), (batch_size, self.hidden_size))\n            back_word_value += functions.reshape(functions.batch_matmul(back_word, exponential), (batch_size, self.hidden_size))\n        return annotion_value, back_word_value\n```\n\n## attention_decoder.py\n\n<img width=\"216\" alt=\"Screen Shot 2016-12-26 at 9.38.00 AM.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/10496/d654e5ce-e3a5-6a6d-93a6-cb8c5dd000c7.png\">\n\n\u51fa\u529b\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u5bfe\u8a71\u306e\u5834\u5408\u306f\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u306a\u308a\u307e\u3059\u3002\n\u5165\u529b\u3088\u308a\u8907\u96d1\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n`embed_vocab`\uff1a\u51fa\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3059\u308b\u90e8\u5206\n`embed_hidden`\uff1a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5024\u3092LSTM\u306b\u4f1d\u642c\u3059\u308b\u90e8\u5206\n`hidden_hidden`\uff1a\u96a0\u308c\u5c64\u306e\u4f1d\u642c\u90e8\u5206\n`annotation_hidden`\uff1a\u30d5\u30a9\u30ef\u30fc\u30c9\u578b\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\n`back_word_hidden`\uff1a\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u578b\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\n`hidden_embed`\uff1a\u96a0\u308c\u5c64\u304b\u3089\u51fa\u529b\u5c64\uff08\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u5f53\u305f\u308b\uff09\u3078\u306e\u4f1d\u642c\n`embded_target`\uff1a\u51fa\u529b\u5c64\u304b\u3089\u30b7\u30b9\u30c6\u30e0\u306e\u51fa\u529b\uff08\u30b7\u30b9\u30c6\u30e0\u306e\u5fdc\u7b54\u306b\u5f53\u305f\u308b\uff09\u3078\u306e\u4f1d\u642c\n\n```py\n        super(AttentionDecoder, self).__init__(\n            embed_vocab=links.EmbedID(vocab_size, embed_size),\n            embed_hidden=links.Linear(embed_size, 4 * hidden_size),\n            hidden_hidden=links.Linear(hidden_size, 4 * hidden_size),\n            annotation_hidden=links.Linear(embed_size, 4 * hidden_size),\n            back_word_hidden=links.Linear(hidden_size, 4 * hidden_size),\n            hidden_embed=links.Linear(hidden_size, embed_size),\n            embded_target=links.Linear(embed_size, vocab_size),\n        )\n\n```\n\n\u51fa\u529b\u5358\u8a9e\u3092\u96a0\u308c\u5c64\u3078\u5199\u50cf\u3057\u3066\u5fae\u5206\u53ef\u80fd\u306a\u53cc\u6975\u95a2\u6570\u3092\u4f7f\u7528\n\u51fa\u529b\u5358\u8a9e\u306e\u96a0\u308c\u5c64\u3001\u96a0\u308c\u5c64\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u306e\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u7dcf\u548c\u3092lsm\u306b\u4e0e\u3048\u3066\u3001\u72b6\u614b\u3068\u96a0\u308c\u5c64\u3092\u4e88\u6e2c\n\u51fa\u529b\u306e\u305f\u3081\u306e\u96a0\u308c\u5c64\u3092\u5148\u307b\u3069\u4e88\u6e2c\u3057\u305f\u96a0\u308c\u5c64\u3092\u4f7f\u3063\u3066\u5fae\u5206\u53ef\u80fd\u306a\u53cc\u6975\u95a2\u6570\u3067\u4e88\u6e2c\u3059\u308b\n\u51fa\u529b\u306e\u305f\u3081\u306e\u96a0\u308c\u5c64\u3092\u4f7f\u7528\u3057\u3066\u51fa\u529b\u5358\u8a9e\u3092\u4e88\u6e2c\u3001\u73fe\u5728\u306e\u72b6\u614b\u3001\u96a0\u308c\u5c64\u3092\u8fd4\u3059\n\n```py\n        embed = functions.tanh(self.embed_vocab(target))\n        current, hidden = functions.lstm(current, self.embed_hidden(embed) + self.hidden_hidden(hidden) +\n                                         self.annotation_hidden(annotation) + self.back_word_hidden(back_word))\n        embed_hidden = functions.tanh(self.hidden_embed(hidden))\n        return self.embded_target(embed_hidden), current, hidden\n```\n\n## attention_dialogue.py\n\n\u5177\u4f53\u7684\u306a\u5bfe\u8a71\u7528\u306e\u51e6\u7406\u3092\u884c\u3046\u90e8\u5206\u3067\u3059\u3002\n\u5148\u307b\u3069\u8aac\u660e\u3057\u305f4\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n`emb`\u3067\u5165\u529b\u8a00\u8a9e\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3057\u307e\u3059\u3002\n`forward_encode`\uff1a\u30d5\u30a9\u30ef\u30fc\u30c9\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u3066\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u7528\u306b\u7528\u610f\u3057\u307e\u3059\u3002\n`back_encdode`\uff1a\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u30a8\u30f3\u30b3\u30fc\u30c9\u3057\u3066\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u7528\u306b\u7528\u610f\u3057\u307e\u3059\u3002\n`attention`\uff1a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u306b\u7528\u610f\n`dec`\uff1a\u51fa\u529b\u7528\u306e\u5358\u8a9e\u306e\u305f\u3081\u306b\u7528\u610f\n\n\u8a9e\u5f59\u306e\u30b5\u30a4\u30ba\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7a7a\u9593\u306b\u5199\u50cf\u3059\u308b\u305f\u3081\u306e\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3001XP\u3067gpu\u3092\u4f7f\u7528\u3059\u308b\u304b\u3069\u3046\u304b\u6c7a\u3081\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n        super(AttentionDialogue, self).__init__(\n            emb=SrcEmbed(vocab_size, embed_size),\n            forward_encode=AttentionEncoder(embed_size, hidden_size),\n            back_encdode=AttentionEncoder(embed_size, hidden_size),\n            attention=Attention(hidden_size),\n            dec=AttentionDecoder(vocab_size, embed_size, hidden_size),\n        )\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.XP = XP\n```\n\n\u521d\u671f\u5316\u3092\u3057\u3066\u52fe\u914d\u3092\u30bc\u30ed\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n    def reset(self):\n        self.zerograds()\n        self.source_list = []\n```\n\n\u5165\u529b\u8a00\u8a9e\uff08\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\uff09\u3092\u5358\u8a9e\u30ea\u30b9\u30c8\u3068\u3057\u3066\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n    def embed(self, source):\n        self.source_list.append(self.emb(source))\n```\n\n`encode`\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u90e8\u5206\u3067\u3059\u3002\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u53d6\u5f97\u306e\u305f\u3081\u306b\u5165\u529b\u8a00\u8a9e\u306e1\u6b21\u5143\u90e8\u5206\u306e\u307f\u53d6\u3063\u3066\u3044\u307e\u3059\u3002\n\u56f3\n\u521d\u671f\u5316\u3092\u3057\u3066\u3044\u307e\u3059\u304cgpu\u3068cpu\u3067\u521d\u671f\u5316\u306e\u5024\u304c\u7570\u306a\u308b\u306e\u3067`self.XP.fzeros`\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u4f5c\u6210\u306e\u305f\u3081\u306b\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002\n\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3082\u540c\u69d8\u306e\u51e6\u7406\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n    def encode(self):\n        batch_size = self.source_list[0].data.shape[0]\n        ZEROS = self.XP.fzeros((batch_size, self.hidden_size))\n        context = ZEROS\n        annotion = ZEROS\n        annotion_list = []\n        # Get the annotion list\n        for source in self.source_list:\n            context, annotion = self.forward_encode(source, context, annotion)\n            annotion_list.append(annotion)\n        context = ZEROS\n        back_word = ZEROS\n        back_word_list = []\n        # Get the back word list\n        for source in reversed(self.source_list):\n            context, back_word = self.back_encdode(source, context, back_word)\n            back_word_list.insert(0, back_word)\n        self.annotion_list = annotion_list\n        self.back_word_list = back_word_list\n        self.context = ZEROS\n        self.hidden = ZEROS\n```\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u3001\u96a0\u308c\u5c64\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u5c64\u306b\u5165\u308c\u3066\u305d\u308c\u305e\u308c\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30ef\u30fc\u30c9\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08dec\u306b\u3088\u3063\u3066\u5f97\u3089\u308c\u308b\uff09\u3001\u96a0\u308c\u5c64\u306e\u5024\u3001\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u5024\u3001\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u5024\u3067\u51fa\u529b\u3057\u305f\u5358\u8a9e\u3092\u8fd4\u3057\u3066\u3044\u308b\u3002\n\n```py\n\n    def decode(self, target_word):\n        annotion_value, back_word_value = self.attention(self.annotion_list, self.back_word_list, self.hidden)\n        target_word, self.context, self.hidden = self.dec(target_word, self.context, self.hidden, annotion_value, back_word_value)\n        return target_word\n```\n\n\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n\u8a9e\u5f59\u30b5\u30a4\u30ba\u3001\u6f5c\u5728\u5c64\u306e\u5199\u50cf\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u3092\u4fdd\u5b58\u3057\u3066\u3044\u308b\u3002\n\n```py\n\n    def save_spec(self, filename):\n        with open(filename, 'w') as fp:\n            print(self.vocab_size, file=fp)\n            print(self.embed_size, file=fp)\n            print(self.hidden_size, file=fp)\n```\n\n\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u90e8\u5206\u3002\u3053\u3053\u3067\u8aad\u307f\u8fbc\u3093\u3060\u5024\u3092\u53d6\u5f97\u3057\u3066\u30e2\u30c7\u30eb\u306b\u6e21\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n    def load_spec(filename, XP):\n        with open(filename) as fp:\n            vocab_size = int(next(fp))\n            embed_size = int(next(fp))\n            hidden_size = int(next(fp))\n        return AttentionDialogue(vocab_size, embed_size, hidden_size, XP)\n```\n\n## EncoderDecoderModelAttention.py\n\n\u3053\u306e\u90e8\u5206\u3067\u5b9f\u969b\u5148\u307b\u3069\u8aac\u660e\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\n\u5404\u7a2e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n    def __init__(self, parameter_dict):\n        self.parameter_dict       = parameter_dict\n        self.source               = parameter_dict[\"source\"]\n        self.target               = parameter_dict[\"target\"]\n        self.test_source          = parameter_dict[\"test_source\"]\n        self.test_target          = parameter_dict[\"test_target\"]\n        self.vocab                = parameter_dict[\"vocab\"]\n        self.embed                = parameter_dict[\"embed\"]\n        self.hidden               = parameter_dict[\"hidden\"]\n        self.epoch                = parameter_dict[\"epoch\"]\n        self.minibatch            = parameter_dict[\"minibatch\"]\n        self.generation_limit     = parameter_dict[\"generation_limit\"]\n        self.word2vec = parameter_dict[\"word2vec\"]\n        self.word2vecFlag = parameter_dict[\"word2vecFlag\"]\n        self.model = parameter_dict[\"model\"]\n        self.attention_dialogue   = parameter_dict[\"attention_dialogue\"]\n        XP.set_library(False, 0)\n        self.XP = XP\n```\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u51e6\u7406\u306e\u5b9f\u88c5\u3067\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\u3068\u30bd\u30fc\u30b9\u306e\u30b5\u30a4\u30ba\u3092\u53d6\u5f97\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n    def forward_implement(self, src_batch, trg_batch, src_vocab, trg_vocab, attention, is_training, generation_limit):\n        batch_size = len(src_batch)\n        src_len = len(src_batch[0])\n        trg_len = len(trg_batch[0]) if trg_batch else 0\n        src_stoi = src_vocab.stoi\n        trg_stoi = trg_vocab.stoi\n        trg_itos = trg_vocab.itos\n        attention.reset()\n```\n\n\u5165\u529b\u8a00\u8a9e\u3092\u9006\u65b9\u5411\u304b\u3089\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\u9006\u65b9\u5411\u304b\u3089\u5165\u529b\u3059\u308b\u3068\u6a5f\u68b0\u7ffb\u8a33\u7d50\u679c\u304c\u4e0a\u304c\u3063\u3066\u3044\u308b\u306e\u3067\u5bfe\u8a71\u3082\u540c\u69d8\u306e\u5f62\u306b\u3057\u3066\u3044\u307e\u3059\u304c\u52b9\u679c\u306f\u306a\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n        x = self.XP.iarray([src_stoi('</s>') for _ in range(batch_size)])\n        attention.embed(x)\n        for l in reversed(range(src_len)):\n            x = self.XP.iarray([src_stoi(src_batch[k][l]) for k in range(batch_size)])\n            attention.embed(x)\n\n        attention.encode()\n```\n\n\u53d6\u5f97\u3057\u305f\u3044\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u8a00\u8a9e\u5217\u3092`<s>`\u3067\u521d\u671f\u5316\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```py\n\n        t = self.XP.iarray([trg_stoi('<s>') for _ in range(batch_size)])\n        hyp_batch = [[] for _ in range(batch_size)]\n```\n\n\u5b66\u7fd2\u90e8\u5206\u3067\u3059\u3002\n\u8a00\u8a9e\u60c5\u5831\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u3057\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044\u306e\u3067`stoi`\u3067\u8a00\u8a9e\u304b\u3089\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n\u30bf\u30fc\u30b2\u30c3\u30c8\uff08\u3053\u306e\u5834\u5408\u306f\u5bfe\u8a71\u306e\u51fa\u529b\uff09\u3092\u51fa\u3057\u3066\u3001\u6b63\u89e3\u30c7\u30fc\u30bf\u3068\u6bd4\u8f03\u3057\u3066\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\n\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306f\u78ba\u7387\u5206\u5e03\u9593\u306e\u8ddd\u96e2\u3092\u51fa\u3057\u3066\u3044\u308b\u306e\u3067\u3053\u306e\u8a08\u7b97\u306e\u30ed\u30b9\u304c\u5c11\u306a\u3044\u307b\u3069\u51fa\u529b\u7d50\u679c\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u306b\u8fd1\u3065\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002\n\u4eee\u8aac\u5019\u88dc\u3068\u7b97\u51fa\u3057\u305f\u640d\u5931\u3092\u8fd4\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n        if is_training:\n            loss = self.XP.fzeros(())\n            for l in range(trg_len):\n                y = attention.decode(t)\n                t = self.XP.iarray([trg_stoi(trg_batch[k][l]) for k in range(batch_size)])\n                loss += functions.softmax_cross_entropy(y, t)\n                output = cuda.to_cpu(y.data.argmax(1))\n                for k in range(batch_size):\n                    hyp_batch[k].append(trg_itos(output[k]))\n            return hyp_batch, loss\n```\n\n\u3053\u3053\u306f\u30c6\u30b9\u30c8\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306f\u7121\u9650\u306b\u5019\u88dc\u751f\u6210\u304c\u3067\u304d\u3001\u7279\u306blstm\u306e\u30e2\u30c7\u30eb\u306e\u5834\u5408\u306f\u904e\u53bb\u306e\u72b6\u614b\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u3001\u7121\u9650\u306e\u30eb\u30fc\u30d7\u306b\u5165\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u5236\u9650\u3092\u304b\u3051\u3066\u3044\u307e\u3059\u3002\n\u521d\u671f\u5316\u3057\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5358\u8a9e\u5217\u3092\u5229\u7528\u3057\u3066\u51fa\u529b\u3057\u307e\u3059\u3002\n\u51fa\u529b\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u6700\u5927\u5024\u3092\u51fa\u529b\u3057\u3066`t`\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u51fa\u529b\u3055\u308c\u305f\u5019\u88dc\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u304b\u3089\u8a00\u8a9e\u60c5\u5831\u306b\u5909\u63db\u3057\u3066\u3044\u307e\u3059\u3002\n\u3059\u3079\u3066\u306e\u5019\u88dc\u304c`</s>`\u306e\u7d42\u4e86\u8a18\u53f7\u3067\u7d42\u308f\u3063\u3066\u3044\u308b\u5834\u5408\u306b\u51e6\u7406\u3092`break`\u3059\u308b\u3002\n\n```py\n\n        else:\n            while len(hyp_batch[0]) < generation_limit:\n                y = attention.decode(t)\n                output = cuda.to_cpu(y.data.argmax(1))\n                t = self.XP.iarray(output)\n                for k in range(batch_size):\n                    hyp_batch[k].append(trg_itos(output[k]))\n                if all(hyp_batch[k][-1] == '</s>' for k in range(batch_size)):\n                    break\n\n        return hyp_batch\n```\n\n\u5b66\u7fd2\u5168\u4f53\u306e\u51e6\u7406\u3067\u3059\u3002\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u521d\u671f\u5316\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n`self.vocab`\u306f\u5168\u4f53\u306e\u8a9e\u5f59\u3067`gens.word_list`\u3067\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n```\n        src_vocab = Vocabulary.new(gens.word_list(self.source), self.vocab)\n        trg_vocab = Vocabulary.new(gens.word_list(self.target), self.vocab)\n```\n\n` Vocabulary.new()`\u3067\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u8a9e\u5f59\u60c5\u5831\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n`gens.word_list(self.source)`\u3067\u4e0b\u8a18\u306e\u3088\u3046\u306a`generator`\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002`self.source`\u3067\u4e0e\u3048\u308b\u306e\u306f\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\ndef word_list(filename):\n    with open(filename) as fp:\n        for l in fp:\n            yield l.split()\n```\n\n\u8a9e\u5f59\u60c5\u5831\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u60c5\u5831\u306b\u5909\u63db\u3059\u308b\u51e6\u7406\u306f\u4e0b\u8a18\u306e\u90e8\u5206\u3067\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n`<unk>`\u306f\u672a\u77e5\u8a9e\u30670\u3001`<s>`\u306f\u63a5\u982d\u8a9e\u30671\u3001`</s>`\u306f\u672b\u5c3e\u306e\u8a9e\u53e5\u30672\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\u3042\u3089\u304b\u3058\u3081\u306b\u3053\u308c\u3089\u306b\u5024\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u305f\u3081+3\u3092\u3057\u3066\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u4e88\u7d04\u8a9e\u3088\u308a\u5f8c\u306b\u306a\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n    @staticmethod\n    def new(list_generator, size):\n        self = Vocabulary()\n        self.__size = size\n\n        word_freq = defaultdict(lambda: 0)\n        for words in list_generator:\n            for word in words:\n                word_freq[word] += 1\n\n        self.__stoi = defaultdict(lambda: 0)\n        self.__stoi['<unk>'] = 0\n        self.__stoi['<s>'] = 1\n        self.__stoi['</s>'] = 2\n        self.__itos = [''] * self.__size\n        self.__itos[0] = '<unk>'\n        self.__itos[1] = '<s>'\n        self.__itos[2] = '</s>'\n\n        for i, (k, v) in zip(range(self.__size - 3), sorted(word_freq.items(), key=lambda x: -x[1])):\n            self.__stoi[k] = i + 3\n            self.__itos[i + 3] = k\n\n        return self\n\n```\n\n\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3067\u3059\u3002\n\u8a9e\u5f59\u3068\u57cb\u3081\u8fbc\u307f\u5c64\u3068\u96a0\u308c\u5c64\u3068`XP`\u3092\u4e0e\u3048\u307e\u3059\u3002\n`XP`\u306fcpu\u8a08\u7b97\u3068gpu\u8a08\u7b97\u3092\u884c\u3046\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\n\n        trace('making model ...')\n        self.attention_dialogue = AttentionDialogue(self.vocab, self.embed, self.hidden, self.XP)\n```\n\n\u8ee2\u79fb\u5b66\u7fd2\u306e\u90e8\u5206\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u3061\u3089\u3067word2vec\u3067\u4f5c\u6210\u3057\u305f\u91cd\u307f\u3092\u8ee2\u79fb\u3055\u305b\u3066\u3044\u307e\u3059\u3002\nword2vec\u3067\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u306e\u540d\u524d`weight_xi`\u306a\u306e\u3067\u5165\u529b\u767a\u8a71\u306f\u7d71\u4e00\u3057\u3066\u3044\u307e\u3059\u304c\u51fa\u529b\u767a\u8a71\u306e\u90e8\u5206\u306f`embded_target`\u3067\u7570\u306a\u308b\u305f\u3081\u4e0b\u8a18\u306e\u51e6\u7406\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n[0]\u306e\u90e8\u5206\u306f\u91cd\u307f\u306e\u540d\u524d\n[1]\u306e\u90e8\u5206\u306f\u5024\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\n                if dst[\"embded_target\"] and child.name == \"weight_xi\" and self.word2vecFlag:\n                    for a, b in zip(child.namedparams(), dst[\"embded_target\"].namedparams()):\n                        b[1].data = a[1].data\n```\n\n\u91cd\u307f\u306e\u30b3\u30d4\u30fc\u90e8\u5206\u3067\u3059\u3002\n\u5143\u3068\u306a\u308b\u90e8\u5206\u306e\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u56de\u3057\u3001\u6761\u4ef6\u304c\u4e00\u81f4\u3059\u308b\u5834\u5408\u306f\u91cd\u307f\u306e\u30b3\u30d4\u30fc\u3092\u3057\u307e\u3059\u3002\n\u6761\u4ef6\uff11\uff1a\u91cd\u307f\u306b\u547d\u540d\u3055\u308c\u305f\u540d\u524d\u306b\u4e00\u81f4\u3059\u308b\u3082\u306e\u304c\u3042\u308b\u3053\u3068\n\u6761\u4ef6\uff12\uff1a\u91cd\u307f\u306e\u578b\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\n\u6761\u4ef6\uff13\uff1a`link.Link`\u306e\u90e8\u5206\u3064\u307e\u308a\u30e2\u30c7\u30eb\u90e8\u5206\u307e\u3067\u5230\u9054\u3057\u3066\u3044\u308b\u3053\u3068\n\u6761\u4ef6\uff14\uff1a\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u306e\u884c\u5217\u306e\u9577\u3055\u304c\u540c\u4e00\u3067\u3042\u308b\u3053\u3068\n\n```py\n\n    def copy_model(self, src, dst, dec_flag=False):\n        print(\"start copy\")\n        for child in src.children():\n            if dec_flag:\n                if dst[\"embded_target\"] and child.name == \"weight_xi\" and self.word2vecFlag:\n                    for a, b in zip(child.namedparams(), dst[\"embded_target\"].namedparams()):\n                        b[1].data = a[1].data\n                    print('Copy weight_jy')\n            if child.name not in dst.__dict__: continue\n            dst_child = dst[child.name]\n            if type(child) != type(dst_child): continue\n            if isinstance(child, link.Chain):\n                self.copy_model(child, dst_child)\n            if isinstance(child, link.Link):\n                match = True\n                for a, b in zip(child.namedparams(), dst_child.namedparams()):\n                    if a[0] != b[0]:\n                        match = False\n                        break\n                    if a[1].data.shape != b[1].data.shape:\n                        match = False\n                        break\n                if not match:\n                    print('Ignore %s because of parameter mismatch' % child.name)\n                    continue\n                for a, b in zip(child.namedparams(), dst_child.namedparams()):\n                    b[1].data = a[1].data\n                print('Copy %s' % child.name)\n```\n\n\n```py\n\n        if self.word2vecFlag:\n            self.copy_model(self.word2vec, self.attention_dialogue.emb)\n            self.copy_model(self.word2vec, self.attention_dialogue.dec, dec_flag=True)\n```\n\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```py\n            gen1 = gens.word_list(self.source)\n            gen2 = gens.word_list(self.target)\n            gen3 = gens.batch(gens.sorted_parallel(gen1, gen2, 100 * self.minibatch), self.minibatch)\n```\n\u4e21\u8005\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u4e0b\u8a18\u3067\u30bf\u30d7\u30eb\u5f62\u5f0f\u3067\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```py\ndef batch(generator, batch_size):\n    batch = []\n    is_tuple = False\n    for l in generator:\n        is_tuple = isinstance(l, tuple)\n        batch.append(l)\n        if len(batch) == batch_size:\n            yield tuple(list(x) for x in zip(*batch)) if is_tuple else batch\n            batch = []\n    if batch:\n        yield tuple(list(x) for x in zip(*batch)) if is_tuple else batch\n```\n\n\u5165\u529b\u767a\u8a71\u3068\u51fa\u529b\u767a\u8a71\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u5206\u4f5c\u6210\u3057\u3066\u30bd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\ndef sorted_parallel(generator1, generator2, pooling, order=1):\n    gen1 = batch(generator1, pooling)\n    gen2 = batch(generator2, pooling)\n    for batch1, batch2 in zip(gen1, gen2):\n        #yield from sorted(zip(batch1, batch2), key=lambda x: len(x[1]))\n        for x in sorted(zip(batch1, batch2), key=lambda x: len(x[order])):\n            yield x\n```\n\n\u6700\u9069\u5316\u306b\u306fAdagrad\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\u5358\u7d14\u306b\u66f4\u65b0\u56de\u6570\u304c\u7d2f\u7a4d\u3059\u308b\u307b\u3069\u66f4\u65b0\u5e45\u304c\u5c0f\u3055\u304f\u306a\u308b\u624b\u6cd5\u3067\u3059\u3002\n\n```math\nr \u2190 r + g^2_{\\vec{w}}\\\\\nw \u2190 w - \\frac{\\alpha}{r + }g^2_{\\vec{w}}\n```\n\n`optimizer.GradientClipping(5)`\u3067L2\u6b63\u5247\u5316\u3092\u4f7f\u7528\u3057\u3066\u52fe\u914d\u304c\u4e00\u5b9a\u306e\u7bc4\u56f2\u5185\u306b\u306a\u308b\u3088\u3046\u306b\u6291\u3048\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n            opt = optimizers.AdaGrad(lr = 0.01)\n            opt.setup(self.attention_dialogue)\n            opt.add_hook(optimizer.GradientClipping(5))\n```\n\n\u4e0b\u8a18\u3067\u5165\u529b\u30e6\u30fc\u30b6\u30fc\u767a\u8a71\u3068\u5bfe\u5fdc\u30e6\u30fc\u30b6\u30fc\u767a\u8a71\u3092`fill_batch`\u306b\u3088\u308a`*`\u3067\u7a74\u57cb\u3081\u3057\u3066\u6df1\u5c64\u5b66\u7fd2\u53ef\u80fd\u306a\u5f62\u306b\u3057\u307e\u3059\u3002\n\n```py\ndef fill_batch(batch, token='</s>'):\n    max_len = max(len(x) for x in batch)\n    return [x + [token] * (max_len - len(x) + 1) for x in batch]\n```\n\n\u30d5\u30a9\u30ef\u30fc\u30c9\u51e6\u7406\u3067\u5f97\u3089\u308c\u305f\u640d\u5931\u3092\u4f7f\u3063\u3066\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u51e6\u7406\u3092\u884c\u3044\u3001\u91cd\u307f\u306e\u66f4\u65b0\u3092\u884c\u3046\u3002\n\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\u306e\u51e6\u7406\u306f\u6d3b\u6027\u5316\u95a2\u6570\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3002\n\u66f4\u65b0\u90e8\u5206\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\u30c7\u30fc\u30bf\u3092gpu\u3067\u51e6\u7406\u3059\u308b\u304bcpu\u3067\u51e6\u7406\u3059\u308b\u304b\u5909\u3048\u3066\n`tuple`\u3001`dict`\u3001\u305d\u308c\u4ee5\u5916\u3067\u30c7\u30fc\u30bf\u306e\u4e0e\u3048\u65b9\u3092\u5909\u3048\u3066\u3001\u640d\u5931\u95a2\u6570\u306b\u3088\u308b\u6700\u9069\u5316\u3092\u5909\u3048\u3066\u3044\u308b\u3002\n\n```py\n\n    def update_core(self):\n        batch = self._iterators['main'].next()\n        in_arrays = self.converter(batch, self.device)\n\n        optimizer = self._optimizers['main']\n        loss_func = self.loss_func or optimizer.target\n\n        if isinstance(in_arrays, tuple):\n            in_vars = tuple(variable.Variable(x) for x in in_arrays)\n            optimizer.update(loss_func, *in_vars)\n        elif isinstance(in_arrays, dict):\n            in_vars = {key: variable.Variable(x)\n                       for key, x in six.iteritems(in_arrays)}\n            optimizer.update(loss_func, **in_vars)\n        else:\n            in_var = variable.Variable(in_arrays)\n            optimizer.update(loss_func, in_var)\n```\n\n```py\n            for src_batch, trg_batch in gen3:\n                src_batch = fill_batch(src_batch)\n                trg_batch = fill_batch(trg_batch)\n                K = len(src_batch)\n                hyp_batch, loss = self.forward_implement(src_batch, trg_batch, src_vocab, trg_vocab, self.attention_dialogue, True, 0)\n                loss.backward()\n                opt.update()\n\n```\n\n\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\nsave\u3068save_spec\u306fchainer\u306e\u6a19\u6e96\u3067\u306f\u5b58\u5728\u305b\u305a\u3001\u8a00\u8a9e\u306b\u95a2\u3059\u308b\u60c5\u5831\u306e\u4fdd\u5b58\u306e\u305f\u3081\u3001\u5225\u306b\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\n\n`save`\u306f\u767a\u8a71\u30c7\u30fc\u30bf\u306e\u60c5\u5831\u3092\u4fdd\u5b58\n`save_spec`\u306f\u8a9e\u5f59\u306e\u30b5\u30a4\u30ba\u3084\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u30b5\u30a4\u30ba\u3001\u96a0\u308c\u5c64\u306e\u30b5\u30a4\u30ba\u306e\u4fdd\u5b58\n`save_hdf5`\u306fhdf5\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n\n```py\n\n        trace('saving model ...')\n        prefix = self.model\n        model_path = APP_ROOT + \"/model/\" + prefix\n        src_vocab.save(model_path + '.srcvocab')\n        trg_vocab.save(model_path + '.trgvocab')\n        self.attention_dialogue.save_spec(model_path + '.spec')\n        serializers.save_hdf5(model_path + '.weights', self.attention_dialogue)\n\n```\n\n\u30c6\u30b9\u30c8\u306e\u90e8\u5206\u3067\u3059\u3002\n\u5b66\u7fd2\u306e\u969b\u306b\u51fa\u529b\u3055\u308c\u305f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5165\u529b\u767a\u8a71\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u767a\u8a71\u5185\u5bb9\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py\n\n    def test(self):\n        trace('loading model ...')\n        prefix = self.model\n        model_path = APP_ROOT + \"/model/\" + prefix\n        src_vocab = Vocabulary.load(model_path + '.srcvocab')\n        trg_vocab = Vocabulary.load(model_path + '.trgvocab')\n        self.attention_dialogue = AttentionDialogue.load_spec(model_path + '.spec', self.XP)\n        serializers.load_hdf5(model_path + '.weights', self.attention_dialogue)\n\n        trace('generating translation ...')\n        generated = 0\n\n        with open(self.test_target, 'w') as fp:\n            for src_batch in gens.batch(gens.word_list(self.source), self.minibatch):\n                src_batch = fill_batch(src_batch)\n                K = len(src_batch)\n\n                trace('sample %8d - %8d ...' % (generated + 1, generated + K))\n                hyp_batch = self.forward_implement(src_batch, None, src_vocab, trg_vocab, self.attention_dialogue, False, self.generation_limit)\n\n                source_cuont = 0\n                for hyp in hyp_batch:\n                    hyp.append('</s>')\n                    hyp = hyp[:hyp.index('</s>')]\n                    print(\"src : \" + \"\".join(src_batch[source_cuont]).replace(\"</s>\", \"\"))\n                    print('hyp : ' +''.join(hyp))\n                    print(' '.join(hyp), file=fp)\n                    source_cuont = source_cuont + 1\n\n                generated += K\n\n        trace('finished.')\n```\n\n# \u307e\u3068\u3081\n\nPyCon 2016\u3067\u767a\u8868\u3057\u305f\u5185\u5bb9\u3067\u3059\u304c\u3001\u3053\u308c\u3067\u3082\u4e00\u90e8\u3060\u3068\u601d\u3046\u3068\u4ed6\u306e\u90e8\u5206\u306e\u8aac\u660e\u3082\u5408\u308f\u305b\u308b\u3068\u9053\u306e\u308a\u304c\u9577\u305d\u3046\u3067\u3059\u3002\n\u73fe\u72b6\u3067\u5358\u7d14\u306a\u6df1\u5c64\u5b66\u7fd2\u3067\u5bfe\u5fdc\u3067\u304d\u308b\u7bc4\u56f2\u306f\u9650\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u8907\u5408\u7684\u306a\u6280\u8853\u3092\u4f7f\u3063\u3066\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002\n\u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u5bfe\u8a71\u7528\u306e\u30e2\u30c7\u30eb\u304c\u591a\u6570\u51fa\u3066\u304d\u3066\u3044\u308b\u72b6\u6cc1\u306a\u306e\u3067\u8a55\u4fa1\u6307\u6a19\u3092\u78ba\u5b9a\u3057\u3066\u3001\u6df1\u5c64\u5b66\u7fd2\u306e\u30e2\u30c7\u30eb\u3092\u5909\u66f4\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u6027\u80fd\u5411\u4e0a\u306b\u7e4b\u304c\u3063\u3066\u3044\u304f\u3068\u601d\u3044\u307e\u3059\u3002\n\n# \u53c2\u8003\n\n[Attention and Memory in Deep Learning and NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)\n", "tags": ["Chainer", "Python", "\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "\u6a5f\u68b0\u5b66\u7fd2"]}