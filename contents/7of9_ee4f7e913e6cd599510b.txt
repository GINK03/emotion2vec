{"context": "Recurrent Neural Networks (RNNs) \n\u306e\u9762\u767d\u305d\u3046\u306a\u8a18\u4e8b\u3092\u898b\u3064\u3051\u305f\u3002\nhttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n\u4ee5\u4e0b\u3067\u7ffb\u8a33\u3092\u898b\u3064\u3051\u305f\u3002\nhttp://qiita.com/kiminaka/items/87afd4a433dc655d8cfd\n\u7ffb\u8a33\u3092\u8aad\u3093\u3067\u3044\u305f\u304c\u3001\u3044\u304f\u3064\u304b\u306e\u6587\u66f8\u306f\u629c\u3051\u3066\u3044\u308b\u3002\u5b8c\u5168\u7ffb\u8a33\u3067\u3082\u306a\u3044\u3088\u3046\u3060\u3002\n\u5177\u4f53\u7684\u306b\u306f\u3001WHAT ARE RNNS?\u7bc0\u306e\u4ee5\u4e0b\u306e\u6587\u7ae0\u306e\u5bfe\u5fdc\u3059\u308b\u7ffb\u8a33\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u3088\u3046\u3060\u3002\n\u500b\u4eba\u7684\u306b\u8208\u5473\u3092\u3072\u304b\u308c\u305f\u90e8\u5206\u3092\u8272\u4ed8\u3051\u3057\u3066\u307f\u305f\u3002\n\nThere are a few things to note here:\nYou can think of the hidden state $s_t$ as the memory of the network. $s_t$ captures information about what happened in all the previous time steps. The output at step $o_t$ is calculated solely based on the memory at time $t$. As briefly mentioned above, it\u2019s a bit more complicated  in practice because $s_t$ typically can\u2019t capture information from too many time steps ago.\nUnlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters ($U$, $V$, $W$ above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\nThe above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n\n\u4f59\u8ac7\nQiita\u3067\u306f\u6570\u5f0f\u8868\u793a\u306b\u3082\u8272\u3092\u3064\u3051\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u524d\u304b\u3089\u306a\u306e\u304b\u4e0d\u660e\u3060\u304c\u3001\u3053\u308c\u306f\u4fbf\u5229\u304b\u3082\u3057\u308c\u306a\u3044\u3002\nRecurrent Neural Networks (RNNs) \n\u306e\u9762\u767d\u305d\u3046\u306a\u8a18\u4e8b\u3092\u898b\u3064\u3051\u305f\u3002\nhttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n\n\u4ee5\u4e0b\u3067\u7ffb\u8a33\u3092\u898b\u3064\u3051\u305f\u3002\nhttp://qiita.com/kiminaka/items/87afd4a433dc655d8cfd\n\n\u7ffb\u8a33\u3092\u8aad\u3093\u3067\u3044\u305f\u304c\u3001\u3044\u304f\u3064\u304b\u306e\u6587\u66f8\u306f\u629c\u3051\u3066\u3044\u308b\u3002\u5b8c\u5168\u7ffb\u8a33\u3067\u3082\u306a\u3044\u3088\u3046\u3060\u3002\n\n\u5177\u4f53\u7684\u306b\u306f\u3001WHAT ARE RNNS?\u7bc0\u306e\u4ee5\u4e0b\u306e\u6587\u7ae0\u306e\u5bfe\u5fdc\u3059\u308b\u7ffb\u8a33\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u3088\u3046\u3060\u3002\n\n\u500b\u4eba\u7684\u306b\u8208\u5473\u3092\u3072\u304b\u308c\u305f\u90e8\u5206\u3092\u8272\u4ed8\u3051\u3057\u3066\u307f\u305f\u3002\n\n> There are a few things to note here:\n\n> You can think of the hidden state $s_t$ as the memory of the network. $s_t$ captures information about what happened in all the previous time steps. The output at step $o_t$ is calculated solely based on the memory at time $t$. As briefly mentioned above, it\u2019s a bit more complicated  in practice <font color=red>because $s_t$ typically can\u2019t capture information from too many time steps ago</font>.\n\n> Unlike a traditional deep neural network, which uses different parameters at each layer, <font color=red>a RNN shares the same parameters ($U$, $V$, $W$ above) across all steps</font>. This reflects the fact that we are performing the same task at each step, just with different inputs. <font color=red>This greatly reduces the total number of parameters we need to learn</font>.\n\n> The above diagram has outputs at each time step, <font color=red>but depending on the task this may not be necessary</font>. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, <font color=red>we may not need inputs at each time step</font>. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n\n\u4f59\u8ac7\nQiita\u3067\u306f\u6570\u5f0f\u8868\u793a\u306b\u3082\u8272\u3092\u3064\u3051\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u524d\u304b\u3089\u306a\u306e\u304b\u4e0d\u660e\u3060\u304c\u3001\u3053\u308c\u306f\u4fbf\u5229\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n", "tags": ["RNN", "borgWarp", "link"]}