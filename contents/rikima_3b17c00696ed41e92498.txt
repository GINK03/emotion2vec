{"context": "\n\n\u306f\u3058\u3081\u306b\n\u3053\u306e\u8a18\u4e8b\u306fApache Spark Advent Calendar 2015\u306e24\u65e5\u76ee\u306e\u8a18\u4e8b\u3067\u3059\u3002\nSpark mllib\u306eSVM, Logistic Regression\u306e\u5b66\u7fd2\u306b\u4f7f\u308f\u308c\u3066\u3044\u308b mini batch SGD\u5b9f\u88c5\u306b\u3064\u3044\u3066\u8abf\u3079\u305f\u7d50\u679c\u306a\u3069\u3092\u66f8\u304d\u307e\u3059\u3002\nSGD\u306e\u5b9f\u88c5\u306f\u7c21\u5358\u3067\u5fdc\u7528\u3082\u5e83\u304f\u6709\u7528\u306a\u6700\u9069\u5316\u624b\u6cd5\u306a\u306e\u3067Spark\u306e\u5206\u6563\u51e6\u7406\u306e\u67a0\u7d44\u307f\u306e\u4e2d\u3067\u3069\u3046\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u304b\u8208\u5473\u3092\u6301\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\u307e\u305f\u3001SGD\u306e\u5b66\u7fd2\u7387\u3092\u81ea\u52d5\u306b\u6700\u9069\u5316\u3059\u308b\u624b\u6cd5, AdaGrad, Adam etc.\u3092Spark mllib\u3067\u3069\u3046\u5b9f\u88c5\u3067\u304d\u308b\u304b\u306b\u3082\u8208\u5473\u3092\u6301\u3063\u3066\u3044\u3066\u305d\u306e\u89b3\u70b9\u3067\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002\n\u6b21\u306e\u9806\u756a\u306b\u8efd\u304f\u66f8\u3044\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nGradientDescent.scala\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\nSGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308bSpark JIRA\u30c1\u30b1\u30c3\u30c8\nAdaGrad\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\n\n\nGradientDescent.scala\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\nSpark\u306emini batch SGD\u5b9f\u88c5\u306f\n\nSVM\nLogisticRegression\n\n\u306b\u4f7f\u308f\u308c\u3066\u3044\u3066\u3001\u305d\u306e\u672c\u4f53\u306fGradientDescent.scala\u306b\u306a\u308a\u307e\u3059\u3002\n\u3053\u306eGradientDescent.scala\u3067Spark\u306e\u5206\u6563\u51e6\u7406\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u3001\u5206\u6563\u306emini batch SGD\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u3053\u3053\u3067\u3001GradientDescent.scala\u306e\u4ee5\u4e0b\u306e\u89b3\u70b9\u3067\u89e3\u8aac\u3092\u884c\u3063\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u5206\u6563\u3067\u3069\u3046\u3084\u3063\u3066gradient update\u3084\u3063\u3066\u3044\u308b\u304b\n\n\nworker\u306e\u7d50\u679c\u3092\u5e73\u5747\u3057\u3066\u3044\u308b\u3002\n\u3069\u3053\u3067\u3001broadcast\u3067\u914d\u3063\u3066, driver\u3067\u5e73\u5747\u3068\u3063\u3066\u3044\u308b\nshuffle\u3067\u5e73\u5747\u3057\u3066\u3044\u308b\uff1f\n\n\n\u5b66\u7fd2\u4fc2\u6570\u306f\u3069\u3046\u8a2d\u5b9a\u3001\u66f4\u65b0\u3055\u308c\u308b\u306e\u304b\uff1f\nAdaGrad\u3068\u304b\u306b\u4fee\u6b63\u5b9f\u88c5\u3067\u304d\u305d\u3046\u304b\uff1f\nBSP\u3068\u306e\u9055\u3044\u306f\uff1f\nSSP\u306b\u3059\u308b\u306b\u306f\u4f55\u304c\u3044\u308b\uff1f\n\nGradientDescent.scala\u306e\u30b3\u30a2\u306e\u51e6\u7406\u306frunMiniBatchSGD\u3067\u3001\n\u3053\u306emethod\u5185\u3060\u3051\u3067\u5206\u6563\u51e6\u7406\u306b\u3088\u308bSGD\u304c\u51e6\u7406\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u51e6\u7406\u81ea\u4f53\u306f\u3055\u307b\u3069\u9577\u304f\u306a\u304f\u3001\u57fa\u672c\u7684\u306bSpark RDD\u306b\u3088\u308b\u96c6\u8a08\u51e6\u7406\u306e\u7e70\u308a\u8fd4\u3057\u306e\u307f\u3067\u5b9f\u73fe\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\nGradientDescent.scala\n/**\n     * For the first iteration, the regVal will be initialized as sum of weight squares\n     * if it's L2 updater; for L1 updater, the same logic is followed.\n     */\n    var regVal = updater.compute(\n      weights, Vectors.zeros(weights.size), 0, 1, regParam)._2\n\n    var converged = false // indicates whether converged based on convergenceTol\n    var i = 1\n\u203b1  while (!converged && i <= numIterations) {\n\u203b2    val bcWeights = data.context.broadcast(weights)\n      // Sample a subset (fraction miniBatchFraction) of the total data\n      // compute and sum up the subgradients on this subset (this is one map-reduce)\n\u203b3      val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))\n            (c._1, c._2 + l, c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      if (miniBatchSize > 0) {\n        /**\n         * lossSum is computed using the weights from the previous iteration\n         * and regVal is the regularization value computed in the previous iteration as well.\n         */\n        stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n\u203b4       val update = updater.compute(\n          weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble),\n          stepSize, i, regParam)\n        weights = update._1\n        regVal = update._2\n\n        previousWeights = currentWeights\n        currentWeights = Some(weights)\n\u203b5      if (previousWeights != None && currentWeights != None) {\n          converged = isConverged(previousWeights.get,\n            currentWeights.get, convergenceTol)\n        }\n      } else {\n        logWarning(s\"Iteration ($i/$numIterations). The size of sampled batch is zero\")\n      }\n      i += 1\n    }\n\n    logInfo(\"GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses %s\".format(\n      stochasticLossHistory.takeRight(10).mkString(\", \")))\n\n    (weights, stochasticLossHistory.toArray)\n\n\n\u4e0a\u8a18\u306e\u30bd\u30fc\u30b9\u3092\u9806\u306b\u8aac\u660e\u3057\u307e\u3059\u3002\n\n\u203b1\u304c\u30e1\u30a4\u30f3\u306e\u53cd\u5fa9\u51e6\u7406\u3067\u3001maxIteration\u56de\u51e6\u7406\u3059\u308b\u304b\u53ce\u675f\u3059\u308b\u304b\u307e\u3067\u52fe\u914d\u306e\u8a08\u7b97\u3001\u66f4\u65b0\u304c\u51e6\u7406\u3055\u308c\u307e\u3059\u3002\n\u203b2\u3067\u3001current\u306eweight vector\u304cdriver\u304b\u3089\u5404worker\u306bbroadcast\u3055\u308c\u307e\u3059\u3002\u5404worker\u3067\u8a08\u7b97\u3055\u308c\u305f\u52fe\u914d\u304c\u6b21\u306etreeAggregation\u3067\u96c6\u8a08\u3055\u308c\u3066\u5168\u4f53\u306e\u52fe\u914d\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u96c6\u8a08\u3055\u308c\u305f\u52fe\u914d\u3092\u4f7f\u3063\u3066weight vector\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002\n\u203b3 broadcast\u3055\u308c\u305fcurrent weight  vector\u3092\u4f7f\u3063\u3066\u5404example\u306e\u640d\u5931\u3001\u52fe\u914d\u304c\u8a08\u7b97\u3055\u308c\u3066\u3001treeAggregation\u3067\u96c6\u8a08\u3055\u308c\u3066driver\u306b\u4fdd\u6301\u3055\u308c\u307e\u3059\u3002\n\u203b4 \u96c6\u8a08\u3055\u308c\u305f\u52fe\u914d\u3092\u4f7f\u3063\u3066weight vector\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u53ce\u675f\u5224\u5b9a\u306f\u3001loss\u3092\u898b\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u304f\u3066\u3001weight vector\u306e\u5909\u5316\u91cf\u3067\u5224\u65ad\u3057\u3066\u3044\u307e\u3059\u3002\u203b\uff15\u3067\u3002\n\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001Spark mllib\u306eSGD\u5b9f\u88c5\u306f\u3001\u5404worker\u3067\u72ec\u7acb\u306bcurrent weight vector\u3092\u4f7f\u3063\u3066\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u3001\u305d\u306e\u52fe\u914d\u3092driver\u3067\u96c6\u8a08\u3057\u3066weight vector\u3092\u8a08\u7b97\u3059\u308bBSP\u306b\u57fa\u3065\u304f\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\nUpdator.scala\n\n/**\n * :: DeveloperApi ::\n * Updater for L2 regularized problems.\n *          R(w) = 1/2 ||w||^2\n * Uses a step-size decreasing with the square root of the number of iterations.\n */\n@DeveloperApi\nclass SquaredL2Updater extends Updater {\n\u203b6 override def compute(\n      weightsOld: Vector,\n      gradient: Vector,\n      stepSize: Double,\n      iter: Int,\n      regParam: Double): (Vector, Double) = {\n    // add up both updates from the gradient of the loss (= step) as well as\n    // the gradient of the regularizer (= regParam * weightsOld)\n    // w' = w - thisIterStepSize * (gradient + regParam * w)\n    // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient\n\u203b7  val thisIterStepSize = stepSize / math.sqrt(iter)\n    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n    brzWeights :*= (1.0 - thisIterStepSize * regParam)\n    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n    val norm = brzNorm(brzWeights, 2.0)\n\n    (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)\n  }\n}\n\n\nweight vector\u306e\u66f4\u65b0\u306fUpdator.scala\u3067\u884c\u308f\u308c\u307e\u3059\u3002L2\u6b63\u5247\u5316\u3092\u4f7f\u3046\u5834\u5408\u306f\n\u203b6\u306e\u90e8\u5206\u306ecompute method\u3067\u884c\u308f\u308c\u307e\u3059\u3002\n\u5b66\u7fd2\u7387\u3001currentStepSize\u306f\u203b7\u306b\u304a\u3044\u3066 1/sqrt(iter)\u3067\u8abf\u6574\u3055\u308c\u307e\u3059\u3002\nspark mllib\u306eSGD\u5b9f\u88c5\u306f\u4ee5\u4e0a\u306e\u3088\u3046\u306b\u3001RDD\u3092\u4f7f\u3063\u3066\u5206\u6563\u51e6\u7406\u3067\u5404\u4e8b\u4f8b\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3092\u3057\u3066\u3001\u305d\u306e\u96c6\u8a08\u3092RDD\u306etreeAggregation\u3092\u4f7f\u3063\u3066\u306f\u3044\u307e\u3059\u304c\u3001\u57fa\u672c\u3001\u666e\u901a\u306e\u52fe\u914d\u6cd5\u3068\u4f55\u3082\u5909\u308f\u3089\u306a\u3044\u3001\u7c21\u5358\u3067\u3001\u308f\u304b\u308a\u3084\u3059\u3044\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u5404\u53cd\u5fa9\u6642\u306b\u3001worker\u306e\u52fe\u914d\u8a08\u7b97\u7d50\u679c\u3092\u3059\u3079\u3066driver\u3067\u96c6\u8a08\u3059\u308bBSP\u5b9f\u88c5\u306b\u3082\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u5b66\u7fd2\u7387\u306b\u95a2\u3057\u3066\u306f\u3001\u53cd\u5fa9\u6570\u306b\u3088\u308a\u6e1b\u8870\u3055\u305b\u3066\u3044\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u3082\u306e\u3067\u3001AdaGrad, ADAM\u7b49\u306b\u306f\u6bd4\u8f03\u7684\u7c21\u5358\u306b\u62e1\u5f35\u3001\u5909\u66f4\u304c\u3067\u304d\u307e\u3059\u3002\n\uff08\u3069\u3053\u3067\u3001\u3069\u306e\u3088\u3046\u306b\u5b9f\u88c5\u3059\u308b\u304b\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3001\u8a2d\u8a08\u7684\u306a\u89b3\u70b9\u3067\u3044\u308d\u3044\u308d\u96e3\u3057\u3044\u3001\u8abf\u6574\u3001\u8b70\u8ad6\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3002\u3002\u3002\uff09\n\u3053\u306e\u3088\u3046\u306a\u3001\u30b7\u30f3\u30d7\u30eb\u3067\u308f\u304b\u308a\u3084\u3059\u3044SGD\u5b9f\u88c5\u3067\u3059\u304c\u3001\u53ce\u675f\u304c\u9045\u3044\u3001mini batch sampling\u304c\u52b9\u7387\u7684\u3067\u306a\u3044\u306a\u3069\u3001\u3044\u308d\u3044\u308d\u8a00\u308f\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3001JIRA\u306e\u30c1\u30b1\u30c3\u30c8\u3067\u8b70\u8ad6\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u6b21\u306b\u3053\u308c\u306b\u3064\u3044\u3066\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\nSpark mllib\u306eSGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308b Jira\u30c1\u30b1\u30c3\u30c8\nSGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308bJIRA\u30c1\u30b1\u30c3\u30c8\u306b\u4e0b\u8a18\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002\n\n\nUse faster converging optimization method in MLlib \nImplement Nesterov's accelerated first-order method\nAn optimized gradient descent implementation\nSGD implementation is not efficient\n\n2\uff0e\u306f\u3001\u666e\u901a\u306e\u52fe\u914d\u6cd5\u3088\u308a\u3082\u901f\u304f\u53ce\u675f\u3059\u308bNesterov's accelerated first-order method\u3092\u4f7f\u3046\u5b9f\u88c5\u3067SGD, L-BFGS\u306e\u5b9f\u88c5\u3092\u7f6e\u304d\u63db\u3048\u307e\u3057\u3087\u3046\u3068\u3044\u3046\u8a71\u3002\u6628\u5e74\u306e6\u6708\u3067\u8b70\u8ad6\u304c\u6b62\u307e\u3063\u3066\u3044\u308b\u3002pull-req\u3082\u51fa\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u624b\u6cd5\u3092\u8abf\u67fb\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u308b\u304c\u898b\u308c\u3066\u306a\u3044\u3002\n3\uff0e \u306fmini batch sampling\u3059\u308b\u969b\u306bRDD\u306eiterator interface\u3092\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u5168\u306a\u3081\u3059\u308b\u306e\u3067\uff1f\u52b9\u7387\u7684\u3067\u306a\u3044\u304b\u3089\u3082\u3063\u3068\u52b9\u7387\u7684\u306b\u3067\u304d\u306a\u3044\u304b\uff1f\u3068\u3044\u3046\u30c1\u30b1\u30c3\u30c8\n2.\u306f\u4eca\u306eSGD\u5b9f\u88c5\u306fBSP\u30d9\u30fc\u30b9\u306a\u306e\u3067SSP\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3082\u3046\u3061\u3087\u3063\u3068\u52b9\u7387\u5316\u3001\u9ad8\u901f\u5316\u3067\u304d\u3093\u3058\u3083\u306a\u3044\u306e\u3068\u3044\u3046\u8a71\uff1f local update\u3068DistBrief\u307f\u305f\u3044\u306b\u3059\u308b\u3068\u3044\u3046\u5b9f\u88c5\u306f\u5177\u4f53\u7684\u306b\u66f8\u3044\u3066\u306a\u3044\u306e\u3067\u8a73\u7d30\u306f\u4e0d\u660e\n\u52fe\u914d\u6cd5\u306e\u5b9f\u88c5\u3082\u3001BSP\u30d9\u30fc\u30b9\u306e\u5206\u6563\u52fe\u914d\u6cd5\u306e\u5b9f\u88c5\u3082\u7c21\u5358\u306a\u306e\u3067\u3001\u4e0a\u8a18\u306e\u30c1\u30b1\u30c3\u30c8\u3082\u3059\u3093\u306a\u308a\u9032\u3080\u3068\u601d\u3063\u3066\u3044\u305f\u304c\u3001\u3069\u308c\u3082\u6b62\u307e\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\nSpark mllib \u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u7cbe\u5ea6\u6e2c\u5b9a\u306b\u3082\u66f8\u3044\u305f\u304c\u3001\u73fe\u72b6\u306eSGD\u5b9f\u88c5\u3092\u4f7f\u3063\u305fSVM, Logistic Regression\u306e\u7cbe\u5ea6\u306fL-BFGS\u306b\u3088\u308bLogisitic Regression\u306b\u6bd4\u3079\u3066\u8457\u3057\u304f\u60aa\u304b\u3063\u305f\u3002\u3053SGD\u3092\u4f7f\u3063\u305f\u5834\u5408\u306b\u7cbe\u5ea6\u304c\u4e0b\u304c\u308b\u554f\u984c\u306b\u624b\u3092\u3064\u3051\u3089\u308c\u3066\u306a\u3044\u306e\u3067\u3001\u5148\u306b\u9032\u3081\u3066\u3044\u306a\u3044\u306e\u3060\u304c\u3001\u4e0a\u306e\u30c1\u30b1\u30c3\u30c8\u306b\u95a2\u4fc2\u3059\u308bSGD\u306e\u62e1\u5f35\u3001improve\u3059\u308b\u3088\u3046\u306a\u4f55\u304b\u3092\u3057\u305f\u3044\u306a\u3069\u3068\u601d\u3063\u3066\u307e\u3059\u3002\n\nAdaGrad \u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\nAdaGrad\u306fSGD\u306e\u3092weight vector\u306e\u8981\u7d20\u6bce\u306e\u66f4\u65b0\u7387\u3092\u306e\u52fe\u914d\u306e\u5c65\u6b74\u304b\u3089\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\n\\begin{equation}\n{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\alpha \\ {\\bf k}^{(t)} \\circ {\\bf g}^{(t)}\n\\end{equation}\nw(t+1)=w(t)\u2212\u03b1\u00a0k(t)\u2218g(t){\\begin{equation}\n{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\alpha \\ {\\bf k}^{(t)} \\circ {\\bf g}^{(t)}\n\\end{equation}\n}\n\\begin{equation}\n({\\bf k}^{(t)})_a = \n\\frac{1}{ \\sqrt {\\epsilon + \\sum_{k=1}^t |g_a^{(k)}|^2 } }\n\\end{equation}\n(k(t))a=1\u03f5+\u2211tk=1|g(k)a|2\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u221a{\\begin{equation}\n({\\bf k}^{(t)})_a = \n\\frac{1}{ \\sqrt {\\epsilon + \\sum_{k=1}^t |g_a^{(k)}|^2 } }\n\\end{equation}\n}\n\u3053\u3053\u3067\u3001\u25cb\u306fvector\u306eelement wise\u306e\u7a4d\u3002\u6dfb\u5b57\u306ea\u306f\u6210\u5206\u3092\u8868\u3059\u3002\nSpark mllib\u3067\u306e\u5b9f\u88c5\u3082\u3054\u3061\u3083\u3054\u3061\u3083\u8a00\u308f\u306a\u3051\u308c\u3070\uff08\u7dba\u9e97\u3068\u304b\u3001\u30dd\u30ea\u30b7\u30fc\u3068\u304b\u3001\u3044\u308d\u3044\u308d\uff09\u975e\u5e38\u306b\u7c21\u5358\u306b\u3067\u304d\u308b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n\u3067\u5b9f\u88c5\u3057\u3066\u305d\u306e\u52b9\u679c\u3092\u5171\u6709\u3057\u305f\u3044\u3068\u304a\u3082\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u3001\u3001\u3082\u3068\u3082\u3068\u306eSGD\u305d\u306e\u3082\u306e\u306e\u7cbe\u5ea6\u3001\u53ce\u675f\u304c\u3061\u3087\u3063\u3068\u304a\u304b\u3057\u304f\u3066\u3001AdaGrad\u306b\u3088\u308b\u52b9\u679c\u3092\u6e2c\u5b9a\u3001\u8a55\u4fa1\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\u3002\u3002\n\u4ed6\u306e\u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u30aa\u30ea\u30b8\u30ca\u30eb\u5b9f\u88c5\u3092Spark\u4e0a\u3067\u3082\u5b9f\u88c5\u3057\u306a\u304a\u3057\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u305d\u306e\u6642\u306b\u3001SGD\u5b9f\u88c5\u306e\u59a5\u5f53\u6027\u3068AdaGrad\u7b49\u306e\u5b66\u7fd2\u7387\u306e\u81ea\u52d5\u8abf\u6574\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u52b9\u679c\u3092\u8a55\u4fa1\u3057\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u3055\u3044\u3054\u306b\nSpark mllib\u306eSGD\u5b9f\u88c5\u306b\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002\u524d\u304b\u3089\u6982\u8981\u306f\u7406\u89e3\u3057\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u8a73\u7d30\u3092\u898b\u308b\u3053\u3068\u3067\n\nBSP\u3068\u540c\u3058\u3002worker\u3067\u5404\u4e8b\u4f8b\u6bce\u306b\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u3001driver\u3067\u96c6\u8a08(super iteration)weight vector\u3092\u66f4\u65b0\u3059\u308b\u3002\n\u5b66\u7fd2\u7387\u306e\u66f4\u65b0\u306fUpdator\u3067\u3084\u3063\u3066\u3044\u308b\u3002AdaGrad etc.\u3078\u306e\u5909\u66f4\u306f\u7c21\u6613\u306b\u3067\u304d\u305d\u3046\n\u52fe\u914d\u306e\u96c6\u8a08\u306fRDD treeAggregation\u3092\u4f7f\u3063\u3066\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3002\u96e3\u3057\u3044\u3053\u3068\u306f\u4f55\u3082\u3057\u3066\u306a\u3044\u3002reduce\u3057\u3066\u3044\u308b\u3060\u3051\n\u5206\u6563\u3067SGD\u306a\u306e\u3067\u3001\u5404worker\u3067\u52fe\u914d\u3001weight vector\u3092\u66f4\u65b0\u3057\u3066\u3001driver\u3067weight vector\u306e\u5e73\u5747\u3092\u3068\u3063\u3066\u3044\u308b\u3068\u52d8\u9055\u3044\u3057\u3066\u3044\u305f\u304c\u305d\u3046\u3067\u306f\u306a\u3044\u3002\nSGD\u3067\u304d\u3066\u3044\u308b\u306a\u3089online\u5b66\u7fd2\u3082\u305d\u306e\u307e\u307e\u884c\u3051\u308b\u3068\u601d\u3063\u305f\u304c\u52b9\u7387\u7684\u306a\u5206\u6563\u51e6\u7406\u3060\u3068\u81ea\u660e\u3058\u3083\u306a\u3044\u3063\u307d\u3044\u3002\u305d\u3053\u306f\u3061\u3087\u3063\u3068\u307e\u3058\u3081\u306b\u8003\u3048\u305f\u3044\n\n\n\nAsynchronous Complex Analytics in a Distributed\nDataflow Architecture (Extended Abstract) \u3068\u304b\u3001SSP\u3068\u304b\u3092\u771f\u9762\u76ee\u306b\u8003\u3048\u306a\u3044\u3068\u3044\u3051\u306a\u3044\n\n\n\n\u3068\u3044\u3046\u3088\u3046\u306a\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3053\u306e\u8fba\u30e1\u30a4\u30f3\u306b\u8003\u3048\u305f\u3044\u306a\u3068\u601d\u3046\u3053\u306e\u3054\u308d\u3067\u3059\u3002\n\nP.S. \u7684\u306a\n\u66f8\u3044\u3066\u6700\u5f8c\u306b\u3055\u3089\u306b\u5099\u5fd8\u9332\u7684\u306b\u8ffd\u8a18\u3059\u308b\u3068\nonline\u5b66\u7fd2\u3068\u304b\u3001SSP\u3068\u304b\u3084\u308d\u3046\u3068\u3059\u308b\u3068Spark\u3001RDD\u3092\u4f7f\u3063\u3066\u3084\u308b\u306e\u3068\n\u7d20\u3067Akka\u3092\u4f7f\u3063\u3066\u81ea\u524d\u5b9f\u88c5\u3059\u308b\u306e\u3068\u3069\u3063\u3061\u304c\u826f\u3044\u304b\u3001\u3069\u3063\u3061\u304c\u7c21\u5358\u3067\u958b\u767a\u3057\u3084\u3059\u3044\u304b\u3068\u601d\u3046\u3002\n\u672a\u3060\u306bSpark\u4e0a\u3067\u958b\u767a\u3059\u308b\u306e\u306f\u52b9\u7387\u7684\u306b\u884c\u3048\u3066\u306a\u3044\u306e\u3067\u3002\u3002\u3002\nAkka\u3092\u4f7f\u3046\u30b7\u30b9\u30c6\u30e0\u306e\u30b9\u30b1\u30eb\u30c8\u30f3\u3092\u4f5c\u3063\u3066\u3057\u307e\u3063\u3066\u3001\nworker\u4e0a\u306e\u30de\u30b7\u30f3\u304c\u30ed\u30fc\u30ab\u30eb\u306e\u30c7\u30fc\u30bf\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f5c\u3063\u3066\u3057\u307e\u3048\u3070\nAkka\u4e0a\u3067\u5b9f\u88c5\u3059\u308b\u307b\u3046\u304c\u67d4\u8edf\u306a\u6c17\u3082\u3057\u306a\u3044\u3067\u3082\u306a\u3044\u3053\u306e\u3054\u308d\u3002\u3002\u3002\n#\u306f\u3058\u3081\u306b\n\n\u3053\u306e\u8a18\u4e8b\u306fApache [Spark Advent Calendar 2015](http://qiita.com/advent-calendar/2015/apache-spark)\u306e24\u65e5\u76ee\u306e\u8a18\u4e8b\u3067\u3059\u3002\nSpark mllib\u306eSVM, Logistic Regression\u306e\u5b66\u7fd2\u306b\u4f7f\u308f\u308c\u3066\u3044\u308b mini batch SGD\u5b9f\u88c5\u306b\u3064\u3044\u3066\u8abf\u3079\u305f\u7d50\u679c\u306a\u3069\u3092\u66f8\u304d\u307e\u3059\u3002\n\nSGD\u306e\u5b9f\u88c5\u306f\u7c21\u5358\u3067\u5fdc\u7528\u3082\u5e83\u304f\u6709\u7528\u306a\u6700\u9069\u5316\u624b\u6cd5\u306a\u306e\u3067Spark\u306e\u5206\u6563\u51e6\u7406\u306e\u67a0\u7d44\u307f\u306e\u4e2d\u3067\u3069\u3046\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u304b\u8208\u5473\u3092\u6301\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n\u307e\u305f\u3001SGD\u306e\u5b66\u7fd2\u7387\u3092\u81ea\u52d5\u306b\u6700\u9069\u5316\u3059\u308b\u624b\u6cd5, AdaGrad, Adam etc.\u3092Spark mllib\u3067\u3069\u3046\u5b9f\u88c5\u3067\u304d\u308b\u304b\u306b\u3082\u8208\u5473\u3092\u6301\u3063\u3066\u3044\u3066\u305d\u306e\u89b3\u70b9\u3067\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u6b21\u306e\u9806\u756a\u306b\u8efd\u304f\u66f8\u3044\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n- GradientDescent.scala\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\n- SGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308bSpark JIRA\u30c1\u30b1\u30c3\u30c8\n- AdaGrad\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\n\n\n# GradientDescent.scala\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u30ea\u30fc\u30c7\u30a3\u30f3\u30b0\n\nSpark\u306emini batch SGD\u5b9f\u88c5\u306f\n\n- SVM\n- LogisticRegression\n\n\u306b\u4f7f\u308f\u308c\u3066\u3044\u3066\u3001\u305d\u306e\u672c\u4f53\u306f[GradientDescent.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/optimization/GradientDescent.scala\n)\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u3053\u306eGradientDescent.scala\u3067Spark\u306e\u5206\u6563\u51e6\u7406\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066\u3001\u5206\u6563\u306emini batch SGD\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u3001GradientDescent.scala\u306e\u4ee5\u4e0b\u306e\u89b3\u70b9\u3067\u89e3\u8aac\u3092\u884c\u3063\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n- \u5206\u6563\u3067\u3069\u3046\u3084\u3063\u3066gradient update\u3084\u3063\u3066\u3044\u308b\u304b\n - worker\u306e\u7d50\u679c\u3092\u5e73\u5747\u3057\u3066\u3044\u308b\u3002\n - \u3069\u3053\u3067\u3001broadcast\u3067\u914d\u3063\u3066, driver\u3067\u5e73\u5747\u3068\u3063\u3066\u3044\u308b\n - shuffle\u3067\u5e73\u5747\u3057\u3066\u3044\u308b\uff1f\n- \u5b66\u7fd2\u4fc2\u6570\u306f\u3069\u3046\u8a2d\u5b9a\u3001\u66f4\u65b0\u3055\u308c\u308b\u306e\u304b\uff1f\n- AdaGrad\u3068\u304b\u306b\u4fee\u6b63\u5b9f\u88c5\u3067\u304d\u305d\u3046\u304b\uff1f\n- BSP\u3068\u306e\u9055\u3044\u306f\uff1f\n- SSP\u306b\u3059\u308b\u306b\u306f\u4f55\u304c\u3044\u308b\uff1f\n\nGradientDescent.scala\u306e\u30b3\u30a2\u306e\u51e6\u7406\u306f[runMiniBatchSGD](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/optimization/GradientDescent.scala#L174)\u3067\u3001\n\n\n\u3053\u306emethod\u5185\u3060\u3051\u3067\u5206\u6563\u51e6\u7406\u306b\u3088\u308bSGD\u304c\u51e6\u7406\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u51e6\u7406\u81ea\u4f53\u306f\u3055\u307b\u3069\u9577\u304f\u306a\u304f\u3001\u57fa\u672c\u7684\u306bSpark RDD\u306b\u3088\u308b\u96c6\u8a08\u51e6\u7406\u306e\u7e70\u308a\u8fd4\u3057\u306e\u307f\u3067\u5b9f\u73fe\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n```GradientDescent.scala\n/**\n     * For the first iteration, the regVal will be initialized as sum of weight squares\n     * if it's L2 updater; for L1 updater, the same logic is followed.\n     */\n    var regVal = updater.compute(\n      weights, Vectors.zeros(weights.size), 0, 1, regParam)._2\n\n    var converged = false // indicates whether converged based on convergenceTol\n    var i = 1\n\u203b1  while (!converged && i <= numIterations) {\n\u203b2    val bcWeights = data.context.broadcast(weights)\n      // Sample a subset (fraction miniBatchFraction) of the total data\n      // compute and sum up the subgradients on this subset (this is one map-reduce)\n\u203b3      val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction, 42 + i)\n        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(\n          seqOp = (c, v) => {\n            // c: (grad, loss, count), v: (label, features)\n            val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))\n            (c._1, c._2 + l, c._3 + 1)\n          },\n          combOp = (c1, c2) => {\n            // c: (grad, loss, count)\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\n          })\n\n      if (miniBatchSize > 0) {\n        /**\n         * lossSum is computed using the weights from the previous iteration\n         * and regVal is the regularization value computed in the previous iteration as well.\n         */\n        stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n\u203b4       val update = updater.compute(\n          weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble),\n          stepSize, i, regParam)\n        weights = update._1\n        regVal = update._2\n\n        previousWeights = currentWeights\n        currentWeights = Some(weights)\n\u203b5      if (previousWeights != None && currentWeights != None) {\n          converged = isConverged(previousWeights.get,\n            currentWeights.get, convergenceTol)\n        }\n      } else {\n        logWarning(s\"Iteration ($i/$numIterations). The size of sampled batch is zero\")\n      }\n      i += 1\n    }\n\n    logInfo(\"GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses %s\".format(\n      stochasticLossHistory.takeRight(10).mkString(\", \")))\n\n    (weights, stochasticLossHistory.toArray)\n```\n\n\u4e0a\u8a18\u306e\u30bd\u30fc\u30b9\u3092\u9806\u306b\u8aac\u660e\u3057\u307e\u3059\u3002\n\n- \u203b1\u304c\u30e1\u30a4\u30f3\u306e\u53cd\u5fa9\u51e6\u7406\u3067\u3001maxIteration\u56de\u51e6\u7406\u3059\u308b\u304b\u53ce\u675f\u3059\u308b\u304b\u307e\u3067\u52fe\u914d\u306e\u8a08\u7b97\u3001\u66f4\u65b0\u304c\u51e6\u7406\u3055\u308c\u307e\u3059\u3002\n\n- \u203b2\u3067\u3001current\u306eweight vector\u304cdriver\u304b\u3089\u5404worker\u306bbroadcast\u3055\u308c\u307e\u3059\u3002\u5404worker\u3067\u8a08\u7b97\u3055\u308c\u305f\u52fe\u914d\u304c\u6b21\u306etreeAggregation\u3067\u96c6\u8a08\u3055\u308c\u3066\u5168\u4f53\u306e\u52fe\u914d\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u96c6\u8a08\u3055\u308c\u305f\u52fe\u914d\u3092\u4f7f\u3063\u3066weight vector\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002\n\n- \u203b3 broadcast\u3055\u308c\u305fcurrent weight  vector\u3092\u4f7f\u3063\u3066\u5404example\u306e\u640d\u5931\u3001\u52fe\u914d\u304c\u8a08\u7b97\u3055\u308c\u3066\u3001treeAggregation\u3067\u96c6\u8a08\u3055\u308c\u3066driver\u306b\u4fdd\u6301\u3055\u308c\u307e\u3059\u3002\n\n- \u203b4 \u96c6\u8a08\u3055\u308c\u305f\u52fe\u914d\u3092\u4f7f\u3063\u3066weight vector\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u53ce\u675f\u5224\u5b9a\u306f\u3001loss\u3092\u898b\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u304f\u3066\u3001weight vector\u306e\u5909\u5316\u91cf\u3067\u5224\u65ad\u3057\u3066\u3044\u307e\u3059\u3002\u203b\uff15\u3067\u3002\n\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001Spark mllib\u306eSGD\u5b9f\u88c5\u306f\u3001\u5404worker\u3067\u72ec\u7acb\u306bcurrent weight vector\u3092\u4f7f\u3063\u3066\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u3001\u305d\u306e\u52fe\u914d\u3092driver\u3067\u96c6\u8a08\u3057\u3066weight vector\u3092\u8a08\u7b97\u3059\u308bBSP\u306b\u57fa\u3065\u304f\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\n\n```Updator.scala\n\n/**\n * :: DeveloperApi ::\n * Updater for L2 regularized problems.\n *          R(w) = 1/2 ||w||^2\n * Uses a step-size decreasing with the square root of the number of iterations.\n */\n@DeveloperApi\nclass SquaredL2Updater extends Updater {\n\u203b6 override def compute(\n      weightsOld: Vector,\n      gradient: Vector,\n      stepSize: Double,\n      iter: Int,\n      regParam: Double): (Vector, Double) = {\n    // add up both updates from the gradient of the loss (= step) as well as\n    // the gradient of the regularizer (= regParam * weightsOld)\n    // w' = w - thisIterStepSize * (gradient + regParam * w)\n    // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient\n\u203b7  val thisIterStepSize = stepSize / math.sqrt(iter)\n    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n    brzWeights :*= (1.0 - thisIterStepSize * regParam)\n    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n    val norm = brzNorm(brzWeights, 2.0)\n\n    (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)\n  }\n}\n```\n\nweight vector\u306e\u66f4\u65b0\u306fUpdator.scala\u3067\u884c\u308f\u308c\u307e\u3059\u3002L2\u6b63\u5247\u5316\u3092\u4f7f\u3046\u5834\u5408\u306f\n\u203b6\u306e\u90e8\u5206\u306ecompute method\u3067\u884c\u308f\u308c\u307e\u3059\u3002\n\n\u5b66\u7fd2\u7387\u3001currentStepSize\u306f\u203b7\u306b\u304a\u3044\u3066 1/sqrt(iter)\u3067\u8abf\u6574\u3055\u308c\u307e\u3059\u3002\n\nspark mllib\u306eSGD\u5b9f\u88c5\u306f\u4ee5\u4e0a\u306e\u3088\u3046\u306b\u3001RDD\u3092\u4f7f\u3063\u3066\u5206\u6563\u51e6\u7406\u3067\u5404\u4e8b\u4f8b\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3092\u3057\u3066\u3001\u305d\u306e\u96c6\u8a08\u3092RDD\u306etreeAggregation\u3092\u4f7f\u3063\u3066\u306f\u3044\u307e\u3059\u304c\u3001\u57fa\u672c\u3001\u666e\u901a\u306e\u52fe\u914d\u6cd5\u3068\u4f55\u3082\u5909\u308f\u3089\u306a\u3044\u3001\u7c21\u5358\u3067\u3001\u308f\u304b\u308a\u3084\u3059\u3044\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u5404\u53cd\u5fa9\u6642\u306b\u3001worker\u306e\u52fe\u914d\u8a08\u7b97\u7d50\u679c\u3092\u3059\u3079\u3066driver\u3067\u96c6\u8a08\u3059\u308bBSP\u5b9f\u88c5\u306b\u3082\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u5b66\u7fd2\u7387\u306b\u95a2\u3057\u3066\u306f\u3001\u53cd\u5fa9\u6570\u306b\u3088\u308a\u6e1b\u8870\u3055\u305b\u3066\u3044\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u3082\u306e\u3067\u3001AdaGrad, ADAM\u7b49\u306b\u306f\u6bd4\u8f03\u7684\u7c21\u5358\u306b\u62e1\u5f35\u3001\u5909\u66f4\u304c\u3067\u304d\u307e\u3059\u3002\n\uff08\u3069\u3053\u3067\u3001\u3069\u306e\u3088\u3046\u306b\u5b9f\u88c5\u3059\u308b\u304b\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3001\u8a2d\u8a08\u7684\u306a\u89b3\u70b9\u3067\u3044\u308d\u3044\u308d\u96e3\u3057\u3044\u3001\u8abf\u6574\u3001\u8b70\u8ad6\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3002\u3002\u3002\uff09\n\n\u3053\u306e\u3088\u3046\u306a\u3001\u30b7\u30f3\u30d7\u30eb\u3067\u308f\u304b\u308a\u3084\u3059\u3044SGD\u5b9f\u88c5\u3067\u3059\u304c\u3001\u53ce\u675f\u304c\u9045\u3044\u3001mini batch sampling\u304c\u52b9\u7387\u7684\u3067\u306a\u3044\u306a\u3069\u3001\u3044\u308d\u3044\u308d\u8a00\u308f\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3001JIRA\u306e\u30c1\u30b1\u30c3\u30c8\u3067\u8b70\u8ad6\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u6b21\u306b\u3053\u308c\u306b\u3064\u3044\u3066\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\n# Spark mllib\u306eSGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308b Jira\u30c1\u30b1\u30c3\u30c8\n\nSGD\u5b9f\u88c5\u306b\u95a2\u4fc2\u3059\u308bJIRA\u30c1\u30b1\u30c3\u30c8\u306b\u4e0b\u8a18\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002\n\n\n1. [Use faster converging optimization method in MLlib]( https://issues.apache.org/jira/browse/SPARK-6346?filter=12332858) \n2. [Implement Nesterov's accelerated first-order method]( https://issues.apache.org/jira/browse/SPARK-1503)\n- [An optimized gradient descent implementation](\nhttps://issues.apache.org/jira/browse/SPARK-1270?filter=12332858)\n- [SGD implementation is not efficient](\nhttps://issues.apache.org/jira/browse/SPARK-1359?filter=12332858)\n\n2\uff0e\u306f\u3001\u666e\u901a\u306e\u52fe\u914d\u6cd5\u3088\u308a\u3082\u901f\u304f\u53ce\u675f\u3059\u308bNesterov's accelerated first-order method\u3092\u4f7f\u3046\u5b9f\u88c5\u3067SGD, L-BFGS\u306e\u5b9f\u88c5\u3092\u7f6e\u304d\u63db\u3048\u307e\u3057\u3087\u3046\u3068\u3044\u3046\u8a71\u3002\u6628\u5e74\u306e6\u6708\u3067\u8b70\u8ad6\u304c\u6b62\u307e\u3063\u3066\u3044\u308b\u3002pull-req\u3082\u51fa\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u624b\u6cd5\u3092\u8abf\u67fb\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u308b\u304c\u898b\u308c\u3066\u306a\u3044\u3002\n\n3\uff0e \u306fmini batch sampling\u3059\u308b\u969b\u306bRDD\u306eiterator interface\u3092\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u5168\u306a\u3081\u3059\u308b\u306e\u3067\uff1f\u52b9\u7387\u7684\u3067\u306a\u3044\u304b\u3089\u3082\u3063\u3068\u52b9\u7387\u7684\u306b\u3067\u304d\u306a\u3044\u304b\uff1f\u3068\u3044\u3046\u30c1\u30b1\u30c3\u30c8\n\n2.\u306f\u4eca\u306eSGD\u5b9f\u88c5\u306fBSP\u30d9\u30fc\u30b9\u306a\u306e\u3067SSP\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3082\u3046\u3061\u3087\u3063\u3068\u52b9\u7387\u5316\u3001\u9ad8\u901f\u5316\u3067\u304d\u3093\u3058\u3083\u306a\u3044\u306e\u3068\u3044\u3046\u8a71\uff1f local update\u3068DistBrief\u307f\u305f\u3044\u306b\u3059\u308b\u3068\u3044\u3046\u5b9f\u88c5\u306f\u5177\u4f53\u7684\u306b\u66f8\u3044\u3066\u306a\u3044\u306e\u3067\u8a73\u7d30\u306f\u4e0d\u660e\n\n\n\u52fe\u914d\u6cd5\u306e\u5b9f\u88c5\u3082\u3001BSP\u30d9\u30fc\u30b9\u306e\u5206\u6563\u52fe\u914d\u6cd5\u306e\u5b9f\u88c5\u3082\u7c21\u5358\u306a\u306e\u3067\u3001\u4e0a\u8a18\u306e\u30c1\u30b1\u30c3\u30c8\u3082\u3059\u3093\u306a\u308a\u9032\u3080\u3068\u601d\u3063\u3066\u3044\u305f\u304c\u3001\u3069\u308c\u3082\u6b62\u307e\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\n[Spark mllib \u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u7cbe\u5ea6\u6e2c\u5b9a](http://qiita.com/rikima/items/733294daeb4ae8766bf7)\u306b\u3082\u66f8\u3044\u305f\u304c\u3001\u73fe\u72b6\u306eSGD\u5b9f\u88c5\u3092\u4f7f\u3063\u305fSVM, Logistic Regression\u306e\u7cbe\u5ea6\u306fL-BFGS\u306b\u3088\u308bLogisitic Regression\u306b\u6bd4\u3079\u3066\u8457\u3057\u304f\u60aa\u304b\u3063\u305f\u3002\u3053SGD\u3092\u4f7f\u3063\u305f\u5834\u5408\u306b\u7cbe\u5ea6\u304c\u4e0b\u304c\u308b\u554f\u984c\u306b\u624b\u3092\u3064\u3051\u3089\u308c\u3066\u306a\u3044\u306e\u3067\u3001\u5148\u306b\u9032\u3081\u3066\u3044\u306a\u3044\u306e\u3060\u304c\u3001\u4e0a\u306e\u30c1\u30b1\u30c3\u30c8\u306b\u95a2\u4fc2\u3059\u308bSGD\u306e\u62e1\u5f35\u3001improve\u3059\u308b\u3088\u3046\u306a\u4f55\u304b\u3092\u3057\u305f\u3044\u306a\u3069\u3068\u601d\u3063\u3066\u307e\u3059\u3002\n\n# AdaGrad \u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\n\nAdaGrad\u306fSGD\u306e\u3092weight vector\u306e\u8981\u7d20\u6bce\u306e\u66f4\u65b0\u7387\u3092\u306e\u52fe\u914d\u306e\u5c65\u6b74\u304b\u3089\u8a08\u7b97\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002\n\n```math\n\\begin{equation}\n{\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\alpha \\ {\\bf k}^{(t)} \\circ {\\bf g}^{(t)}\n\\end{equation}\n```\n\n```math\n\\begin{equation}\n({\\bf k}^{(t)})_a = \n\\frac{1}{ \\sqrt {\\epsilon + \\sum_{k=1}^t |g_a^{(k)}|^2 } }\n\\end{equation}\n```\n\u3053\u3053\u3067\u3001\u25cb\u306fvector\u306eelement wise\u306e\u7a4d\u3002\u6dfb\u5b57\u306ea\u306f\u6210\u5206\u3092\u8868\u3059\u3002\n\nSpark mllib\u3067\u306e\u5b9f\u88c5\u3082\u3054\u3061\u3083\u3054\u3061\u3083\u8a00\u308f\u306a\u3051\u308c\u3070\uff08\u7dba\u9e97\u3068\u304b\u3001\u30dd\u30ea\u30b7\u30fc\u3068\u304b\u3001\u3044\u308d\u3044\u308d\uff09\u975e\u5e38\u306b\u7c21\u5358\u306b\u3067\u304d\u308b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n\n\u3067\u5b9f\u88c5\u3057\u3066\u305d\u306e\u52b9\u679c\u3092\u5171\u6709\u3057\u305f\u3044\u3068\u304a\u3082\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u3001\u3001\u3082\u3068\u3082\u3068\u306eSGD\u305d\u306e\u3082\u306e\u306e\u7cbe\u5ea6\u3001\u53ce\u675f\u304c\u3061\u3087\u3063\u3068\u304a\u304b\u3057\u304f\u3066\u3001AdaGrad\u306b\u3088\u308b\u52b9\u679c\u3092\u6e2c\u5b9a\u3001\u8a55\u4fa1\u304c\u3067\u304d\u3066\u3044\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\u3002\u3002\n\n\u4ed6\u306e\u5206\u985e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u30aa\u30ea\u30b8\u30ca\u30eb\u5b9f\u88c5\u3092Spark\u4e0a\u3067\u3082\u5b9f\u88c5\u3057\u306a\u304a\u3057\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u305d\u306e\u6642\u306b\u3001SGD\u5b9f\u88c5\u306e\u59a5\u5f53\u6027\u3068AdaGrad\u7b49\u306e\u5b66\u7fd2\u7387\u306e\u81ea\u52d5\u8abf\u6574\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u52b9\u679c\u3092\u8a55\u4fa1\u3057\u305f\u3044\u3068\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n \n#\u3055\u3044\u3054\u306b\n\nSpark mllib\u306eSGD\u5b9f\u88c5\u306b\u8abf\u3079\u3066\u307f\u307e\u3057\u305f\u3002\u524d\u304b\u3089\u6982\u8981\u306f\u7406\u89e3\u3057\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u8a73\u7d30\u3092\u898b\u308b\u3053\u3068\u3067\n\n- BSP\u3068\u540c\u3058\u3002worker\u3067\u5404\u4e8b\u4f8b\u6bce\u306b\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u3001driver\u3067\u96c6\u8a08(super iteration)weight vector\u3092\u66f4\u65b0\u3059\u308b\u3002\n- \u5b66\u7fd2\u7387\u306e\u66f4\u65b0\u306fUpdator\u3067\u3084\u3063\u3066\u3044\u308b\u3002AdaGrad etc.\u3078\u306e\u5909\u66f4\u306f\u7c21\u6613\u306b\u3067\u304d\u305d\u3046\n- \u52fe\u914d\u306e\u96c6\u8a08\u306fRDD treeAggregation\u3092\u4f7f\u3063\u3066\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3002\u96e3\u3057\u3044\u3053\u3068\u306f\u4f55\u3082\u3057\u3066\u306a\u3044\u3002reduce\u3057\u3066\u3044\u308b\u3060\u3051\n- \u5206\u6563\u3067SGD\u306a\u306e\u3067\u3001\u5404worker\u3067\u52fe\u914d\u3001weight vector\u3092\u66f4\u65b0\u3057\u3066\u3001driver\u3067weight vector\u306e\u5e73\u5747\u3092\u3068\u3063\u3066\u3044\u308b\u3068\u52d8\u9055\u3044\u3057\u3066\u3044\u305f\u304c\u305d\u3046\u3067\u306f\u306a\u3044\u3002\n- SGD\u3067\u304d\u3066\u3044\u308b\u306a\u3089online\u5b66\u7fd2\u3082\u305d\u306e\u307e\u307e\u884c\u3051\u308b\u3068\u601d\u3063\u305f\u304c\u52b9\u7387\u7684\u306a\u5206\u6563\u51e6\u7406\u3060\u3068\u81ea\u660e\u3058\u3083\u306a\u3044\u3063\u307d\u3044\u3002\u305d\u3053\u306f\u3061\u3087\u3063\u3068\u307e\u3058\u3081\u306b\u8003\u3048\u305f\u3044\n - [Asynchronous Complex Analytics in a Distributed\nDataflow Architecture (Extended Abstract)](http://learningsys.org/papers/LearningSys_2015_paper_17.pdf) \u3068\u304b\u3001SSP\u3068\u304b\u3092\u771f\u9762\u76ee\u306b\u8003\u3048\u306a\u3044\u3068\u3044\u3051\u306a\u3044\n\n\u3068\u3044\u3046\u3088\u3046\u306a\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\u3053\u306e\u8fba\u30e1\u30a4\u30f3\u306b\u8003\u3048\u305f\u3044\u306a\u3068\u601d\u3046\u3053\u306e\u3054\u308d\u3067\u3059\u3002\n\n# P.S. \u7684\u306a \n\u66f8\u3044\u3066\u6700\u5f8c\u306b\u3055\u3089\u306b\u5099\u5fd8\u9332\u7684\u306b\u8ffd\u8a18\u3059\u308b\u3068\n\nonline\u5b66\u7fd2\u3068\u304b\u3001SSP\u3068\u304b\u3084\u308d\u3046\u3068\u3059\u308b\u3068Spark\u3001RDD\u3092\u4f7f\u3063\u3066\u3084\u308b\u306e\u3068\n\u7d20\u3067Akka\u3092\u4f7f\u3063\u3066\u81ea\u524d\u5b9f\u88c5\u3059\u308b\u306e\u3068\u3069\u3063\u3061\u304c\u826f\u3044\u304b\u3001\u3069\u3063\u3061\u304c\u7c21\u5358\u3067\u958b\u767a\u3057\u3084\u3059\u3044\u304b\u3068\u601d\u3046\u3002\n\n\u672a\u3060\u306bSpark\u4e0a\u3067\u958b\u767a\u3059\u308b\u306e\u306f\u52b9\u7387\u7684\u306b\u884c\u3048\u3066\u306a\u3044\u306e\u3067\u3002\u3002\u3002\n\nAkka\u3092\u4f7f\u3046\u30b7\u30b9\u30c6\u30e0\u306e\u30b9\u30b1\u30eb\u30c8\u30f3\u3092\u4f5c\u3063\u3066\u3057\u307e\u3063\u3066\u3001\nworker\u4e0a\u306e\u30de\u30b7\u30f3\u304c\u30ed\u30fc\u30ab\u30eb\u306e\u30c7\u30fc\u30bf\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f5c\u3063\u3066\u3057\u307e\u3048\u3070\nAkka\u4e0a\u3067\u5b9f\u88c5\u3059\u308b\u307b\u3046\u304c\u67d4\u8edf\u306a\u6c17\u3082\u3057\u306a\u3044\u3067\u3082\u306a\u3044\u3053\u306e\u3054\u308d\u3002\u3002\u3002\n", "tags": ["machineleargning", "\u6a5f\u68b0\u5b66\u7fd2", "Spark"]}