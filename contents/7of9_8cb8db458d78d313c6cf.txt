{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n\n\nv0.1 http://qiita.com/7of9/items/b364d897b95476a30754\nsine curve\u3092\u5b66\u7fd2\u3059\u308b\u30b3\u30fc\u30c9\u3092placeholder\u4f7f\u7528\u306b\u5909\u66f4\u3057\u3066\u3044\u308b\u3002\n\ninput.csv\u751f\u6210\nhttp://qiita.com/7of9/items/b364d897b95476a30754#\u30c7\u30fc\u30bf\u751f\u6210\u90e8\n\noriginal (placeholder\u4e0d\u4f7f\u7528)\n\nlinreg2.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306e Queue \u3092\u4f5c\u6210\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# CSV \u3092 parse\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(inputs_batch, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n\nloss = tf.contrib.losses.mean_squared_error(prediction, output_batch)\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n      _, t_loss = sess.run([train_op, loss])\n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n\n\n\nv0.6 (placeholder\u4f7f\u7528)\n.eval()\u3092\u4f7f\u308f\u306a\u3044\u65b9\u91dd\u3068\u3057\u305f\u3002\nhttp://qiita.com/7of9/items/40a7cfc741f4b11f50d2\n\nlinreg2_noEval.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306e Queue \u3092\u4f5c\u6210\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# CSV \u3092 parse\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n#if 1 // no eval()\n      inpbt, outbt = sess.run([inputs_batch, output_batch])\n      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inpbt, output_ph: outbt})\n#else\n#      _, t_loss = sess.run([train_op, loss], feed_dict=feed_dict(inputs_batch, output_batch))\n#endif      \n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n\n\n\n\u7d50\u679c\npython linreg2.py > log.learn_original\npython linreg2_noEval.py > log.learn_noEval\nmatplotlib\u30b3\u30fc\u30c9 on Jupyter\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('log.learn_original', delimiter=',')\ndata2 = np.loadtxt('log.learn_noEval', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\ninput2 = data2[:,0]\noutput2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\n#ax.plot(input1, output1, color='black', linestyle='dotted', label='rate=0.001')\nax1.plot(input1, output1, color='black', linestyle='solid', label='original')\nax2.plot(input2, output2, color='red', linestyle='solid', label='placeholder')\n#ax.scatter(input1, output1)\n\nax1.set_title('loss')\nax1.set_xlabel('step')\nax1.set_ylabel('loss')\nax1.grid(True)\nax1.legend()\n\n#ax2.set_title('loss')\nax2.set_xlabel('step')\nax2.set_ylabel('loss')\nax2.grid(True)\nax2.legend()\n\nfig.show()\n\n\n\u540c\u3058\u3088\u3046\u306a\u7d50\u679c\u304c\u3088\u3046\u3084\u304f\u5f97\u3089\u308c\u305f\u3002\n11/14\u304b\u3089\u4eca\u65e5(11/20)\u307e\u3067\u306f\u307e\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u7d50\u69cb\u82e6\u52b4\u3057\u305f\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n```\n\nv0.1 http://qiita.com/7of9/items/b364d897b95476a30754\n\nsine curve\u3092\u5b66\u7fd2\u3059\u308b\u30b3\u30fc\u30c9\u3092placeholder\u4f7f\u7528\u306b\u5909\u66f4\u3057\u3066\u3044\u308b\u3002\n\n### input.csv\u751f\u6210\n\nhttp://qiita.com/7of9/items/b364d897b95476a30754#\u30c7\u30fc\u30bf\u751f\u6210\u90e8\n\n### original (placeholder\u4e0d\u4f7f\u7528)\n\n```linreg2.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306e Queue \u3092\u4f5c\u6210\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# CSV \u3092 parse\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(inputs_batch, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\n\nloss = tf.contrib.losses.mean_squared_error(prediction, output_batch)\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n      _, t_loss = sess.run([train_op, loss])\n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n```\n\n\n### v0.6 (placeholder\u4f7f\u7528)\n\n.eval()\u3092\u4f7f\u308f\u306a\u3044\u65b9\u91dd\u3068\u3057\u305f\u3002\nhttp://qiita.com/7of9/items/40a7cfc741f4b11f50d2\n\n\n```linreg2_noEval.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306e Queue \u3092\u4f5c\u6210\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# CSV \u3092 parse\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n#if 1 // no eval()\n      inpbt, outbt = sess.run([inputs_batch, output_batch])\n      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inpbt, output_ph: outbt})\n#else\n#      _, t_loss = sess.run([train_op, loss], feed_dict=feed_dict(inputs_batch, output_batch))\n#endif      \n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n```\n\n## \u7d50\u679c\n\npython linreg2.py > log.learn_original\npython linreg2_noEval.py > log.learn_noEval\n\nmatplotlib\u30b3\u30fc\u30c9 on Jupyter\n\n```py\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('log.learn_original', delimiter=',')\ndata2 = np.loadtxt('log.learn_noEval', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\ninput2 = data2[:,0]\noutput2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\n#ax.plot(input1, output1, color='black', linestyle='dotted', label='rate=0.001')\nax1.plot(input1, output1, color='black', linestyle='solid', label='original')\nax2.plot(input2, output2, color='red', linestyle='solid', label='placeholder')\n#ax.scatter(input1, output1)\n\nax1.set_title('loss')\nax1.set_xlabel('step')\nax1.set_ylabel('loss')\nax1.grid(True)\nax1.legend()\n\n#ax2.set_title('loss')\nax2.set_xlabel('step')\nax2.set_ylabel('loss')\nax2.grid(True)\nax2.legend()\n\nfig.show()\n```\n\n![qiita.png](https://qiita-image-store.s3.amazonaws.com/0/32870/059d5407-2594-c40e-a5a6-21423bc06532.png)\n\n\n\u540c\u3058\u3088\u3046\u306a\u7d50\u679c\u304c\u3088\u3046\u3084\u304f\u5f97\u3089\u308c\u305f\u3002\n\n11/14\u304b\u3089\u4eca\u65e5(11/20)\u307e\u3067\u306f\u307e\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u7d50\u69cb\u82e6\u52b4\u3057\u305f\u3002\n\n\n\n", "tags": ["TensorFlow", "borgWarp"]}