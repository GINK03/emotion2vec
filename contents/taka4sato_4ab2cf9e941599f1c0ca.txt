{"context": "\n\n\u306f\u3058\u3081\u306b\uff1aSpark Dataframe\u3068\u306f\nSpark Ver 1.3\u304b\u3089Spark Dataframe\u3068\u3044\u3046\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f\u3002\u7279\u5fb4\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u69d8\u306a\u7269\u304c\u3042\u308a\u307e\u3059\u3002\n\nSpark RDD\u306bSchema\u8a2d\u5b9a\u3092\u52a0\u3048\u308b\u3068\u3001Spark Dataframe\u306eObject\u3092\u4f5c\u6210\u3067\u304d\u308b\nDataframe\u306e\u5229\u70b9\u306f\u3001\n\n\nSQL\u98a8\u306e\u6587\u6cd5\u3067\u3001\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u3092\u62bd\u51fa\u3057\u305f\u308a\u3001Dataframe\u540c\u58eb\u306eJoin\u304c\u3067\u304d\u308b\n\nfilter, select\u3068\u3044\u3046method\u3067\u3001\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u3001\u5217\u3092\u62bd\u51fa\u3067\u304d\u308b\n\ngroupBy \u2192 agg\u3068\u3044\u3046method\u3067\u3001Log\u306e\u69d8\u3005\u306a\u96c6\u8a08\u304c\u3067\u304d\u308b\nUDF(User Defined Function)\u3067\u72ec\u81ea\u95a2\u6570\u3067\u5217\u306b\u51e6\u7406\u304c\u3067\u304d\u308b\nSQL\u3067\u8a00\u3046Pivot\u3082\u30b5\u30dd\u30fc\u30c8 (Spark v1.6\u304b\u3089\u306e\u6a5f\u80fd)\n\n\n\n\u3064\u307e\u308a\u3001RDD\u306emap\u3084filter\u3067\u30b7\u30b3\u30b7\u30b3\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082Simple Code\u3067\u3001\u4e14\u3064\u9ad8\u901f\u306b\u51e6\u7406\u304c\u884c\u3048\u308b\u306e\u304c\u30a6\u30ea\u3067\u3059\u3002Data\u306e\u524d\u51e6\u7406\u306fRDD\u3067\u3084\u308b\u3068\u3057\u3066\u3001\u3055\u3063\u3055\u3068Dataframe\u306b\u8aad\u307f\u8fbc\u3093\u3060\u65b9\u304cmaji\u3067\u6357\u308a\u307e\u3059\u3002Dataframe\u306e\u30e1\u30e2\u304c\u6563\u5728\u3057\u305f\u306e\u3067\u3001\u5099\u5fd8\u9332\u304c\u3066\u3089Sample code\u3092\u30c1\u30e9\u88cf\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u306a\u304a\u3001\n\n\u3053\u306e\u8a18\u4e8b\u306b\u3042\u308bAPI\u4ee5\u5916\u306b\u3082\u3001\u6ca2\u5c71API\u304c\u3042\u308a\u307e\u3059\n\n\u3053\u306e\u8a18\u4e8b\u306eIPython Notebook\u3092\u3053\u3053\u306b\u7f6e\u3044\u3066\u304a\u304f\u306e\u3067\u3001\u968f\u6642\u5b9f\u884c\u3057\u3066\u307f\u3066\u4e0b\u3055\u3044\nSpark Dataframe\u306e\u7e2e\u7d04/\u96c6\u8a08\u306eSample Code\n\n\n\nSample Log\u306e\u8aad\u307f\u8fbc\u307f\nAccess Log\u3092\u984c\u6750\u3068\u3057\u3066\u4f7f\u3044\u307e\u3059\u3002\u6280\u8853\u8a55\u8ad6\u793e\u3055\u3093\u306e\u672c\u3067\u4f7f\u308f\u308c\u3066\u3044\u305fAccess Log(csv)\u3067\u3001csv file\u3078\u306e\u76f4\u30ea\u30f3\u306f\u3053\u3061\u3089\u3067\u3059\u3002csv\u306e\u4e2d\u8eab\u306f\u3001\u65e5\u4ed8\u3001User_ID, Campaign_ID\u306e3\u3064\u306e\u60c5\u5831\u3092\u6301\u3064\u4ee5\u4e0b\u306e\u69d8\u306aLog\u3067\u3059\nclick.at    user.id campaign.id\n2015/4/27 20:40 144012  Campaign077\n2015/4/27 0:27  24485   Campaign063\n2015/4/27 0:28  24485   Campaign063\n2015/4/27 0:33  24485   Campaign038\n\ncsv\u3092\u8aad\u307f\u8fbc\u3093\u3067RDD\u306b\u3057\u307e\u3059\u30021\u884c\u76ee\u306eheader\u306e\u524a\u9664\u3068\u30011\u5217\u76ee\u3092datetime Object\u3068\u3057\u3066\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\nimport json, os, datetime, collections, commands\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.types import *\n\nif not os.path.exists(\"./click_data_sample.csv\"):\n    print \"csv file not found at master node, will download and copy to HDFS\"\n    commands.getoutput(\"wget -q http://image.gihyo.co.jp/assets/files/book/2015/978-4-7741-7631-4/download/click_data_sample.csv\")\n    commands.getoutput(\"hadoop fs -copyFromLocal -f ./click_data_sample.csv /user/hadoop/\")\n\nwhole_raw_log = sc.textFile(\"/user/hadoop/click_data_sample.csv\")\nheader = whole_raw_log.first()\nwhole_log = whole_raw_log.filter(lambda x:x !=header).map(lambda line: line.split(\",\"))\\\n            .map(lambda line: [datetime.datetime.strptime(line[0].replace('\"', ''), '%Y-%m-%d %H:%M:%S'), int(line[1]), line[2].replace('\"', '')])\n\nwhole_log.take(3)\n#[[datetime.datetime(2015, 4, 27, 20, 40, 40), 144012, u'Campaign077'],\n# [datetime.datetime(2015, 4, 27, 0, 27, 55), 24485, u'Campaign063'],\n# [datetime.datetime(2015, 4, 27, 0, 28, 13), 24485, u'Campaign063']]\n\n\nDataframe\u306e\u4f5c\u6210\u65b9\u6cd5\n\nRDD\u304b\u3089\u4f5c\u6210\n\nDataframe\u306f\u3001\u5143\u3068\u306a\u308bRDD\u304c\u3042\u308c\u3070\u3001Column\u306e\u540d\u524d\u3068\u305d\u308c\u305e\u308c\u306eType(TimestampType, IntegerType, StringType\u306a\u3069)\u3092\u6307\u5b9a\u3057\u3066\u3001sqlContext.createDataFrame(my_rdd, my_schema)\u3067\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002Schema\u306e\u5b9a\u7fa9\u306f\u3053\u3053\u3092\u53c2\u7167\u3002\nprintSchema(), dtypes\u3067Schema\u60c5\u5831\u3001count()\u3067\u884c\u6570\u3001show(n)\u3067\u6700\u521d\u306en\u4ef6\u306erecord\u306e\u8868\u793a\u3067\u3059\u3002\nfields = [StructField(\"access_time\", TimestampType(), True), StructField(\"userID\", IntegerType(), True), StructField(\"campaignID\", StringType(), True)]\nschema = StructType(fields)\n\nwhole_log_df = sqlContext.createDataFrame(whole_log, schema)\nprint whole_log_df.count()\nprint whole_log_df.printSchema()\nprint whole_log_df.dtypes\nprint whole_log_df.show(5)\n\n#327430\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- userID: integer (nullable = true)\n# |-- campaignID: string (nullable = true)\n#\n#[('access_time', 'timestamp'), ('userID', 'int'), ('campaignID', 'string')]\n#\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-04-27 20:40:...|144012|Campaign077|\n#|2015-04-27 00:27:...| 24485|Campaign063|\n#|2015-04-27 00:28:...| 24485|Campaign063|\n#|2015-04-27 00:33:...| 24485|Campaign038|\n#|2015-04-27 01:00:...| 24485|Campaign063|\n\n\ncsv file\u304b\u3089\u76f4\u63a5\u4f5c\u6210\n\ncsv\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306f\u3001Spark Package\u306e1\u3064\u3067\u3042\u308bspark-csv\u3092\u4f7f\u3046\u3068\u697d\u3067\u3059\u3002\u7279\u306b\u6307\u5b9a\u3057\u306a\u3044\u3068\u5168\u3066string\u3068\u3057\u3066\u8aad\u307f\u8fbc\u307f\u307e\u3059\u304c\u3001inferSchema\u3092\u6307\u5b9a\u3057\u3066\u3042\u3052\u308b\u3068\u826f\u3044\u611f\u3058\u306b\u985e\u63a8\u3057\u3066\u304f\u308c\u307e\u3059\u3002\nwhole_log_df_2 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"/user/hadoop/click_data_sample.csv\")\nprint whole_log_df_2.printSchema()\nprint whole_log_df_2.show(5)\n\n#root\n# |-- click.at: string (nullable = true)\n# |-- user.id: string (nullable = true)\n# |-- campaign.id: string (nullable = true)\n#\n#+-------------------+-------+-----------+\n#|           click.at|user.id|campaign.id|\n#+-------------------+-------+-----------+\n#|2015-04-27 20:40:40| 144012|Campaign077|\n#|2015-04-27 00:27:55|  24485|Campaign063|\n#|2015-04-27 00:28:13|  24485|Campaign063|\n#|2015-04-27 00:33:42|  24485|Campaign038|\n#|2015-04-27 01:00:04|  24485|Campaign063|\n\nwhole_log_df_3 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/user/hadoop/click_data_sample.csv\")\nprint whole_log_df_3.printSchema()\n\n#root\n# |-- click.at: timestamp (nullable = true)\n# |-- user.id: integer (nullable = true)\n# |-- campaign.id: string (nullable = true)\n\n\u3061\u306a\u307f\u306b\u3001column\u540d\u306b.\u304c\u5165\u3063\u3066\u5c45\u308b\u3068\u8272\u3005\u9762\u5012\u306a\u306e\u3067\u3001withColumnRenamed\u3067rename\u53ef\u80fd\u3067\u3059(rename\u3057\u305f\u5225\u306eDataframe\u3092\u4f5c\u6210\u53ef\u80fd\u3067\u3059)\u3002\nwhole_log_df_4 = whole_log_df_3.withColumnRenamed(\"click.at\", \"access_time\")\\\n                 .withColumnRenamed(\"user.id\", \"userID\")\\\n                 .withColumnRenamed(\"campaign.id\", \"campaignID\")\nprint whole_log_df_4.printSchema()\n\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- userID: integer (nullable = true)\n# |-- campaignID: string (nullable = true)\n\n\njson\u304b\u3089\u76f4\u63a5\u4f5c\u6210\n\njson file\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306fsqlContext.read.json\u3092\u4f7f\u3044\u307e\u3059\u3002file\u306e\u5404\u884c\u30921 json object\u3068\u3057\u3066\u6271\u3044\u307e\u3059\u3001\u5b58\u5728\u3057\u306a\u3044Key\u304c\u3042\u308b\u5834\u5408\u306b\u306f\u3001null\u304c\u5165\u308a\u307e\u3059\u3002\n# test_json.json contains following 3 lines, last line doesn't have \"campaignID\" key\n#\n#{\"access_time\": \"2015-04-27 20:40:40\", \"userID\": \"24485\", \"campaignID\": \"Campaign063\"}\n#{\"access_time\": \"2015-04-27 00:27:55\", \"userID\": \"24485\", \"campaignID\": \"Campaign038\"}\n#{\"access_time\": \"2015-04-27 00:27:55\", \"userID\": \"24485\"}\n\ndf_json = sqlContext.read.json(\"/user/hadoop/test_json.json\")\ndf_json.printSchema()\ndf_json.show(5)\n\n#root\n# |-- access_time: string (nullable = true)\n# |-- campaignID: string (nullable = true)\n# |-- userID: string (nullable = true)\n#\n#+-------------------+-----------+------+\n#|        access_time| campaignID|userID|\n#+-------------------+-----------+------+\n#|2015-04-27 20:40:40|Campaign063| 24485|\n#|2015-04-27 00:27:55|Campaign038| 24485|\n#|2015-04-27 00:27:55|       null| 24485|\n#+-------------------+-----------+------+\n\n\nparquet\u304b\u3089\u76f4\u63a5\u4f5c\u6210\n\nparquet file\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306fsqlContext.read.parquet\u3092\u4f7f\u3044\u307e\u3059\u3002parquet file\u304c\u7f6e\u3044\u3066\u3042\u308bFolder\u3092\u6307\u5b9a\u3059\u308b\u3068\u3001\u305d\u306eFolder\u4ee5\u4e0b\u306eparquet file\u3092\u4e00\u62ec\u3067\u8aad\u307f\u8fbc\u3093\u3067\u304f\u308c\u307e\u3059\u3002\nsqlContext.read.parquet(\"/user/hadoop/parquet_folder/\")\n\n\nSQL\u6587\u3067Query\nDataframe\u306b\u5bfe\u3057\u3066\u3001SQL\u306e\u6587\u3067Query\u3092\u639b\u3051\u308bSample\u3067\u3059\u3002registerTempTable\u3067Dataframe\u306bSQL Table name\u3092\u4ed8\u4e0e\u3059\u308b\u3068\u3001SQL\u306eTable\u540d\u3068\u3057\u3066\u53c2\u7167\u3067\u304d\u307e\u3059\u3002sqlContext.sql(SQL\u6587)\u306e\u623b\u308a\u5024\u3082Dataframe\u3067\u3059\u3002\n\u306a\u304a\u3001Sub Query\u3092\u8a18\u8f09\u3059\u308b\u4e8b\u3082\u53ef\u80fd\u306a\u306e\u3067\u3059\u304c\u3001Sub Query\u5074\u306bAlias\u3092\u4ed8\u4e0e\u3057\u306a\u3044\u3068\u3001\u4f55\u6545\u304bSyntax error\u304c\u8d77\u304d\u308b\u306e\u3067\u6ce8\u610f\u3067\u3059\u3002\n#\u5358\u7d14\u306aSQL query\n\nwhole_log_df.registerTempTable(\"whole_log_table\")\n\nprint sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").count()\n#18081\nprint sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").show(5)\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:27:...| 14151|Campaign047|\n#|2015-04-27 05:28:...| 14151|Campaign047|\n#+--------------------+------+-----------+\n\n\n#SQL\u6587\u306e\u4e2d\u306b\u5909\u6570\u3092\u5165\u308c\u308b\u5834\u5408\nfor count in range(1, 3):\n    print \"Campaign00\" + str(count)\n    print sqlContext.sql(\"SELECT count(*) as access_num FROM whole_log_table where campaignID == 'Campaign00\" + str(count) + \"'\").show()\n\n#Campaign001\n#+----------+\n#|access_num|\n#+----------+\n#|      2407|\n#+----------+\n#\n#Campaign002\n#+----------+\n#|access_num|\n#+----------+\n#|      1674|\n#+----------+\n\n#Sub Query\u306e\u5834\u5408\uff1a\nprint sqlContext.sql(\"SELECT count(*) as first_count FROM (SELECT userID, min(access_time) as first_access_date FROM whole_log_table GROUP BY userID) subquery_alias WHERE first_access_date < '2015-04-28'\").show(5)\n#+------------+\n#|first_count |\n#+------------+\n#|       20480|\n#+------------+\n\n\nfilter, select\u3067\u6761\u4ef6\u4ed8\u304d\u691c\u7d22\nDataframe\u306b\u5bfe\u3057\u3066\u306e\u7c21\u6613\u7684\u306a\u691c\u7d22\u6a5f\u80fd\u3067\u3059\u3002\u4e0a\u8a18\u306b\u3042\u308bSQL\u6587\u3067Query\u3068\u6a5f\u80fd\u306f\u4f3c\u3066\u3044\u307e\u3059\u304c\u3001filter, select\u306f\u7c21\u6613\u7684\u306a\u691c\u7d22\u6a5f\u80fd\u3068\u3044\u3046\u4f4d\u7f6e\u3065\u3051\u3067\u3059\u3002filter\u306f\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u306e\u62bd\u51fa\u3001select\u306f\u5217\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002RDD\u306efilter\u3068\u3061\u3087\u3063\u3068\u6587\u6cd5\u304c\u9055\u3046\u306e\u306b\u6ce8\u610f\u3067\u3059\u3002\n#Sample for filter\nprint whole_log_df.filter(whole_log_df[\"access_time\"] < \"2015-04-28\").count()\n#41434\nprint whole_log_df.filter(whole_log_df[\"access_time\"] > \"2015-05-01\").show(3)\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-05-01 22:11:...|114157|Campaign002|\n#|2015-05-01 23:36:...| 93708|Campaign055|\n#|2015-05-01 22:51:...| 57798|Campaign046|\n#+--------------------+------+-----------+\n\n#Sample for select\nprint whole_log_df.select(\"access_time\", \"userID\").show(3)\n#+--------------------+------+\n#|         access_time|userID|\n#+--------------------+------+\n#|2015-04-27 20:40:...|144012|\n#|2015-04-27 00:27:...| 24485|\n#|2015-04-27 00:28:...| 24485|\n#+--------------------+------+\n\n\ngroupBy\u3067\u96c6\u8a08\ngroupBy\u306f\u3001RDD\u306ereduceByKey\u306b\u4f3c\u305f\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u304c\u3001groupBy\u306f\u3053\u3053\u306b\u3042\u308bmethod\u3092\u305d\u306e\u5f8c\u308d\u3067Call\u3059\u308b\u4e8b\u3067\u3001\u69d8\u3005\u306a\u96c6\u8a08\u6a5f\u80fd\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002\u4ee3\u8868\u7684\u306a\u306e\u306fagg\u3068count\u3067\u3059\u3002\n\ngroupBy\u2192count\u3067\u96c6\u8a08\ncampaignID\u3092Key\u306b\u3057\u3066groupBy\u3092\u5b9f\u884c\u3057\u3001\u305d\u306eRecord\u6570\u3092count()\u3067\u96c6\u8a08\u3057\u3066\u304f\u308c\u307e\u3059\u3002groupBy\u306b\u8907\u6570\u306eKey\u3092\u5217\u6319\u3059\u308c\u3070\u3001\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092key\u3068\u3057\u3066groupBy\u3057\u3066\u304f\u308c\u307e\u3059\u3002\nprint whole_log_df.groupBy(\"campaignID\").count().sort(\"count\", ascending=False).show(5)\n#+-----------+-----+\n#| campaignID|count|\n#+-----------+-----+\n#|Campaign116|22193|\n#|Campaign027|19206|\n#|Campaign047|18081|\n#|Campaign107|13295|\n#|Campaign131| 9068|\n#+-----------+-----+\n\nprint whole_log_df.groupBy(\"campaignID\", \"userID\").count().sort(\"count\", ascending=False).show(5)\n#+-----------+------+-----+\n#| campaignID|userID|count|\n#+-----------+------+-----+\n#|Campaign047| 30292|  633|\n#|Campaign086|107624|  623|\n#|Campaign047|121150|  517|\n#|Campaign086| 22975|  491|\n#|Campaign122| 90714|  431|\n#+-----------+------+-----+\n\n\ngroupBy\u2192agg\u3067\u96c6\u8a08\nuserID\u3092Key\u306b\u3057\u3066GroupBy\u3092\u5b9f\u884c\u3057\u3001\u305d\u306e\u96c6\u8a08\u7d50\u679c\u306e\u5e73\u5747\u3084\u6700\u5927/\u6700\u5c0f\u3092\u8a08\u7b97\u304c\u53ef\u80fd\u3067\u3059\u3002agg({key:value})\u3067\u3001key\u306e\u5217\u306b\u5bfe\u3057\u3066value\u306e\u95a2\u6570(min,sum, ave etc)\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u8fd4\u3057\u307e\u3059\u3002\u623b\u308a\u5024\u306fDataframe\u306a\u306e\u3067\u3001.filter()\u3067\u66f4\u306b\u884c\u3092\u7d5e\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3059\u3002\nprint whole_log_df.groupBy(\"userID\").agg({\"access_time\": \"min\"}).show(3)\n#+------+--------------------+\n#|userID|    min(access_time)|\n#+------+--------------------+\n#|  4831|2015-04-27 22:49:...|\n#| 48631|2015-04-27 22:15:...|\n#|143031|2015-04-27 21:52:...|\n#+------+--------------------+\n\nprint whole_log_df.groupBy(\"userID\").agg({\"access_time\": \"min\"}).filter(\"min(access_time) < '2015-04-28'\").count()\n#20480\n\n\ngroupBy\u2192pivot\u3067\u7e26\u6a2a\u5909\u63db\nPivot\u306fSpark v1.6\u304b\u3089\u306e\u65b0\u6a5f\u80fd\u3067SQL\u306ePivot\u3068\u4f3c\u305f\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002Sample code\u306ePivot\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u69d8\u306b\u7e26\u6a2a\u304c\u5909\u5316\u3057\u307e\u3059\u3002\n\npivot\u524d(agged_df)\n\n\n\u884c\u6570\u304c(UserID\u6570 (=75,545) x campainID\u6570 (=133) )\n\u5217\u304c3\u5217\n\n\npivot\u5f8c(pivot_df)\n\n\n\u884c\u6570\u304cUserID\u6570 (=75,545)\n\u5217\u304c UserID + CampainID\u6570 = 1 + 133 = 134\n\n\n\n\u5fc5\u305a\u3001groupBy(\"\u7e26\u306e\u307e\u307e\u306e\u5217\").pivot(\"\u7e26\u304b\u3089\u6a2a\u3078\u5909\u63db\u3057\u305f\u3044\u5217\").sum(\"\u96c6\u8a08\u5024\u306e\u5217\")\u30683\u3064\u306emethod\u3092chain\u3067\u547c\u3076\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\nagged_df = whole_log_df.groupBy(\"userID\", \"campaignID\").count()\nprint agged_df.show(3)\n\n#+------+-----------+-----+\n#|userID| campaignID|count|\n#+------+-----------+-----+\n#|155812|Campaign107|    4|\n#|103339|Campaign027|    1|\n#|169114|Campaign112|    1|\n#+------+-----------+-----+\n\n#\u5024\u304c\u7121\u3044Cell\u306f\u3001null\u304c\u5165\u308b\npivot_df = agged_df.groupBy(\"userID\").pivot(\"campaignID\").sum(\"count\")\nprint pivot_df.printSchema()\n\n#root\n# |-- userID: integer (nullable = true)\n# |-- Campaign001: long (nullable = true)\n# |-- Campaign002: long (nullable = true)\n# ..\n# |-- Campaign133: long (nullable = true)\n\n#\u5024\u304c\u7121\u3044Cell\u30920\u3067\u57cb\u3081\u305f\u3044\u5834\u5408\npivot_df2 = agged_df.groupBy(\"userID\").pivot(\"campaignID\").sum(\"count\").fillna(0)\n\n\nUDF\u3067\u5217\u306e\u8ffd\u52a0\nSpark Dataframe\u3067\u306fUDF\u304c\u4f7f\u3048\u307e\u3059\u3001\u4e3b\u306a\u7528\u9014\u306f\u3001\u5217\u306e\u8ffd\u52a0\u306b\u306a\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002Dataframe\u306f\u57fa\u672cImmutable(\u4e0d\u5909)\u306a\u306e\u3067\u3001\u5217\u306e\u4e2d\u8eab\u306e\u5909\u66f4\u306f\u3067\u304d\u305a\u3001\u5217\u3092\u8ffd\u52a0\u3057\u305f\u5225\u306eDataframe\u3092\u4f5c\u6210\u3059\u308b\u4e8b\u306b\u306a\u308a\u307e\u3059\u3002\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import DoubleType\n\ndef add_day_column(access_time):\n    return int(access_time.strftime(\"%Y%m%d\"))\n\nmy_udf = UserDefinedFunction(add_day_column, IntegerType())\nprint whole_log_df.withColumn(\"access_day\", my_udf(\"access_time\")).show(5)\n\n#+--------------------+------+-----------+----------+\n#|         access_time|userID| campaignID|access_day|\n#+--------------------+------+-----------+----------+\n#|2015-04-27 20:40:...|144012|Campaign077|  20150427|\n#|2015-04-27 00:27:...| 24485|Campaign063|  20150427|\n#|2015-04-27 00:28:...| 24485|Campaign063|  20150427|\n#|2015-04-27 00:33:...| 24485|Campaign038|  20150427|\n#|2015-04-27 01:00:...| 24485|Campaign063|  20150427|\n#+--------------------+------+-----------+----------+\n\nUDF\u306e\u8868\u8a18\u306f\u3001lambda\u95a2\u6570\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\nmy_udf2 = UserDefinedFunction(lambda x: x + 5, IntegerType())\nprint whole_log_df.withColumn(\"userID_2\", my_udf2(\"userID\")).show(5)\n\n#+--------------------+------+-----------+--------+\n#|         access_time|userID| campaignID|userID_2|\n#+--------------------+------+-----------+--------+\n#|2015-04-27 20:40:...|144012|Campaign077|  144017|\n#|2015-04-27 00:27:...| 24485|Campaign063|   24490|\n#|2015-04-27 00:28:...| 24485|Campaign063|   24490|\n#|2015-04-27 00:33:...| 24485|Campaign038|   24490|\n#|2015-04-27 01:00:...| 24485|Campaign063|   24490|\n#+--------------------+------+-----------+--------+\n\n\u9006\u306b\u3001\u7279\u5b9a\u306e\u5217\u3092\u524a\u9664\u3057\u305f\u3044Dataframe\u3092\u4f5c\u308b\u306b\u306fdf.drop()\u3092\u4f7f\u3044\u307e\u3059\u3002\nprint whole_log_df.drop(\"userID\").show(3)\n\n#+--------------------+-----------+\n#|         access_time| campaignID|\n#+--------------------+-----------+\n#|2015-04-27 20:40:...|Campaign077|\n#|2015-04-27 00:27:...|Campaign063|\n#|2015-04-27 00:28:...|Campaign063|\n#+--------------------+-----------+\n\n\nJoin\u30672\u3064\u306eDataframe\u3092\u7d50\u5408\u3055\u305b\u308b\n2\u3064\u306eDataframe\u3092Join\u3055\u305b\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001Heavy User(Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser)\u306eLog\u306e\u307f\u3092\u5168\u4f53\u306eLog\u304b\u3089\u62bd\u51fa\u3059\u308b\u30b1\u30fc\u30b9\u3092\u8003\u3048\u3066\u307f\u307e\u3059\u3002\n\u307e\u305a\u3001Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser\u306eUser ID\u3068\u305d\u306eAccess\u6570\u3092\u3001.groupBy(\"userID\").count()\u3067\u96c6\u8a08\u3057\u3001filter\u3067100\u56de\u4ee5\u4e0a\u306b\u7d5e\u308a\u8fbc\u307f\u307e\u3059\u3002\nheavy_user_df1 = whole_log_df.groupBy(\"userID\").count()\nheavy_user_df2 = heavy_user_df1.filter(heavy_user_df1 [\"count\"] >= 100)\n\nprint heavy_user_df2 .printSchema()\nprint heavy_user_df2 .show(3)\nprint heavy_user_df2 .count()\n\n#root\n# |-- userID: integer (nullable = true)\n# |-- count: long (nullable = false)\n#\n#+------+-----+\n#|userID|count|\n#+------+-----+\n#| 84231|  134|\n#| 13431|  128|\n#|144432|  113|\n#+------+-----+\n#\n#177\n\n\u5143\u306eDataframe(\u3053\u3061\u3089\u304cLeft\u306b\u306a\u308b)\u3067join method\u3092\u547c\u3073\u3001join\u306e\u76f8\u624b(Right\u306b\u306a\u308b)\u3068join\u306e\u6761\u4ef6\u3092\u66f8\u304f\u3068\u3001SQL\u306ejoin\u306e\u69d8\u306bDataframe\u306e\u7d50\u5408\u304c\u53ef\u80fd\u3067\u3059\u3002\njoin\u306e\u5f62\u5f0f\u306f\u3001inner, outer, left_outer, rignt_outer\u306a\u3069\u304c\u9078\u3079\u308b\u306f\u305a\u306a\u306e\u3067\u3059\u304c\u3001inner\u4ee5\u5916\u306f\u610f\u56f3\u3057\u305f\u6319\u52d5\u3067\u52d5\u3044\u3066\u304f\u308c\u306a\u3044\u70ba(\u3057\u304b\u3082outer\u3068\u3057\u3066\u51e6\u7406\u3055\u308c\u308b)\u3001\u3068\u308a\u3042\u3048\u305ainner\u3067\u5916\u90e8\u7d50\u5408\u3057\u3066\u5f8c\u306bdrop\u3067\u8981\u3089\u306a\u3044Column\u3092\u524a\u9664\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u8a73\u7d30Option\u7b49\u306f\u516c\u5f0fPage\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002\n\u4ee5\u4e0b\u306ejoin\u51e6\u7406\u306b\u3088\u308a\u3001Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser(177\u540d)\u306b\u8a72\u5f53\u3059\u308b38,729\u884c\u306eLog\u3092\u53d6\u308a\u51fa\u3059\u4e8b\u304c\u3067\u304d\u307e\u3057\u305f(\u5168\u4f53\u306eLog\u306f\u7d0432\u4e07\u884c)\u3002\njoinded_df = whole_log_df.join(heavy_user_df2, whole_log_df[\"userID\"] == heavy_user_df2[\"userID\"], \"inner\").drop(heavy_user_df2[\"userID\"]).drop(\"count\")\nprint joinded_df.printSchema()\nprint joinded_df.show(3)\nprint joinded_df.count()\n\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- campaignID: string (nullable = true)\n# |-- userID: integer (nullable = true)\n\n#None\n#+--------------------+-----------+------+\n#|         access_time| campaignID|userID|\n#+--------------------+-----------+------+\n#|2015-04-27 02:07:...|Campaign086| 13431|\n#|2015-04-28 00:07:...|Campaign086| 13431|\n#|2015-04-29 06:01:...|Campaign047| 13431|\n#+--------------------+-----------+------+\n#\n#38729\n\n\nDataframe\u304b\u3089\u5217\u3092\u53d6\u308a\u51fa\u3059\n\n\u5217\u306eLabel\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3001df.columns\u3067\u5217\u306eLabel\u306eList(not Dataframe)\u304c\u53d6\u308a\u51fa\u305b\u308b\n\u7279\u5b9a\u306e\u5217\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3001df.select(\"userID\").map(lambda x: x[0]).collect()\u3067\u3001\"userID\"\u5217\u306eList(not RDD/Dataframe)\u304c\u53d6\u308a\u51fa\u305b\u308b\n\u7279\u5b9a\u306e\u5217\u3067\u91cd\u8907\u304c\u7121\u3044List\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f.distinct()\u3092Dataframe\u306e\u6700\u5f8c\u306b\u8ffd\u52a0\u3059\u308c\u3070OK\n\nprint whole_log_df.columns\n#['access_time', 'userID', 'campaignID']\n\nprint whole_log_df.select(\"userID\").map(lambda x: x[0]).collect()[:5]\n#[144012, 24485, 24485, 24485, 24485]\n\nprint whole_log_df.select(\"userID\").distinct().map(lambda x:x[0]).collect()[:5]\n#[4831, 48631, 143031, 39631, 80831]\n\n\nDataframe\u304b\u3089RDD/List\u306b\u623b\u3059\nDataframe\u306f\u3092RDD\u306b\u623b\u3059\u306b\u306f\u3001\u5927\u304d\u304f2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\n\n\n.map\u3092\u547c\u3076\n\n\nDataframe\u306eSchema\u60c5\u5831\u306f\u7834\u68c4\u3055\u308c\u3001Dataframe\u306e\u5404\u884c\u304c\u305d\u308c\u305e\u308clist\u306b\u306a\u3063\u305fRDD\u306b\u5909\u63db\u3055\u308c\u307e\u3059\n\n\n\n.rdd\u3092\u547c\u3076\n\n\nDataframe\u306e\u5404\u884c\u304c\u305d\u308c\u305e\u308cRow Ojbect\u306aRDD\u306b\u5909\u63db\u3055\u308c\u307e\u3059\u3002Row Object\u306fSpark SQL\u3067\u4e00\u884c\u5206\u306e\u30c7\u30fc\u30bf\u3092\u4fdd\u6301\u3059\u308b\u70ba\u306eObject\u3067\u3059\n\nmy_rdd.rdd.map(lambda x:x.asDict())\u3068\u3001Row object\u306b\u5bfe\u3057\u3066.asDict()\u3092\u547c\u3093\u3067\u3042\u3052\u308b\u3068\u3001Key-Value\u306aRDD\u306b\u5909\u63db\u53ef\u80fd\u3067\u3059\n\n\n\n#convert to rdd by \".map\"\nprint whole_log_df.groupBy(\"campaignID\").count().map(lambda x: [x[0], x[1]]).take(5)\n#[[u'Campaign033', 786], [u'Campaign034', 3867], [u'Campaign035', 963], [u'Campaign036', 1267], [u'Campaign037', 1010]]\n\n# rdd -> normal list can be done with \"collect\".\nprint whole_log_df.groupBy(\"campaignID\").count().map(lambda x: [x[0], x[1]]).collect()[:5]\n#[[u'Campaign033', 786], [u'Campaign034', 3867], [u'Campaign035', 963], [u'Campaign036', 1267], [u'Campaign037', 1010]]\n\n#convert to rdd by \".rdd\" will return \"Row\" object\nprint whole_log_df.groupBy(\"campaignID\").rdd.take(3)\n#[Row(campaignID=u'Campaign033', count=786), Row(campaignID=u'Campaign034', count=3867), Row(campaignID=u'Campaign035', count=963)]\n\n#`.asDict()` will convert to Key-Value RDD from Row object\nprint whole_log_df.groupBy(\"campaignID\").rdd.map(lambda x:x.asDict()).take(3)\n#[{'count': 786, 'campaignID': u'Campaign033'}, {'count': 3867, 'campaignID': u'Campaign034'}, {'count': 963, 'campaignID': u'Campaign035'}]\n\n\nDataframe\u304b\u3089Parquet file\u306b\u66f8\u304d\u51fa\u3059\nDataframe\u3092Parquet\u5f62\u5f0f\u3067file\u306b\u66f8\u304d\u51fa\u305b\u3070\u3001schema\u60c5\u5831\u3092\u4fdd\u6301\u3057\u305f\u307e\u307efile\u306bExport\u304c\u53ef\u80fd\u3067\u3059\u3002\u306a\u304a\u3001Export\u3059\u308bS3 bucket\u306edirectory\u304c\u65e2\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u306f\u66f8\u304d\u8fbc\u307f\u304c\u5931\u6557\u3057\u307e\u3059\u3001\u307e\u3060\u5b58\u5728\u3057\u3066\u3044\u306a\u3044Directory\u540d\u3092\u6307\u5b9a\u3057\u3066\u4e0b\u3055\u3044\u3002\n#write to parquet filed\nwhole_log_df.select(\"access_time\", \"userID\").write.parquet(\"s3n://my_S3_bucket/parquet_export\") \n\n#reload from parquet filed\nreload_df = sqlContext.read.parquet(\"s3n://my_S3_bucket/parquet_export\") \nprint reload_df.printSchema()\n\n## \u306f\u3058\u3081\u306b\uff1aSpark Dataframe\u3068\u306f\n\nSpark Ver 1.3\u304b\u3089[Spark Dataframe\u3068\u3044\u3046\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f](http://www.infoq.com/jp/news/2015/04/apache-spark-1.3-released)\u3002\u7279\u5fb4\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u69d8\u306a\u7269\u304c\u3042\u308a\u307e\u3059\u3002\n\n* Spark RDD\u306b**Schema\u8a2d\u5b9a\u3092\u52a0\u3048\u308b\u3068**\u3001Spark Dataframe\u306eObject\u3092\u4f5c\u6210\u3067\u304d\u308b\n* Dataframe\u306e\u5229\u70b9\u306f\u3001\n  * SQL\u98a8\u306e\u6587\u6cd5\u3067\u3001\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u3092\u62bd\u51fa\u3057\u305f\u308a\u3001Dataframe\u540c\u58eb\u306eJoin\u304c\u3067\u304d\u308b\n  * `filter`, `select`\u3068\u3044\u3046method\u3067\u3001\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u3001\u5217\u3092\u62bd\u51fa\u3067\u304d\u308b\n  * `groupBy \u2192 agg`\u3068\u3044\u3046method\u3067\u3001Log\u306e\u69d8\u3005\u306a\u96c6\u8a08\u304c\u3067\u304d\u308b\n  * UDF(User Defined Function)\u3067\u72ec\u81ea\u95a2\u6570\u3067\u5217\u306b\u51e6\u7406\u304c\u3067\u304d\u308b\n  * SQL\u3067\u8a00\u3046Pivot\u3082\u30b5\u30dd\u30fc\u30c8 (Spark v1.6\u304b\u3089\u306e\u6a5f\u80fd)\n\n\u3064\u307e\u308a\u3001RDD\u306e`map`\u3084`filter`\u3067\u30b7\u30b3\u30b7\u30b3\u8a18\u8ff0\u3059\u308b\u3088\u308a\u3082Simple Code\u3067\u3001\u4e14\u3064\u9ad8\u901f\u306b\u51e6\u7406\u304c\u884c\u3048\u308b\u306e\u304c\u30a6\u30ea\u3067\u3059\u3002Data\u306e\u524d\u51e6\u7406\u306fRDD\u3067\u3084\u308b\u3068\u3057\u3066\u3001\u3055\u3063\u3055\u3068Dataframe\u306b\u8aad\u307f\u8fbc\u3093\u3060\u65b9\u304cmaji\u3067\u6357\u308a\u307e\u3059\u3002Dataframe\u306e\u30e1\u30e2\u304c\u6563\u5728\u3057\u305f\u306e\u3067\u3001\u5099\u5fd8\u9332\u304c\u3066\u3089Sample code\u3092\u30c1\u30e9\u88cf\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u306a\u304a\u3001\n\n* \u3053\u306e\u8a18\u4e8b\u306b\u3042\u308bAPI\u4ee5\u5916\u306b\u3082\u3001[\u6ca2\u5c71API\u304c\u3042\u308a\u307e\u3059](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n* \u3053\u306e\u8a18\u4e8b\u306eIPython Notebook\u3092[\u3053\u3053\u306b\u7f6e\u3044\u3066\u304a\u304f\u306e\u3067](https://github.com/taka4sato/qiita_articles/blob/master/README.md)\u3001\u968f\u6642\u5b9f\u884c\u3057\u3066\u307f\u3066\u4e0b\u3055\u3044\n* Spark Dataframe\u306e[\u7e2e\u7d04/\u96c6\u8a08\u306eSample Code](http://sinhrks.hatenablog.com/entry/2015/04/29/085353)\n\n## Sample Log\u306e\u8aad\u307f\u8fbc\u307f\n\nAccess Log\u3092\u984c\u6750\u3068\u3057\u3066\u4f7f\u3044\u307e\u3059\u3002[\u6280\u8853\u8a55\u8ad6\u793e\u3055\u3093\u306e\u672c](http://gihyo.jp/book/2015/978-4-7741-7631-4/support)\u3067\u4f7f\u308f\u308c\u3066\u3044\u305fAccess Log(csv)\u3067\u3001[csv file\u3078\u306e\u76f4\u30ea\u30f3\u306f\u3053\u3061\u3089](http://image.gihyo.co.jp/assets/files/book/2015/978-4-7741-7631-4/download/click_data_sample.csv)\u3067\u3059\u3002csv\u306e\u4e2d\u8eab\u306f\u3001\u65e5\u4ed8\u3001User_ID, Campaign_ID\u306e3\u3064\u306e\u60c5\u5831\u3092\u6301\u3064\u4ee5\u4e0b\u306e\u69d8\u306aLog\u3067\u3059\n\n```\nclick.at\tuser.id\tcampaign.id\n2015/4/27 20:40\t144012\tCampaign077\n2015/4/27 0:27\t24485\tCampaign063\n2015/4/27 0:28\t24485\tCampaign063\n2015/4/27 0:33\t24485\tCampaign038\n```\n\ncsv\u3092\u8aad\u307f\u8fbc\u3093\u3067RDD\u306b\u3057\u307e\u3059\u30021\u884c\u76ee\u306eheader\u306e\u524a\u9664\u3068\u30011\u5217\u76ee\u3092datetime Object\u3068\u3057\u3066\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```py\nimport json, os, datetime, collections, commands\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.types import *\n\nif not os.path.exists(\"./click_data_sample.csv\"):\n    print \"csv file not found at master node, will download and copy to HDFS\"\n    commands.getoutput(\"wget -q http://image.gihyo.co.jp/assets/files/book/2015/978-4-7741-7631-4/download/click_data_sample.csv\")\n    commands.getoutput(\"hadoop fs -copyFromLocal -f ./click_data_sample.csv /user/hadoop/\")\n\nwhole_raw_log = sc.textFile(\"/user/hadoop/click_data_sample.csv\")\nheader = whole_raw_log.first()\nwhole_log = whole_raw_log.filter(lambda x:x !=header).map(lambda line: line.split(\",\"))\\\n            .map(lambda line: [datetime.datetime.strptime(line[0].replace('\"', ''), '%Y-%m-%d %H:%M:%S'), int(line[1]), line[2].replace('\"', '')])\n\nwhole_log.take(3)\n#[[datetime.datetime(2015, 4, 27, 20, 40, 40), 144012, u'Campaign077'],\n# [datetime.datetime(2015, 4, 27, 0, 27, 55), 24485, u'Campaign063'],\n# [datetime.datetime(2015, 4, 27, 0, 28, 13), 24485, u'Campaign063']]\n```\n\n\n## Dataframe\u306e\u4f5c\u6210\u65b9\u6cd5\n##### **RDD\u304b\u3089\u4f5c\u6210**\nDataframe\u306f\u3001\u5143\u3068\u306a\u308bRDD\u304c\u3042\u308c\u3070\u3001Column\u306e\u540d\u524d\u3068\u305d\u308c\u305e\u308c\u306eType(`TimestampType`, `IntegerType`, `StringType`\u306a\u3069)\u3092\u6307\u5b9a\u3057\u3066\u3001`sqlContext.createDataFrame(my_rdd, my_schema)`\u3067\u4f5c\u6210\u3067\u304d\u307e\u3059\u3002[Schema\u306e\u5b9a\u7fa9\u306f\u3053\u3053\u3092\u53c2\u7167](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types)\u3002\n\n`printSchema()`, `dtypes`\u3067Schema\u60c5\u5831\u3001`count()`\u3067\u884c\u6570\u3001`show(n)`\u3067\u6700\u521d\u306en\u4ef6\u306erecord\u306e\u8868\u793a\u3067\u3059\u3002\n\n```py\nfields = [StructField(\"access_time\", TimestampType(), True), StructField(\"userID\", IntegerType(), True), StructField(\"campaignID\", StringType(), True)]\nschema = StructType(fields)\n\nwhole_log_df = sqlContext.createDataFrame(whole_log, schema)\nprint whole_log_df.count()\nprint whole_log_df.printSchema()\nprint whole_log_df.dtypes\nprint whole_log_df.show(5)\n\n#327430\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- userID: integer (nullable = true)\n# |-- campaignID: string (nullable = true)\n#\n#[('access_time', 'timestamp'), ('userID', 'int'), ('campaignID', 'string')]\n#\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-04-27 20:40:...|144012|Campaign077|\n#|2015-04-27 00:27:...| 24485|Campaign063|\n#|2015-04-27 00:28:...| 24485|Campaign063|\n#|2015-04-27 00:33:...| 24485|Campaign038|\n#|2015-04-27 01:00:...| 24485|Campaign063|\n```\n##### **csv file\u304b\u3089\u76f4\u63a5\u4f5c\u6210**\ncsv\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306f\u3001[Spark Package](http://spark-packages.org/)\u306e1\u3064\u3067\u3042\u308b[`spark-csv`](https://github.com/databricks/spark-csv)\u3092\u4f7f\u3046\u3068\u697d\u3067\u3059\u3002\u7279\u306b\u6307\u5b9a\u3057\u306a\u3044\u3068\u5168\u3066string\u3068\u3057\u3066\u8aad\u307f\u8fbc\u307f\u307e\u3059\u304c\u3001`inferSchema`\u3092\u6307\u5b9a\u3057\u3066\u3042\u3052\u308b\u3068\u826f\u3044\u611f\u3058\u306b\u985e\u63a8\u3057\u3066\u304f\u308c\u307e\u3059\u3002\n\n```py\nwhole_log_df_2 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"/user/hadoop/click_data_sample.csv\")\nprint whole_log_df_2.printSchema()\nprint whole_log_df_2.show(5)\n\n#root\n# |-- click.at: string (nullable = true)\n# |-- user.id: string (nullable = true)\n# |-- campaign.id: string (nullable = true)\n#\n#+-------------------+-------+-----------+\n#|           click.at|user.id|campaign.id|\n#+-------------------+-------+-----------+\n#|2015-04-27 20:40:40| 144012|Campaign077|\n#|2015-04-27 00:27:55|  24485|Campaign063|\n#|2015-04-27 00:28:13|  24485|Campaign063|\n#|2015-04-27 00:33:42|  24485|Campaign038|\n#|2015-04-27 01:00:04|  24485|Campaign063|\n\nwhole_log_df_3 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/user/hadoop/click_data_sample.csv\")\nprint whole_log_df_3.printSchema()\n\n#root\n# |-- click.at: timestamp (nullable = true)\n# |-- user.id: integer (nullable = true)\n# |-- campaign.id: string (nullable = true)\n```\n\n\u3061\u306a\u307f\u306b\u3001column\u540d\u306b`.`\u304c\u5165\u3063\u3066\u5c45\u308b\u3068\u8272\u3005\u9762\u5012\u306a\u306e\u3067\u3001`withColumnRenamed`\u3067rename\u53ef\u80fd\u3067\u3059(rename\u3057\u305f\u5225\u306eDataframe\u3092\u4f5c\u6210\u53ef\u80fd\u3067\u3059)\u3002\n\n\n```py\nwhole_log_df_4 = whole_log_df_3.withColumnRenamed(\"click.at\", \"access_time\")\\\n                 .withColumnRenamed(\"user.id\", \"userID\")\\\n                 .withColumnRenamed(\"campaign.id\", \"campaignID\")\nprint whole_log_df_4.printSchema()\n\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- userID: integer (nullable = true)\n# |-- campaignID: string (nullable = true)\n```\n\n##### **json\u304b\u3089\u76f4\u63a5\u4f5c\u6210**\njson file\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306f`sqlContext.read.json`\u3092\u4f7f\u3044\u307e\u3059\u3002file\u306e\u5404\u884c\u30921 json object\u3068\u3057\u3066\u6271\u3044\u307e\u3059\u3001\u5b58\u5728\u3057\u306a\u3044Key\u304c\u3042\u308b\u5834\u5408\u306b\u306f\u3001`null`\u304c\u5165\u308a\u307e\u3059\u3002\n\n```py\n# test_json.json contains following 3 lines, last line doesn't have \"campaignID\" key\n#\n#{\"access_time\": \"2015-04-27 20:40:40\", \"userID\": \"24485\", \"campaignID\": \"Campaign063\"}\n#{\"access_time\": \"2015-04-27 00:27:55\", \"userID\": \"24485\", \"campaignID\": \"Campaign038\"}\n#{\"access_time\": \"2015-04-27 00:27:55\", \"userID\": \"24485\"}\n\ndf_json = sqlContext.read.json(\"/user/hadoop/test_json.json\")\ndf_json.printSchema()\ndf_json.show(5)\n\n#root\n# |-- access_time: string (nullable = true)\n# |-- campaignID: string (nullable = true)\n# |-- userID: string (nullable = true)\n#\n#+-------------------+-----------+------+\n#|        access_time| campaignID|userID|\n#+-------------------+-----------+------+\n#|2015-04-27 20:40:40|Campaign063| 24485|\n#|2015-04-27 00:27:55|Campaign038| 24485|\n#|2015-04-27 00:27:55|       null| 24485|\n#+-------------------+-----------+------+\n```\n\n##### **parquet\u304b\u3089\u76f4\u63a5\u4f5c\u6210**\nparquet file\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060data\u3092\u305d\u306e\u307e\u307eDataframe\u306b\u3059\u308b\u306b\u306f`sqlContext.read.parquet`\u3092\u4f7f\u3044\u307e\u3059\u3002parquet file\u304c\u7f6e\u3044\u3066\u3042\u308bFolder\u3092\u6307\u5b9a\u3059\u308b\u3068\u3001\u305d\u306eFolder\u4ee5\u4e0b\u306eparquet file\u3092\u4e00\u62ec\u3067\u8aad\u307f\u8fbc\u3093\u3067\u304f\u308c\u307e\u3059\u3002\n\n```py\nsqlContext.read.parquet(\"/user/hadoop/parquet_folder/\")\n```\n\n## SQL\u6587\u3067Query\nDataframe\u306b\u5bfe\u3057\u3066\u3001SQL\u306e\u6587\u3067Query\u3092\u639b\u3051\u308bSample\u3067\u3059\u3002`registerTempTable`\u3067Dataframe\u306bSQL Table name\u3092\u4ed8\u4e0e\u3059\u308b\u3068\u3001SQL\u306eTable\u540d\u3068\u3057\u3066\u53c2\u7167\u3067\u304d\u307e\u3059\u3002`sqlContext.sql(SQL\u6587)`\u306e\u623b\u308a\u5024\u3082Dataframe\u3067\u3059\u3002\n\n\u306a\u304a\u3001Sub Query\u3092\u8a18\u8f09\u3059\u308b\u4e8b\u3082\u53ef\u80fd\u306a\u306e\u3067\u3059\u304c\u3001Sub Query\u5074\u306bAlias\u3092\u4ed8\u4e0e\u3057\u306a\u3044\u3068\u3001\u4f55\u6545\u304bSyntax error\u304c\u8d77\u304d\u308b\u306e\u3067\u6ce8\u610f\u3067\u3059\u3002\n\n```py\n#\u5358\u7d14\u306aSQL query\n\nwhole_log_df.registerTempTable(\"whole_log_table\")\n\nprint sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").count()\n#18081\nprint sqlContext.sql(\" SELECT * FROM whole_log_table where campaignID == 'Campaign047' \").show(5)\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:26:...| 14151|Campaign047|\n#|2015-04-27 05:27:...| 14151|Campaign047|\n#|2015-04-27 05:28:...| 14151|Campaign047|\n#+--------------------+------+-----------+\n\n\n#SQL\u6587\u306e\u4e2d\u306b\u5909\u6570\u3092\u5165\u308c\u308b\u5834\u5408\nfor count in range(1, 3):\n    print \"Campaign00\" + str(count)\n    print sqlContext.sql(\"SELECT count(*) as access_num FROM whole_log_table where campaignID == 'Campaign00\" + str(count) + \"'\").show()\n\n#Campaign001\n#+----------+\n#|access_num|\n#+----------+\n#|      2407|\n#+----------+\n#\n#Campaign002\n#+----------+\n#|access_num|\n#+----------+\n#|      1674|\n#+----------+\n\n#Sub Query\u306e\u5834\u5408\uff1a\nprint sqlContext.sql(\"SELECT count(*) as first_count FROM (SELECT userID, min(access_time) as first_access_date FROM whole_log_table GROUP BY userID) subquery_alias WHERE first_access_date < '2015-04-28'\").show(5)\n#+------------+\n#|first_count |\n#+------------+\n#|       20480|\n#+------------+\n```\n\n## **filter**, **select**\u3067\u6761\u4ef6\u4ed8\u304d\u691c\u7d22\nDataframe\u306b\u5bfe\u3057\u3066\u306e\u7c21\u6613\u7684\u306a\u691c\u7d22\u6a5f\u80fd\u3067\u3059\u3002\u4e0a\u8a18\u306b\u3042\u308b`SQL\u6587\u3067Query`\u3068\u6a5f\u80fd\u306f\u4f3c\u3066\u3044\u307e\u3059\u304c\u3001`filter`, `select`\u306f\u7c21\u6613\u7684\u306a\u691c\u7d22\u6a5f\u80fd\u3068\u3044\u3046\u4f4d\u7f6e\u3065\u3051\u3067\u3059\u3002`filter`\u306f\u6761\u4ef6\u306b\u8a72\u5f53\u3059\u308b\u884c\u306e\u62bd\u51fa\u3001`select`\u306f\u5217\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002RDD\u306e`filter`\u3068\u3061\u3087\u3063\u3068\u6587\u6cd5\u304c\u9055\u3046\u306e\u306b\u6ce8\u610f\u3067\u3059\u3002\n\n```py\n#Sample for filter\nprint whole_log_df.filter(whole_log_df[\"access_time\"] < \"2015-04-28\").count()\n#41434\nprint whole_log_df.filter(whole_log_df[\"access_time\"] > \"2015-05-01\").show(3)\n#+--------------------+------+-----------+\n#|         access_time|userID| campaignID|\n#+--------------------+------+-----------+\n#|2015-05-01 22:11:...|114157|Campaign002|\n#|2015-05-01 23:36:...| 93708|Campaign055|\n#|2015-05-01 22:51:...| 57798|Campaign046|\n#+--------------------+------+-----------+\n\n#Sample for select\nprint whole_log_df.select(\"access_time\", \"userID\").show(3)\n#+--------------------+------+\n#|         access_time|userID|\n#+--------------------+------+\n#|2015-04-27 20:40:...|144012|\n#|2015-04-27 00:27:...| 24485|\n#|2015-04-27 00:28:...| 24485|\n#+--------------------+------+\n```\n\n## **groupBy**\u3067\u96c6\u8a08\n`groupBy`\u306f\u3001RDD\u306e`reduceByKey`\u306b\u4f3c\u305f\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u304c\u3001`groupBy`\u306f[\u3053\u3053\u306b\u3042\u308bmethod](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData)\u3092\u305d\u306e\u5f8c\u308d\u3067Call\u3059\u308b\u4e8b\u3067\u3001\u69d8\u3005\u306a\u96c6\u8a08\u6a5f\u80fd\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002\u4ee3\u8868\u7684\u306a\u306e\u306f`agg`\u3068`count`\u3067\u3059\u3002\n\n##### **groupBy**\u2192**count**\u3067\u96c6\u8a08\n`campaignID`\u3092Key\u306b\u3057\u3066`groupBy`\u3092\u5b9f\u884c\u3057\u3001\u305d\u306eRecord\u6570\u3092`count()`\u3067\u96c6\u8a08\u3057\u3066\u304f\u308c\u307e\u3059\u3002`groupBy`\u306b\u8907\u6570\u306eKey\u3092\u5217\u6319\u3059\u308c\u3070\u3001\u305d\u306e\u7d44\u307f\u5408\u308f\u305b\u3092key\u3068\u3057\u3066groupBy\u3057\u3066\u304f\u308c\u307e\u3059\u3002\n\n```py\nprint whole_log_df.groupBy(\"campaignID\").count().sort(\"count\", ascending=False).show(5)\n#+-----------+-----+\n#| campaignID|count|\n#+-----------+-----+\n#|Campaign116|22193|\n#|Campaign027|19206|\n#|Campaign047|18081|\n#|Campaign107|13295|\n#|Campaign131| 9068|\n#+-----------+-----+\n\nprint whole_log_df.groupBy(\"campaignID\", \"userID\").count().sort(\"count\", ascending=False).show(5)\n#+-----------+------+-----+\n#| campaignID|userID|count|\n#+-----------+------+-----+\n#|Campaign047| 30292|  633|\n#|Campaign086|107624|  623|\n#|Campaign047|121150|  517|\n#|Campaign086| 22975|  491|\n#|Campaign122| 90714|  431|\n#+-----------+------+-----+\n```\n\n\n##### **groupBy**\u2192**agg**\u3067\u96c6\u8a08\n`userID`\u3092Key\u306b\u3057\u3066GroupBy\u3092\u5b9f\u884c\u3057\u3001\u305d\u306e\u96c6\u8a08\u7d50\u679c\u306e\u5e73\u5747\u3084\u6700\u5927/\u6700\u5c0f\u3092\u8a08\u7b97\u304c\u53ef\u80fd\u3067\u3059\u3002`agg({key:value})`\u3067\u3001`key`\u306e\u5217\u306b\u5bfe\u3057\u3066`value`\u306e\u95a2\u6570(`min`,`sum`, `ave` etc)\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u8fd4\u3057\u307e\u3059\u3002\u623b\u308a\u5024\u306fDataframe\u306a\u306e\u3067\u3001`.filter()`\u3067\u66f4\u306b\u884c\u3092\u7d5e\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3059\u3002\n\n```py\nprint whole_log_df.groupBy(\"userID\").agg({\"access_time\": \"min\"}).show(3)\n#+------+--------------------+\n#|userID|    min(access_time)|\n#+------+--------------------+\n#|  4831|2015-04-27 22:49:...|\n#| 48631|2015-04-27 22:15:...|\n#|143031|2015-04-27 21:52:...|\n#+------+--------------------+\n\nprint whole_log_df.groupBy(\"userID\").agg({\"access_time\": \"min\"}).filter(\"min(access_time) < '2015-04-28'\").count()\n#20480\n```\n\n##### **groupBy**\u2192**pivot**\u3067\u7e26\u6a2a\u5909\u63db\nPivot\u306fSpark v1.6\u304b\u3089\u306e\u65b0\u6a5f\u80fd\u3067[SQL\u306ePivot\u3068\u4f3c\u305f\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059](https://databricks.com/blog/2016/02/09/reshaping-data-with-pivot-in-spark.html)\u3002Sample code\u306ePivot\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u69d8\u306b\u7e26\u6a2a\u304c\u5909\u5316\u3057\u307e\u3059\u3002\n\n* pivot\u524d(`agged_df`)\n  * \u884c\u6570\u304c(UserID\u6570 (=75,545) x campainID\u6570 (=133) )\n  * \u5217\u304c3\u5217\n* pivot\u5f8c(`pivot_df`)\n  * \u884c\u6570\u304cUserID\u6570 (=75,545)\n  * \u5217\u304c UserID + CampainID\u6570 = 1 + 133 = 134\n\n\u5fc5\u305a\u3001groupBy(\"\u7e26\u306e\u307e\u307e\u306e\u5217\").pivot(\"\u7e26\u304b\u3089\u6a2a\u3078\u5909\u63db\u3057\u305f\u3044\u5217\").sum(\"\u96c6\u8a08\u5024\u306e\u5217\")\u30683\u3064\u306emethod\u3092chain\u3067\u547c\u3076\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n```py\nagged_df = whole_log_df.groupBy(\"userID\", \"campaignID\").count()\nprint agged_df.show(3)\n\n#+------+-----------+-----+\n#|userID| campaignID|count|\n#+------+-----------+-----+\n#|155812|Campaign107|    4|\n#|103339|Campaign027|    1|\n#|169114|Campaign112|    1|\n#+------+-----------+-----+\n\n#\u5024\u304c\u7121\u3044Cell\u306f\u3001null\u304c\u5165\u308b\npivot_df = agged_df.groupBy(\"userID\").pivot(\"campaignID\").sum(\"count\")\nprint pivot_df.printSchema()\n\n#root\n# |-- userID: integer (nullable = true)\n# |-- Campaign001: long (nullable = true)\n# |-- Campaign002: long (nullable = true)\n# ..\n# |-- Campaign133: long (nullable = true)\n\n#\u5024\u304c\u7121\u3044Cell\u30920\u3067\u57cb\u3081\u305f\u3044\u5834\u5408\npivot_df2 = agged_df.groupBy(\"userID\").pivot(\"campaignID\").sum(\"count\").fillna(0)\n```\n\n\n\n## UDF\u3067\u5217\u306e\u8ffd\u52a0\nSpark Dataframe\u3067\u306fUDF\u304c\u4f7f\u3048\u307e\u3059\u3001\u4e3b\u306a\u7528\u9014\u306f\u3001\u5217\u306e\u8ffd\u52a0\u306b\u306a\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002Dataframe\u306f\u57fa\u672cImmutable(\u4e0d\u5909)\u306a\u306e\u3067\u3001\u5217\u306e\u4e2d\u8eab\u306e\u5909\u66f4\u306f\u3067\u304d\u305a\u3001\u5217\u3092\u8ffd\u52a0\u3057\u305f\u5225\u306eDataframe\u3092\u4f5c\u6210\u3059\u308b\u4e8b\u306b\u306a\u308a\u307e\u3059\u3002\n\n```py\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import DoubleType\n\ndef add_day_column(access_time):\n    return int(access_time.strftime(\"%Y%m%d\"))\n    \nmy_udf = UserDefinedFunction(add_day_column, IntegerType())\nprint whole_log_df.withColumn(\"access_day\", my_udf(\"access_time\")).show(5)\n\n#+--------------------+------+-----------+----------+\n#|         access_time|userID| campaignID|access_day|\n#+--------------------+------+-----------+----------+\n#|2015-04-27 20:40:...|144012|Campaign077|  20150427|\n#|2015-04-27 00:27:...| 24485|Campaign063|  20150427|\n#|2015-04-27 00:28:...| 24485|Campaign063|  20150427|\n#|2015-04-27 00:33:...| 24485|Campaign038|  20150427|\n#|2015-04-27 01:00:...| 24485|Campaign063|  20150427|\n#+--------------------+------+-----------+----------+\n```\n\nUDF\u306e\u8868\u8a18\u306f\u3001lambda\u95a2\u6570\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\n\n```py\nmy_udf2 = UserDefinedFunction(lambda x: x + 5, IntegerType())\nprint whole_log_df.withColumn(\"userID_2\", my_udf2(\"userID\")).show(5)\n\n#+--------------------+------+-----------+--------+\n#|         access_time|userID| campaignID|userID_2|\n#+--------------------+------+-----------+--------+\n#|2015-04-27 20:40:...|144012|Campaign077|  144017|\n#|2015-04-27 00:27:...| 24485|Campaign063|   24490|\n#|2015-04-27 00:28:...| 24485|Campaign063|   24490|\n#|2015-04-27 00:33:...| 24485|Campaign038|   24490|\n#|2015-04-27 01:00:...| 24485|Campaign063|   24490|\n#+--------------------+------+-----------+--------+\n```\n\n\u9006\u306b\u3001\u7279\u5b9a\u306e\u5217\u3092\u524a\u9664\u3057\u305f\u3044Dataframe\u3092\u4f5c\u308b\u306b\u306f`df.drop()`\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n```py\nprint whole_log_df.drop(\"userID\").show(3)\n\n#+--------------------+-----------+\n#|         access_time| campaignID|\n#+--------------------+-----------+\n#|2015-04-27 20:40:...|Campaign077|\n#|2015-04-27 00:27:...|Campaign063|\n#|2015-04-27 00:28:...|Campaign063|\n#+--------------------+-----------+\n```\n\n## Join\u30672\u3064\u306eDataframe\u3092\u7d50\u5408\u3055\u305b\u308b\n2\u3064\u306eDataframe\u3092Join\u3055\u305b\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3059\u3002\u3053\u3053\u3067\u306f\u3001Heavy User(Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser)\u306eLog\u306e\u307f\u3092\u5168\u4f53\u306eLog\u304b\u3089\u62bd\u51fa\u3059\u308b\u30b1\u30fc\u30b9\u3092\u8003\u3048\u3066\u307f\u307e\u3059\u3002\n\n\u307e\u305a\u3001Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser\u306eUser ID\u3068\u305d\u306eAccess\u6570\u3092\u3001`.groupBy(\"userID\").count()`\u3067\u96c6\u8a08\u3057\u3001`filter`\u3067100\u56de\u4ee5\u4e0a\u306b\u7d5e\u308a\u8fbc\u307f\u307e\u3059\u3002\n\n```py\nheavy_user_df1 = whole_log_df.groupBy(\"userID\").count()\nheavy_user_df2 = heavy_user_df1.filter(heavy_user_df1 [\"count\"] >= 100)\n\nprint heavy_user_df2 .printSchema()\nprint heavy_user_df2 .show(3)\nprint heavy_user_df2 .count()\n\n#root\n# |-- userID: integer (nullable = true)\n# |-- count: long (nullable = false)\n#\n#+------+-----+\n#|userID|count|\n#+------+-----+\n#| 84231|  134|\n#| 13431|  128|\n#|144432|  113|\n#+------+-----+\n#\n#177\n```\n\n\u5143\u306eDataframe(\u3053\u3061\u3089\u304cLeft\u306b\u306a\u308b)\u3067`join` method\u3092\u547c\u3073\u3001join\u306e\u76f8\u624b(Right\u306b\u306a\u308b)\u3068join\u306e\u6761\u4ef6\u3092\u66f8\u304f\u3068\u3001SQL\u306ejoin\u306e\u69d8\u306bDataframe\u306e\u7d50\u5408\u304c\u53ef\u80fd\u3067\u3059\u3002\n\njoin\u306e\u5f62\u5f0f\u306f\u3001`inner`, `outer`, `left_outer`, `rignt_outer`\u306a\u3069\u304c\u9078\u3079\u308b\u306f\u305a\u306a\u306e\u3067\u3059\u304c\u3001`inner`\u4ee5\u5916\u306f\u610f\u56f3\u3057\u305f\u6319\u52d5\u3067\u52d5\u3044\u3066\u304f\u308c\u306a\u3044\u70ba(\u3057\u304b\u3082`outer`\u3068\u3057\u3066\u51e6\u7406\u3055\u308c\u308b)\u3001\u3068\u308a\u3042\u3048\u305a`inner`\u3067\u5916\u90e8\u7d50\u5408\u3057\u3066\u5f8c\u306b`drop`\u3067\u8981\u3089\u306a\u3044Column\u3092\u524a\u9664\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002[\u8a73\u7d30Option\u7b49\u306f\u516c\u5f0fPage](https://spark.apache.org/docs/1.5.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join)\u3092\u53c2\u7167\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n\u4ee5\u4e0b\u306ejoin\u51e6\u7406\u306b\u3088\u308a\u3001Access\u6570\u304c100\u56de\u4ee5\u4e0a\u3042\u308bUser(177\u540d)\u306b\u8a72\u5f53\u3059\u308b38,729\u884c\u306eLog\u3092\u53d6\u308a\u51fa\u3059\u4e8b\u304c\u3067\u304d\u307e\u3057\u305f(\u5168\u4f53\u306eLog\u306f\u7d0432\u4e07\u884c)\u3002\n\n\n```py\njoinded_df = whole_log_df.join(heavy_user_df2, whole_log_df[\"userID\"] == heavy_user_df2[\"userID\"], \"inner\").drop(heavy_user_df2[\"userID\"]).drop(\"count\")\nprint joinded_df.printSchema()\nprint joinded_df.show(3)\nprint joinded_df.count()\n\n#root\n# |-- access_time: timestamp (nullable = true)\n# |-- campaignID: string (nullable = true)\n# |-- userID: integer (nullable = true)\n\n#None\n#+--------------------+-----------+------+\n#|         access_time| campaignID|userID|\n#+--------------------+-----------+------+\n#|2015-04-27 02:07:...|Campaign086| 13431|\n#|2015-04-28 00:07:...|Campaign086| 13431|\n#|2015-04-29 06:01:...|Campaign047| 13431|\n#+--------------------+-----------+------+\n#\n#38729\n```\n\n\n\n\n\n## Dataframe\u304b\u3089\u5217\u3092\u53d6\u308a\u51fa\u3059\n* \u5217\u306eLabel\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3001df.columns\u3067\u5217\u306eLabel\u306eList(not Dataframe)\u304c\u53d6\u308a\u51fa\u305b\u308b\n* \u7279\u5b9a\u306e\u5217\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f\u3001df.select(\"userID\").map(lambda x: x[0]).collect()\u3067\u3001\"userID\"\u5217\u306eList(not RDD/Dataframe)\u304c\u53d6\u308a\u51fa\u305b\u308b\n* \u7279\u5b9a\u306e\u5217\u3067**\u91cd\u8907\u304c\u7121\u3044**List\u3092\u53d6\u308a\u51fa\u3059\u306b\u306f`.distinct()`\u3092Dataframe\u306e\u6700\u5f8c\u306b\u8ffd\u52a0\u3059\u308c\u3070OK\n\n\n```py\nprint whole_log_df.columns\n#['access_time', 'userID', 'campaignID']\n\nprint whole_log_df.select(\"userID\").map(lambda x: x[0]).collect()[:5]\n#[144012, 24485, 24485, 24485, 24485]\n\nprint whole_log_df.select(\"userID\").distinct().map(lambda x:x[0]).collect()[:5]\n#[4831, 48631, 143031, 39631, 80831]\n```\n\n\n\n## Dataframe\u304b\u3089RDD/List\u306b\u623b\u3059\nDataframe\u306f\u3092RDD\u306b\u623b\u3059\u306b\u306f\u3001\u5927\u304d\u304f2\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\n\n* `.map`\u3092\u547c\u3076\n  * Dataframe\u306eSchema\u60c5\u5831\u306f\u7834\u68c4\u3055\u308c\u3001Dataframe\u306e\u5404\u884c\u304c\u305d\u308c\u305e\u308clist\u306b\u306a\u3063\u305fRDD\u306b\u5909\u63db\u3055\u308c\u307e\u3059\n* `.rdd`\u3092\u547c\u3076\n  * Dataframe\u306e\u5404\u884c\u304c\u305d\u308c\u305e\u308c[Row Ojbect](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)\u306aRDD\u306b\u5909\u63db\u3055\u308c\u307e\u3059\u3002Row Object\u306fSpark SQL\u3067\u4e00\u884c\u5206\u306e\u30c7\u30fc\u30bf\u3092\u4fdd\u6301\u3059\u308b\u70ba\u306eObject\u3067\u3059\n  * `my_rdd.rdd.map(lambda x:x.asDict())`\u3068\u3001Row object\u306b\u5bfe\u3057\u3066`.asDict()`\u3092\u547c\u3093\u3067\u3042\u3052\u308b\u3068\u3001Key-Value\u306aRDD\u306b\u5909\u63db\u53ef\u80fd\u3067\u3059\n\n\n```py\n#convert to rdd by \".map\"\nprint whole_log_df.groupBy(\"campaignID\").count().map(lambda x: [x[0], x[1]]).take(5)\n#[[u'Campaign033', 786], [u'Campaign034', 3867], [u'Campaign035', 963], [u'Campaign036', 1267], [u'Campaign037', 1010]]\n\n# rdd -> normal list can be done with \"collect\".\nprint whole_log_df.groupBy(\"campaignID\").count().map(lambda x: [x[0], x[1]]).collect()[:5]\n#[[u'Campaign033', 786], [u'Campaign034', 3867], [u'Campaign035', 963], [u'Campaign036', 1267], [u'Campaign037', 1010]]\n\n#convert to rdd by \".rdd\" will return \"Row\" object\nprint whole_log_df.groupBy(\"campaignID\").rdd.take(3)\n#[Row(campaignID=u'Campaign033', count=786), Row(campaignID=u'Campaign034', count=3867), Row(campaignID=u'Campaign035', count=963)]\n\n#`.asDict()` will convert to Key-Value RDD from Row object\nprint whole_log_df.groupBy(\"campaignID\").rdd.map(lambda x:x.asDict()).take(3)\n#[{'count': 786, 'campaignID': u'Campaign033'}, {'count': 3867, 'campaignID': u'Campaign034'}, {'count': 963, 'campaignID': u'Campaign035'}]\n```\n\n## Dataframe\u304b\u3089Parquet file\u306b\u66f8\u304d\u51fa\u3059\nDataframe\u3092Parquet\u5f62\u5f0f\u3067file\u306b\u66f8\u304d\u51fa\u305b\u3070\u3001schema\u60c5\u5831\u3092\u4fdd\u6301\u3057\u305f\u307e\u307efile\u306bExport\u304c\u53ef\u80fd\u3067\u3059\u3002\u306a\u304a\u3001Export\u3059\u308bS3 bucket\u306edirectory\u304c\u65e2\u306b\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u306f\u66f8\u304d\u8fbc\u307f\u304c\u5931\u6557\u3057\u307e\u3059\u3001\u307e\u3060\u5b58\u5728\u3057\u3066\u3044\u306a\u3044Directory\u540d\u3092\u6307\u5b9a\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n```py\n#write to parquet filed\nwhole_log_df.select(\"access_time\", \"userID\").write.parquet(\"s3n://my_S3_bucket/parquet_export\") \n\n#reload from parquet filed\nreload_df = sqlContext.read.parquet(\"s3n://my_S3_bucket/parquet_export\") \nprint reload_df.printSchema()\n```\n", "tags": ["Spark", "DataFrame", "Python"]}