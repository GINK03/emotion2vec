{"tags": ["Chainer", "\u6a5f\u68b0\u5b66\u7fd2", "\u6df1\u5c64\u5b66\u7fd2"], "context": "\u590f\u4f11\u307f\u6700\u7d42\u65e5\uff0e\n\u30ec\u30b8\u306b\u7740\u3044\u305f\u30890.3\u79d2\u3067\u8cb7\u3048\u308b\u672c\u3092\u5165\u624b\u3057\uff0c\u308a\u3093\u304b\u3044\u7dda\u306b\u4e57\u3063\u305f\u79c1\u306f\u3075\u3068ChainerJP\u306e\u30c4\u30a4\u30fc\u30c8\u3092\u8aad\u3093\u3060\uff0e\n\u3069\u3046\u3084\u3089Chainer\u304cv 1.13.0\u306b\u306a\u3063\u305f\u3089\u3057\u3044\uff0e\n\u524d\u56de\u306e\u66f4\u65b0\u304b\u30892\u9031\u9593\u3067\u4ee5\u4e0b\u306e\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u305f\u3053\u3068\u306b\u306a\u308b\uff0e\n\u30ac\u30c3\u30ba\u30a3\u30fc\u30e9\u4e26\u307f\u306e\u66f4\u65b0\u983b\u5ea6\u3067\u3042\u308b\uff0e\n\nLinear\u3068Convolution2D\u3067 input channel\u306e\u521d\u671f\u5316\u304c\u81ea\u52d5\u5316\u3055\u308c\u305f\nin_size = None\u3068\u3059\u308b\u3053\u3068\u3067\uff0c\u6700\u521d\u306b__call__\u304c\u547c\u3070\u308c\u305f\u969b\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5316\u304c\u884c\u308f\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n\u8a66\u884c\u932f\u8aa4\u3059\u308b\u969b\uff0c\u975e\u5e38\u306b\u4fbf\u5229\u3067\u3042\u308b\uff0e\nhttps://github.com/pfnet/chainer/blob/master/chainer/links/connection/linear.py\nclass MnistModel(chainer.Chain):\n    def __init__(self):\n        super(MnistModel,self).__init__(\n                l1 = L.Linear(out_size=100,in_size=None),\n                l2 = L.Linear(out_size=100,in_size=None),\n                l3 = L.Linear(out_size=10,in_size=None)\n        )\n\n    def __call__(self,x):\n        h = F.relu(self.l1(x))\n        h = F.relu(self.l2(h))\n        return self.l3(h)\n\n\nLinear\u306e__init__\u304c\n__init__(self, in_size, out_size, wscale=1, (\u7565))\n\n\u304b\u3089\n__init__(self, out_size, in_size = None,wscale=1, (\u7565))\n\n\u306b\u306a\u308b\u3068\u3082\u3063\u3068\u5b09\u3057\u3044\u306e\u3060\u304c\uff0cLinear,Convolution2D\u4ee5\u5916\u306elinks\u3068\u306e\u6574\u5408\u6027\u3092\u4fdd\u3064\u305f\u3081\u306b\u3053\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u601d\u308f\u308c\u308b\uff0e\nChainer\u306f\u4eba\u9593\u306b\u512a\u3057\u3044\uff0e\n\nlinks\u306bCRF1d\u8ffd\u52a0\n\u3044\u308f\u3086\u308b\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u3092\u5b66\u7fd2\u3059\u308b\u305f\u3081\u306eLayer\u3067\uff0c\u7cfb\u5217\u30e9\u30d9\u30ea\u30f3\u30b0\u306a\u3069\u306b\u7528\u3044\u3089\u308c\u3066\u3044\u308b\uff0e\n( https://github.com/pfnet/chainer/issues/1020 )\n\u8ad6\u6587( http://arxiv.org/pdf/1508.01991v1.pdf )\u3067\u306f Part-Of-Speech Tagging, Chunking, Entity Recognition\u306b\u304a\u3044\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u305f\u3068\u66f8\u304b\u308c\u3066\u3044\u308b\uff0e\n\u5b9f\u88c5\u3068\u3057\u3066\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u304d\u306eloss\u95a2\u6570\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3042\u308b\uff0e\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30baN\uff0c\u9577\u3055L\uff0c\u30af\u30e9\u30b9\u6570K\u306e\u7cfb\u5217\u30e9\u30d9\u30ea\u30f3\u30b0\u554f\u984c\u306b\u5bfe\u3057\u3066\ny = [ Variable(N,K) * L\u500b ] , t = [Variable(N) * L\u500b ]\u3068\u3044\u3046\u5f62\u3067\u5165\u529b\u3059\u308b\u3068\u52d5\u4f5c\u3059\u308b\uff08List of Variable\uff09\n\u304a\u307e\u3051 \u306e\u90e8\u5206\u306bCRF1d\u3092\u7528\u3044\u305f\u718a\u672c\u5f01\u7ffb\u8a33\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u63b2\u8f09\u3057\u3066\u304a\u304f\uff0e\n\u4f55\u304b\u306e\u53c2\u8003\u306b\u306a\u308c\u3070\u5e78\u3044\u3067\u3042\u308b\uff0e\n\uff08 \u718a\u672c\u5f01\u7ffb\u8a33\u89e3\u8aac http://qiita.com/_329_/items/1d5de7b8213b112a3df7 \uff09\n\nfunctions\u306bseparate\u8ffd\u52a0\nchainer.functions.stack\u306e\u9006\u306e\u52d5\u4f5c\u3092\u884c\u3046.\nhttp://docs.chainer.org/en/latest/_modules/chainer/functions/array/separate.html\n\nStateful Peephole LSTM\u3000\u8ffd\u52a0\nLSTM\u306eVariant\u3067\u3042\u308bPeephole LSTM\u304c\u8ffd\u52a0\u3055\u308c\u305f\uff0e\nPeepholeLSTM\u81ea\u4f53\u306e\u89e3\u8aac\u306f\u308f\u304b\u308bLSTM \uff5e \u6700\u8fd1\u306e\u52d5\u5411\u3068\u5171\u306b\n( http://qiita.com/t_Signull/items/21b82be280b46f467d1b )\u3067\uff0eDRYDRY\uff0e\n\nCupy\u306b\u95a2\u3059\u308b\u8ffd\u52a0\nfill_diagonal\u3068\u3044\u3046\uff0cnumpy.fill_diagonal\u76f8\u5f53\u306e\u95a2\u6570\u304c\u8ffd\u52a0\u3055\u308c\u305f\uff0e\n\u307e\u305f\uff0ccupy.core.core.scan\u306b\u95a2\u3059\u308b\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u5411\u4e0a\u304c\u884c\u308f\u308c\u305f\uff0e\n\u5b66\u3073\u304c\u3042\u308b\uff0e( https://github.com/pfnet/chainer/pull/1367/files )\nhttps://github.com/pfnet/chainer/pull/1367 \u3092\u898b\u308b\u3068\uff0c10\u500d\u8fd1\u304f\u9ad8\u901f\u5316\u3055\u308c\u305f\u306e\u3060\u308d\u3046\u304b\uff0e\n\n\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\n\u4ee5\u4e0b\u306e\u9805\u76ee\u306b\u3064\u3044\u3066\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u304c\u884c\u308f\u308c\u305f\uff0e\n- cupy.ndarray.view (dtype)\n- get_cifar10 , get_cifar100\n- ImageDataset\n- LogReport (on Windows)\n- ParallelUpdater (for dict mini-batch)\n- SerialIterator(dataset size\u304cbatchsize\u3067\u5272\u308a\u5207\u308c\u306a\u304b\u3063\u305f\u5834\u5408, shuffle\u3055\u308c\u306a\u3044)\nget_cifar10 , get_cifar100\u306e\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u306f\u3053\u308c\u304b\u3089Chainer\u3092\u89e6\u308b\u4eba\u306b\u3068\u3063\u3066\u91cd\u8981\u306aBug Fix\u3067\u3042\u308b\u3068\u601d\u3046\uff0e\nchainer.datasets.get_cifar10\u3068\u5531\u3048\u308b\u3068TupleDataset\u3067\u5305\u307e\u308c\u305f\u30c7\u30fc\u30bf\u3092\u624b\u306b\u5165\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\n\u305d\u306e\u4ed6\u306e\u5909\u66f4\u306b\u3064\u3044\u3066\u306f\uff0c\u666e\u6bb5\u304b\u3089Chainer\u3092\u89e6\u308b\u4eba\u306b\u3068\u3063\u3066\u3082\u91cd\u8981\u306a\u4fee\u6b63\u3067\u3042\u308b\u3068\u601d\u308f\u308c\u308b\uff0e\n\u307f\u3093\u306aUpdate\u3057\u3088\u3046\uff0e\n\n\u5404\u7a2e\u6539\u5584\nChain.zerograds\u306b\u95a2\u3059\u308b\u6539\u5584\u3068\uff0castype\u3092\u4f7f\u3063\u3066\u3044\u308b\u3044\u304f\u3064\u304b\u306e\u7b87\u6240\u3067\u306e\u30b3\u30d4\u30fc\u3092\u524a\u6e1b\u3057\u305f\uff0e\nhttps://github.com/pfnet/chainer/pull/1444/files \u3092\u8aad\u3080\u3053\u3068\u3067\uff0c\u81ea\u524d\u306e\u30b3\u30fc\u30c9\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u5411\u4e0a\u306b\u7e4b\u304c\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff0e\n\u5b66\u3073\u304c\u3042\u308b\uff0e\n\n\u4ee5\u4e0a\u3067\uff0cChainer v 1.13.0\u306b\u304a\u3051\u308b\u3059\u3079\u3066\u306e\u65b0\u6a5f\u80fd\u30fb\u5909\u66f4\u70b9\u306b\u3064\u3044\u3066\u306e\u89e3\u8aac\u304c\u7d42\u4e86\u3057\u305f\u306f\u305a\u3067\u3042\u308b\uff0e\n\u6691\u3044\u4e2d\uff0cChainer\u306e\u958b\u767a\u306b\u95a2\u308f\u3063\u3066\u4e0b\u3055\u3063\u305f\u3059\u3079\u3066\u306e\u65b9\u306b\u611f\u8b1d\u306e\u6c17\u6301\u3061\u3092\u4f1d\u3048\u305f\u3044\uff0e\n\u304a\u75b2\u308c\u30b5\u30de\u30fc\u3067\u3057\u305f\uff0e\n\n\u304a\u307e\u3051\nCRF1d\u3092\u7528\u3044\u305f\u718a\u672c\u5f01\u7ffb\u8a33\u30d7\u30ed\u30b0\u30e9\u30e0\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import optimizers\nimport numpy as np\n\ndef make_vocab_dict(vocab):\n    id2word = {}\n    word2id = {}\n    for id, word in enumerate(vocab):\n        id2word[id] = word\n        word2id[word] = id\n    return id2word, word2id\n\n\nclass Seq2Seq(chainer.Chain):\n    dropout_ratio = 0.5\n\n    def __init__(self, input_vocab, output_vocab, feature_num, hidden_num):\n        \"\"\"\n        :param input_vocab: array of input  vocab\n        :param output_vocab: array of output  vocab\n        :param feature_num: size of feature layer\n        :param hidden_num: size of hidden layer\n        :return:\n        \"\"\"\n        self.id2word_input, self.word2id_input = make_vocab_dict(input_vocab)\n        self.id2word_output, self.word2id_output = make_vocab_dict(output_vocab)\n        self.input_vocab_size = len(self.word2id_input)\n        self.output_vocab_size = len(self.word2id_output)\n\n        # CRF Layer\n        self.crf = L.CRF1d(n_label=4)\n\n        super(Seq2Seq, self).__init__(\n                # encoder\n                word_vec=L.EmbedID(self.input_vocab_size, feature_num),\n                input_vec=L.LSTM(feature_num, hidden_num),\n\n                # connect layer\n                context_lstm=L.LSTM(hidden_num, self.output_vocab_size),\n\n                # decoder\n                output_lstm=L.LSTM(self.output_vocab_size, self.output_vocab_size),\n                out_word=L.Linear(self.output_vocab_size, self.output_vocab_size),\n        )\n\n    def encode(self, src_text):\n        \"\"\"\n\n        :param src_text: input text embed id ex.) [ 1, 0 ,14 ,5 ]\n        :return: context vector\n        \"\"\"\n        for word in src_text:\n            word = chainer.Variable(np.array([[word]], dtype=np.int32))\n            embed_vector = F.tanh(self.word_vec(word))\n            input_feature = self.input_vec(embed_vector)\n            context = self.context_lstm(input_feature)\n\n        return context\n\n    def decode(self, context):\n        \"\"\"\n        :param context: context vector made `encode` function\n        :return: decoded embed vector ( Batchsize , ClassNum)\n        \"\"\"\n\n        return self.out_word(context)\n\n    def initialize(self):\n        \"\"\"\n        LSTM state initialize\n\n        :param image_feature:\n        :param train:\n        :return:\n        \"\"\"\n        self.input_vec.reset_state()\n        self.context_lstm.reset_state()\n        self.output_lstm.reset_state()\n\n    def generate(self, start_word_id, sentence_limit):\n        \"\"\"\n        \u6587\u7ae0\u751f\u6210\n        :param start_word_id:\n        :param sentence_limit:\n        :return:\n        \"\"\"\n\n        context = self.encode([start_word_id])\n        sentence = \"\"\n\n        for _ in range(sentence_limit):\n            context = self.decode(context)\n            word = self.id2word_output[np.argmax(context.data)]\n            if word == \"<eos>\":\n                break\n            sentence = sentence + word + \" \"\n        return sentence\n\n    def __call__(self, input_seq, teacher_seq):\n\n        \"\"\"\n\n        :param input_seq: list of\n        :param teacher_seq: list of valiable\n        :return:\n        \"\"\"\n\n        context = model.encode(input_seq)\n\n        # make list of Valiable\n        predict_seq = []\n        for _ in range(4):\n            context = self.decode(context)\n            predict_seq.append(context)\n\n        loss = self.crf(predict_seq, teacher_seq)\n\n        return loss\n\nif __name__ == \"__main__\":\n\n    input_vocab = [\"<start>\", u\"\u9ec4\u660f\u306b\", u\"\u5929\u4f7f\u306e\u58f0\", u\"\u97ff\u304f\u6642\uff0c\", u\"\u8056\u306a\u308b\u6cc9\u306e\u524d\u306b\u3066\", u\"\u5f85\u3064\", \"<eos>\"]\n    output_vocab = [u\"5\u6642\u306b\", u\"\u5674\u6c34\u306e\u524d\u3067\", u\"\u5f85\u3063\u3066\u307e\u3059\", \"<eos>\"]\n\n    model = Seq2Seq(input_vocab, output_vocab, feature_num=4, hidden_num=10)\n\n    # reverse jp\n    input = [model.word2id_input[word] for word in reversed(input_vocab)]\n\n    # list of Variable\n    teacher = []\n    for i in range(len(output_vocab)):\n        t = chainer.Variable(np.array([i], dtype=np.int32))\n        teacher.append(t)\n\n\n    optimizer = optimizers.RMSprop()\n    optimizer.setup(model)\n\n    for i in range(10):\n        # init\n        model.initialize()\n        loss = 0\n\n        # calc loss\n        loss = model(input, teacher)\n\n        # update\n        model.zerograds()\n        loss.backward()\n        loss.unchain_backward()\n        optimizer.update()\n\n        # \u9032\u6357\u78ba\u8a8d\n        start = model.word2id_input[\"<start>\"]\n        sentence = model.generate(start, 7)\n\n        print \"teacher : \", \"\".join(input_vocab[1:6])\n        print i , \") -> \", sentence\n        print\n\n\n\n\n\u590f\u4f11\u307f\u6700\u7d42\u65e5\uff0e\n\u30ec\u30b8\u306b\u7740\u3044\u305f\u30890.3\u79d2\u3067\u8cb7\u3048\u308b\u672c\u3092\u5165\u624b\u3057\uff0c\u308a\u3093\u304b\u3044\u7dda\u306b\u4e57\u3063\u305f\u79c1\u306f\u3075\u3068ChainerJP\u306e\u30c4\u30a4\u30fc\u30c8\u3092\u8aad\u3093\u3060\uff0e\n\u3069\u3046\u3084\u3089Chainer\u304cv 1.13.0\u306b\u306a\u3063\u305f\u3089\u3057\u3044\uff0e\n\u524d\u56de\u306e\u66f4\u65b0\u304b\u30892\u9031\u9593\u3067\u4ee5\u4e0b\u306e\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u305f\u3053\u3068\u306b\u306a\u308b\uff0e\n\u30ac\u30c3\u30ba\u30a3\u30fc\u30e9\u4e26\u307f\u306e\u66f4\u65b0\u983b\u5ea6\u3067\u3042\u308b\uff0e\n\n# Linear\u3068Convolution2D\u3067 input channel\u306e\u521d\u671f\u5316\u304c\u81ea\u52d5\u5316\u3055\u308c\u305f\n\n`in_size = None`\u3068\u3059\u308b\u3053\u3068\u3067\uff0c\u6700\u521d\u306b`__call__`\u304c\u547c\u3070\u308c\u305f\u969b\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5316\u304c\u884c\u308f\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n\u8a66\u884c\u932f\u8aa4\u3059\u308b\u969b\uff0c\u975e\u5e38\u306b\u4fbf\u5229\u3067\u3042\u308b\uff0e\nhttps://github.com/pfnet/chainer/blob/master/chainer/links/connection/linear.py\n\n```py\nclass MnistModel(chainer.Chain):\n    def __init__(self):\n        super(MnistModel,self).__init__(\n                l1 = L.Linear(out_size=100,in_size=None),\n                l2 = L.Linear(out_size=100,in_size=None),\n                l3 = L.Linear(out_size=10,in_size=None)\n        )\n\n    def __call__(self,x):\n        h = F.relu(self.l1(x))\n        h = F.relu(self.l2(h))\n        return self.l3(h)\n\n```\n\nLinear\u306e`__init__`\u304c\n\n```py\n__init__(self, in_size, out_size, wscale=1, (\u7565))\n```\n\n\u304b\u3089\n\n```py\n__init__(self, out_size, in_size = None,wscale=1, (\u7565))\n```\n\n\u306b\u306a\u308b\u3068\u3082\u3063\u3068\u5b09\u3057\u3044\u306e\u3060\u304c\uff0cLinear,Convolution2D\u4ee5\u5916\u306elinks\u3068\u306e\u6574\u5408\u6027\u3092\u4fdd\u3064\u305f\u3081\u306b\u3053\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3082\u306e\u3068\u601d\u308f\u308c\u308b\uff0e\nChainer\u306f\u4eba\u9593\u306b\u512a\u3057\u3044\uff0e\n\n# links\u306bCRF1d\u8ffd\u52a0\n\n\u3044\u308f\u3086\u308b\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u3092\u5b66\u7fd2\u3059\u308b\u305f\u3081\u306eLayer\u3067\uff0c\u7cfb\u5217\u30e9\u30d9\u30ea\u30f3\u30b0\u306a\u3069\u306b\u7528\u3044\u3089\u308c\u3066\u3044\u308b\uff0e\n( https://github.com/pfnet/chainer/issues/1020 )\n\n\u8ad6\u6587( http://arxiv.org/pdf/1508.01991v1.pdf )\u3067\u306f Part-Of-Speech Tagging, Chunking, Entity Recognition\u306b\u304a\u3044\u3066\u7cbe\u5ea6\u5411\u4e0a\u304c\u898b\u3089\u308c\u305f\u3068\u66f8\u304b\u308c\u3066\u3044\u308b\uff0e\n\n\u5b9f\u88c5\u3068\u3057\u3066\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u4ed8\u304d\u306eloss\u95a2\u6570\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3042\u308b\uff0e\n\n\u30d0\u30c3\u30c1\u30b5\u30a4\u30baN\uff0c\u9577\u3055L\uff0c\u30af\u30e9\u30b9\u6570K\u306e\u7cfb\u5217\u30e9\u30d9\u30ea\u30f3\u30b0\u554f\u984c\u306b\u5bfe\u3057\u3066\n`y = [ Variable(N,K) * L\u500b ]` , `t = [Variable(N) * L\u500b ]`\u3068\u3044\u3046\u5f62\u3067\u5165\u529b\u3059\u308b\u3068\u52d5\u4f5c\u3059\u308b\uff08List of Variable\uff09\n\n_\u304a\u307e\u3051_ \u306e\u90e8\u5206\u306bCRF1d\u3092\u7528\u3044\u305f\u718a\u672c\u5f01\u7ffb\u8a33\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u63b2\u8f09\u3057\u3066\u304a\u304f\uff0e\n\u4f55\u304b\u306e\u53c2\u8003\u306b\u306a\u308c\u3070\u5e78\u3044\u3067\u3042\u308b\uff0e\n\uff08 \u718a\u672c\u5f01\u7ffb\u8a33\u89e3\u8aac http://qiita.com/_329_/items/1d5de7b8213b112a3df7 \uff09\n\n# functions\u306bseparate\u8ffd\u52a0\n\nchainer.functions.stack\u306e\u9006\u306e\u52d5\u4f5c\u3092\u884c\u3046.\nhttp://docs.chainer.org/en/latest/_modules/chainer/functions/array/separate.html\n\n# Stateful Peephole LSTM\u3000\u8ffd\u52a0\n\nLSTM\u306eVariant\u3067\u3042\u308bPeephole LSTM\u304c\u8ffd\u52a0\u3055\u308c\u305f\uff0e\nPeepholeLSTM\u81ea\u4f53\u306e\u89e3\u8aac\u306f\u308f\u304b\u308bLSTM \uff5e \u6700\u8fd1\u306e\u52d5\u5411\u3068\u5171\u306b\n( http://qiita.com/t_Signull/items/21b82be280b46f467d1b )\u3067\uff0eDRYDRY\uff0e\n\n# Cupy\u306b\u95a2\u3059\u308b\u8ffd\u52a0\n`fill_diagonal`\u3068\u3044\u3046\uff0cnumpy.fill_diagonal\u76f8\u5f53\u306e\u95a2\u6570\u304c\u8ffd\u52a0\u3055\u308c\u305f\uff0e\n\u307e\u305f\uff0c`cupy.core.core.scan`\u306b\u95a2\u3059\u308b\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u5411\u4e0a\u304c\u884c\u308f\u308c\u305f\uff0e\n\u5b66\u3073\u304c\u3042\u308b\uff0e( https://github.com/pfnet/chainer/pull/1367/files )\nhttps://github.com/pfnet/chainer/pull/1367 \u3092\u898b\u308b\u3068\uff0c10\u500d\u8fd1\u304f\u9ad8\u901f\u5316\u3055\u308c\u305f\u306e\u3060\u308d\u3046\u304b\uff0e\n\n\n# \u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\n\n\u4ee5\u4e0b\u306e\u9805\u76ee\u306b\u3064\u3044\u3066\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u304c\u884c\u308f\u308c\u305f\uff0e\n- cupy.ndarray.view (dtype)\n- get_cifar10 , get_cifar100\n- ImageDataset\n- LogReport (on Windows)\n- ParallelUpdater (for dict mini-batch)\n- SerialIterator(dataset size\u304cbatchsize\u3067\u5272\u308a\u5207\u308c\u306a\u304b\u3063\u305f\u5834\u5408, shuffle\u3055\u308c\u306a\u3044)\n\n`get_cifar10` , `get_cifar100`\u306e\u30d0\u30b0\u30d5\u30a3\u30c3\u30af\u30b9\u306f\u3053\u308c\u304b\u3089Chainer\u3092\u89e6\u308b\u4eba\u306b\u3068\u3063\u3066\u91cd\u8981\u306aBug Fix\u3067\u3042\u308b\u3068\u601d\u3046\uff0e\n`chainer.datasets.get_cifar10`\u3068\u5531\u3048\u308b\u3068`TupleDataset`\u3067\u5305\u307e\u308c\u305f\u30c7\u30fc\u30bf\u3092\u624b\u306b\u5165\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\n\u305d\u306e\u4ed6\u306e\u5909\u66f4\u306b\u3064\u3044\u3066\u306f\uff0c\u666e\u6bb5\u304b\u3089Chainer\u3092\u89e6\u308b\u4eba\u306b\u3068\u3063\u3066\u3082\u91cd\u8981\u306a\u4fee\u6b63\u3067\u3042\u308b\u3068\u601d\u308f\u308c\u308b\uff0e\n\u307f\u3093\u306aUpdate\u3057\u3088\u3046\uff0e\n\n# \u5404\u7a2e\u6539\u5584\nChain.zerograds\u306b\u95a2\u3059\u308b\u6539\u5584\u3068\uff0castype\u3092\u4f7f\u3063\u3066\u3044\u308b\u3044\u304f\u3064\u304b\u306e\u7b87\u6240\u3067\u306e\u30b3\u30d4\u30fc\u3092\u524a\u6e1b\u3057\u305f\uff0e\nhttps://github.com/pfnet/chainer/pull/1444/files \u3092\u8aad\u3080\u3053\u3068\u3067\uff0c\u81ea\u524d\u306e\u30b3\u30fc\u30c9\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u5411\u4e0a\u306b\u7e4b\u304c\u308b\u304b\u3082\u3057\u308c\u306a\u3044\uff0e\n\u5b66\u3073\u304c\u3042\u308b\uff0e\n\n---\n\n\u4ee5\u4e0a\u3067\uff0cChainer v 1.13.0\u306b\u304a\u3051\u308b\u3059\u3079\u3066\u306e\u65b0\u6a5f\u80fd\u30fb\u5909\u66f4\u70b9\u306b\u3064\u3044\u3066\u306e\u89e3\u8aac\u304c\u7d42\u4e86\u3057\u305f\u306f\u305a\u3067\u3042\u308b\uff0e\n\n\u6691\u3044\u4e2d\uff0cChainer\u306e\u958b\u767a\u306b\u95a2\u308f\u3063\u3066\u4e0b\u3055\u3063\u305f\u3059\u3079\u3066\u306e\u65b9\u306b\u611f\u8b1d\u306e\u6c17\u6301\u3061\u3092\u4f1d\u3048\u305f\u3044\uff0e\n\u304a\u75b2\u308c__\u30b5\u30de\u30fc__\u3067\u3057\u305f\uff0e\n\n\n\n# \u304a\u307e\u3051\n\nCRF1d\u3092\u7528\u3044\u305f\u718a\u672c\u5f01\u7ffb\u8a33\u30d7\u30ed\u30b0\u30e9\u30e0\n\n```py\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import optimizers\nimport numpy as np\n\ndef make_vocab_dict(vocab):\n    id2word = {}\n    word2id = {}\n    for id, word in enumerate(vocab):\n        id2word[id] = word\n        word2id[word] = id\n    return id2word, word2id\n\n\nclass Seq2Seq(chainer.Chain):\n    dropout_ratio = 0.5\n\n    def __init__(self, input_vocab, output_vocab, feature_num, hidden_num):\n        \"\"\"\n        :param input_vocab: array of input  vocab\n        :param output_vocab: array of output  vocab\n        :param feature_num: size of feature layer\n        :param hidden_num: size of hidden layer\n        :return:\n        \"\"\"\n        self.id2word_input, self.word2id_input = make_vocab_dict(input_vocab)\n        self.id2word_output, self.word2id_output = make_vocab_dict(output_vocab)\n        self.input_vocab_size = len(self.word2id_input)\n        self.output_vocab_size = len(self.word2id_output)\n\n        # CRF Layer\n        self.crf = L.CRF1d(n_label=4)\n\n        super(Seq2Seq, self).__init__(\n                # encoder\n                word_vec=L.EmbedID(self.input_vocab_size, feature_num),\n                input_vec=L.LSTM(feature_num, hidden_num),\n\n                # connect layer\n                context_lstm=L.LSTM(hidden_num, self.output_vocab_size),\n\n                # decoder\n                output_lstm=L.LSTM(self.output_vocab_size, self.output_vocab_size),\n                out_word=L.Linear(self.output_vocab_size, self.output_vocab_size),\n        )\n\n    def encode(self, src_text):\n        \"\"\"\n\n        :param src_text: input text embed id ex.) [ 1, 0 ,14 ,5 ]\n        :return: context vector\n        \"\"\"\n        for word in src_text:\n            word = chainer.Variable(np.array([[word]], dtype=np.int32))\n            embed_vector = F.tanh(self.word_vec(word))\n            input_feature = self.input_vec(embed_vector)\n            context = self.context_lstm(input_feature)\n\n        return context\n\n    def decode(self, context):\n        \"\"\"\n        :param context: context vector made `encode` function\n        :return: decoded embed vector ( Batchsize , ClassNum)\n        \"\"\"\n\n        return self.out_word(context)\n\n    def initialize(self):\n        \"\"\"\n        LSTM state initialize\n\n        :param image_feature:\n        :param train:\n        :return:\n        \"\"\"\n        self.input_vec.reset_state()\n        self.context_lstm.reset_state()\n        self.output_lstm.reset_state()\n\n    def generate(self, start_word_id, sentence_limit):\n        \"\"\"\n        \u6587\u7ae0\u751f\u6210\n        :param start_word_id:\n        :param sentence_limit:\n        :return:\n        \"\"\"\n\n        context = self.encode([start_word_id])\n        sentence = \"\"\n\n        for _ in range(sentence_limit):\n            context = self.decode(context)\n            word = self.id2word_output[np.argmax(context.data)]\n            if word == \"<eos>\":\n                break\n            sentence = sentence + word + \" \"\n        return sentence\n\n    def __call__(self, input_seq, teacher_seq):\n\n        \"\"\"\n\n        :param input_seq: list of\n        :param teacher_seq: list of valiable\n        :return:\n        \"\"\"\n\n        context = model.encode(input_seq)\n\n        # make list of Valiable\n        predict_seq = []\n        for _ in range(4):\n            context = self.decode(context)\n            predict_seq.append(context)\n\n        loss = self.crf(predict_seq, teacher_seq)\n\n        return loss\n\nif __name__ == \"__main__\":\n\n    input_vocab = [\"<start>\", u\"\u9ec4\u660f\u306b\", u\"\u5929\u4f7f\u306e\u58f0\", u\"\u97ff\u304f\u6642\uff0c\", u\"\u8056\u306a\u308b\u6cc9\u306e\u524d\u306b\u3066\", u\"\u5f85\u3064\", \"<eos>\"]\n    output_vocab = [u\"5\u6642\u306b\", u\"\u5674\u6c34\u306e\u524d\u3067\", u\"\u5f85\u3063\u3066\u307e\u3059\", \"<eos>\"]\n\n    model = Seq2Seq(input_vocab, output_vocab, feature_num=4, hidden_num=10)\n\n    # reverse jp\n    input = [model.word2id_input[word] for word in reversed(input_vocab)]\n\n    # list of Variable\n    teacher = []\n    for i in range(len(output_vocab)):\n        t = chainer.Variable(np.array([i], dtype=np.int32))\n        teacher.append(t)\n\n\n    optimizer = optimizers.RMSprop()\n    optimizer.setup(model)\n\n    for i in range(10):\n        # init\n        model.initialize()\n        loss = 0\n\n        # calc loss\n        loss = model(input, teacher)\n\n        # update\n        model.zerograds()\n        loss.backward()\n        loss.unchain_backward()\n        optimizer.update()\n\n        # \u9032\u6357\u78ba\u8a8d\n        start = model.word2id_input[\"<start>\"]\n        sentence = model.generate(start, 7)\n\n        print \"teacher : \", \"\".join(input_vocab[1:6])\n        print i , \") -> \", sentence\n        print\n\n\n\n```\n\n\n"}