{"context": "\u5b9f\u88c5\u306f\u3053\u3061\u3089\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306ednn/optimizers.py\u4e2d\u306b\u3042\u308a\u307e\u3059\uff0e\n\n\u306f\u3058\u3081\u306b\nMNIST\u3092\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u3057\u3066\u7c21\u5358\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u4ee5\u4e0b\u306e\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\uff0e\u305d\u308c\u305e\u308c\u306e\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3059\u308baccuracy\u3092\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\uff0e\n\nSGD\nMomentum SGD\nAdaGrad\nRMSprop\nAdaDelta\nAdam\n\n\n\n\u5b9f\u88c5\n\u5b9f\u88c5\u306fTheano\u3092\u4f7f\u3044\u307e\u3057\u305f\uff0e\n\u4eca\u56de\u306f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u79c1\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\uff0e\n\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u306b\u95a2\u3057\u3066\u306f\u3053\u3053\u3089\u8fba\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\uff0e\nhttps://gist.github.com/SnippyHolloW/67effa81dd1cd5a488b4\nhttps://gist.github.com/skaae/ae7225263ca8806868cb\nhttp://chainer.readthedocs.org/en/stable/reference/optimizers.html?highlight=optimizers\nhttp://qiita.com/skitaoka/items/e6afbe238cd69c899b2a\n\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306fparams(or self.params)\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5168\u4f53\u306e\u91cd\u307f\uff0c\u30d0\u30a4\u30a2\u30b9\u3092\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\uff0e\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u5b66\u7fd2\u3059\u308b\u306e\u3067\uff0c\u8aa4\u5dee\u95a2\u6570\u5024loss\u3092params\u3067\u5fae\u5206\u3057\u305f\u5024\u304c\u5fc5\u8981\u3067\u3059\u304c\uff0cgparams(or self.gparams)\u304c\u305d\u308c\u306b\u5f53\u305f\u308a\u307e\u3059\uff0e(T.grad(loss, param)\u3067\u5fae\u5206\u3057\u3066\u3044\u307e\u3059\uff0e)\u3000\u5fae\u5206\u3057\u3066\u50be\u304d\u3092\u6c42\u3081\u308b\u3068\u3053\u308d\u307e\u3067\u3092Optimizer\u30af\u30e9\u30b9\u3068\u3057\u3066\uff0c\u305d\u308c\u3092\u7d99\u627f\u3057\u3066SGD\uff0cMomentum SGD\u306a\u3069\u3092\u5b9f\u88c5\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\noptimizers.py\nclass Optimizer(object):\n    def __init__(self, params=None):\n        if params is None:\n            return NotImplementedError()\n        self.params = params\n\n    def updates(self, loss=None):\n        if loss is None:\n            return NotImplementedError()\n\n        self.updates = OrderedDict()\n        self.gparams = [T.grad(loss, param) for param in self.params]\n\n\n\u3061\u306a\u307f\u306b\u3053\u3053\u3067\u306eself.updates\u306f\u91cd\u307f\u306a\u3069\u3092\u66f4\u65b0\u3059\u308b\u3068\u304d\u306b\u4f7f\u3044\u307e\u3059\uff0e\n\nSGD\n\noptimizers.py\nclass SGD(Optimizer):\n    def __init__(self, learning_rate=0.01, params=None):\n        super(SGD, self).__init__(params=params)\n        self.learning_rate = 0.01\n\n    def updates(self, loss=None):\n        super(SGD, self).updates(loss=loss)\n\n        for param, gparam in zip(self.params, self.gparams):\n            self.updates[param] = param - self.learning_rate * gparam\n\n        return self.updates \n\n\n\nMomentum SGD\n\noptimizers.py\nclass MomentumSGD(Optimizer):\n    def __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n        super(MomentumSGD, self).__init__(params=params)\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n    def updates(self, loss=None):\n        super(MomentumSGD, self).updates(loss=loss)\n\n        for v, param, gparam in zip(self.vs, self.params, self.gparams):\n            _v = v * self.momentum\n            _v = _v - self.learning_rate * gparam\n            self.updates[param] = param + _v\n            self.updates[v] = _v\n\n        return self.updates\n\n\n\nAdaGrad\n\noptimizers.py\nclass AdaGrad(Optimizer):\n    def __init__(self, learning_rate=0.01, eps=1e-6, params=None):\n        super(AdaGrad, self).__init__(params=params)\n\n        self.learning_rate = learning_rate\n        self.eps = eps\n        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n\n    def updates(self, loss=None):\n        super(AdaGrad, self).updates(loss=loss)\n\n        for accugrad, param, gparam\\\n        in zip(self.accugrads, self.params, self.gparams):\n            agrad = accugrad + gparam * gparam\n            dx = - (self.learning_rate / T.sqrt(agrad + self.eps)) * gparam\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n\n\n\nRMSprop\n\noptimizers.py\nclass RMSprop(Optimizer):\n    def __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n        super(RMSprop, self).__init__(params=params)\n\n        self.learning_rate = learning_rate\n        self.alpha = alpha\n        self.eps = eps\n\n        self.mss = [build_shared_zeros(t.shape.eval(),'ms') for t in self.params]\n\n    def updates(self, loss=None):\n        super(RMSprop, self).updates(loss=loss)\n\n        for ms, param, gparam in zip(self.mss, self.params, self.gparams):\n            _ms = ms*self.alpha\n            _ms += (1 - self.alpha) * gparam * gparam\n            self.updates[ms] = _ms\n            self.updates[param] = param - self.learning_rate * gparam / T.sqrt(_ms + self.eps)\n\n        return self.updates\n\n\n\nAdaDelta\n\noptimizers.py\nclass AdaDelta(Optimizer):\n    def __init__(self, rho=0.95, eps=1e-6, params=None):\n        super(AdaDelta, self).__init__(params=params)\n\n        self.rho = rho\n        self.eps = eps\n        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n        self.accudeltas = [build_shared_zeros(t.shape.eval(),'accudelta') for t in self.params]\n\n    def updates(self, loss=None):\n        super(AdaDelta, self).updates(loss=loss)\n\n        for accugrad, accudelta, param, gparam\\\n        in zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n            agrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n            dx = - T.sqrt((accudelta + self.eps)/(agrad + self.eps)) * gparam\n            self.updates[accudelta] = (self.rho*accudelta + (1 - self.rho) * dx * dx)\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n\n\n\nAdam\n\noptimizers.py\nclass Adam(Optimizer):\n    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1-1e-8, params=None):\n        super(Adam, self).__init__(params=params)\n\n        self.alpha = alpha\n        self.b1 = beta1\n        self.b2 = beta2\n        self.gamma = gamma\n        self.t = theano.shared(np.float32(1))\n        self.eps = eps\n\n        self.ms = [build_shared_zeros(t.shape.eval(), 'm') for t in self.params]\n        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n    def updates(self, loss=None):\n        super(Adam, self).updates(loss=loss)\n        self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n\n        for m, v, param, gparam \\\n        in zip(self.ms, self.vs, self.params, self.gparams):\n            _m = self.b1_t * m + (1 - self.b1_t) * gparam\n            _v = self.b2 * v + (1 - self.b2) * gparam ** 2\n\n            m_hat = _m / (1 - self.b1 ** self.t)\n            v_hat = _v / (1 - self.b2 ** self.t)\n\n            self.updates[param] = param - self.alpha*m_hat / (T.sqrt(v_hat) + self.eps)\n            self.updates[m] = _m\n            self.updates[v] = _v\n        self.updates[self.t] = self.t + 1.0\n\n        return self.updates\n\n\n\n\u5b9f\u9a13\nMNIST\u3092\u4f7f\u3063\u306620epoch\u305a\u306430\u30b7\u30fc\u30c9\u5e73\u5747\u3068\u308a\u307e\u3057\u305f\uff0e\u5b66\u7fd2\u4fc2\u6570\u3084\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7d30\u304b\u3044\u8a2d\u5b9a\u306b\u95a2\u3057\u3066\u306f\u79c1\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\n\n\u7d50\u679c\n\n\u3046\u3063\uff0c\u4e0a\u306e\u65b9\u304c\u3054\u3061\u3083\u3054\u3061\u3083\u3057\u3066\u3066\u308f\u304b\u3089\u306a\u3044\uff0c\u62e1\u5927\u3057\u3088\u3046\uff0e\n\nSGD\u3055\u3093\u304c\u6d88\u3048\u3066\u3057\u307e\u3063\u305f\uff0e\n\n\u304a\u308f\u308a\u306b\n\u8aa4\u5dee\u95a2\u6570\u5024\u3082\u3068\u3063\u3066\u304a\u3051\u3070\u3088\u304b\u3063\u305f\uff0e\uff0e\uff0e\uff0e\n\u4eca\u56de\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3084\u3089Stacked Denoising Autoencoders\u3084\u3089\u4eca\u5f8c\u8ffd\u52a0\u3057\u3066\u3044\u304f\u4e88\u5b9a\u3067\u3059\uff0e\n\u5909\u306a\u3068\u3053\u308d\u3068\u304b\u3042\u308a\u307e\u3057\u305f\u3089\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\uff0e\n\u5b9f\u88c5\u306f**[\u3053\u3061\u3089\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/hogefugabar/deep-learning-theano)**\u306e`dnn/optimizers.py`\u4e2d\u306b\u3042\u308a\u307e\u3059\uff0e\n\n#\u306f\u3058\u3081\u306b\nMNIST\u3092\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u3057\u3066\u7c21\u5358\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u4ee5\u4e0b\u306e\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\uff0e\u305d\u308c\u305e\u308c\u306e\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3059\u308baccuracy\u3092\u6bd4\u8f03\u3057\u3066\u307f\u307e\u3059\uff0e\n\n- SGD\n- Momentum SGD\n- AdaGrad\n- RMSprop\n- AdaDelta\n- Adam  \n  \n  \n#\u5b9f\u88c5\n\u5b9f\u88c5\u306f[Theano](http://deeplearning.net/software/theano/)\u3092\u4f7f\u3044\u307e\u3057\u305f\uff0e  \n\u4eca\u56de\u306f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092[\u79c1\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/hogefugabar/deep-learning-theano)\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\uff0e\n\n\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u306b\u95a2\u3057\u3066\u306f\u3053\u3053\u3089\u8fba\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\uff0e\nhttps://gist.github.com/SnippyHolloW/67effa81dd1cd5a488b4\nhttps://gist.github.com/skaae/ae7225263ca8806868cb\nhttp://chainer.readthedocs.org/en/stable/reference/optimizers.html?highlight=optimizers\nhttp://qiita.com/skitaoka/items/e6afbe238cd69c899b2a\n\n  \n\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f```params```(or ```self.params```)\u304c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u5168\u4f53\u306e\u91cd\u307f\uff0c\u30d0\u30a4\u30a2\u30b9\u3092\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\uff0e\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u5b66\u7fd2\u3059\u308b\u306e\u3067\uff0c\u8aa4\u5dee\u95a2\u6570\u5024```loss```\u3092```params```\u3067\u5fae\u5206\u3057\u305f\u5024\u304c\u5fc5\u8981\u3067\u3059\u304c\uff0c```gparams```(or ```self.gparams```)\u304c\u305d\u308c\u306b\u5f53\u305f\u308a\u307e\u3059\uff0e(```T.grad(loss, param)```\u3067\u5fae\u5206\u3057\u3066\u3044\u307e\u3059\uff0e)\u3000\u5fae\u5206\u3057\u3066\u50be\u304d\u3092\u6c42\u3081\u308b\u3068\u3053\u308d\u307e\u3067\u3092Optimizer\u30af\u30e9\u30b9\u3068\u3057\u3066\uff0c\u305d\u308c\u3092\u7d99\u627f\u3057\u3066SGD\uff0cMomentum SGD\u306a\u3069\u3092\u5b9f\u88c5\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\n```Python:optimizers.py\nclass Optimizer(object):\n\tdef __init__(self, params=None):\n\t\tif params is None:\n\t\t\treturn NotImplementedError()\n\t\tself.params = params\n\n\tdef updates(self, loss=None):\n\t\tif loss is None:\n\t\t\treturn NotImplementedError()\n\n\t\tself.updates = OrderedDict()\n\t\tself.gparams = [T.grad(loss, param) for param in self.params]\n```\n\u3061\u306a\u307f\u306b\u3053\u3053\u3067\u306e```self.updates```\u306f\u91cd\u307f\u306a\u3069\u3092\u66f4\u65b0\u3059\u308b\u3068\u304d\u306b\u4f7f\u3044\u307e\u3059\uff0e\n\n\n\n##SGD\n\n```Python:optimizers.py\nclass SGD(Optimizer):\n\tdef __init__(self, learning_rate=0.01, params=None):\n\t\tsuper(SGD, self).__init__(params=params)\n\t\tself.learning_rate = 0.01\n\n\tdef updates(self, loss=None):\n\t\tsuper(SGD, self).updates(loss=loss)\n\n\t\tfor param, gparam in zip(self.params, self.gparams):\n\t\t\tself.updates[param] = param - self.learning_rate * gparam\n\n\t\treturn self.updates\t\n```\n\n\n##Momentum SGD\n\n```Python:optimizers.py\nclass MomentumSGD(Optimizer):\n\tdef __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n\t\tsuper(MomentumSGD, self).__init__(params=params)\n\t\tself.learning_rate = learning_rate\n\t\tself.momentum = momentum\n\t\tself.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n\tdef updates(self, loss=None):\n\t\tsuper(MomentumSGD, self).updates(loss=loss)\n\n\t\tfor v, param, gparam in zip(self.vs, self.params, self.gparams):\n\t\t\t_v = v * self.momentum\n\t\t\t_v = _v - self.learning_rate * gparam\n\t\t\tself.updates[param] = param + _v\n\t\t\tself.updates[v] = _v\n\n\t\treturn self.updates\n```\n\n\n##AdaGrad\n\n```Python:optimizers.py\nclass AdaGrad(Optimizer):\n\tdef __init__(self, learning_rate=0.01, eps=1e-6, params=None):\n\t\tsuper(AdaGrad, self).__init__(params=params)\n\n\t\tself.learning_rate = learning_rate\n\t\tself.eps = eps\n\t\tself.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n\n\tdef updates(self, loss=None):\n\t\tsuper(AdaGrad, self).updates(loss=loss)\n\n\t\tfor accugrad, param, gparam\\\n\t\tin zip(self.accugrads, self.params, self.gparams):\n\t\t\tagrad = accugrad + gparam * gparam\n\t\t\tdx = - (self.learning_rate / T.sqrt(agrad + self.eps)) * gparam\n\t\t\tself.updates[param] = param + dx\n\t\t\tself.updates[accugrad] = agrad\n\n\t\treturn self.updates\n```\n\n##RMSprop\n\n```Python:optimizers.py\nclass RMSprop(Optimizer):\n\tdef __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n\t\tsuper(RMSprop, self).__init__(params=params)\n\n\t\tself.learning_rate = learning_rate\n\t\tself.alpha = alpha\n\t\tself.eps = eps\n\n\t\tself.mss = [build_shared_zeros(t.shape.eval(),'ms') for t in self.params]\n\n\tdef updates(self, loss=None):\n\t\tsuper(RMSprop, self).updates(loss=loss)\n\n\t\tfor ms, param, gparam in zip(self.mss, self.params, self.gparams):\n\t\t\t_ms = ms*self.alpha\n\t\t\t_ms += (1 - self.alpha) * gparam * gparam\n\t\t\tself.updates[ms] = _ms\n\t\t\tself.updates[param] = param - self.learning_rate * gparam / T.sqrt(_ms + self.eps)\n\n\t\treturn self.updates\n```\n\n##AdaDelta\n\n```Python:optimizers.py\nclass AdaDelta(Optimizer):\n\tdef __init__(self, rho=0.95, eps=1e-6, params=None):\n\t\tsuper(AdaDelta, self).__init__(params=params)\n\n\t\tself.rho = rho\n\t\tself.eps = eps\n\t\tself.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n\t\tself.accudeltas = [build_shared_zeros(t.shape.eval(),'accudelta') for t in self.params]\n\n\tdef updates(self, loss=None):\n\t\tsuper(AdaDelta, self).updates(loss=loss)\n\n\t\tfor accugrad, accudelta, param, gparam\\\n\t\tin zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n\t\t\tagrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n\t\t\tdx = - T.sqrt((accudelta + self.eps)/(agrad + self.eps)) * gparam\n\t\t\tself.updates[accudelta] = (self.rho*accudelta + (1 - self.rho) * dx * dx)\n\t\t\tself.updates[param] = param + dx\n\t\t\tself.updates[accugrad] = agrad\n\n\t\treturn self.updates\n```\n\n\n\n##Adam\n\n```Python:optimizers.py\nclass Adam(Optimizer):\n\tdef __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1-1e-8, params=None):\n\t\tsuper(Adam, self).__init__(params=params)\n\n\t\tself.alpha = alpha\n\t\tself.b1 = beta1\n\t\tself.b2 = beta2\n\t\tself.gamma = gamma\n\t\tself.t = theano.shared(np.float32(1))\n\t\tself.eps = eps\n\n\t\tself.ms = [build_shared_zeros(t.shape.eval(), 'm') for t in self.params]\n\t\tself.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n\tdef updates(self, loss=None):\n\t\tsuper(Adam, self).updates(loss=loss)\n\t\tself.b1_t = self.b1 * self.gamma ** (self.t - 1)\n\n\t\tfor m, v, param, gparam \\\n\t\tin zip(self.ms, self.vs, self.params, self.gparams):\n\t\t\t_m = self.b1_t * m + (1 - self.b1_t) * gparam\n\t\t\t_v = self.b2 * v + (1 - self.b2) * gparam ** 2\n\n\t\t\tm_hat = _m / (1 - self.b1 ** self.t)\n\t\t\tv_hat = _v / (1 - self.b2 ** self.t)\n\n\t\t\tself.updates[param] = param - self.alpha*m_hat / (T.sqrt(v_hat) + self.eps)\n\t\t\tself.updates[m] = _m\n\t\t\tself.updates[v] = _v\n\t\tself.updates[self.t] = self.t + 1.0\n\n\t\treturn self.updates\n```\n\n\n#\u5b9f\u9a13\nMNIST\u3092\u4f7f\u3063\u306620epoch\u305a\u306430\u30b7\u30fc\u30c9\u5e73\u5747\u3068\u308a\u307e\u3057\u305f\uff0e\u5b66\u7fd2\u4fc2\u6570\u3084\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u7d30\u304b\u3044\u8a2d\u5b9a\u306b\u95a2\u3057\u3066\u306f[\u79c1\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/hogefugabar/deep-learning-theano)\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\uff0e\n\n\n\n#\u7d50\u679c\n<img src=\"https://dl.dropboxusercontent.com/u/38631959/figure_1.png\" width=700>\n\n\u3046\u3063\uff0c\u4e0a\u306e\u65b9\u304c\u3054\u3061\u3083\u3054\u3061\u3083\u3057\u3066\u3066\u308f\u304b\u3089\u306a\u3044\uff0c\u62e1\u5927\u3057\u3088\u3046\uff0e\n\n<img src=\"https://dl.dropboxusercontent.com/u/38631959/figure_2.png\" width=700>\n\nSGD\u3055\u3093\u304c\u6d88\u3048\u3066\u3057\u307e\u3063\u305f\uff0e\n\n#\u304a\u308f\u308a\u306b\n\u8aa4\u5dee\u95a2\u6570\u5024\u3082\u3068\u3063\u3066\u304a\u3051\u3070\u3088\u304b\u3063\u305f\uff0e\uff0e\uff0e\uff0e\n\n\n\n\n[\u4eca\u56de\u306eGitHub\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/hogefugabar/deep-learning-theano)\u306b\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3084\u3089Stacked Denoising Autoencoders\u3084\u3089\u4eca\u5f8c\u8ffd\u52a0\u3057\u3066\u3044\u304f\u4e88\u5b9a\u3067\u3059\uff0e\n\n\u5909\u306a\u3068\u3053\u308d\u3068\u304b\u3042\u308a\u307e\u3057\u305f\u3089\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\uff0e\n", "tags": ["\u6a5f\u68b0\u5b66\u7fd2", "Python", "Theano", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2"]}