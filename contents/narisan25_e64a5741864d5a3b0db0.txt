{"context": "\u521d\u3081\u3066\u306eQiita\u6295\u7a3f\u3067\u3059\u3002Ridge-i\u3068\u3044\u3046\u4f1a\u793e\u3067\u6a5f\u68b0\u5b66\u7fd2\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u30b3\u30f3\u30b5\u30eb\uff5e\u958b\u767a\u3092\u3057\u3066\u307e\u3059\u3002\n\u5f37\u5316\u5b66\u7fd2\u306b\u3064\u3044\u3066\u6559\u3048\u308b\u6a5f\u4f1a\u304c\u51fa\u3066\u304d\u305f\u306e\u3067\u3001\u4e09\u76ee\u4e26\u3079\u3092\u30d9\u30fc\u30b9\u306b\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\nQ-Learning\nDeep Q Network (\u3044\u308f\u3086\u308bDQN)\n\n\u306b\u3064\u3044\u3066Jupyter\uff08ipython) \u3067\u5b9f\u88c5\u3057\u3066\u6559\u6750\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002\n\u3061\u306a\u307f\u306b\u5f37\u3044\u30d7\u30ec\u30fc\u30e4\u30fc\u540c\u58eb\u306a\u3089\u3070\u3001\u30c9\u30ed\u30fc\u3060\u3051\u304c\u7e70\u308a\u8fd4\u3055\u308c\u308b\u306f\u305a\u3067\u3059\u3002\uff08WarGame\u306e\u6709\u540d\u306a\u3084\u3064\u3067\u3059\u306d\u3002\uff09\n\u7d50\u8ad6\u3068\u3057\u3066\u306f\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u3000\u5b9f\u88c5\u7c21\u5358\u3002100\u56de\u8a66\u884c\u4f4d\u3067\u307b\u307c\u8ca0\u3051\u306a\u3057\uff0850\u56de\u304f\u3089\u3044\u3060\u3068\u6642\u3005\u8ca0\u3051\u308b\uff09\nQ-Learning \u3000\u66f4\u65b0\u5f0f\u306e\u8a2d\u8a08\u306b\u6c17\u3092\u9063\u3046\u3002\uff13\u76ee\u4e26\u3079\u7a0b\u5ea6\u306a\u308910\u4e07\u5bfe\u6226\u3067\u6700\u5f37\nDeep Q Network \u8272\u3005\u306a\u843d\u3068\u3057\u7a74\u304c\u591a\u6570\u3002\u6700\u9069\u306a\u30c8\u30dd\u30ed\u30b8\u30fc\u304c\u308f\u304b\u3089\u3093\u3002Leaky Relu\u306b\u3059\u308b\u307e\u3067\u6700\u5f31\u3002\u6559\u3048\u65b9\u30df\u30b9\u308b\u3068\u4f55\u3082\u5b66\u7fd2\u3057\u306a\u3044\u3002\u306a\u3069\u306a\u3069\n\nQ-Learning\u307e\u3067\u306e\u5b9f\u88c5\u306f1-2\u6642\u9593\u4f4d\u3057\u304b\u304b\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u306b\u3001DQN\u304c\u304d\u3061\u3093\u3068\u5b66\u7fd2\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u3060\u3051\u30676\u6642\u9593\u304f\u3089\u3044\u304b\u304b\u308a\u307e\u3057\u305f\u3002(\u6ce3\uff09\n\u5b66\u7fd2\u306e\u52b9\u7387\u5316\u3068\u3044\u3046\u9762\u3067\u306f\u304d\u3061\u3093\u3068\u76e4\u9762\u306e\u53cd\u8ee2\u3068\u56de\u8ee2\u3092\u3059\u308b\u3068\u3088\u3044\u306e\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u5206\u304b\u308a\u3084\u3059\u3055\u91cd\u8996\u3067\u629c\u304d\u307e\u3057\u305f\u3002\n\u3042\u3068\u3001\u304d\u3061\u3093\u3068\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u8a2d\u8a08\u3068\u5909\u6570\u306e\u96a0\u533f\u5316\u3092\u3057\u306a\u304b\u3063\u305f\u306e\u3067\u6642\u3005\u9055\u53cd\u3057\u3066\u307e\u3059\u3002\u305d\u306e\u3042\u305f\u308a\u306f\u7121\u8996\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nGithub\u306f\u3053\u3061\u3089\u3002\nhttps://github.com/narisan25/TTT-RL\n\u7406\u8ad6\u7684\u306a\u8a73\u7d30\u3092\u77e5\u308a\u305f\u3044\u5834\u5408\u306f\u5225\u9014\u9023\u7d61\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u6307\u6458\u30fb\u6539\u826f\u30fb\u30a2\u30c9\u30d0\u30a4\u30b9\u3001\u5927Welcome\u3067\u3059\u3002\n\n\u4e09\u76ee\u4e26\u3079\u306e\u8a2d\u8a08\n\u3053\u308c\u306f\u3056\u3056\u3063\u3068\u3084\u308a\u307e\u3059\u3002\u826f\u304f\u3059\u308b\u3079\u304d\u70b9\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u5206\u304b\u308a\u3084\u3059\u3055\u3068\u65e9\u3055\uff08\u52d5\u3051\u3070\u3044\u3044\uff09\u3092\u91cd\u8996\n\n\u76e4\u9762\n0-9\u306e\u914d\u5217\u306bX\u306a\u30891\u3001\u30d6\u30e9\u30f3\u30af\u306a\u30890\u3001\u25cb\u306a\u3089-1\u3092\u5165\u308c\u307e\u3059\u3002\nEMPTY=0\nPLAYER_X=1\nPLAYER_O=-1\nMARKS={PLAYER_X:\"X\",PLAYER_O:\"O\",EMPTY:\" \"}\nDRAW=2\n\nclass TTTBoard:\n\n    def __init__(self,board=None):\n        if board==None:\n            self.board = []\n            for i in range(9):self.board.append(EMPTY)\n        else:\n            self.board=board\n        self.winner=None\n\n    def get_possible_pos(self):\n        pos=[]\n        for i in range(9):\n            if self.board[i]==EMPTY:\n                pos.append(i)\n        return pos\n\n    def print_board(self):\n        tempboard=[]\n        for i in self.board:\n            tempboard.append(MARKS[i])\n        row = ' {} | {} | {} '\n        hr = '\\n-----------\\n'\n        print((row + hr + row + hr + row).format(*tempboard))\n\n\n\n    def check_winner(self):\n        win_cond = ((1,2,3),(4,5,6),(7,8,9),(1,4,7),(2,5,8),(3,6,9),(1,5,9),(3,5,7))\n        for each in win_cond:\n            if self.board[each[0]-1] == self.board[each[1]-1]  == self.board[each[2]-1]:\n                if self.board[each[0]-1]!=EMPTY:\n                    self.winner=self.board[each[0]-1]\n                    return self.winner\n        return None\n\n    def check_draw(self):\n        if len(self.get_possible_pos())==0 and self.winner is None:\n            self.winner=DRAW\n            return DRAW\n        return None\n\n    def move(self,pos,player):\n        if self.board[pos]== EMPTY:\n            self.board[pos]=player\n        else:\n            self.winner=-1*player\n        self.check_winner()\n        self.check_draw()\n\n    def clone(self):\n        return TTTBoard(self.board.copy())\n\n    def switch_player(self):\n        if self.player_turn == self.player_x:\n            self.player_turn=self.player_o\n        else:\n            self.player_turn=self.player_x\n\n\n\u30b2\u30fc\u30e0\u306e\u9032\u884c\u5f79\nObserver+Mediator\u30e2\u30c7\u30eb\u306b\u3057\u307e\u3059\u3002\u3053\u308c\u3067Player\u304c\u76f4\u63a5\u76e4\u9762\u3092\u3044\u3058\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3064\u3064\u3001\u30bf\u30fc\u30f3\u30fb\u52dd\u6557\u30fb\u8868\u793a\u306e\u7ba1\u7406\u306a\u3069\u3092\u884c\u3044\u307e\u3059\u3002\nclass TTT_GameOrganizer:\n\nact_turn=0\nwinner=None\n\n    def __init__(self,px,po,nplay=1,showBoard=True,showResult=True,stat=100):\n        self.player_x=px\n        self.player_o=po\n        self.nwon={px.myturn:0,po.myturn:0,DRAW:0}\n        self.nplay=nplay\n        self.players=(self.player_x,self.player_o)\n        self.board=None\n        self.disp=showBoard\n        self.showResult=showResult\n        self.player_turn=self.players[random.randrange(2)]\n        self.nplayed=0\n        self.stat=stat\n\n    def progress(self):\n        while self.nplayed<self.nplay:\n            self.board=TTTBoard()\n            while self.board.winner==None:\n                if self.disp:print(\"Turn is \"+self.player_turn.name)\n                act=self.player_turn.act(self.board)\n                self.board.move(act,self.player_turn.myturn)\n                if self.disp:self.board.print_board()\n\n                if self.board.winner != None:\n                    # notice every player that game ends\n                    for i in self.players:\n                        i.getGameResult(self.board) \n                    if self.board.winner == DRAW:\n                        if self.showResult:print (\"Draw Game\")\n                    elif self.board.winner == self.player_turn.myturn:\n                        out = \"Winner : \" + self.player_turn.name\n                        if self.showResult: print(out)\n                    else:\n                        print (\"Invalid Move!\")\n                    self.nwon[self.board.winner]+=1\n                else:\n                    self.switch_player()\n                    #Notice other player that the game is going\n                    self.player_turn.getGameResult(self.board)\n\n            self.nplayed+=1\n            if self.nplayed%self.stat==0 or self.nplayed==self.nplay:\n                    \u3000\u3000  print(self.player_x.name+\":\"+str(self.nwon[self.player_x.myturn])+\",\"+self.player_o.name+\":\"+str(self.nwon[self.player_o.myturn])\n             +\",DRAW:\"+str(self.nwon[DRAW]))\n\n\n    def switch_player(self):\n        if self.player_turn == self.player_x:\n            self.player_turn=self.player_o\n        else:\n            self.player_turn=self.player_x\n\n\n\n\u8272\u3005\u306a\u30d7\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u6210\n\n\u30e9\u30f3\u30c0\u30e0\u541b\n\u7f6e\u3051\u308b\u3068\u3053\u308d\u306b\u7f6e\u304f\u3060\u3051\u306e\u30b7\u30f3\u30d7\u30eb\u306a\u3084\u3064\u3067\u3059\u3002\nimport random\n\n\nclass PlayerRandom:\n    def __init__(self,turn):\n        self.name=\"Random\"\n        self.myturn=turn\n\n    def act(self,board):\n        acts=board.get_possible_pos()\n        i=random.randrange(len(acts))\n        return acts[i]\n\n\n    def getGameResult(self,board):\n        pass\n\n\n\n\u4eba\u9593\nclass PlayerHuman:\n    def __init__(self,turn):\n        self.name=\"Human\"\n        self.myturn=turn\n\n    def act(self,board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Where would you like to place \" + str(self.myturn) + \" (1-9)? \")\n                act = int(act)\n                #if act >= 1 and act <= 9 and board.board[act-1]==EMPTY:\n                if act >= 1 and act <= 9:\n                    valid=True\n                    return act-1\n                else:\n                    print (\"That is not a valid move! Please try again.\")\n            except Exception as e:\n                    print (act +  \"is not a valid move! Please try again.\")\n        return act\n\n    def getGameResult(self,board):\n        if board.winner is not None and board.winner!=self.myturn and board.winner!=DRAW:\n            print(\"I lost...\")\n\n\u307e\u305a\u306f\u4eba\u9593\u3068\u30e9\u30f3\u30c0\u30e0\u3067\u6226\u308f\u305b\u3066\u307f\u307e\u3059\u3002\u8ca0\u3051\u305f\u3089\u6065\u3002    \ndef Human_vs_Random():\n\n    p1=PlayerHuman(PLAYER_X)\n    p2=PlayerRandom(PLAYER_O)\n    game=TTT_GameOrganizer(p1,p2)\n    game.progress()\n\n\n\n\u5b9f\u884c\u7d50\u679c\nHuman_vs_Random()\nTurn is Random\n   |   |   \n-----------\n   |   | O \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place 1 (1-9)? 1\n X |   |   \n-----------\n   |   | O \n-----------\n   |   |   \nTurn is Random\n X |   |   \n-----------\n O |   | O \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place 1 (1-9)? 2\n X | X |   \n-----------\n O |   | O \n-----------\n   |   |   \nTurn is Random\n X | X |   \n-----------\n O | O | O \n-----------\n   |   |   \nI lost...\nWinner : Random\nHuman:0,Random:1,DRAW:0\n\n\n\u306a\u3093\u3068\u3001\u30ed\u30b0\u306b\u6b8b\u3063\u3066\u3044\u305f\u306e\u306f\u6c17\u3092\u629c\u3044\u3066\u8ca0\u3051\u305f\u4f8b\u3067\u3057\u305f\u3002\n\n\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b (Alpha Random)\n\u3055\u3059\u304c\u306b\u305f\u3060\u306e\u30e9\u30f3\u30c0\u30e0\u3060\u3068\u3064\u307e\u3089\u306a\u3044\u306e\u3067\u3001\u6b21\u306e\u624b\u3067\u52dd\u3066\u308b\u3068\u3053\u308d\u304c\u3042\u308c\u3070\u304d\u3061\u3093\u3068\u6307\u3059\u3088\u3046\u306b\u3057\u307e\u3059\u3002\uff08\u306a\u3051\u308c\u3070\u30e9\u30f3\u30c0\u30e0\uff09\nclass PlayerAlphaRandom:\n\n\n    def __init__(self,turn,name=\"AlphaRandom\"):\n        self.name=name\n        self.myturn=turn\n\n    def getGameResult(self,winner):\n        pass\n\n    def act(self,board):\n        acts=board.get_possible_pos()\n        #see only next winnable act\n        for act in acts:\n            tempboard=board.clone()\n            tempboard.move(act,self.myturn)\n            # check if win\n            if tempboard.winner==self.myturn:\n                #print (\"Check mate\")\n                return act\n        i=random.randrange(len(acts))\n        return acts[i]\n\n\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\n\u3055\u3066\u3001\u3053\u3053\u304b\u3089\u304c\u5f37\u5316\u5b66\u7fd2\u9818\u57df\u306b\u5165\u3063\u3066\u304d\u307e\u3059\u3002\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u3067\u306f\u3001\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u306b\u6cbf\u3063\u3066\u4e00\u5b9a\u56de\u6570\u3092\u30b2\u30fc\u30e0\u7d42\u4e86\u307e\u3067\u8a66\u3057\u3001\u305d\u306e\u52dd\u7387\u3092\u898b\u308b\u3053\u3068\u3067\u3044\u3044\u624b\u3092\u63a2\u7d22\u3057\u307e\u3059\u3002\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u306e\u91cd\u8981\u306a\u70b9\u306f\u3001\u300c\u4e00\u5b9a\u56de\u6570\u300d\u306e\u6570\u3068\u3001\u300c\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u300d\u306b\u3088\u3063\u3066\u5f37\u3055\u304c\u5de6\u53f3\u3055\u308c\u308b\u3053\u3068\u3002\n\u300c\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u300d\u306b\u306f\u3001\u5148\u307b\u3069\u306e\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b\u3092\u4f7f\u3044\u307e\u3057\u305f\u3002\n\u300c\u4e00\u5b9a\u56de\u6570\u300d\u306f\u8a66\u3057\u305f\u3068\u3053\u308d\uff11\uff10\uff10\u56de\u3092\u8d85\u3048\u308c\u3070\u3001\u307e\u305a\u9593\u9055\u3063\u305f\u624b\u306f\u6253\u305f\u306a\u3044\u3088\u3046\u3067\u3059\u3002\uff15\uff10\u56de\u304f\u3089\u3044\u3060\u3068\u3001\u4e71\u6570\u6b21\u7b2c\u3067\u6642\u3005\u8ca0\u3051\u305f\u308a\u3057\u307e\u3059\u3002\n\u3053\u3053\u304b\u3089\u5b9f\u7e3e\u3092\u52d5\u7684\u8a08\u753b\u6cd5\u306b\u6301\u3063\u3066\u3063\u305f\u308a\u3001\u30e2\u30f3\u30c6\u30ab\u30eb\u30c8\u6728\u63a2\u7d22\uff08MCTS)\u306b\u6d3e\u751f\u3057\u3066\u5f37\u3044\u30dd\u30ea\u30b7\u30fc\u3068\u5f31\u3044\u30dd\u30ea\u30b7\u30fc\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u8a00\u53ca\u3057\u307e\u305b\u3093\u3002\nclass PlayerMC:\n    def __init__(self,turn,name=\"MC\"):\n        self.name=name\n        self.myturn=turn\n\n    def getGameResult(self,winner):\n        pass\n\n    def win_or_rand(self,board,turn):\n        acts=board.get_possible_pos()\n        #see only next winnable act\n        for act in acts:\n            tempboard=board.clone()\n            tempboard.move(act,turn)\n            # check if win\n            if tempboard.winner==turn:\n                return act\n        i=random.randrange(len(acts))\n        return acts[i]\n\n    def trial(self,score,board,act):\n        tempboard=board.clone()\n        tempboard.move(act,self.myturn)\n        tempturn=self.myturn\n        while tempboard.winner is None:\n            tempturn=tempturn*-1\n            tempboard.move(self.win_or_rand(tempboard,tempturn),tempturn)\n\n        if tempboard.winner==self.myturn:\n            score[act]+=1\n        elif tempboard.winner==DRAW:\n            pass\n        else:\n            score[act]-=1\n\n\n    def getGameResult(self,board):\n        pass\n\n\n    def act(self,board):\n        acts=board.get_possible_pos()\n        scores={}\n        n=50\n        for act in acts:\n            scores[act]=0\n            for i in range(n):\n                #print(\"Try\"+str(i))\n                self.trial(scores,board,act)\n\n            #print(scores)\n            scores[act]/=n\n\n        max_score=max(scores.values())\n        for act, v in scores.items():\n            if v == max_score:\n                #print(str(act)+\"=\"+str(v))\n                return act\n\n\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u540c\u58eb\u3067\u6226\u308f\u305b\u308b\np1=PlayerMC(PLAYER_X,\"M1\")\np2=PlayerMC(PLAYER_O,\"M2\")\ngame=TTT_GameOrganizer(p1,p2,10,False)\ngame.progress()\n\nDraw Game\nWinner : M2\nDraw Game\nDraw Game\nDraw Game\nDraw Game\nDraw Game\nWinner : M2\nDraw Game\nDraw Game\nM1:0,M2:2,DRAW:8\n\n\n\u5927\u4f53\u30c9\u30ed\u30fc\u3067\u3059\u306d\u3002\u6c17\u304c\u5411\u3044\u305f\u3089\u8a66\u884c\u56de\u6570\u3092100\u56de\u306b\u3057\u3066\u8a66\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u5168\u90e8\u30c9\u30ed\u30fc\u306b\u306a\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\nQ-Learning\n\u3055\u3066\u3001\u5f37\u5316\u5b66\u7fd2\u3068\u3044\u3048\u3070\u3001Q-Learning\u3002\n\u30b2\u30fc\u30e0\u9014\u4e2d\u306f\u5831\u916c 0 \u3001\u52dd\u3063\u305f\u3089+1\u3001\u8ca0\u3051\u305f\u3089-1\u3001\u5f15\u304d\u5206\u3051\u306f0\u3067\u3059\u3002\n\u305d\u308c\u3092\u30d9\u30fc\u30b9\u306bQ(s,a)\u306eDictionary\u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u66f4\u65b0\u3057\u3066\u3044\u304d\u307e\u3059\u3002\nQ(s,a) = Q(s,a) + alpha (reward + gammma* max(Q(s',a')- Q(s,a)) \n\n\u91cd\u8981\u306a\u306e\u306f\u03b5-greedy\u3068\u3044\u3046\u30b3\u30f3\u30bb\u30d7\u30c8\u3067\u30e9\u30f3\u30c0\u30e0\u306e\u6253\u3061\u624b\u3092\u63a8\u5968\u3059\u308b\u3053\u3068\u3002\u3067\u306a\u3044\u3068\u3059\u3050\u306blocal optimum\u306b\u9665\u308a\u307e\u3059\u3002\n\u6700\u521d\u306eQ\u306e\u5024\u3082\u91cd\u8981\u3067\u3059\u3002\u3067\u306a\u3044\u3068\u3001\u3059\u3050\u63a2\u3059\u6c17\u3092\u306a\u304f\u3057\u3061\u3083\u3044\u307e\u3059\u3002\u308f\u304c\u307e\u307e\u306a\u5b50\u3067\u3059\u3002\n\u30b2\u30fc\u30e0\u7d42\u4e86\u3067\u306emax(Q)\u3092\u3069\u3046\u3057\u3088\u3046\u3068\u304b\u3001\u30b2\u30fc\u30e0\u9014\u4e2d\u306e\u66f4\u65b0\u3092\u5fd8\u308c\u305f\u308a\u3068\u3001\u610f\u5916\u3068\u306f\u307e\u308a\u307e\u3057\u305f\u3002\nclass PlayerQL:\n    def __init__(self,turn,name=\"QL\",e=0.2,alpha=0.3):\n        self.name=name\n        self.myturn=turn\n        self.q={} #set of s,a\n        self.e=e\n        self.alpha=alpha\n        self.gamma=0.9\n        self.last_move=None\n        self.last_board=None\n        self.totalgamecount=0\n\n\n    def policy(self,board):\n        self.last_board=board.clone()\n        acts=board.get_possible_pos()\n        #Explore sometimes\n        if random.random() < (self.e/(self.totalgamecount//10000+1)):\n                i=random.randrange(len(acts))\n                return acts[i]\n        qs = [self.getQ(tuple(self.last_board.board),act) for act in acts]\n        maxQ= max(qs)\n\n        if qs.count(maxQ) > 1:\n            # more than 1 best option; choose among them randomly\n            best_options = [i for i in range(len(acts)) if qs[i] == maxQ]\n            i = random.choice(best_options)\n        else:\n            i = qs.index(maxQ)\n\n        self.last_move = acts[i]\n        return acts[i]\n\n    def getQ(self, state, act):\n        # encourage exploration; \"optimistic\" 1.0 initial values\n        if self.q.get((state, act)) is None:\n            self.q[(state, act)] = 1\n        return self.q.get((state, act))\n\n    def getGameResult(self,board):\n        r=0\n        if self.last_move is not None:\n            if board.winner is None:\n                self.learn(self.last_board,self.last_move, 0, board)\n                pass\n            else:\n                if board.winner == self.myturn:\n                    self.learn(self.last_board,self.last_move, 1, board)\n                elif board.winner !=DRAW:\n                    self.learn(self.last_board,self.last_move, -1, board)\n                else:\n                    self.learn(self.last_board,self.last_move, 0, board)\n                self.totalgamecount+=1\n                self.last_move=None\n                self.last_board=None\n\n    def learn(self,s,a,r,fs):\n        pQ=self.getQ(tuple(s.board),a)\n        if fs.winner is not None:\n            maxQnew=0\n        else:\n            maxQnew=max([self.getQ(tuple(fs.board),act) for act in fs.get_possible_pos()])\n        self.q[(tuple(s.board),a)]=pQ+self.alpha*((r+self.gamma*maxQnew)-pQ)\n        #print (str(s.board)+\"with \"+str(a)+\" is updated from \"+str(pQ)+\" refs MAXQ=\"+str(maxQnew)+\":\"+str(r))\n        #print(self.q)\n\n\n    def act(self,board):\n        return self.policy(board)\n\n\nQ-Learning \u540c\u58eb\u306710\u4e07\u56de\u3000\u6226\u308f\u305b\u307e\u3059\u3002\npQ=PlayerQL(PLAYER_O,\"QL1\")\np2=PlayerQL(PLAYER_X,\"QL2\")\ngame=TTT_GameOrganizer(pQ,p2,100000,False,False,10000)\ngame.progress()\n\nQL1:4371,QL2:4436,DRAW:1193\nQL1:8328,QL2:8456,DRAW:3216\nQL1:11903,QL2:11952,DRAW:6145\nQL1:14268,QL2:14221,DRAW:11511\nQL1:15221,QL2:15099,DRAW:19680\nQL1:15730,QL2:15667,DRAW:28603\nQL1:16136,QL2:16090,DRAW:37774\nQL1:16489,QL2:16439,DRAW:47072\nQL1:16832,QL2:16791,DRAW:56377\nQL1:17128,QL2:17121,DRAW:65751\n\n\u5927\u4f53\u30c9\u30ed\u30fc\u306b\u53ce\u675f\u3057\u3066\u3044\u307e\u3059\u306d\u3002\u03b5\u3067\u52dd\u6557\u304c\u5206\u304b\u308c\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u3068\u6226\u308f\u305b\u308b\n\u03b5\u3092\u30bc\u30ed\u306b\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u8981\u7d20\u3092\u306a\u304f\u3057\u3066\u6700\u826f\u306e\u6253\u3061\u624b\u3067\u653b\u3081\u3066\u307f\u307e\u3059\npQ.e=0\np2=PlayerMC(PLAYER_X,\"M1\")\ngame=TTT_GameOrganizer(pQ,p2,100,False,False,10)\ngame.progress()\nQL1:1,M1:0,DRAW:9\nQL1:1,M1:0,DRAW:19\nQL1:2,M1:0,DRAW:28\nQL1:2,M1:0,DRAW:38\nQL1:3,M1:0,DRAW:47\nQL1:4,M1:0,DRAW:56\nQL1:5,M1:0,DRAW:65\nQL1:5,M1:0,DRAW:75\nQL1:6,M1:0,DRAW:84\nQL1:6,M1:0,DRAW:94\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u3082\u6642\u3005\u8ca0\u3051\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3084\u306f\u308a\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u306f\u4e71\u6570\u306e\u6c17\u307e\u3050\u308c\u304b\u3089\u9003\u308c\u3089\u308c\u307e\u305b\u3093\u3002\n\u3061\u306a\u307f\u306b\u3053\u306e\u72b6\u614b\u306eQ-Learning\u541b\u3068\u6226\u3046\u3068\u3001\u304b\u306a\u308a\u75db\u3044\u76ee\u306b\u3042\u3044\u307e\u3059\u3002\n\nDQN (Deep Q Network)\n\u3055\u3066\u4eca\u56de\u306e\u672c\u984c\u3002DQN\u3067\u3059\u3002\nQ-Learning\u306e\u30cd\u30c3\u30af\u306fQ(s,a)\u306e\u30c6\u30fc\u30d6\u30eb\u304c\u81a8\u5927\u306b\u306a\u308b\u3053\u3068\u3002\u4e09\u76ee\u4e26\u3079\u306a\u3089\u3044\u3044\u3051\u3069\u3001\u7881\u3067\u306fS\u3082a\u3082\u83ab\u5927\u306a\u30e1\u30e2\u30ea\u3068\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002\nQ-Network\u3067\u306fQ(s)\u306b\u3088\u308a\u5404a\u306e\u671f\u5f85\u5831\u916c\u3092\u51fa\u3057\u3066\u3001argmax\u3092\u53d6\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\nDeep Q Network\u306a\u3089\u3000\u300c\u52dd\u624b\u306b\u52dd\u3064\u7279\u5fb4\u3092\u307f\u3064\u3051\u3061\u3083\u3046\u3093\u3058\u3083\u306a\u3044\uff01\uff1f\u300d\u30011000\u56de\u4f4d\u306e\u5bfe\u6226\u3067\u3044\u3044\u3093\u3058\u3083\u306a\u3044\uff1f\u3068\u305f\u304b\u3092\u304f\u304f\u3063\u3066\u307e\u3057\u305f\u3002\n\u30fb\u30fb\u30fb\u30fb\u304c\u3001\u30c9\u306f\u307e\u308a\u3057\u307e\u3057\u305f\u3002\n\u5168\u7136\u5f37\u304f\u306a\u3089\u306a\u3044\u3002\u3057\u304b\u3082Hyper Parameter\u304c\u591a\u3059\u304e\u308b\u3002Hidden Layer\u4f55\u500b\u306b\u3059\u308c\u3070\u3044\u3044\u306e\uff1fRelu? \u8aa4\u5dee\u306fSME\u3067\u3044\u3044\u306e\uff1fOptimizer\u306fAdam\u304bSGD?\u3000\u6b63\u76f4Grid Search\u304c\u307b\u3057\u304b\u3063\u305f\u3067\u3059\u3002\u3067\u3082\u305d\u3082\u305d\u3082\u6559\u5e2b\u304c\u306a\u3044\u306e\u3067\u30ed\u30b9\u306e\u6e2c\u5b9a\u3082\u3067\u304d\u306a\u3044\u3093\u3067\u3059\u3088\u306d\u3002\u7cbe\u5ea6\u306f\u3001\u76f8\u624b\u3068\u6226\u308f\u305b\u3066\u521d\u3081\u3066\u308f\u304b\u308b\u3002\n\u3055\u3089\u306b\u3001\u5b66\u7fd2\u3055\u305b\u3066\u3044\u3066\u3082\u3001Variable Link\u304c\u5207\u308c\u3066\u5168\u7136\u5b66\u7fd2\u3057\u306a\u304b\u3063\u305f\u308a\u3002\u307b\u3093\u3068\u30cf\u30de\u308a\u307e\u3059\u3002\n\u3057\u304b\u3082GPU\u306e\u306a\u3044\u30de\u30b7\u30f3\u3067\u3044\u308d\u3044\u308d\u3084\u3063\u3066\u305f\u306e\u3067\u9045\u3044\u3002\u3002\u3002GPU\u306a\u3044\u3068\u5b66\u7fd2\u306b5-10\u5206\u304f\u3089\u3044\u304b\u304b\u3063\u305f\u308a\u3057\u307e\u3059\u3002\nimport chainer\n\nfrom chainer import Function, gradient_check, Variable, optimizers, serializers, utils\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\nfrom chainer import computational_graph as c\n\n# Network definition\nclass MLP(chainer.Chain):\n\n    def __init__(self, n_in, n_units, n_out):\n        super(MLP, self).__init__(\n            l1=L.Linear(n_in, n_units),  # first layer\n            l2=L.Linear(n_units, n_units),  # second layer\n            l3=L.Linear(n_units, n_units),  # Third layer\n            l4=L.Linear(n_units, n_out),  # output layer\n        )\n\n    def __call__(self, x, t=None, train=False):\n        h = F.leaky_relu(self.l1(x))\n        h = F.leaky_relu(self.l2(h))\n        h = F.leaky_relu(self.l3(h))\n        h = self.l4(h)\n\n        if train:\n            return F.mean_squared_error(h,t)\n        else:\n            return h\n\n    def get(self,x):\n        # input x as float, output float\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(1,1))).data[0][0]\n\n\nclass DQNPlayer:\n    def __init__(self, turn,name=\"DQN\",e=1,dispPred=False):\n        self.name=name\n        self.myturn=turn\n        self.model = MLP(9, 162,9)\n        self.optimizer = optimizers.SGD()\n        self.optimizer.setup(self.model)\n        self.e=e\n        self.gamma=0.95\n        self.dispPred=dispPred\n        self.last_move=None\n        self.last_board=None\n        self.last_pred=None\n        self.totalgamecount=0\n        self.rwin,self.rlose,self.rdraw,self.rmiss=1,-1,0,-1.5\n\n\n    def act(self,board):\n\n        self.last_board=board.clone()\n        x=np.array([board.board],dtype=np.float32).astype(np.float32)\n\n        pred=self.model(x)\n        if self.dispPred:print(pred.data)\n        self.last_pred=pred.data[0,:]\n        act=np.argmax(pred.data,axis=1)\n        if self.e > 0.2: #decrement epsilon over time\n            self.e -= 1/(20000)\n        if random.random() < self.e:\n            acts=board.get_possible_pos()\n            i=random.randrange(len(acts))\n            act=acts[i]\n        i=0\n        while board.board[act]!=EMPTY:\n            #print(\"Wrong Act \"+str(board.board)+\" with \"+str(act))\n            self.learn(self.last_board,act, -1, self.last_board)\n            x=np.array([board.board],dtype=np.float32).astype(np.float32)\n            pred=self.model(x)\n            #print(pred.data)\n            act=np.argmax(pred.data,axis=1)\n            i+=1\n            if i>10:\n                print(\"Exceed Pos Find\"+str(board.board)+\" with \"+str(act))\n                acts=self.last_board.get_possible_pos()\n                act=acts[random.randrange(len(acts))]\n\n        self.last_move=act\n        #self.last_pred=pred.data[0,:]\n        return act\n\n    def getGameResult(self,board):\n        r=0\n        if self.last_move is not None:\n            if board.winner is None:\n                self.learn(self.last_board,self.last_move, 0, board)\n                pass\n            else:\n                if board.board== self.last_board.board:            \n                    self.learn(self.last_board,self.last_move, self.rmiss, board)\n                elif board.winner == self.myturn:\n                    self.learn(self.last_board,self.last_move, self.rwin, board)\n                elif board.winner !=DRAW:\n                    self.learn(self.last_board,self.last_move, self.rlose, board)\n                else:                    #DRAW\n                    self.learn(self.last_board,self.last_move, self.rdraw, board)\n                self.totalgamecount+=1\n                self.last_move=None\n                self.last_board=None\n                self.last_pred=None\n\n    def learn(self,s,a,r,fs):\n        if fs.winner is not None:\n            maxQnew=0\n        else:\n            x=np.array([fs.board],dtype=np.float32).astype(np.float32)\n            maxQnew=np.max(self.model(x).data[0])\n        update=r+self.gamma*maxQnew\n        #print(('Prev Board:{} ,ACT:{}, Next Board:{}, Get Reward {}, Update {}').format(s.board,a,fs.board,r,update))\n        #print(('PREV:{}').format(self.last_pred))\n        self.last_pred[a]=update\n\n        x=np.array([s.board],dtype=np.float32).astype(np.float32)\n        t=np.array([self.last_pred],dtype=np.float32).astype(np.float32)\n        self.model.zerograds()\n        loss=self.model(x,t,train=True)\n        loss.backward()\n        self.optimizer.update()\n        #print(('Updated:{}').format(self.model(x).data))\n        #print (str(s.board)+\"with \"+str(a)+\" is updated from \"+str(pQ)+\" refs MAXQ=\"+str(maxQnew)+\":\"+str(r))\n        #print(self.q)\n\n\n\u30bd\u30fc\u30b9\u9577\u3044\u3067\u3059\u306d\u3047\u3002\u3002\u3002\u305f\u3076\u3093\u8a66\u884c\u932f\u8aa4\u306e\u5f62\u8de1\u304c\u6b8b\u3063\u3066\u3044\u308b\u6c17\u304c\u3057\u307e\u3059\u304c\u3001\u304d\u308c\u3044\u306b\u3057\u3066\u3044\u307e\u305b\u3093\u3002\n\n\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u3068\u5bfe\u6226\n\u3053\u308c\u3067\u307e\u305a\u306f\u3056\u3063\u304f\u308a\u6253\u3061\u65b9\u3068\u52dd\u3061\u65b9\u3092\u899a\u3048\u3055\u305b\u307e\u3059\u3002\npDQ=DQNPlayer(PLAYER_X)\np2=PlayerAlphaRandom(PLAYER_O)\ngame=TTT_GameOrganizer(pDQ,p2,20000,False,False,1000)\ngame.progress()\nDQN:206,AlphaRandom:727,DRAW:67\nDQN:468,AlphaRandom:1406,DRAW:126\nDQN:861,AlphaRandom:1959,DRAW:180\nDQN:1458,AlphaRandom:2315,DRAW:227\nDQN:2185,AlphaRandom:2560,DRAW:255\nDQN:3022,AlphaRandom:2704,DRAW:274\nDQN:3832,AlphaRandom:2856,DRAW:312\nDQN:4632,AlphaRandom:3023,DRAW:345\nDQN:5481,AlphaRandom:3153,DRAW:366\nDQN:6326,AlphaRandom:3280,DRAW:394\nDQN:7181,AlphaRandom:3400,DRAW:419\nDQN:8032,AlphaRandom:3522,DRAW:446\nDQN:8902,AlphaRandom:3618,DRAW:480\nDQN:9791,AlphaRandom:3705,DRAW:504\nDQN:10673,AlphaRandom:3793,DRAW:534\nDQN:11545,AlphaRandom:3893,DRAW:562\nDQN:12420,AlphaRandom:3986,DRAW:594\nDQN:13300,AlphaRandom:4074,DRAW:626\nDQN:14183,AlphaRandom:4158,DRAW:659\nDQN:15058,AlphaRandom:4246,DRAW:696\n\n\u3060\u3044\u3076\u5f37\u304f\u306a\u308a\u307e\u3057\u305f\u306d\u3002\u3055\u3059\u304c\u306b\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b\u306b\u306f\u52dd\u3066\u308b\u3088\u3046\u3067\u3002\n\n\u672c\u547d Q-Learning\u3068\u5bfe\u6c7a\npDQ.e=1\ngame=TTT_GameOrganizer(pDQ,pQ,30000,False,False,1000)\ngame.progress()\n\nDQN:4,QL1:436,DRAW:560\nDQN:6,QL1:790,DRAW:1204\nDQN:6,QL1:1135,DRAW:1859\nDQN:6,QL1:1472,DRAW:2522\nDQN:6,QL1:1801,DRAW:3193\nDQN:6,QL1:2123,DRAW:3871\nDQN:6,QL1:2439,DRAW:4555\nDQN:6,QL1:2777,DRAW:5217\nDQN:7,QL1:3128,DRAW:5865\nDQN:9,QL1:3648,DRAW:6343\nDQN:13,QL1:4132,DRAW:6855\nDQN:13,QL1:4606,DRAW:7381\nDQN:13,QL1:5087,DRAW:7900\nDQN:14,QL1:5536,DRAW:8450\nDQN:14,QL1:6011,DRAW:8975\nDQN:14,QL1:6496,DRAW:9490\nDQN:14,QL1:7394,DRAW:9592\nDQN:14,QL1:7925,DRAW:10061\nDQN:16,QL1:8357,DRAW:10627\nDQN:16,QL1:8777,DRAW:11207\n\n\u3088\u3046\u3084\u304f\u5f37\u304f\u306a\u3063\u305f\u306e\u304b\u306a\u3002\u3002\u3002\u306a\u3093\u3060\u304b\u5fae\u5999\u306a\u5f37\u3055\u3067\u3059\u304c\u3002\u3002\n\u3053\u306e\u30ed\u30b0\u3067\u306f\u307e\u3060\u8ca0\u3051\u304c\u591a\u3044\u3067\u3059\u304c\u300110\u4e07\u56de\u5bfe\u6226\u4f4d\u3059\u308b\u3068\u5927\u4f53\u30c9\u30ed\u30fc\u306b\u843d\u3061\u7740\u304d\u307e\u3057\u305f\u3002\n\n\u6700\u5f8c\u306b\u4eba\u9593\u3068\u5bfe\u6226\u3002\u03b5\u306f\u30bc\u30ed\npDQ.e=0\np2=PlayerHuman(PLAYER_O)\ngame=TTT_GameOrganizer(pDQ,p2,2)\ngame.progress()\nTurn is Human\nWhere would you like to place -1 (1-9)? 1\n//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:69: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n\n O |   |   \n-----------\n   |   |   \n-----------\n   |   |   \nTurn is DQN\n O |   |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 2\n O | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is DQN\n O | O | X \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n   | X |   \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X |   \n-----------\n O |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 6\n O | O | X \n-----------\n X | X | O \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nI lost...\nInvalid Move!\nTurn is Human\nWhere would you like to place -1 (1-9)? 2\n   | O |   \n-----------\n   |   |   \n-----------\n   |   |   \nTurn is DQN\n   | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 1\n O | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is DQN\n O | O | X \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n   | X |   \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X |   \n-----------\n O |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 6\n O | O | X \n-----------\n X | X | O \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nTurn is Human\nWhere would you like to place -1 (1-9)? 8\n O | O | X \n-----------\n X | X | O \n-----------\n O | O | X \nDraw Game\nDQN:1,Human:0,DRAW:1\n\n\u307e\u3041\u305d\u308c\u3089\u3057\u3044\u6253\u3061\u624b\u306b\u306f\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n\u7d50\u8ad6\nQ-Learning\u306f\u5b9f\u88c5\u306f\u304b\u306a\u308a\u697d\u3067\u3057\u305f\u304c\u3001Deep Q Network\u3067\u306f\u8a66\u884c\u932f\u8aa4\u3057\u306a\u3044\u3068\u6c7a\u3081\u3089\u308c\u306a\u3044\u3053\u3068\u304c\u975e\u5e38\u306b\u591a\u304b\u3063\u305f\u3067\u3059\u3002\n\u6700\u521dRelu\u3067\u5b9f\u88c5\u3057\u3066\u3082\u5168\u7136\u5f37\u304f\u306a\u3089\u305a\u3001Leaky Relu\u306b\u3057\u3066\u3088\u304f\u306a\u308a\u307e\u3057\u305f\u3002\u305f\u3076\u3093\u76e4\u9762\u306b-1\u304c\u5165\u308b\u306e\u3067Relu\u3067\u306f\u3046\u307e\u304f\u8868\u73fe\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u304b\u306a\u3002\n\u3042\u3068\u3001\u306a\u305c\u304bAdam\u3067\u306f\u3060\u3081\u3067SGD\u3067\u306f\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u4e0d\u601d\u8b70\u3067\u3059\u3002Adam\u6700\u5f37\u3068\u52dd\u624b\u306b\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\u3055\u3089\u306bAlphaRandom\u3068\u5bfe\u6226\u3055\u305b\u306a\u3044\u3067\u3001\u6700\u521d\u304b\u3089Q-Learning(\u5f37\u3044\u3084\u3064\uff09\u3068\u6226\u308f\u3055\u305b\u308b\u3068\u3001\u3059\u3050\u306b\u3084\u308b\u6c17\u3092\u306a\u304f\u3057\u3066\u3001\u8ca0\u3051\u3063\u3071\u306a\u3057\u306b\u306a\u3063\u305f\u308a\u3057\u307e\u3059\u3002\u6700\u521d\u306f\u5f31\u3044\u3084\u3064\u3068\u6226\u308f\u305b\u3066\u3001\u3084\u308b\u6c17\u3092\u51fa\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3088\u3046\u3067\u3059\u3002\n\u307e\u305f\u3001\u4f55\u3088\u308a\u82e6\u52b4\u3057\u305f\u306e\u304c\n\n\u3069\u3046\u3084\u3063\u3066\u3001DQN\u306b\u6253\u3063\u3066\u306f\u3044\u3051\u306a\u3044\u624b\u3092\u6559\u3048\u308b\u304b\n\n\u3068\u3044\u3046\u70b9\u3067\u3059\u3002\n\u4eca\u56de\u306f\u65e2\u306b\u30b3\u30de\u306e\u3042\u308b\u3068\u3053\u308d\u306b\u6253\u3063\u305f\u3089\u3001\u5f37\u5236\u7684\u306b\u30de\u30a4\u30ca\u30b9\u5831\u916c\u3092\u4e0e\u3048\u306610\u56de\u5b66\u7fd2\u3068\u3044\u3046\u3001\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3044\u308c\u3066\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u6253\u3063\u3066\u3088\u3044\u5834\u6240\u3092\u5165\u529b\u306b\u5165\u308c\u308c\u3070\u3044\u3044\u306e\u304b\u306a\uff1f\n\u7591\u554f\u3084\u6539\u5584\u3057\u305f\u3044\u3068\u3053\u308d\u304c\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3082\u3057\u308f\u304b\u308b\u4eba\u3044\u305f\u3089\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\u30a2\u30c9\u30d0\u30a4\u30b9\u3084\u6307\u6458\u3001\u3069\u3093\u3069\u3093\u30a6\u30a7\u30eb\u30ab\u30e0\u3067\u3059\u3002\n\u305d\u308c\u306b\u3057\u3066\u3082\u3001\u4e09\u76ee\u4e26\u3079\u306a\u30892-3\u6642\u9593\u304f\u3089\u3044\u3042\u308c\u3070\u3067\u304d\u308b\u3060\u308d\u3046\u3001\u3068\u305f\u304b\u3092\u304f\u304f\u3063\u3066\u3044\u305f\u306e\u306b\u3001\u601d\u3044\u306e\u307b\u304b\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3057\u305f\u3002\n\u77e5\u3063\u3066\u3044\u308b\u3001\u5b9f\u88c5\u3067\u304d\u308b\uff08\u30b3\u30fc\u30c9\u66f8\u3051\u308b\uff09\u3001\u3067\u304d\u308b\uff08\u304d\u3061\u3093\u3068\u52d5\u304f\uff09\u3000\u306b\u306f\u3068\u3093\u3067\u3082\u306a\u3044\u30ae\u30e3\u30c3\u30d7\u304c\u3042\u308b\u3053\u3068\u3092\u75db\u611f\u3002\u305c\u3072\u7686\u3055\u3093\u3082\u4eca\u56de\u306e\u30bd\u30fc\u30b9\u306f\u3042\u307e\u308a\u898b\u305a\u306b\u3001\u3054\u81ea\u8eab\u3067\u5b9f\u88c5\u3055\u308c\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n\u3055\u3066\u3001\u6642\u9593\u306e\u3042\u308b\u3068\u304d\u306b\u30aa\u30bb\u30ed\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u4f5c\u308b\u304b\u306a\u3041\u3002\u3002\u3002\n\u305d\u308c\u304b\u304d\u3061\u3093\u3068AlphaGo\u306e3NN\u69cb\u6210\u3092\u8a66\u3057\u3066\u307f\u3088\u3046\u304b\u3001\u6b21\u306e\u8ab2\u984c\u306b\u60a9\u307f\u4e2d\u3067\u3059\u3002\n\u521d\u3081\u3066\u306eQiita\u6295\u7a3f\u3067\u3059\u3002Ridge-i\u3068\u3044\u3046\u4f1a\u793e\u3067\u6a5f\u68b0\u5b66\u7fd2\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u30b3\u30f3\u30b5\u30eb\uff5e\u958b\u767a\u3092\u3057\u3066\u307e\u3059\u3002\n\n\u5f37\u5316\u5b66\u7fd2\u306b\u3064\u3044\u3066\u6559\u3048\u308b\u6a5f\u4f1a\u304c\u51fa\u3066\u304d\u305f\u306e\u3067\u3001\u4e09\u76ee\u4e26\u3079\u3092\u30d9\u30fc\u30b9\u306b\n\n - \u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\n - Q-Learning\n - Deep Q Network (\u3044\u308f\u3086\u308bDQN)\n\n\u306b\u3064\u3044\u3066Jupyter\uff08ipython) \u3067\u5b9f\u88c5\u3057\u3066\u6559\u6750\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002\n\n\u3061\u306a\u307f\u306b\u5f37\u3044\u30d7\u30ec\u30fc\u30e4\u30fc\u540c\u58eb\u306a\u3089\u3070\u3001\u30c9\u30ed\u30fc\u3060\u3051\u304c\u7e70\u308a\u8fd4\u3055\u308c\u308b\u306f\u305a\u3067\u3059\u3002\uff08WarGame\u306e\u6709\u540d\u306a\u3084\u3064\u3067\u3059\u306d\u3002\uff09\n\n\u7d50\u8ad6\u3068\u3057\u3066\u306f\n\n- \u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u3000\u5b9f\u88c5\u7c21\u5358\u3002100\u56de\u8a66\u884c\u4f4d\u3067\u307b\u307c\u8ca0\u3051\u306a\u3057\uff0850\u56de\u304f\u3089\u3044\u3060\u3068\u6642\u3005\u8ca0\u3051\u308b\uff09\n- Q-Learning \u3000\u66f4\u65b0\u5f0f\u306e\u8a2d\u8a08\u306b\u6c17\u3092\u9063\u3046\u3002\uff13\u76ee\u4e26\u3079\u7a0b\u5ea6\u306a\u308910\u4e07\u5bfe\u6226\u3067\u6700\u5f37\n- Deep Q Network \u8272\u3005\u306a\u843d\u3068\u3057\u7a74\u304c\u591a\u6570\u3002\u6700\u9069\u306a\u30c8\u30dd\u30ed\u30b8\u30fc\u304c\u308f\u304b\u3089\u3093\u3002Leaky Relu\u306b\u3059\u308b\u307e\u3067\u6700\u5f31\u3002\u6559\u3048\u65b9\u30df\u30b9\u308b\u3068\u4f55\u3082\u5b66\u7fd2\u3057\u306a\u3044\u3002\u306a\u3069\u306a\u3069\n\nQ-Learning\u307e\u3067\u306e\u5b9f\u88c5\u306f1-2\u6642\u9593\u4f4d\u3057\u304b\u304b\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u306b\u3001DQN\u304c\u304d\u3061\u3093\u3068\u5b66\u7fd2\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u3060\u3051\u30676\u6642\u9593\u304f\u3089\u3044\u304b\u304b\u308a\u307e\u3057\u305f\u3002(\u6ce3\uff09\n\n\u5b66\u7fd2\u306e\u52b9\u7387\u5316\u3068\u3044\u3046\u9762\u3067\u306f\u304d\u3061\u3093\u3068\u76e4\u9762\u306e\u53cd\u8ee2\u3068\u56de\u8ee2\u3092\u3059\u308b\u3068\u3088\u3044\u306e\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u5206\u304b\u308a\u3084\u3059\u3055\u91cd\u8996\u3067\u629c\u304d\u307e\u3057\u305f\u3002\n\u3042\u3068\u3001\u304d\u3061\u3093\u3068\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u8a2d\u8a08\u3068\u5909\u6570\u306e\u96a0\u533f\u5316\u3092\u3057\u306a\u304b\u3063\u305f\u306e\u3067\u6642\u3005\u9055\u53cd\u3057\u3066\u307e\u3059\u3002\u305d\u306e\u3042\u305f\u308a\u306f\u7121\u8996\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\nGithub\u306f\u3053\u3061\u3089\u3002\nhttps://github.com/narisan25/TTT-RL\n\n\u7406\u8ad6\u7684\u306a\u8a73\u7d30\u3092\u77e5\u308a\u305f\u3044\u5834\u5408\u306f\u5225\u9014\u9023\u7d61\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u6307\u6458\u30fb\u6539\u826f\u30fb\u30a2\u30c9\u30d0\u30a4\u30b9\u3001\u5927Welcome\u3067\u3059\u3002\n\n\n# \u4e09\u76ee\u4e26\u3079\u306e\u8a2d\u8a08\n\n\u3053\u308c\u306f\u3056\u3056\u3063\u3068\u3084\u308a\u307e\u3059\u3002\u826f\u304f\u3059\u308b\u3079\u304d\u70b9\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u5206\u304b\u308a\u3084\u3059\u3055\u3068\u65e9\u3055\uff08\u52d5\u3051\u3070\u3044\u3044\uff09\u3092\u91cd\u8996\n\n## \u76e4\u9762\n0-9\u306e\u914d\u5217\u306bX\u306a\u30891\u3001\u30d6\u30e9\u30f3\u30af\u306a\u30890\u3001\u25cb\u306a\u3089-1\u3092\u5165\u308c\u307e\u3059\u3002\n\n```py\nEMPTY=0\nPLAYER_X=1\nPLAYER_O=-1\nMARKS={PLAYER_X:\"X\",PLAYER_O:\"O\",EMPTY:\" \"}\nDRAW=2\n\nclass TTTBoard:\n   \n    def __init__(self,board=None):\n        if board==None:\n            self.board = []\n            for i in range(9):self.board.append(EMPTY)\n        else:\n            self.board=board\n        self.winner=None\n    \n    def get_possible_pos(self):\n        pos=[]\n        for i in range(9):\n            if self.board[i]==EMPTY:\n                pos.append(i)\n        return pos\n    \n    def print_board(self):\n        tempboard=[]\n        for i in self.board:\n            tempboard.append(MARKS[i])\n        row = ' {} | {} | {} '\n        hr = '\\n-----------\\n'\n        print((row + hr + row + hr + row).format(*tempboard))\n               \n\n                    \n    def check_winner(self):\n        win_cond = ((1,2,3),(4,5,6),(7,8,9),(1,4,7),(2,5,8),(3,6,9),(1,5,9),(3,5,7))\n        for each in win_cond:\n            if self.board[each[0]-1] == self.board[each[1]-1]  == self.board[each[2]-1]:\n                if self.board[each[0]-1]!=EMPTY:\n                    self.winner=self.board[each[0]-1]\n                    return self.winner\n        return None\n    \n    def check_draw(self):\n        if len(self.get_possible_pos())==0 and self.winner is None:\n            self.winner=DRAW\n            return DRAW\n        return None\n    \n    def move(self,pos,player):\n        if self.board[pos]== EMPTY:\n            self.board[pos]=player\n        else:\n            self.winner=-1*player\n        self.check_winner()\n        self.check_draw()\n    \n    def clone(self):\n        return TTTBoard(self.board.copy())\n    \n    def switch_player(self):\n        if self.player_turn == self.player_x:\n            self.player_turn=self.player_o\n        else:\n            self.player_turn=self.player_x\n```\n\n## \u30b2\u30fc\u30e0\u306e\u9032\u884c\u5f79\nObserver+Mediator\u30e2\u30c7\u30eb\u306b\u3057\u307e\u3059\u3002\u3053\u308c\u3067Player\u304c\u76f4\u63a5\u76e4\u9762\u3092\u3044\u3058\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3064\u3064\u3001\u30bf\u30fc\u30f3\u30fb\u52dd\u6557\u30fb\u8868\u793a\u306e\u7ba1\u7406\u306a\u3069\u3092\u884c\u3044\u307e\u3059\u3002\n\n```py\nclass TTT_GameOrganizer:\n\nact_turn=0\nwinner=None\n    \n    def __init__(self,px,po,nplay=1,showBoard=True,showResult=True,stat=100):\n        self.player_x=px\n        self.player_o=po\n        self.nwon={px.myturn:0,po.myturn:0,DRAW:0}\n        self.nplay=nplay\n        self.players=(self.player_x,self.player_o)\n        self.board=None\n        self.disp=showBoard\n        self.showResult=showResult\n        self.player_turn=self.players[random.randrange(2)]\n        self.nplayed=0\n        self.stat=stat\n    \n    def progress(self):\n        while self.nplayed<self.nplay:\n            self.board=TTTBoard()\n            while self.board.winner==None:\n                if self.disp:print(\"Turn is \"+self.player_turn.name)\n                act=self.player_turn.act(self.board)\n                self.board.move(act,self.player_turn.myturn)\n                if self.disp:self.board.print_board()\n               \n                if self.board.winner != None:\n                    # notice every player that game ends\n                    for i in self.players:\n                        i.getGameResult(self.board) \n                    if self.board.winner == DRAW:\n                        if self.showResult:print (\"Draw Game\")\n                    elif self.board.winner == self.player_turn.myturn:\n                        out = \"Winner : \" + self.player_turn.name\n                        if self.showResult: print(out)\n                    else:\n                        print (\"Invalid Move!\")\n                    self.nwon[self.board.winner]+=1\n                else:\n                    self.switch_player()\n                    #Notice other player that the game is going\n                    self.player_turn.getGameResult(self.board)\n\n            self.nplayed+=1\n            if self.nplayed%self.stat==0 or self.nplayed==self.nplay:\n                    \u3000\u3000  print(self.player_x.name+\":\"+str(self.nwon[self.player_x.myturn])+\",\"+self.player_o.name+\":\"+str(self.nwon[self.player_o.myturn])\n             +\",DRAW:\"+str(self.nwon[DRAW]))\n\n            \n    def switch_player(self):\n        if self.player_turn == self.player_x:\n            self.player_turn=self.player_o\n        else:\n            self.player_turn=self.player_x\n\n```\n\n# \u8272\u3005\u306a\u30d7\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u6210\n## \u30e9\u30f3\u30c0\u30e0\u541b\n\u7f6e\u3051\u308b\u3068\u3053\u308d\u306b\u7f6e\u304f\u3060\u3051\u306e\u30b7\u30f3\u30d7\u30eb\u306a\u3084\u3064\u3067\u3059\u3002\n\n```py\nimport random\n             \n\nclass PlayerRandom:\n    def __init__(self,turn):\n        self.name=\"Random\"\n        self.myturn=turn\n        \n    def act(self,board):\n        acts=board.get_possible_pos()\n        i=random.randrange(len(acts))\n        return acts[i]\n    \n    \n    def getGameResult(self,board):\n        pass\n\n```\n\n## \u4eba\u9593\n\n```py\nclass PlayerHuman:\n    def __init__(self,turn):\n        self.name=\"Human\"\n        self.myturn=turn\n        \n    def act(self,board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Where would you like to place \" + str(self.myturn) + \" (1-9)? \")\n                act = int(act)\n                #if act >= 1 and act <= 9 and board.board[act-1]==EMPTY:\n                if act >= 1 and act <= 9:\n                    valid=True\n                    return act-1\n                else:\n                    print (\"That is not a valid move! Please try again.\")\n            except Exception as e:\n                    print (act +  \"is not a valid move! Please try again.\")\n        return act\n    \n    def getGameResult(self,board):\n        if board.winner is not None and board.winner!=self.myturn and board.winner!=DRAW:\n            print(\"I lost...\")\n```\n\n\u307e\u305a\u306f\u4eba\u9593\u3068\u30e9\u30f3\u30c0\u30e0\u3067\u6226\u308f\u305b\u3066\u307f\u307e\u3059\u3002\u8ca0\u3051\u305f\u3089\u6065\u3002    \n\n```py\ndef Human_vs_Random():\n    \n    p1=PlayerHuman(PLAYER_X)\n    p2=PlayerRandom(PLAYER_O)\n    game=TTT_GameOrganizer(p1,p2)\n    game.progress()\n\n```\n\n### \u5b9f\u884c\u7d50\u679c\n\n```\nHuman_vs_Random()\nTurn is Random\n   |   |   \n-----------\n   |   | O \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place 1 (1-9)? 1\n X |   |   \n-----------\n   |   | O \n-----------\n   |   |   \nTurn is Random\n X |   |   \n-----------\n O |   | O \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place 1 (1-9)? 2\n X | X |   \n-----------\n O |   | O \n-----------\n   |   |   \nTurn is Random\n X | X |   \n-----------\n O | O | O \n-----------\n   |   |   \nI lost...\nWinner : Random\nHuman:0,Random:1,DRAW:0\n\n```\n\n\u306a\u3093\u3068\u3001\u30ed\u30b0\u306b\u6b8b\u3063\u3066\u3044\u305f\u306e\u306f\u6c17\u3092\u629c\u3044\u3066\u8ca0\u3051\u305f\u4f8b\u3067\u3057\u305f\u3002\n\n\n## \u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b (Alpha Random)\n\u3055\u3059\u304c\u306b\u305f\u3060\u306e\u30e9\u30f3\u30c0\u30e0\u3060\u3068\u3064\u307e\u3089\u306a\u3044\u306e\u3067\u3001\u6b21\u306e\u624b\u3067\u52dd\u3066\u308b\u3068\u3053\u308d\u304c\u3042\u308c\u3070\u304d\u3061\u3093\u3068\u6307\u3059\u3088\u3046\u306b\u3057\u307e\u3059\u3002\uff08\u306a\u3051\u308c\u3070\u30e9\u30f3\u30c0\u30e0\uff09\n\n```py\nclass PlayerAlphaRandom:\n    \n    \n    def __init__(self,turn,name=\"AlphaRandom\"):\n        self.name=name\n        self.myturn=turn\n        \n    def getGameResult(self,winner):\n        pass\n        \n    def act(self,board):\n        acts=board.get_possible_pos()\n        #see only next winnable act\n        for act in acts:\n            tempboard=board.clone()\n            tempboard.move(act,self.myturn)\n            # check if win\n            if tempboard.winner==self.myturn:\n                #print (\"Check mate\")\n                return act\n        i=random.randrange(len(acts))\n        return acts[i]\n\n```\n\n## \u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\n\n\u3055\u3066\u3001\u3053\u3053\u304b\u3089\u304c\u5f37\u5316\u5b66\u7fd2\u9818\u57df\u306b\u5165\u3063\u3066\u304d\u307e\u3059\u3002\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u3067\u306f\u3001\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u306b\u6cbf\u3063\u3066\u4e00\u5b9a\u56de\u6570\u3092\u30b2\u30fc\u30e0\u7d42\u4e86\u307e\u3067\u8a66\u3057\u3001\u305d\u306e\u52dd\u7387\u3092\u898b\u308b\u3053\u3068\u3067\u3044\u3044\u624b\u3092\u63a2\u7d22\u3057\u307e\u3059\u3002\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u306e\u91cd\u8981\u306a\u70b9\u306f\u3001\u300c\u4e00\u5b9a\u56de\u6570\u300d\u306e\u6570\u3068\u3001\u300c\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u300d\u306b\u3088\u3063\u3066\u5f37\u3055\u304c\u5de6\u53f3\u3055\u308c\u308b\u3053\u3068\u3002\n\n\u300c\u3042\u308b\u30dd\u30ea\u30b7\u30fc\u300d\u306b\u306f\u3001\u5148\u307b\u3069\u306e\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b\u3092\u4f7f\u3044\u307e\u3057\u305f\u3002\n\u300c\u4e00\u5b9a\u56de\u6570\u300d\u306f\u8a66\u3057\u305f\u3068\u3053\u308d\uff11\uff10\uff10\u56de\u3092\u8d85\u3048\u308c\u3070\u3001\u307e\u305a\u9593\u9055\u3063\u305f\u624b\u306f\u6253\u305f\u306a\u3044\u3088\u3046\u3067\u3059\u3002\uff15\uff10\u56de\u304f\u3089\u3044\u3060\u3068\u3001\u4e71\u6570\u6b21\u7b2c\u3067\u6642\u3005\u8ca0\u3051\u305f\u308a\u3057\u307e\u3059\u3002\n\n\u3053\u3053\u304b\u3089\u5b9f\u7e3e\u3092\u52d5\u7684\u8a08\u753b\u6cd5\u306b\u6301\u3063\u3066\u3063\u305f\u308a\u3001\u30e2\u30f3\u30c6\u30ab\u30eb\u30c8\u6728\u63a2\u7d22\uff08MCTS)\u306b\u6d3e\u751f\u3057\u3066\u5f37\u3044\u30dd\u30ea\u30b7\u30fc\u3068\u5f31\u3044\u30dd\u30ea\u30b7\u30fc\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u65b9\u6cd5\u3082\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u8a00\u53ca\u3057\u307e\u305b\u3093\u3002\n\n```py\nclass PlayerMC:\n    def __init__(self,turn,name=\"MC\"):\n        self.name=name\n        self.myturn=turn\n    \n    def getGameResult(self,winner):\n        pass\n        \n    def win_or_rand(self,board,turn):\n        acts=board.get_possible_pos()\n        #see only next winnable act\n        for act in acts:\n            tempboard=board.clone()\n            tempboard.move(act,turn)\n            # check if win\n            if tempboard.winner==turn:\n                return act\n        i=random.randrange(len(acts))\n        return acts[i]\n           \n    def trial(self,score,board,act):\n        tempboard=board.clone()\n        tempboard.move(act,self.myturn)\n        tempturn=self.myturn\n        while tempboard.winner is None:\n            tempturn=tempturn*-1\n            tempboard.move(self.win_or_rand(tempboard,tempturn),tempturn)\n        \n        if tempboard.winner==self.myturn:\n            score[act]+=1\n        elif tempboard.winner==DRAW:\n            pass\n        else:\n            score[act]-=1\n\n        \n    def getGameResult(self,board):\n        pass\n        \n    \n    def act(self,board):\n        acts=board.get_possible_pos()\n        scores={}\n        n=50\n        for act in acts:\n            scores[act]=0\n            for i in range(n):\n                #print(\"Try\"+str(i))\n                self.trial(scores,board,act)\n            \n            #print(scores)\n            scores[act]/=n\n        \n        max_score=max(scores.values())\n        for act, v in scores.items():\n            if v == max_score:\n                #print(str(act)+\"=\"+str(v))\n                return act\n\n```\n\n### \u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u540c\u58eb\u3067\u6226\u308f\u305b\u308b\n\n```\np1=PlayerMC(PLAYER_X,\"M1\")\np2=PlayerMC(PLAYER_O,\"M2\")\ngame=TTT_GameOrganizer(p1,p2,10,False)\ngame.progress()\n\nDraw Game\nWinner : M2\nDraw Game\nDraw Game\nDraw Game\nDraw Game\nDraw Game\nWinner : M2\nDraw Game\nDraw Game\nM1:0,M2:2,DRAW:8\n\n```\n\u5927\u4f53\u30c9\u30ed\u30fc\u3067\u3059\u306d\u3002\u6c17\u304c\u5411\u3044\u305f\u3089\u8a66\u884c\u56de\u6570\u3092100\u56de\u306b\u3057\u3066\u8a66\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u5168\u90e8\u30c9\u30ed\u30fc\u306b\u306a\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n## Q-Learning\n\n\u3055\u3066\u3001\u5f37\u5316\u5b66\u7fd2\u3068\u3044\u3048\u3070\u3001Q-Learning\u3002\n\u30b2\u30fc\u30e0\u9014\u4e2d\u306f\u5831\u916c 0 \u3001\u52dd\u3063\u305f\u3089+1\u3001\u8ca0\u3051\u305f\u3089-1\u3001\u5f15\u304d\u5206\u3051\u306f0\u3067\u3059\u3002\n\u305d\u308c\u3092\u30d9\u30fc\u30b9\u306bQ(s,a)\u306eDictionary\u3092\u4ee5\u4e0b\u306e\u66f4\u65b0\u5f0f\u3067\u66f4\u65b0\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n```\nQ(s,a) = Q(s,a) + alpha (reward + gammma* max(Q(s',a')- Q(s,a)) \n```\n\n\u91cd\u8981\u306a\u306e\u306f\u03b5-greedy\u3068\u3044\u3046\u30b3\u30f3\u30bb\u30d7\u30c8\u3067\u30e9\u30f3\u30c0\u30e0\u306e\u6253\u3061\u624b\u3092\u63a8\u5968\u3059\u308b\u3053\u3068\u3002\u3067\u306a\u3044\u3068\u3059\u3050\u306blocal optimum\u306b\u9665\u308a\u307e\u3059\u3002\n\u6700\u521d\u306eQ\u306e\u5024\u3082\u91cd\u8981\u3067\u3059\u3002\u3067\u306a\u3044\u3068\u3001\u3059\u3050\u63a2\u3059\u6c17\u3092\u306a\u304f\u3057\u3061\u3083\u3044\u307e\u3059\u3002\u308f\u304c\u307e\u307e\u306a\u5b50\u3067\u3059\u3002\n\n\u30b2\u30fc\u30e0\u7d42\u4e86\u3067\u306emax(Q)\u3092\u3069\u3046\u3057\u3088\u3046\u3068\u304b\u3001\u30b2\u30fc\u30e0\u9014\u4e2d\u306e\u66f4\u65b0\u3092\u5fd8\u308c\u305f\u308a\u3068\u3001\u610f\u5916\u3068\u306f\u307e\u308a\u307e\u3057\u305f\u3002\n\n```py\nclass PlayerQL:\n    def __init__(self,turn,name=\"QL\",e=0.2,alpha=0.3):\n        self.name=name\n        self.myturn=turn\n        self.q={} #set of s,a\n        self.e=e\n        self.alpha=alpha\n        self.gamma=0.9\n        self.last_move=None\n        self.last_board=None\n        self.totalgamecount=0\n        \n    \n    def policy(self,board):\n        self.last_board=board.clone()\n        acts=board.get_possible_pos()\n        #Explore sometimes\n        if random.random() < (self.e/(self.totalgamecount//10000+1)):\n                i=random.randrange(len(acts))\n                return acts[i]\n        qs = [self.getQ(tuple(self.last_board.board),act) for act in acts]\n        maxQ= max(qs)\n\n        if qs.count(maxQ) > 1:\n            # more than 1 best option; choose among them randomly\n            best_options = [i for i in range(len(acts)) if qs[i] == maxQ]\n            i = random.choice(best_options)\n        else:\n            i = qs.index(maxQ)\n\n        self.last_move = acts[i]\n        return acts[i]\n    \n    def getQ(self, state, act):\n        # encourage exploration; \"optimistic\" 1.0 initial values\n        if self.q.get((state, act)) is None:\n            self.q[(state, act)] = 1\n        return self.q.get((state, act))\n    \n    def getGameResult(self,board):\n        r=0\n        if self.last_move is not None:\n            if board.winner is None:\n                self.learn(self.last_board,self.last_move, 0, board)\n                pass\n            else:\n                if board.winner == self.myturn:\n                    self.learn(self.last_board,self.last_move, 1, board)\n                elif board.winner !=DRAW:\n                    self.learn(self.last_board,self.last_move, -1, board)\n                else:\n                    self.learn(self.last_board,self.last_move, 0, board)\n                self.totalgamecount+=1\n                self.last_move=None\n                self.last_board=None\n\n    def learn(self,s,a,r,fs):\n        pQ=self.getQ(tuple(s.board),a)\n        if fs.winner is not None:\n            maxQnew=0\n        else:\n            maxQnew=max([self.getQ(tuple(fs.board),act) for act in fs.get_possible_pos()])\n        self.q[(tuple(s.board),a)]=pQ+self.alpha*((r+self.gamma*maxQnew)-pQ)\n        #print (str(s.board)+\"with \"+str(a)+\" is updated from \"+str(pQ)+\" refs MAXQ=\"+str(maxQnew)+\":\"+str(r))\n        #print(self.q)\n\n    \n    def act(self,board):\n        return self.policy(board)\n```\n\n### Q-Learning \u540c\u58eb\u306710\u4e07\u56de\u3000\u6226\u308f\u305b\u307e\u3059\u3002\n\n```\npQ=PlayerQL(PLAYER_O,\"QL1\")\np2=PlayerQL(PLAYER_X,\"QL2\")\ngame=TTT_GameOrganizer(pQ,p2,100000,False,False,10000)\ngame.progress()\n\nQL1:4371,QL2:4436,DRAW:1193\nQL1:8328,QL2:8456,DRAW:3216\nQL1:11903,QL2:11952,DRAW:6145\nQL1:14268,QL2:14221,DRAW:11511\nQL1:15221,QL2:15099,DRAW:19680\nQL1:15730,QL2:15667,DRAW:28603\nQL1:16136,QL2:16090,DRAW:37774\nQL1:16489,QL2:16439,DRAW:47072\nQL1:16832,QL2:16791,DRAW:56377\nQL1:17128,QL2:17121,DRAW:65751\n```\n\n\u5927\u4f53\u30c9\u30ed\u30fc\u306b\u53ce\u675f\u3057\u3066\u3044\u307e\u3059\u306d\u3002\u03b5\u3067\u52dd\u6557\u304c\u5206\u304b\u308c\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\n###\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u3068\u6226\u308f\u305b\u308b\n\u03b5\u3092\u30bc\u30ed\u306b\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u8981\u7d20\u3092\u306a\u304f\u3057\u3066\u6700\u826f\u306e\u6253\u3061\u624b\u3067\u653b\u3081\u3066\u307f\u307e\u3059\n\n```\npQ.e=0\np2=PlayerMC(PLAYER_X,\"M1\")\ngame=TTT_GameOrganizer(pQ,p2,100,False,False,10)\ngame.progress()\nQL1:1,M1:0,DRAW:9\nQL1:1,M1:0,DRAW:19\nQL1:2,M1:0,DRAW:28\nQL1:2,M1:0,DRAW:38\nQL1:3,M1:0,DRAW:47\nQL1:4,M1:0,DRAW:56\nQL1:5,M1:0,DRAW:65\nQL1:5,M1:0,DRAW:75\nQL1:6,M1:0,DRAW:84\nQL1:6,M1:0,DRAW:94\n```\n\n\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u3082\u6642\u3005\u8ca0\u3051\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3084\u306f\u308a\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\u541b\u306f\u4e71\u6570\u306e\u6c17\u307e\u3050\u308c\u304b\u3089\u9003\u308c\u3089\u308c\u307e\u305b\u3093\u3002\n\u3061\u306a\u307f\u306b\u3053\u306e\u72b6\u614b\u306eQ-Learning\u541b\u3068\u6226\u3046\u3068\u3001\u304b\u306a\u308a\u75db\u3044\u76ee\u306b\u3042\u3044\u307e\u3059\u3002\n\n## DQN (Deep Q Network)\n\n\u3055\u3066\u4eca\u56de\u306e\u672c\u984c\u3002DQN\u3067\u3059\u3002\nQ-Learning\u306e\u30cd\u30c3\u30af\u306fQ(s,a)\u306e\u30c6\u30fc\u30d6\u30eb\u304c\u81a8\u5927\u306b\u306a\u308b\u3053\u3068\u3002\u4e09\u76ee\u4e26\u3079\u306a\u3089\u3044\u3044\u3051\u3069\u3001\u7881\u3067\u306fS\u3082a\u3082\u83ab\u5927\u306a\u30e1\u30e2\u30ea\u3068\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002\nQ-Network\u3067\u306fQ(s)\u306b\u3088\u308a\u5404a\u306e\u671f\u5f85\u5831\u916c\u3092\u51fa\u3057\u3066\u3001argmax\u3092\u53d6\u308b\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\nDeep Q Network\u306a\u3089\u3000\u300c\u52dd\u624b\u306b\u52dd\u3064\u7279\u5fb4\u3092\u307f\u3064\u3051\u3061\u3083\u3046\u3093\u3058\u3083\u306a\u3044\uff01\uff1f\u300d\u30011000\u56de\u4f4d\u306e\u5bfe\u6226\u3067\u3044\u3044\u3093\u3058\u3083\u306a\u3044\uff1f\u3068\u305f\u304b\u3092\u304f\u304f\u3063\u3066\u307e\u3057\u305f\u3002\n\n\u30fb\u30fb\u30fb\u30fb\u304c\u3001\u30c9\u306f\u307e\u308a\u3057\u307e\u3057\u305f\u3002\n\n\u5168\u7136\u5f37\u304f\u306a\u3089\u306a\u3044\u3002\u3057\u304b\u3082Hyper Parameter\u304c\u591a\u3059\u304e\u308b\u3002Hidden Layer\u4f55\u500b\u306b\u3059\u308c\u3070\u3044\u3044\u306e\uff1fRelu? \u8aa4\u5dee\u306fSME\u3067\u3044\u3044\u306e\uff1fOptimizer\u306fAdam\u304bSGD?\u3000\u6b63\u76f4Grid Search\u304c\u307b\u3057\u304b\u3063\u305f\u3067\u3059\u3002\u3067\u3082\u305d\u3082\u305d\u3082\u6559\u5e2b\u304c\u306a\u3044\u306e\u3067\u30ed\u30b9\u306e\u6e2c\u5b9a\u3082\u3067\u304d\u306a\u3044\u3093\u3067\u3059\u3088\u306d\u3002\u7cbe\u5ea6\u306f\u3001\u76f8\u624b\u3068\u6226\u308f\u305b\u3066\u521d\u3081\u3066\u308f\u304b\u308b\u3002\n\n\u3055\u3089\u306b\u3001\u5b66\u7fd2\u3055\u305b\u3066\u3044\u3066\u3082\u3001Variable Link\u304c\u5207\u308c\u3066\u5168\u7136\u5b66\u7fd2\u3057\u306a\u304b\u3063\u305f\u308a\u3002\u307b\u3093\u3068\u30cf\u30de\u308a\u307e\u3059\u3002\n\u3057\u304b\u3082GPU\u306e\u306a\u3044\u30de\u30b7\u30f3\u3067\u3044\u308d\u3044\u308d\u3084\u3063\u3066\u305f\u306e\u3067\u9045\u3044\u3002\u3002\u3002GPU\u306a\u3044\u3068\u5b66\u7fd2\u306b5-10\u5206\u304f\u3089\u3044\u304b\u304b\u3063\u305f\u308a\u3057\u307e\u3059\u3002\n\n\n```py\nimport chainer\n\nfrom chainer import Function, gradient_check, Variable, optimizers, serializers, utils\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\nfrom chainer import computational_graph as c\n\n# Network definition\nclass MLP(chainer.Chain):\n\n    def __init__(self, n_in, n_units, n_out):\n        super(MLP, self).__init__(\n            l1=L.Linear(n_in, n_units),  # first layer\n            l2=L.Linear(n_units, n_units),  # second layer\n            l3=L.Linear(n_units, n_units),  # Third layer\n            l4=L.Linear(n_units, n_out),  # output layer\n        )\n\n    def __call__(self, x, t=None, train=False):\n        h = F.leaky_relu(self.l1(x))\n        h = F.leaky_relu(self.l2(h))\n        h = F.leaky_relu(self.l3(h))\n        h = self.l4(h)\n\n        if train:\n            return F.mean_squared_error(h,t)\n        else:\n            return h\n\n    def get(self,x):\n        # input x as float, output float\n        return self.predict(Variable(np.array([x]).astype(np.float32).reshape(1,1))).data[0][0]\n\n\nclass DQNPlayer:\n    def __init__(self, turn,name=\"DQN\",e=1,dispPred=False):\n        self.name=name\n        self.myturn=turn\n        self.model = MLP(9, 162,9)\n        self.optimizer = optimizers.SGD()\n        self.optimizer.setup(self.model)\n        self.e=e\n        self.gamma=0.95\n        self.dispPred=dispPred\n        self.last_move=None\n        self.last_board=None\n        self.last_pred=None\n        self.totalgamecount=0\n        self.rwin,self.rlose,self.rdraw,self.rmiss=1,-1,0,-1.5\n        \n    \n    def act(self,board):\n        \n        self.last_board=board.clone()\n        x=np.array([board.board],dtype=np.float32).astype(np.float32)\n        \n        pred=self.model(x)\n        if self.dispPred:print(pred.data)\n        self.last_pred=pred.data[0,:]\n        act=np.argmax(pred.data,axis=1)\n        if self.e > 0.2: #decrement epsilon over time\n            self.e -= 1/(20000)\n        if random.random() < self.e:\n            acts=board.get_possible_pos()\n            i=random.randrange(len(acts))\n            act=acts[i]\n        i=0\n        while board.board[act]!=EMPTY:\n            #print(\"Wrong Act \"+str(board.board)+\" with \"+str(act))\n            self.learn(self.last_board,act, -1, self.last_board)\n            x=np.array([board.board],dtype=np.float32).astype(np.float32)\n            pred=self.model(x)\n            #print(pred.data)\n            act=np.argmax(pred.data,axis=1)\n            i+=1\n            if i>10:\n                print(\"Exceed Pos Find\"+str(board.board)+\" with \"+str(act))\n                acts=self.last_board.get_possible_pos()\n                act=acts[random.randrange(len(acts))]\n            \n        self.last_move=act\n        #self.last_pred=pred.data[0,:]\n        return act\n    \n    def getGameResult(self,board):\n        r=0\n        if self.last_move is not None:\n            if board.winner is None:\n                self.learn(self.last_board,self.last_move, 0, board)\n                pass\n            else:\n                if board.board== self.last_board.board:            \n                    self.learn(self.last_board,self.last_move, self.rmiss, board)\n                elif board.winner == self.myturn:\n                    self.learn(self.last_board,self.last_move, self.rwin, board)\n                elif board.winner !=DRAW:\n                    self.learn(self.last_board,self.last_move, self.rlose, board)\n                else:                    #DRAW\n                    self.learn(self.last_board,self.last_move, self.rdraw, board)\n                self.totalgamecount+=1\n                self.last_move=None\n                self.last_board=None\n                self.last_pred=None\n\n    def learn(self,s,a,r,fs):\n        if fs.winner is not None:\n            maxQnew=0\n        else:\n            x=np.array([fs.board],dtype=np.float32).astype(np.float32)\n            maxQnew=np.max(self.model(x).data[0])\n        update=r+self.gamma*maxQnew\n        #print(('Prev Board:{} ,ACT:{}, Next Board:{}, Get Reward {}, Update {}').format(s.board,a,fs.board,r,update))\n        #print(('PREV:{}').format(self.last_pred))\n        self.last_pred[a]=update\n        \n        x=np.array([s.board],dtype=np.float32).astype(np.float32)\n        t=np.array([self.last_pred],dtype=np.float32).astype(np.float32)\n        self.model.zerograds()\n        loss=self.model(x,t,train=True)\n        loss.backward()\n        self.optimizer.update()\n        #print(('Updated:{}').format(self.model(x).data))\n        #print (str(s.board)+\"with \"+str(a)+\" is updated from \"+str(pQ)+\" refs MAXQ=\"+str(maxQnew)+\":\"+str(r))\n        #print(self.q)\n\n```\n\n\u30bd\u30fc\u30b9\u9577\u3044\u3067\u3059\u306d\u3047\u3002\u3002\u3002\u305f\u3076\u3093\u8a66\u884c\u932f\u8aa4\u306e\u5f62\u8de1\u304c\u6b8b\u3063\u3066\u3044\u308b\u6c17\u304c\u3057\u307e\u3059\u304c\u3001\u304d\u308c\u3044\u306b\u3057\u3066\u3044\u307e\u305b\u3093\u3002\n\n### \u6539\u826f\u30e9\u30f3\u30c0\u30e0\u3068\u5bfe\u6226\n\u3053\u308c\u3067\u307e\u305a\u306f\u3056\u3063\u304f\u308a\u6253\u3061\u65b9\u3068\u52dd\u3061\u65b9\u3092\u899a\u3048\u3055\u305b\u307e\u3059\u3002\n\n\n```\npDQ=DQNPlayer(PLAYER_X)\np2=PlayerAlphaRandom(PLAYER_O)\ngame=TTT_GameOrganizer(pDQ,p2,20000,False,False,1000)\ngame.progress()\nDQN:206,AlphaRandom:727,DRAW:67\nDQN:468,AlphaRandom:1406,DRAW:126\nDQN:861,AlphaRandom:1959,DRAW:180\nDQN:1458,AlphaRandom:2315,DRAW:227\nDQN:2185,AlphaRandom:2560,DRAW:255\nDQN:3022,AlphaRandom:2704,DRAW:274\nDQN:3832,AlphaRandom:2856,DRAW:312\nDQN:4632,AlphaRandom:3023,DRAW:345\nDQN:5481,AlphaRandom:3153,DRAW:366\nDQN:6326,AlphaRandom:3280,DRAW:394\nDQN:7181,AlphaRandom:3400,DRAW:419\nDQN:8032,AlphaRandom:3522,DRAW:446\nDQN:8902,AlphaRandom:3618,DRAW:480\nDQN:9791,AlphaRandom:3705,DRAW:504\nDQN:10673,AlphaRandom:3793,DRAW:534\nDQN:11545,AlphaRandom:3893,DRAW:562\nDQN:12420,AlphaRandom:3986,DRAW:594\nDQN:13300,AlphaRandom:4074,DRAW:626\nDQN:14183,AlphaRandom:4158,DRAW:659\nDQN:15058,AlphaRandom:4246,DRAW:696\n```\n\n\u3060\u3044\u3076\u5f37\u304f\u306a\u308a\u307e\u3057\u305f\u306d\u3002\u3055\u3059\u304c\u306b\u6539\u826f\u30e9\u30f3\u30c0\u30e0\u541b\u306b\u306f\u52dd\u3066\u308b\u3088\u3046\u3067\u3002\n\n### \u672c\u547d Q-Learning\u3068\u5bfe\u6c7a\n\n```\npDQ.e=1\ngame=TTT_GameOrganizer(pDQ,pQ,30000,False,False,1000)\ngame.progress()\n\nDQN:4,QL1:436,DRAW:560\nDQN:6,QL1:790,DRAW:1204\nDQN:6,QL1:1135,DRAW:1859\nDQN:6,QL1:1472,DRAW:2522\nDQN:6,QL1:1801,DRAW:3193\nDQN:6,QL1:2123,DRAW:3871\nDQN:6,QL1:2439,DRAW:4555\nDQN:6,QL1:2777,DRAW:5217\nDQN:7,QL1:3128,DRAW:5865\nDQN:9,QL1:3648,DRAW:6343\nDQN:13,QL1:4132,DRAW:6855\nDQN:13,QL1:4606,DRAW:7381\nDQN:13,QL1:5087,DRAW:7900\nDQN:14,QL1:5536,DRAW:8450\nDQN:14,QL1:6011,DRAW:8975\nDQN:14,QL1:6496,DRAW:9490\nDQN:14,QL1:7394,DRAW:9592\nDQN:14,QL1:7925,DRAW:10061\nDQN:16,QL1:8357,DRAW:10627\nDQN:16,QL1:8777,DRAW:11207\n```\n\n\u3088\u3046\u3084\u304f\u5f37\u304f\u306a\u3063\u305f\u306e\u304b\u306a\u3002\u3002\u3002\u306a\u3093\u3060\u304b\u5fae\u5999\u306a\u5f37\u3055\u3067\u3059\u304c\u3002\u3002\n\u3053\u306e\u30ed\u30b0\u3067\u306f\u307e\u3060\u8ca0\u3051\u304c\u591a\u3044\u3067\u3059\u304c\u300110\u4e07\u56de\u5bfe\u6226\u4f4d\u3059\u308b\u3068\u5927\u4f53\u30c9\u30ed\u30fc\u306b\u843d\u3061\u7740\u304d\u307e\u3057\u305f\u3002\n\n\n### \u6700\u5f8c\u306b\u4eba\u9593\u3068\u5bfe\u6226\u3002\u03b5\u306f\u30bc\u30ed\n\n```\npDQ.e=0\np2=PlayerHuman(PLAYER_O)\ngame=TTT_GameOrganizer(pDQ,p2,2)\ngame.progress()\nTurn is Human\nWhere would you like to place -1 (1-9)? 1\n//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:69: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n\n O |   |   \n-----------\n   |   |   \n-----------\n   |   |   \nTurn is DQN\n O |   |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 2\n O | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is DQN\n O | O | X \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n   | X |   \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X |   \n-----------\n O |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 6\n O | O | X \n-----------\n X | X | O \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nI lost...\nInvalid Move!\nTurn is Human\nWhere would you like to place -1 (1-9)? 2\n   | O |   \n-----------\n   |   |   \n-----------\n   |   |   \nTurn is DQN\n   | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 1\n O | O |   \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is DQN\n O | O | X \n-----------\n   | X |   \n-----------\n   |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 7\n O | O | X \n-----------\n   | X |   \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X |   \n-----------\n O |   |   \nTurn is Human\nWhere would you like to place -1 (1-9)? 6\n O | O | X \n-----------\n X | X | O \n-----------\n O |   |   \nTurn is DQN\n O | O | X \n-----------\n X | X | O \n-----------\n O |   | X \nTurn is Human\nWhere would you like to place -1 (1-9)? 8\n O | O | X \n-----------\n X | X | O \n-----------\n O | O | X \nDraw Game\nDQN:1,Human:0,DRAW:1\n```\n\n\u307e\u3041\u305d\u308c\u3089\u3057\u3044\u6253\u3061\u624b\u306b\u306f\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n\n# \u7d50\u8ad6\n\nQ-Learning\u306f\u5b9f\u88c5\u306f\u304b\u306a\u308a\u697d\u3067\u3057\u305f\u304c\u3001Deep Q Network\u3067\u306f\u8a66\u884c\u932f\u8aa4\u3057\u306a\u3044\u3068\u6c7a\u3081\u3089\u308c\u306a\u3044\u3053\u3068\u304c\u975e\u5e38\u306b\u591a\u304b\u3063\u305f\u3067\u3059\u3002\n\u6700\u521dRelu\u3067\u5b9f\u88c5\u3057\u3066\u3082\u5168\u7136\u5f37\u304f\u306a\u3089\u305a\u3001Leaky Relu\u306b\u3057\u3066\u3088\u304f\u306a\u308a\u307e\u3057\u305f\u3002\u305f\u3076\u3093\u76e4\u9762\u306b-1\u304c\u5165\u308b\u306e\u3067Relu\u3067\u306f\u3046\u307e\u304f\u8868\u73fe\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u304b\u306a\u3002\n\u3042\u3068\u3001\u306a\u305c\u304bAdam\u3067\u306f\u3060\u3081\u3067SGD\u3067\u306f\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u4e0d\u601d\u8b70\u3067\u3059\u3002Adam\u6700\u5f37\u3068\u52dd\u624b\u306b\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\u3055\u3089\u306bAlphaRandom\u3068\u5bfe\u6226\u3055\u305b\u306a\u3044\u3067\u3001\u6700\u521d\u304b\u3089Q-Learning(\u5f37\u3044\u3084\u3064\uff09\u3068\u6226\u308f\u3055\u305b\u308b\u3068\u3001\u3059\u3050\u306b\u3084\u308b\u6c17\u3092\u306a\u304f\u3057\u3066\u3001\u8ca0\u3051\u3063\u3071\u306a\u3057\u306b\u306a\u3063\u305f\u308a\u3057\u307e\u3059\u3002\u6700\u521d\u306f\u5f31\u3044\u3084\u3064\u3068\u6226\u308f\u305b\u3066\u3001\u3084\u308b\u6c17\u3092\u51fa\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3088\u3046\u3067\u3059\u3002\n\n\n\u307e\u305f\u3001\u4f55\u3088\u308a\u82e6\u52b4\u3057\u305f\u306e\u304c\n\n- \u3069\u3046\u3084\u3063\u3066\u3001DQN\u306b\u6253\u3063\u3066\u306f\u3044\u3051\u306a\u3044\u624b\u3092\u6559\u3048\u308b\u304b\n\n\u3068\u3044\u3046\u70b9\u3067\u3059\u3002\n\n\u4eca\u56de\u306f\u65e2\u306b\u30b3\u30de\u306e\u3042\u308b\u3068\u3053\u308d\u306b\u6253\u3063\u305f\u3089\u3001\u5f37\u5236\u7684\u306b\u30de\u30a4\u30ca\u30b9\u5831\u916c\u3092\u4e0e\u3048\u306610\u56de\u5b66\u7fd2\u3068\u3044\u3046\u3001\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3044\u308c\u3066\u3046\u307e\u304f\u3044\u304d\u307e\u3057\u305f\u3002\u6253\u3063\u3066\u3088\u3044\u5834\u6240\u3092\u5165\u529b\u306b\u5165\u308c\u308c\u3070\u3044\u3044\u306e\u304b\u306a\uff1f\n\n\u7591\u554f\u3084\u6539\u5584\u3057\u305f\u3044\u3068\u3053\u308d\u304c\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3082\u3057\u308f\u304b\u308b\u4eba\u3044\u305f\u3089\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\u30a2\u30c9\u30d0\u30a4\u30b9\u3084\u6307\u6458\u3001\u3069\u3093\u3069\u3093\u30a6\u30a7\u30eb\u30ab\u30e0\u3067\u3059\u3002\n\n\u305d\u308c\u306b\u3057\u3066\u3082\u3001\u4e09\u76ee\u4e26\u3079\u306a\u30892-3\u6642\u9593\u304f\u3089\u3044\u3042\u308c\u3070\u3067\u304d\u308b\u3060\u308d\u3046\u3001\u3068\u305f\u304b\u3092\u304f\u304f\u3063\u3066\u3044\u305f\u306e\u306b\u3001\u601d\u3044\u306e\u307b\u304b\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3057\u305f\u3002\n\u77e5\u3063\u3066\u3044\u308b\u3001\u5b9f\u88c5\u3067\u304d\u308b\uff08\u30b3\u30fc\u30c9\u66f8\u3051\u308b\uff09\u3001\u3067\u304d\u308b\uff08\u304d\u3061\u3093\u3068\u52d5\u304f\uff09\u3000\u306b\u306f\u3068\u3093\u3067\u3082\u306a\u3044\u30ae\u30e3\u30c3\u30d7\u304c\u3042\u308b\u3053\u3068\u3092\u75db\u611f\u3002\u305c\u3072\u7686\u3055\u3093\u3082\u4eca\u56de\u306e\u30bd\u30fc\u30b9\u306f\u3042\u307e\u308a\u898b\u305a\u306b\u3001\u3054\u81ea\u8eab\u3067\u5b9f\u88c5\u3055\u308c\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n\n\u3055\u3066\u3001\u6642\u9593\u306e\u3042\u308b\u3068\u304d\u306b\u30aa\u30bb\u30ed\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u4f5c\u308b\u304b\u306a\u3041\u3002\u3002\u3002\n\u305d\u308c\u304b\u304d\u3061\u3093\u3068AlphaGo\u306e3NN\u69cb\u6210\u3092\u8a66\u3057\u3066\u307f\u3088\u3046\u304b\u3001\u6b21\u306e\u8ab2\u984c\u306b\u60a9\u307f\u4e2d\u3067\u3059\u3002\n", "tags": ["Python", "Chainer", "MachineLearning", "ReinforcementLearning"]}