{"tags": ["hadoop", "YARN", "Spark", "Scala"], "context": "\n\n\u306f\u3058\u3081\u306b\n\u30bf\u30a4\u30c8\u30eb\u306e\u901a\u308aYARN\u4e0a\u3067Spark\u306e\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u52d5\u304b\u3057\u3066\u307f\u307e\u3057\u305f\uff0e\n\u7d50\u679c\u304c\u51fa\u305f\u306e\u3067\u52d5\u3044\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u304c\uff0c\u8a66\u884c\u932f\u8aa4\u3057\u305f\u3089\u3067\u304d\u305f\u3063\u3066\u611f\u3058\u3067\u3059\uff0e\n\u77e5\u8b58\u307b\u307c0\u3067\u6311\u3093\u3060\u7d50\u679c\u306e\u307e\u3068\u3081\u306a\u306e\u3067\uff0c\u30e1\u30e2\u66f8\u304d\u7a0b\u5ea6\u3060\u3068\u601d\u3063\u3066\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\uff0e\nYARN\u3063\u3066\u4f55\uff1fSpark\u3063\u3066\u4f55\uff1f\u3063\u3066\u65b9\u306f\u4e0b\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u79c1\u3082\u5206\u304b\u3063\u3066\u307e\u305b\u3093\uff09\n\u53c2\u8003\u8a18\u4e8b\nYARN\u306e\u7d39\u4ecb-IBM\n\u7b2c15\u56de \u8a08\u7b97\u6a5f\u30af\u30e9\u30b9\u30bf\u306e\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9\u7ba1\u7406\u57fa\u76e4 Hadoop YARN\n\u5206\u6563\u51e6\u7406\u306b\u5165\u9580\u3057\u3066\u307f\u305f\uff08Hadoop+Spark\uff09\nApache Spark\u3092\u52c9\u5f37\u3057\u3066\u5206\u6563\u51e6\u7406\u3067\u304d\u307e\u3059\u3088\uff01\u3063\u3066\u8a00\u3048\u308b\u3088\u3046\u306b\u306a\u308b\n\n\u5185\u5bb9\n\u30fb \u74b0\u5883\u69cb\u7bc9\uff08Hadoop, Spark\u306e\u30d3\u30eb\u30c9\u304a\u3088\u3073conf\u8a2d\u5b9a\uff09\n\u30fb SparkContext\u3092\u4f7f\u3063\u3066\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\n\n\u74b0\u5883\n\u30fb Ubuntu 14.04\n\u30fb Java 1.7.0_95\n\u30fb Hadoop 2.7.1\n\u30fb Spark 1.6.1\n\u30fb maven 3.3.9\n\u30fb Scala 2.11.7\n\u30fb sbt 0.13.7\n\nHadoop\u74b0\u5883\u8a2d\u5b9a\n< \u30d3\u30eb\u30c9 >\nhttp://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/hadoop-2.7.1-src.tar.gz \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3092\u89e3\u51cd\u3057\u3066\u30d3\u30eb\u30c9\u3057\u307e\u3057\u305f\uff0e\n\u30d3\u30eb\u30c9\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u306fhadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1\u306b\u3042\u308a\u307e\u3059\uff0e\n\u672c\u8a18\u4e8b\u3067\u306f\uff0chadoop-2.7.1\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ${HADOOP_HOME} \u3068\u3057\u307e\u3059\uff0e\nwget http://ftp.meisei-u.ac.jp/mirror/apache/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1-src.tar.gz\ntar xvfz hadoop-2.7.1-src.tar.gz\ncd hadoop-2.7.1-src\nmvn compile\n\n< conf\u8a2d\u5b9a >\nhttp://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html \u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\uff0e\n\u4e0a\u306e\u8a18\u4e8b\u901a\u308a\u306b\u3084\u308c\u3070Hadoop\u306e\u8a2d\u5b9a\u306f\u7d42\u308f\u308a\u3067\u3059\uff0e\n\nSpark\u74b0\u5883\u8a2d\u5b9a\n< \u30d3\u30eb\u30c9 >\ngit://github.com/apache/spark.git \u304b\u3089\u30af\u30ed\u30fc\u30f3\u3057\u3066\u30d3\u30eb\u30c9\u3057\u307e\u3057\u305f\uff0e\u30d6\u30e9\u30f3\u30c1\u306f1.6\u3067\u3059\uff0e\n\u672c\u8a18\u4e8b\u3067\u306f\uff0cspark\u30c7\u30a3\u30af\u30c8\u30ea\u3092 ${SPARK_HOME} \u3068\u3057\u307e\u3059\uff0e\ngit clone git://github.com/apache/spark.git\ncd spark\ngit checkout branch-1.6\nmvn -Pyarn -Phadoop-2.6 -Dscala-2.11 -DskipTests clean package\n\nhadoop\u306f2.7.1\u306a\u306e\u3067 -Phadoop-2.6\nscala\u306f2.11.7\u306a\u306e\u3067 -Dscala-2.11\n\u3068\u3057\u3066\u3044\u307e\u3059\uff0e\n\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u7570\u306a\u308b\u65b9\u306f http://spark.apache.org/docs/latest/building-spark.html \u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n\n\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\n\nspark-submit\u306b\u6e21\u3059\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u306ejar\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210\n\u672c\u8a18\u4e8b\u3067\u306f\uff0cspark_test\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ${SPARK_TEST_HOME} \u3068\u3057\u307e\u3059\uff0e\nsbt assembly\u3067 ${SPARK_TEST_HOME}/target/scala-2.11/spark_test-assembly-1.0.jar \u304c\u3067\u304d\u307e\u3059\uff0e\n/spark_test   \n  |- build.sbt\n  |- project/\n       |- plugins.sbt\n  |- src/\n     |- resources/\n        |- wordcount_input.txt\n     |- main/\n        |- scala/\n           |- WordCount.scala\n\n\nbuild.sbt\nname := \"spark_test\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" % \"spark-core_2.11\" % \"1.6.1\",\n  \"org.apache.spark\" % \"spark-streaming_2.11\" % \"1.6.1\"\n)\n\n// Merge Strategy\nassemblyMergeStrategy in assembly := {\n  case PathList(\"javax\", \"servlet\", xs @ _*)         => MergeStrategy.first\n  case PathList(ps @ _*) if ps.last endsWith \".class\" => MergeStrategy.first\n  case \"application.conf\"                            => MergeStrategy.concat\n  case \"unwanted.txt\"                                => MergeStrategy.discard\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n\n\n\nplugins.sbt\nlogLevel := Level.Warn\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n\n\n\nWordCount.scala\npackage examples\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\n\nobject WordCount extends Logging {\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"spark_test\")\n    val sc = new SparkContext(conf)\n    val files = sc.textFile(\"hdfs://localhost:9000/data/wordcount_input.txt\")\n    val words = files.flatMap(_.split(\" \"))\n    val wordCounts = words.map(s => (s, 1)).reduceByKey(_ + _)\n    wordCounts.saveAsTextFile(\"hdfs://localhost:9000/result\")\n  }\n}\n\n\nwordcount_input.txt\u306f http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip \u304b\u3089\u62dd\u501f\u3057\u307e\u3057\u305f\uff0e\nwget http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip\nunzip Hadoop-WordCount.zip\n    Archive:  Hadoop-WordCount.zip\n    creating: Hadoop-WordCount/\n    creating: Hadoop-WordCount/classes/\n    creating: Hadoop-WordCount/input/\n    inflating: Hadoop-WordCount/input/Word_Count_input.txt\n    inflating: Hadoop-WordCount/WordCount.java\n    inflating: Hadoop-WordCount/clean.sh\n    inflating: Hadoop-WordCount/build.sh\n    inflating: Hadoop-WordCount/classes/WordCount$Reduce.class\n    inflating: Hadoop-WordCount/classes/WordCount.class\n    inflating: Hadoop-WordCount/classes/WordCount$Map.class\n    inflating: Hadoop-WordCount/wordcount.jar\n\n\n./resources/wordcount_input.txt\nAFTER such a scene as the last, Walter Morel was for some days abashed\nand ashamed, but he soon regained his old bullying indifference.\nYet there was a slight shrinking, a diminishing in his assurance.\nPhysically even, he shrank, and his fine full presence waned.\nHe never grew in the least stout, so that, as he sank from his erect,\nassertive bearing, his physique seemed to contract along with his pride\nand moral strength.\n\nBut now he realised how hard it was for his wife to drag\nabout at her work, and, his sympathy quickened by penitence,\nhastened forward with his help. He came straight home from the pit,\n...\n\n\n\u53c2\u8003\u8a18\u4e8b\nSpark1.5\u3067SparkStreaming\u958b\u767a [\u3053\u3068\u59cb\u3081\u7de8]\nWord Count (Spark, YARN, HDFS)\n\n\u5b9f\u884c\n1.Hadoop\u3092\u7acb\u3061\u4e0a\u3052\uff0cHDFS\u306b\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u306e\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u7f6e\u304d\u307e\u3059\uff0e\n${HADOOP_HOME}/bin/hdfs namenode -format\n${HADOOP_HOME}/sbin/start-dfs.sh\n${HADOOP_HOME}/bin/hdfs dfs -mkdir hdfs://localhost:9000/data\n${HADOOP_HOME}/bin/hdfs dfs -put ${SPARK_TEST_HOME}/src/main/resources/wordcount_input.txt hfs://localhost:9000/data\n${HADOOP_HOME}/sbin/start-yarn.sh\n\n2.jar\u30d5\u30a1\u30a4\u30eb\u3092spark-submit\u306b\u6e21\u3057\uff0c\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u3092\u5b9f\u884c\u3057\u307e\u3059\uff0e\n${SPARK_HOME}/bin/spark-submit --class examples.WordCount --master yarn-client --num-executors 1 --driver-memory 2g --executor-memory 1g --executor-cores 1 ${SPARK_TEST_HOME}/target/scala-2.11/spark_test-assembly-1.0.jar\n\n3.\u7d50\u679c\u3092\u78ba\u8a8d\u3057\uff0cwordcount_result.txt\u306b\u4fdd\u5b58\u3057\u307e\u3059\uff0e\n${HADOOP_HOME}/bin/hdfs dfs -ls hdfs://localhost:9000/result\n    # ls\u306e\u7d50\u679c\u3067\u3059\uff0epart-*\u306b\u7d50\u679c\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\n    hdfs://localhost:9000/result/_SUCCESS\n    hdfs://localhost:9000/result/part-00000\n    hdfs://localhost:9000/result/part-00001\n\ntouch (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n${HADOOP_HOME}/bin/hdfs dfs -cat hdfs://localhost:9000/result/part-00000 >> (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n${HADOOP_HOME}/bin/hdfs dfs -cat hdfs://localhost:9000/result/part-00001 >> (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n\n\u53c2\u8003\u8a18\u4e8b\nYARN\u4e0a\u3067\u30b8\u30e7\u30d6\u3092\u8d70\u3089\u305b\u3066\u307f\u308b\uff08Spark\u7de8\uff09\nApache Spark\u3092YARN\u4e0a\u3067\u52d5\u304b\u3057\u3066\u307f\u308b\n\n\u7d50\u679c\n\u4e0b\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3057\u305f\uff0e\n\nwordcount_result.txt\n(simply,,1)\n(bone,1)\n(roses.,3)\n(Whatstandwell.,1)\n(stuck,,1)\n(bananas.,1)\n(letter:,1)\n(insufferably,1)\n(derniers,1)\n(hem,1)\n(sweat.,1)\n(think,\",1)\n(wasn't,7)\n(been,85)\n(they,,2)\n(PAUL,2)\n(jelly,\",1)\n(does---\",1)\n(pig,1)\n(crying,5)\n(soon;,1)\n(Sunday,\",1)\n(breath,2)\n(knows,4)\n(so's,2)\n(whistled,,1)\n(ignore,1)\n(Western,7)\n(smooth,2)\n(BLOWN,1)\n...\n\n\n\n\u304a\u308f\u308a\u306b\nYARN\u4e0a\u3067SPARK\u3092\u4f7f\u3063\u3066\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u3067\u304d\u307e\u3057\u305f\uff08\u591a\u5206\uff09\n\u5206\u304b\u3063\u3066\u306a\u3044\u3053\u3068\u3070\u304b\u308a\u306a\u306e\u3067\uff0c\u30b3\u30e1\u30f3\u30c8\u8cb0\u3048\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\uff0e\n# \u306f\u3058\u3081\u306b\n\n\u30bf\u30a4\u30c8\u30eb\u306e\u901a\u308aYARN\u4e0a\u3067Spark\u306e\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u52d5\u304b\u3057\u3066\u307f\u307e\u3057\u305f\uff0e\n\u7d50\u679c\u304c\u51fa\u305f\u306e\u3067\u52d5\u3044\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u304c\uff0c\u8a66\u884c\u932f\u8aa4\u3057\u305f\u3089\u3067\u304d\u305f\u3063\u3066\u611f\u3058\u3067\u3059\uff0e\n\u77e5\u8b58\u307b\u307c0\u3067\u6311\u3093\u3060\u7d50\u679c\u306e\u307e\u3068\u3081\u306a\u306e\u3067\uff0c\u30e1\u30e2\u66f8\u304d\u7a0b\u5ea6\u3060\u3068\u601d\u3063\u3066\u8aad\u3093\u3067\u304f\u3060\u3055\u3044\uff0e\nYARN\u3063\u3066\u4f55\uff1fSpark\u3063\u3066\u4f55\uff1f\u3063\u3066\u65b9\u306f\u4e0b\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u79c1\u3082\u5206\u304b\u3063\u3066\u307e\u305b\u3093\uff09\n\n\u53c2\u8003\u8a18\u4e8b\n[YARN\u306e\u7d39\u4ecb-IBM](https://www.ibm.com/developerworks/jp/analytics/library/bd-yarn-intro/)\n[\u7b2c15\u56de \u8a08\u7b97\u6a5f\u30af\u30e9\u30b9\u30bf\u306e\u305f\u3081\u306e\u30ea\u30bd\u30fc\u30b9\u7ba1\u7406\u57fa\u76e4 Hadoop YARN](http://gihyo.jp/admin/serial/01/how_hadoop_works/0015)\n[\u5206\u6563\u51e6\u7406\u306b\u5165\u9580\u3057\u3066\u307f\u305f\uff08Hadoop+Spark\uff09](http://www.casleyconsulting.co.jp/blog-engineer/\u5206\u6563\u51e6\u7406/\u5206\u6563\u51e6\u7406\u306b\u5165\u9580\u3057\u3066\u307f\u305f\uff08hadoop-spark\uff09/)\n[Apache Spark\u3092\u52c9\u5f37\u3057\u3066\u5206\u6563\u51e6\u7406\u3067\u304d\u307e\u3059\u3088\uff01\u3063\u3066\u8a00\u3048\u308b\u3088\u3046\u306b\u306a\u308b](http://qiita.com/rtoya/items/d80e59371832877fbbb1)\n\n\n# \u5185\u5bb9\n\n\u30fb \u74b0\u5883\u69cb\u7bc9\uff08Hadoop, Spark\u306e\u30d3\u30eb\u30c9\u304a\u3088\u3073conf\u8a2d\u5b9a\uff09\n\u30fb SparkContext\u3092\u4f7f\u3063\u3066\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306e\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\n\n\n# \u74b0\u5883\n\n\u30fb Ubuntu 14.04\n\u30fb Java 1.7.0_95\n\u30fb Hadoop 2.7.1\n\u30fb Spark 1.6.1\n\u30fb maven 3.3.9\n\u30fb Scala 2.11.7\n\u30fb sbt 0.13.7\n\n\n# Hadoop\u74b0\u5883\u8a2d\u5b9a\n\n**< \u30d3\u30eb\u30c9 >**\nhttp://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.1/hadoop-2.7.1-src.tar.gz \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3092\u89e3\u51cd\u3057\u3066\u30d3\u30eb\u30c9\u3057\u307e\u3057\u305f\uff0e\n\u30d3\u30eb\u30c9\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u306fhadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1\u306b\u3042\u308a\u307e\u3059\uff0e\n\u672c\u8a18\u4e8b\u3067\u306f\uff0chadoop-2.7.1\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ${HADOOP_HOME} \u3068\u3057\u307e\u3059\uff0e\n\n```\nwget http://ftp.meisei-u.ac.jp/mirror/apache/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1-src.tar.gz\ntar xvfz hadoop-2.7.1-src.tar.gz\ncd hadoop-2.7.1-src\nmvn compile\n```\n\n**< conf\u8a2d\u5b9a >**\nhttp://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html \u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\uff0e\n\u4e0a\u306e\u8a18\u4e8b\u901a\u308a\u306b\u3084\u308c\u3070Hadoop\u306e\u8a2d\u5b9a\u306f\u7d42\u308f\u308a\u3067\u3059\uff0e\n\n\n# Spark\u74b0\u5883\u8a2d\u5b9a\n\n**< \u30d3\u30eb\u30c9 >**\ngit://github.com/apache/spark.git \u304b\u3089\u30af\u30ed\u30fc\u30f3\u3057\u3066\u30d3\u30eb\u30c9\u3057\u307e\u3057\u305f\uff0e\u30d6\u30e9\u30f3\u30c1\u306f1.6\u3067\u3059\uff0e\n\u672c\u8a18\u4e8b\u3067\u306f\uff0cspark\u30c7\u30a3\u30af\u30c8\u30ea\u3092 ${SPARK_HOME} \u3068\u3057\u307e\u3059\uff0e\n\n```\ngit clone git://github.com/apache/spark.git\ncd spark\ngit checkout branch-1.6\nmvn -Pyarn -Phadoop-2.6 -Dscala-2.11 -DskipTests clean package\n```\n\nhadoop\u306f2.7.1\u306a\u306e\u3067 -Phadoop-2.6\nscala\u306f2.11.7\u306a\u306e\u3067 -Dscala-2.11\n\u3068\u3057\u3066\u3044\u307e\u3059\uff0e\n\n\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u7570\u306a\u308b\u65b9\u306f http://spark.apache.org/docs/latest/building-spark.html \u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n\n\n# \u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\n\n## spark-submit\u306b\u6e21\u3059\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u306ejar\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210\n\n\u672c\u8a18\u4e8b\u3067\u306f\uff0cspark_test\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ${SPARK_TEST_HOME} \u3068\u3057\u307e\u3059\uff0e\n```sbt assembly```\u3067 ${SPARK_TEST_HOME}/target/scala-2.11/spark_test-assembly-1.0.jar \u304c\u3067\u304d\u307e\u3059\uff0e\n\n```\n/spark_test   \n  |- build.sbt\n  |- project/\n       |- plugins.sbt\n  |- src/\n     |- resources/\n        |- wordcount_input.txt\n     |- main/\n        |- scala/\n           |- WordCount.scala\n```\n\n``` build.sbt\nname := \"spark_test\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.7\"\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" % \"spark-core_2.11\" % \"1.6.1\",\n  \"org.apache.spark\" % \"spark-streaming_2.11\" % \"1.6.1\"\n)\n\n// Merge Strategy\nassemblyMergeStrategy in assembly := {\n  case PathList(\"javax\", \"servlet\", xs @ _*)         => MergeStrategy.first\n  case PathList(ps @ _*) if ps.last endsWith \".class\" => MergeStrategy.first\n  case \"application.conf\"                            => MergeStrategy.concat\n  case \"unwanted.txt\"                                => MergeStrategy.discard\n  case x =>\n    val oldStrategy = (assemblyMergeStrategy in assembly).value\n    oldStrategy(x)\n}\n```\n\n``` plugins.sbt\nlogLevel := Level.Warn\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n```\n\n``` WordCount.scala\npackage examples\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\n\nobject WordCount extends Logging {\n  def main(args: Array[String]) {\n    val conf = new SparkConf().setAppName(\"spark_test\")\n    val sc = new SparkContext(conf)\n    val files = sc.textFile(\"hdfs://localhost:9000/data/wordcount_input.txt\")\n    val words = files.flatMap(_.split(\" \"))\n    val wordCounts = words.map(s => (s, 1)).reduceByKey(_ + _)\n    wordCounts.saveAsTextFile(\"hdfs://localhost:9000/result\")\n  }\n}\n```\n\nwordcount_input.txt\u306f http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip \u304b\u3089\u62dd\u501f\u3057\u307e\u3057\u305f\uff0e\n\n```\nwget http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip\nunzip Hadoop-WordCount.zip\n    Archive:  Hadoop-WordCount.zip\n    creating: Hadoop-WordCount/\n    creating: Hadoop-WordCount/classes/\n    creating: Hadoop-WordCount/input/\n    inflating: Hadoop-WordCount/input/Word_Count_input.txt\n    inflating: Hadoop-WordCount/WordCount.java\n    inflating: Hadoop-WordCount/clean.sh\n    inflating: Hadoop-WordCount/build.sh\n    inflating: Hadoop-WordCount/classes/WordCount$Reduce.class\n    inflating: Hadoop-WordCount/classes/WordCount.class\n    inflating: Hadoop-WordCount/classes/WordCount$Map.class\n    inflating: Hadoop-WordCount/wordcount.jar\n```\n\n``` ../resources/wordcount_input.txt\nAFTER such a scene as the last, Walter Morel was for some days abashed\nand ashamed, but he soon regained his old bullying indifference.\nYet there was a slight shrinking, a diminishing in his assurance.\nPhysically even, he shrank, and his fine full presence waned.\nHe never grew in the least stout, so that, as he sank from his erect,\nassertive bearing, his physique seemed to contract along with his pride\nand moral strength.\n\nBut now he realised how hard it was for his wife to drag\nabout at her work, and, his sympathy quickened by penitence,\nhastened forward with his help. He came straight home from the pit,\n...\n```\n\n\u53c2\u8003\u8a18\u4e8b\n[Spark1.5\u3067SparkStreaming\u958b\u767a [\u3053\u3068\u59cb\u3081\u7de8]](http://qiita.com/kaz3284/items/72dc7483872c412b6ba7)\n[Word Count (Spark, YARN, HDFS)](https://docs.continuum.io/anaconda-cluster/examples/spark-wordcount)\n\n\n## \u5b9f\u884c\n\n1.Hadoop\u3092\u7acb\u3061\u4e0a\u3052\uff0cHDFS\u306b\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u306e\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u7f6e\u304d\u307e\u3059\uff0e\n\n```\n${HADOOP_HOME}/bin/hdfs namenode -format\n${HADOOP_HOME}/sbin/start-dfs.sh\n${HADOOP_HOME}/bin/hdfs dfs -mkdir hdfs://localhost:9000/data\n${HADOOP_HOME}/bin/hdfs dfs -put ${SPARK_TEST_HOME}/src/main/resources/wordcount_input.txt hfs://localhost:9000/data\n${HADOOP_HOME}/sbin/start-yarn.sh\n```\n\n2.jar\u30d5\u30a1\u30a4\u30eb\u3092spark-submit\u306b\u6e21\u3057\uff0c\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u3092\u5b9f\u884c\u3057\u307e\u3059\uff0e\n\n```\n${SPARK_HOME}/bin/spark-submit --class examples.WordCount --master yarn-client --num-executors 1 --driver-memory 2g --executor-memory 1g --executor-cores 1 ${SPARK_TEST_HOME}/target/scala-2.11/spark_test-assembly-1.0.jar\n```\n\n3.\u7d50\u679c\u3092\u78ba\u8a8d\u3057\uff0cwordcount_result.txt\u306b\u4fdd\u5b58\u3057\u307e\u3059\uff0e\n\n```\n${HADOOP_HOME}/bin/hdfs dfs -ls hdfs://localhost:9000/result\n    # ls\u306e\u7d50\u679c\u3067\u3059\uff0epart-*\u306b\u7d50\u679c\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\n    hdfs://localhost:9000/result/_SUCCESS\n    hdfs://localhost:9000/result/part-00000\n    hdfs://localhost:9000/result/part-00001\n\ntouch (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n${HADOOP_HOME}/bin/hdfs dfs -cat hdfs://localhost:9000/result/part-00000 >> (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n${HADOOP_HOME}/bin/hdfs dfs -cat hdfs://localhost:9000/result/part-00001 >> (\u9069\u5f53\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)/wordcount_result.txt\n```\n\n\u53c2\u8003\u8a18\u4e8b\n[YARN\u4e0a\u3067\u30b8\u30e7\u30d6\u3092\u8d70\u3089\u305b\u3066\u307f\u308b\uff08Spark\u7de8\uff09](http://qiita.com/uryyyyyyy/items/72624c7b1e9d021d9ee0)\n[Apache Spark\u3092YARN\u4e0a\u3067\u52d5\u304b\u3057\u3066\u307f\u308b](http://d.hatena.ne.jp/Kazuhira/20150802/1438515482)\n\n## \u7d50\u679c\n\n\u4e0b\u306e\u3088\u3046\u306a\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3057\u305f\uff0e\n\n```wordcount_result.txt\n(simply,,1)\n(bone,1)\n(roses.,3)\n(Whatstandwell.,1)\n(stuck,,1)\n(bananas.,1)\n(letter:,1)\n(insufferably,1)\n(derniers,1)\n(hem,1)\n(sweat.,1)\n(think,\",1)\n(wasn't,7)\n(been,85)\n(they,,2)\n(PAUL,2)\n(jelly,\",1)\n(does---\",1)\n(pig,1)\n(crying,5)\n(soon;,1)\n(Sunday,\",1)\n(breath,2)\n(knows,4)\n(so's,2)\n(whistled,,1)\n(ignore,1)\n(Western,7)\n(smooth,2)\n(BLOWN,1)\n...\n```\n\n\n# \u304a\u308f\u308a\u306b\n\nYARN\u4e0a\u3067SPARK\u3092\u4f7f\u3063\u3066\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u3067\u304d\u307e\u3057\u305f\uff08\u591a\u5206\uff09\n\u5206\u304b\u3063\u3066\u306a\u3044\u3053\u3068\u3070\u304b\u308a\u306a\u306e\u3067\uff0c\u30b3\u30e1\u30f3\u30c8\u8cb0\u3048\u308b\u3068\u3042\u308a\u304c\u305f\u3044\u3067\u3059\uff0e\n"}