{"context": "\n\n\uff08 \u53c2\u8003 )\n\n\nyukiB\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u6df1\u5c64\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaKeras\u3067RNN\u3092\u4f7f\u3063\u3066sin\u6ce2\u4e88\u6e2c\u300d\n\n\n\nPython3 Jupyter notebook\n%matplotlib inline\n\n\n\nyukiB\u3055\u3093 Qiita\u8a18\u4e8b \u3092 \u4e00\u90e8\u6539\u826f\u3057\u3066\u3001\u5b66\u7fd2\u7528 \uff06 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u751f\u6210\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f\n\n\nPython3 Jupyter notebook\ndef get_n_sequence_dataset_and_m_peripd_later_data(data, n_prev = 100, m = 1):  \n   \"\"\"\n   data should be pd.DataFrame()\n   \"\"\"\n\n   docX, docY = [], []\n   for i in range(len(data)-n_prev*m):\n\n       docX.append(data.iloc[i:i+n_prev].as_matrix())\n       docY.append(data.iloc[i+n_prev+(m-1)].as_matrix())\n\n   alsX = np.array(docX)\n   alsY = np.array(docY)\n\n   return alsX, alsY\n\n\n\nPython3 Jupyter notebook\ndef create_train_data_and_test_data(df, test_size=0.1, n_prev = 100, m = 1):  \n   \"\"\"\n   This just splits data to training and testing parts\n   \"\"\"\n   ntrn = round(len(df) * (1 - test_size))\n   ntrn = int(ntrn)\n   X_train, y_train = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[0:ntrn], n_prev, m)\n   X_test, y_test = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[ntrn:], n_prev, m)\n\n   return (X_train, y_train), (X_test, y_test)\n\n\n\n\u6b63\u5f26(sin)\u66f2\u7dda \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u4f5c\u6210\n\n\nPython3 Jupyter notebook\nimport pandas as pd\nimport numpy as np\nimport math\nimport random\nimport seaborn as sns\n\n\n\nPython3 Jupyter notebook\n# \u4e71\u6570\u306e\u7a2e\u3068\u3057\u3066\u30010\u3092\u6e21\u3059\nrandom.seed(0)\nrandom_factor = 0.05\n# sin\u66f2\u7dda 1\u5468\u671f\u3042\u305f\u308a\u306e\u30b9\u30c6\u30c3\u30d7\u6570\nsteps_per_cycle = 100\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 1000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n\n\n\n\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30002\u5468\u671f\u5206\u3000\u63cf\u753b\n\n\nPython3 Jupyter notebook\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x114b30e48>\n\n\n\n\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30001\u5468\u671f\u5206\u3000\u63cf\u753b\n\n\nPython3 Jupyter notebook\n# 1\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x114ed4c88>\n\n\n\n\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30000.5\u5468\u671f\u5206\u3000\u63cf\u753b\n\n\nPython3 Jupyter notebook\n# 0.5\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 0.5).plot()\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pandas/core/indexing.py:1227: FutureWarning: slice indexers when using iloc should be integers and not floating point\n  return self._getitem_axis(key, axis=0)\n\n\n\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x114e34fd0>\n\n\n\n\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30001.5\u5468\u671f\u5206\u3000\u63cf\u753b\n\n\nPython3 Jupyter notebook\n# 1.5\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 1.5).plot()\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pandas/core/indexing.py:1227: FutureWarning: slice indexers when using iloc should be integers and not floating point\n  return self._getitem_axis(key, axis=0)\n\n\n\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x114dd1160>\n\n\n\n\nPython3 Jupyter notebook\n# 1\u5468\u671f\uff1d100\u6642\u70b9\nprint(steps_per_cycle)\n\n\n100\n\n\n\nLSTM\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 \u3068 \u4e88\u6e2c\u7cbe\u5ea6\u691c\u8a3c\n\n\n1\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u56de\u5e30\u4e88\u6e2c\u30e2\u30c7\u30eb\n\n\n\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09101\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210\n\n1\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210\n\nPython3 Jupyter notebook\n# 1\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e1\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=1)  \n\n\n\nPython3 Jupyter notebook\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89851\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89851\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9850\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9850\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\nPython3 Jupyter notebook\n# \u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u30011.5\u5468\u671f\u5206\uff1d150\u6642\u70b9 \u306e \u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n# \u76ee\u7684\u5909\u6570\uff08\u88ab\u8aac\u660e\u5909\u6570\uff09\u3068\u3057\u3066\u3001\uff11\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.03443538]\n [ 0.08851131]\n [ 0.11744915]\n [ 0.16364786]\n [ 0.24978178]\n [ 0.29996187]\n [ 0.3943597 ]\n [ 0.40790129]\n [ 0.47970153]\n [ 0.54284827]]\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.0172632]\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.07660351]\n [ 0.09925078]\n [ 0.14849661]\n [ 0.20115338]\n [ 0.33418164]\n [ 0.41373467]\n [ 0.41169616]\n [ 0.45590469]\n [ 0.56993065]\n [ 0.58197538]]\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.09686316]\n\n\nPython3 Jupyter notebook\nX_train[0][-1]\n\n\narray([ 0.09574251])\n\n\nPython3 Jupyter notebook\nfrom matplotlib import pyplot as plt\nplt.plot(X_train[0])\n\n\n[<matplotlib.lines.Line2D at 0x116977668>]\n\n\n\nPython3 Jupyter notebook\nplt.plot(X_test[0])\n\n\n[<matplotlib.lines.Line2D at 0x1156ec128>]\n\n\n\nPython3 Jupyter notebook\nimport types\ntype(X_train[0])\n\n\nnumpy.ndarray\n\n\nLSTM\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\n\n\n\u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u6570: 300\n\n\nPython3 Jupyter notebook\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\n#\n# batch_input_shape=(None, int(steps_per_cycle)*1.5, 1) \u306e \u610f\u5473\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308b\u3002\n#\n# batch_input_shape=(None, input_length=int(steps_per_cycle)*1.5, input_dim=1)\n# batch_input_shape=(None,\\\n#                   \u5165\u529b\u30c7\u30fc\u30bf\u306e\u8981\u7d20\uff08\u30ce\u30fc\u30c9\uff09\u6570\uff08LSTM\u5c64\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306b\u4e00\u81f4\u3059\u308b\uff09\uff08\u203b\u6587\u66f8\u30c7\u30fc\u30bf\u306a\u3089\u5358\u8a9e\u306e\u6570\u3002\u4eca\u56de\u306f\u8aac\u660e\u5909\u6570\u306e\u6570\uff09,\\\n#                   LSTM\u4e2d\u9593\u5c64\u306b\u6295\u5165\u3059\u308b\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u6570\uff08\u203b\u6587\u66f8\u30c7\u30fc\u30bf\u306a\u3089\uff11\u6b21\u5143\u914d\u5217\u306a\u306e\u30671)\n#                  )\n# \uff08\u53c2\u8003\uff09 http://qiita.com/HirofumiYashima/items/a4651229865918ae2a78\n\nmodel_predict_1_ahead = Sequential()  \nmodel_predict_1_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_1_ahead.add(Dense(1))  \nmodel_predict_1_ahead.add(Activation(\"linear\"))  \nmodel_predict_1_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\nUsing TensorFlow backend.\n\n\nPython3 Jupyter notebook\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_1_ahead =   model_predict_1_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 3s - loss: 0.5521 - val_loss: 0.2536\nEpoch 2/10\n95/95 [==============================] - 2s - loss: 0.1339 - val_loss: 0.1074\nEpoch 3/10\n95/95 [==============================] - 2s - loss: 0.4587 - val_loss: 0.0347\nEpoch 4/10\n95/95 [==============================] - 2s - loss: 0.0572 - val_loss: 0.0083\nEpoch 5/10\n95/95 [==============================] - 2s - loss: 0.0176 - val_loss: 0.0028\nEpoch 6/10\n95/95 [==============================] - 2s - loss: 0.0017 - val_loss: 0.0070\n\n\nPython3 Jupyter notebook\nloss = model_1_ahead.history['loss']\nval_loss = model_1_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\nmodel_predict_1_ahead.summary()\n\n\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nlstm_1 (LSTM)                    (None, 300)           362400      lstm_input_1[0][0]               \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (None, 1)             301         lstm_1[0][0]                     \n____________________________________________________________________________________________________\nactivation_1 (Activation)        (None, 1)             0           dense_1[0][0]                    \n====================================================================================================\nTotal params: 362,701\nTrainable params: 362,701\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_1_ahead = model_predict_1_ahead.predict(X_test) \nlen(predicted_1_ahead)\n\n\n9850\n\n\nPython3 Jupyter notebook\nlen(X_test)\n\n\n9850\n\n\u5165\u529b\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\uff08\uff1d\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306e\u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\uff09 \u3068 \u51fa\u529b\u6570\u5024\uff08\uff1d1\u6642\u70b9\u5f8c\u306e\u4e88\u6e2c\u5024\uff09\u306e\u4ef6\u6570 \u306f \u4e00\u81f4\u3057\u3066\u3044\u308b\u3002\n\nPython3 Jupyter notebook\nlen(X_test[0])\n\n\n150\n\n\n\u4e88\u6e2c\u5024\u3000\u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u306e\u4e00\u81f4\u5ea6\u5408\u3044 \u3092 \u6bd4\u8f03\n\n\nPython3 Jupyter notebook\ndataf =  pd.DataFrame(predicted_1_ahead)\ndataf.columns = [\"predict\"]\ndataf[\"true_value(observed_value)\"] = y_test\ndataf.plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x126905be0>\n\n\n\nPython3 Jupyter notebook\ndataf[0:100].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12720bda0>\n\n\n\n\u4e88\u6e2c\u7cbe\u5ea6\u7d50\u679c\n\n\n1\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03\n\n\n\u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\uff09\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3002\n\n\nPython3 Jupyter notebook\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x127753ac8>\n\n\n\n5\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03\n\n\nPython3 Jupyter notebook\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle*5].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x1276b9dd8>\n\n\n\n20\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03\n\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle*20].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x125f9bf98>\n\n\n\n\u30e9\u30b9\u30c820\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03\n\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[-20*steps_per_cycle:].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x128128d30>\n\n\n\n\nLSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570\uff08\uff1dLSTM\u30d6\u30ed\u30c3\u30af\u6570\uff09 \u3092 2\u500d \u306e 600 \u306b\u3059\u308b\u3002\n\n\nPython3 Jupyter notebook\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_1_ahead2 = Sequential()  \nmodel_predict_1_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_1_ahead2.add(Dense(1))  \nmodel_predict_1_ahead2.add(Activation(\"linear\"))  \nmodel_predict_1_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_1_ahead2 =   model_predict_1_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 9s - loss: 0.4234 - val_loss: 5.7625\nEpoch 2/10\n95/95 [==============================] - 7s - loss: 4.5148 - val_loss: 0.0040\nEpoch 3/10\n95/95 [==============================] - 7s - loss: 0.2204 - val_loss: 1.7629\n\n\nPython3 Jupyter notebook\nloss = model_1_ahead2.history['loss']\nval_loss = model_1_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\nmodel_predict_1_ahead2.summary()\n\n\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nlstm_2 (LSTM)                    (None, 600)           1444800     lstm_input_2[0][0]               \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (None, 1)             601         lstm_2[0][0]                     \n____________________________________________________________________________________________________\nactivation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n====================================================================================================\nTotal params: 1,445,401\nTrainable params: 1,445,401\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_1_ahead2 = model_predict_1_ahead.predict(X_test) \nlen(predicted_1_ahead2)\n\n\n9850\n\n\nPython3 Jupyter notebook\ndataf2 =  pd.DataFrame(predicted_1_ahead2)\ndataf2.columns = [\"predict\"]\ndataf2[\"true_value(observed_value)\"] = y_test\n\n\n\nPython3 Jupyter notebook\ndataf2.shape\n\n\n(9850, 2)\n\n\nPython3 Jupyter notebook\n# \u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\uff09\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12a010c50>\n\n\n\nPython3 Jupyter notebook\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle*5].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12a8d1400>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12ec8cc50>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[-20*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12ecb3e48>\n\n\n\nPython3 Jupyter notebook\n# \u7d50\u679c\u306f\u3042\u307e\u308a\u5909\u308f\u3089\u306a\u304b\u3063\u305f\u3002\n\n\n\n\n5\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c\n\n\n\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09105\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210\n\n5\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210\n\nPython3 Jupyter notebook\n# 5\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e5\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=5)  \n\n\n\nPython3 Jupyter notebook\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89251\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89251\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9250\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9250\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.03443538]\n [ 0.08851131]\n [ 0.11744915]\n [ 0.16364786]\n [ 0.24978178]\n [ 0.29996187]\n [ 0.3943597 ]\n [ 0.40790129]\n [ 0.47970153]\n [ 0.54284827]]\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.28199213]\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.07660351]\n [ 0.09925078]\n [ 0.14849661]\n [ 0.20115338]\n [ 0.33418164]\n [ 0.41373467]\n [ 0.41169616]\n [ 0.45590469]\n [ 0.56993065]\n [ 0.58197538]]\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.34252502]\n\n\nPython3 Jupyter notebook\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_5_ahead = Sequential()  \nmodel_predict_5_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_5_ahead.add(Dense(1))  \nmodel_predict_5_ahead.add(Activation(\"linear\"))  \nmodel_predict_5_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_5_ahead =   model_predict_5_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 5s - loss: 0.5488 - val_loss: 0.5149\nEpoch 2/10\n95/95 [==============================] - 2s - loss: 0.2190 - val_loss: 0.0118\nEpoch 3/10\n95/95 [==============================] - 2s - loss: 0.6881 - val_loss: 0.1715\n\n\nPython3 Jupyter notebook\nloss = model_5_ahead.history['loss']\nval_loss = model_5_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\nmodel_predict_5_ahead.summary()\n\n\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nlstm_6 (LSTM)                    (None, 300)           362400      lstm_input_6[0][0]               \n____________________________________________________________________________________________________\ndense_6 (Dense)                  (None, 1)             301         lstm_6[0][0]                     \n____________________________________________________________________________________________________\nactivation_6 (Activation)        (None, 1)             0           dense_6[0][0]                    \n====================================================================================================\nTotal params: 362,701\nTrainable params: 362,701\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_5_ahead = model_predict_5_ahead.predict(X_test) \nlen(predicted_5_ahead)\n\n\n9250\n\n\nPython3 Jupyter notebook\ndataf_ahead_5 =  pd.DataFrame(predicted_5_ahead)\ndataf_ahead_5.columns = [\"predict\"]\ndataf_ahead_5[\"true_value(observed_value)\"] = y_test\n\n\n\nPython3 Jupyter notebook\ndataf_ahead_5.shape\n\n\n(9250, 2)\n\n\nPython3 Jupyter notebook\nprint(dataf_ahead_5.ix[0,])\n\n\npredict                       0.174520\ntrue_value(observed_value)   -0.342525\nName: 0, dtype: float64\n\n\nPython3 Jupyter notebook\nprint(dataf_ahead_5.ix[1:10,])\n\n\n     predict  true_value(observed_value)\n1   0.131979                   -0.400385\n2   0.086908                   -0.390119\n3   0.043136                   -0.519381\n4  -0.001042                   -0.565102\n5  -0.047355                   -0.583281\n6  -0.095603                   -0.618386\n7  -0.144298                   -0.681583\n8  -0.189855                   -0.730022\n9  -0.236214                   -0.791564\n10 -0.281993                   -0.826755\n\n\nPython3 Jupyter notebook\n# \u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\uff09\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12968e240>\n\n\n\nPython3 Jupyter notebook\n# \u4e88\u6e2c\u5024\uff08\u9752\u7dda\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001\u7dd1\u7dda\uff09\u3088\u308a\u3082\u3001\u9045\u308c\u3092\u4f34\u3063\u3066\u3044\u308b\u3002\n\n\n\nPython3 Jupyter notebook\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*5].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12c962470>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12dedbe48>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[-20*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12f7a1160>\n\n\n\nPython3 Jupyter notebook\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_5_ahead2 = Sequential()  \nmodel_predict_5_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_5_ahead2.add(Dense(1))  \nmodel_predict_5_ahead2.add(Activation(\"linear\"))  \nmodel_predict_5_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_5_ahead2 =   model_predict_5_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 10s - loss: 0.5913 - val_loss: 6.0171\nEpoch 2/10\n95/95 [==============================] - 11s - loss: 2.9635 - val_loss: 0.0176\nEpoch 3/10\n95/95 [==============================] - 12s - loss: 0.2258 - val_loss: 2.5224\n\n\nPython3 Jupyter notebook\nloss = model_5_ahead2.history['loss']\nval_loss = model_5_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\nmodel_predict_5_ahead2.summary()\n\n\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nlstm_7 (LSTM)                    (None, 600)           1444800     lstm_input_7[0][0]               \n____________________________________________________________________________________________________\ndense_7 (Dense)                  (None, 1)             601         lstm_7[0][0]                     \n____________________________________________________________________________________________________\nactivation_7 (Activation)        (None, 1)             0           dense_7[0][0]                    \n====================================================================================================\nTotal params: 1,445,401\nTrainable params: 1,445,401\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff085\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_5_ahead2 = model_predict_5_ahead2.predict(X_test) \nlen(predicted_5_ahead2)\n\n\n9250\n\n\nPython3 Jupyter notebook\ndataf_ahead_5_2 =  pd.DataFrame(predicted_5_ahead2)\ndataf_ahead_5_2.columns = [\"predict\"]\ndataf_ahead_5_2[\"true_value(observed_value)\"] = y_test\n\n\n\nPython3 Jupyter notebook\ndataf_ahead_5_2.shape\n\n\n(9250, 2)\n\n\nPython3 Jupyter notebook\nprint(dataf_ahead_5_2.ix[1:10,])\n\n\n     predict  true_value(observed_value)\n1   0.663027                   -0.400385\n2   0.310947                   -0.390119\n3   0.048220                   -0.519381\n4  -0.140018                   -0.565102\n5  -0.115694                   -0.583281\n6  -0.040395                   -0.618386\n7  -0.061709                   -0.681583\n8  -0.089087                   -0.730022\n9  -0.034119                   -0.791564\n10 -0.024306                   -0.826755\n\n\nPython3 Jupyter notebook\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12968a160>\n\n\n\nPython3 Jupyter notebook\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u4e88\u6e2c\u5024\u7cfb\u5217\u306f\u304a\u304b\u3057\u306a\u6319\u52d5\u3092\u793a\u3057\u3066\u3044\u308b\ndataf_ahead_5_2[0:steps_per_cycle*5].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13142c710>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x141211d30>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[-20*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12cf18b38>\n\n\n\n\n50\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c\n\n\n\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09150\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210\n\n50\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210\n\nPython3 Jupyter notebook\n# 50\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=50)  \n\n\n\nPython3 Jupyter notebook\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  82501\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  82501\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  2500\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  2500\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.03443538]\n [ 0.08851131]\n [ 0.11744915]\n [ 0.16364786]\n [ 0.24978178]\n [ 0.29996187]\n [ 0.3943597 ]\n [ 0.40790129]\n [ 0.47970153]\n [ 0.54284827]]\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.10794316]\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.07660351]\n [ 0.09925078]\n [ 0.14849661]\n [ 0.20115338]\n [ 0.33418164]\n [ 0.41373467]\n [ 0.41169616]\n [ 0.45590469]\n [ 0.56993065]\n [ 0.58197538]]\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [ 0.02981266]\n\n\nPython3 Jupyter notebook\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_50_ahead = Sequential()  \nmodel_predict_50_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_50_ahead.add(Dense(1))  \nmodel_predict_50_ahead.add(Activation(\"linear\"))  \nmodel_predict_50_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead =   model_predict_50_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 4s - loss: 0.6719 - val_loss: 0.4122\nEpoch 2/10\n95/95 [==============================] - 2s - loss: 0.3176 - val_loss: 0.0783\nEpoch 3/10\n95/95 [==============================] - 4s - loss: 0.3776 - val_loss: 0.0041\nEpoch 4/10\n95/95 [==============================] - 4s - loss: 0.0833 - val_loss: 6.8842e-04\nEpoch 5/10\n95/95 [==============================] - 4s - loss: 0.0105 - val_loss: 0.0111\n\n\nPython3 Jupyter notebook\nloss = model_50_ahead.history['loss']\nval_loss = model_50_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0850\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_50_ahead = model_predict_50_ahead.predict(X_test) \nlen(predicted_50_ahead)\n\n\n2500\n\n\nPython3 Jupyter notebook\ndataf_ahead_50 =  pd.DataFrame(predicted_50_ahead)\ndataf_ahead_50.columns = [\"predict\"]\ndataf_ahead_50[\"true_value(observed_value)\"] = y_test\n\n\n\nPython3 Jupyter notebook\ndataf_ahead_50[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x1394724a8>\n\n\n\nPython3 Jupyter notebook\n# 50\u671f\u5148\u306e\u4e88\u6e2c\u5024\u306f\u3001\u6b63\u89e3\u5024\u3088\u308a\u3082\u5468\u671f\u304c\u82e5\u5e72\u65e9\u3081\u306b\u51fa\u3066\u3044\u308b\u3002\n# \u632f\u5e45\u306e\u5927\u304d\u3055\uff08\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\uff09\u306f\u3001\u6b63\u89e3\u5024\u3068\u307b\u307c\u540c\u3058\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x131402588>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[-20*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x139279f60>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c85\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[-5*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x132074c50>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c83\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u6b63\u89e3\u5024\u3088\u308a\u3082\u5468\u671f\u304c\u82e5\u5e72\u65e9\u3081\u306b\u51fa\u3066\u3044\u308b\u3002\n# \u632f\u5e45\u306e\u5927\u304d\u3055\uff08\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\uff09\u306f\u3001\u6b63\u89e3\u5024\u3068\u307b\u307c\u540c\u3058\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\ndataf_ahead_50[-2*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x131f033c8>\n\n\n\nPython3 Jupyter notebook\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_50_ahead2 = Sequential()  \nmodel_predict_50_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_50_ahead2.add(Dense(1))  \nmodel_predict_50_ahead2.add(Activation(\"linear\"))  \nmodel_predict_50_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead2 =   model_predict_50_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/10\n95/95 [==============================] - 11s - loss: 0.5276 - val_loss: 7.0810\nEpoch 2/10\n95/95 [==============================] - 11s - loss: 3.8192 - val_loss: 0.1959\nEpoch 3/10\n95/95 [==============================] - 12s - loss: 0.4445 - val_loss: 0.1841\nEpoch 4/10\n95/95 [==============================] - 12s - loss: 0.2441 - val_loss: 0.0827\nEpoch 5/10\n95/95 [==============================] - 12s - loss: 0.1769 - val_loss: 0.0435\nEpoch 6/10\n95/95 [==============================] - 12s - loss: 0.6151 - val_loss: 0.0145\nEpoch 7/10\n95/95 [==============================] - 12s - loss: 0.1097 - val_loss: 0.0034\nEpoch 8/10\n95/95 [==============================] - 12s - loss: 0.0668 - val_loss: 5.6836e-04\nEpoch 9/10\n95/95 [==============================] - 11s - loss: 0.0359 - val_loss: 0.0023\n\n\nPython3 Jupyter notebook\n# \u307e\u3060\u8aa4\u5dee\u306f\u6e1b\u308a\u305d\u3046\u306a\u306e\u3067\u3001epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead2 =   model_predict_50_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/20\n95/95 [==============================] - 7s - loss: 0.0141 - val_loss: 0.0029\nEpoch 2/20\n95/95 [==============================] - 7s - loss: 0.0041 - val_loss: 2.7682e-04\nEpoch 3/20\n95/95 [==============================] - 7s - loss: 0.0014 - val_loss: 1.7764e-04\nEpoch 4/20\n95/95 [==============================] - 12s - loss: 0.0010 - val_loss: 2.1413e-04\n\n\nPython3 Jupyter notebook\nloss = model_50_ahead2.history['loss']\nval_loss = model_50_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0850\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_50_ahead2 = model_predict_50_ahead2.predict(X_test) \nlen(predicted_50_ahead2)\n\n\n2500\n\n\nPython3 Jupyter notebook\ndataf_ahead_50_2 =  pd.DataFrame(predicted_50_ahead2)\ndataf_ahead_50_2.columns = [\"predict\"]\ndataf_ahead_50_2[\"true_value(observed_value)\"] = y_test\n\n\n\nPython3 Jupyter notebook\ndataf_ahead_50_2.shape\n\n\n(2500, 2)\n\n\nPython3 Jupyter notebook\n# 50\u671f\u5148\u306e\u4e88\u6e2c\u5024 \u3068 \u5b9f\u6e2c\u5024 \u306f\u3001\u307b\u307c\u4e00\u81f4\u3057\u3066\u3044\u308b\u3002\ndataf_ahead_50_2[0:steps_per_cycle].plot()# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13dde4ba8>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50_2[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13dffa9e8>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50_2[-20*steps_per_cycle*20:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x14fcec160>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u4e88\u6e2c\u5024 \u3068 \u5b9f\u6e2c\u5024 \u306f\u3001\u307b\u307c\u4e00\u81f4\u3057\u3066\u3044\u308b\u3002\ndataf_ahead_50_2[-2*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x14e7c4588>\n\n\n\nPython3 Jupyter notebook\n# LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\uff08300\uff09\u3092\uff12\u500d\u306e600\u306b\u5897\u3084\u3057\u305f\u7d50\u679c\u3001\n# \u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u5024\u306f\u307b\u307c\u4e00\u81f4\u3059\u308b\u307e\u3067\u306b\u4e88\u6e2c\u7cbe\u5ea6\u304c\u4e0a\u304c\u3063\u305f\u3002\n\n\n\n\n150\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c\n\n\n\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09150\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210\n\n150\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210\n\nPython3 Jupyter notebook\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n\n\n\nPython3 Jupyter notebook\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  67501\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  67501\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5f97\u3089\u308c\u306a\u304b\u3063\u305f\u3002\n\n\n\nPython3 Jupyter notebook\n# \u5143\u306e\u30c7\u30fc\u30bf\u5468\u671f\u6570\u3092\u5897\u3084\u3059\n\n\n\nPython3 Jupyter notebook\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 2000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13e20ccf8>\n\n\n\nPython3 Jupyter notebook\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n\n\n\nPython3 Jupyter notebook\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  157501\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  157501\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\n\nPython3 Jupyter notebook\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 20000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13e22b2e8>\n\n\n\nPython3 Jupyter notebook\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n\n\n\nPython3 Jupyter notebook\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  1777501\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  1777501\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  177500\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  177500\n\n\nPython3 Jupyter notebook\n# \u4eca\u5ea6\u306f\u3001\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u5f97\u3089\u308c\u305f\u3002\n\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.02051963]\n [ 0.05079059]\n [ 0.17277323]\n [ 0.17119517]\n [ 0.25317441]\n [ 0.30671668]\n [ 0.39315019]\n [ 0.41498027]\n [ 0.48016081]\n [ 0.52335404]]\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.1055117]\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.05010115]\n [ 0.13582259]\n [ 0.17097283]\n [ 0.28140028]\n [ 0.30962029]\n [ 0.36655806]\n [ 0.40651334]\n [ 0.48688099]\n [ 0.56543229]\n [ 0.58529144]]\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [ 0.03704185]\n\n\nPython3 Jupyter notebook\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_150_ahead = Sequential()  \nmodel_predict_150_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_150_ahead.add(Dense(1))  \nmodel_predict_150_ahead.add(Activation(\"linear\"))  \nmodel_predict_150_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\n# epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_150_ahead =   model_predict_150_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/20\n95/95 [==============================] - 4s - loss: 0.4581 - val_loss: 0.4851\nEpoch 2/20\n95/95 [==============================] - 2s - loss: 0.3592 - val_loss: 0.0381\nEpoch 3/20\n95/95 [==============================] - 2s - loss: 0.2773 - val_loss: 0.0080\nEpoch 4/20\n95/95 [==============================] - 2s - loss: 0.0593 - val_loss: 9.9821e-04\nEpoch 5/20\n95/95 [==============================] - 2s - loss: 0.0056 - val_loss: 0.0074\n\n\nPython3 Jupyter notebook\nloss = model_150_ahead.history['loss']\nval_loss = model_150_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff08150\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_150_ahead = model_predict_150_ahead.predict(X_test[0:10000]) \nlen(predicted_150_ahead)\n\n\n10000\n\n\nPython3 Jupyter notebook\ndataf_ahead_150 =  pd.DataFrame(predicted_150_ahead)\ndataf_ahead_150.columns = [\"predict\"]\ndataf_ahead_150[\"true_value(observed_value)\"] = y_test[0:10000]\n\n\n\nPython3 Jupyter notebook\ndataf_ahead_150.shape\n\n\n(10000, 2)\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c81\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13e6720f0>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x1377bcc88>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[-20*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x137933a90>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[-2*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12fa0b1d0>\n\n\n\nPython3 Jupyter notebook\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001600\u306b\u3059\u308b\nmodel_predict_150_ahead2 = Sequential()  \nmodel_predict_150_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_150_ahead2.add(Dense(1))  \nmodel_predict_150_ahead2.add(Activation(\"linear\"))  \nmodel_predict_150_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n\n\n\nPython3 Jupyter notebook\n# epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_150_ahead2 =   model_predict_150_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n\nTrain on 95 samples, validate on 5 samples\nEpoch 1/20\n95/95 [==============================] - 10s - loss: 0.5216 - val_loss: 3.8104\nEpoch 2/20\n95/95 [==============================] - 7s - loss: 2.3219 - val_loss: 0.1407\nEpoch 3/20\n95/95 [==============================] - 7s - loss: 0.3426 - val_loss: 0.0352\nEpoch 4/20\n95/95 [==============================] - 10s - loss: 0.1739 - val_loss: 0.0010\nEpoch 5/20\n95/95 [==============================] - 9s - loss: 0.0503 - val_loss: 0.0291\n\n\nPython3 Jupyter notebook\nloss = model_150_ahead2.history['loss']\nval_loss = model_150_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff08150\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_150_ahead2 = model_predict_150_ahead2.predict(X_test[0:10000]) \nlen(predicted_150_ahead2)\n\n\n10000\n\n\nPython3 Jupyter notebook\ndataf_ahead_150_2 =  pd.DataFrame(predicted_150_ahead2)\ndataf_ahead_150_2.columns = [\"predict\"]\ndataf_ahead_150_2[\"true_value(observed_value)\"] = y_test[0:10000]\ndataf_ahead_150_2.ix[0:10, ]\n\n\n\n\n\n\n\npredict\ntrue_value(observed_value)\n\n\n\n\n0\n0.123705\n0.037042\n\n\n1\n0.170413\n0.041712\n\n\n2\n0.218634\n0.140744\n\n\n3\n0.265900\n0.210152\n\n\n4\n0.312858\n0.229661\n\n\n5\n0.354751\n0.325469\n\n\n6\n0.396260\n0.343859\n\n\n7\n0.434023\n0.466696\n\n\n8\n0.468966\n0.490038\n\n\n9\n0.500620\n0.514457\n\n\n10\n0.528240\n0.611611\n\n\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c81\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# LSTM\u30d6\u30ed\u30c3\u30af\u6570\u3092\uff12\u500d\u306e600\u306b\u3057\u305f\u7d50\u679c\u3001\u4ee5\u4e0b\u304c\u5f97\u3089\u308c\u305f\u3002\n# \u5468\u671f: \u4e88\u6e2c\u5024\u306e\u5468\u671f\u304c\u3084\u3084\u5c0f\u3055\u304f\u306a\u3063\u305f\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u304f\u306a\u3063\u305f\ndataf_ahead_150_2[0:steps_per_cycle].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x14ff44c88>\n\n\n\nPython3 Jupyter notebook\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u3044\ndataf_ahead_150_2[0:steps_per_cycle*20].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x137988390>\n\n\n\nPython3 Jupyter notebook\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u306e\u5468\u671f\u304c\u3084\u3084\u5c0f\u3055\u3044\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u3044 \ndataf_ahead_150_2[-2*steps_per_cycle:].plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x13e822748>\n\n\n###__\uff08 \u53c2\u8003 )__\n\n* yukiB\u3055\u3093 Qiita\u8a18\u4e8b \u300c\u6df1\u5c64\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaKeras\u3067RNN\u3092\u4f7f\u3063\u3066sin\b\u6ce2\u4e88\u6e2c\u300d\n\n\n___\n\n```{python:Python3 Jupyter notebook}\n%matplotlib inline\n```\n\n###__yukiB\u3055\u3093 Qiita\u8a18\u4e8b \u3092 \u4e00\u90e8\u6539\u826f\u3057\u3066\u3001\u5b66\u7fd2\u7528 \uff06 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u751f\u6210\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f__\n\n\n```{python:Python3 Jupyter notebook}\ndef get_n_sequence_dataset_and_m_peripd_later_data(data, n_prev = 100, m = 1):  \n   \"\"\"\n   data should be pd.DataFrame()\n   \"\"\"\n\n   docX, docY = [], []\n   for i in range(len(data)-n_prev*m):\n\n       docX.append(data.iloc[i:i+n_prev].as_matrix())\n       docY.append(data.iloc[i+n_prev+(m-1)].as_matrix())\n\n   alsX = np.array(docX)\n   alsY = np.array(docY)\n\n   return alsX, alsY\n```\n\n\n```{python:Python3 Jupyter notebook}\ndef create_train_data_and_test_data(df, test_size=0.1, n_prev = 100, m = 1):  \n   \"\"\"\n   This just splits data to training and testing parts\n   \"\"\"\n   ntrn = round(len(df) * (1 - test_size))\n   ntrn = int(ntrn)\n   X_train, y_train = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[0:ntrn], n_prev, m)\n   X_test, y_test = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[ntrn:], n_prev, m)\n\n   return (X_train, y_train), (X_test, y_test)\n```\n\n###__\u6b63\u5f26(*sin*)\u66f2\u7dda \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\nimport pandas as pd\nimport numpy as np\nimport math\nimport random\nimport seaborn as sns\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u4e71\u6570\u306e\u7a2e\u3068\u3057\u3066\u30010\u3092\u6e21\u3059\nrandom.seed(0)\nrandom_factor = 0.05\n# sin\u66f2\u7dda 1\u5468\u671f\u3042\u305f\u308a\u306e\u30b9\u30c6\u30c3\u30d7\u6570\nsteps_per_cycle = 100\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 1000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n```\n\n####__\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30002\u5468\u671f\u5206\u3000\u63cf\u753b__\n\n```{python:Python3 Jupyter notebook}                                               \n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x114b30e48>\n\n\n\n\n![output_4_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/8ab6f45e-7bb0-dcdb-f4a7-2eba0485eb89.png)\n\n\n####__\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30001\u5468\u671f\u5206\u3000\u63cf\u753b__\n\n```{python:Python3 Jupyter notebook}\n# 1\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x114ed4c88>\n\n\n\n\n![output_5_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/17135f3a-3112-8a84-9ec5-a8403aa87e5e.png)\n\n\n####__\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30000.5\u5468\u671f\u5206\u3000\u63cf\u753b__\n\n```{python:Python3 Jupyter notebook}\n# 0.5\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 0.5).plot()\n```\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pandas/core/indexing.py:1227: FutureWarning: slice indexers when using iloc should be integers and not floating point\n      return self._getitem_axis(key, axis=0)\n\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x114e34fd0>\n\n\n\n\n![output_6_2.png](https://qiita-image-store.s3.amazonaws.com/0/43487/2d2fd409-2c7a-e17d-fc36-1fce75c4b2dc.png)\n\n\n####__\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3000\u3092\u30001.5\u5468\u671f\u5206\u3000\u63cf\u753b__\n\n```{python:Python3 Jupyter notebook}\n# 1.5\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 1.5).plot()\n```\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pandas/core/indexing.py:1227: FutureWarning: slice indexers when using iloc should be integers and not floating point\n      return self._getitem_axis(key, axis=0)\n\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x114dd1160>\n\n\n\n\n![output_7_2.png](https://qiita-image-store.s3.amazonaws.com/0/43487/6b57c93d-06cf-241e-b544-0badaa35f5ac.png)\n\n\n___\n\n```{python:Python3 Jupyter notebook}\n# 1\u5468\u671f\uff1d100\u6642\u70b9\nprint(steps_per_cycle)\n```\n\n    100\n\n___\n\n#__LSTM\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 \u3068 \u4e88\u6e2c\u7cbe\u5ea6\u691c\u8a3c__\n\n##__1\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u56de\u5e30\u4e88\u6e2c\u30e2\u30c7\u30eb__\n\n###__\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09101\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210__\n\n__1\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\n# 1\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e1\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=1)  \n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89851\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89851\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9850\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9850\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u30011.5\u5468\u671f\u5206\uff1d150\u6642\u70b9 \u306e \u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n# \u76ee\u7684\u5909\u6570\uff08\u88ab\u8aac\u660e\u5909\u6570\uff09\u3068\u3057\u3066\u3001\uff11\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.03443538]\n     [ 0.08851131]\n     [ 0.11744915]\n     [ 0.16364786]\n     [ 0.24978178]\n     [ 0.29996187]\n     [ 0.3943597 ]\n     [ 0.40790129]\n     [ 0.47970153]\n     [ 0.54284827]]\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.0172632]\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.07660351]\n     [ 0.09925078]\n     [ 0.14849661]\n     [ 0.20115338]\n     [ 0.33418164]\n     [ 0.41373467]\n     [ 0.41169616]\n     [ 0.45590469]\n     [ 0.56993065]\n     [ 0.58197538]]\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.09686316]\n\n\n\n```{python:Python3 Jupyter notebook}\nX_train[0][-1]\n```\n\n\n\n\n    array([ 0.09574251])\n\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom matplotlib import pyplot as plt\nplt.plot(X_train[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x116977668>]\n\n\n\n\n![output_15_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/4dc7d3ff-1659-ff28-c2c4-5c65ff1f5874.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nplt.plot(X_test[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x1156ec128>]\n\n\n\n\n![output_16_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/56d73fae-cd59-3eb9-d975-9c0a2a3178f3.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nimport types\ntype(X_train[0])\n```\n\n\n\n\n    numpy.ndarray\n\n\n###__LSTM\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2__\n\n* \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u6570: 300\n\n```{python:Python3 Jupyter notebook}\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\n#\n# batch_input_shape=(None, int(steps_per_cycle)*1.5, 1) \u306e \u610f\u5473\u306f\u3001\u4ee5\u4e0b\u306b\u306a\u308b\u3002\n#\n# batch_input_shape=(None, input_length=int(steps_per_cycle)*1.5, input_dim=1)\n# batch_input_shape=(None,\\\n#                   \u5165\u529b\u30c7\u30fc\u30bf\u306e\u8981\u7d20\uff08\u30ce\u30fc\u30c9\uff09\u6570\uff08LSTM\u5c64\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306b\u4e00\u81f4\u3059\u308b\uff09\uff08\u203b\u6587\u66f8\u30c7\u30fc\u30bf\u306a\u3089\u5358\u8a9e\u306e\u6570\u3002\u4eca\u56de\u306f\u8aac\u660e\u5909\u6570\u306e\u6570\uff09,\\\n#                   LSTM\u4e2d\u9593\u5c64\u306b\u6295\u5165\u3059\u308b\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u6570\uff08\u203b\u6587\u66f8\u30c7\u30fc\u30bf\u306a\u3089\uff11\u6b21\u5143\u914d\u5217\u306a\u306e\u30671)\n#                  )\n# \uff08\u53c2\u8003\uff09 http://qiita.com/HirofumiYashima/items/a4651229865918ae2a78\n\nmodel_predict_1_ahead = Sequential()  \nmodel_predict_1_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_1_ahead.add(Dense(1))  \nmodel_predict_1_ahead.add(Activation(\"linear\"))  \nmodel_predict_1_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n    Using TensorFlow backend.\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_1_ahead =   model_predict_1_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 3s - loss: 0.5521 - val_loss: 0.2536\n    Epoch 2/10\n    95/95 [==============================] - 2s - loss: 0.1339 - val_loss: 0.1074\n    Epoch 3/10\n    95/95 [==============================] - 2s - loss: 0.4587 - val_loss: 0.0347\n    Epoch 4/10\n    95/95 [==============================] - 2s - loss: 0.0572 - val_loss: 0.0083\n    Epoch 5/10\n    95/95 [==============================] - 2s - loss: 0.0176 - val_loss: 0.0028\n    Epoch 6/10\n    95/95 [==============================] - 2s - loss: 0.0017 - val_loss: 0.0070\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_1_ahead.history['loss']\nval_loss = model_1_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_20_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/33a59df4-c1c0-a95b-e1ed-2e94eb3cda96.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nmodel_predict_1_ahead.summary()\n```\n\n    ____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    lstm_1 (LSTM)                    (None, 300)           362400      lstm_input_1[0][0]               \n    ____________________________________________________________________________________________________\n    dense_1 (Dense)                  (None, 1)             301         lstm_1[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_1 (Activation)        (None, 1)             0           dense_1[0][0]                    \n    ====================================================================================================\n    Total params: 362,701\n    Trainable params: 362,701\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_1_ahead = model_predict_1_ahead.predict(X_test) \nlen(predicted_1_ahead)\n```\n\n\n\n\n    9850\n\n\n\n\n```{python:Python3 Jupyter notebook}\nlen(X_test)\n```\n\n\n\n\n    9850\n\n\n\n__\u5165\u529b\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\uff08\uff1d\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306e\u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\uff09 \u3068 \u51fa\u529b\u6570\u5024\uff08\uff1d1\u6642\u70b9\u5f8c\u306e\u4e88\u6e2c\u5024\uff09\u306e\u4ef6\u6570 \u306f \u4e00\u81f4\u3057\u3066\u3044\u308b\u3002__\n\n\n```{python:Python3 Jupyter notebook}\nlen(X_test[0])\n```\n\n\n\n\n    150\n\n\n###__\u4e88\u6e2c\u5024\u3000\u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u306e\u4e00\u81f4\u5ea6\u5408\u3044 \u3092 \u6bd4\u8f03__\n\n\n```{python:Python3 Jupyter notebook}\ndataf =  pd.DataFrame(predicted_1_ahead)\ndataf.columns = [\"predict\"]\ndataf[\"true_value(observed_value)\"] = y_test\ndataf.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x126905be0>\n\n\n\n\n![output_26_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/48560ae3-a4db-0f94-7d53-36e60c98bb46.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf[0:100].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12720bda0>\n\n\n\n\n![output_27_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/24e3df05-c37f-970d-2731-de45b6da82db.png)\n\n\n\n###__\u4e88\u6e2c\u7cbe\u5ea6\u7d50\u679c__\n\n####__1\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03__\n\n* \u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\uff09\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\u3002\n\n\n```{python:Python3 Jupyter notebook}\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x127753ac8>\n\n\n\n\n![output_28_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/0b8de404-1ee1-7644-aac1-decdda17f0cc.png)\n\n\n####__5\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03__\n\n```{python:Python3 Jupyter notebook}\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle*5].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x1276b9dd8>\n\n\n\n\n![output_29_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/2483610e-d1e7-cecc-f7a7-e88ab494d0e1.png)\n\n\n####__20\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03__\n\n```python\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x125f9bf98>\n\n\n\n\n![output_30_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a5c5ba79-e6b1-7b8a-d33a-118aa7bf7f97.png)\n\n\n####__\u30e9\u30b9\u30c820\u5468\u671f\u5206 \u306e \u4e88\u6e2c\u7d50\u679c \u3068 \u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09 \u3092 \u6bd4\u8f03__\n\n```python\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x128128d30>\n\n\n\n\n![output_31_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/59eb5265-e509-25b2-b59f-d71172840ab2.png)\n\n\n___\n\n###__LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570\uff08\uff1dLSTM\u30d6\u30ed\u30c3\u30af\u6570\uff09 \u3092 2\u500d \u306e 600 \u306b\u3059\u308b\u3002__\n\n\n```{python:Python3 Jupyter notebook}\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_1_ahead2 = Sequential()  \nmodel_predict_1_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_1_ahead2.add(Dense(1))  \nmodel_predict_1_ahead2.add(Activation(\"linear\"))  \nmodel_predict_1_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_1_ahead2 =   model_predict_1_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 9s - loss: 0.4234 - val_loss: 5.7625\n    Epoch 2/10\n    95/95 [==============================] - 7s - loss: 4.5148 - val_loss: 0.0040\n    Epoch 3/10\n    95/95 [==============================] - 7s - loss: 0.2204 - val_loss: 1.7629\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_1_ahead2.history['loss']\nval_loss = model_1_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_34_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/fd712c16-a56c-e5bf-b3b8-fbb0ebda319c.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nmodel_predict_1_ahead2.summary()\n```\n\n    ____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    lstm_2 (LSTM)                    (None, 600)           1444800     lstm_input_2[0][0]               \n    ____________________________________________________________________________________________________\n    dense_2 (Dense)                  (None, 1)             601         lstm_2[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n    ====================================================================================================\n    Total params: 1,445,401\n    Trainable params: 1,445,401\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_1_ahead2 = model_predict_1_ahead.predict(X_test) \nlen(predicted_1_ahead2)\n```\n\n\n\n\n    9850\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf2 =  pd.DataFrame(predicted_1_ahead2)\ndataf2.columns = [\"predict\"]\ndataf2[\"true_value(observed_value)\"] = y_test\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf2.shape\n```\n\n\n\n\n    (9850, 2)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\uff09\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12a010c50>\n\n\n\n![output_39_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/3b914e3f-a1b0-4ea2-e0dc-c52df9f39f00.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle*5].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12a8d1400>\n\n\n\n\n![output_40_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/9436f509-a018-392f-cad2-ea6e28b1d30c.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12ec8cc50>\n\n\n\n\n![output_41_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/871fa5ba-57b3-11da-f34d-947726d53fad.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf2[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12ecb3e48>\n\n\n\n\n![output_42_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/930749a3-759b-73f5-9a41-de2167b2206f.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u7d50\u679c\u306f\u3042\u307e\u308a\u5909\u308f\u3089\u306a\u304b\u3063\u305f\u3002\n```\n\n___\n\n##__5\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c__\n\n###__\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09105\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210__\n\n__5\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\n# 5\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e5\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=5)  \n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89251\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  89251\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9250\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  9250\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.03443538]\n     [ 0.08851131]\n     [ 0.11744915]\n     [ 0.16364786]\n     [ 0.24978178]\n     [ 0.29996187]\n     [ 0.3943597 ]\n     [ 0.40790129]\n     [ 0.47970153]\n     [ 0.54284827]]\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.28199213]\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.07660351]\n     [ 0.09925078]\n     [ 0.14849661]\n     [ 0.20115338]\n     [ 0.33418164]\n     [ 0.41373467]\n     [ 0.41169616]\n     [ 0.45590469]\n     [ 0.56993065]\n     [ 0.58197538]]\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.34252502]\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_5_ahead = Sequential()  \nmodel_predict_5_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_5_ahead.add(Dense(1))  \nmodel_predict_5_ahead.add(Activation(\"linear\"))  \nmodel_predict_5_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_5_ahead =   model_predict_5_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 5s - loss: 0.5488 - val_loss: 0.5149\n    Epoch 2/10\n    95/95 [==============================] - 2s - loss: 0.2190 - val_loss: 0.0118\n    Epoch 3/10\n    95/95 [==============================] - 2s - loss: 0.6881 - val_loss: 0.1715\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_5_ahead.history['loss']\nval_loss = model_5_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_50_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/191a5cd6-fd72-6f04-f966-7de9c1191dde.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nmodel_predict_5_ahead.summary()\n```\n\n    ____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    lstm_6 (LSTM)                    (None, 300)           362400      lstm_input_6[0][0]               \n    ____________________________________________________________________________________________________\n    dense_6 (Dense)                  (None, 1)             301         lstm_6[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_6 (Activation)        (None, 1)             0           dense_6[0][0]                    \n    ====================================================================================================\n    Total params: 362,701\n    Trainable params: 362,701\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff081\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_5_ahead = model_predict_5_ahead.predict(X_test) \nlen(predicted_5_ahead)\n```\n\n\n\n\n    9250\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_5 =  pd.DataFrame(predicted_5_ahead)\ndataf_ahead_5.columns = [\"predict\"]\ndataf_ahead_5[\"true_value(observed_value)\"] = y_test\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_5.shape\n```\n\n\n\n\n    (9250, 2)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nprint(dataf_ahead_5.ix[0,])\n```\n\n    predict                       0.174520\n    true_value(observed_value)   -0.342525\n    Name: 0, dtype: float64\n\n\n\n```{python:Python3 Jupyter notebook}\nprint(dataf_ahead_5.ix[1:10,])\n```\n\n         predict  true_value(observed_value)\n    1   0.131979                   -0.400385\n    2   0.086908                   -0.390119\n    3   0.043136                   -0.519381\n    4  -0.001042                   -0.565102\n    5  -0.047355                   -0.583281\n    6  -0.095603                   -0.618386\n    7  -0.144298                   -0.681583\n    8  -0.189855                   -0.730022\n    9  -0.236214                   -0.791564\n    10 -0.281993                   -0.826755\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u4e88\u6e2c\u5024(predicted\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001true_value\u3088\u308a\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4f4e\u3081\u306b\u4e88\u6e2c\u3057\u3066\u3044\u308b\uff09\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12968e240>\n\n\n\n\n![output_57_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/bfa801c5-4323-971d-996b-836f8d9de472.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u4e88\u6e2c\u5024\uff08\u9752\u7dda\uff09\u306f\u3001\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\u3001\u7dd1\u7dda\uff09\u3088\u308a\u3082\u3001\u9045\u308c\u3092\u4f34\u3063\u3066\u3044\u308b\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*5].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12c962470>\n\n\n\n\n![output_59_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/6372fbc9-51de-6922-a456-a29bf0e9dbc4.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12dedbe48>\n\n\n\n\n![output_60_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/c1601b32-4f3a-248f-7aa7-8c278fa0464a.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12f7a1160>\n\n\n\n\n![output_61_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/d4ae39ab-7578-e78b-9a55-e83c89a131cf.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_5_ahead2 = Sequential()  \nmodel_predict_5_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_5_ahead2.add(Dense(1))  \nmodel_predict_5_ahead2.add(Activation(\"linear\"))  \nmodel_predict_5_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_5_ahead2 =   model_predict_5_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 10s - loss: 0.5913 - val_loss: 6.0171\n    Epoch 2/10\n    95/95 [==============================] - 11s - loss: 2.9635 - val_loss: 0.0176\n    Epoch 3/10\n    95/95 [==============================] - 12s - loss: 0.2258 - val_loss: 2.5224\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_5_ahead2.history['loss']\nval_loss = model_5_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_64_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/6bb1dc76-5be9-b44a-8f5e-9365ec9b5293.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nmodel_predict_5_ahead2.summary()\n```\n\n    ____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    lstm_7 (LSTM)                    (None, 600)           1444800     lstm_input_7[0][0]               \n    ____________________________________________________________________________________________________\n    dense_7 (Dense)                  (None, 1)             601         lstm_7[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_7 (Activation)        (None, 1)             0           dense_7[0][0]                    \n    ====================================================================================================\n    Total params: 1,445,401\n    Trainable params: 1,445,401\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff085\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_5_ahead2 = model_predict_5_ahead2.predict(X_test) \nlen(predicted_5_ahead2)\n```\n\n\n\n\n    9250\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_5_2 =  pd.DataFrame(predicted_5_ahead2)\ndataf_ahead_5_2.columns = [\"predict\"]\ndataf_ahead_5_2[\"true_value(observed_value)\"] = y_test\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_5_2.shape\n```\n\n\n\n\n    (9250, 2)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nprint(dataf_ahead_5_2.ix[1:10,])\n```\n\n         predict  true_value(observed_value)\n    1   0.663027                   -0.400385\n    2   0.310947                   -0.390119\n    3   0.048220                   -0.519381\n    4  -0.140018                   -0.565102\n    5  -0.115694                   -0.583281\n    6  -0.040395                   -0.618386\n    7  -0.061709                   -0.681583\n    8  -0.089087                   -0.730022\n    9  -0.034119                   -0.791564\n    10 -0.024306                   -0.826755\n\n\n\n```{python:Python3 Jupyter notebook}\n# 1\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12968a160>\n\n\n\n\n![output_70_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/9c530c73-e0b2-1602-a5fa-74b034824977.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 5\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u4e88\u6e2c\u5024\u7cfb\u5217\u306f\u304a\u304b\u3057\u306a\u6319\u52d5\u3092\u793a\u3057\u3066\u3044\u308b\ndataf_ahead_5_2[0:steps_per_cycle*5].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13142c710>\n\n\n\n\n![output_71_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/92d3bcdc-4d09-69bb-3eab-054274ed674a.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x141211d30>\n\n\n\n\n![output_72_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a9698583-c686-15c1-21dd-b049c584c213.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5_2[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12cf18b38>\n\n\n\n\n![output_73_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/5484434f-be83-58fe-70c2-4cfa4683d117.png)\n\n\n___\n\n##__50\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c__\n\n###__\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09150\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210__\n\n__50\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210__\n\n\n```{python:Python3 Jupyter notebook}\n# 50\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=50)  \n```\n\n\n```{python:Python3 Jupyter notebook}\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  82501\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  82501\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  2500\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  2500\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.03443538]\n     [ 0.08851131]\n     [ 0.11744915]\n     [ 0.16364786]\n     [ 0.24978178]\n     [ 0.29996187]\n     [ 0.3943597 ]\n     [ 0.40790129]\n     [ 0.47970153]\n     [ 0.54284827]]\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.10794316]\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.07660351]\n     [ 0.09925078]\n     [ 0.14849661]\n     [ 0.20115338]\n     [ 0.33418164]\n     [ 0.41373467]\n     [ 0.41169616]\n     [ 0.45590469]\n     [ 0.56993065]\n     [ 0.58197538]]\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [ 0.02981266]\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_50_ahead = Sequential()  \nmodel_predict_50_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_50_ahead.add(Dense(1))  \nmodel_predict_50_ahead.add(Activation(\"linear\"))  \nmodel_predict_50_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba \u306f\u3001steps_per_cycle * 1 \u306b\u3059\u308b\n# \u3010 \u8aac\u660e\u5909\u6570 \u3011X_train[0:steps_per_cycle]\n# \u3010 \u76ee\u7684\u5909\u6570 \u3011y_train[0:steps_per_cycle]\n#\n# \u3053\u308c\u306f\u3001\u30e2\u30c7\u30eb \u306b \u4e0e\u3048\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf \u3068\u3057\u3066\u3001\n# \u300c\u76ee\u7684\u5909\u6570\uff1a\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\u300d\u53ca\u3073\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\n# \u3092\u3001\u4e0e\u3048\u308b\u969b\u3001\n# \u76ee\u7684\u5909\u6570\uff08\uff11\u6642\u70b9\u306e\u30c7\u30fc\u30bf\uff09\u304c\u3001sin\u66f2\u7dda\uff11\u5468\u671f \u5185\u306e\u4efb\u610f\u306e\u70b9\u3067\u3042\u308b\u5834\u5408\n# \u306e\uff08\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\uff09\uff08\u8aac\u660e\u5909\u6570\uff09\u300c\u305d\u306e\u76f4\u524d\u306e1.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u300d\u3092\u3001\n# \u30e2\u30c7\u30eb\u304c\u5b66\u7fd2\u3059\u308b\u6a5f\u4f1a\u3092\u4e0e\u3048\u308b\u305f\u3081\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead =   model_predict_50_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 4s - loss: 0.6719 - val_loss: 0.4122\n    Epoch 2/10\n    95/95 [==============================] - 2s - loss: 0.3176 - val_loss: 0.0783\n    Epoch 3/10\n    95/95 [==============================] - 4s - loss: 0.3776 - val_loss: 0.0041\n    Epoch 4/10\n    95/95 [==============================] - 4s - loss: 0.0833 - val_loss: 6.8842e-04\n    Epoch 5/10\n    95/95 [==============================] - 4s - loss: 0.0105 - val_loss: 0.0111\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_50_ahead.history['loss']\nval_loss = model_50_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_80_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/70aaffed-5422-1b89-97a4-e05c1f7f6319.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0850\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_50_ahead = model_predict_50_ahead.predict(X_test) \nlen(predicted_50_ahead)\n```\n\n\n\n\n    2500\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_50 =  pd.DataFrame(predicted_50_ahead)\ndataf_ahead_50.columns = [\"predict\"]\ndataf_ahead_50[\"true_value(observed_value)\"] = y_test\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_50[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x1394724a8>\n\n\n\n\n![output_83_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/0fa781ec-e5c1-bc67-2786-a48b66303444.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 50\u671f\u5148\u306e\u4e88\u6e2c\u5024\u306f\u3001\u6b63\u89e3\u5024\u3088\u308a\u3082\u5468\u671f\u304c\u82e5\u5e72\u65e9\u3081\u306b\u51fa\u3066\u3044\u308b\u3002\n# \u632f\u5e45\u306e\u5927\u304d\u3055\uff08\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\uff09\u306f\u3001\u6b63\u89e3\u5024\u3068\u307b\u307c\u540c\u3058\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x131402588>\n\n\n\n\n![output_85_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/8aa6d033-c362-37b3-75db-57dfadf557dc.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x139279f60>\n\n\n\n\n![output_86_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/5f2deac4-8e5e-3691-2b6a-a2da45ed0c7a.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c85\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50[-5*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x132074c50>\n\n\n\n\n![output_87_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/11dfc111-6fa6-07fa-ba00-2fa8a82e9f2e.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c83\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u6b63\u89e3\u5024\u3088\u308a\u3082\u5468\u671f\u304c\u82e5\u5e72\u65e9\u3081\u306b\u51fa\u3066\u3044\u308b\u3002\n# \u632f\u5e45\u306e\u5927\u304d\u3055\uff08\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\uff09\u306f\u3001\u6b63\u89e3\u5024\u3068\u307b\u307c\u540c\u3058\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\ndataf_ahead_50[-2*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x131f033c8>\n\n\n\n\n![output_88_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/ebb74454-cf67-c5b0-2195-47144dfc3b06.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# LSTM\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9\u6570 \uff1d LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570 \u3092 600\u306b\u5897\u3084\u3057\u3066\u307f\u308b\u3002\nmodel_predict_50_ahead2 = Sequential()  \nmodel_predict_50_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_50_ahead2.add(Dense(1))  \nmodel_predict_50_ahead2.add(Activation(\"linear\"))  \nmodel_predict_50_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead2 =   model_predict_50_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=10, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/10\n    95/95 [==============================] - 11s - loss: 0.5276 - val_loss: 7.0810\n    Epoch 2/10\n    95/95 [==============================] - 11s - loss: 3.8192 - val_loss: 0.1959\n    Epoch 3/10\n    95/95 [==============================] - 12s - loss: 0.4445 - val_loss: 0.1841\n    Epoch 4/10\n    95/95 [==============================] - 12s - loss: 0.2441 - val_loss: 0.0827\n    Epoch 5/10\n    95/95 [==============================] - 12s - loss: 0.1769 - val_loss: 0.0435\n    Epoch 6/10\n    95/95 [==============================] - 12s - loss: 0.6151 - val_loss: 0.0145\n    Epoch 7/10\n    95/95 [==============================] - 12s - loss: 0.1097 - val_loss: 0.0034\n    Epoch 8/10\n    95/95 [==============================] - 12s - loss: 0.0668 - val_loss: 5.6836e-04\n    Epoch 9/10\n    95/95 [==============================] - 11s - loss: 0.0359 - val_loss: 0.0023\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u307e\u3060\u8aa4\u5dee\u306f\u6e1b\u308a\u305d\u3046\u306a\u306e\u3067\u3001epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_50_ahead2 =   model_predict_50_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/20\n    95/95 [==============================] - 7s - loss: 0.0141 - val_loss: 0.0029\n    Epoch 2/20\n    95/95 [==============================] - 7s - loss: 0.0041 - val_loss: 2.7682e-04\n    Epoch 3/20\n    95/95 [==============================] - 7s - loss: 0.0014 - val_loss: 1.7764e-04\n    Epoch 4/20\n    95/95 [==============================] - 12s - loss: 0.0010 - val_loss: 2.1413e-04\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_50_ahead2.history['loss']\nval_loss = model_50_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_92_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/0ae08d84-c548-9261-3d8a-a13f6dc1b2ad.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0850\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_50_ahead2 = model_predict_50_ahead2.predict(X_test) \nlen(predicted_50_ahead2)\n```\n\n\n\n\n    2500\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_50_2 =  pd.DataFrame(predicted_50_ahead2)\ndataf_ahead_50_2.columns = [\"predict\"]\ndataf_ahead_50_2[\"true_value(observed_value)\"] = y_test\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_50_2.shape\n```\n\n\n\n\n    (2500, 2)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 50\u671f\u5148\u306e\u4e88\u6e2c\u5024 \u3068 \u5b9f\u6e2c\u5024 \u306f\u3001\u307b\u307c\u4e00\u81f4\u3057\u3066\u3044\u308b\u3002\ndataf_ahead_50_2[0:steps_per_cycle].plot()# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_5[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13dde4ba8>\n\n\n\n\n![output_96_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a65263d7-aa04-1603-6bde-7c7ceac03cc1.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50_2[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13dffa9e8>\n\n\n\n\n![output_97_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/5e0df4c8-b2af-67f9-ece5-eeedbcf54e9a.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\ndataf_ahead_50_2[-20*steps_per_cycle*20:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14fcec160>\n\n\n\n\n![output_98_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/f8ebf508-bb0f-7df4-6bb2-6589936a7626.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u4e88\u6e2c\u5024 \u3068 \u5b9f\u6e2c\u5024 \u306f\u3001\u307b\u307c\u4e00\u81f4\u3057\u3066\u3044\u308b\u3002\ndataf_ahead_50_2[-2*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14e7c4588>\n\n\n\n\n![output_99_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/c7d85388-e728-8716-59e5-bb91d7d1d70d.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# LSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\uff08300\uff09\u3092\uff12\u500d\u306e600\u306b\u5897\u3084\u3057\u305f\u7d50\u679c\u3001\n# \u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u5024\u306f\u307b\u307c\u4e00\u81f4\u3059\u308b\u307e\u3067\u306b\u4e88\u6e2c\u7cbe\u5ea6\u304c\u4e0a\u304c\u3063\u305f\u3002\n```\n\n___\n\n\n##__150\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3059\u308b LSTM\u30e2\u30c7\u30eb \u306e \u4e88\u6e2c\u7cbe\u5ea6 \u3092 \u691c\u8a3c__\n\n###__\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09150\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210__\n\n__150\u6642\u70b9\u5148\u306e\u5024 \u3092 \u4e88\u6e2c\u3059\u308b \u56de\u5e30\u30e2\u30c7\u30eb \u306e \u5b66\u7fd2\u7528 \u53ca\u3073 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3092 \u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n```\n\n\n```{python:Python3 Jupyter notebook}\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  67501\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  67501\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5f97\u3089\u308c\u306a\u304b\u3063\u305f\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5143\u306e\u30c7\u30fc\u30bf\u5468\u671f\u6570\u3092\u5897\u3084\u3059\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 2000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13e20ccf8>\n\n\n\n\n![output_105_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/77f0ce62-9093-e281-d6e1-0fc405ed4c57.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n```\n\n\n```{python:Python3 Jupyter notebook}\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  157501\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  157501\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  0\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 20000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13e22b2e8>\n\n\n\n\n![output_108_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/7872abef-9c74-d708-a98e-4940e8ceedae.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 150\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e50\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=150)  \n```\n\n\n```{python:Python3 Jupyter notebook}\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  1777501\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  1777501\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  177500\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  177500\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u4eca\u5ea6\u306f\u3001\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u5f97\u3089\u308c\u305f\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.02051963]\n     [ 0.05079059]\n     [ 0.17277323]\n     [ 0.17119517]\n     [ 0.25317441]\n     [ 0.30671668]\n     [ 0.39315019]\n     [ 0.41498027]\n     [ 0.48016081]\n     [ 0.52335404]]\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.1055117]\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.05010115]\n     [ 0.13582259]\n     [ 0.17097283]\n     [ 0.28140028]\n     [ 0.30962029]\n     [ 0.36655806]\n     [ 0.40651334]\n     [ 0.48688099]\n     [ 0.56543229]\n     [ 0.58529144]]\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [ 0.03704185]\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001300\u306b\u3059\u308b\nmodel_predict_150_ahead = Sequential()  \nmodel_predict_150_ahead.add(LSTM(300, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_150_ahead.add(Dense(1))  \nmodel_predict_150_ahead.add(Activation(\"linear\"))  \nmodel_predict_150_ahead.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\n# epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_150_ahead =   model_predict_150_ahead.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/20\n    95/95 [==============================] - 4s - loss: 0.4581 - val_loss: 0.4851\n    Epoch 2/20\n    95/95 [==============================] - 2s - loss: 0.3592 - val_loss: 0.0381\n    Epoch 3/20\n    95/95 [==============================] - 2s - loss: 0.2773 - val_loss: 0.0080\n    Epoch 4/20\n    95/95 [==============================] - 2s - loss: 0.0593 - val_loss: 9.9821e-04\n    Epoch 5/20\n    95/95 [==============================] - 2s - loss: 0.0056 - val_loss: 0.0074\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_150_ahead.history['loss']\nval_loss = model_150_ahead.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_115_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/fa600385-6323-a4c9-d5ed-343cb8b08e99.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff08150\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_150_ahead = model_predict_150_ahead.predict(X_test[0:10000]) \nlen(predicted_150_ahead)\n```\n\n\n\n\n    10000\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_150 =  pd.DataFrame(predicted_150_ahead)\ndataf_ahead_150.columns = [\"predict\"]\ndataf_ahead_150[\"true_value(observed_value)\"] = y_test[0:10000]\n```\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_150.shape\n```\n\n\n\n\n    (10000, 2)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c81\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13e6720f0>\n\n\n\n\n![output_119_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/00acc4fa-f905-c796-2011-0dc201f11538.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x1377bcc88>\n\n\n\n\n![output_120_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/a799438c-c044-4ff9-ce7d-9361081bcbc6.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c820\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[-20*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x137933a90>\n\n\n\n\n![output_121_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/558042c4-7ad8-f214-5d8d-d6d811f2fe56.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u3068\u5b9f\u6e2c\u5024 \u306f \u307b\u307c\u4e00\u81f4\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u3084\u3084\u5927\u304d\u3044 \ndataf_ahead_150[-2*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12fa0b1d0>\n\n\n\n\n![output_122_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/80c51d09-b255-0acf-8d7f-4632a4fd6aa7.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom keras.models import Sequential  \nfrom keras.layers.core import Dense, Activation  \nfrom keras.layers.recurrent import LSTM\n\n# \u4e2d\u9593\u5c64\uff08LSTM\u5c64\uff09\u306eLSTM\u30d6\u30ed\u30c3\u30af\u306e\u6570\u306f\u3001600\u306b\u3059\u308b\nmodel_predict_150_ahead2 = Sequential()  \nmodel_predict_150_ahead2.add(LSTM(600, batch_input_shape=(None, int(steps_per_cycle)*1.5, 1), return_sequences=False))  \nmodel_predict_150_ahead2.add(Dense(1))  \nmodel_predict_150_ahead2.add(Activation(\"linear\"))  \nmodel_predict_150_ahead2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n```\n\n\n```{python:Python3 Jupyter notebook}\n# epoch\u6570\u309220\u307e\u3067\u5897\u3084\u3057\u3066\u3001\n# early stopping\u3092\u304b\u3051\u3066\u3001\u81ea\u52d5\u53ce\u675f\u5224\u5b9a\u3057\u305f\u3089\u300120\u307e\u3067epoch\u5b9f\u65bd\u305b\u305a\u306b\u9014\u4e2d\u3067\u6b62\u3081\u308b\u3002\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel_150_ahead2 =   model_predict_150_ahead2.fit(X_train[0:steps_per_cycle], y_train[0:steps_per_cycle], \n                                                    batch_size=int(len(X_train)/6), \n                                                    nb_epoch=20, \n                                                    validation_split=0.05,\n                                                    callbacks=[early_stopping])\n```\n\n    Train on 95 samples, validate on 5 samples\n    Epoch 1/20\n    95/95 [==============================] - 10s - loss: 0.5216 - val_loss: 3.8104\n    Epoch 2/20\n    95/95 [==============================] - 7s - loss: 2.3219 - val_loss: 0.1407\n    Epoch 3/20\n    95/95 [==============================] - 7s - loss: 0.3426 - val_loss: 0.0352\n    Epoch 4/20\n    95/95 [==============================] - 10s - loss: 0.1739 - val_loss: 0.0010\n    Epoch 5/20\n    95/95 [==============================] - 9s - loss: 0.0503 - val_loss: 0.0291\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_150_ahead2.history['loss']\nval_loss = model_150_ahead2.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_125_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/d8050d1f-fe95-0644-968e-9ab9e4c40709.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff08150\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\npredicted_150_ahead2 = model_predict_150_ahead2.predict(X_test[0:10000]) \nlen(predicted_150_ahead2)\n```\n\n\n\n\n    10000\n\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf_ahead_150_2 =  pd.DataFrame(predicted_150_ahead2)\ndataf_ahead_150_2.columns = [\"predict\"]\ndataf_ahead_150_2[\"true_value(observed_value)\"] = y_test[0:10000]\ndataf_ahead_150_2.ix[0:10, ]\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predict</th>\n      <th>true_value(observed_value)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.123705</td>\n      <td>0.037042</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.170413</td>\n      <td>0.041712</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.218634</td>\n      <td>0.140744</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.265900</td>\n      <td>0.210152</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.312858</td>\n      <td>0.229661</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.354751</td>\n      <td>0.325469</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.396260</td>\n      <td>0.343859</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.434023</td>\n      <td>0.466696</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.468966</td>\n      <td>0.490038</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.500620</td>\n      <td>0.514457</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.528240</td>\n      <td>0.611611</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c81\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# LSTM\u30d6\u30ed\u30c3\u30af\u6570\u3092\uff12\u500d\u306e600\u306b\u3057\u305f\u7d50\u679c\u3001\u4ee5\u4e0b\u304c\u5f97\u3089\u308c\u305f\u3002\n# \u5468\u671f: \u4e88\u6e2c\u5024\u306e\u5468\u671f\u304c\u3084\u3084\u5c0f\u3055\u304f\u306a\u3063\u305f\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u304f\u306a\u3063\u305f\ndataf_ahead_150_2[0:steps_per_cycle].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x14ff44c88>\n\n\n\n\n![output_128_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/dd059b6f-1e0c-2bad-bb0d-d7d24fc180af.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 20\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u3044\ndataf_ahead_150_2[0:steps_per_cycle*20].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x137988390>\n\n\n\n\n![output_129_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/8356c883-4166-b818-9361-8b5a176a1fad.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u30e9\u30b9\u30c82\u5468\u671f\u5206\u306e\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\uff08\u6b63\u89e3\u5024\uff09\u3092\u6bd4\u8f03\u3057\u3066\u307f\u308b\n# \u5468\u671f: \u4e88\u6e2c\u5024\u306e\u5468\u671f\u304c\u3084\u3084\u5c0f\u3055\u3044\n# \u632f\u5e45: \u4e88\u6e2c\u5024\u306e\u65b9\u304c\u5c0f\u3055\u3044 \ndataf_ahead_150_2[-2*steps_per_cycle:].plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x13e822748>\n\n\n\n\n![output_130_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/e7388c21-93d9-c609-0385-0cd60953832f.png)\n", "tags": ["DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "\u6642\u7cfb\u5217\u89e3\u6790", "LSTM", "Keras"]}