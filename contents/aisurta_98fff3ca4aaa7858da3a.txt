{"context": "\n\n\u6982\u8981\nTheano\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\uff08 http://deeplearning.net/tutorial/contents.html \uff09\n\u3092\u3082\u3068\u306bKaggle\u306eMNIST\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002\n\n\u5b9f\u88c5\nfrom __future__ import print_function\n\n__docformat__ = 'restructedtext en'\n\nimport six.moves.cPickle as pickle\nimport timeit\nimport numpy \nimport theano\nimport theano.tensor as T\nimport numpy as np\n\nrng = numpy.random.RandomState(8000)\n\n#\u5171\u6709\u5909\u6570\u306e\u5b9a\u7fa9\n# \u3053\u306e\u8868\u73fe\u3067\u4f5c\u3089\u308c\u308b\u30c7\u30fc\u30bf\u306f\n\n#-------------------------------------------------------------------------\n#Logistic Regression \u30af\u30e9\u30b9\n#-------------------------------------------------------------------------\nclass LogisticRegression(object):\n    \"\"\"Multi-class Logistic Regression Clas\uff53\n    \"\"\"\n    def __init__(self, input, n_in, n_out):\n\n\n        #MODEL\n        #\u5171\u6709\u5909\u6570\u8868\u73fe\n        #W\u306b\u3064\u3044\u3066\n        self.W = theano.shared(\n            value=numpy.zeros(\n                (n_in, n_out),\n                dtype=theano.config.floatX\n            ),\n            name='W',\n            borrow=True\n        )\n        #borrow \u306f Python \u7a7a\u9593\u4e0a\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u5b9f\u4f53\u3092 \u5171\u6709\u5909\u6570\u3067\u3082\u5171\u6709\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u6c7a\u3081\u308b\u3002\n        #\uff42\u306b\u3064\u3044\u3066\n        self.b = theano.shared(\n            value=numpy.zeros(\n                (n_out,),\n                dtype=theano.config.floatX\n            ),\n            name='b',\n            borrow=True\n        )\n\n\n        #softmax\u95a2\u6570\u306e\u8a08\u7b97\uff08\u30af\u30e9\u30b9\u306b\u5272\u308a\u3042\u305f\u308b\u78ba\u7387\u3092\u8a08\u7b97\uff09\n        #\uff50(y\uff1di|x,W,b)\n        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n        #argmax\u3092\u3068\u3063\u3066\u4e88\u6e2c\u5024\u3092\u51fa\u529b\n        #ypred\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n        #.paramsn\u306b\u683c\u7d0d\n        self.params = [self.W, self.b]\n        #.input\u306b\u683c\u7d0d        \n        self.input = input\n\n    #\u640d\u5931\u95a2\u6570\n    def negative_log_likelihood(self, y):\n        #\u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9 \n        #self.p_y_given_x\u306fsoftmax\u95a2\u6570\uff08\u51fa\u529byn\uff09\n        #T.arange(y.shape[0])\u306fNumpy\u306e\u884c\u3001\u5217[]\n        #\u30fcln(p(t\uff5cw)).PRML4.90\u5f0f  \n        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n\n    #\u30a8\u30e9\u30fc\u7387\n    def errors(self, y):\n\n        #\u51fa\u529b\u306e\u6b21\u5143\u306e\u78ba\u8a8d\n        if y.ndim != self.y_pred.ndim:\n            raise TypeError(\n                'y should have the same shape as self.y_pred',\n                ('y', y.type, 'y_pred', self.y_pred.type)\n            )\n        # \u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d \n        if y.dtype.startswith('int'):\n            # neq\u306fnot equal \u51fa\u529b\u3057\u305f\u7b54\u3048\u304c\u6b63\u89e3\u3068\u6b63\u3057\u3044\u78ba\u7387\u3092\u51fa\u529b \n            # \u7b2c\u4e00\u5f15\u6570\uff1a\u51fa\u529b\u78ba\u7387\u753b\u6700\u5927\u306eargument\n            # \u7b2c\u4e8c\u5f15\u6570\uff1a\u6b63\u89e3\u30e9\u30d9\u30eb \n            return T.mean(T.neq(self.y_pred, y))\n        else:\n            raise NotImplementedError()\n\n#\u30df\u30cb\u30d0\u30c3\u30c1\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\ndef sgd_optimization_mnist(learning_rate=0.10, n_epochs=100,\n                           dataset='mnist.pkl.gz',\n                           batch_size=500):\n\n    dtype = np.float32\n    data = np.loadtxt(\"../input/train.csv\", dtype=dtype,\n                           delimiter=',', skiprows=1)\n    test_data = np.loadtxt(\"../input/test.csv\", dtype=dtype,\n                          delimiter=',', skiprows=1)\n    test_datasets=[test_data]\n\n    print (data.shape)\n\n    labels = data[:,0]\n    data = data[:, 1:]                               \n\n    index = T.lscalar()  # minibatch\u306eindex\n    NUM_TRAIN = len(data)\n    NUM_TEST = len(test_data)\n    if NUM_TRAIN % batch_size != 0: \n        whole = (NUM_TRAIN // batch_size) * batch_size\n        data = data[:whole]\n        NUM_TRAIN = len(data) \n\n    # random \u4e26\u3079\u66ff\u3048\n    indices = rng.permutation(NUM_TRAIN)\n    data, labels = data[indices, :], labels[indices]\n    # batch_size \u306f\u3000500, (480, 20)\u3057\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e96\u30d1\u30fc\u30bb\u30f3\u30c8\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3001\u6b8b\u308a\u3067\uff56\uff41\uff4c\uff49\uff44\uff41\uff54\uff45\u3059\u308b\n    is_train = numpy.array( ([0]* (batch_size - 20) + [1] * 20) * (NUM_TRAIN // batch_size))\n\n    test_indices = rng.permutation(NUM_TEST)\n    test_data, test_labels = test_data[test_indices, :], labels[test_indices]\n    is_test = numpy.array( ([0]* (batch_size - 20) + [1] * 20) * (NUM_TEST // batch_size))\n    # trai_set,valid_set,test_set\u306e\u6e96\u5099\n    train_set_x, train_set_y = numpy.array(data[is_train==0]), labels[is_train==0]\n    valid_set_x, valid_set_y = numpy.array(data[is_train==1]), labels[is_train==1]\n    test_set_x,test_set_y    = numpy.array(data[is_test==0]), labels[is_test==0]\n    # minibatch\u306e\u8a08\u7b97\n    n_train_batches = len(train_set_y) // batch_size\n    n_valid_batches = len(valid_set_y) // batch_size\n    n_test_batches = len(test_set_y)   // batch_size\n\n    ##############\n    # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 #\n    ##############\n\n    print('... building the model')\n\n    # long\u578b\u306escalar \n    index = T.lscalar()  \n\n    #matrix\u578b\n    x = T.matrix('x')\n    #int\u578b\u306evector    \n    y = T.ivector('y') \n\n    # logistic regression \u30af\u30e9\u30b9\u306e\u69cb\u7bc9\n    # MNIST\u30c7\u30fc\u30bf\u306f28X28\n    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n\n    # cost function \u306f\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6 \n    cost = classifier.negative_log_likelihood(y)\n\n    # \u30df\u30cb\u30d0\u30c3\u30c1\u578b\u3067theano.function\u3067\u307e\u3068\u3081\u308b \n    train_set_x = theano.shared(numpy.asarray(train_set_x, dtype=theano.config.floatX))\n    train_set_y = T.cast(theano.shared(numpy.asarray(train_set_y, dtype=theano.config.floatX)), 'int32')\n    test_set_x  = theano.shared(numpy.asarray(test_set_x, dtype=theano.config.floatX))\n    test_set_y   = T.cast(theano.shared(numpy.asarray(test_set_y, dtype=theano.config.floatX)), 'int32')\n    valid_set_x = theano.shared(numpy.asarray(valid_set_x, dtype=theano.config.floatX)) \n    valid_set_y = T.cast(theano.shared(numpy.asarray(valid_set_y, dtype=theano.config.floatX)), 'int32')\n\n    # validate\u30e2\u30c7\u30eb\n    validate_model = theano.function(\n        inputs=[index],\n        outputs=classifier.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n\n    # test\u30e2\u30c7\u30eb \n    test_model = theano.function(\n        inputs=[index],\n        outputs=classifier.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n        }\n    )\n\n    #cost\u3092W\u3067\u5fae\u5206\n    g_W = T.grad(cost=cost, wrt=classifier.W)\n    #cost\u3092b\u3067\u5fae\u5206    \n    g_b = T.grad(cost=cost, wrt=classifier.b)\n    # cost function \u306f\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6  \n\n    #\u30d1\u30e9\u30e1\u30fc\u30bf\uff08W,\uff42\uff09\u306e\u66f4\u65b0\n    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n               (classifier.b, classifier.b - learning_rate * g_b)]\n\n    # train\u30e2\u30c7\u30eb\n    # updates\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\uff08W,b\uff09\u306e\u66f4\u65b0\u3092\u884c\u3046\n    train_model = theano.function(\n        inputs=[index],\n        outputs=cost,\n        updates=updates,\n        givens={\n            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n\n    #################\n    # \u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 #\n    #################\n    print('... training the model')\n    # LOOP\u56de\u6570\u306e\u4e0a\u9650\n    patience = 50000  \n    # 2\u4ee5\u4e0a\u5897\u3048\u306a\u304f\u306a\u3063\u305f\u3089\n    patience_increase = 5  \n    #\u6210\u7e3e\u306e\u4e0a\u9650                         \n    improvement_threshold = 0.995         \n    #patience\u3092\u6e80\u305f\u3059\u304b\u306e\u983b\u5ea6\u3002\u3053\u306e\u5834\u5408\u306fevery epoch\u3067\u30c1\u30a7\u30c3\u30af\u3059\u308b\n    validation_frequency = min(n_train_batches, patience // 2)\n\n    best_validation_loss = numpy.inf\n    test_score = 0.\n    start_time = timeit.default_timer()\n\n    #LOOP\u306e\u306f\u3058\u307e\u308a\n    done_looping = False\n    epoch = 0\n    minibatch_index = 1\n    while (epoch < n_epochs) and (not done_looping):\n        epoch = epoch + 1\n        for minibatch_index in range(n_train_batches):\n\n            minibatch_avg_cost = train_model(minibatch_index)\n            # iteration\u306e\u8a08\u7b97\n            iter = (epoch - 1) * n_train_batches + minibatch_index\n\n            if (iter + 1) % validation_frequency == 0:\n                # zero-one\u640d\u5931\u3092Validation \u3067\u8a08\u7b97\n                validation_losses = [validate_model(i)\n                                     for i in range(n_valid_batches)]\n                this_validation_loss = numpy.mean(validation_losses)\n\n                print(\n                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n                    (\n                        epoch,\n                        minibatch_index + 1,\n                        n_train_batches,\n                        this_validation_loss * 100.\n                    )\n                )\n\n                # validation loss\u304c\u6700\u4f4e\u306e\u5834\u5408\n                if this_validation_loss < best_validation_loss:\n\n                    if this_validation_loss < best_validation_loss *  \\\n                       improvement_threshold:\n                        patience = max(patience, iter * patience_increase)\n\n                    best_validation_loss = this_validation_loss\n                    # test\u30bb\u30c3\u30c8\u3067\u30c6\u30b9\u30c8\u3059\u308b\n\n                    test_losses = [test_model(i)\n                                   for it in range(len(test_datasets))]\n                    test_score = numpy.mean(test_losses)\n\n                    print(\n                        (\n                            '     epoch %i, minibatch %i/%i, test error of'\n                            ' best model %f %%'\n                        ) %\n                        (\n                            epoch,\n                            minibatch_index + 1,\n                            n_train_batches,\n                            test_score * 100.\n                        )\n                    )\n\n                    # best\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n                    with open('best_model.pkl', 'wb') as f:\n                        pickle.dump(classifier, f)\n\n            if patience <= iter:\n                done_looping = True\n                break\n\n    end_time = timeit.default_timer()\n    print(\n        (\n            'Optimization complete with best validation score of %f %%,'\n            'with test performance %f %%'\n        )\n        % (best_validation_loss * 100., test_score * 100.)\n    )\n    print('The code run for %d epochs, with %f epochs/sec' % (\n        epoch, 1. * epoch / (end_time - start_time)))\n\ndef predict():\n    \"\"\"\n    An example of how to load a trained model and use it\n    to predict labels.\n    \"\"\"\n    # Train\u3057\u305f\u30e2\u30c7\u30eb\u3092\u547c\u3073\u51fa\u3059\n    classifier = pickle.load(open('best_model.pkl'))\n\n    # predict\u30e2\u30c7\u30eb\n    predict_model = theano.function(\n    inputs=[classifier.input],\n        outputs=classifier.y_pred) \n\n    # test\u30bb\u30c3\u30c8\u3092\u547c\u3073\u51fa\u3059\n    test_data = np.loadtxt(\"../input/test.csv\", dtype=dtype,\n                          delimiter=',', skiprows=1)\n    #predict\u3059\u308b                          \n    predicted_values = predict_model(test_data)\n    print(\"Predicted values for the first 10 examples in test set:\")\n    print(predicted_values)\n\n    np.savetxt('Submit_TheanoLR3.csv', np.c_[range(1, len(test_data) + 1), predicted_values], delimiter=',', comments = '', header = 'ImageId,Label', fmt='%d')\n\nif __name__ == '__main__':    \n    sgd_optimization_mnist()\n\n\n\u7d50\u679c\n\u7d50\u679c\u306f0.90757\u3002scikit-learn\u30670.90900\u3060\u3063\u305f\u3057\u3001\u3053\u3093\u306a\u3082\u306e\u304b\u3068\u601d\u308f\u308c\u308b\u3002\n#\u6982\u8981\nTheano\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\uff08 http://deeplearning.net/tutorial/contents.html \uff09\n\u3092\u3082\u3068\u306bKaggle\u306eMNIST\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002\n\n#\u5b9f\u88c5\n```py\nfrom __future__ import print_function\n\n__docformat__ = 'restructedtext en'\n\nimport six.moves.cPickle as pickle\nimport timeit\nimport numpy \nimport theano\nimport theano.tensor as T\nimport numpy as np\n\nrng = numpy.random.RandomState(8000)\n\n#\u5171\u6709\u5909\u6570\u306e\u5b9a\u7fa9\n# \u3053\u306e\u8868\u73fe\u3067\u4f5c\u3089\u308c\u308b\u30c7\u30fc\u30bf\u306f\n\n#-------------------------------------------------------------------------\n#Logistic Regression \u30af\u30e9\u30b9\n#-------------------------------------------------------------------------\nclass LogisticRegression(object):\n    \"\"\"Multi-class Logistic Regression Clas\uff53\n    \"\"\"\n    def __init__(self, input, n_in, n_out):\n\n\n        #MODEL\n        #\u5171\u6709\u5909\u6570\u8868\u73fe\n        #W\u306b\u3064\u3044\u3066\n        self.W = theano.shared(\n            value=numpy.zeros(\n                (n_in, n_out),\n                dtype=theano.config.floatX\n            ),\n            name='W',\n            borrow=True\n        )\n        #borrow \u306f Python \u7a7a\u9593\u4e0a\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u5b9f\u4f53\u3092 \u5171\u6709\u5909\u6570\u3067\u3082\u5171\u6709\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u6c7a\u3081\u308b\u3002\n        #\uff42\u306b\u3064\u3044\u3066\n        self.b = theano.shared(\n            value=numpy.zeros(\n                (n_out,),\n                dtype=theano.config.floatX\n            ),\n            name='b',\n            borrow=True\n        )\n\n\n        #softmax\u95a2\u6570\u306e\u8a08\u7b97\uff08\u30af\u30e9\u30b9\u306b\u5272\u308a\u3042\u305f\u308b\u78ba\u7387\u3092\u8a08\u7b97\uff09\n        #\uff50(y\uff1di|x,W,b)\n        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n        #argmax\u3092\u3068\u3063\u3066\u4e88\u6e2c\u5024\u3092\u51fa\u529b\n        #ypred\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n        #.paramsn\u306b\u683c\u7d0d\n        self.params = [self.W, self.b]\n        #.input\u306b\u683c\u7d0d        \n        self.input = input\n\n    #\u640d\u5931\u95a2\u6570\n    def negative_log_likelihood(self, y):\n        #\u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9 \n        #self.p_y_given_x\u306fsoftmax\u95a2\u6570\uff08\u51fa\u529byn\uff09\n        #T.arange(y.shape[0])\u306fNumpy\u306e\u884c\u3001\u5217[]\n        #\u30fcln(p(t\uff5cw)).PRML4.90\u5f0f  \n        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n\n    #\u30a8\u30e9\u30fc\u7387\n    def errors(self, y):\n\n        #\u51fa\u529b\u306e\u6b21\u5143\u306e\u78ba\u8a8d\n        if y.ndim != self.y_pred.ndim:\n            raise TypeError(\n                'y should have the same shape as self.y_pred',\n                ('y', y.type, 'y_pred', self.y_pred.type)\n            )\n        # \u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d \n        if y.dtype.startswith('int'):\n            # neq\u306fnot equal \u51fa\u529b\u3057\u305f\u7b54\u3048\u304c\u6b63\u89e3\u3068\u6b63\u3057\u3044\u78ba\u7387\u3092\u51fa\u529b \n            # \u7b2c\u4e00\u5f15\u6570\uff1a\u51fa\u529b\u78ba\u7387\u753b\u6700\u5927\u306eargument\n            # \u7b2c\u4e8c\u5f15\u6570\uff1a\u6b63\u89e3\u30e9\u30d9\u30eb \n            return T.mean(T.neq(self.y_pred, y))\n        else:\n            raise NotImplementedError()\n\n#\u30df\u30cb\u30d0\u30c3\u30c1\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\ndef sgd_optimization_mnist(learning_rate=0.10, n_epochs=100,\n                           dataset='mnist.pkl.gz',\n                           batch_size=500):\n                               \n    dtype = np.float32\n    data = np.loadtxt(\"../input/train.csv\", dtype=dtype,\n                           delimiter=',', skiprows=1)\n    test_data = np.loadtxt(\"../input/test.csv\", dtype=dtype,\n                          delimiter=',', skiprows=1)\n    test_datasets=[test_data]\n    \n    print (data.shape)\n\n    labels = data[:,0]\n    data = data[:, 1:]                               \n\n    index = T.lscalar()  # minibatch\u306eindex\n    NUM_TRAIN = len(data)\n    NUM_TEST = len(test_data)\n    if NUM_TRAIN % batch_size != 0: \n        whole = (NUM_TRAIN // batch_size) * batch_size\n        data = data[:whole]\n        NUM_TRAIN = len(data) \n        \n    # random \u4e26\u3079\u66ff\u3048\n    indices = rng.permutation(NUM_TRAIN)\n    data, labels = data[indices, :], labels[indices]\n    # batch_size \u306f\u3000500, (480, 20)\u3057\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e96\u30d1\u30fc\u30bb\u30f3\u30c8\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3001\u6b8b\u308a\u3067\uff56\uff41\uff4c\uff49\uff44\uff41\uff54\uff45\u3059\u308b\n    is_train = numpy.array( ([0]* (batch_size - 20) + [1] * 20) * (NUM_TRAIN // batch_size))\n    \n    test_indices = rng.permutation(NUM_TEST)\n    test_data, test_labels = test_data[test_indices, :], labels[test_indices]\n    is_test = numpy.array( ([0]* (batch_size - 20) + [1] * 20) * (NUM_TEST // batch_size))\n    # trai_set,valid_set,test_set\u306e\u6e96\u5099\n    train_set_x, train_set_y = numpy.array(data[is_train==0]), labels[is_train==0]\n    valid_set_x, valid_set_y = numpy.array(data[is_train==1]), labels[is_train==1]\n    test_set_x,test_set_y    = numpy.array(data[is_test==0]), labels[is_test==0]\n    # minibatch\u306e\u8a08\u7b97\n    n_train_batches = len(train_set_y) // batch_size\n    n_valid_batches = len(valid_set_y) // batch_size\n    n_test_batches = len(test_set_y)   // batch_size\n    \n    ##############\n    # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 #\n    ##############\n\n    print('... building the model')\n\n    # long\u578b\u306escalar \n    index = T.lscalar()  \n\n    #matrix\u578b\n    x = T.matrix('x')\n    #int\u578b\u306evector    \n    y = T.ivector('y') \n\n    # logistic regression \u30af\u30e9\u30b9\u306e\u69cb\u7bc9\n    # MNIST\u30c7\u30fc\u30bf\u306f28X28\n    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n\n    # cost function \u306f\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6 \n    cost = classifier.negative_log_likelihood(y)\n\n    # \u30df\u30cb\u30d0\u30c3\u30c1\u578b\u3067theano.function\u3067\u307e\u3068\u3081\u308b \n    train_set_x = theano.shared(numpy.asarray(train_set_x, dtype=theano.config.floatX))\n    train_set_y = T.cast(theano.shared(numpy.asarray(train_set_y, dtype=theano.config.floatX)), 'int32')\n    test_set_x  = theano.shared(numpy.asarray(test_set_x, dtype=theano.config.floatX))\n    test_set_y   = T.cast(theano.shared(numpy.asarray(test_set_y, dtype=theano.config.floatX)), 'int32')\n    valid_set_x = theano.shared(numpy.asarray(valid_set_x, dtype=theano.config.floatX)) \n    valid_set_y = T.cast(theano.shared(numpy.asarray(valid_set_y, dtype=theano.config.floatX)), 'int32')\n\n    # validate\u30e2\u30c7\u30eb\n    validate_model = theano.function(\n        inputs=[index],\n        outputs=classifier.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n\n    # test\u30e2\u30c7\u30eb \n    test_model = theano.function(\n        inputs=[index],\n        outputs=classifier.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n        }\n    )\n\n    #cost\u3092W\u3067\u5fae\u5206\n    g_W = T.grad(cost=cost, wrt=classifier.W)\n    #cost\u3092b\u3067\u5fae\u5206    \n    g_b = T.grad(cost=cost, wrt=classifier.b)\n    # cost function \u306f\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6  \n    \n    #\u30d1\u30e9\u30e1\u30fc\u30bf\uff08W,\uff42\uff09\u306e\u66f4\u65b0\n    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n               (classifier.b, classifier.b - learning_rate * g_b)]\n\n    # train\u30e2\u30c7\u30eb\n    # updates\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\uff08W,b\uff09\u306e\u66f4\u65b0\u3092\u884c\u3046\n    train_model = theano.function(\n        inputs=[index],\n        outputs=cost,\n        updates=updates,\n        givens={\n            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n\n    #################\n    # \u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 #\n    #################\n    print('... training the model')\n    # LOOP\u56de\u6570\u306e\u4e0a\u9650\n    patience = 50000  \n    # 2\u4ee5\u4e0a\u5897\u3048\u306a\u304f\u306a\u3063\u305f\u3089\n    patience_increase = 5  \n    #\u6210\u7e3e\u306e\u4e0a\u9650                         \n    improvement_threshold = 0.995         \n    #patience\u3092\u6e80\u305f\u3059\u304b\u306e\u983b\u5ea6\u3002\u3053\u306e\u5834\u5408\u306fevery epoch\u3067\u30c1\u30a7\u30c3\u30af\u3059\u308b\n    validation_frequency = min(n_train_batches, patience // 2)\n\n    best_validation_loss = numpy.inf\n    test_score = 0.\n    start_time = timeit.default_timer()\n\n    #LOOP\u306e\u306f\u3058\u307e\u308a\n    done_looping = False\n    epoch = 0\n    minibatch_index = 1\n    while (epoch < n_epochs) and (not done_looping):\n        epoch = epoch + 1\n        for minibatch_index in range(n_train_batches):\n\n            minibatch_avg_cost = train_model(minibatch_index)\n            # iteration\u306e\u8a08\u7b97\n            iter = (epoch - 1) * n_train_batches + minibatch_index\n\n            if (iter + 1) % validation_frequency == 0:\n                # zero-one\u640d\u5931\u3092Validation \u3067\u8a08\u7b97\n                validation_losses = [validate_model(i)\n                                     for i in range(n_valid_batches)]\n                this_validation_loss = numpy.mean(validation_losses)\n\n                print(\n                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n                    (\n                        epoch,\n                        minibatch_index + 1,\n                        n_train_batches,\n                        this_validation_loss * 100.\n                    )\n                )\n\n                # validation loss\u304c\u6700\u4f4e\u306e\u5834\u5408\n                if this_validation_loss < best_validation_loss:\n\n                    if this_validation_loss < best_validation_loss *  \\\n                       improvement_threshold:\n                        patience = max(patience, iter * patience_increase)\n\n                    best_validation_loss = this_validation_loss\n                    # test\u30bb\u30c3\u30c8\u3067\u30c6\u30b9\u30c8\u3059\u308b\n\n                    test_losses = [test_model(i)\n                                   for it in range(len(test_datasets))]\n                    test_score = numpy.mean(test_losses)\n\n                    print(\n                        (\n                            '     epoch %i, minibatch %i/%i, test error of'\n                            ' best model %f %%'\n                        ) %\n                        (\n                            epoch,\n                            minibatch_index + 1,\n                            n_train_batches,\n                            test_score * 100.\n                        )\n                    )\n\n                    # best\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n                    with open('best_model.pkl', 'wb') as f:\n                        pickle.dump(classifier, f)\n\n            if patience <= iter:\n                done_looping = True\n                break\n\n    end_time = timeit.default_timer()\n    print(\n        (\n            'Optimization complete with best validation score of %f %%,'\n            'with test performance %f %%'\n        )\n        % (best_validation_loss * 100., test_score * 100.)\n    )\n    print('The code run for %d epochs, with %f epochs/sec' % (\n        epoch, 1. * epoch / (end_time - start_time)))\n          \ndef predict():\n    \"\"\"\n    An example of how to load a trained model and use it\n    to predict labels.\n    \"\"\"\n    # Train\u3057\u305f\u30e2\u30c7\u30eb\u3092\u547c\u3073\u51fa\u3059\n    classifier = pickle.load(open('best_model.pkl'))\n\n    # predict\u30e2\u30c7\u30eb\n    predict_model = theano.function(\n    inputs=[classifier.input],\n        outputs=classifier.y_pred) \n\n    # test\u30bb\u30c3\u30c8\u3092\u547c\u3073\u51fa\u3059\n    test_data = np.loadtxt(\"../input/test.csv\", dtype=dtype,\n                          delimiter=',', skiprows=1)\n    #predict\u3059\u308b                          \n    predicted_values = predict_model(test_data)\n    print(\"Predicted values for the first 10 examples in test set:\")\n    print(predicted_values)\n\n    np.savetxt('Submit_TheanoLR3.csv', np.c_[range(1, len(test_data) + 1), predicted_values], delimiter=',', comments = '', header = 'ImageId,Label', fmt='%d')\n\nif __name__ == '__main__':    \n    sgd_optimization_mnist()\n```\n\n#\u7d50\u679c\n\u7d50\u679c\u306f0.90757\u3002scikit-learn\u30670.90900\u3060\u3063\u305f\u3057\u3001\u3053\u3093\u306a\u3082\u306e\u304b\u3068\u601d\u308f\u308c\u308b\u3002\n", "tags": ["Kaggle", "logisticregression", "Theano", "Python", "MNIST"]}