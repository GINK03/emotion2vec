{"context": "\n\n\uff11.\u30e1\u30c7\u30a3\u30a2\u8a18\u4e8b\n\nWired \uff082014.06.15 SUN 09:40\uff09 \u300c\u52d5\u753b\u306e\u4e2d\u306e\u4eba\u304c\u4f55\u3092\u3057\u3066\u3044\u308b\u304b\u5206\u304b\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3001\u958b\u767a\u3055\u308c\u308b\u300d \nWired \uff082016.03.06 SUN 16:30\uff09\u300c\u3053\u306e12\u679a\u306e\u5199\u771f\u306b\u306f\u300c\u76e3\u8996\u30ab\u30e1\u30e9\u306b\u7591\u308f\u308c\u308b\u884c\u52d5\u300d\u304c\u5199\u3063\u3066\u3044\u307e\u3059\u300d\nITmedia\u30a8\u30f3\u30bf\u30fc\u30d7\u30e9\u30a4\u30ba\u300c2015\u5e7410\u670813\u65e5 16\u664239\u5206 \u66f4\u65b0\u300c\u201c\u4eba\u306e\u52d5\u4f5c\u201d\u3092\u8a8d\u8b58\u3059\u308b\u4eba\u5de5\u77e5\u80fd\u304c\u767b\u5834\u30008\u5272\u5f37\u306e\u8a8d\u8b58\u7cbe\u5ea6\u300d \n\n\n\uff12. \u96a0\u308c\u30de\u30eb\u30b3\u30d5\u30e2\u30c7\u30eb\u306a\u3069\u3001\u591a\u65b9\u9762\u304b\u3089\u306e\u6a5f\u68b0\u5b66\u7fd2\u6280\u8853\u3092\u6d3b\u7528\u3057\u305f\u884c\u52d5\u89e3\u6790\u3001\u884c\u52d5\u5206\u985e\u63a8\u5b9a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n\n\n\n\u5c0f\u6797 \u96bc\u4eba\u30fb\u67f3\u4e95 \u5553\u53f8\u300c\u30c6\u30ec\u30d3\u6620\u50cf\u304b\u3089\u306e\u7279\u5b9a\u52d5\u4f5c\u30b7\u30fc\u30f3\u306e\u81ea\u52d5\u691c\u51fa\u300dDEIM Forum 2016 E5-6\n\u78ef\u90e8 \u7fd4\u30fb\u5897\u7530 \u8aa0\u300c\u884c\u52d5\u8a8d\u8b58\u306e\u305f\u3081\u306e\u4eba\u7269\u8ffd\u8de1\u6280\u8853\u306e\u958b\u767a\u300d \n\u5c71\u8fba \u667a\u6643 et.al, \u300c\u884c\u52d5\u8a8d\u8b58\u306e\u305f\u3081\u306e\u89d2\u5ea6\u5909\u4f4d\u91cf\u5b50\u5316\u7279\u5fb4\u306e\u691c\u8a0e\u300d\u96fb\u6c17\u5b66\u4f1a\u8ad6\u6587\u8a8c C(\u96fb\u5b50\u30fb\u60c5\u5831\u30fb\u30b7\u30b9\u30c6\u30e0\u90e8\u9580\u8a8c)\nIEEJ Transactions on Electronics, Information and Systems\n \u6cc9 \u77e5\u8ad6 \u4ed6\u3000\u300c\u4e0d\u5be9\u4eba\u7269\u306e\u8a8d\u8b58\u8ffd\u8de1\u6280\u8853\u306e\u9ad8\u5ea6\u5316\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u300d\u7acb\u547d\u9928\u5927\u5b66 \u6b74\u53f2\u90fd\u5e02\u9632\u707d\u7814\u7a76\u6240 2015 \u5e74 3 \u6708\n\uff08SlideShare\uff09\u7247\u5ca1 \u88d5\u96c4\u300c\u4eba\u3092\u89b3\u308b\u6280\u8853\u306e\u5148\u7aef\u7684\u7814\u7a76\u300d \n\n(SlideShare)  \u7247\u5ca1 \u88d5\u96c4\u300c\u3010\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3011\u52d5\u7684\u306a\u4eba\u7269\u30fb\u7269\u4f53\u8a8d\u8b58\u6280\u8853 -Dense Trajectories-\u300d\nKaren Simonyan, Andrew Zisserman, _Two-Stream Convolutional Networks for Action Recognition in Videos _\nHeng Wang, et.al, Action Recognition by Dense Trajectories\nHeng Wang, et.al, Dense trajectories and motion boundary descriptors for action recognition\n Heng Wang, Cordelia Schmid, Action Recognition with Improved Trajectories\n\u52dd\u624b \u7f8e\u7d17 \u4ed6\u300c\u7269\u4f53\u3068\u52d5\u304d\u7279\u5fb4\u3092\u7528\u3044\u305f\u884c\u52d5\u8a8d\u8b58\u300d\u793e\u56e3\u6cd5\u4eba \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a \u4fe1\u5b66\u6280\u5831\n\u5f13\u5834 \u7adc \u5e73\u621026\u5e74\u5ea6 \u4e2d\u90e8\u5927\u5b66\u5927\u5b66\u9662\u5de5\u5b66\u7814\u7a76\u79d1\u60c5\u5831\u5de5\u5b66\u5c02\u653b \u535a\u58eb\u5b66\u4f4d\u8ad6\u6587 \u300c\u6642\u7a7a\u9593\u7279\u5fb4\u91cf\u3068\u7d71\u8a08\u7684\u5b66\u7fd2\u3092\u7528\u3044\u305f\u52d5\u4f5c\u8a8d\u8b58\u306b\u95a2\u3059\u308b\u7814\u7a76\u300d\n\u897f\u91ce \u8070\u30fb\u5927\u5d8b \u5efa\u6b21\u300c\u753b\u7d20\u6570\u5909\u5316\u306bDP\u30de\u30c3\u30c1\u30f3\u30b0\u9069\u7528\u3057\u305f\u4eba\u7269\u306e\u884c\u52d5\u8a8d\u8b58\u300d\u5c0f\u5c71\u5de5\u696d\u9ad8\u7b49\u5c02\u9580\u5b66\u6821\u7814\u7a76\u7d00\u8981 \u7b2c39\u53f7(2007)89-98 \n\u4e2d\u6751\u514b\u884c\u3001\u8d99 \u5349\u83c1\u3001\u67f4\u5d0e\u4eae\u4ecb \uff08\u6771\u4eac\u5927\u5b66 \u7a7a\u9593\u60c5\u5831\u79d1\u5b66\u7814\u7a76\u30bb\u30f3\u30bf\u30fc\uff09\u300c\u30de\u30eb\u30c1\u30bb\u30f3\u30b5\u3092\u7528\u3044\u305f\u7fa4\u96c6\u306e\u884c\u52d5\u8a8d\u8b58 Recognizing People Activities by using Multiple Sensors\u300d\n\u91ce\u53e3 \u9855\u55e3\u3001\u4e0b\u7530 \u4fdd\u5fd7\u3001\u67f3\u4e95 \u5553\u53f8\u300c\u52d5\u4f5c\u8a8d\u8b58\u306e\u305f\u3081\u306e\u6642\u7a7a\u9593\u7279\u5fb4\u91cf\u3068\u7279\u5fb4\u7d71\u5408\u624b\u6cd5\u306e\u63d0\u6848\u300d\u300c\u753b\u50cf\u306e\u8a8d\u8b58\u30fb\u7406\u89e3\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0 (MIRU2010)\u300d 2010 \u5e74 7 \u6708\n\u4e95\u53e3 \u8302 \uff08\u65e9\u7a32\u7530\u5927\u5b66\u7406\u5de5\u5b66\u90e8\u60c5\u5831\u5b66\u79d1\uff092004 \u5e74\u5ea6 \u5352\u696d\u8ad6\u6587\u300c\u985e\u4f3c\u52d5\u4f5c\u304b\u3089\u306e\u7279\u5fb4\u90e8\u4f4d\u62bd\u51fa\u2010\u4eba\u9593\u306e\u884c\u52d5\u8a8d\u8b58\u3092\u4f8b\u3068\u3057\u3066\u2010\u300d \n\u5927\u548c \u6df3\u53f8\uff08NTT\u30d2\u30e5\u30fc\u30de\u30f3\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u7814\u7a76\u6240\uff09\u4ed6\u300c\u96a0\u308c\u30de\u30eb\u30b3\u30d5\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u52d5\u753b\u50cf\u304b\u3089\u306e\u4eba\u7269\u306e\u884c\u52d5\u8a8d\u8b58\u300cRrecognizing Human Action in Time-Sequential Images Using Hidden Markov Models\u300d\n\n\n\n\uff13.Deep Learning \u3092\u7528\u3044\u305f\u7814\u7a76\n\n\n\uff08SlideShare\uff09\u5927\u653f \u5b5d\u5145\uff08\u682a\u5f0f\u4f1a\u793e\u30a6\u30a7\u30d6\u30d5\u30a1\u30fc\u30de\u30fc\uff09\u300c\u52d5\u4f5c\u8a8d\u8b58\u306b\u304a\u3051\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u6700\u65b0\u52d5\u5411\uff11\u30003D-CNN\u300d \n\uff08SlideShare\uff09\u5927\u653f \u5b5d\u5145\uff08\u682a\u5f0f\u4f1a\u793e\u30a6\u30a7\u30d6\u30d5\u30a1\u30fc\u30de\u30fc\uff09\u300c\u52d5\u4f5c\u8a8d\u8b58\u306b\u304a\u3051\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u6700\u65b0\u52d5\u5411\uff12Sequential DL for HAR\u300d \n\u6a2a\u5c71 \u6643\u3001 \u5ca1\u7559 \u525b\u3001 \u89d2\u6240 \u8003\uff08\u95a2\u897f\u5b66\u9662\u5927\u5b66\u7406\u5de5\u5b66\u90e8\u4eba\u9593\u30b7\u30b9\u30c6\u30e0\u5de5\u5b66\u79d1\uff09\u300cDeepLearning \u306b\u3088\u308b\u6b21\u5143\u5727\u7e2e\u3092\u7528\u3044\u305f\u6642\u7cfb\u5217\u884c\u52d5\u8a8d\u8b58\uff09The  29th Annual Conference of the Japanese Society for Arti cial Intelligence, 2015\nKaren Simonyan Andrew Zisserman, Two-Stream Convolutional Networks for Action Recognition in Videos\nWei Xu, Ming Yang and Kai Yu, 3D Convolutional Neural Networks for Human Action Recognition\nAnoosha. P and Nandakumar. P, A Technique of Human Action Recognition Using 3D Convolutional Neural Network, International Journal of Emerging Technology and Advanced Engineering\n\u6280\u30e9\u30dc \u300c3DConvolution + Residual Network\u3067\u904a\u3093\u3067\u307f\u305f\u300d\n\n\uff08 \u4ee5\u4e0b\u306f\u3001\u5bfe\u8c61\u306f\u4eba\u7269\u306e\u52d5\u304d\u3067\u306f\u306a\u3044\u304c\u3001\u95a2\u9023\u3059\u308b\u8a18\u4e8b \uff09\n\nDaniel Maturana and Sebastian Scherer, 3D Convolutional Neural Networks for Landing Zone Detection from LiDAR\n\n###__\uff11.\u30e1\u30c7\u30a3\u30a2\u8a18\u4e8b__\n\n[Wired \uff082014.06.15 SUN 09:40\uff09 \u300c\u52d5\u753b\u306e\u4e2d\u306e\u4eba\u304c\u4f55\u3092\u3057\u3066\u3044\u308b\u304b\u5206\u304b\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3001\u958b\u767a\u3055\u308c\u308b\u300d ](http://wired.jp/2014/06/15/activity-recognition/)\n[Wired \uff082016.03.06 SUN 16:30\uff09\u300c\u3053\u306e12\u679a\u306e\u5199\u771f\u306b\u306f\u300c\u76e3\u8996\u30ab\u30e1\u30e9\u306b\u7591\u308f\u308c\u308b\u884c\u52d5\u300d\u304c\u5199\u3063\u3066\u3044\u307e\u3059\u300d](http://wired.jp/2016/03/06/esther-hovers-false-positives/)\n[ITmedia\u30a8\u30f3\u30bf\u30fc\u30d7\u30e9\u30a4\u30ba\u300c2015\u5e7410\u670813\u65e5 16\u664239\u5206 \u66f4\u65b0\u300c\u201c\u4eba\u306e\u52d5\u4f5c\u201d\u3092\u8a8d\u8b58\u3059\u308b\u4eba\u5de5\u77e5\u80fd\u304c\u767b\u5834\u30008\u5272\u5f37\u306e\u8a8d\u8b58\u7cbe\u5ea6\u300d ](http://www.itmedia.co.jp/enterprise/articles/1510/13/news113.html)\n\n___\n\n###__\uff12. \u96a0\u308c\u30de\u30eb\u30b3\u30d5\u30e2\u30c7\u30eb\u306a\u3069\u3001\u591a\u65b9\u9762\u304b\u3089\u306e\u6a5f\u68b0\u5b66\u7fd2\u6280\u8853\u3092\u6d3b\u7528\u3057\u305f\u884c\u52d5\u89e3\u6790\u3001\u884c\u52d5\u5206\u985e\u63a8\u5b9a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0__\n\n* [\u5c0f\u6797 \u96bc\u4eba\u30fb\u67f3\u4e95 \u5553\u53f8\u300c\u30c6\u30ec\u30d3\u6620\u50cf\u304b\u3089\u306e\u7279\u5b9a\u52d5\u4f5c\u30b7\u30fc\u30f3\u306e\u81ea\u52d5\u691c\u51fa\u300dDEIM Forum 2016 E5-6](http://db-event.jpn.org/deim2016/papers/418.pdf)\n[\u78ef\u90e8 \u7fd4\u30fb\u5897\u7530 \u8aa0\u300c\u884c\u52d5\u8a8d\u8b58\u306e\u305f\u3081\u306e\u4eba\u7269\u8ffd\u8de1\u6280\u8853\u306e\u958b\u767a\u300d ](https://www.oki.com/jp/otr/2014/n224/pdf/224_r14.pdf)\n[\u5c71\u8fba \u667a\u6643 et.al, \u300c\u884c\u52d5\u8a8d\u8b58\u306e\u305f\u3081\u306e\u89d2\u5ea6\u5909\u4f4d\u91cf\u5b50\u5316\u7279\u5fb4\u306e\u691c\u8a0e\u300d\u96fb\u6c17\u5b66\u4f1a\u8ad6\u6587\u8a8c C(\u96fb\u5b50\u30fb\u60c5\u5831\u30fb\u30b7\u30b9\u30c6\u30e0\u90e8\u9580\u8a8c)\nIEEJ Transactions on Electronics, Information and Systems](http://www.hirokatsukataoka.net/pdf/ieej15_yamabe_posturebasedactivity.pdf)\n[ \u6cc9 \u77e5\u8ad6 \u4ed6\u3000\u300c\u4e0d\u5be9\u4eba\u7269\u306e\u8a8d\u8b58\u8ffd\u8de1\u6280\u8853\u306e\u9ad8\u5ea6\u5316\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u300d\u7acb\u547d\u9928\u5927\u5b66 \u6b74\u53f2\u90fd\u5e02\u9632\u707d\u7814\u7a76\u6240 2015 \u5e74 3 \u6708](http://rits-dmuch.jp/jp/results/nenpou/dl_files/2014/2014_14.pdf)\n[\uff08SlideShare\uff09\u7247\u5ca1 \u88d5\u96c4\u300c\u4eba\u3092\u89b3\u308b\u6280\u8853\u306e\u5148\u7aef\u7684\u7814\u7a76\u300d ](http://www.slideshare.net/HirokatsuKataoka/ssii2015)\n* [(SlideShare)  \u7247\u5ca1 \u88d5\u96c4\u300c\u3010\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3011\u52d5\u7684\u306a\u4eba\u7269\u30fb\u7269\u4f53\u8a8d\u8b58\u6280\u8853 -Dense Trajectories-\u300d](http://www.slideshare.net/HirokatsuKataoka/dt-tutorial)\n* [Karen Simonyan, Andrew Zisserman, _Two-Stream Convolutional Networks for Action Recognition in Videos _](https://arxiv.org/pdf/1406.2199.pdf)\n* [Heng Wang, _et.al, Action Recognition by Dense Trajectories_](https://hal.inria.fr/inria-00583818/document)\n* [Heng Wang, _et.al, Dense trajectories and motion boundary descriptors for action recognition_](https://hal.inria.fr/hal-00803241/PDF/IJCV.pdf)\n* [ Heng Wang, Cordelia Schmid, _Action Recognition with Improved Trajectories_](https://hal.inria.fr/hal-00873267v2/document)\n\n* [\u52dd\u624b \u7f8e\u7d17 \u4ed6\u300c\u7269\u4f53\u3068\u52d5\u304d\u7279\u5fb4\u3092\u7528\u3044\u305f\u884c\u52d5\u8a8d\u8b58\u300d\u793e\u56e3\u6cd5\u4eba \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a \u4fe1\u5b66\u6280\u5831](http://www.m.cs.osakafu-u.ac.jp/publication_data/1247/2012_02.pdf)\n* [\u5f13\u5834 \u7adc \u5e73\u621026\u5e74\u5ea6 \u4e2d\u90e8\u5927\u5b66\u5927\u5b66\u9662\u5de5\u5b66\u7814\u7a76\u79d1\u60c5\u5831\u5de5\u5b66\u5c02\u653b \u535a\u58eb\u5b66\u4f4d\u8ad6\u6587 \u300c\u6642\u7a7a\u9593\u7279\u5fb4\u91cf\u3068\u7d71\u8a08\u7684\u5b66\u7fd2\u3092\u7528\u3044\u305f\u52d5\u4f5c\u8a8d\u8b58\u306b\u95a2\u3059\u308b\u7814\u7a76\u300d](http://www.vision.cs.chubu.ac.jp/FLABResearchArchive/Doctor/D14/Paper/yumiba.pdf)\n* [\u897f\u91ce \u8070\u30fb\u5927\u5d8b \u5efa\u6b21\u300c\u753b\u7d20\u6570\u5909\u5316\u306bDP\u30de\u30c3\u30c1\u30f3\u30b0\u9069\u7528\u3057\u305f\u4eba\u7269\u306e\u884c\u52d5\u8a8d\u8b58\u300d\u5c0f\u5c71\u5de5\u696d\u9ad8\u7b49\u5c02\u9580\u5b66\u6821\u7814\u7a76\u7d00\u8981 \u7b2c39\u53f7(2007)89-98 ](http://www.oyama-ct.ac.jp/tosyo/kiyou/kiyou39/014%20nishino.pdf)\n* [\u4e2d\u6751\u514b\u884c\u3001\u8d99 \u5349\u83c1\u3001\u67f4\u5d0e\u4eae\u4ecb \uff08\u6771\u4eac\u5927\u5b66 \u7a7a\u9593\u60c5\u5831\u79d1\u5b66\u7814\u7a76\u30bb\u30f3\u30bf\u30fc\uff09\u300c\u30de\u30eb\u30c1\u30bb\u30f3\u30b5\u3092\u7528\u3044\u305f\u7fa4\u96c6\u306e\u884c\u52d5\u8a8d\u8b58 Recognizing People Activities by using Multiple Sensors\u300d](http://gi-studentjp.org/s_forum/pdf/2006/02/19_nakamura.pdf)\n* [\u91ce\u53e3 \u9855\u55e3\u3001\u4e0b\u7530 \u4fdd\u5fd7\u3001\u67f3\u4e95 \u5553\u53f8\u300c\u52d5\u4f5c\u8a8d\u8b58\u306e\u305f\u3081\u306e\u6642\u7a7a\u9593\u7279\u5fb4\u91cf\u3068\u7279\u5fb4\u7d71\u5408\u624b\u6cd5\u306e\u63d0\u6848\u300d\u300c\u753b\u50cf\u306e\u8a8d\u8b58\u30fb\u7406\u89e3\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0 (MIRU2010)\u300d 2010 \u5e74 7 \u6708](http://img.cs.uec.ac.jp/pub/conf10/100727noguchi_2.pdf)\n* [\u4e95\u53e3 \u8302 \uff08\u65e9\u7a32\u7530\u5927\u5b66\u7406\u5de5\u5b66\u90e8\u60c5\u5831\u5b66\u79d1\uff092004 \u5e74\u5ea6 \u5352\u696d\u8ad6\u6587\u300c\u985e\u4f3c\u52d5\u4f5c\u304b\u3089\u306e\u7279\u5fb4\u90e8\u4f4d\u62bd\u51fa\u2010\u4eba\u9593\u306e\u884c\u52d5\u8a8d\u8b58\u3092\u4f8b\u3068\u3057\u3066\u2010\u300d ](https://dspace.wul.waseda.ac.jp/dspace/bitstream/2065/694/1/1g01p007.pdf)\n* [\u5927\u548c \u6df3\u53f8\uff08NTT\u30d2\u30e5\u30fc\u30de\u30f3\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u7814\u7a76\u6240\uff09\u4ed6\u300c\u96a0\u308c\u30de\u30eb\u30b3\u30d5\u30e2\u30c7\u30eb\u3092\u7528\u3044\u305f\u52d5\u753b\u50cf\u304b\u3089\u306e\u4eba\u7269\u306e\u884c\u52d5\u8a8d\u8b58\u300cRrecognizing Human Action in Time-Sequential Images Using Hidden Markov Models\u300d](http://ci.nii.ac.jp/naid/110003228291)\n\n___\n\n###__\uff13.Deep Learning \u3092\u7528\u3044\u305f\u7814\u7a76__\n\n* [\uff08SlideShare\uff09\u5927\u653f \u5b5d\u5145\uff08\u682a\u5f0f\u4f1a\u793e\u30a6\u30a7\u30d6\u30d5\u30a1\u30fc\u30de\u30fc\uff09\u300c\u52d5\u4f5c\u8a8d\u8b58\u306b\u304a\u3051\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u6700\u65b0\u52d5\u5411\uff11\u30003D-CNN\u300d ](http://www.slideshare.net/ssuser07aa33/3dcnn)\n* [\uff08SlideShare\uff09\u5927\u653f \u5b5d\u5145\uff08\u682a\u5f0f\u4f1a\u793e\u30a6\u30a7\u30d6\u30d5\u30a1\u30fc\u30de\u30fc\uff09\u300c\u52d5\u4f5c\u8a8d\u8b58\u306b\u304a\u3051\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u6700\u65b0\u52d5\u5411\uff12Sequential DL for HAR\u300d ](http://www.slideshare.net/ssuser07aa33/sequential-dl-for-har)\n* [\u6a2a\u5c71 \u6643\u3001 \u5ca1\u7559 \u525b\u3001 \u89d2\u6240 \u8003\uff08\u95a2\u897f\u5b66\u9662\u5927\u5b66\u7406\u5de5\u5b66\u90e8\u4eba\u9593\u30b7\u30b9\u30c6\u30e0\u5de5\u5b66\u79d1\uff09\u300cDeepLearning \u306b\u3088\u308b\u6b21\u5143\u5727\u7e2e\u3092\u7528\u3044\u305f\u6642\u7cfb\u5217\u884c\u52d5\u8a8d\u8b58\uff09The  29th Annual Conference of the Japanese Society for Arti cial Intelligence, 2015](https://kaigi.org/jsai/webprogram/2015/pdf/3L4-2.pdf)\n* [Karen Simonyan Andrew Zisserman, _Two-Stream Convolutional Networks for Action Recognition in Videos_](https://arxiv.org/pdf/1406.2199.pdf)\n\n* [Wei Xu, Ming Yang and Kai Yu, _3D Convolutional Neural Networks for Human Action Recognition_](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf)\n* [Anoosha. P and Nandakumar. P, _A Technique of Human Action Recognition Using 3D Convolutional Neural Network_, International Journal of Emerging Technology and Advanced Engineering](http://www.ijetae.com/files/Volume4Issue9/IJETAE_0914_35.pdf)\n* [\u6280\u30e9\u30dc \u300c3DConvolution + Residual Network\u3067\u904a\u3093\u3067\u307f\u305f\u300d](http://wazalabo.com/3dconvolution-residual-network.html)\n\n\uff08 \u4ee5\u4e0b\u306f\u3001\u5bfe\u8c61\u306f\u4eba\u7269\u306e\u52d5\u304d\u3067\u306f\u306a\u3044\u304c\u3001\u95a2\u9023\u3059\u308b\u8a18\u4e8b \uff09\n\n* [Daniel Maturana and Sebastian Scherer, _3D Convolutional Neural Networks for Landing Zone Detection from LiDAR_](http://www.dimatura.net/extra/3dcnn_lz_maturana_scherer_icra15.pdf)\n\n", "tags": ["DeepLearning", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "\u6a5f\u68b0\u5b66\u7fd2", "MachineLearning", "\u4eba\u5de5\u77e5\u80fd"]}