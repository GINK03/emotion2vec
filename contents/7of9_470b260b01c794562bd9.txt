{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\n\n\nTensorFlow\u3092\u4f7f\u3063\u3066\u3001input:100, output:100\u7a0b\u5ea6\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u3092\u3057\u3088\u3046\u304b\u3068\u4f5c\u696d\u4e2d\u3002\nv0.6 / v0.4: http://qiita.com/7of9/items/7394f4326a88500a67b9\n\n\u30c7\u30fc\u30bf\u751f\u6210\u30b3\u30fc\u30c9 > v0.7\nNUM_FILEOUT = 100\u3092NUM_FILEOUT = 10000\u306b\u3057\u3066\u307f\u305f\u3002\n\n\u5b66\u7fd2\u30b3\u30fc\u30c9 > v0.7\ndropout\u3092\u4f7f\u3063\u3066\u307f\u305f\u3002\ndropout\u4f7f\u7528\u306b\u306f @shngt \u3055\u3093\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\nhttp://qiita.com/shngt/items/f532601b4f059ce8584f\n\nlearn_in100out100.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\n'''\nv0.7 Feb. 22, 2017\n    - treat each hidden layer to handle dropout instead of slim.stack()\nv0.6 Feb. 19, 2017\n    - add sys.stdout.flush() to immediately print\nv0.5 Feb. 18, 2017\n    - add fc_drop()\nv0.4 Feb. 15, 2017\n    - fix bug > ZeroDivisionError: float division by zero @ shuffle_batch()\nv0.3 Feb. 15, 2017\n    - tweak [batch_size] for shuffle_batch()\nv0.2 Feb. 15, 2017\n    - fix bug > different dimensions for placeholder and network\nv0.1 Feb. 06, 2017\n    - read [test_in.csv],[test_out.csv]\n'''\n\n'''\ncodingrule:PEP8\n'''\n\n\ndef fc_drop(inputs, *args, **kwargs):\n    # Thanks to: http://qiita.com/shngt/items/f532601b4f059ce8584f\n    net = slim.fully_connected(inputs, *args, **kwargs)\n    return slim.dropout(net, 0.5)\n\nfilename_inp = tf.train.string_input_producer([\"test_in.csv\"])\nfilename_out = tf.train.string_input_producer([\"test_out.csv\"])\nNUM_INP_NODE = 100\nNUM_OUT_NODE = 100\n\n# parse csv\n# a. input node\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_inp)\ndeflist = [[0.] for idx in range(NUM_INP_NODE)]\ninput1 = tf.decode_csv(value, record_defaults=deflist)\n# b. output node\nkey, value = reader.read(filename_out)\ndeflist = [[0.] for idx in range(NUM_OUT_NODE)]\noutput1 = tf.decode_csv(value, record_defaults=deflist)\n# c. pack\n# inputs = tf.pack([input1])\ninputs = input1\n# outputs = tf.pack([output1])\noutputs = output1\n\nbatch_size = 2\ninputs_batch, output_batch = tf.train.shuffle_batch(\n    [inputs, outputs], batch_size, capacity=10, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None, 100])\noutput_ph = tf.placeholder(\"float\", [None, 100])\n\n# network\n# a. with stack\n# hiddens = slim.stack(input_ph, slim.fully_connected, [7, 7, 7],\n#                     activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n# b. without stack\n# sgm = tf.nn.sigmoid\n# hiddens = slim.fully_connected(input_ph, 7, activation_fn=sgm, scope='hidden1')\n# hiddens = slim.fully_connected(hiddens, 7, activation_fn=sgm, scope='hidden2')\n# hiddens = slim.fully_connected(hiddens, 7, activation_fn=sgm, scope='hidden3')\n# c. without stack, with dropout\nsgm = tf.nn.sigmoid\nhiddens = slim.fully_connected(input_ph, 7, activation_fn=sgm, scope='hidden1')\ndrpout = slim.stack(hiddens, fc_drop, [7, 7], scope='hd_fc1')\nhiddens = slim.fully_connected(drpout, 7, activation_fn=sgm, scope='hidden2')\ndrpout = slim.stack(hiddens, fc_drop, [7, 7], scope='hd_fc2')\nhiddens = slim.fully_connected(drpout, 7, activation_fn=sgm, scope='hidden3')\n\n\n# a. without dropout\n# prediction = slim.fully_connected(\n#    hiddens, 100, activation_fn=None, scope=\"output\")\n# b. with dropout\ndrpout = slim.stack(hiddens, fc_drop, [100, 100], scope='fc')\nprediction = slim.fully_connected(\n    drpout, 100, activation_fn=None, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    try:\n        sess.run(init_op)\n        for idx in range(10000):\n            # for idx in range(100000):\n            inpbt, outbt = sess.run([inputs_batch, output_batch])\n            _, t_loss = sess.run(\n                [train_op, loss],\n                feed_dict={input_ph: inpbt, output_ph: outbt})\n\n            if (idx+1) % 100 == 0:\n                print(\"%d,%f\" % (idx+1, t_loss))\n                sys.stdout.flush()\n    finally:\n        coord.request_stop()\n\n    coord.join(threads)\n\n\n\n\n\u5b66\u7fd2\u7d50\u679c\nJupyter\u30b3\u30fc\u30c9\n\ncheck_result_170215.ipynb\n%matplotlib inline\n\n'''\nv0.1 Feb. 16, 2017\n'''\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# data1 = np.loadtxt('res.learn.170215', delimiter=',')\n# data1 = np.loadtxt('res.learn.N=1000_170216', delimiter=',')\ndata1 = np.loadtxt('res.withStack_170222', delimiter=',')\n\n# data2 = np.loadtxt('res.learn.N=1000_dropout@output_170218', delimiter=',')\n# data2 = np.loadtxt('res.learn.N=1000_dropout@output_longrun_170218', delimiter=',')\n# data2 = np.loadtxt('res.withoutStack_170222', delimiter=',')\ndata2 = np.loadtxt('res.withoutStack_withDropout_170222', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\ninput2 = data2[:,0]\noutput2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\nax1.scatter(input1,output1)\nax2.scatter(input2,output2)\n\n\nax1.set_xlabel('step')\nax1.set_ylabel('Error')\nax1.set_xlim(0, 10000)\nax1.set_ylim(0, 0.2)\nax1.grid(True)\n\nax2.set_xlabel('step')\nax2.set_ylabel('Error')\nax2.set_xlim(0, 10000)\nax2.set_ylim(0, 0.2)\nax2.grid(True)\n\nfig.show()\n\n\n\n\u4e0a\u306foutput layer\u306e\u307fdropout\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3002\n\u4e0b\u306fhidden layer\u306b\u3082dropout\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3002\n\u3068\u3082\u306b\u3001\u671f\u5f85\u3057\u3066\u3044\u308b\u8aa4\u5dee\u306e\u6e1b\u5c11\u50be\u5411\u306f\u898b\u3089\u308c\u306a\u3044\u3002\n\u3053\u306e\u3042\u305f\u308a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u7bc9\u306e\u77e5\u8b58\u306f\u95a2\u9023\u8ad6\u6587\u306a\u3069\u306e\u60c5\u5831\u3092\u6574\u7406\u3057\u305f\u65b9\u304c\u3044\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\n```\n\nTensorFlow\u3092\u4f7f\u3063\u3066\u3001input:100, output:100\u7a0b\u5ea6\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u3092\u3057\u3088\u3046\u304b\u3068\u4f5c\u696d\u4e2d\u3002\n\nv0.6 / v0.4: http://qiita.com/7of9/items/7394f4326a88500a67b9\n\n## \u30c7\u30fc\u30bf\u751f\u6210\u30b3\u30fc\u30c9 > v0.7\n\n`NUM_FILEOUT = 100`\u3092`NUM_FILEOUT = 10000`\u306b\u3057\u3066\u307f\u305f\u3002\n\n## \u5b66\u7fd2\u30b3\u30fc\u30c9 > v0.7\n\ndropout\u3092\u4f7f\u3063\u3066\u307f\u305f\u3002\n\ndropout\u4f7f\u7528\u306b\u306f @shngt \u3055\u3093\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\nhttp://qiita.com/shngt/items/f532601b4f059ce8584f\n\n```learn_in100out100.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\n'''\nv0.7 Feb. 22, 2017\n    - treat each hidden layer to handle dropout instead of slim.stack()\nv0.6 Feb. 19, 2017\n    - add sys.stdout.flush() to immediately print\nv0.5 Feb. 18, 2017\n    - add fc_drop()\nv0.4 Feb. 15, 2017\n    - fix bug > ZeroDivisionError: float division by zero @ shuffle_batch()\nv0.3 Feb. 15, 2017\n    - tweak [batch_size] for shuffle_batch()\nv0.2 Feb. 15, 2017\n    - fix bug > different dimensions for placeholder and network\nv0.1 Feb. 06, 2017\n    - read [test_in.csv],[test_out.csv]\n'''\n\n'''\ncodingrule:PEP8\n'''\n\n\ndef fc_drop(inputs, *args, **kwargs):\n    # Thanks to: http://qiita.com/shngt/items/f532601b4f059ce8584f\n    net = slim.fully_connected(inputs, *args, **kwargs)\n    return slim.dropout(net, 0.5)\n\nfilename_inp = tf.train.string_input_producer([\"test_in.csv\"])\nfilename_out = tf.train.string_input_producer([\"test_out.csv\"])\nNUM_INP_NODE = 100\nNUM_OUT_NODE = 100\n\n# parse csv\n# a. input node\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_inp)\ndeflist = [[0.] for idx in range(NUM_INP_NODE)]\ninput1 = tf.decode_csv(value, record_defaults=deflist)\n# b. output node\nkey, value = reader.read(filename_out)\ndeflist = [[0.] for idx in range(NUM_OUT_NODE)]\noutput1 = tf.decode_csv(value, record_defaults=deflist)\n# c. pack\n# inputs = tf.pack([input1])\ninputs = input1\n# outputs = tf.pack([output1])\noutputs = output1\n\nbatch_size = 2\ninputs_batch, output_batch = tf.train.shuffle_batch(\n    [inputs, outputs], batch_size, capacity=10, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None, 100])\noutput_ph = tf.placeholder(\"float\", [None, 100])\n\n# network\n# a. with stack\n# hiddens = slim.stack(input_ph, slim.fully_connected, [7, 7, 7],\n#                     activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n# b. without stack\n# sgm = tf.nn.sigmoid\n# hiddens = slim.fully_connected(input_ph, 7, activation_fn=sgm, scope='hidden1')\n# hiddens = slim.fully_connected(hiddens, 7, activation_fn=sgm, scope='hidden2')\n# hiddens = slim.fully_connected(hiddens, 7, activation_fn=sgm, scope='hidden3')\n# c. without stack, with dropout\nsgm = tf.nn.sigmoid\nhiddens = slim.fully_connected(input_ph, 7, activation_fn=sgm, scope='hidden1')\ndrpout = slim.stack(hiddens, fc_drop, [7, 7], scope='hd_fc1')\nhiddens = slim.fully_connected(drpout, 7, activation_fn=sgm, scope='hidden2')\ndrpout = slim.stack(hiddens, fc_drop, [7, 7], scope='hd_fc2')\nhiddens = slim.fully_connected(drpout, 7, activation_fn=sgm, scope='hidden3')\n\n\n# a. without dropout\n# prediction = slim.fully_connected(\n#    hiddens, 100, activation_fn=None, scope=\"output\")\n# b. with dropout\ndrpout = slim.stack(hiddens, fc_drop, [100, 100], scope='fc')\nprediction = slim.fully_connected(\n    drpout, 100, activation_fn=None, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    try:\n        sess.run(init_op)\n        for idx in range(10000):\n            # for idx in range(100000):\n            inpbt, outbt = sess.run([inputs_batch, output_batch])\n            _, t_loss = sess.run(\n                [train_op, loss],\n                feed_dict={input_ph: inpbt, output_ph: outbt})\n\n            if (idx+1) % 100 == 0:\n                print(\"%d,%f\" % (idx+1, t_loss))\n                sys.stdout.flush()\n    finally:\n        coord.request_stop()\n\n    coord.join(threads)\n\n```\n\n### \u5b66\u7fd2\u7d50\u679c\n\nJupyter\u30b3\u30fc\u30c9\n\n```py:check_result_170215.ipynb\n%matplotlib inline\n\n'''\nv0.1 Feb. 16, 2017\n'''\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# data1 = np.loadtxt('res.learn.170215', delimiter=',')\n# data1 = np.loadtxt('res.learn.N=1000_170216', delimiter=',')\ndata1 = np.loadtxt('res.withStack_170222', delimiter=',')\n\n# data2 = np.loadtxt('res.learn.N=1000_dropout@output_170218', delimiter=',')\n# data2 = np.loadtxt('res.learn.N=1000_dropout@output_longrun_170218', delimiter=',')\n# data2 = np.loadtxt('res.withoutStack_170222', delimiter=',')\ndata2 = np.loadtxt('res.withoutStack_withDropout_170222', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\ninput2 = data2[:,0]\noutput2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\nax1.scatter(input1,output1)\nax2.scatter(input2,output2)\n\n\nax1.set_xlabel('step')\nax1.set_ylabel('Error')\nax1.set_xlim(0, 10000)\nax1.set_ylim(0, 0.2)\nax1.grid(True)\n\nax2.set_xlabel('step')\nax2.set_ylabel('Error')\nax2.set_xlim(0, 10000)\nax2.set_ylim(0, 0.2)\nax2.grid(True)\n\nfig.show()\n```\n\n![qiita.png](https://qiita-image-store.s3.amazonaws.com/0/32870/1fff0a9d-f4bd-759a-978f-e5702d70cb0e.png)\n\n\u4e0a\u306foutput layer\u306e\u307fdropout\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3002\n\u4e0b\u306fhidden layer\u306b\u3082dropout\u3092\u9069\u7528\u3057\u305f\u7d50\u679c\u3002\n\n\u3068\u3082\u306b\u3001\u671f\u5f85\u3057\u3066\u3044\u308b\u8aa4\u5dee\u306e\u6e1b\u5c11\u50be\u5411\u306f\u898b\u3089\u308c\u306a\u3044\u3002\n\n\u3053\u306e\u3042\u305f\u308a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u7bc9\u306e\u77e5\u8b58\u306f\u95a2\u9023\u8ad6\u6587\u306a\u3069\u306e\u60c5\u5831\u3092\u6574\u7406\u3057\u305f\u65b9\u304c\u3044\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\n\n\n", "tags": ["borgWarp"]}