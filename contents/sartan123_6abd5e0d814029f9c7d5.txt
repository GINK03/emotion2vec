{"context": "\n\n\u306f\u3058\u3081\u306b\n\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\n\u7d9a\u30fb\u6df1\u5c64\u5b66\u7fd2\u3067\u30a2\u30cb\u30e1\u9854\u3092\u5206\u985e\u3059\u308b with Keras\n\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u305d\u306e\u6b63\u89e3\u7387\u306f\u78ba\u304b\u3081\u308b\u3053\u3068\u306f\u3067\u304d\u305f\u3051\u3069\u3001\u50d5\u304c\u3084\u308a\u305f\u304b\u3063\u305f\u3053\u3068\u306f\u30cd\u30c3\u30c8\u3067\u62fe\u3063\u3066\u304d\u305f\u30a2\u30cb\u30e1\u30ad\u30e3\u30e9\u306e\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u305d\u308c\u3092\u5224\u5225\u3059\u308b\u3053\u3068\u3060\u3063\u305f\u306e\u3067\u3053\u306e\u8a18\u4e8b\u3067\u306f\u305d\u306e\u8fba\u3082\u542b\u3081\u3066\u66f8\u3044\u3066\u3044\u307e\u3059\u3002\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306fanimeface-character-dataset\u304b\u3089\u5165\u624b\u3057\u307e\u3057\u305f\u3002\n\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306eanimeface-character-dataset\u3092\u89e3\u51cd\u3057\u3066\u3001\u305d\u306e\u30d5\u30a1\u30eb\u30c0\u306e\u4e2d\u306fthumb\u3060\u3051\u306b\u3057\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u524d\u51e6\u7406\n\u753b\u50cf\u306fopencv\u3092\u4f7f\u3063\u306632\u00d732\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3001\u3059\u3079\u3066\u306e\u753b\u50cf(14490\u679a)\u306e\u884c\u5217\u3092numpy\u306esave()\u3092\u4f7f\u3063\u3066\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\u3053\u308c\u3067\u6bce\u56de\u753b\u50cf\u3092\u6271\u308f\u305a\u306b\u6e08\u307f\u5b9f\u884c\u901f\u5ea6\u3082\u5c11\u3057\u901f\u304f\u306a\u308a\u307e\u3059\u3002\n\u753b\u50cf\u306e\u6b63\u89e3\u30e9\u30d9\u30eb\u3082\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u683c\u7d0d\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\n\ndataset_predisporsal.py\nimport os\nimport numpy as np\nimport cv2 as cv\n\ndata_dir_path = \"./animeface-character-dataset/thumb/\"\ntmp = os.listdir(data_dir_path)\ntmp=sorted([x for x in tmp if os.path.isdir(data_dir_path+x)])\ndir_list = tmp\n\nX_target=[]\nfor dir_name in dir_list:\n    file_list = os.listdir(data_dir_path+dir_name)\n    for file_name in file_list:\n        if file_name.endswith('.png'):\n            image_path=str(data_dir_path)+str(dir_name)+'/'+str(file_name)\n            image = cv.imread(image_path)\n            image = cv.resize(image, (32, 32))\n            image = image.transpose(2,0,1)\n            image = image/255.\n            X_target.append(image)\n\nanime_class=[]\ncount=0\nfor dir_name in dir_list:\n    file_list = os.listdir(data_dir_path+dir_name)\n    for file_name in file_list:\n        if file_name.endswith('.png'):\n            anime_class.append(count)\n    count+=1\n\nanime_arr2=np.array(anime_class)\nnp.save('anime_face_target.npy',anime_arr2)\nanime_arr=np.array(X_target)\nnp.save('anime_face_data.npy',anime_arr)\n\n\nanime_face_data.npy\u306b\u306f\u753b\u50cf\u306e\u884c\u5217(14490,3,28,28)\nanime_face_target.npy\u306b\u306f\u6b63\u89e3\u30e9\u30d9\u30eb(14490,)\n\u304c\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\n\nKeras\u306b\u3088\u308b\u30e2\u30c7\u30eb\u69cb\u7bc9\u3068\u5b66\u7fd2\n\u524d\u51e6\u7406\u3067\u4f5c\u6210\u3057\u305f2\u3064\u306enpy\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5b66\u7fd2\u3055\u305b\u307e\u3059\u3002\n\nanime_face.py\nimport numpy as np\nnp.random.seed(20160715) # \u30b7\u30fc\u30c9\u5024\u3092\u56fa\u5b9a\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nimport sklearn.cross_validation\n\nX_test=np.load('anime_face_data.npy')\nY_target=np.load('anime_face_target.npy')\n\na_train, a_test, b_train, b_test = sklearn.cross_validation.train_test_split(X_test,Y_target)\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(96, 3, 3, border_mode='same', input_shape=(3, 32, 32)))\nmodel.add(Activation('relu'))\n\nmodel.add(Convolution2D(128, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(203))\nmodel.add(Activation('softmax'))\n\ninit_learning_rate = 1e-2\nopt = SGD(lr=init_learning_rate, decay=0.0, momentum=0.9, nesterov=False)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"acc\"])\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\nlrs = LearningRateScheduler(0.01)\n\nhist = model.fit(a_train,b_train, \n                batch_size=128, \n                nb_epoch=50, \n                validation_split=0.1, \n                verbose=1)\n\nmodel_json_str = model.to_json()\nopen('anime_face_model.json', 'w').write(model_json_str)\nmodel.save_weights('anime_face_model.h5')\n\nscore=model.evaluate(a_test, b_test, verbose=0)\nprint(score[1])\n\n\n\u7d50\u679c\u306f\u7d0455%\u306e\u6b63\u89e3\u7387\u3067\u3057\u305f\u3002\n\u6b63\u89e3\u7387\u3092\u4e0a\u3052\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u4eca\u56de\u306f\u89e6\u308c\u307e\u305b\u3093\u3002\nmodel_json_str = model.to_json()\nopen('anime_face_model.json', 'w').write(model_json_str)\nmodel.save_weights('anime_face_model.h5')\n\n\u3053\u306e\u90e8\u5206\u304c\u91cd\u8981\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u7d50\u679c\u3092anime_face_model.json\u3068nime_face_model.h5\u306b\u305d\u308c\u305e\u308c\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u3053\u308c\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u7d50\u679c\u3092\u4f7f\u3044\u56de\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u5b9f\u884c\u3059\u308b\u6642\u3082\u4e00\u77ac\u3067\u3059\u3002\n\u3055\u3066\u6b21\u304c\u672c\u984c\u3067\u3059\u3002\n\n\u5b66\u7fd2\u7d50\u679c\u3092\u4f7f\u3063\u3066\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u3092\u5224\u5225\u3059\u308b\n\u4eca\u56de\u4f7f\u7528\u3059\u308b\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u306f\u9b54\u6cd5\u5c11\u5973\u30ea\u30ea\u30ab\u30eb\u306a\u306e\u306f\u306e\u300c\u516b\u795e\u306f\u3084\u3066\u300d(yagami.png)\u3067\u3059\n\n\nload_anime_face.py\nimport numpy as np\nfrom keras.models import model_from_json\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\nimport sklearn.cross_validation\nimport cv2 as cv\nnp.random.seed(20160717)\n\nX_test=np.load('anime_face_data.npy')\nY_target=np.load('anime_face_target.npy')\n\nmodel = model_from_json(open('anime_face_model.json').read())\nmodel.load_weights('anime_face_model.h5')\ninit_learning_rate = 1e-2\nopt = SGD(lr=init_learning_rate, decay=0.0, momentum=0.9, nesterov=False)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"acc\"])\n\nimage = cv.imread('yagami.png')\nimage = cv.resize(image, (32, 32))\nimage = image.transpose(2,0,1)\nimage = image/255.\n\nimage=image.reshape(1,3,32,32)\n\nfor i in range(202):\n    sample_target=np.array([i])\n    score = model.evaluate(image, sample_target, verbose=0)\n    if score[1]==1.0:\n        break\nprint(i)\n\n\n39\u3068\u51fa\u529b\u3055\u308c\u307e\u3057\u305f\u3002\n\u3053\u306e\u6570\u5b57\u306fthumb\u306e\u4e2d\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5148\u982d\u306e\u6570\u5b57\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u3057\u305f\u304c\u3063\u306639\u756a\u306e\u516b\u795e\u306f\u3084\u3066\u3092\u6b63\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\u3057\u304b\u3057\u6b63\u89e3\u7387\u306f55%\u306a\u306e\u3067\u3001\u4e0d\u6b63\u89e3\u3067\u3042\u308b\u3053\u3068\u3082\u304b\u306a\u308a\u591a\u3044\u3067\u3059\u3002\n\n\u304a\u308f\u308a\u306b\n\u7279\u306b\u89e3\u8aac\u3082\u305b\u305a\u6de1\u3005\u3068\u30b3\u30fc\u30c9\u3068\u51e6\u7406\u5185\u5bb9\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u304c\u3001\u3053\u308c\u3067\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u30ad\u30e3\u30e9\u3092\u5224\u5225\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u6b63\u89e3\u7387\u3060\u3051\u3092\u898b\u3066\u3082\u3042\u307e\u308a\u5b9f\u611f\u304c\u6e67\u304b\u306a\u3044\u306e\u3067\u3001\u3053\u306e\u3088\u3046\u306b1\u679a\u306e\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u7d50\u679c\u304c\u8fd4\u3063\u3066\u304f\u308b\u3068\u50d5\u307f\u305f\u3044\u306a\u6a5f\u68b0\u5b66\u7fd2\u306e\u521d\u5fc3\u8005\u304b\u3089\u3059\u308b\u3068\u3084\u3063\u305f\u305c\u3068\u3044\u3046\u6c17\u6301\u3061\u306b\u306a\u308a\u307e\u3059\u3002\n\u4f55\u304b\u7591\u554f\u3084\u7a81\u3063\u8fbc\u307f\u304c\u3042\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002\n\u7c21\u5358\u3067\u3059\u304c\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002\n#\u306f\u3058\u3081\u306b\n\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\n[\u7d9a\u30fb\u6df1\u5c64\u5b66\u7fd2\u3067\u30a2\u30cb\u30e1\u9854\u3092\u5206\u985e\u3059\u308b with Keras](http://qiita.com/hogefugabar/items/afb4f6c9a93a4bbda51a)\n\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u305d\u306e\u6b63\u89e3\u7387\u306f\u78ba\u304b\u3081\u308b\u3053\u3068\u306f\u3067\u304d\u305f\u3051\u3069\u3001\u50d5\u304c\u3084\u308a\u305f\u304b\u3063\u305f\u3053\u3068\u306f\u30cd\u30c3\u30c8\u3067\u62fe\u3063\u3066\u304d\u305f\u30a2\u30cb\u30e1\u30ad\u30e3\u30e9\u306e\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u305d\u308c\u3092\u5224\u5225\u3059\u308b\u3053\u3068\u3060\u3063\u305f\u306e\u3067\u3053\u306e\u8a18\u4e8b\u3067\u306f\u305d\u306e\u8fba\u3082\u542b\u3081\u3066\u66f8\u3044\u3066\u3044\u307e\u3059\u3002\n\n#\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f[animeface-character-dataset](http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/)\u304b\u3089\u5165\u624b\u3057\u307e\u3057\u305f\u3002\n\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306eanimeface-character-dataset\u3092\u89e3\u51cd\u3057\u3066\u3001\u305d\u306e\u30d5\u30a1\u30eb\u30c0\u306e\u4e2d\u306fthumb\u3060\u3051\u306b\u3057\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n\n#\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u524d\u51e6\u7406\n\u753b\u50cf\u306fopencv\u3092\u4f7f\u3063\u306632\u00d732\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u3001\u3059\u3079\u3066\u306e\u753b\u50cf(14490\u679a)\u306e\u884c\u5217\u3092numpy\u306esave()\u3092\u4f7f\u3063\u3066\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\u3053\u308c\u3067\u6bce\u56de\u753b\u50cf\u3092\u6271\u308f\u305a\u306b\u6e08\u307f\u5b9f\u884c\u901f\u5ea6\u3082\u5c11\u3057\u901f\u304f\u306a\u308a\u307e\u3059\u3002\n\u753b\u50cf\u306e\u6b63\u89e3\u30e9\u30d9\u30eb\u3082\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u683c\u7d0d\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\n\n```dataset_predisporsal.py\nimport os\nimport numpy as np\nimport cv2 as cv\n\ndata_dir_path = \"./animeface-character-dataset/thumb/\"\ntmp = os.listdir(data_dir_path)\ntmp=sorted([x for x in tmp if os.path.isdir(data_dir_path+x)])\ndir_list = tmp\n\nX_target=[]\nfor dir_name in dir_list:\n    file_list = os.listdir(data_dir_path+dir_name)\n    for file_name in file_list:\n        if file_name.endswith('.png'):\n            image_path=str(data_dir_path)+str(dir_name)+'/'+str(file_name)\n            image = cv.imread(image_path)\n            image = cv.resize(image, (32, 32))\n            image = image.transpose(2,0,1)\n            image = image/255.\n            X_target.append(image)\n\nanime_class=[]\ncount=0\nfor dir_name in dir_list:\n    file_list = os.listdir(data_dir_path+dir_name)\n    for file_name in file_list:\n        if file_name.endswith('.png'):\n            anime_class.append(count)\n    count+=1\n\nanime_arr2=np.array(anime_class)\nnp.save('anime_face_target.npy',anime_arr2)\nanime_arr=np.array(X_target)\nnp.save('anime_face_data.npy',anime_arr)\n```\nanime_face_data.npy\u306b\u306f\u753b\u50cf\u306e\u884c\u5217(14490,3,28,28)\nanime_face_target.npy\u306b\u306f\u6b63\u89e3\u30e9\u30d9\u30eb(14490,)\n\u304c\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\n\n#Keras\u306b\u3088\u308b\u30e2\u30c7\u30eb\u69cb\u7bc9\u3068\u5b66\u7fd2\n\u524d\u51e6\u7406\u3067\u4f5c\u6210\u3057\u305f2\u3064\u306enpy\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5b66\u7fd2\u3055\u305b\u307e\u3059\u3002\n\n```anime_face.py\nimport numpy as np\nnp.random.seed(20160715) # \u30b7\u30fc\u30c9\u5024\u3092\u56fa\u5b9a\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nimport sklearn.cross_validation\n\nX_test=np.load('anime_face_data.npy')\nY_target=np.load('anime_face_target.npy')\n\na_train, a_test, b_train, b_test = sklearn.cross_validation.train_test_split(X_test,Y_target)\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(96, 3, 3, border_mode='same', input_shape=(3, 32, 32)))\nmodel.add(Activation('relu'))\n\nmodel.add(Convolution2D(128, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(203))\nmodel.add(Activation('softmax'))\n\ninit_learning_rate = 1e-2\nopt = SGD(lr=init_learning_rate, decay=0.0, momentum=0.9, nesterov=False)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"acc\"])\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\nlrs = LearningRateScheduler(0.01)\n\nhist = model.fit(a_train,b_train, \n                batch_size=128, \n                nb_epoch=50, \n                validation_split=0.1, \n                verbose=1)\n\nmodel_json_str = model.to_json()\nopen('anime_face_model.json', 'w').write(model_json_str)\nmodel.save_weights('anime_face_model.h5')\n\nscore=model.evaluate(a_test, b_test, verbose=0)\nprint(score[1])\n```\n\u7d50\u679c\u306f\u7d0455%\u306e\u6b63\u89e3\u7387\u3067\u3057\u305f\u3002\n\u6b63\u89e3\u7387\u3092\u4e0a\u3052\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u4eca\u56de\u306f\u89e6\u308c\u307e\u305b\u3093\u3002\n\n```\nmodel_json_str = model.to_json()\nopen('anime_face_model.json', 'w').write(model_json_str)\nmodel.save_weights('anime_face_model.h5')\n```\n\u3053\u306e\u90e8\u5206\u304c\u91cd\u8981\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u7d50\u679c\u3092anime_face_model.json\u3068nime_face_model.h5\u306b\u305d\u308c\u305e\u308c\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\u3053\u308c\u3067\u5b66\u7fd2\u30e2\u30c7\u30eb\u3068\u5b66\u7fd2\u7d50\u679c\u3092\u4f7f\u3044\u56de\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u5b9f\u884c\u3059\u308b\u6642\u3082\u4e00\u77ac\u3067\u3059\u3002\n\u3055\u3066\u6b21\u304c\u672c\u984c\u3067\u3059\u3002\n\n#\u5b66\u7fd2\u7d50\u679c\u3092\u4f7f\u3063\u3066\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u3092\u5224\u5225\u3059\u308b\n\n\u4eca\u56de\u4f7f\u7528\u3059\u308b\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u306f\u9b54\u6cd5\u5c11\u5973\u30ea\u30ea\u30ab\u30eb\u306a\u306e\u306f\u306e\u300c\u516b\u795e\u306f\u3084\u3066\u300d(yagami.png)\u3067\u3059\n\n![yagami.png](https://qiita-image-store.s3.amazonaws.com/0/125193/c6d5a5c1-c8f7-4a8f-5b90-5794bfa0c246.png)\n\n```load_anime_face.py\nimport numpy as np\nfrom keras.models import model_from_json\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\nimport sklearn.cross_validation\nimport cv2 as cv\nnp.random.seed(20160717)\n\nX_test=np.load('anime_face_data.npy')\nY_target=np.load('anime_face_target.npy')\n\nmodel = model_from_json(open('anime_face_model.json').read())\nmodel.load_weights('anime_face_model.h5')\ninit_learning_rate = 1e-2\nopt = SGD(lr=init_learning_rate, decay=0.0, momentum=0.9, nesterov=False)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=[\"acc\"])\n\nimage = cv.imread('yagami.png')\nimage = cv.resize(image, (32, 32))\nimage = image.transpose(2,0,1)\nimage = image/255.\n\nimage=image.reshape(1,3,32,32)\n\nfor i in range(202):\n    sample_target=np.array([i])\n    score = model.evaluate(image, sample_target, verbose=0)\n    if score[1]==1.0:\n        break\nprint(i)\n```\n39\u3068\u51fa\u529b\u3055\u308c\u307e\u3057\u305f\u3002\n\u3053\u306e\u6570\u5b57\u306fthumb\u306e\u4e2d\u306b\u3042\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5148\u982d\u306e\u6570\u5b57\u306b\u306a\u308a\u307e\u3059\u3002\n\n![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-10-18 13.51.55.png](https://qiita-image-store.s3.amazonaws.com/0/125193/6b1135b7-e813-4b27-4c40-47def28d352a.png)\n\n\u3057\u305f\u304c\u3063\u306639\u756a\u306e\u516b\u795e\u306f\u3084\u3066\u3092\u6b63\u89e3\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\u3057\u304b\u3057\u6b63\u89e3\u7387\u306f55%\u306a\u306e\u3067\u3001\u4e0d\u6b63\u89e3\u3067\u3042\u308b\u3053\u3068\u3082\u304b\u306a\u308a\u591a\u3044\u3067\u3059\u3002\n\n#\u304a\u308f\u308a\u306b\n\u7279\u306b\u89e3\u8aac\u3082\u305b\u305a\u6de1\u3005\u3068\u30b3\u30fc\u30c9\u3068\u51e6\u7406\u5185\u5bb9\u3092\u66f8\u3044\u3066\u304d\u307e\u3057\u305f\u304c\u3001\u3053\u308c\u3067\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u30ad\u30e3\u30e9\u3092\u5224\u5225\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u6b63\u89e3\u7387\u3060\u3051\u3092\u898b\u3066\u3082\u3042\u307e\u308a\u5b9f\u611f\u304c\u6e67\u304b\u306a\u3044\u306e\u3067\u3001\u3053\u306e\u3088\u3046\u306b1\u679a\u306e\u753b\u50cf\u3092\u5165\u529b\u3068\u3057\u3066\u7d50\u679c\u304c\u8fd4\u3063\u3066\u304f\u308b\u3068\u50d5\u307f\u305f\u3044\u306a\u6a5f\u68b0\u5b66\u7fd2\u306e\u521d\u5fc3\u8005\u304b\u3089\u3059\u308b\u3068\u3084\u3063\u305f\u305c\u3068\u3044\u3046\u6c17\u6301\u3061\u306b\u306a\u308a\u307e\u3059\u3002\n\u4f55\u304b\u7591\u554f\u3084\u7a81\u3063\u8fbc\u307f\u304c\u3042\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002\n\n\n\u7c21\u5358\u3067\u3059\u304c\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002\n", "tags": ["Keras", "DeepLearning", "Python", "\u6df1\u5c64\u5b66\u7fd2", "\u6a5f\u68b0\u5b66\u7fd2"]}