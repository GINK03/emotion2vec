{"context": " More than 1 year has passed since last update.\u65e5\u672c\u8a9e\u306e\u81ea\u7136\u8a00\u8a9e\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4e2d\u306b\u306f\u3001\u30d8\u30d6\u30e9\u30a4\u8a9e\u3084\u97d3\u56fd\u8a9e\u3092\u3076\u3061\u3053\u3093\u3060\u77ac\u9593\u306b\u30a8\u30e9\u30fc\u3067\u51e6\u7406\u304c\u6b62\u307e\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u591a\u3044\u3067\u3059\u3002\u305d\u3093\u306a\u3068\u304d\u306b\u4fbf\u5229\u306a\u546a\u6587\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u305f\u3068\u3048\u3070PyCon2015\u3067\u7d39\u4ecb\u3055\u308c\u305f\u306ejanome\u306f\u97d3\u56fd\u8a9e\u304c\u6df7\u5165\u3059\u308b\u3068\u30a8\u30e9\u30fc\u3067\u6b7b\u306c\njanome\u306fMeCab\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u624b\u9593\u3092\u7701\u3044\u3066\u304f\u308c\u308b\u7d20\u6674\u3089\u3057\u3044\u5f62\u614b\u7d20\u89e3\u6790\u5668\u306a\u306e\u3067\u3059\u304c\u3001\u65e5\u672c\u8a9e\u4ee5\u5916\u306e\u6587\u5b57\u304c1\u6587\u5b57\u3067\u3082\u6df7\u5165\u3059\u308b\u3068\u30a8\u30e9\u30fc\u3067\u6b7b\u306b\u307e\u3059\u3002wikipedia\u306e\u5de6\u8a00\u8a9e\u5207\u308a\u63db\u3048\u30d0\u30fc\u3092\u8aad\u307f\u8fbc\u3093\u3060\u4f8b\u3060\u3068...\n\nwikipedia\u306e\u5de6\u306e\u30d0\u30fc\u3088\u308a\ntext = \"\u4ed6\u8a00\u8a9e\u7248Italiano\ud55c\uad6d\uc5b4PolskiSimple English\"\nt = Tokenizer()\nfor token in t.tokenize(text):\n     print token\n\n---------------\nTraceback (most recent call last):\n  File \"tests.py\", line 98, in <module>\n    for token in t.tokenize(text):\n  File \"lib/python2.7/site-packages/janome/tokenizer.py\", line 107, in tokenize\n    pos += lattice.forward()\n  File \"lib/python2.7/site-packages/janome/lattice.py\", line 124, in forward\n    while not self.enodes[self.p]:\nIndexError: list index out of range\n\n\n\n\u305d\u3093\u306a\u3068\u304d\u306f\u3053\u306e\u546a\u6587\nimport re\nimport nltk\n\ndef filter(text):\n   \"\"\"\n   :param text: str\n   :rtype : str\n   \"\"\"\n   # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u3068\u534a\u89d2\u82f1\u6570\u3068\u8a18\u53f7\u3068\u6539\u884c\u3068\u30bf\u30d6\u3092\u6392\u9664\n   text = re.sub(r'[a-zA-Z0-9\u00a5\"\u00a5.\u00a5,\u00a5@]+', '', text)\n   text = re.sub(r'[!\"\u201c#$%&()\\*\\+\\-\\.,\\/:;<=>?@\\[\\\\\\]^_`{|}~]', '', text)\n   text = re.sub(r'[\\n|\\r|\\t]', '', text)\n\n   # \u65e5\u672c\u8a9e\u4ee5\u5916\u306e\u6587\u5b57\u3092\u6392\u9664(\u97d3\u56fd\u8a9e\u3068\u304b\u4e2d\u56fd\u8a9e\u3068\u304b\u30d8\u30d6\u30e9\u30a4\u8a9e\u3068\u304b)\n   jp_chartype_tokenizer = nltk.RegexpTokenizer(u'([\u3041-\u3093\u30fc]+|[\u30a1-\u30f3\u30fc]+|[\\u4e00-\\u9FFF]+|[\u3041-\u3093\u30a1-\u30f3\u30fc\\u4e00-\\u9FFF]+)')\n   text = \"\".join(jp_chartype_tokenizer.tokenize(text))\n   return text\n\n\ntext = \"\u4ed6\u8a00\u8a9e\u7248Italiano\ud55c\uad6d\uc5b4PolskiSimple English\"\ntext = filter(text)\nt = Tokenizer()\nfor token in t.tokenize(text):\n     print token\n\n\n------------------\n\u4ed6 \u63a5\u982d\u8a5e,\u540d\u8a5e\u63a5\u7d9a,*,*,*,*,\u4ed6,\u30bf,\u30bf\n\u8a00\u8a9e  \u540d\u8a5e,\u4e00\u822c,*,*,*,*,\u8a00\u8a9e,\u30b2\u30f3\u30b4,\u30b2\u30f3\u30b4\n\u7248 \u540d\u8a5e,\u63a5\u5c3e,\u4e00\u822c,*,*,*,\u7248,\u30d0\u30f3,\u30d0\u30f3\n\n\u65e5\u672c\u8a9e\u306e\u81ea\u7136\u8a00\u8a9e\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4e2d\u306b\u306f\u3001\u30d8\u30d6\u30e9\u30a4\u8a9e\u3084\u97d3\u56fd\u8a9e\u3092\u3076\u3061\u3053\u3093\u3060\u77ac\u9593\u306b\u30a8\u30e9\u30fc\u3067\u51e6\u7406\u304c\u6b62\u307e\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u591a\u3044\u3067\u3059\u3002\u305d\u3093\u306a\u3068\u304d\u306b\u4fbf\u5229\u306a\u546a\u6587\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n# \u305f\u3068\u3048\u3070PyCon2015\u3067\u7d39\u4ecb\u3055\u308c\u305f\u306ejanome\u306f\u97d3\u56fd\u8a9e\u304c\u6df7\u5165\u3059\u308b\u3068\u30a8\u30e9\u30fc\u3067\u6b7b\u306c\njanome\u306fMeCab\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u624b\u9593\u3092\u7701\u3044\u3066\u304f\u308c\u308b\u7d20\u6674\u3089\u3057\u3044\u5f62\u614b\u7d20\u89e3\u6790\u5668\u306a\u306e\u3067\u3059\u304c\u3001\u65e5\u672c\u8a9e\u4ee5\u5916\u306e\u6587\u5b57\u304c1\u6587\u5b57\u3067\u3082\u6df7\u5165\u3059\u308b\u3068\u30a8\u30e9\u30fc\u3067\u6b7b\u306b\u307e\u3059\u3002wikipedia\u306e\u5de6\u8a00\u8a9e\u5207\u308a\u63db\u3048\u30d0\u30fc\u3092\u8aad\u307f\u8fbc\u3093\u3060\u4f8b\u3060\u3068...\n\n```py:wikipedia\u306e\u5de6\u306e\u30d0\u30fc\u3088\u308a\ntext = \"\u4ed6\u8a00\u8a9e\u7248Italiano\ud55c\uad6d\uc5b4PolskiSimple English\"\nt = Tokenizer()\nfor token in t.tokenize(text):\n     print token\n\n---------------\nTraceback (most recent call last):\n  File \"tests.py\", line 98, in <module>\n    for token in t.tokenize(text):\n  File \"lib/python2.7/site-packages/janome/tokenizer.py\", line 107, in tokenize\n    pos += lattice.forward()\n  File \"lib/python2.7/site-packages/janome/lattice.py\", line 124, in forward\n    while not self.enodes[self.p]:\nIndexError: list index out of range\n```\n\n# \u305d\u3093\u306a\u3068\u304d\u306f\u3053\u306e\u546a\u6587\n\n```py:\nimport re\nimport nltk\n\ndef filter(text):\n   \"\"\"\n   :param text: str\n   :rtype : str\n   \"\"\"\n   # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u3068\u534a\u89d2\u82f1\u6570\u3068\u8a18\u53f7\u3068\u6539\u884c\u3068\u30bf\u30d6\u3092\u6392\u9664\n   text = re.sub(r'[a-zA-Z0-9\u00a5\"\u00a5.\u00a5,\u00a5@]+', '', text)\n   text = re.sub(r'[!\"\u201c#$%&()\\*\\+\\-\\.,\\/:;<=>?@\\[\\\\\\]^_`{|}~]', '', text)\n   text = re.sub(r'[\\n|\\r|\\t]', '', text)\n\n   # \u65e5\u672c\u8a9e\u4ee5\u5916\u306e\u6587\u5b57\u3092\u6392\u9664(\u97d3\u56fd\u8a9e\u3068\u304b\u4e2d\u56fd\u8a9e\u3068\u304b\u30d8\u30d6\u30e9\u30a4\u8a9e\u3068\u304b)\n   jp_chartype_tokenizer = nltk.RegexpTokenizer(u'([\u3041-\u3093\u30fc]+|[\u30a1-\u30f3\u30fc]+|[\\u4e00-\\u9FFF]+|[\u3041-\u3093\u30a1-\u30f3\u30fc\\u4e00-\\u9FFF]+)')\n   text = \"\".join(jp_chartype_tokenizer.tokenize(text))\n   return text\n\n\ntext = \"\u4ed6\u8a00\u8a9e\u7248Italiano\ud55c\uad6d\uc5b4PolskiSimple English\"\ntext = filter(text)\nt = Tokenizer()\nfor token in t.tokenize(text):\n     print token\n\n\n------------------\n\u4ed6\t\u63a5\u982d\u8a5e,\u540d\u8a5e\u63a5\u7d9a,*,*,*,*,\u4ed6,\u30bf,\u30bf\n\u8a00\u8a9e\t\u540d\u8a5e,\u4e00\u822c,*,*,*,*,\u8a00\u8a9e,\u30b2\u30f3\u30b4,\u30b2\u30f3\u30b4\n\u7248\t\u540d\u8a5e,\u63a5\u5c3e,\u4e00\u822c,*,*,*,\u7248,\u30d0\u30f3,\u30d0\u30f3\n```\n", "tags": ["Python", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406"]}