{"context": "\n\n\u30b7\u30ea\u30fc\u30ba\u76ee\u6b21\n\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u5c0e\u5165\u7de8\uff5e\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u5b9f\u88c5\u7de8\uff5e\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u3044\u3056\u5bfe\u6226\u7de8\uff5e\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b chainer\u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \u301c\u30ea\u30d9\u30f3\u30b8\u7de8\u305d\u306e\uff11\u301c\n\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b chainer\u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \u301c\u30ea\u30d9\u30f3\u30b8\u7de8\u305d\u306e\uff12\u301c\n\n\u524d\u56de\u306e\u7d9a\u304d...\n\u3053\u306e\u5206\u91ce\u3067\u306f\u9580\u5916\u6f22\u306e\u79c1\u304c\u3001\u300c\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u300d\u3092\u307e\u3063\u305f\u304f\u52c9\u5f37\u305b\u305a\u306b\n\u30aa\u30bb\u30ed\u306eAI \u3092\u4f5c\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\u306f\u3053\u3061\u3089\n\u30fb DQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b\n\u30fb Training TensorFlow neural network to play Tic-Tac-Toe game using one-step Q-learning algorithm.\n\n\u5f37\u5316\u5b66\u7fd2\u306e\u57fa\u790e\n\u300c\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u300d\u3092\u307e\u3063\u305f\u304f\u52c9\u5f37\u305b\u305a\u306b\u30aa\u30bb\u30ed\u306eAI \u3092\u4f5c\u308b\u306e\u3067\u3059\u304c\n\u5b9f\u88c5\u3059\u308b\u306e\u306b\u5fc5\u8981\u306a\u6700\u4f4e\u9650\u306e\u77e5\u8b58\u3092\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3068\u5f79\u5272\u308a\n\u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3068\u5f79\u5272\u308a\u306f\u3053\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3059\u3002 \n\n- train.py\u3000---\u3000AI \u306e\u8a13\u7df4\u3092\u884c\u3046\n- Reversi.py\u3000---\u3000\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u306e\u7ba1\u7406\n- dqn_agent.py\u3000---\u3000AI \u306e\u8a13\u7df4\u306e\u7ba1\u7406\n- FightWithAI.py\u3000---\u3000\u30e6\u30fc\u30b6\u30fc\u3068\u306e\u5bfe\u6226\n\n\u5168\u4f53\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n\u4eca\u56de\u5b9f\u88c5\u3059\u308bDQN\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3053\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3059\u3002 \n\n\u3053\u306e\u6d41\u308c\u3092\u982d\u306b\u5165\u308c\u3066\u304a\u3051\u3070\u3001\u3053\u308c\u304b\u3089\u8aac\u660e\u3059\u308b\u3082\u306e\u3082\u3001\u3069\u3053\u306e\u4f55\u306e\u3053\u3068\u3092\u8a00\u3063\u3066\u3044\u308b\u306e\u304b\u304c\u5206\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u306e\u4ed5\u69d8\n\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u304a\u3088\u3073AI \u306e\u8a13\u7df4\u3067\u7528\u3044\u308b\u76e4\u9762\u306f\n\u4e0b\u56f3\u306eNo\u3092\u632f\u3063\u305f2\u6b21\u5143\u914d\u5217\u3092\u7528\u3044\u3066\u884c\u3044\u307e\u3059\u3002\n\n\nReversi.py\nself.screen[0\uff5e7][0\uff5e7]\n\n\nAI\u304c\u9078\u629e\u3067\u304d\u308b\u52d5\u4f5c\u306f\u4e0a\u56f3\u306e 0\uff5e63 \u306e\u756a\u53f7\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\nReversi.py\nself.enable_actions[0\uff5e63]\n\n\n\nAI \u306e\u8a13\u7df4\nAI \u306e\u8a13\u7df4 \u306f\u3001players[0] \u3068 players[1] \u304c \u30aa\u30bb\u30ed\u5bfe\u6226\u3092n_epochs = 1000\u56de\u884c\u3044\u3001\n\u6700\u5f8c\u306b\u5f8c\u653b\u306e players[1] \u306eAI \u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\nAI\u306b\u5bfe\u3059\u308b\u5831\u916c\n\n\u30b2\u30fc\u30e0\u306b\u52dd\u3063\u305f\u3089 \u5831\u916c(reward)=1 \u3068\u3059\u308b\n\u305d\u308c\u4ee5\u5916\u306f \u5831\u916c(reward)=0\n\n\n\u8a13\u7df4\u65b9\u6cd5\n\uff12\u4f53\u306e AI \u3067\u5bfe\u6226\u3059\u308b\u306e\u3067\u3059\u304c\u3001\u76f8\u624b\u306e\u30bf\u30fc\u30f3\u3067\u3082\u884c\u52d5\u3057\u305f\u3053\u3068\u306b\u3057\u306a\u3044\u3068\n\u7d42\u5c40\u307e\u3067\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u304c\u3064\u306a\u304c\u3089\u306a\u3044\uff08\uff31\u5024\u304c\u4f1d\u9054\u3057\u306a\u3044\uff09\u306e\u3067\n\n\u3059\u3079\u3066\u306e\u30bf\u30fc\u30f3\u3067\u4e21\u8005\u884c\u52d5\u3057\u307e\u3059\n\u4eca\u56de\u306f\u3001\u30b2\u30fc\u30e0\u306e\u9032\u884c\u3068\u306f\u5225\u306b\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\u5168\u3066\u300cD\u306b\u63a8\u79fb\u3092\u4fdd\u5b58\u300d\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\ntrain.py\n\n#targets \u306b \u3053\u306e\u30bf\u30fc\u30f3\u3067\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\u304c\u5168\u3066\u5165\u3063\u3066\u3044\u308b\nfor tr in targets:\n    #\u73fe\u72b6\u3092\u8907\u88fd\n    tmp = copy.deepcopy(env)\n    #\u884c\u52d5\n    tmp.update(tr, playerID[i])\n    #\u7d42\u4e86\u5224\u5b9a\n    win = tmp.winner()\n    end = tmp.isEnd()\n    #\u884c\u52d5\u3057\u305f\u5f8c\u306e\u76e4\u9762\n    state_X = tmp.screen\n    #\u884c\u52d5\u3057\u305f\u5f8c\u306e\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\n    target_X = tmp.get_enables(playerID[i+1])\n\n    # \u4e21\u8005\u884c\u52d5\n    for j in range(0, len(players)):\n        reword = 0\n        if end == True:\n            if win == playerID[j]:\n                # \u52dd\u3063\u305f\u3089\u5831\u916c1\u3092\u5f97\u308b\n                reword = 1\n        # \u4e21\u8005\u300cD\u306b\u63a8\u79fb\u3092\u4fdd\u5b58\u300d                  \n        players[j].store_experience(state, targets, tr, reword, state_X, target_X, end)\n        players[j].experience_replay()\n\n\n\nDQN\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u3046\u3061\u4e0b\u8a18\u306e\u90e8\u5206\u306f dqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\nD\u306b\u63a8\u79fb(si,ai,ri,si+1,tarminal)\u3092\u4fdd\u5b58\nD\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u63a8\u79fb\u306e\u30df\u30cb\u30d1\u30c3\u30c1(si,ai,ri,si+1,tarminal)\u3092\u30b5\u30f3\u30d7\u30eb\n\u6559\u5e2b\u4fe1\u53f7 yi=ri+\u03b3max Q(si+1, a:\u03b8)\nQ Network\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u03b8\u306b\u3064\u3044\u3066(yi-Q(si, ai;\u03b8))^2 \u3067\u52fe\u914d\u6cd5\u3092\u5b9f\u884c\n\u5b9a\u671f\u7684\u306b Target Network \u3092\u30ea\u30bb\u30c3\u30c8 Q=Q\n\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\u306e \u307e\u308b\u30d1\u30af\u30ea \u306a\u306e\u3067\u308f\u3051\u306f\u5206\u304b\u3063\u3066\u3044\u307e\u305b\u3093\u304c\n\n\ndqn_agent.py\n    def store_experience(self, state, targets, action, reward, state_1, targets_1, terminal):\n        self.D.append((state, targets, action, reward, state_1, targets_1, terminal))\n\n    def experience_replay(self):\n        state_minibatch = []\n        y_minibatch = []\n\n        # sample random minibatch\n        minibatch_size = min(len(self.D), self.minibatch_size)\n        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n\n        for j in minibatch_indexes:\n            state_j, targets_j, action_j, reward_j, state_j_1, targets_j_1, terminal = self.D[j]\n            action_j_index = self.enable_actions.index(action_j)\n\n            y_j = self.Q_values(state_j)\n\n            if terminal:\n                y_j[action_j_index] = reward_j\n            else:\n                # reward_j + gamma * max_action' Q(state', action')\n                qvalue, action = self.select_enable_action(state_j_1, targets_j_1)\n                y_j[action_j_index] = reward_j + self.discount_factor * qvalue\n\n            state_minibatch.append(state_j)\n            y_minibatch.append(y_j)\n\n        # training\n        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n\n        # for log\n        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n\n\n\n\n\n\n\u5909\u6570\u540d\n\u5185\u5bb9\n\n\n\n\nstate\n\u76e4\u9762( = Reversi.screen[0\uff5e7][0\uff5e7] )\n\n\ntargets\n\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\n\n\naction\n\u9078\u629e\u3057\u305f\u884c\u52d5\n\n\nreward\n\u884c\u52d5\u306b\u5bfe\u3059\u308b\u5831\u916c\u30000\uff5e1\n\n\nstate_1\n\u884c\u52d5\u3057\u305f\u5f8c\u306e\u76e4\u9762\n\n\ntargets_1\n\u884c\u52d5\u3057\u305f\u5f8c\u306e\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\n\n\nterminal\n\u30b2\u30fc\u30e0\u304c\u7d42\u4e86\uff1dTrue\n\n\n\n\n\n\n\u5b9f\u88c5\nAI \u306e\u8a13\u7df4 \u306f\u3001players[0] \u3068 players[1] \u304c \u30aa\u30bb\u30ed\u5bfe\u6226\u3092n_epochs = 1000\u56de\u884c\u3044\u3001\n\u6700\u5f8c\u306b\u5f8c\u653b\u306e players[1] \u306eAI \u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\ntrain.py\n   # parameters\n    n_epochs = 1000\n    # environment, agent\n    env = Reversi()\n\n    # playerID    \n    playerID = [env.Black, env.White, env.Black]\n\n    # player agent    \n    players = []\n    # player[0]= env.Black\n    players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))\n    # player[1]= env.White\n    players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))\n\n\n\n\u3053\u306e DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols) \u90e8\u5206\u304c\u3001\n\n\nReplay Memory D \u306e\u521d\u671f\u5316\nQ NetworkQ \u3092\u30e9\u30f3\u30c0\u30e0\u306a\u91cd\u307f\u03b8\u3067\u521d\u671f\u5316\nTarget NetworkQ \u3092\u521d\u671f\u5316 \u03b8^=\u03b8\n\n\n\u3067\u3001dqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n\ndqn_agent.py\nclass DQNAgent:\n\n    def __init__(self, enable_actions, environment_name, rows, cols):\n        ... \u7701\u7565 ...\n        # Replay Memory D \u306e\u521d\u671f\u5316\n        self.D = deque(maxlen=self.replay_memory_size)\n        ... \u7701\u7565 ...\n\n    def init_model(self):\n        # input layer (rows x cols)\n        self.x = tf.placeholder(tf.float32, [None, self.rows, self.cols])\n\n        # flatten (rows x cols)\n        size = self.rows * self.cols\n        x_flat = tf.reshape(self.x, [-1, size])\n\n        # Q NetworkQ \u3092\u30e9\u30f3\u30c0\u30e0\u306a\u91cd\u307f\u03b8\u3067\u521d\u671f\u5316\n        W_fc1 = tf.Variable(tf.truncated_normal([size, size], stddev=0.01))\n        b_fc1 = tf.Variable(tf.zeros([size]))\n        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n\n        # Target NetworkQ \u3092\u521d\u671f\u5316 \u03b8^=\u03b8\n        W_out = tf.Variable(tf.truncated_normal([size, self.n_actions], stddev=0.01))\n        b_out = tf.Variable(tf.zeros([self.n_actions]))\n        self.y = tf.matmul(h_fc1, W_out) + b_out\n\n        # loss function\n        self.y_ = tf.placeholder(tf.float32, [None, self.n_actions])\n        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n\n        # train operation\n        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n        self.training = optimizer.minimize(self.loss)\n\n        # saver\n        self.saver = tf.train.Saver()\n\n        # session\n        self.sess = tf.Session()\n        self.sess.run(tf.initialize_all_variables())\n\n\n\n\n    for e in range(n_epochs):\n        # reset\n        env.reset()\n        terminal = False\n\n\n\nfor episode =1, M do\n\n\n\u521d\u671f\u753b\u9762x1,\u524d\u51e6\u7406\u3057\u521d\u671f\u72b6\u614bs1\u3092\u4f5c\u308b\n\n\n\n\n        while terminal == False: # 1\u30a8\u30d4\u30bd\u30fc\u30c9\u304c\u7d42\u308f\u308b\u307e\u3067\u30eb\u30fc\u30d7\n\n            for i in range(0, len(players)): \n\n                state = env.screen\n                targets = env.get_enables(playerID[i])\n\n                if len(targets) > 0:\n                    # \u3069\u3053\u304b\u306b\u7f6e\u304f\u5834\u6240\u304c\u3042\u308b\u5834\u5408 \n\n#\u2190 \u3053\u3053\u3067\u3001\u524d\u8ff0\u306e\u3059\u3079\u3066\u306e\u624b\u3092\u300cD\u306b\u4fdd\u5b58\u300d\u3057\u3066\u3044\u307e\u3059\n\n                    # \u884c\u52d5\u3092\u9078\u629e  \n                    action = players[i].select_action(state, targets, players[i].exploration)\n                    # \u884c\u52d5\u3092\u5b9f\u884c\n                    env.update(action, playerID[i])\n\n\n\nwhile not terminal\n\n\n\u884c\u52d5\u9078\u629e\n\n\n\n\n\u884c\u52d5\u9078\u629e agent.select_action(state_t, targets, agent.exploration)\u3000\u306f\u3001\ndqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u884c\u52d5\u9078\u629e\n\n\n\u30e9\u30f3\u30c0\u30e0\u306b\u884c\u52d5 ai\n\u307e\u305f\u306f ai= argmax Q(s1, a:\u03b8)\n\n\n\n\ndqn_agent.py\n    def Q_values(self, state):\n        # Q(state, action) of all actions\n        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n\n    def select_action(self, state, targets, epsilon):\n\n        if np.random.rand() <= epsilon:\n            # random\n            return np.random.choice(targets)\n        else:\n            # max_action Q(state, action)\n            qvalue, action = self.select_enable_action(state, targets)\n            return action\n\n    #\u305d\u306e\u76e4\u9762(state)\u3067, \u7f6e\u3044\u3066\u3044\u3044\u5834\u6240(targets)\u304b\u3089\uff31\u5024\u304c\u6700\u5927\u3068\u306a\u308b\uff31\u5024\u3068\u756a\u53f7\u3092\u8fd4\u3059          \n    def select_enable_action(self, state, targets):\n        Qs = self.Q_values(state)\n        #descend = np.sort(Qs)\n        index = np.argsort(Qs)\n        for action in reversed(index):\n            if action in targets:\n                break \n        # max_action Q(state, action)\n        qvalue = Qs[action]       \n\n        return qvalue, action          \n\n\n\n\n\u884c\u52d5ai\u3092\u5b9f\u884c\u3057\u3001\u5831\u916cri\u3068\u6b21\u306e\u753b\u9762xi+1\u3068\u7d42\u4e86\u5224\u5b9a tarminal\u3092\u89b3\u6e2c\n\u524d\u51e6\u7406\u3057\u6b21\u306e\u72b6\u614bsi+1\u3092\u4f5c\u308b\n\n\n\u6700\u5f8c\u306b\u5f8c\u624b\u306eAI\u3092\u4fdd\u5b58\u3057\u307e\u3059\n\n                # \u884c\u52d5\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\n                terminal = env.isEnd()     \n\n        w = env.winner()                    \n        print(\"EPOCH: {:03d}/{:03d} | WIN: player{:1d}\".format(\n                         e, n_epochs, w))\n\n\n    # \u4fdd\u5b58\u306f\u5f8c\u653b\u306eplayer2 \u3092\u4fdd\u5b58\u3059\u308b\u3002\n    players[1].save_model()\n\n\n\u30bd\u30fc\u30b9\u306f\u3053\u3053\u306b\u304a\u3044\u3066\u304a\u304d\u307e\u3059\u3002\n$ git clone https://github.com/sasaco/tf-dqn-reversi.git\n\u6b21\u56de\u306f\u3001\u3044\u3056\u5bfe\u6226\u7de8\u306b\u3064\u3044\u3066\u304a\u5c4a\u3051\u3057\u307e\u3059\u3002\n#\u30b7\u30ea\u30fc\u30ba\u76ee\u6b21\n- [\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u5c0e\u5165\u7de8\uff5e](http://qiita.com/sasaco/items/3b0b8565d6aa2a640caf)\n- \u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u5b9f\u88c5\u7de8\uff5e\n- [\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b tensorflow \u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \uff5e\u3044\u3056\u5bfe\u6226\u7de8\uff5e](http://qiita.com/sasaco/items/f9aa608860eebb3026c1)\n- [\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b chainer\u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \u301c\u30ea\u30d9\u30f3\u30b8\u7de8\u305d\u306e\uff11\u301c](http://www.structuralengine.com/?p=7)\n- [\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u3092\u7406\u89e3\u305b\u305a\u306b chainer\u3067 \u30aa\u30bb\u30ed AI \u3092\u4f5c\u3063\u3066\u307f\u305f \u301c\u30ea\u30d9\u30f3\u30b8\u7de8\u305d\u306e\uff12\u301c](http://www.structuralengine.com/?p=42)\n\n[\u524d\u56de](http://qiita.com/sasaco/items/3b0b8565d6aa2a640caf)\u306e\u7d9a\u304d...\n\u3053\u306e\u5206\u91ce\u3067\u306f\u9580\u5916\u6f22\u306e\u79c1\u304c\u3001\u300c\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u300d\u3092\u307e\u3063\u305f\u304f\u52c9\u5f37\u305b\u305a\u306b\n\u30aa\u30bb\u30ed\u306eAI \u3092\u4f5c\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\u306f\u3053\u3061\u3089\n\u30fb [DQN\u3092Keras\u3068TensorFlow\u3068OpenAI Gym\u3067\u5b9f\u88c5\u3059\u308b](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html)\n\u30fb [Training TensorFlow neural network to play Tic-Tac-Toe game using one-step Q-learning algorithm.](https://github.com/3cky/tensorflow-rl-tictactoe)\n\n#\u5f37\u5316\u5b66\u7fd2\u306e\u57fa\u790e\n\u300c\u6a5f\u68b0\u5b66\u7fd2\u306e\u7406\u8ad6\u300d\u3092\u307e\u3063\u305f\u304f\u52c9\u5f37\u305b\u305a\u306b\u30aa\u30bb\u30ed\u306eAI \u3092\u4f5c\u308b\u306e\u3067\u3059\u304c\n\u5b9f\u88c5\u3059\u308b\u306e\u306b\u5fc5\u8981\u306a\u6700\u4f4e\u9650\u306e\u77e5\u8b58\u3092\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002\n\n##\u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3068\u5f79\u5272\u308a\n\n\u30d5\u30a1\u30a4\u30eb\u69cb\u6210\u3068\u5f79\u5272\u308a\u306f\u3053\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3059\u3002 \n![\u69cb\u6210.png](https://qiita-image-store.s3.amazonaws.com/0/142847/240b3f5e-2b60-2c2b-f09d-ecc2f14a794f.png)\n- train.py\u3000---\u3000AI \u306e\u8a13\u7df4\u3092\u884c\u3046\n- Reversi.py\u3000---\u3000[\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u306e\u7ba1\u7406](http://qiita.com/Tsutomu-KKE@github/items/5824eb00250bf08f9197)\n- dqn_agent.py\u3000---\u3000AI \u306e\u8a13\u7df4\u306e\u7ba1\u7406\n- FightWithAI.py\u3000---\u3000\u30e6\u30fc\u30b6\u30fc\u3068\u306e\u5bfe\u6226\n\n\n##\u5168\u4f53\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n\n\u4eca\u56de\u5b9f\u88c5\u3059\u308bDQN\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u3053\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u3059\u3002 \n\n![algorithm.png](https://qiita-image-store.s3.amazonaws.com/0/142847/afa03c7b-1cbb-d58f-955a-9552108d45be.png)\n\u3053\u306e\u6d41\u308c\u3092\u982d\u306b\u5165\u308c\u3066\u304a\u3051\u3070\u3001\u3053\u308c\u304b\u3089\u8aac\u660e\u3059\u308b\u3082\u306e\u3082\u3001\u3069\u3053\u306e\u4f55\u306e\u3053\u3068\u3092\u8a00\u3063\u3066\u3044\u308b\u306e\u304b\u304c\u5206\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n#\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u306e\u4ed5\u69d8\n\n\u30aa\u30bb\u30ed\u30b2\u30fc\u30e0\u304a\u3088\u3073AI \u306e\u8a13\u7df4\u3067\u7528\u3044\u308b\u76e4\u9762\u306f\n\u4e0b\u56f3\u306eNo\u3092\u632f\u3063\u305f2\u6b21\u5143\u914d\u5217\u3092\u7528\u3044\u3066\u884c\u3044\u307e\u3059\u3002\n![screen.png](https://qiita-image-store.s3.amazonaws.com/0/142847/203d1eb0-729c-0f23-962b-afb8adfe5aa2.png)\n\n```python3:Reversi.py\nself.screen[0\uff5e7][0\uff5e7]\n```\nAI\u304c\u9078\u629e\u3067\u304d\u308b\u52d5\u4f5c\u306f\u4e0a\u56f3\u306e 0\uff5e63 \u306e\u756a\u53f7\u3092\u9078\u629e\u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\n```python3:Reversi.py\nself.enable_actions[0\uff5e63]\n```\n\n#AI \u306e\u8a13\u7df4\n\nAI \u306e\u8a13\u7df4 \u306f\u3001players[0] \u3068 players[1] \u304c \u30aa\u30bb\u30ed\u5bfe\u6226\u3092n_epochs = 1000\u56de\u884c\u3044\u3001\n\u6700\u5f8c\u306b\u5f8c\u653b\u306e players[1] \u306eAI \u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\n\n##AI\u306b\u5bfe\u3059\u308b\u5831\u916c\n\n- \u30b2\u30fc\u30e0\u306b\u52dd\u3063\u305f\u3089 \u5831\u916c(reward)=1 \u3068\u3059\u308b\n- \u305d\u308c\u4ee5\u5916\u306f \u5831\u916c(reward)=0\n\n\n##\u8a13\u7df4\u65b9\u6cd5\n\n\uff12\u4f53\u306e AI \u3067\u5bfe\u6226\u3059\u308b\u306e\u3067\u3059\u304c\u3001\u76f8\u624b\u306e\u30bf\u30fc\u30f3\u3067\u3082\u884c\u52d5\u3057\u305f\u3053\u3068\u306b\u3057\u306a\u3044\u3068\n\u7d42\u5c40\u307e\u3067\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u304c\u3064\u306a\u304c\u3089\u306a\u3044\uff08\uff31\u5024\u304c\u4f1d\u9054\u3057\u306a\u3044\uff09\u306e\u3067\n\n![8151b82b-2d1a-7b02-4282-1e98a5a9a265.png](https://qiita-image-store.s3.amazonaws.com/0/142847/f721dbe2-b13a-c9c8-56b0-223a13ef6474.png)\n\n\n\u3059\u3079\u3066\u306e\u30bf\u30fc\u30f3\u3067\u4e21\u8005\u884c\u52d5\u3057\u307e\u3059\n\u4eca\u56de\u306f\u3001\u30b2\u30fc\u30e0\u306e\u9032\u884c\u3068\u306f\u5225\u306b\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\u5168\u3066\u300cD\u306b\u63a8\u79fb\u3092\u4fdd\u5b58\u300d\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\n```python3:train.py\n\n#targets \u306b \u3053\u306e\u30bf\u30fc\u30f3\u3067\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\u304c\u5168\u3066\u5165\u3063\u3066\u3044\u308b\nfor tr in targets:\n    #\u73fe\u72b6\u3092\u8907\u88fd\n    tmp = copy.deepcopy(env)\n    #\u884c\u52d5\n    tmp.update(tr, playerID[i])\n    #\u7d42\u4e86\u5224\u5b9a\n    win = tmp.winner()\n    end = tmp.isEnd()\n    #\u884c\u52d5\u3057\u305f\u5f8c\u306e\u76e4\u9762\n    state_X = tmp.screen\n    #\u884c\u52d5\u3057\u305f\u5f8c\u306e\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7\n    target_X = tmp.get_enables(playerID[i+1])\n                       \n    # \u4e21\u8005\u884c\u52d5\n    for j in range(0, len(players)):\n        reword = 0\n        if end == True:\n            if win == playerID[j]:\n                # \u52dd\u3063\u305f\u3089\u5831\u916c1\u3092\u5f97\u308b\n                reword = 1\n        # \u4e21\u8005\u300cD\u306b\u63a8\u79fb\u3092\u4fdd\u5b58\u300d                  \n        players[j].store_experience(state, targets, tr, reword, state_X, target_X, end)\n        players[j].experience_replay()\n```\n\n>DQN\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u3046\u3061\u4e0b\u8a18\u306e\u90e8\u5206\u306f dqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n>\n- D\u306b\u63a8\u79fb(si,ai,ri,si+1,tarminal)\u3092\u4fdd\u5b58\n- D\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u63a8\u79fb\u306e\u30df\u30cb\u30d1\u30c3\u30c1(si,ai,ri,si+1,tarminal)\u3092\u30b5\u30f3\u30d7\u30eb\n- \u6559\u5e2b\u4fe1\u53f7 yi=ri+\u03b3max Q(si+1, a:\u03b8)\n- Q Network\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u03b8\u306b\u3064\u3044\u3066(yi-Q(si, ai;\u03b8))^2 \u3067\u52fe\u914d\u6cd5\u3092\u5b9f\u884c\n- \u5b9a\u671f\u7684\u306b Target Network \u3092\u30ea\u30bb\u30c3\u30c8 Q=Q\n\n>\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\u306e \u307e\u308b\u30d1\u30af\u30ea \u306a\u306e\u3067\u308f\u3051\u306f\u5206\u304b\u3063\u3066\u3044\u307e\u305b\u3093\u304c\n\n>>\n```python3:dqn_agent.py\n    def store_experience(self, state, targets, action, reward, state_1, targets_1, terminal):\n        self.D.append((state, targets, action, reward, state_1, targets_1, terminal))\n>>\n    def experience_replay(self):\n        state_minibatch = []\n        y_minibatch = []\n>>\n        # sample random minibatch\n        minibatch_size = min(len(self.D), self.minibatch_size)\n        minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)\n>>\n        for j in minibatch_indexes:\n            state_j, targets_j, action_j, reward_j, state_j_1, targets_j_1, terminal = self.D[j]\n            action_j_index = self.enable_actions.index(action_j)\n>>\n            y_j = self.Q_values(state_j)\n>>\n            if terminal:\n                y_j[action_j_index] = reward_j\n            else:\n                # reward_j + gamma * max_action' Q(state', action')\n                qvalue, action = self.select_enable_action(state_j_1, targets_j_1)\n                y_j[action_j_index] = reward_j + self.discount_factor * qvalue\n>>\n            state_minibatch.append(state_j)\n            y_minibatch.append(y_j)\n>>\n        # training\n        self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n>>\n        # for log\n        self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})\n>>\n```\n>>\n| \u5909\u6570\u540d | \u5185\u5bb9 |\n|:-:|:-:|\n| state  | \u76e4\u9762( = Reversi.screen[0\uff5e7][0\uff5e7] )  |\n| targets  | \u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7  |\n|  action | \u9078\u629e\u3057\u305f\u884c\u52d5  |\n| reward  | \u884c\u52d5\u306b\u5bfe\u3059\u308b\u5831\u916c\u30000\uff5e1  |\n|  state_1 |  \u884c\u52d5\u3057\u305f\u5f8c\u306e\u76e4\u9762 |\n|  targets_1 | \u884c\u52d5\u3057\u305f\u5f8c\u306e\u7f6e\u3044\u3066\u3044\u3044\u756a\u53f7  |\n|  terminal | \u30b2\u30fc\u30e0\u304c\u7d42\u4e86\uff1dTrue  |\n\n\n\n#\u5b9f\u88c5\n\nAI \u306e\u8a13\u7df4 \u306f\u3001players[0] \u3068 players[1] \u304c \u30aa\u30bb\u30ed\u5bfe\u6226\u3092n_epochs = 1000\u56de\u884c\u3044\u3001\n\u6700\u5f8c\u306b\u5f8c\u653b\u306e players[1] \u306eAI \u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\n```python3:train.py\n   # parameters\n    n_epochs = 1000\n    # environment, agent\n    env = Reversi()\n \n    # playerID    \n    playerID = [env.Black, env.White, env.Black]\n\n    # player agent    \n    players = []\n    # player[0]= env.Black\n    players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))\n    # player[1]= env.White\n    players.append(DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols))\n```\n>\u3053\u306e `DQNAgent(env.enable_actions, env.name, env.screen_n_rows, env.screen_n_cols)` \u90e8\u5206\u304c\u3001\n>>- Replay Memory D \u306e\u521d\u671f\u5316\n- Q NetworkQ \u3092\u30e9\u30f3\u30c0\u30e0\u306a\u91cd\u307f\u03b8\u3067\u521d\u671f\u5316\n- Target NetworkQ \u3092\u521d\u671f\u5316 \u03b8^=\u03b8\n\n>\u3067\u3001dqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n>>\n```python3:dqn_agent.py\nclass DQNAgent:\n>>\n    def __init__(self, enable_actions, environment_name, rows, cols):\n        ... \u7701\u7565 ...\n        # Replay Memory D \u306e\u521d\u671f\u5316\n        self.D = deque(maxlen=self.replay_memory_size)\n        ... \u7701\u7565 ...\n>>\n    def init_model(self):\n        # input layer (rows x cols)\n        self.x = tf.placeholder(tf.float32, [None, self.rows, self.cols])\n>>\n        # flatten (rows x cols)\n        size = self.rows * self.cols\n        x_flat = tf.reshape(self.x, [-1, size])\n>>\n        # Q NetworkQ \u3092\u30e9\u30f3\u30c0\u30e0\u306a\u91cd\u307f\u03b8\u3067\u521d\u671f\u5316\n        W_fc1 = tf.Variable(tf.truncated_normal([size, size], stddev=0.01))\n        b_fc1 = tf.Variable(tf.zeros([size]))\n        h_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n>>\n        # Target NetworkQ \u3092\u521d\u671f\u5316 \u03b8^=\u03b8\n        W_out = tf.Variable(tf.truncated_normal([size, self.n_actions], stddev=0.01))\n        b_out = tf.Variable(tf.zeros([self.n_actions]))\n        self.y = tf.matmul(h_fc1, W_out) + b_out\n>>\n        # loss function\n        self.y_ = tf.placeholder(tf.float32, [None, self.n_actions])\n        self.loss = tf.reduce_mean(tf.square(self.y_ - self.y))\n>>\n        # train operation\n        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n        self.training = optimizer.minimize(self.loss)\n>>\n        # saver\n        self.saver = tf.train.Saver()\n>>\n        # session\n        self.sess = tf.Session()\n        self.sess.run(tf.initialize_all_variables())\n```\n\n\n```python3:\n    for e in range(n_epochs):\n        # reset\n        env.reset()\n        terminal = False\n```\n>- for episode =1, M do\n    - \u521d\u671f\u753b\u9762x1,\u524d\u51e6\u7406\u3057\u521d\u671f\u72b6\u614bs1\u3092\u4f5c\u308b\n   \n```python3:\n        while terminal == False: # 1\u30a8\u30d4\u30bd\u30fc\u30c9\u304c\u7d42\u308f\u308b\u307e\u3067\u30eb\u30fc\u30d7\n\n            for i in range(0, len(players)): \n                \n                state = env.screen\n                targets = env.get_enables(playerID[i])\n                \n                if len(targets) > 0:\n                    # \u3069\u3053\u304b\u306b\u7f6e\u304f\u5834\u6240\u304c\u3042\u308b\u5834\u5408 \n\n#\u2190 \u3053\u3053\u3067\u3001\u524d\u8ff0\u306e\u3059\u3079\u3066\u306e\u624b\u3092\u300cD\u306b\u4fdd\u5b58\u300d\u3057\u3066\u3044\u307e\u3059\n\n                    # \u884c\u52d5\u3092\u9078\u629e  \n                    action = players[i].select_action(state, targets, players[i].exploration)\n                    # \u884c\u52d5\u3092\u5b9f\u884c\n                    env.update(action, playerID[i])\n``` \n>- while not terminal\n    - \u884c\u52d5\u9078\u629e\n    \n>>\u884c\u52d5\u9078\u629e `agent.select_action(state_t, targets, agent.exploration)`\u3000\u306f\u3001\ndqn_agent.py \u304c\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n>>- \u884c\u52d5\u9078\u629e\n    - \u30e9\u30f3\u30c0\u30e0\u306b\u884c\u52d5 ai\n    - \u307e\u305f\u306f ai= argmax Q(s1, a:\u03b8)\n>>\n```python3:dqn_agent.py\n    def Q_values(self, state):\n        # Q(state, action) of all actions\n        return self.sess.run(self.y, feed_dict={self.x: [state]})[0]\n>>\n    def select_action(self, state, targets, epsilon):\n>>    \n        if np.random.rand() <= epsilon:\n            # random\n            return np.random.choice(targets)\n        else:\n            # max_action Q(state, action)\n            qvalue, action = self.select_enable_action(state, targets)\n            return action\n>>  \n    #\u305d\u306e\u76e4\u9762(state)\u3067, \u7f6e\u3044\u3066\u3044\u3044\u5834\u6240(targets)\u304b\u3089\uff31\u5024\u304c\u6700\u5927\u3068\u306a\u308b\uff31\u5024\u3068\u756a\u53f7\u3092\u8fd4\u3059          \n    def select_enable_action(self, state, targets):\n        Qs = self.Q_values(state)\n        #descend = np.sort(Qs)\n        index = np.argsort(Qs)\n        for action in reversed(index):\n            if action in targets:\n                break \n        # max_action Q(state, action)\n        qvalue = Qs[action]       \n>>\n        return qvalue, action          \n``` \n\n\n>- \u884c\u52d5ai\u3092\u5b9f\u884c\u3057\u3001\u5831\u916cri\u3068\u6b21\u306e\u753b\u9762xi+1\u3068\u7d42\u4e86\u5224\u5b9a tarminal\u3092\u89b3\u6e2c\n- \u524d\u51e6\u7406\u3057\u6b21\u306e\u72b6\u614bsi+1\u3092\u4f5c\u308b\n\n\n\n\n\u6700\u5f8c\u306b\u5f8c\u624b\u306eAI\u3092\u4fdd\u5b58\u3057\u307e\u3059\n\n```python3:                           \n\n                # \u884c\u52d5\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\n                terminal = env.isEnd()     \n                              \n        w = env.winner()                    \n        print(\"EPOCH: {:03d}/{:03d} | WIN: player{:1d}\".format(\n                         e, n_epochs, w))\n\n\n    # \u4fdd\u5b58\u306f\u5f8c\u653b\u306eplayer2 \u3092\u4fdd\u5b58\u3059\u308b\u3002\n    players[1].save_model()\n\n```\n\n\n\u30bd\u30fc\u30b9\u306f\u3053\u3053\u306b\u304a\u3044\u3066\u304a\u304d\u307e\u3059\u3002\n[`$ git clone https://github.com/sasaco/tf-dqn-reversi.git`](https://github.com/sasaco/tf-dqn-reversi.git)\n\n[\u6b21\u56de](http://qiita.com/sasaco/items/f9aa608860eebb3026c1)\u306f\u3001\u3044\u3056\u5bfe\u6226\u7de8\u306b\u3064\u3044\u3066\u304a\u5c4a\u3051\u3057\u307e\u3059\u3002\n\n", "tags": ["TensorFlow", "Python", "DQN", "\u5f37\u5316\u5b66\u7fd2", "DeepLearning"]}