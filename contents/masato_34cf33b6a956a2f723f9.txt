{"context": " More than 1 year has passed since last update.\n\n\u3053\u306e\u30b7\u30ea\u30fc\u30ba\n\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 1: \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 2: Ubuntu\u3067IPython Notebook\u3092\u4f7f\u3046\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 3: \u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b\nSpark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 4: Ambari\u3067Hadoop\u3068Spark\u306esingle node\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\n\nApache Ambari\n\u3000Spark\u306e\u52c9\u5f37\u306f\u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9\u3067\u3082spark-shell\u3084spark-submit\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\u3001\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u306e\u305f\u3081\u306b\u306f\u6700\u521d\u304b\u3089\u5206\u6563\u74b0\u5883\u3067\u52d5\u304b\u3059\u65b9\u304c\u3084\u308b\u6c17\u304c\u51fa\u307e\u3059\u3002\n\u3000\u4eca\u56de\u306fHadoop\u30af\u30e9\u30b9\u30bf\u7ba1\u7406\u30c4\u30fc\u30eb\u306eApach Ambari\u3067Hadoop\u3092Docker\u4e0a\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002Ambari\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u30b3\u30f3\u30c6\u30ca\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305fSpark\u304b\u3089YARN\u3068HDFS\u3092\u4f7f\u3063\u3066\u307f\u307e\u3059\u3002\n\u3000Apach Ambari\u306fHDP\u30af\u30e9\u30b9\u30bf\u7ba1\u7406\u30c4\u30fc\u30eb\u306eCloudbreak\u306e\u4e2d\u5fc3\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u958b\u767a\u5143\u306eSequenceIQ2015\u5e744\u6708\u306bHortonworks\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001HDP\u3092\u30af\u30e9\u30a6\u30c9\u3068Docker\u3067\u69cb\u7bc9\u3059\u308b\u5834\u5408\u306b\u5b89\u5fc3\u3057\u3066\u4f7f\u3048\u307e\u3059\u3002\n\nBlueprint\n\u3000\n\u3000Ambari\u306e\u7279\u5fb4\u306bBlueprint\u304c\u3042\u308a\u307e\u3059\u3002JSON\u306eBlueprint\u5b9a\u7fa9\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u306fAmbari Shell\u306eCLI\u3001Cloudbreak\u306eAPI\u306a\u3069\u304b\u3089\u4f7f\u3044\u30af\u30e9\u30a6\u30c9\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u3084Docker\u30b3\u30f3\u30c6\u30ca\u306b\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u307e\u3059\u3002\n\nsingle-node-hdfs-yarn\n\u3000SequenceIQ\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u3042\u308bsingle-node-hdfs-yarn\u306eBlueprint\u3092\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\u4ed6\u306b\u3082Cloudbreak\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3082\u3042\u308b\u306e\u3067\u3082\u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3000Ambari provisioned Hadoop cluster on Docker\u3092\u8aad\u307f\u306a\u304c\u3089\u4f5c\u696d\u3092\u9032\u3081\u307e\u3059\u3002\n\nDocker\n\u3000Docker\u306fMesos\u3092Docker Compose\u3092\u4f7f\u3044\u8d77\u52d5\u3059\u308b - Part 1: 1\u3064\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\u3067\u69cb\u7bc9\u3057\u305fDebian\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u3092\u4f7f\u3044\u307e\u3059\u3002\n\nAmbari\n\u3000Ambari\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u306f\u3068\u3066\u3082\u7c21\u5358\u3067\u3059\u3002j.mp/docker-ambari\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u95a2\u6570\u3084\u74b0\u5883\u5909\u6570\u306e\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u306e\u5f15\u6570\u306f2 single-node-hdfs-yarn\u3092\u6307\u5b9a\u3059\u308b\u3068amb-server\u3068amb-agent\u3067\u8a082\u53f0\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u3067\u304d\u307e\u3059\u3002single\u3067\u3059\u304c\u5f15\u6570\u306f2\u306a\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\n$ curl -Lo .amb j.mp/docker-ambari && . .amb && amb-deploy-cluster 2 single-node-hdfs-yarn\n...\nambari-shell>cluster build --blueprint single-node-hdfs-yarn\n  HOSTNAME             STATE\n  -------------------  -------------------\n  amb1.service.consul  amb1.service.consul\n\n  HOSTGROUP  COMPONENT                                                   Installation: 6.81% ----------\n  ---------  -------------------\n  master     ZOOKEEPER_CLIENT\n  master     MAPREDUCE2_CLIENT\n  master     RESOURCEMANAGER\n  master     TEZ_CLIENT\n  master     SECONDARY_NAMENODE\n  master     HISTORYSERVER\n  master     APP_TIMELINE_SERVER\n  master     HDFS_CLIENT\n  master     DATANODE\n  master     NAMENODE\n  master     NODEMANAGER\n  master     ZOOKEEPER_SERVER\n  master     YARN_CLIENT\nCLUSTER_BUILD:single-node-hdfs-yarn>cluster autoAssign\n  HOSTGROUP  HOST\n  ---------  -------------------\n  master     amb1.service.consul\n\nCLUSTER_BUILD:single-node-hdfs-yarn>cluster create --exitOnFinish true\nSuccessfully created the cluster\nCLUSTER:single-node-hdfs-yarn>\n\n\u3000\u3053\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u3067\u306fDocker\u30a4\u30e1\u30fc\u30b8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3068\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u3001Ambari Shell\u3092\u4f7f\u3063\u305f\u8a2d\u5b9a\u307e\u3067\u81ea\u52d5\u7684\u306b\u884c\u3063\u3066\u304f\u308c\u307e\u3059\u3002\u30b7\u30a7\u30eb\u306e\u53f3\u4e0a\u306b\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u304c\u8868\u793a\u3055\u308c\u3057\u3070\u3089\u304f\u5f85\u3064\u3068\u7d42\u4e86\u3057\u307e\u3059\u3002\n\u3000\u4f5c\u6210\u3055\u308c\u305fDocker\u30b3\u30f3\u30c6\u30ca\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002Ambari Server\u3001Ambari Aagent\u3001Consul\u306e3\u3064\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u307e\u3057\u305f\u3002\u3000\n$ docker ps\nCONTAINER ID        IMAGE                         COMMAND                  CREATED             STATUS              PORTS                                                              NAMES\n70cbaaf536d9        sequenceiq/ambari:2.1.2-v1    \"/start-agent\"           9 minutes ago       Up 9 minutes        8080/tcp                                                           amb1\n682212a98724        sequenceiq/ambari:2.1.2-v1    \"/start-server\"          9 minutes ago       Up 9 minutes        8080/tcp                                                           amb-server\n04bf47cee35e        sequenceiq/consul:v0.5.0-v6   \"/bin/start -server -\"   9 minutes ago       Up 9 minutes        53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 8500/tcp   amb-consul\n\n\nSpark\n\u3000Ambari Agent\u306eamb1\u30b3\u30f3\u30c6\u30ca\u306b\u5165\u308aSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3044\u307e\u3059\u3002\n$ docker exec -it amb1 bash\n\n\u3000Single node\u306a\u306e\u3067\u3053\u306e\u30b3\u30f3\u30c6\u30ca\u306bHDP\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u5168\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002 HDP\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.3.2\u3067\u3059\u3002\n$ rpm -qa | grep hdp-select\nhdp-select-2.3.2.0-2950.el6.noarch\n$ hdp-select versions\n2.3.2.0-2950\n$ /usr/hdp/2.3.2.0-2950\n\n\u3000Install Apache Spark with Cloudbreak\u3092\u8aad\u307f\u306a\u304c\u3089\u4f5c\u696d\u3092\u9032\u3081\u3066\u3044\u304d\u307e\u3059\u3002\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u95a2\u6570\u3068\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002Ambari\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u540c\u3058\u3088\u3046\u306a\u624b\u9806\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n$ curl -Lo .spark-install j.mp/spark-hdp-install && . .spark-install\n$ install-spark\nupload assembly jar to hdfs\n\n\u3000HDFS\u306bSpark\u306ejar\u304c\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u3080\u3068Spark\u306e\u8d77\u52d5\u306b\u5fc5\u8981\u306a\u74b0\u5883\u5909\u6570\u3082\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n$ echo $YARN_CONF_DIR\n/usr/hdp/2.2.0.0-2041/hadoop/conf\n$ echo $SPARK_JAR\nhdfs:///spark/spark-assembly-1.2.0.2.2.0.0-82-hadoop2.6.0.2.2.0.0-2041.jar\n\n\u3000\u6b21\u306bspark-env.sh\u306f\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u30b3\u30d4\u30fc\u3057\u3066HADOOP_CONF_DIR\u74b0\u5883\u5909\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n$ cd /usr/local/spark/conf\n$ cp spark-env.sh.template spark-env.sh\n$ vi spark-env.sh\nHADOOP_CONF_DIR=/usr/hdp/current/hadoop-client/conf\n\n\nspark-submit\n\u3000spark-submit\u3092\u5b9f\u884c\u3057\u3066\u5186\u5468\u7387\u306e\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\n$ spark-submit --class org.apache.spark.examples.SparkPi   \\\n  --master yarn-cluster  \\\n  --num-executors 3 \\\n  --driver-memory 512m  \\\n  --executor-memory 512m  \\\n  --executor-cores 1  \\\n  /usr/local/spark/lib/spark-examples*.jar\n...\n15/11/30 02:04:13 INFO yarn.Client:\n         client token: N/A\n         diagnostics: N/A\n         ApplicationMaster host: amb1.service.consul\n         ApplicationMaster RPC port: 0\n         queue: default\n         start time: 1448850560018\n         final status: SUCCEEDED\n         tracking URL: http://amb1.service.consul:8088/proxy/application_1448849730081_0004/\n         user: hdfs\n\nYARN\u306elog\u306b\u8a08\u7b97\u3055\u308c\u305f\u5186\u5468\u7387\u304c\u51fa\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n$ yarn logs -applicationId application_1448849730081_0004\n...\nLogType:stdout\nLog Upload Time:Mon Nov 30 02:48:54 +0000 2015\nLogLength:23\nLog Contents:\nPi is roughly 3.140252\nEnd of LogType:stdout\n\n\nHDFS\n\n$ hadoop fs -ls /user\nFound 2 items\ndrwxrwx---   - ambari-qa hdfs          0 2015-11-30 02:14 /user/ambari-qa\ndrwxr-xr-x   - hdfs      hdfs          0 2015-11-30 02:18 /user/hdfs\n$ hadoop fs -ls /user/hdfs\nFound 1 items\ndrwxr-xr-x   - hdfs hdfs          0 2015-11-30 02:35 /user/hdfs/.sparkStaging\n\n\nspark-shell\n\u3000\u6b21\u306bspark-shell\u306e\u8d77\u52d5\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068\u30ed\u30b0\u304c\u305f\u304f\u3055\u3093\u51fa\u308b\u306e\u3067rootCategory\u306e\u30ed\u30b0\u30ec\u30d9\u30eb\u3092INFO\u304b\u3089WARN\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n$ cd /usr/local/spark/conf\n$ cp log4j.properties.template log4j.properties\n$ sed -i 's/log4j.rootCategory=INFO/log4j.rootCategory=WARN/' log4j.properties\n\n\u3000--master\u30d5\u30e9\u30b0\u306byarn\u3092\u6307\u5b9a\u3057\u3066YARN\u4e0a\u3067spark-shell\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n$ spark-shell --master yarn\ntput: No value for $TERM and no -T specified\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)\nType in expressions to have them evaluated.\nType :help for more information.\n15/11/30 03:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n15/11/30 03:06:09 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n15/11/30 03:06:09 WARN ClientBase: SPARK_JAR detected in the system environment. This variable has been deprecated in favor of the spark.yarn.jar configuration variable.\n15/11/30 03:06:09 WARN ClientBase: SPARK_JAR detected in the system environment. This variable has been deprecated in favor of the spark.yarn.jar configuration variable.\nSpark context available as sc.\n\nscala>\n\n\u3000SPARK_JAR\u74b0\u5883\u5909\u6570\u304cdeprecated\u306b\u306a\u3063\u305f\u8b66\u544a\u304c\u51fa\u3066\u3044\u307e\u3059\u3002\u74b0\u5883\u5909\u6570\u3092unset\u3001\u74b0\u5883\u8a2d\u5b9a\u30b9\u30af\u30ea\u30d7\u30c8\u3082\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3057\u3066\u6b21\u56de\u5b9f\u884c\u6642\u306b\u306f\u30ed\u30fc\u30c9\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n$ echo $SPARK_JAR\nhdfs:///spark/spark-assembly-1.2.0.2.2.0.0-82-hadoop2.6.0.2.2.0.0-2041.jar\n$ unset SPARK_JAR\n$ sed -i 's/^ *\\(export SPARK_JAR.*\\)$/# \\1/' /tmp/.spark-install}}}\n\n\u3000\u518d\u5ea6spark-shell\u3092\u5b9f\u884c\u3059\u308b\u3068\u8b66\u544a\u304c\u6d88\u3048\u307e\u3057\u305f\u3002\nspark-shell --master yarn\ntput: No value for $TERM and no -T specified\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)\nType in expressions to have them evaluated.\nType :help for more information.\n15/11/30 03:14:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n15/11/30 03:14:14 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\nSpark context available as sc.\n\nscala>\n\n\n\u6b21\u306bSpark\u3092\u4f7f\u3046\u3068\u304d\n\u3000\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u3057\u305f\u30b7\u30a7\u30eb\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u629c\u3051\u305f\u5f8c\u306bSpark\u3092\u5b9f\u884c\u3059\u308b\u5834\u5408\u306f\u3001\u74b0\u5883\u8a2d\u5b9a\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u5fc5\u8981\u306a\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n$ cd /tmp\n$ source spark-install\n$ set-spark-envs\n$ echo $SPARK_HOME\n/usr/local/spark\n$ echo $HADOOP_USER_NAME\nhdfs\n$ which spark-submit\n/usr/local/spark/bin/spark-submit\n\n\u3000set-spark-envs\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u74b0\u5883\u5909\u6570\u3092export\u3057\u307e\u3059\u3002\nset-spark-envs() {\n  export SPARK_HOME=/usr/local/spark\n  export YARN_CONF_DIR=/usr/hdp/$HDP_VERSION/hadoop/conf\n  export PATH=$PATH:$SPARK_HOME/bin\n  export HADOOP_USER_NAME=hdfs\n#  export SPARK_JAR=hdfs:///spark/spark-assembly-$SPARK_ASSEMBLY_VERSION.jar\n}\n\n\n\u307e\u3068\u3081\n\u3000\u6b21\u56de\u306fAmbari\u306eBlueprint\u306b\u3042\u308bmulti-node-hdfs-yarn\u3084\u3001Apache Zeppelin\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308bdatascientist.bp\u306a\u3069\u3082\u3046\u5c11\u3057\u5927\u304d\u3081\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u53c2\u8003\n\u4ee5\u4e0b\u306e\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\nAmbari provisioned Hadoop cluster on Docker\nInstall Apache Hadoop And Spark On Docker (aim for cluster)\nInstall Apache Spark with Cloudbreak\n\n\n## \u3053\u306e\u30b7\u30ea\u30fc\u30ba\n\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 1: \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb](http://qiita.com/masato/items/fe93157293114999b0d8)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 2: Ubuntu\u3067IPython Notebook\u3092\u4f7f\u3046](http://qiita.com/masato/items/be383a81e323f3b5b42e)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 3: \u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9\u3067spark-shell\u3092\u8d77\u52d5\u3059\u308b](http://qiita.com/masato/items/9398cccde46cf62aa609)\n* [Spark on Docker\u3067\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u3092\u59cb\u3081\u308b - Part 4: Ambari\u3067Hadoop\u3068Spark\u306esingle node\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7](http://qiita.com/masato/items/34cf33b6a956a2f723f9)\n\n\n## Apache Ambari\n\n\u3000Spark\u306e\u52c9\u5f37\u306f[\u30ed\u30fc\u30ab\u30eb\u30e2\u30fc\u30c9](http://qiita.com/masato/items/9398cccde46cf62aa609)\u3067\u3082spark-shell\u3084spark-submit\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\u3001\u5206\u6563\u578b\u6a5f\u68b0\u5b66\u7fd2\u306e\u305f\u3081\u306b\u306f\u6700\u521d\u304b\u3089\u5206\u6563\u74b0\u5883\u3067\u52d5\u304b\u3059\u65b9\u304c\u3084\u308b\u6c17\u304c\u51fa\u307e\u3059\u3002\n\u3000\u4eca\u56de\u306fHadoop\u30af\u30e9\u30b9\u30bf\u7ba1\u7406\u30c4\u30fc\u30eb\u306e[Apach Ambari](http://incubator.apache.org/ambari/)\u3067Hadoop\u3092Docker\u4e0a\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002Ambari\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u30b3\u30f3\u30c6\u30ca\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u305fSpark\u304b\u3089YARN\u3068HDFS\u3092\u4f7f\u3063\u3066\u307f\u307e\u3059\u3002\n\u3000[Apach Ambari](http://incubator.apache.org/ambari/)\u306f[HDP](http://jp.hortonworks.com/hdp/)\u30af\u30e9\u30b9\u30bf\u7ba1\u7406\u30c4\u30fc\u30eb\u306e[Cloudbreak](https://github.com/sequenceiq/cloudbreak)\u306e\u4e2d\u5fc3\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u958b\u767a\u5143\u306e[SequenceIQ](http://sequenceiq.com/)2015\u5e744\u6708\u306b[Hortonworks](http://hortonworks.com/)\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001[HDP](http://jp.hortonworks.com/hdp/)\u3092\u30af\u30e9\u30a6\u30c9\u3068Docker\u3067\u69cb\u7bc9\u3059\u308b\u5834\u5408\u306b\u5b89\u5fc3\u3057\u3066\u4f7f\u3048\u307e\u3059\u3002\n\n\n### Blueprint\n\u3000\n\u3000Ambari\u306e\u7279\u5fb4\u306b[Blueprint](https://cwiki.apache.org/confluence/display/AMBARI/Blueprints)\u304c\u3042\u308a\u307e\u3059\u3002JSON\u306eBlueprint\u5b9a\u7fa9\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u306f[Ambari Shell](https://github.com/sequenceiq/ambari-shell)\u306eCLI\u3001[Cloudbreak](http://docs.cloudbreak.apiary.io)\u306eAPI\u306a\u3069\u304b\u3089\u4f7f\u3044\u30af\u30e9\u30a6\u30c9\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u3084Docker\u30b3\u30f3\u30c6\u30ca\u306b\u30c7\u30d7\u30ed\u30a4\u3067\u304d\u307e\u3059\u3002\n\n## single-node-hdfs-yarn\n\n\u3000SequenceIQ\u306e[\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/sequenceiq/ambari-rest-client/tree/master/src/main/resources/blueprints)\u306b\u3042\u308b[single-node-hdfs-yarn](https://github.com/sequenceiq/ambari-rest-client/blob/master/src/main/resources/blueprints/single-node-hdfs-yarn)\u306eBlueprint\u3092\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\u4ed6\u306b\u3082Cloudbreak\u306e[\u30ea\u30dd\u30b8\u30c8\u30ea](https://github.com/sequenceiq/cloudbreak/tree/master/core/src/main/resources/defaults/blueprints)\u3082\u3042\u308b\u306e\u3067\u3082\u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3000[Ambari provisioned Hadoop cluster on Docker](http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/)\u3092\u8aad\u307f\u306a\u304c\u3089\u4f5c\u696d\u3092\u9032\u3081\u307e\u3059\u3002\n\n## Docker\n\u3000Docker\u306f[Mesos\u3092Docker Compose\u3092\u4f7f\u3044\u8d77\u52d5\u3059\u308b - Part 1: 1\u3064\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u306b\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b](http://qiita.com/masato/items/12960a98eb95f3cacb05)\u3067\u69cb\u7bc9\u3057\u305fDebian\u306e\u4eee\u60f3\u30de\u30b7\u30f3\u3092\u4f7f\u3044\u307e\u3059\u3002\n\n## Ambari\n\n\u3000Ambari\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u306f\u3068\u3066\u3082\u7c21\u5358\u3067\u3059\u3002[j.mp/docker-ambari](https://raw.githubusercontent.com/sequenceiq/docker-ambari/master/ambari-functions)\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u95a2\u6570\u3084\u74b0\u5883\u5909\u6570\u306e\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u306e\u5f15\u6570\u306f`2 single-node-hdfs-yarn`\u3092\u6307\u5b9a\u3059\u308b\u3068`amb-server`\u3068`amb-agent`\u3067\u8a082\u53f0\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u3067\u304d\u307e\u3059\u3002`single`\u3067\u3059\u304c\u5f15\u6570\u306f2\u306a\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n```bash\n$ curl -Lo .amb j.mp/docker-ambari && . .amb && amb-deploy-cluster 2 single-node-hdfs-yarn\n...\nambari-shell>cluster build --blueprint single-node-hdfs-yarn\n  HOSTNAME             STATE\n  -------------------  -------------------\n  amb1.service.consul  amb1.service.consul\n\n  HOSTGROUP  COMPONENT                                                   Installation: 6.81% ----------\n  ---------  -------------------\n  master     ZOOKEEPER_CLIENT\n  master     MAPREDUCE2_CLIENT\n  master     RESOURCEMANAGER\n  master     TEZ_CLIENT\n  master     SECONDARY_NAMENODE\n  master     HISTORYSERVER\n  master     APP_TIMELINE_SERVER\n  master     HDFS_CLIENT\n  master     DATANODE\n  master     NAMENODE\n  master     NODEMANAGER\n  master     ZOOKEEPER_SERVER\n  master     YARN_CLIENT\nCLUSTER_BUILD:single-node-hdfs-yarn>cluster autoAssign\n  HOSTGROUP  HOST\n  ---------  -------------------\n  master     amb1.service.consul\n\nCLUSTER_BUILD:single-node-hdfs-yarn>cluster create --exitOnFinish true\nSuccessfully created the cluster\nCLUSTER:single-node-hdfs-yarn>\n```\n\n\u3000\u3053\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u3067\u306fDocker\u30a4\u30e1\u30fc\u30b8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3068\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u3001Ambari Shell\u3092\u4f7f\u3063\u305f\u8a2d\u5b9a\u307e\u3067\u81ea\u52d5\u7684\u306b\u884c\u3063\u3066\u304f\u308c\u307e\u3059\u3002\u30b7\u30a7\u30eb\u306e\u53f3\u4e0a\u306b\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u304c\u8868\u793a\u3055\u308c\u3057\u3070\u3089\u304f\u5f85\u3064\u3068\u7d42\u4e86\u3057\u307e\u3059\u3002\n\u3000\u4f5c\u6210\u3055\u308c\u305fDocker\u30b3\u30f3\u30c6\u30ca\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002Ambari Server\u3001Ambari Aagent\u3001[Consul](https://www.consul.io/)\u306e3\u3064\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u307e\u3057\u305f\u3002\u3000\n\n```bash\n$ docker ps\nCONTAINER ID        IMAGE                         COMMAND                  CREATED             STATUS              PORTS                                                              NAMES\n70cbaaf536d9        sequenceiq/ambari:2.1.2-v1    \"/start-agent\"           9 minutes ago       Up 9 minutes        8080/tcp                                                           amb1\n682212a98724        sequenceiq/ambari:2.1.2-v1    \"/start-server\"          9 minutes ago       Up 9 minutes        8080/tcp                                                           amb-server\n04bf47cee35e        sequenceiq/consul:v0.5.0-v6   \"/bin/start -server -\"   9 minutes ago       Up 9 minutes        53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 8500/tcp   amb-consul\n```\n\n## Spark\n\n\u3000Ambari Agent\u306eamb1\u30b3\u30f3\u30c6\u30ca\u306b\u5165\u308aSpark\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3044\u307e\u3059\u3002\n\n```bash\n$ docker exec -it amb1 bash\n```\n\n\u3000Single node\u306a\u306e\u3067\u3053\u306e\u30b3\u30f3\u30c6\u30ca\u306bHDP\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u5168\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002 HDP\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f2.3.2\u3067\u3059\u3002\n\n```bash\n$ rpm -qa | grep hdp-select\nhdp-select-2.3.2.0-2950.el6.noarch\n$ hdp-select versions\n2.3.2.0-2950\n$ /usr/hdp/2.3.2.0-2950\n```\n\n\u3000[Install Apache Spark with Cloudbreak](http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak/)\u3092\u8aad\u307f\u306a\u304c\u3089\u4f5c\u696d\u3092\u9032\u3081\u3066\u3044\u304d\u307e\u3059\u3002\u30b7\u30a7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u95a2\u6570\u3068\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002Ambari\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3068\u540c\u3058\u3088\u3046\u306a\u624b\u9806\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u95a2\u6570\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```bash\n$ curl -Lo .spark-install j.mp/spark-hdp-install && . .spark-install\n$ install-spark\nupload assembly jar to hdfs\n```\n\n\u3000HDFS\u306bSpark\u306ejar\u304c\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u3080\u3068Spark\u306e\u8d77\u52d5\u306b\u5fc5\u8981\u306a\u74b0\u5883\u5909\u6570\u3082\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n```bash\n$ echo $YARN_CONF_DIR\n/usr/hdp/2.2.0.0-2041/hadoop/conf\n$ echo $SPARK_JAR\nhdfs:///spark/spark-assembly-1.2.0.2.2.0.0-82-hadoop2.6.0.2.2.0.0-2041.jar\n```\n\n\u3000\u6b21\u306bspark-env.sh\u306f\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u30b3\u30d4\u30fc\u3057\u3066`HADOOP_CONF_DIR`\u74b0\u5883\u5909\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n```bash\n$ cd /usr/local/spark/conf\n$ cp spark-env.sh.template spark-env.sh\n$ vi spark-env.sh\nHADOOP_CONF_DIR=/usr/hdp/current/hadoop-client/conf\n```\n\n### spark-submit\n\n\u3000spark-submit\u3092\u5b9f\u884c\u3057\u3066\u5186\u5468\u7387\u306e\u8a08\u7b97\u3092\u3057\u307e\u3059\u3002\n\n```bash\n$ spark-submit --class org.apache.spark.examples.SparkPi   \\\n  --master yarn-cluster  \\\n  --num-executors 3 \\\n  --driver-memory 512m  \\\n  --executor-memory 512m  \\\n  --executor-cores 1  \\\n  /usr/local/spark/lib/spark-examples*.jar\n...\n15/11/30 02:04:13 INFO yarn.Client:\n         client token: N/A\n         diagnostics: N/A\n         ApplicationMaster host: amb1.service.consul\n         ApplicationMaster RPC port: 0\n         queue: default\n         start time: 1448850560018\n         final status: SUCCEEDED\n         tracking URL: http://amb1.service.consul:8088/proxy/application_1448849730081_0004/\n         user: hdfs\n```\n\nYARN\u306elog\u306b\u8a08\u7b97\u3055\u308c\u305f\u5186\u5468\u7387\u304c\u51fa\u529b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n```bash\n$ yarn logs -applicationId application_1448849730081_0004\n...\nLogType:stdout\nLog Upload Time:Mon Nov 30 02:48:54 +0000 2015\nLogLength:23\nLog Contents:\nPi is roughly 3.140252\nEnd of LogType:stdout\n```\n\n * HDFS\n\n```bash\n$ hadoop fs -ls /user\nFound 2 items\ndrwxrwx---   - ambari-qa hdfs          0 2015-11-30 02:14 /user/ambari-qa\ndrwxr-xr-x   - hdfs      hdfs          0 2015-11-30 02:18 /user/hdfs\n$ hadoop fs -ls /user/hdfs\nFound 1 items\ndrwxr-xr-x   - hdfs hdfs          0 2015-11-30 02:35 /user/hdfs/.sparkStaging\n```\n\n### spark-shell\n\n\u3000\u6b21\u306bspark-shell\u306e\u8d77\u52d5\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068\u30ed\u30b0\u304c\u305f\u304f\u3055\u3093\u51fa\u308b\u306e\u3067`rootCategory`\u306e\u30ed\u30b0\u30ec\u30d9\u30eb\u3092INFO\u304b\u3089WARN\u306b\u5909\u66f4\u3057\u307e\u3059\u3002\n\n```bash\n$ cd /usr/local/spark/conf\n$ cp log4j.properties.template log4j.properties\n$ sed -i 's/log4j.rootCategory=INFO/log4j.rootCategory=WARN/' log4j.properties\n```\n\n\u3000`--master`\u30d5\u30e9\u30b0\u306b`yarn`\u3092\u6307\u5b9a\u3057\u3066YARN\u4e0a\u3067spark-shell\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```bash\n$ spark-shell --master yarn\ntput: No value for $TERM and no -T specified\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)\nType in expressions to have them evaluated.\nType :help for more information.\n15/11/30 03:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n15/11/30 03:06:09 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n15/11/30 03:06:09 WARN ClientBase: SPARK_JAR detected in the system environment. This variable has been deprecated in favor of the spark.yarn.jar configuration variable.\n15/11/30 03:06:09 WARN ClientBase: SPARK_JAR detected in the system environment. This variable has been deprecated in favor of the spark.yarn.jar configuration variable.\nSpark context available as sc.\n\nscala>\n```\n\n\u3000SPARK_JAR\u74b0\u5883\u5909\u6570\u304cdeprecated\u306b\u306a\u3063\u305f\u8b66\u544a\u304c\u51fa\u3066\u3044\u307e\u3059\u3002\u74b0\u5883\u5909\u6570\u3092unset\u3001\u74b0\u5883\u8a2d\u5b9a\u30b9\u30af\u30ea\u30d7\u30c8\u3082\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3057\u3066\u6b21\u56de\u5b9f\u884c\u6642\u306b\u306f\u30ed\u30fc\u30c9\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n\n```bash\n$ echo $SPARK_JAR\nhdfs:///spark/spark-assembly-1.2.0.2.2.0.0-82-hadoop2.6.0.2.2.0.0-2041.jar\n$ unset SPARK_JAR\n$ sed -i 's/^ *\\(export SPARK_JAR.*\\)$/# \\1/' /tmp/.spark-install}}}\n```\n\n\u3000\u518d\u5ea6spark-shell\u3092\u5b9f\u884c\u3059\u308b\u3068\u8b66\u544a\u304c\u6d88\u3048\u307e\u3057\u305f\u3002\n\n```bash\nspark-shell --master yarn\ntput: No value for $TERM and no -T specified\nSpark assembly has been built with Hive, including Datanucleus jars on classpath\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.2.0\n      /_/\n\nUsing Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)\nType in expressions to have them evaluated.\nType :help for more information.\n15/11/30 03:14:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n15/11/30 03:14:14 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\nSpark context available as sc.\n\nscala>\n```\n\n### \u6b21\u306bSpark\u3092\u4f7f\u3046\u3068\u304d\n\n\u3000\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u3057\u305f\u30b7\u30a7\u30eb\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u629c\u3051\u305f\u5f8c\u306bSpark\u3092\u5b9f\u884c\u3059\u308b\u5834\u5408\u306f\u3001\u74b0\u5883\u8a2d\u5b9a\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8aad\u307f\u8fbc\u307f\u5fc5\u8981\u306a\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```bash\n$ cd /tmp\n$ source spark-install\n$ set-spark-envs\n$ echo $SPARK_HOME\n/usr/local/spark\n$ echo $HADOOP_USER_NAME\nhdfs\n$ which spark-submit\n/usr/local/spark/bin/spark-submit\n```\n\n\u3000`set-spark-envs`\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u74b0\u5883\u5909\u6570\u3092export\u3057\u307e\u3059\u3002\n\n```bash\nset-spark-envs() {\n  export SPARK_HOME=/usr/local/spark\n  export YARN_CONF_DIR=/usr/hdp/$HDP_VERSION/hadoop/conf\n  export PATH=$PATH:$SPARK_HOME/bin\n  export HADOOP_USER_NAME=hdfs\n#  export SPARK_JAR=hdfs:///spark/spark-assembly-$SPARK_ASSEMBLY_VERSION.jar\n}\n```\n\n## \u307e\u3068\u3081\n\n\u3000\u6b21\u56de\u306fAmbari\u306eBlueprint\u306b\u3042\u308b[multi-node-hdfs-yarn](https://github.com/sequenceiq/ambari-rest-client/blob/master/src/main/resources/blueprints/multi-node-hdfs-yarn)\u3084\u3001[Apache Zeppelin](https://zeppelin.incubator.apache.org/)\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b[datascientist.bp](https://github.com/sequenceiq/cloudbreak/blob/master/core/src/main/resources/defaults/blueprints/datascientist.bp)\u306a\u3069\u3082\u3046\u5c11\u3057\u5927\u304d\u3081\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\n## \u53c2\u8003\n\n\u4ee5\u4e0b\u306e\u30b5\u30a4\u30c8\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\n* [Ambari provisioned Hadoop cluster on Docker](http://blog.sequenceiq.com/blog/2014/06/17/ambari-cluster-on-docker/)\n* [Install Apache Hadoop And Spark On Docker (aim for cluster)](http://blog.iamtao.me/2015/08/13/Install-Apache-Hadoop-And-Spark-On-Docker/)\n* [Install Apache Spark with Cloudbreak](http://blog.sequenceiq.com/blog/2015/01/30/spark-with-cloudbreak/)\n", "tags": ["Spark1.2.0", "Ambari2.1.2", "HDP2.3.2", "docker1.9.1", "hadoop2.6.0"]}