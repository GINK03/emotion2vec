{"context": "Sim-to-Real Robot Learning from Pixels with Progressive Nets\npixel\u30ec\u30d9\u30eb\u306e\u5165\u529b\u304b\u3089endtoend\u3067\u30bf\u30b9\u30af\u3092\u884c\u3046\u5236\u5fa1\u51fa\u529b\u3092\u884c\u3046\u554f\u984c\u306f\u89e3\u304f\u306e\u304c\u96e3\u3057\u3044\u3002\n\u305d\u306e\u305f\u3081\u306e\u624b\u6cd5\u306e\uff11\u3064\u3068\u3057\u3066\u306fdeep reinforcement learning\u306b\u57fa\u3065\u304f\u624b\u6cd5\u304c\u3042\u308b\u304c\u3001\u57fa\u672c\u7684\u306bbrute force\u63a2\u7d22\u306e\u305f\u3081\u73fe\u5b9f\u4e16\u754c\u3067\u306e\u8a66\u884c\u3067\u306f\u52b9\u7387\u304c\u60aa\u3044\u3002\n\u3057\u304b\u3057\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u74b0\u5883\u306b\u304a\u3044\u3066\u306f\u30b3\u30b9\u30c8\u304c\u4f4e\u3044\u305f\u3081\u4e00\u5b9a\u306e\u52b9\u679c\u3092\u671f\u5f85\u3067\u304d\u308b\u3002\n\u672c\u8ad6\u3067\u306f\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u74b0\u5883\u3067\u5b66\u7fd2\u3055\u308c\u305f\u30e2\u30c7\u30eb\u306b\u95a2\u3057\u3066\u73fe\u5b9f\u4e16\u754c\u3068\u306e\u30ae\u30e3\u30c3\u30d7\u3092\u57cb\u3081\u308b\u3088\u3046\u306a\u624b\u6cd5\u3068\u3057\u3066progressive network\u3092\u63d0\u6848\u3059\u308b\u3002\nProgressive network\u305d\u306e\u3082\u306e\u306f\u3001\u4f4e\u30ec\u30d9\u30eb\u304b\u3089\u9ad8\u30ec\u30d9\u30eb\u307e\u3067\u306e\u7279\u5fb4\u3092\u8ee2\u79fb\u3055\u305b\u3066\u518d\u5229\u7528\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3042\u308b\u3002\n\u3053\u306e\u5229\u7528\u306b\u3088\u308a\u3001\u5b9f\u4e16\u754c\u3067\u306e\u30bf\u30b9\u30af\u3092\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9\u624b\u6cd5\u306b\u983c\u3089\u305a\u3001deep reinforcement learning\u306e\u624b\u6cd5\u306b\u3088\u308a\u5b66\u7fd2\u3059\u308b\u4e8b\u306b\u6210\u529f\u3057\u305f\u3002\nSHIV: Reducing Supervisor Burden in DAgger using Support Vectors for Efficient Learning from Demonstrations in High Dimensional State Spaces, ICRA2016\n\u30b7\u30b9\u30c6\u30e0\u7279\u6027\u3084\u30b3\u30b9\u30c8\u95a2\u6570\u304c\u4e0d\u660e\u306a\u5834\u5408\u306b\u304a\u3051\u308b\u3001\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u57fa\u3065\u3044\u305f\u30aa\u30f3\u30e9\u30a4\u30f3\u5b66\u7fd2\u624b\u6cd5\u3092\u8003\u3048\u308b\u3002\nDAgger\u306e\u3088\u3046\u306a\u5f93\u6765\u624b\u6cd5\u306b\u304a\u3044\u3066\u306f\u3001agent\u304c\u65b0\u3057\u3044\u72b6\u614b\u306b\u306a\u308b\u305f\u3073\u306b\u30af\u30a8\u30ea\u3092\u884c\u3046\u305f\u3081\u3001\u305d\u306e\u30b3\u30b9\u30c8\u306f\u5927\u304d\u304f\u306a\u308b\u3002\n\u305d\u306e\u30b3\u30b9\u30c8\u3092\u8efd\u6e1b\u3059\u308b\u7b56\u3068\u3057\u3066\u3001\u5f97\u3066\u3044\u308b\u30c7\u30fc\u30bf\u5206\u5e03\u304b\u3089\u6709\u610f\u306a\u5dee\u304c\u306a\u3044\u30af\u30a8\u30ea\u30b5\u30f3\u30d7\u30eb\u3092\u68c4\u5374\u3059\u308b\u624b\u6cd5\u304c\u3042\u308b\u3002\n\u672c\u8ad6\u3067\u306f\u305d\u308c\u3092SVM\u30d9\u30fc\u30b9\u306b\u884c\u3044\u3001\u30c7\u30fc\u30bf\u5206\u5e03\u306elevel set estimation\u3092\u884c\u3044\u3001\u305d\u306e\u5883\u754c\u3092\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u3092\u7528\u3044\u3066\u8868\u73fe\u3059\u308b\u3002\n\u8868\u73fe\u3055\u308c\u305f\u5883\u754c\u304b\u3089\u306e\u8ddd\u96e2\u3092\u7528\u3044\u308b\u4e8b\u3067\u30af\u30a8\u30ea\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3059\u308b\u30ea\u30b9\u30af\u3092\u8a08\u7b97\u53ef\u80fd\u306b\u3059\u308b\u3002\n\u4ee5\u4e0a\u306e\u624b\u6cd5\u306b\u3088\u308a\u5fc5\u8981\u306a\u30af\u30a8\u30ea\u3092\u6e1b\u3089\u3059\u4e8b\u306b\u6210\u529f\u3057\u305f\u3002\nPLATO: Policy Learning using Adaptive Trajectory Optimization\n\u753b\u50cf\u306a\u3069\u306e\u8907\u96d1\u306a\u5165\u529b\u306b\u5bfe\u3057\u3066\u8907\u96d1\u306apolicy\u3092\u5b66\u7fd2\u3059\u308b\u4e8b\u306f\u96e3\u3057\u3044\u3002\n\u672c\u8ad6\u3067\u63d0\u6848\u3059\u308b\u624b\u6cd5\u306ePLATO\u306f\u3001model-predictive control(MPC)\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u305f\u6559\u5e2b\u60c5\u5831\u3092\u7528\u3044\u3066\u5b66\u7fd2\u3092\u884c\u3046\u3002\n\u3053\u308c\u306b\u3088\u308a\u3001\u5b66\u7fd2\u9014\u4e2d\u306e\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u884c\u52d5\u3092\u884c\u3046\u5fc5\u8981\u304c\u306a\u304f\u306a\u308b\u3002\nPLATO\u306fMPC\u306e\u6319\u52d5\u3092\u5909\u5316\u3055\u305b\u3066\u5c11\u3057\u305a\u3064\u5b66\u7fd2\u3057\u3066\u3044\u308b\u30e2\u30c7\u30eb\u306epolicy\u3078\u4e00\u81f4\u3055\u305b\u308b\u4e8b\u3067\u3001\u30e2\u30c7\u30eb\u306e\u72b6\u614b\u306b\u5bfe\u5fdc\u3057\u305f\u8a13\u7df4\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u53ef\u80fd\u306b\u3059\u308b\u3002\n\u307e\u305fMPC\u306b\u57fa\u3065\u304f\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u8ffd\u52a0\u3057\u3001\u671b\u307e\u3057\u304f\u306a\u3044\u884c\u52d5\u3092\u56de\u907f\u3059\u308b\u3088\u3046\u306b\u3067\u304d\u308b\u3002\n\n[Sim-to-Real Robot Learning from Pixels with Progressive Nets](https://arxiv.org/pdf/1610.04286v1.pdf)\npixel\u30ec\u30d9\u30eb\u306e\u5165\u529b\u304b\u3089endtoend\u3067\u30bf\u30b9\u30af\u3092\u884c\u3046\u5236\u5fa1\u51fa\u529b\u3092\u884c\u3046\u554f\u984c\u306f\u89e3\u304f\u306e\u304c\u96e3\u3057\u3044\u3002\n\u305d\u306e\u305f\u3081\u306e\u624b\u6cd5\u306e\uff11\u3064\u3068\u3057\u3066\u306fdeep reinforcement learning\u306b\u57fa\u3065\u304f\u624b\u6cd5\u304c\u3042\u308b\u304c\u3001\u57fa\u672c\u7684\u306bbrute force\u63a2\u7d22\u306e\u305f\u3081\u73fe\u5b9f\u4e16\u754c\u3067\u306e\u8a66\u884c\u3067\u306f\u52b9\u7387\u304c\u60aa\u3044\u3002\n\u3057\u304b\u3057\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u74b0\u5883\u306b\u304a\u3044\u3066\u306f\u30b3\u30b9\u30c8\u304c\u4f4e\u3044\u305f\u3081\u4e00\u5b9a\u306e\u52b9\u679c\u3092\u671f\u5f85\u3067\u304d\u308b\u3002\n\u672c\u8ad6\u3067\u306f\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u74b0\u5883\u3067\u5b66\u7fd2\u3055\u308c\u305f\u30e2\u30c7\u30eb\u306b\u95a2\u3057\u3066\u73fe\u5b9f\u4e16\u754c\u3068\u306e\u30ae\u30e3\u30c3\u30d7\u3092\u57cb\u3081\u308b\u3088\u3046\u306a\u624b\u6cd5\u3068\u3057\u3066progressive network\u3092\u63d0\u6848\u3059\u308b\u3002\nProgressive network\u305d\u306e\u3082\u306e\u306f\u3001\u4f4e\u30ec\u30d9\u30eb\u304b\u3089\u9ad8\u30ec\u30d9\u30eb\u307e\u3067\u306e\u7279\u5fb4\u3092\u8ee2\u79fb\u3055\u305b\u3066\u518d\u5229\u7528\u3059\u308b\u305f\u3081\u306e\u4e00\u822c\u7684\u306a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3042\u308b\u3002\n\u3053\u306e\u5229\u7528\u306b\u3088\u308a\u3001\u5b9f\u4e16\u754c\u3067\u306e\u30bf\u30b9\u30af\u3092\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9\u624b\u6cd5\u306b\u983c\u3089\u305a\u3001deep reinforcement learning\u306e\u624b\u6cd5\u306b\u3088\u308a\u5b66\u7fd2\u3059\u308b\u4e8b\u306b\u6210\u529f\u3057\u305f\u3002\n\n[SHIV: Reducing Supervisor Burden in DAgger using Support Vectors for Efficient Learning from Demonstrations in High Dimensional State Spaces](http://goldberg.berkeley.edu/pubs/icra16-submitted-SHIV.pdf), ICRA2016\n\u30b7\u30b9\u30c6\u30e0\u7279\u6027\u3084\u30b3\u30b9\u30c8\u95a2\u6570\u304c\u4e0d\u660e\u306a\u5834\u5408\u306b\u304a\u3051\u308b\u3001\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u57fa\u3065\u3044\u305f\u30aa\u30f3\u30e9\u30a4\u30f3\u5b66\u7fd2\u624b\u6cd5\u3092\u8003\u3048\u308b\u3002\nDAgger\u306e\u3088\u3046\u306a\u5f93\u6765\u624b\u6cd5\u306b\u304a\u3044\u3066\u306f\u3001agent\u304c\u65b0\u3057\u3044\u72b6\u614b\u306b\u306a\u308b\u305f\u3073\u306b\u30af\u30a8\u30ea\u3092\u884c\u3046\u305f\u3081\u3001\u305d\u306e\u30b3\u30b9\u30c8\u306f\u5927\u304d\u304f\u306a\u308b\u3002\n\u305d\u306e\u30b3\u30b9\u30c8\u3092\u8efd\u6e1b\u3059\u308b\u7b56\u3068\u3057\u3066\u3001\u5f97\u3066\u3044\u308b\u30c7\u30fc\u30bf\u5206\u5e03\u304b\u3089\u6709\u610f\u306a\u5dee\u304c\u306a\u3044\u30af\u30a8\u30ea\u30b5\u30f3\u30d7\u30eb\u3092\u68c4\u5374\u3059\u308b\u624b\u6cd5\u304c\u3042\u308b\u3002\n\u672c\u8ad6\u3067\u306f\u305d\u308c\u3092SVM\u30d9\u30fc\u30b9\u306b\u884c\u3044\u3001\u30c7\u30fc\u30bf\u5206\u5e03\u306elevel set estimation\u3092\u884c\u3044\u3001\u305d\u306e\u5883\u754c\u3092\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u3092\u7528\u3044\u3066\u8868\u73fe\u3059\u308b\u3002\n\u8868\u73fe\u3055\u308c\u305f\u5883\u754c\u304b\u3089\u306e\u8ddd\u96e2\u3092\u7528\u3044\u308b\u4e8b\u3067\u30af\u30a8\u30ea\u30b5\u30f3\u30d7\u30eb\u306b\u5bfe\u3059\u308b\u30ea\u30b9\u30af\u3092\u8a08\u7b97\u53ef\u80fd\u306b\u3059\u308b\u3002\n\u4ee5\u4e0a\u306e\u624b\u6cd5\u306b\u3088\u308a\u5fc5\u8981\u306a\u30af\u30a8\u30ea\u3092\u6e1b\u3089\u3059\u4e8b\u306b\u6210\u529f\u3057\u305f\u3002\n\n[PLATO: Policy Learning using Adaptive Trajectory Optimization](https://arxiv.org/pdf/1603.00622.pdf)\n\u753b\u50cf\u306a\u3069\u306e\u8907\u96d1\u306a\u5165\u529b\u306b\u5bfe\u3057\u3066\u8907\u96d1\u306apolicy\u3092\u5b66\u7fd2\u3059\u308b\u4e8b\u306f\u96e3\u3057\u3044\u3002\n\u672c\u8ad6\u3067\u63d0\u6848\u3059\u308b\u624b\u6cd5\u306ePLATO\u306f\u3001model-predictive control(MPC)\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u305f\u6559\u5e2b\u60c5\u5831\u3092\u7528\u3044\u3066\u5b66\u7fd2\u3092\u884c\u3046\u3002\n\u3053\u308c\u306b\u3088\u308a\u3001\u5b66\u7fd2\u9014\u4e2d\u306e\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u884c\u52d5\u3092\u884c\u3046\u5fc5\u8981\u304c\u306a\u304f\u306a\u308b\u3002\nPLATO\u306fMPC\u306e\u6319\u52d5\u3092\u5909\u5316\u3055\u305b\u3066\u5c11\u3057\u305a\u3064\u5b66\u7fd2\u3057\u3066\u3044\u308b\u30e2\u30c7\u30eb\u306epolicy\u3078\u4e00\u81f4\u3055\u305b\u308b\u4e8b\u3067\u3001\u30e2\u30c7\u30eb\u306e\u72b6\u614b\u306b\u5bfe\u5fdc\u3057\u305f\u8a13\u7df4\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u53ef\u80fd\u306b\u3059\u308b\u3002\n\u307e\u305fMPC\u306b\u57fa\u3065\u304f\u30b3\u30b9\u30c8\u95a2\u6570\u3092\u8ffd\u52a0\u3057\u3001\u671b\u307e\u3057\u304f\u306a\u3044\u884c\u52d5\u3092\u56de\u907f\u3059\u308b\u3088\u3046\u306b\u3067\u304d\u308b\u3002\n", "tags": ["\u8ad6\u6587\u8aad\u307f", "\u6a5f\u68b0\u5b66\u7fd2", "MachineLearning"]}