{"context": "\n\nTheano \u5b9f\u88c5\u30b3\u30fc\u30c9\n\n\n\uff08 GitHub \uff09pascanur/natgrad Natural Gradient implementation in Theano\n\n\nImplementation of natural gradient in python using Theano. \nContact: Razvan Pascanu (r.pascanu@gmail...)\nLicense: 3-clause BSD\n\n\n\uff08 GitHub \uff09yingzha/natgrad forked from pascanur/natgrad\n\n\nNatural Gradient implementation in Theano\nContact: Razvan Pascanu (r.pascanu@gmail...)\nLicense: 3-clause BSD\n\n\n\n\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5\u3068\u306f \uff1f\n\n\n\u6731\u9df2\u306e\u675cWiki \u81ea\u7136\u52fe\u914d\n\n\n\n\u7518\u5229 \u4fca\u4e00 \u300c\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5-\u5b66\u7fd2\u7a7a\u9593\u306e\u5e7e\u4f55\u5b66\u300d\n\n\n\nShun-ichi Amari, Natural Gradient Works Efficiently in Learning\n\n\n\nAbstract\nWhen a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction but the natural gradient does. \nInformation geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation) and the space of linear dynamical systems (for blind source deconvolution). \nThe dynamical behavior of natural gradient on-line learning is analyzed and is proved to be Fisher ecient, implying that it\nhas asymptotically the same performance as the optimal batch estimation of parameters.\nThis suggests that the plateau phenomenon which appears in the back propagation learning algorithm of multilayer perceptrons might disappear or might be not so serious when the natural gradient is used. \nAn adaptive method of updating the learning rate is proposed\nand analyzed.\n\n\n\n\u306a\u305c\u3001\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d\u306e\u5b9f\u88c5\u3001\u5229\u7528 \u306f \u5e83\u7bc4\u56f2\u306e\u30d7\u30ed\u30b0\u30e9\u30de\u307e\u3067\u666e\u53ca\u3057\u5f97\u3066\u3044\u306a\u3044\u306e\u304b\uff1f\n\n\nreddit MACHINE LEARNING Why is the natural gradient not used more in machine learning?\n\n\n\uff08 \u56de\u7b54\u4f8b \uff09\n\nFor deep learning models, people were using Hessian Free optimization for a while.\nWith the approximations people were using, Hessian Free turns out to be equivalent to natural gradient.\nBut eventually people quit using Hessian Free / Natural Gradient because Stochastic Gradient Descent was usually more efficient.\nIt's a lot easier to change your model family (e.g., use rectified linear units/maxout rather than sigmoids, or use an LSTM instead of a traditional RNN) to make SGD work well than to use a difficult model family with an expensive optimization method.\nNatural gradient often converges much faster than SGD in terms of the number of updates it makes.\nBut the cost in wall time per update is high enough to cancel out that effect.\n\n\n\u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u306f\u3001\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d\u3068 \u7b49\u4fa1\nSGD\uff08\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\uff09\u3088\u308a\u3082\u3001\u81ea\u7136\u52fe\u914d\u6cd5\u306e\u65b9\u304c\u8aa4\u5dee\u304c\u5168\u4f53\u6700\u5c0f\u5024\u306b\u53ce\u675f\u3059\u308b\u307e\u3067\u306b\u8981\u3059\u308b\u7e70\u308a\u8fd4\u3057\u8a08\u7b97\u56de\u6570\uff08 iteration\u56de\u6570 \uff09\u306f\u5c11\u306a\u304f\u3066\u6e08\u3080 \u304c\u3001\nRNN \u3092 LSTM \u306b \u5909\u3048\u305f\u308a\u3001\u6d3b\u6027\u5316\u95a2\u6570 \u3092 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 \u304b\u3089 RELU\u95a2\u6570 \u3084 maxout\u95a2\u6570 \u306b \u5909\u3048\u305f\u65b9 \u304c DNN\u30e2\u30c7\u30eb\u5168\u4f53\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u6539\u5584\u3055\u305b\u308b\u624b\u9593\uff08\u30b3\u30b9\u30c8\uff09\u304c\u5c11\u306a\u304f\u3066\u6e08\u3080 \u304b\u3089\n\u591a\u304f\u306e\u4eba\u3005\u306f\u3001\u3053\u3046\u3057\u305f\u30e2\u30c7\u30eb\u306e\u6539\u826f\u3092\u884c\u3063\u305f\u4e0a\u3067\u3001SGD\u6cd5 \u3067 \u30e2\u30c7\u30eb\u5b66\u7fd2\u3055\u305b\u3066\u3044\u308b\n\n\u3068 \u3057\u3066\u3044\u308b\u3002\n\u4e0a\u8a18 reddit \u306e\u56de\u7b54\u8005\u306e\u4e2d\u3067\u3001\u81ea\u7136\u52fe\u914d\u6cd5\u306e\u5229\u7528\u4e8b\u4f8b\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u8ad6\u6587\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002\n\nDaniel Povey, Xiaohui Zhang, Sanjeev Khudanpur\n, Parallel training of DNNs with Natural Gradient and Parameter Averaging\n\n\nWe describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines.\nIn order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. >\nOur method is to average the neural network parameters periodically (typically every minute or two), and __redistribute the averaged parameters to the machines for further training. Each machine sees different data_.\nBy itself, this method does not work very well. \nHowever, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.\n\n\n\n\u3088\u308a \u201d\u8907\u96d1\u306a\u201d ANN\u30e2\u30c7\u30eb \u3092 \u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5 \u3067\u6271\u3046\u305f\u3081 \u306e \u63d0\u6848\n\n\nRelative Natural Gradient for Learning Large Complex Models by Ke Sun and Frank Nielsen\n\n\n\uff08 Python \u30b3\u30fc\u30c9 \uff09\n\nhttps://www.lix.polytechnique.fr/~nielsen/RFIM/PRNGD.zip\n\n\uff08 \u8ad6\u6587 \uff09\n\nKe Sun and Frank Nielsen, Relative Natural Gradient for Learning Large Complex Models\n\n\nAbstract\nFisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. \nHowever related analysis becomes more and more difficult as the\nlearner\u2019s structure turns large and complex.\nThis paper makes a preliminary step towards a new direction. \nWe extract a local component of a large neuron system, and defines its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system.\nThis concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. \nWe provide an analysis on a list of f commonly used components, and demonstrate how to use this concept to further improve optimization.\n\n\n\n\uff08 \u6bd4\u8f03 \uff09\u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u7b49 \u3068 \u300c\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5\u300d\n\n\n\n\u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u3068\u306f\n\n\uff08 \u30d8\u30c3\u30bb\u884c\u5217\u3001\u30d8\u30b7\u30a2\u30f3\u884c\u5217 \uff09\n\nWikipedia \u300c\u30d8\u30c3\u30bb\u884c\u5217\u300d\n\n\n\u6570\u5b66\u306b\u304a\u3051\u308b\u30d8\u30c3\u30bb\u884c\u5217\uff08\u30d8\u30c3\u30bb-\u304e\u3087\u3046\u308c\u3064\u3001\u82f1: Hessian matrix\uff09\u306f\u3001\u591a\u5909\u6570\u30b9\u30ab\u30e9\u30fc\u5024\u95a2\u6570\u306e\u4e8c\u968e\u504f\u5c0e\u95a2\u6570\u5168\u4f53\u304c\u4f5c\u308b\u6b63\u65b9\u884c\u5217\u3067\u3042\u308b\u3002\u5b9f\u6570\u5024\u95a2\u6570\u306e\u6975\u5024\u5224\u5b9a\u306b\u7528\u3044\u3089\u308c\u308b\u3002\u30d8\u30c3\u30bb\u884c\u5217\u306f\u3001\u30b8\u30a7\u30fc\u30e0\u30b9\u30fb\u30b8\u30e7\u30bb\u30d5\u30fb\u30b7\u30eb\u30d9\u30b9\u30bf\u30fc\u304c\u3001\u30c9\u30a4\u30c4\u306e\u6570\u5b66\u8005\u30eb\u30fc\u30c8\u30f4\u30a3\u30d2\u30fb\u30aa\u30c3\u30c8\u30fc\u30fb\u30d8\u30c3\u30bb\u306b\u7531\u6765\u3057\u3066\u540d\u3065\u3051\u305f\u3002\n\n\n\u95a2\u6570\u306e\u52fe\u914d\u3068\u30d8\u30c3\u30bb\u884c\u5217\n\uff08SlideShare\uff09KOTARO SETOYAMA, Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks\nHatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc\uff082014-02-08\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u300f\n\n\n\"Hessian-Free\"\u3068\u3044\u3046\u65b0\u3057\u3044\u6700\u9069\u5316\u624b\u6cd5\u3092Deep Learning\u306eauto-encoder\u306e\u5b66\u7fd2\u306b\u4f7f\u3063\u3066\u307f\u305f\u3089\u4e8b\u524d\u5b66\u7fd2\u306a\u3057\u3067\u65e2\u5b58\u306e\u5831\u544a\u306e\u6027\u80fd\u3092\u8d85\u3048\u305f\u305e\u51c4\u3044\u3060\u308d\u3046\u3001\u3068\u3044\u3046\u8a71\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6c7a\u5b9a\u306f\u3088\u304f\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u554f\u984c\u3067\u3001\u52fe\u914d\u6cd5\u3067\u52b9\u7387\u3088\u304f\u8a08\u7b97\u3067\u304d\u308b\u3068\u8a00\u308f\u308c\u3066\u3044\u308b\u3002\n\u3057\u304b\u3057Deep Learning\u306e\u3088\u3046\u306b\u96a0\u308c\u5c64\u304c\u3068\u3066\u3082\u591a\u3044\u30b1\u30fc\u30b9\u3067\u306f\u3046\u307e\u304f\u3044\u304b\u306a\u3044\u3002\u5b66\u7fd2\u306b\u3068\u3066\u3082\u6642\u9593\u304c\u304b\u304b\u3063\u305f\u308a\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3055\u3048\u9177\u3044\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3057\u304b\u51fa\u305b\u306a\u304b\u3063\u305f\u308a\u3059\u308b(under-fitting)\u3002\n\u6700\u9069\u5316\u306b\u95a2\u3059\u308b\u7814\u7a76\u8005\u306e\u9593\u3067\u306f\u52fe\u914d\u6cd5\u304c\u75c5\u7684\u306a\u66f2\u7387\u3092\u6301\u3063\u305f\u76ee\u7684\u95a2\u6570\u306b\u5bfe\u3057\u3066\u306f\u4e0d\u5b89\u5b9a\u3067\u3042\u308b\u3053\u3068\u304c\u3088\u304f\u77e5\u3089\u308c\u3066\u3044\u308b\u30022\u6b21\u306e\u6700\u9069\u5316\u6cd5\u306f\u3053\u306e\u3088\u3046\u306a\u76ee\u7684\u95a2\u6570\u306b\u5bfe\u3057\u3066\u3082\u3046\u307e\u304f\u50cd\u304f\u3002\u3060\u304b\u3089Deep Learning\u306b\u3082\u3053\u306e\u7a2e\u306e\u6700\u9069\u5316\u3092\u4f7f\u3063\u305f\u3089\u3044\u3044\u3093\u3058\u3083\u306a\u3044\u304b\u3002\n\u3067\u3082\u307e\u3060\u3044\u304f\u3064\u304b\u554f\u984c\u304c\u3042\u308b\u3002\u307e\u305a\u3067\u304b\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u73fe\u5b9f\u7684\u306a\u901f\u5ea6\u3067\u52d5\u304f\u3088\u3046\u306b\u3059\u308b\u3053\u3068\u3002\u3053\u308c\u306f\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5(SGD)\u307f\u305f\u3044\u306a\u611f\u3058\u3067\u30aa\u30f3\u30e9\u30a4\u30f3\u5b66\u7fd2\u306b\u3057\u305f\u3089\u3044\u3044\u3002\n\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306bpre-training\u3059\u308b\u3063\u3066\u306e\u3082\u5927\u304d\u306a\u9032\u6b69\u306e\u4e00\u3064\u3060\u3002\u3053\u308c\u3092\u3084\u3063\u3066\u304b\u3089SGD\u3059\u308b\u3068\u554f\u984c\u3092\u56de\u907f\u3067\u304d\u308b\u3088\u3046\u306b\u307f\u3048\u308b\u3002\u5b9f\u969b\u3001\u6210\u529f\u3057\u3066\u3044\u308b\u5fdc\u7528\u4f8b\u3082\u8272\u3005\u3042\u308b\u3002\u3057\u304b\u3057\u7591\u554f\u304c\u6b8b\u308b\uff1a\u306a\u3093\u3067\u3053\u308c\u3067\u4e0a\u624b\u304f\u3044\u304f\u306e\uff1f\u306a\u3093\u3067\u5fc5\u8981\u306a\u306e\uff1f\u30ed\u30fc\u30ab\u30eb\u30df\u30cb\u30de\u30e0\u304c\u5927\u91cf\u306b\u3042\u308b\u3053\u3068\u304c\u539f\u56e0\u3060\u3068\u3044\u3046\u8aac\u660e\u3092\u3057\u3066\u3044\u308b\u7814\u7a76\u8005\u3082\u3044\u308b\u3002\n\u5225\u306e\u8aac\u660e\u3068\u3057\u3066\u3001\u76ee\u7684\u95a2\u6570\u304c\u75c5\u7684\u306a\u66f2\u7387\uff08\u4f8b\u3048\u3070\u7d30\u9577\u3044\u8c37\u3068\u304b\uff09\u3092\u6301\u3063\u3066\u3044\u3066\u3001\u52fe\u914d\u6cd5\u306e\u3088\u3046\u306a\u66f2\u7387\u3092\u898b\u306a\u3044\u6700\u9069\u5316\u6cd5\u3067\u306f\u6700\u9069\u5316\u4e0d\u53ef\u80fd\u3060\u3001\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308b\u3002\u305d\u3053\u3067\u3053\u306e\u8996\u70b9\u306b\u7acb\u3063\u3066\u3001\u30bb\u30df\u30aa\u30f3\u30e9\u30a4\u30f3\u306e2\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b\u3002\u3053\u308c\u3092\u4f7f\u3048\u3070under-fitting\u3057\u306a\u3044\u3057\u65e2\u5b58\u306epre-training\u3092\u4f7f\u3063\u305f\u65b9\u6cd5\u3088\u308a\u52b9\u7387\u7684\u3002\n\u307e\u305a\u6a19\u6e96\u7684\u306a2\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3067\u3042\u308b\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306b\u3064\u3044\u3066\u89e3\u8aac\u3059\u308b\u3002\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u500b\u6570\u306e2\u4e57\u306e\u30b5\u30a4\u30ba\u306e\u30d8\u30b7\u30a2\u30f3\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u3067\u304b\u3044\u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u306b\u306f\u73fe\u5b9f\u7684\u3067\u306f\u306a\u3044\u3002\u304c\u3001\u3053\u308c\u3092\u5b66\u3076\u3053\u3068\u3067\u3001\u3082\u3063\u3068\u73fe\u5b9f\u7684\u306a\u4e9c\u7a2e(quasi-Newton methods\u3068\u306f)\u304c\u3069\u3046\u632f\u308b\u821e\u3046\u306e\u304b\u306b\u3064\u3044\u3066\u7406\u89e3\u3067\u304d\u308b\u3002\n\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\u52fe\u914d\u6cd5\u540c\u69d8\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u9010\u6b21\u7684\u306b\u66f4\u65b0\u3057\u3066\u3044\u304f\u624b\u6cd5\u3002\u30b3\u30a2\u306e\u30a2\u30a4\u30c7\u30a2\u306f\u300c\u76ee\u7684\u95a2\u6570f\u3092\u4e8c\u6b21\u95a2\u6570\u3067\u8fd1\u4f3c\u300d(\u6570\u5f0f1) B\u306f\u30d8\u30b7\u30a2\u30f3\u3060\u3068\u601d\u3063\u3066\u3044\u3044\u3093\u3060\u3051\u3069\u3001\u6b63\u5b9a\u5024\u3067\u306a\u3044\u3053\u3068\u304c\u305f\u307e\u306b\u3042\u3063\u3066\u6700\u5c0f\u5024\u3092\u6301\u305f\u306a\u304f\u3066\u56f0\u308b\u306e\u3067\u9069\u5f53\u306b\u5bfe\u89d2\u6210\u5206\u3092\u5897\u3084\u3057\u3066\u6b63\u5b9a\u5024\u306b\u3059\u308b\n\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\"scale invariance\"\u304c\u91cd\u8981\u306a\u7279\u5fb4\u3002\u7dda\u5f62\u306a\u5909\u63db\u3067\u306f\u632f\u308b\u821e\u3044\u304c\u5909\u308f\u3089\u306a\u3044\u3002\u3053\u308c\u304c\u306a\u3044\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u60aa\u3044\u3068\u6027\u80fd\u304c\u60aa\u5316\u3057\u3066\u3057\u307e\u3046\u3002\u307e\u305f\u3053\u306e\u7279\u5fb4\u306b\u3088\u3063\u3066\u5b66\u7fd2\u7387\u3092\u8abf\u6574\u3057\u305f\u308a\u3068\u304b\u3057\u306a\u304f\u3066\u3088\u304f\u306a\u308b\u3002\n\u9006\u306b\u8a00\u3048\u3070\u73fe\u5728\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5468\u8fba\u306e\u66f2\u7387\u3092\u5143\u306b\u6697\u9ed9\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u6c7a\u307e\u3063\u3066\u3044\u3066\u3001\u305d\u308c\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u5909\u63db\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u8003\u3048\u65b9\u304c\u3067\u304d\u308b\u3002\n\u66f2\u7387\u306e\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u306a\u3089\u52fe\u914d\u306e\u5909\u5316\u306f\u5c11\u306a\u3044\u306e\u3067\u3001\u9577\u3044\u8ddd\u96e2\u9032\u3093\u3067\u3082\u826f\u3044\u3063\u3066\u3053\u3068\u3002\u9006\u306b\u5927\u304d\u3044\u306a\u3089\u3001p\u306e\u65b9\u5411\u306b\u30bb\u30f3\u30b7\u30c6\u30a3\u30d6\u3063\u3066\u3053\u3068\u3060\u304b\u3089\u3061\u3087\u3063\u3068\u3060\u3051\u9032\u3093\u3067\u9069\u5207\u306ap\u306e\u65b9\u5411\u3092\u518d\u78ba\u8a8d\u3059\u3079\u304d\u3002\uff08\u3053\u306e\u6bb5\u843d\u3001\u3042\u3068\u3067\u6570\u5f0f\u3092\u518d\u78ba\u8a8d\u3059\u308b\uff09\n\n\nHatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc\uff082014-02-09\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u305d\u306e2\u300f\n\n\n\"Hessian-Free\"\u3068\u3044\u3046\u65b0\u3057\u3044\u6700\u9069\u5316\u624b\u6cd5\u3092Deep Learning\u306eauto-encoder\u306e\u5b66\u7fd2\u306b\u4f7f\u3063\u3066\u307f\u305f\u3089\u4e8b\u524d\u5b66\u7fd2\u306a\u3057\u3067\u65e2\u5b58\u306e\u5831\u544a\u306e\u6027\u80fd\u3092\u8d85\u3048\u305f\u305e\u51c4\u3044\u3060\u308d\u3046\u3001\u3068\u3044\u3046\u8a71\u3002\nDeep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u306e\u7d9a\u304d\u3002\n\u75c5\u7684\u306a\u52fe\u914d\u306e\u4f8b\u3068\u3001\u305d\u308c\u306b\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u304c\u5411\u3044\u3066\u3044\u308b\u3053\u3068\u306e\u8aac\u660e\u3002\n\u305f\u3068\u3048\u3070\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3042\u308b\u540c\u3058\u5c64\u306e2\u3064\u306e\u30cb\u30e5\u30fc\u30ed\u30f3a, b\u306b\u3064\u3044\u3066\u5165\u51fa\u529b\u306e\u91cd\u307f\u304c\u307b\u307c\u540c\u3058\u3068\u3059\u308b\u3002a\u306e\u3042\u308b\u91cd\u307fi\u3092\u4e0a\u3052\u3066\u3001b\u306e\u5bfe\u5fdc\u3059\u308b\u91cd\u307fj\u3092\u4e0b\u3052\u308b\u3088\u3046\u306a\u65b9\u5411d\u3078\u306e\u66f4\u65b0\u3092\u8003\u3048\u308b\u3002\u3053\u306e\u6642d\u65b9\u5411\u306e\u52fe\u914d\u306f\u307b\u3068\u3093\u30690\u3002\u66f2\u7387\u3082\u307b\u3068\u3093\u30690\u3002\u3060\u304b\u3089\u52fe\u914d\u6cd5\u307f\u305f\u3044\u306a1\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3067\u306f\u307b\u3068\u3093\u3069\u66f4\u65b0\u3055\u308c\u306a\u3044\u3002\n\u4e00\u65b9\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u307f\u305f\u3044\u306a2\u6b21\u306e\u65b9\u6cd5\u306a\u3089\u5206\u6bcd\u306b\u66f2\u7387\u304c\u6765\u308b\u304b\u3089\u3082\u3063\u3068\u901f\u304f\u66f4\u65b0\u3055\u308c\u308b\u3002\n\u672c\u984c\uff1a\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u306b\u3064\u3044\u3066\u3002\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc(HF)\u306f\n\n\n\u30d8\u30b7\u30a2\u30f3H\u3092\u8a08\u7b97\u3057\u306a\u3044(\u4ed6\u306e\u591a\u304f\u306equasi-Newton\u3068\u9055\u3063\u3066)\n\n\u4ee3\u308f\u308a\u306bHd\u3092\u6709\u9650\u5dee\u5206\u6cd5\u3067\u76f4\u63a5\u6c42\u3081\u308b\uff08\u3088\u3046\u3059\u308b\u306b2\u56de\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u5f15\u304f\uff09\n\n\n\u76ee\u7684\u95a2\u6570\u3067\u306f\u306a\u304f\u305d\u308c\u306e\u4e8c\u6b21\u8fd1\u4f3cq\u03b8\u306b\u5bfe\u3057\u3066Linear Conjugate Gradient\u3067\u89e3\u304f\u3002\nN\u6b21\u5143\u306a\u3089\u6700\u5927N\u30b9\u30c6\u30c3\u30d7\u304b\u304b\u308b\u304c\u3001\u9069\u5f53\u306b\u6253\u3061\u5207\u308b\u3002\n\u4e00\u822c\u7684\u306b\u5171\u5f79\u52fe\u914d\u6cd5\u3068\u8a00\u308f\u308c\u308bNon-linear Conjugate Gradient\u3068\u9055\u3063\u3066\u3001\u66f2\u7387\u306b\u95a2\u3059\u308b\u518d\u8a08\u7b97\u304c\u5fc5\u8981\u306a\u3044\u3002\u3053\u308c\u306f\u76ee\u7684\u95a2\u6570\u3092\u66f2\u7387\u4e00\u5b9a\u306e\u66f2\u9762\u3067\u6700\u521d\u306b\u8fd1\u4f3c\u3057\u3066\u308b\u304b\u3089\u3002\u8981\u3059\u308b\u306b\u4e8c\u6b21\u8fd1\u4f3c\u3057\u3066\u308b\u304b\u3089\u4e8c\u968e\u5fae\u5206\u306f\u5b9a\u6570\u3067H\u304c\u5909\u308f\u3089\u306a\u3044\u3063\u3066\u3053\u3068\u3067Hd\u3082\u5909\u308f\u3089\u306a\u3044\u304b\u3089\u518d\u8a08\u7b97\u304c\u3044\u3089\u306a\u3044\u3002\n\n\u611f\u60f3\uff1a\u5171\u5f79\u52fe\u914d\u6cd5\u3001\u76ee\u7684\u95a2\u6570\u304c2\u6b21\u306e\u6642\u306b\u306f2\u56de\u3067\u53ce\u675f\u3059\u308b\u3068\u3044\u3046\u8aa4\u89e3\u3092\u3057\u3066\u3044\u305f\u3051\u3069\u3082\u3001\u305d\u308c\u306f\u300c\u6b21\u5143\u304c2\u6b21\u5143\u3060\u304b\u30892\u56de\u3067\u53ce\u675f\u3059\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u3060\u3063\u305f\u3088\u3046\u3060\u3002\u8981\u3059\u308b\u306bN\u6b21\u5143\u306a\u3089\u6700\u5927N\u672c\u306e\u5171\u5f79\u306a\u65b9\u5411\u304c\u9078\u3070\u308c\u3066\u3001\u305d\u3063\u3061\u65b9\u5411\u306b\u3069\u3093\u3060\u3051\u9032\u3081\u3070\u3044\u3044\u304b\u306f\u76ee\u7684\u95a2\u6570\u304c\u4e8c\u6b21\u306a\u3089\u52fe\u914d\u3092\u66f2\u7387\u3067\u5272\u308b\u3053\u3068\u3067\u4e00\u767a\u3067\u6b63\u78ba\u306b\u6c42\u3081\u3089\u308c\u308b\u3002\u3060\u304b\u3089\u6700\u5927N\u56de\u3002\u300c\u66f2\u7387\u3067\u5272\u308b\u300d\u304c\u4ed6\u306e\u65b9\u6cd5\u3060\u3068NxN\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u306b\u306a\u3063\u3066\u6b7b\u306d\u308b\u3051\u3069Hessian-Free\u3067\u306fHd\u3092\u5dee\u5206\u6cd5\u3067\u6c42\u3081\u3061\u3083\u3046\u304b\u3089\u5927\u5909\u3058\u3083\u306a\u3044\u3001\u3068\u3002\u9762\u767d\u304f\u306a\u3063\u3066\u307e\u3044\u308a\u307e\u3057\u305f\u3002\u6b21\u306f4\u7ae0\u304b\u3089\u3002\n\u51fa\u5178\nhttp://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf\n\n\u4e0a \u3067 \u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587 \u306f \u4ee5\u4e0b\n\nJames Martens\uff08University of Toronto\uff09, Deep learning via Hessian-free optimization\n\n\u30c8\u30ed\u30f3\u30c8\u5927\u5b66\u5f37\u3044\u306a\n\n\nAbstract\nWe develop a 2nd-order optimization method based on the \u201cHessian-free\u201d approach, and apply it to training deep auto-encoders. \nWithout using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. \nOur method is practical, easy to use, scales nicely to very large\ndatasets, and isn\u2019t limited in applicability to autoencoders,\nor any specific model class. \nWe also discuss the issue of \u201cpathological curvature\u201d as a possible explanation for the difficulty of deep learning and how 2nd-order optimization, and our method in particular, effectively deals with it.\n\n\nHatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc \uff082014-02-10\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u305d\u306e3\u300f\n\n\n\u6b8b\u5ff5\u306a\u304a\u77e5\u3089\u305b\u3067\u3059\u304c\u3001\u3053\u306e\u9023\u8f09\u306f\u7d9a\u304d\u307e\u305b\u3093\u3002\n\u305d\u3082\u305d\u3082\u50d5\u306e\u8208\u5473\u306fword2vec\u306b\u3088\u308b\u610f\u5473\u306e\u7406\u89e3\u3068\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u7ae0\u306e\u751f\u6210\u3060\u3063\u305f\u308f\u3051\u3067\u3059\u3002\u5f8c\u8005\u306e\u8ad6\u6587\u306fRNNLM\u3092\u4f7f\u3063\u3066\u3044\u3066\u3001\u305d\u308c\u306f\u524d\u8005\u306e\u8ad6\u6587\u306e\u8457\u8005\u304cword2vec\u306e\u524d\u306b\u3084\u3063\u3066\u3044\u305f\u3053\u3068\u3067\u3059\u3002\n\u3068\u3044\u3046\u308f\u3051\u3067RNNLM\u306b\u3064\u3044\u3066\u8abf\u3079\u59cb\u3081\u3001\u305d\u306e\u6700\u9069\u5316\u304c\u4ed6\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3088\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u3088\u3046\u306a\u78ba\u7387\u7684\u52fe\u914d\u6cd5\u3067\u306f\u96e3\u3057\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u306b\u3064\u3044\u3066\u8abf\u3079\u3066\u3044\u305f\u308f\u3051\u3067\u3059\u3002\n\u524d\u56de\u8aac\u660e\u3057\u305f\u3088\u3046\u306b\u3001\u305f\u3057\u304b\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u91cd\u307f\u304c\u307b\u3068\u3093\u3069\u540c\u4e00\u3067\u3042\u308b\u30cb\u30e5\u30fc\u30ed\u30f3\u304c\u3042\u3063\u305f\u5834\u5408\u306b\u3001\u305d\u306e\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u5206\u5316\u3055\u305b\u308b\u65b9\u5411\u306e\u52fe\u914d\u304c\u5c0f\u3055\u304f\u30661\u6b21\u306e\u52fe\u914d\u6cd5\u3067\u306f\u5b66\u7fd2\u304c\u306f\u304b\u3069\u3089\u306a\u3044\u3068\u3044\u3046\u70b9\u306f\u7406\u89e3\u3067\u304d\u307e\u3059\u3002\n\u3067\u3082\u300c\u3060\u304b\u30892\u6b21\u306e\u6700\u9069\u5316\u6cd5\u3092\u4f7f\u3046\u300d\u3068\u3044\u3046\u7d50\u8ad6\u306b\u3042\u307e\u308a\u540c\u610f\u3067\u304d\u307e\u305b\u3093\u3002\n\u500b\u4eba\u7684\u306b\u306f\u300c\u30b9\u30d1\u30fc\u30b9\u5316\u306e\u5236\u7d04\u3092\u304b\u3051\u305f\u3089\u305d\u306e\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u3069\u3063\u3061\u304b\u3057\u304b\u767a\u706b\u3067\u304d\u306a\u3044\u95a2\u4fc2\u4e0a\u3001\u5206\u5316\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u306b\u8d77\u3053\u308b\u3067\u3057\u3087\u300d\u3068\u601d\u3046\u308f\u3051\u3067\u3059\u3002\n\u6700\u7d42\u7684\u306b\u30b9\u30d1\u30fc\u30b9\u3067\u306a\u3044\u3082\u306e\u304c\u6b32\u3057\u3044\u306e\u3060\u3068\u3057\u3066\u3082\u3001\u6700\u521d\u306f\u30b9\u30d1\u30fc\u30b9\u5236\u7d04\u3092\u304b\u3051\u3066\u300c\u3088\u3044\u521d\u671f\u5024\u300d\u3092\u63a2\u3057\u305f\u3089\u3044\u3044\u3093\u3058\u3083\u306a\u3044\u306e\u3002\n\u300c\u305d\u308c\u3067\u3088\u3044\u300d\u3084\u300c\u305d\u3063\u3061\u306e\u65b9\u304c\u826f\u3044\u300d\u306f\u4eee\u8aac\u306b\u3059\u304e\u306a\u3044\u306e\u3067\u3001\u305d\u306e\u4eee\u8aac\u304c\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3057\u3066\u307f\u308b\u306e\u3082\u4e00\u8208\u3060\u3068\u306f\u601d\u3046\u306e\u3060\u3051\u3069\u3001\u5b9f\u306f\u3042\u3093\u307e\u308a\u8208\u5473\u304c\u6301\u3066\u307e\u305b\u3093\u3002\n\u3082\u3046\u4e00\u70b9\u3001word2vec\u306b\u55b0\u308f\u305b\u308b\u305f\u3081\u306b\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\u3092\u4f5c\u308b\u3068\u3053\u308d\u3067\u3001\u5f62\u614b\u7d20\u89e3\u6790\u3092\u4f7f\u3044\u305f\u304f\u306a\u3044\u3002\n\u305d\u3053\u3067RNNLM\u304b\u3089\u300c\u5358\u8a9e\u300d\u3092\u767a\u898b\u3059\u308b\u7814\u7a76\u304c\u306a\u3044\u304b\u306a\u3068\u8abf\u3079\u3066\u3044\u305f\u3068\u3053\u308d\u3001\u3080\u3057\u308d\u300c\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u7bc0\u304c\u6700\u5c0f\u9650\u3067 deep learning \u306b\u3088\u308b\u7d50\u679c\u3068\u307b\u307c\u540c\u7b49\u306e\u7cbe\u5ea6\u304c\u5f97\u3089\u308c\u308b\u624b\u6cd5\u300d\u304c\u3042\u308b\u3068\u3044\u3046\u8a18\u8ff0\u3092\u767a\u898b\uff08SIGNL 213: \u73fe\u5b9f\u7684\u306a\u5f62\u614b\u7d20\u89e3\u6790\u5668\u306e\u5165\u529b\u2192\u300c\u305a\u3082\u3082\u3082\u3082\u307a\u308d\u307a\u308d\u307a\u308d\u307a\u308d\u30de\u30df\u30bf\u30b9\u30de\u30df\u30bf\u30b9\u30e9\u30d6\u30de\u30df\u30bf\u30b9\u300d - \u6b66\u8535\u91ce\u65e5\u8a18\uff09\n\u3068\u3044\u3046\u308f\u3051\u3067\u95a2\u5fc3\u304c\u3053\u3061\u3089\u306e\u8ad6\u6587\u306b\u79fb\u3063\u305f\u306e\u3067\u9023\u8f09\u306f\u7d42\u4e86\u3067\u3059\uff1a\nhttp://chasen.org/~daiti-m/paper/nl190segment.pdf\n\n\n\n\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d \u3068\u306e \u6bd4\u8f03\n\n\u5192\u982d \u3067 \u793a\u3057\u305f Theano\u5b9f\u88c5\u30b3\u30fc\u30c9 \u3092 \u63d0\u793a\u3057\u305f Pascanu\u6c0f\uff08\u30e2\u30f3\u30c8\u30ea\u30aa\u30fc\u30eb\u5927\u5b66\uff09 \u3068\u3001\u304a\u306a\u3058\u307f Yoshua Bengio\u6c0f \u3068\u306e\u5171\u540c\u57f7\u7b46\u8ad6\u6587\n\nRazvan Pascanu and Yoshua Bengio, Revisiting natural gradient for deep networks\n\n\n\nAbstract\nWe evaluate natural gradient descent, an algorithm originally proposed in Amari(1997), for learning deep models. \nThe contributions of this paper are as follows.\nWe show the connection between natural gradient descent and three other recently proposed methods for training deep models: Hessian-Free Optimization (Martens,2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). \nWe describe how one can use unlabeled data to improve the\ngeneralization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent.\nFinally we extend natural gradient descent to incorporate second\norder information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.\n\n##__Theano \u5b9f\u88c5\u30b3\u30fc\u30c9__\n\n* [\uff08 GitHub \uff09pascanur/natgrad _Natural Gradient implementation in Theano_](https://github.com/pascanur/natgrad)\n\n> Implementation of natural gradient in python using Theano. \n>\n>Contact: Razvan Pascanu (r.pascanu@gmail...)\n>License: 3-clause BSD\n\n* [\uff08 GitHub \uff09yingzha/natgrad _forked from pascanur/natgrad_](https://github.com/yingzha/natgrad)\n\n> Natural Gradient implementation in Theano\n>\n> Contact: Razvan Pascanu (r.pascanu@gmail...)\n> License: 3-clause BSD\n\n___\n\n##__\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5\u3068\u306f \uff1f__\n\n* [\u6731\u9df2\u306e\u675cWiki \u81ea\u7136\u52fe\u914d](http://ibisforest.org/index.php?%E8%87%AA%E7%84%B6%E5%8B%BE%E9%85%8D)\n\n<img width=\"1161\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-06 23.49.40.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/c21cb063-e49c-1483-530b-05c6fa1352b3.png\">\n\n* [\u7518\u5229 \u4fca\u4e00 \u300c\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5-\u5b66\u7fd2\u7a7a\u9593\u306e\u5e7e\u4f55\u5b66\u300d](https://www.jstage.jst.go.jp/article/sicejl1962/40/10/40_10_735/_pdf)\n\n<img width=\"1156\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-07 0.47.13.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/391dbf6e-fcba-2c2f-c3d6-dd201fb6d358.png\">\n\n* [Shun-ichi Amari, _Natural Gradient Works Effi\u000eciently in Learning_](https://personalrobotics.ri.cmu.edu/files/courses/papers/Amari1998a.pdf)\n\n<img width=\"1160\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-06 23.39.09.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/9c536c20-2397-5203-75ff-2fade098f077.png\">\n\n> __Abstract__\n>\n> When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction but the natural gradient does. \n>\n> Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation) and the space of linear dynamical systems (for blind source deconvolution). \n>\n> The dynamical behavior of natural gradient on-line learning is analyzed and is proved to be Fisher e\u000ecient, implying that it\nhas asymptotically the same performance as the optimal batch estimation of parameters.\n>\n> This suggests that the plateau phenomenon which appears in the back propagation learning algorithm of multilayer perceptrons might disappear or might be not so serious when the natural gradient is used. \n>\n> An adaptive method of updating the learning rate is proposed\nand analyzed.\n\n___\n\n###__\u306a\u305c\u3001\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d\u306e\u5b9f\u88c5\u3001\u5229\u7528 \u306f \u5e83\u7bc4\u56f2\u306e\u30d7\u30ed\u30b0\u30e9\u30de\u307e\u3067\u666e\u53ca\u3057\u5f97\u3066\u3044\u306a\u3044\u306e\u304b\uff1f__\n\n* [reddit MACHINE LEARNING _Why is the natural gradient not used more in machine learning?_](https://www.reddit.com/r/MachineLearning/comments/2qpf9x/why_is_the_natural_gradient_not_used_more_in/)\n\n<img width=\"858\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-06 23.53.16.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7f28282a-5783-defb-3a3d-028e833ca514.png\">\n\n__\uff08 \u56de\u7b54\u4f8b \uff09__\n\n> For deep learning models, **people were using Hessian Free optimization for a while.**\n>\n> With the approximations people were using, __*Hessian Free turns out to be equivalent to natural gradient*.__\n>\n> __But eventually people quit using Hessian Free / Natural Gradient because Stochastic Gradient Descent was usually more efficient.__\n>\n> It's **a lot easier to change your model family (e.g., use rectified linear units/maxout rather than sigmoids, or use an LSTM instead of a traditional RNN) to make SGD work well than to use a difficult model family with an expensive optimization method**.\n>\n> _Natural gradient often converges much faster than SGD in terms of the number of updates it makes._\n>\n> __But the cost in wall time per update is high enough to cancel out that effect.__\n\n* \u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u306f\u3001\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d\u3068 \u7b49\u4fa1\n* SGD\uff08\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\uff09\u3088\u308a\u3082\u3001\u81ea\u7136\u52fe\u914d\u6cd5\u306e\u65b9\u304c\u8aa4\u5dee\u304c\u5168\u4f53\u6700\u5c0f\u5024\u306b\u53ce\u675f\u3059\u308b\u307e\u3067\u306b\u8981\u3059\u308b\u7e70\u308a\u8fd4\u3057\u8a08\u7b97\u56de\u6570\uff08 iteration\u56de\u6570 \uff09\u306f\u5c11\u306a\u304f\u3066\u6e08\u3080 \u304c\u3001\n* RNN \u3092 LSTM \u306b \u5909\u3048\u305f\u308a\u3001\u6d3b\u6027\u5316\u95a2\u6570 \u3092 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 \u304b\u3089 RELU\u95a2\u6570 \u3084 maxout\u95a2\u6570 \u306b \u5909\u3048\u305f\u65b9 \u304c DNN\u30e2\u30c7\u30eb\u5168\u4f53\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u6539\u5584\u3055\u305b\u308b\u624b\u9593\uff08\u30b3\u30b9\u30c8\uff09\u304c\u5c11\u306a\u304f\u3066\u6e08\u3080 \u304b\u3089\n* \u591a\u304f\u306e\u4eba\u3005\u306f\u3001\u3053\u3046\u3057\u305f\u30e2\u30c7\u30eb\u306e\u6539\u826f\u3092\u884c\u3063\u305f\u4e0a\u3067\u3001SGD\u6cd5 \u3067 \u30e2\u30c7\u30eb\u5b66\u7fd2\u3055\u305b\u3066\u3044\u308b\n\n\u3068 \u3057\u3066\u3044\u308b\u3002\n\n__\u4e0a\u8a18 reddit \u306e\u56de\u7b54\u8005\u306e\u4e2d\u3067\u3001\u81ea\u7136\u52fe\u914d\u6cd5\u306e\u5229\u7528\u4e8b\u4f8b\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u8ad6\u6587\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u3002__\n\n* [Daniel Povey, Xiaohui Zhang, Sanjeev Khudanpur\n, _Parallel training of DNNs with Natural Gradient and Parameter Averaging_](https://arxiv.org/abs/1410.7455)\n\n> We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines.\n>\n> In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. >\n> \n> __Our method is to average the neural network parameters periodically (typically every minute or two)___, and __redistribute the averaged parameters to the machines for further training. Each machine sees different data__.\n>\n> By itself, this method does not work very well. \n>\n> However, we have another method, __an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD)__, which _seems to allow our periodic-averaging method to work well_, as well as substantially improving the convergence of SGD on a single machine.\n\n___\n\n###__\u3088\u308a \u201d\u8907\u96d1\u306a\u201d ANN\u30e2\u30c7\u30eb \u3092 \u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5 \u3067\u6271\u3046\u305f\u3081 \u306e \u63d0\u6848__\n\n* [_Relative Natural Gradient for Learning Large Complex Models_ by Ke Sun and Frank Nielsen](https://www.lix.polytechnique.fr/~nielsen/RFIM/)\n\n<img width=\"1158\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-07 0.09.06.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/a93f0036-01ba-05e3-e175-0ea55e558500.png\">\n\n__\uff08 Python \u30b3\u30fc\u30c9 \uff09__\n\n* https://www.lix.polytechnique.fr/~nielsen/RFIM/PRNGD.zip\n\n__\uff08 \u8ad6\u6587 \uff09__\n\n* [Ke Sun and Frank Nielsen, _Relative Natural Gradient for Learning Large Complex Models_](https://arxiv.org/pdf/1606.06069v1.pdf)\n\n> __Abstract__\n>\n> Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. \n>\n> __However related analysis becomes more and more difficult as the\nlearner\u2019s structure turns large and complex.__\n>\n> This paper makes a preliminary step towards a new direction. \n>\n> __We extract a local component of a large neuron system, and defines its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system__.\n>\n> This concept is important because __the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks__. \n>\n> We provide an analysis on a list of f commonly used components, and __demonstrate how to use this concept to further improve optimization.__\n\n___\n\n###__\uff08 \u6bd4\u8f03 \uff09\u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u7b49 \u3068 \u300c\u81ea\u7136\u52fe\u914d\u5b66\u7fd2\u6cd5\u300d__\n\n___\n\n####__\u300c\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u300d\u3068\u306f__\n\n__\uff08 \u30d8\u30c3\u30bb\u884c\u5217\u3001\u30d8\u30b7\u30a2\u30f3\u884c\u5217 \uff09__\n\n* [Wikipedia \u300c\u30d8\u30c3\u30bb\u884c\u5217\u300d](https://ja.wikipedia.org/wiki/%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97)\n\n> \u6570\u5b66\u306b\u304a\u3051\u308b\u30d8\u30c3\u30bb\u884c\u5217\uff08\u30d8\u30c3\u30bb-\u304e\u3087\u3046\u308c\u3064\u3001\u82f1: Hessian matrix\uff09\u306f\u3001\u591a\u5909\u6570\u30b9\u30ab\u30e9\u30fc\u5024\u95a2\u6570\u306e\u4e8c\u968e\u504f\u5c0e\u95a2\u6570\u5168\u4f53\u304c\u4f5c\u308b\u6b63\u65b9\u884c\u5217\u3067\u3042\u308b\u3002\u5b9f\u6570\u5024\u95a2\u6570\u306e\u6975\u5024\u5224\u5b9a\u306b\u7528\u3044\u3089\u308c\u308b\u3002\u30d8\u30c3\u30bb\u884c\u5217\u306f\u3001\u30b8\u30a7\u30fc\u30e0\u30b9\u30fb\u30b8\u30e7\u30bb\u30d5\u30fb\u30b7\u30eb\u30d9\u30b9\u30bf\u30fc\u304c\u3001\u30c9\u30a4\u30c4\u306e\u6570\u5b66\u8005\u30eb\u30fc\u30c8\u30f4\u30a3\u30d2\u30fb\u30aa\u30c3\u30c8\u30fc\u30fb\u30d8\u30c3\u30bb\u306b\u7531\u6765\u3057\u3066\u540d\u3065\u3051\u305f\u3002\n\n* [\u95a2\u6570\u306e\u52fe\u914d\u3068\u30d8\u30c3\u30bb\u884c\u5217](http://www.is.kyusan-u.ac.jp/~abe/optprba163.pdf)\n\n* [\uff08SlideShare\uff09KOTARO SETOYAMA, _Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks_](http://www.slideshare.net/KOTAROSETOYAMA/hessianfree-optimization-for-learning-deep-multidimensional-recurrent-neural-networks-64384395)\n\n* [Hatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc\uff082014-02-08\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u300f](http://d.hatena.ne.jp/nishiohirokazu/20140208/1391838220)\n\n> \"Hessian-Free\"\u3068\u3044\u3046\u65b0\u3057\u3044\u6700\u9069\u5316\u624b\u6cd5\u3092Deep Learning\u306eauto-encoder\u306e\u5b66\u7fd2\u306b\u4f7f\u3063\u3066\u307f\u305f\u3089\u4e8b\u524d\u5b66\u7fd2\u306a\u3057\u3067\u65e2\u5b58\u306e\u5831\u544a\u306e\u6027\u80fd\u3092\u8d85\u3048\u305f\u305e\u51c4\u3044\u3060\u308d\u3046\u3001\u3068\u3044\u3046\u8a71\u3002\n>\n>\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6c7a\u5b9a\u306f\u3088\u304f\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u554f\u984c\u3067\u3001\u52fe\u914d\u6cd5\u3067\u52b9\u7387\u3088\u304f\u8a08\u7b97\u3067\u304d\u308b\u3068\u8a00\u308f\u308c\u3066\u3044\u308b\u3002\n>\n>\u3057\u304b\u3057Deep Learning\u306e\u3088\u3046\u306b\u96a0\u308c\u5c64\u304c\u3068\u3066\u3082\u591a\u3044\u30b1\u30fc\u30b9\u3067\u306f\u3046\u307e\u304f\u3044\u304b\u306a\u3044\u3002\u5b66\u7fd2\u306b\u3068\u3066\u3082\u6642\u9593\u304c\u304b\u304b\u3063\u305f\u308a\u3001\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3055\u3048\u9177\u3044\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3057\u304b\u51fa\u305b\u306a\u304b\u3063\u305f\u308a\u3059\u308b(under-fitting)\u3002\n>\n>\u6700\u9069\u5316\u306b\u95a2\u3059\u308b\u7814\u7a76\u8005\u306e\u9593\u3067\u306f\u52fe\u914d\u6cd5\u304c\u75c5\u7684\u306a\u66f2\u7387\u3092\u6301\u3063\u305f\u76ee\u7684\u95a2\u6570\u306b\u5bfe\u3057\u3066\u306f\u4e0d\u5b89\u5b9a\u3067\u3042\u308b\u3053\u3068\u304c\u3088\u304f\u77e5\u3089\u308c\u3066\u3044\u308b\u30022\u6b21\u306e\u6700\u9069\u5316\u6cd5\u306f\u3053\u306e\u3088\u3046\u306a\u76ee\u7684\u95a2\u6570\u306b\u5bfe\u3057\u3066\u3082\u3046\u307e\u304f\u50cd\u304f\u3002\u3060\u304b\u3089Deep Learning\u306b\u3082\u3053\u306e\u7a2e\u306e\u6700\u9069\u5316\u3092\u4f7f\u3063\u305f\u3089\u3044\u3044\u3093\u3058\u3083\u306a\u3044\u304b\u3002\n>\n>\u3067\u3082\u307e\u3060\u3044\u304f\u3064\u304b\u554f\u984c\u304c\u3042\u308b\u3002\u307e\u305a\u3067\u304b\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u73fe\u5b9f\u7684\u306a\u901f\u5ea6\u3067\u52d5\u304f\u3088\u3046\u306b\u3059\u308b\u3053\u3068\u3002\u3053\u308c\u306f\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5(SGD)\u307f\u305f\u3044\u306a\u611f\u3058\u3067\u30aa\u30f3\u30e9\u30a4\u30f3\u5b66\u7fd2\u306b\u3057\u305f\u3089\u3044\u3044\u3002\n>\n>\n>\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306bpre-training\u3059\u308b\u3063\u3066\u306e\u3082\u5927\u304d\u306a\u9032\u6b69\u306e\u4e00\u3064\u3060\u3002\u3053\u308c\u3092\u3084\u3063\u3066\u304b\u3089SGD\u3059\u308b\u3068\u554f\u984c\u3092\u56de\u907f\u3067\u304d\u308b\u3088\u3046\u306b\u307f\u3048\u308b\u3002\u5b9f\u969b\u3001\u6210\u529f\u3057\u3066\u3044\u308b\u5fdc\u7528\u4f8b\u3082\u8272\u3005\u3042\u308b\u3002\u3057\u304b\u3057\u7591\u554f\u304c\u6b8b\u308b\uff1a\u306a\u3093\u3067\u3053\u308c\u3067\u4e0a\u624b\u304f\u3044\u304f\u306e\uff1f\u306a\u3093\u3067\u5fc5\u8981\u306a\u306e\uff1f\u30ed\u30fc\u30ab\u30eb\u30df\u30cb\u30de\u30e0\u304c\u5927\u91cf\u306b\u3042\u308b\u3053\u3068\u304c\u539f\u56e0\u3060\u3068\u3044\u3046\u8aac\u660e\u3092\u3057\u3066\u3044\u308b\u7814\u7a76\u8005\u3082\u3044\u308b\u3002\n>\n>\u5225\u306e\u8aac\u660e\u3068\u3057\u3066\u3001\u76ee\u7684\u95a2\u6570\u304c\u75c5\u7684\u306a\u66f2\u7387\uff08\u4f8b\u3048\u3070\u7d30\u9577\u3044\u8c37\u3068\u304b\uff09\u3092\u6301\u3063\u3066\u3044\u3066\u3001\u52fe\u914d\u6cd5\u306e\u3088\u3046\u306a\u66f2\u7387\u3092\u898b\u306a\u3044\u6700\u9069\u5316\u6cd5\u3067\u306f\u6700\u9069\u5316\u4e0d\u53ef\u80fd\u3060\u3001\u3068\u3044\u3046\u3082\u306e\u304c\u3042\u308b\u3002\u305d\u3053\u3067\u3053\u306e\u8996\u70b9\u306b\u7acb\u3063\u3066\u3001\u30bb\u30df\u30aa\u30f3\u30e9\u30a4\u30f3\u306e2\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b\u3002\u3053\u308c\u3092\u4f7f\u3048\u3070under-fitting\u3057\u306a\u3044\u3057\u65e2\u5b58\u306epre-training\u3092\u4f7f\u3063\u305f\u65b9\u6cd5\u3088\u308a\u52b9\u7387\u7684\u3002\n>\n>\u307e\u305a\u6a19\u6e96\u7684\u306a2\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3067\u3042\u308b\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306b\u3064\u3044\u3066\u89e3\u8aac\u3059\u308b\u3002\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u500b\u6570\u306e2\u4e57\u306e\u30b5\u30a4\u30ba\u306e\u30d8\u30b7\u30a2\u30f3\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u3067\u304b\u3044\u30e2\u30c7\u30eb\u306e\u8a08\u7b97\u306b\u306f\u73fe\u5b9f\u7684\u3067\u306f\u306a\u3044\u3002\u304c\u3001\u3053\u308c\u3092\u5b66\u3076\u3053\u3068\u3067\u3001\u3082\u3063\u3068\u73fe\u5b9f\u7684\u306a\u4e9c\u7a2e(quasi-Newton methods\u3068\u306f)\u304c\u3069\u3046\u632f\u308b\u821e\u3046\u306e\u304b\u306b\u3064\u3044\u3066\u7406\u89e3\u3067\u304d\u308b\u3002\n>\n>\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\u52fe\u914d\u6cd5\u540c\u69d8\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u9010\u6b21\u7684\u306b\u66f4\u65b0\u3057\u3066\u3044\u304f\u624b\u6cd5\u3002\u30b3\u30a2\u306e\u30a2\u30a4\u30c7\u30a2\u306f\u300c\u76ee\u7684\u95a2\u6570f\u3092\u4e8c\u6b21\u95a2\u6570\u3067\u8fd1\u4f3c\u300d(\u6570\u5f0f1) B\u306f\u30d8\u30b7\u30a2\u30f3\u3060\u3068\u601d\u3063\u3066\u3044\u3044\u3093\u3060\u3051\u3069\u3001\u6b63\u5b9a\u5024\u3067\u306a\u3044\u3053\u3068\u304c\u305f\u307e\u306b\u3042\u3063\u3066\u6700\u5c0f\u5024\u3092\u6301\u305f\u306a\u304f\u3066\u56f0\u308b\u306e\u3067\u9069\u5f53\u306b\u5bfe\u89d2\u6210\u5206\u3092\u5897\u3084\u3057\u3066\u6b63\u5b9a\u5024\u306b\u3059\u308b\n>\n>\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u306f\"scale invariance\"\u304c\u91cd\u8981\u306a\u7279\u5fb4\u3002\u7dda\u5f62\u306a\u5909\u63db\u3067\u306f\u632f\u308b\u821e\u3044\u304c\u5909\u308f\u3089\u306a\u3044\u3002\u3053\u308c\u304c\u306a\u3044\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u60aa\u3044\u3068\u6027\u80fd\u304c\u60aa\u5316\u3057\u3066\u3057\u307e\u3046\u3002\u307e\u305f\u3053\u306e\u7279\u5fb4\u306b\u3088\u3063\u3066\u5b66\u7fd2\u7387\u3092\u8abf\u6574\u3057\u305f\u308a\u3068\u304b\u3057\u306a\u304f\u3066\u3088\u304f\u306a\u308b\u3002\n>\n>\u9006\u306b\u8a00\u3048\u3070\u73fe\u5728\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5468\u8fba\u306e\u66f2\u7387\u3092\u5143\u306b\u6697\u9ed9\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u6c7a\u307e\u3063\u3066\u3044\u3066\u3001\u305d\u308c\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u5909\u63db\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u8003\u3048\u65b9\u304c\u3067\u304d\u308b\u3002\n>\n>\u66f2\u7387\u306e\u7d76\u5bfe\u5024\u304c\u5c0f\u3055\u3044\u306a\u3089\u52fe\u914d\u306e\u5909\u5316\u306f\u5c11\u306a\u3044\u306e\u3067\u3001\u9577\u3044\u8ddd\u96e2\u9032\u3093\u3067\u3082\u826f\u3044\u3063\u3066\u3053\u3068\u3002\u9006\u306b\u5927\u304d\u3044\u306a\u3089\u3001p\u306e\u65b9\u5411\u306b\u30bb\u30f3\u30b7\u30c6\u30a3\u30d6\u3063\u3066\u3053\u3068\u3060\u304b\u3089\u3061\u3087\u3063\u3068\u3060\u3051\u9032\u3093\u3067\u9069\u5207\u306ap\u306e\u65b9\u5411\u3092\u518d\u78ba\u8a8d\u3059\u3079\u304d\u3002\uff08\u3053\u306e\u6bb5\u843d\u3001\u3042\u3068\u3067\u6570\u5f0f\u3092\u518d\u78ba\u8a8d\u3059\u308b\uff09\n\n* [Hatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc\uff082014-02-09\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u305d\u306e2\u300f](http://d.hatena.ne.jp/nishiohirokazu/20140209/1391874480)\n\n> \"Hessian-Free\"\u3068\u3044\u3046\u65b0\u3057\u3044\u6700\u9069\u5316\u624b\u6cd5\u3092Deep Learning\u306eauto-encoder\u306e\u5b66\u7fd2\u306b\u4f7f\u3063\u3066\u307f\u305f\u3089\u4e8b\u524d\u5b66\u7fd2\u306a\u3057\u3067\u65e2\u5b58\u306e\u5831\u544a\u306e\u6027\u80fd\u3092\u8d85\u3048\u305f\u305e\u51c4\u3044\u3060\u308d\u3046\u3001\u3068\u3044\u3046\u8a71\u3002\n>\n>Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u306e\u7d9a\u304d\u3002\n>\n> \u75c5\u7684\u306a\u52fe\u914d\u306e\u4f8b\u3068\u3001\u305d\u308c\u306b\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u304c\u5411\u3044\u3066\u3044\u308b\u3053\u3068\u306e\u8aac\u660e\u3002\n>\n> \u305f\u3068\u3048\u3070\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3042\u308b\u540c\u3058\u5c64\u306e2\u3064\u306e\u30cb\u30e5\u30fc\u30ed\u30f3a, b\u306b\u3064\u3044\u3066\u5165\u51fa\u529b\u306e\u91cd\u307f\u304c\u307b\u307c\u540c\u3058\u3068\u3059\u308b\u3002a\u306e\u3042\u308b\u91cd\u307fi\u3092\u4e0a\u3052\u3066\u3001b\u306e\u5bfe\u5fdc\u3059\u308b\u91cd\u307fj\u3092\u4e0b\u3052\u308b\u3088\u3046\u306a\u65b9\u5411d\u3078\u306e\u66f4\u65b0\u3092\u8003\u3048\u308b\u3002\u3053\u306e\u6642d\u65b9\u5411\u306e\u52fe\u914d\u306f\u307b\u3068\u3093\u30690\u3002\u66f2\u7387\u3082\u307b\u3068\u3093\u30690\u3002\u3060\u304b\u3089\u52fe\u914d\u6cd5\u307f\u305f\u3044\u306a1\u6b21\u306e\u6700\u9069\u5316\u624b\u6cd5\u3067\u306f\u307b\u3068\u3093\u3069\u66f4\u65b0\u3055\u308c\u306a\u3044\u3002\n>\n> \u4e00\u65b9\u30cb\u30e5\u30fc\u30c8\u30f3\u6cd5\u307f\u305f\u3044\u306a2\u6b21\u306e\u65b9\u6cd5\u306a\u3089\u5206\u6bcd\u306b\u66f2\u7387\u304c\u6765\u308b\u304b\u3089\u3082\u3063\u3068\u901f\u304f\u66f4\u65b0\u3055\u308c\u308b\u3002\n>\n>\u672c\u984c\uff1a\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u306b\u3064\u3044\u3066\u3002\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc(HF)\u306f\n>\n> * \u30d8\u30b7\u30a2\u30f3H\u3092\u8a08\u7b97\u3057\u306a\u3044(\u4ed6\u306e\u591a\u304f\u306equasi-Newton\u3068\u9055\u3063\u3066)\n>>\u4ee3\u308f\u308a\u306bHd\u3092\u6709\u9650\u5dee\u5206\u6cd5\u3067\u76f4\u63a5\u6c42\u3081\u308b\uff08\u3088\u3046\u3059\u308b\u306b2\u56de\u52fe\u914d\u3092\u8a08\u7b97\u3057\u3066\u5f15\u304f\uff09\n>\n> * \u76ee\u7684\u95a2\u6570\u3067\u306f\u306a\u304f\u305d\u308c\u306e\u4e8c\u6b21\u8fd1\u4f3cq\u03b8\u306b\u5bfe\u3057\u3066Linear Conjugate Gradient\u3067\u89e3\u304f\u3002\nN\u6b21\u5143\u306a\u3089\u6700\u5927N\u30b9\u30c6\u30c3\u30d7\u304b\u304b\u308b\u304c\u3001\u9069\u5f53\u306b\u6253\u3061\u5207\u308b\u3002\n> * \u4e00\u822c\u7684\u306b\u5171\u5f79\u52fe\u914d\u6cd5\u3068\u8a00\u308f\u308c\u308bNon-linear Conjugate Gradient\u3068\u9055\u3063\u3066\u3001\u66f2\u7387\u306b\u95a2\u3059\u308b\u518d\u8a08\u7b97\u304c\u5fc5\u8981\u306a\u3044\u3002\u3053\u308c\u306f\u76ee\u7684\u95a2\u6570\u3092\u66f2\u7387\u4e00\u5b9a\u306e\u66f2\u9762\u3067\u6700\u521d\u306b\u8fd1\u4f3c\u3057\u3066\u308b\u304b\u3089\u3002\u8981\u3059\u308b\u306b\u4e8c\u6b21\u8fd1\u4f3c\u3057\u3066\u308b\u304b\u3089\u4e8c\u968e\u5fae\u5206\u306f\u5b9a\u6570\u3067H\u304c\u5909\u308f\u3089\u306a\u3044\u3063\u3066\u3053\u3068\u3067Hd\u3082\u5909\u308f\u3089\u306a\u3044\u304b\u3089\u518d\u8a08\u7b97\u304c\u3044\u3089\u306a\u3044\u3002\n>\n>\u611f\u60f3\uff1a\u5171\u5f79\u52fe\u914d\u6cd5\u3001\u76ee\u7684\u95a2\u6570\u304c2\u6b21\u306e\u6642\u306b\u306f2\u56de\u3067\u53ce\u675f\u3059\u308b\u3068\u3044\u3046\u8aa4\u89e3\u3092\u3057\u3066\u3044\u305f\u3051\u3069\u3082\u3001\u305d\u308c\u306f\u300c\u6b21\u5143\u304c2\u6b21\u5143\u3060\u304b\u30892\u56de\u3067\u53ce\u675f\u3059\u308b\u300d\u3068\u3044\u3046\u3053\u3068\u3060\u3063\u305f\u3088\u3046\u3060\u3002\u8981\u3059\u308b\u306bN\u6b21\u5143\u306a\u3089\u6700\u5927N\u672c\u306e\u5171\u5f79\u306a\u65b9\u5411\u304c\u9078\u3070\u308c\u3066\u3001\u305d\u3063\u3061\u65b9\u5411\u306b\u3069\u3093\u3060\u3051\u9032\u3081\u3070\u3044\u3044\u304b\u306f\u76ee\u7684\u95a2\u6570\u304c\u4e8c\u6b21\u306a\u3089\u52fe\u914d\u3092\u66f2\u7387\u3067\u5272\u308b\u3053\u3068\u3067\u4e00\u767a\u3067\u6b63\u78ba\u306b\u6c42\u3081\u3089\u308c\u308b\u3002\u3060\u304b\u3089\u6700\u5927N\u56de\u3002\u300c\u66f2\u7387\u3067\u5272\u308b\u300d\u304c\u4ed6\u306e\u65b9\u6cd5\u3060\u3068NxN\u884c\u5217\u306e\u9006\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\u3053\u3068\u306b\u306a\u3063\u3066\u6b7b\u306d\u308b\u3051\u3069Hessian-Free\u3067\u306fHd\u3092\u5dee\u5206\u6cd5\u3067\u6c42\u3081\u3061\u3083\u3046\u304b\u3089\u5927\u5909\u3058\u3083\u306a\u3044\u3001\u3068\u3002\u9762\u767d\u304f\u306a\u3063\u3066\u307e\u3044\u308a\u307e\u3057\u305f\u3002\u6b21\u306f4\u7ae0\u304b\u3089\u3002\n>\n>__\u51fa\u5178__\n>\n>http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf\n\n\n__\u4e0a \u3067 \u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587 \u306f \u4ee5\u4e0b__\n\n* [James Martens\uff08University of Toronto\uff09, _Deep learning via Hessian-free optimization_](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf)\n\n__\u30c8\u30ed\u30f3\u30c8\u5927\u5b66\u5f37\u3044\u306a__\n\n<img width=\"1156\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-07 0.37.39.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/a3a94c4c-cb96-9e6f-9cd2-5fee9e80043b.png\">\n\n> __Abstract__\n>\n> __We develop a 2nd-order optimization method based on the \u201cHessian-free\u201d approach__, and apply it to training __deep auto-encoders__. \n>\n> __Without using pre-training__, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. \n>\n> Our method is practical, easy to use, scales nicely to very large\ndatasets, and isn\u2019t limited in applicability to autoencoders,\nor any specific model class. \n>\n> We also discuss the issue of \u201cpathological curvature\u201d as a possible explanation for the difficulty of deep learning and how 2nd-order optimization, and our method in particular, effectively deals with it.\n\n\n* [Hatena::Diary \u897f\u5c3e\u6cf0\u548c\u306e\u306f\u3066\u306a\u30c0\u30a4\u30a2\u30ea\u30fc \uff082014-02-10\uff09\u300e\u25a0Deep Learning\u8ad6\u6587\u7d39\u4ecb\u300cDeep learning via Hessian-free optimization\u300d\u305d\u306e3\u300f](http://d.hatena.ne.jp/nishiohirokazu/20140210/1392026316)\n\n> \u6b8b\u5ff5\u306a\u304a\u77e5\u3089\u305b\u3067\u3059\u304c\u3001\u3053\u306e\u9023\u8f09\u306f\u7d9a\u304d\u307e\u305b\u3093\u3002\n>\n> \u305d\u3082\u305d\u3082\u50d5\u306e\u8208\u5473\u306fword2vec\u306b\u3088\u308b\u610f\u5473\u306e\u7406\u89e3\u3068\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u7ae0\u306e\u751f\u6210\u3060\u3063\u305f\u308f\u3051\u3067\u3059\u3002\u5f8c\u8005\u306e\u8ad6\u6587\u306fRNNLM\u3092\u4f7f\u3063\u3066\u3044\u3066\u3001\u305d\u308c\u306f\u524d\u8005\u306e\u8ad6\u6587\u306e\u8457\u8005\u304cword2vec\u306e\u524d\u306b\u3084\u3063\u3066\u3044\u305f\u3053\u3068\u3067\u3059\u3002\n>\n> \u3068\u3044\u3046\u308f\u3051\u3067RNNLM\u306b\u3064\u3044\u3066\u8abf\u3079\u59cb\u3081\u3001\u305d\u306e\u6700\u9069\u5316\u304c\u4ed6\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u3088\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u3088\u3046\u306a\u78ba\u7387\u7684\u52fe\u914d\u6cd5\u3067\u306f\u96e3\u3057\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u30d8\u30b7\u30a2\u30f3\u30d5\u30ea\u30fc\u6700\u9069\u5316\u306b\u3064\u3044\u3066\u8abf\u3079\u3066\u3044\u305f\u308f\u3051\u3067\u3059\u3002\n>\n> __\u524d\u56de\u8aac\u660e\u3057\u305f\u3088\u3046\u306b\u3001\u305f\u3057\u304b\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u91cd\u307f\u304c\u307b\u3068\u3093\u3069\u540c\u4e00\u3067\u3042\u308b\u30cb\u30e5\u30fc\u30ed\u30f3\u304c\u3042\u3063\u305f\u5834\u5408\u306b\u3001\u305d\u306e\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u5206\u5316\u3055\u305b\u308b\u65b9\u5411\u306e\u52fe\u914d\u304c\u5c0f\u3055\u304f\u30661\u6b21\u306e\u52fe\u914d\u6cd5\u3067\u306f\u5b66\u7fd2\u304c\u306f\u304b\u3069\u3089\u306a\u3044\u3068\u3044\u3046\u70b9\u306f\u7406\u89e3\u3067\u304d\u307e\u3059\u3002__\n>\n> __\u3067\u3082\u300c\u3060\u304b\u30892\u6b21\u306e\u6700\u9069\u5316\u6cd5\u3092\u4f7f\u3046\u300d\u3068\u3044\u3046\u7d50\u8ad6\u306b\u3042\u307e\u308a\u540c\u610f\u3067\u304d\u307e\u305b\u3093\u3002__\n>\n> __\u500b\u4eba\u7684\u306b\u306f\u300c\u30b9\u30d1\u30fc\u30b9\u5316\u306e\u5236\u7d04\u3092\u304b\u3051\u305f\u3089\u305d\u306e\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u3069\u3063\u3061\u304b\u3057\u304b\u767a\u706b\u3067\u304d\u306a\u3044\u95a2\u4fc2\u4e0a\u3001\u5206\u5316\u304c\u30b9\u30d4\u30fc\u30c7\u30a3\u306b\u8d77\u3053\u308b\u3067\u3057\u3087\u300d\u3068\u601d\u3046\u308f\u3051\u3067\u3059\u3002__\n>\n> __\u6700\u7d42\u7684\u306b\u30b9\u30d1\u30fc\u30b9\u3067\u306a\u3044\u3082\u306e\u304c\u6b32\u3057\u3044\u306e\u3060\u3068\u3057\u3066\u3082\u3001\u6700\u521d\u306f\u30b9\u30d1\u30fc\u30b9\u5236\u7d04\u3092\u304b\u3051\u3066\u300c\u3088\u3044\u521d\u671f\u5024\u300d\u3092\u63a2\u3057\u305f\u3089\u3044\u3044\u3093\u3058\u3083\u306a\u3044\u306e\u3002__\n>\n> __\u300c\u305d\u308c\u3067\u3088\u3044\u300d\u3084\u300c\u305d\u3063\u3061\u306e\u65b9\u304c\u826f\u3044\u300d\u306f\u4eee\u8aac\u306b\u3059\u304e\u306a\u3044\u306e\u3067\u3001\u305d\u306e\u4eee\u8aac\u304c\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3057\u3066\u307f\u308b\u306e\u3082\u4e00\u8208\u3060\u3068\u306f\u601d\u3046\u306e\u3060\u3051\u3069\u3001\u5b9f\u306f\u3042\u3093\u307e\u308a\u8208\u5473\u304c\u6301\u3066\u307e\u305b\u3093\u3002__\n>\n> \u3082\u3046\u4e00\u70b9\u3001word2vec\u306b\u55b0\u308f\u305b\u308b\u305f\u3081\u306b\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\u3092\u4f5c\u308b\u3068\u3053\u308d\u3067\u3001\u5f62\u614b\u7d20\u89e3\u6790\u3092\u4f7f\u3044\u305f\u304f\u306a\u3044\u3002\n>\n> \u305d\u3053\u3067RNNLM\u304b\u3089\u300c\u5358\u8a9e\u300d\u3092\u767a\u898b\u3059\u308b\u7814\u7a76\u304c\u306a\u3044\u304b\u306a\u3068\u8abf\u3079\u3066\u3044\u305f\u3068\u3053\u308d\u3001\u3080\u3057\u308d\u300c\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u7bc0\u304c\u6700\u5c0f\u9650\u3067 deep learning \u306b\u3088\u308b\u7d50\u679c\u3068\u307b\u307c\u540c\u7b49\u306e\u7cbe\u5ea6\u304c\u5f97\u3089\u308c\u308b\u624b\u6cd5\u300d\u304c\u3042\u308b\u3068\u3044\u3046\u8a18\u8ff0\u3092\u767a\u898b\uff08SIGNL 213: \u73fe\u5b9f\u7684\u306a\u5f62\u614b\u7d20\u89e3\u6790\u5668\u306e\u5165\u529b\u2192\u300c\u305a\u3082\u3082\u3082\u3082\u307a\u308d\u307a\u308d\u307a\u308d\u307a\u308d\u30de\u30df\u30bf\u30b9\u30de\u30df\u30bf\u30b9\u30e9\u30d6\u30de\u30df\u30bf\u30b9\u300d - \u6b66\u8535\u91ce\u65e5\u8a18\uff09\n>\n>\u3068\u3044\u3046\u308f\u3051\u3067\u95a2\u5fc3\u304c\u3053\u3061\u3089\u306e\u8ad6\u6587\u306b\u79fb\u3063\u305f\u306e\u3067\u9023\u8f09\u306f\u7d42\u4e86\u3067\u3059\uff1a\n> http://chasen.org/~daiti-m/paper/nl190segment.pdf\n\n___\n\n####__\u300c\u81ea\u7136\u52fe\u914d\u6cd5\u300d \u3068\u306e \u6bd4\u8f03__\n\n__\u5192\u982d \u3067 \u793a\u3057\u305f Theano\u5b9f\u88c5\u30b3\u30fc\u30c9 \u3092 \u63d0\u793a\u3057\u305f Pascanu\u6c0f\uff08\u30e2\u30f3\u30c8\u30ea\u30aa\u30fc\u30eb\u5927\u5b66\uff09 \u3068\u3001\u304a\u306a\u3058\u307f Yoshua Bengio\u6c0f \u3068\u306e\u5171\u540c\u57f7\u7b46\u8ad6\u6587__\n\n* [Razvan Pascanu and Yoshua Bengio, _Revisiting natural gradient for deep networks_](https://arxiv.org/pdf/1301.3584v7.pdf)\n\n<img width=\"1157\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-07 0.21.29.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/404ae97e-0625-06b8-4412-7c4c7b7eeaa9.png\">\n\n> __Abstract__\n>\n> We evaluate natural gradient descent, an algorithm originally proposed in Amari(1997), for learning deep models. \n>\n> The contributions of this paper are as follows.\n>\n> __We show the connection between natural gradient descent and *three other recently proposed methods* for training deep models: *Hessian-Free Optimization (Martens,2010)*, *Krylov Subspace Descent (Vinyals and Povey, 2012)* and *TONGA (Le Roux et al., 2008)*__. \n>\n> We describe how one can use unlabeled data to improve the\ngeneralization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set __compared to stochastic gradient descent__.\n>\n> Finally __we extend natural gradient descent to incorporate second\norder information alongside the manifold information__ and provide __a benchmark of the new algorithm__ using __a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it__.\n\n", "tags": ["DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "\u4eba\u5de5\u77e5\u80fd", "InformationGeometry", "NeuralNetwork"]}