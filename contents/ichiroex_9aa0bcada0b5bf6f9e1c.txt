{"context": "\n\n\u306f\u3058\u3081\u306b\n\u6700\u8fd1\u4f55\u304b\u3068\u8a71\u984c\u306eChainer\u3092\u4f7f\u3063\u3066\u6587\u66f8\u306e\u30dd\u30b8\u30cd\u30ac\u3092\u5224\u5b9a\u3059\u308b2\u5024\u5206\u985e\u5668\u3092\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u521d\u3081\u3066Chainer\u3092\u4f7f\u3063\u305f\u306e\u3067\u3001\u7df4\u7fd2\u7528\u3068\u3044\u3046\u3053\u3068\u3067\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u7b46\u8005\u306e\u3088\u3046\u306bChainer\u3067\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u3066\u4f55\u304b\u3057\u3066\u307f\u305f\u3044\u3068\u3044\u3046\u65b9\u5411\u3051\u3067\u3059\u3002\n\u9593\u9055\u3044\u7b49\u306f\u30b3\u30e1\u30f3\u30c8\u6b04\u3067\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\u5168\u30b3\u30fc\u30c9\u306f\u3053\u3061\u3089\u304b\u3089\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002\n\n\u4e8b\u524d\u6e96\u5099\n\nchainer, gensim, scikit-learn\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\n\u74b0\u5883\n\nPython 2.7\u7cfb\nChainer 1.6.2.1\n\n\n\u4f7f\u7528\u3059\u308b\u30c7\u30fc\u30bf\u306e\u4f8b\n\u4f7f\u7528\u3057\u305f\u30c7\u30fc\u30bf\u306f\u3001\u82f1\u8a9e\u306e\u4f55\u304b\u306e\u30ec\u30d3\u30e5\u30fc\u306b\u95a2\u3059\u308b\u6587\u66f8\u3067\u3059\u3002\n\u5404\u884c\u304c\u4e00\u6587\u66f8\u306b\u5bfe\u5fdc\u3057\u3066\u304a\u308a\u3001\u6587\u66f8\u4e2d\u306e\u5404\u5358\u8a9e\u306f\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3067\u533a\u5207\u3089\u308c\u3066\u3044\u307e\u3059\u3002\n\u5404\u884c\u306e\u5148\u982d\u306e\u6570\u5b57(e.g. 1, 0)\u306f\u3001\u30e9\u30d9\u30eb\u3067\u3059\u3002\n\uff0a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5404\u81ea\u7528\u610f\u3057\u3066\u4e0b\u3055\u3044\u3002\n\uff0a\u65e5\u672c\u8a9e\u306e\u6587\u66f8\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001MeCab\u306a\u3069\u3067\u5206\u304b\u3061\u66f8\u304d\u3092\u884c\u3063\u3066\u4e0b\u3055\u3044\u3002\n\n0 each scene drags , underscoring the obvious , and sentiment is slathered on top .\n0 afraid to pitch into farce , yet only half-hearted in its spy mechanics , all the queen's men is finally just one long drag .\n1 clooney directs this film always keeping the balance between the fantastic and the believable . . .\n1 just about the best straight-up , old-school horror film of the last 15 years .\n\n\n\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n\u5404\u6587\u66f8\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5165\u529b\u3068\u3057\u3066\u6271\u3046\u305f\u3081\u306b\u3001bag-of-words\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3092\u884c\u3044\u307e\u3059\u3002\n\u30d9\u30af\u30c8\u30eb\u5316\u306b\u306f\u3001gensim\u306e\u95a2\u6570\u3092\u5229\u7528\u3057\u307e\u3057\u305f\u3002\n\u8a73\u3057\u304f\u306f\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u3066\u4e0b\u3055\u3044\u3002\u2192  scikit-learn\u3068gensim\u3067\u30cb\u30e5\u30fc\u30b9\u8a18\u4e8b\u3092\u5206\u985e\u3059\u308b\n\u95a2\u6570load_data\u3067\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5404\u6587\u66f8\u306e\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092l.strip().split(\" \", 1)\u3067\u5206\u5272\u3057\u3066\u3044\u307e\u3059\u3002\ntarget\u306b\u306f\u6587\u66f8\u30e9\u30d9\u30eb\u3001source\u306b\u306f\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u305d\u308c\u305e\u308c\u683c\u7d0d\u3057\u3001dataset\u306b\u307e\u3068\u3081\u3066\u623b\u308a\u5024\u3068\u3057\u3066\u8fd4\u3057\u307e\u3059\u3002\ncorpora.Dictionary(document_list)\u3067\u306f\u3001\u5404\u5358\u8a9e\u3092\u8981\u7d20\u3068\u3059\u308b\u6587\u66f8\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8(document_list)\u3092\u6e21\u3059\u3053\u3068\u3067\u3001\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\u672c\u6765\u306a\u3089\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u307f\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u672a\u77e5\u8a9e\u51e6\u7406\u3092\u7701\u304d\u305f\u304b\u3063\u305f\u306e\u3067\u5168\u6587\u66f8\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u3053\u3067vocab_size\u306f\u3001\u5168\u6587\u66f8\u306e\u8a9e\u5f59\u6570\u3067\u3042\u308a\u3001\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002\n\u305d\u306e\u305f\u3081\u3001\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u3001vocab_size\u3068\u7b49\u3057\u3044\u3067\u3059\u3002\ndef load_data(fname):\n    source = []\n    target = []\n    f = open(fname, \"r\")\n\n    document_list = [] #\u5404\u884c\u306b\u4e00\u6587\u66f8. \u6587\u66f8\u5185\u306e\u8981\u7d20\u306f\u5358\u8a9e\n    for l in f.readlines():\n        sample = l.strip().split(\" \", 1)        #\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092\u5206\u3051\u308b\n        label = int(sample[0])                  #\u30e9\u30d9\u30eb\n        target.append(label)\n        document_list.append(sample[1].split()) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n\n    #\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n    dictionary = corpora.Dictionary(document_list)\n    dictionary.filter_extremes(no_below=5, no_above=0.8)\n    # no_below: \u4f7f\u308f\u308c\u3066\u3044\u308b\u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n    # no_above: \u4f7f\u308f\u308c\u3066\u308b\u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n\n    #\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n    for document in document_list:\n        tmp = dictionary.doc2bow(document) #\u6587\u66f8\u3092BoW\u8868\u73fe\n        vec = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0])\n        source.append(vec)\n\n    dataset = {}\n    dataset['target'] = np.array(target)\n    dataset['source'] = np.array(source)\n    print \"vocab size:\", len(dictionary.items()) #\u8a9e\u5f59\u6570 = \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n    return dataset, dictionary\n\n\n\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u306f\u7df4\u7fd2\u7528\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\uff08\u3055\u304d\u307b\u3069\u306e\u95a2\u6570load_data\u304b\u3089\u53d7\u3051\u53d6\u3063\u305fdataset\u3092scikit-learn\u306b\u5165\u3063\u3066\u3044\u308b\u95a2\u6570train_test_split\u3092\u5229\u7528\u3057\u3066\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u3066\u3044\u307e\u3059\u3002\uff09\n\u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570in_units\u306f\u3001\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570(x_train.shape[1])\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\u96a0\u308c\u5c64(\u4e2d\u9593\u5c64)\u306f\u3001\u9069\u5f53\u306b\u8a2d\u5b9a\u3057\u3066\u69cb\u3044\u307e\u305b\u3093\u3002\u4eca\u56de\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067500\u3092\u6e21\u3059\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u51fa\u529b\u5c64\u306f\u3001\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3092\u4f7f\u3046\u306e\u3067\u3001\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u30e9\u30d9\u30eb\u306e\u30bf\u30a4\u30d7\u6570\u3067\u3042\u308b2\u3068\u3057\u3066\u3044\u307e\u3059\u3002\nx_train, x_test, y_train, y_test = train_test_split(dataset['source'], dataset['target'], test_size=0.15)\nN_test = y_test.size         # test data size\nN = len(x_train)             # train data size\nin_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n\nn_units = args.units # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_label = 2          # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = chainer.Chain(l1=L.Linear(in_units, n_units),\n                      l2=L.Linear(n_units, n_units),\n                      l3=L.Linear(n_units,  n_label))\n\n\n\u9806\u4f1d\u642c\n\u95a2\u6570forward\u3067\u306f\u9806\u4f1d\u642c\u3092\u884c\u3044\u307e\u3059\u3002\n\u5165\u529b\u5c64->\u96a0\u308c\u5c64\u3001\u96a0\u308c\u5c64->\u96a0\u308c\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306b\u306f\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\ndef forward(x, t, train=True):\n    h1 = F.sigmoid(model.l1(x))\n    h2 = F.sigmoid(model.l2(h1))\n    y = model.l3(h2)\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n\n\n\u5b66\u7fd2\n\u5168\u4f53\u306e\u6d41\u308c\u3068\u3057\u3066\u306f\u3001\n1. \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n2. \u9806\u4f1d\u642c\n3. \u8aa4\u5dee\u9006\u4f1d\u64ad\n4. \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n\u3068\u3044\u3046\u611f\u3058\u3067\u3059\u3002\uff08\u3042\u307e\u308a\u81ea\u4fe1\u306a\u3044\u2026\uff09\n\u5404epoch\u3067\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u8aa4\u5dee\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u8aa4\u5dee\u3092\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002\n\u307e\u305f\u3001\u5206\u985e\u554f\u984c\u306a\u306e\u3067\u5206\u985e\u6b63\u89e3\u7387accuracy\u3082\u305d\u308c\u305e\u308c\u8a08\u7b97\u3057\u307e\u3059\u3002\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n# Learning loop\nfor epoch in six.moves.range(1, n_epoch + 1):\n\n    print 'epoch', epoch\n\n    # training\n    perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n    sum_train_loss     = 0.0\n    sum_train_accuracy = 0.0\n    for i in six.moves.range(0, N, batchsize):\n\n        #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n\n        model.zerograds()            # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n        loss, acc = forward(x, t)    # \u9806\u4f1d\u642c\n        sum_train_loss      += float(cuda.to_cpu(loss.data)) * len(t)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n        sum_train_accuracy  += float(cuda.to_cpu(acc.data )) * len(t)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n        loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n        optimizer.update()           # \u6700\u9069\u5316\n\n    print('train mean loss={}, accuracy={}'.format(\n        sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n\n\n    # evaluation\n    sum_test_loss     = 0.0\n    sum_test_accuracy = 0.0\n    for i in six.moves.range(0, N_test, batchsize):\n\n        # all test data\n        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n\n        loss, acc = forward(x, t, train=False)\n\n        sum_test_loss     += float(cuda.to_cpu(loss.data)) * len(t)\n        sum_test_accuracy += float(cuda.to_cpu(acc.data))  * len(t)\n\n    print(' test mean loss={}, accuracy={}'.format(\n        sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n\n#model\u3068optimizer\u3092\u4fdd\u5b58\nprint 'save the model'\nserializers.save_npz('pn_classifier_ffnn.model', model)\nprint 'save the optimizer'\nserializers.save_npz('pn_classifier_ffnn.state', optimizer)\n\n\n\u5b9f\u884c\u7d50\u679c\n\u6700\u7d42\u7684\u306a\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u5206\u985e\u6b63\u89e3\u7387\u306faccuracy=0.716875001788\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u3057\u304b\u3057\u3001\u5b66\u7fd2\u304c\u9032\u3080\u306b\u3064\u308c\u3066\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5897\u52a0\u3057\u3066\u304a\u308a\u3001\u904e\u5b66\u7fd2\u304c\u8d77\u304d\u3066\u3044\u307e\u3059...\n\u304a\u305d\u3089\u304f\u3066\u304d\u3068\u3046\u306b\u30e2\u30c7\u30eb\u3092\u7d44\u3093\u3060\u3053\u3068\u304c\u539f\u56e0\u3067\u3057\u3087\u3046\u3002\n>python train.py --gpu 1 --data input.dat --units 1000\nvocab size: 4442\nepoch 1\ntrain mean loss=0.746377664579, accuracy=0.554684912523\n test mean loss=0.622971419245, accuracy=0.706875003874\nepoch 2\ntrain mean loss=0.50845754933, accuracy=0.759408453399\n test mean loss=0.503996372223, accuracy=0.761249992996\nepoch 3\ntrain mean loss=0.386604680468, accuracy=0.826067760105\n test mean loss=0.506066314876, accuracy=0.769374992698\nepoch 4\ntrain mean loss=0.301527346359, accuracy=0.870433726909\n test mean loss=0.553729468957, accuracy=0.774999994785\nepoch 5\ntrain mean loss=0.264981631757, accuracy=0.889085094432\n test mean loss=0.599407823756, accuracy=0.766874998808\nepoch 6\ntrain mean loss=0.231274759588, accuracy=0.901114668847\n test mean loss=0.68350501731, accuracy=0.755625002086\n\n...\n\nepoch 95\ntrain mean loss=0.0158744356008, accuracy=0.993598945303\n test mean loss=5.08019682765, accuracy=0.717499997467\nepoch 96\ntrain mean loss=0.0149783944279, accuracy=0.994261124581\n test mean loss=5.30629962683, accuracy=0.723749995232\nepoch 97\ntrain mean loss=0.00772037562047, accuracy=0.997351288256\n test mean loss=5.49559159577, accuracy=0.720624998212\nepoch 98\ntrain mean loss=0.00569957431572, accuracy=0.99834455516\n test mean loss=5.67661693692, accuracy=0.716875001788\nepoch 99\ntrain mean loss=0.00772406136085, accuracy=0.997240925267\n test mean loss=5.63734056056, accuracy=0.720000002533\nepoch 100\ntrain mean loss=0.0125463016702, accuracy=0.995916569395\n test mean loss=5.23713605106, accuracy=0.716875001788\nsave the model\nsave the optimizer\n\n\n\n\u304a\u308f\u308a\u306b\nChainer\u3092\u4f7f\u3063\u3066\u3001\u6587\u66f8\u5206\u985e\u306e\u305f\u3081\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\u904e\u5b66\u7fd2\u304c\u8d77\u304d\u306a\u3044\u3088\u3046\u306b\u3001\u30e2\u30c7\u30eb\u306e\u6539\u5584\u3092\u884c\u3044\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nChainer\u306e\u52c9\u5f37\u7528\u306b\u30b3\u30fc\u30c9\u3092\u898b\u3066\u307f\u305f\u3044\u65b9\u306f\u3001\u3053\u3061\u3089\u304b\u3089\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002\n\n\u53c2\u8003\u30da\u30fc\u30b8\n\n\u6df1\u5c64\u5b66\u7fd2\u3067\u30a2\u30cb\u30e1\u9854\u3092\u5206\u985e\u3059\u308b\nDeep Learning \u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af Chainer \u3092\u4f7f\u3063\u305f\u753b\u50cf\u5206\u985e \u305d\u306e4\nChainer\u3067\u56de\u5e30\u554f\u984c\u3092\u3057\u305f\n\n#\u306f\u3058\u3081\u306b\n\u6700\u8fd1\u4f55\u304b\u3068\u8a71\u984c\u306eChainer\u3092\u4f7f\u3063\u3066\u6587\u66f8\u306e\u30dd\u30b8\u30cd\u30ac\u3092\u5224\u5b9a\u3059\u308b2\u5024\u5206\u985e\u5668\u3092\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u521d\u3081\u3066Chainer\u3092\u4f7f\u3063\u305f\u306e\u3067\u3001\u7df4\u7fd2\u7528\u3068\u3044\u3046\u3053\u3068\u3067\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u7b46\u8005\u306e\u3088\u3046\u306bChainer\u3067\u30c7\u30a3\u30fc\u30d7\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b9f\u88c5\u3057\u3066\u4f55\u304b\u3057\u3066\u307f\u305f\u3044\u3068\u3044\u3046\u65b9\u5411\u3051\u3067\u3059\u3002\n\n\u9593\u9055\u3044\u7b49\u306f\u30b3\u30e1\u30f3\u30c8\u6b04\u3067\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\n\u5168\u30b3\u30fc\u30c9\u306f[\u3053\u3061\u3089](https://github.com/ichiroex/chainer-ffnn)\u304b\u3089\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002\n\n#\u4e8b\u524d\u6e96\u5099\n - chainer, gensim, scikit-learn\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n#\u74b0\u5883\n - Python 2.7\u7cfb\n - Chainer 1.6.2.1\n\n#\u4f7f\u7528\u3059\u308b\u30c7\u30fc\u30bf\u306e\u4f8b\n\u4f7f\u7528\u3057\u305f\u30c7\u30fc\u30bf\u306f\u3001\u82f1\u8a9e\u306e\u4f55\u304b\u306e\u30ec\u30d3\u30e5\u30fc\u306b\u95a2\u3059\u308b\u6587\u66f8\u3067\u3059\u3002\n\u5404\u884c\u304c\u4e00\u6587\u66f8\u306b\u5bfe\u5fdc\u3057\u3066\u304a\u308a\u3001\u6587\u66f8\u4e2d\u306e\u5404\u5358\u8a9e\u306f\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3067\u533a\u5207\u3089\u308c\u3066\u3044\u307e\u3059\u3002\n\u5404\u884c\u306e\u5148\u982d\u306e\u6570\u5b57(e.g. 1, 0)\u306f\u3001\u30e9\u30d9\u30eb\u3067\u3059\u3002\n\n\uff0a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5404\u81ea\u7528\u610f\u3057\u3066\u4e0b\u3055\u3044\u3002\n\uff0a\u65e5\u672c\u8a9e\u306e\u6587\u66f8\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001MeCab\u306a\u3069\u3067\u5206\u304b\u3061\u66f8\u304d\u3092\u884c\u3063\u3066\u4e0b\u3055\u3044\u3002\n\n> 0 each scene drags , underscoring the obvious , and sentiment is slathered on top .\n> 0 afraid to pitch into farce , yet only half-hearted in its spy mechanics , all the queen's men is finally just one long drag .\n> 1 clooney directs this film always keeping the balance between the fantastic and the believable . . .\n> 1 just about the best straight-up , old-school horror film of the last 15 years .\n\n#\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n\u5404\u6587\u66f8\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5165\u529b\u3068\u3057\u3066\u6271\u3046\u305f\u3081\u306b\u3001bag-of-words\u3067\u30d9\u30af\u30c8\u30eb\u5316\u3092\u884c\u3044\u307e\u3059\u3002\n\u30d9\u30af\u30c8\u30eb\u5316\u306b\u306f\u3001gensim\u306e\u95a2\u6570\u3092\u5229\u7528\u3057\u307e\u3057\u305f\u3002\n\u8a73\u3057\u304f\u306f\u3053\u3061\u3089\u306e\u8a18\u4e8b\u3092\u53c2\u8003\u306b\u3057\u3066\u4e0b\u3055\u3044\u3002\u2192  [scikit-learn\u3068gensim\u3067\u30cb\u30e5\u30fc\u30b9\u8a18\u4e8b\u3092\u5206\u985e\u3059\u308b](http://qiita.com/yasunori/items/31a23eb259482e4824e2)\n\n\u95a2\u6570`load_data`\u3067\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5404\u6587\u66f8\u306e\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092`l.strip().split(\" \", 1)`\u3067\u5206\u5272\u3057\u3066\u3044\u307e\u3059\u3002\n`target`\u306b\u306f\u6587\u66f8\u30e9\u30d9\u30eb\u3001`source`\u306b\u306f\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u3092\u305d\u308c\u305e\u308c\u683c\u7d0d\u3057\u3001`dataset`\u306b\u307e\u3068\u3081\u3066\u623b\u308a\u5024\u3068\u3057\u3066\u8fd4\u3057\u307e\u3059\u3002\n\n`corpora.Dictionary(document_list)`\u3067\u306f\u3001\u5404\u5358\u8a9e\u3092\u8981\u7d20\u3068\u3059\u308b\u6587\u66f8\u30ea\u30b9\u30c8\u306e\u30ea\u30b9\u30c8(`document_list`)\u3092\u6e21\u3059\u3053\u3068\u3067\u3001\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\u672c\u6765\u306a\u3089\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u307f\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u672a\u77e5\u8a9e\u51e6\u7406\u3092\u7701\u304d\u305f\u304b\u3063\u305f\u306e\u3067\u5168\u6587\u66f8\u3092\u4f7f\u3063\u3066\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u3053\u3067`vocab_size`\u306f\u3001\u5168\u6587\u66f8\u306e\u8a9e\u5f59\u6570\u3067\u3042\u308a\u3001\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570\u306b\u5bfe\u5fdc\u3057\u307e\u3059\u3002\n\u305d\u306e\u305f\u3081\u3001\u4eca\u56de\u5b9f\u88c5\u3057\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u3001`vocab_size`\u3068\u7b49\u3057\u3044\u3067\u3059\u3002\n\n```\ndef load_data(fname):\n    source = []\n    target = []\n    f = open(fname, \"r\")\n\n    document_list = [] #\u5404\u884c\u306b\u4e00\u6587\u66f8. \u6587\u66f8\u5185\u306e\u8981\u7d20\u306f\u5358\u8a9e\n    for l in f.readlines():\n        sample = l.strip().split(\" \", 1)        #\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092\u5206\u3051\u308b\n        label = int(sample[0])                  #\u30e9\u30d9\u30eb\n        target.append(label)\n        document_list.append(sample[1].split()) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n\n    #\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n    dictionary = corpora.Dictionary(document_list)\n    dictionary.filter_extremes(no_below=5, no_above=0.8)\n    # no_below: \u4f7f\u308f\u308c\u3066\u3044\u308b\u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n    # no_above: \u4f7f\u308f\u308c\u3066\u308b\u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n\n    #\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n    for document in document_list:\n        tmp = dictionary.doc2bow(document) #\u6587\u66f8\u3092BoW\u8868\u73fe\n        vec = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0])\n        source.append(vec)\n\n    dataset = {}\n    dataset['target'] = np.array(target)\n    dataset['source'] = np.array(source)\n    print \"vocab size:\", len(dictionary.items()) #\u8a9e\u5f59\u6570 = \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n    return dataset, dictionary\n```\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\u4eca\u56de\u306f\u7df4\u7fd2\u7528\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u7c21\u5358\u306a\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\uff08\u3055\u304d\u307b\u3069\u306e\u95a2\u6570`load_data`\u304b\u3089\u53d7\u3051\u53d6\u3063\u305f`dataset`\u3092scikit-learn\u306b\u5165\u3063\u3066\u3044\u308b\u95a2\u6570`train_test_split`\u3092\u5229\u7528\u3057\u3066\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u3066\u3044\u307e\u3059\u3002\uff09\n\n\u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570`in_units`\u306f\u3001\u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570(`x_train.shape[1] `)\u3092\u5165\u308c\u3066\u3044\u307e\u3059\u3002\n\u96a0\u308c\u5c64(\u4e2d\u9593\u5c64)\u306f\u3001\u9069\u5f53\u306b\u8a2d\u5b9a\u3057\u3066\u69cb\u3044\u307e\u305b\u3093\u3002\u4eca\u56de\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067500\u3092\u6e21\u3059\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u51fa\u529b\u5c64\u306f\u3001\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3092\u4f7f\u3046\u306e\u3067\u3001\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u30e9\u30d9\u30eb\u306e\u30bf\u30a4\u30d7\u6570\u3067\u3042\u308b2\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n```\nx_train, x_test, y_train, y_test = train_test_split(dataset['source'], dataset['target'], test_size=0.15)\nN_test = y_test.size         # test data size\nN = len(x_train)             # train data size\nin_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n\nn_units = args.units # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\nn_label = 2          # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n#\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nmodel = chainer.Chain(l1=L.Linear(in_units, n_units),\n                      l2=L.Linear(n_units, n_units),\n                      l3=L.Linear(n_units,  n_label))\n```\n\n#\u9806\u4f1d\u642c\n\u95a2\u6570`forward`\u3067\u306f\u9806\u4f1d\u642c\u3092\u884c\u3044\u307e\u3059\u3002\n\u5165\u529b\u5c64->\u96a0\u308c\u5c64\u3001\u96a0\u308c\u5c64->\u96a0\u308c\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306b\u306f\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\n\n```\ndef forward(x, t, train=True):\n    h1 = F.sigmoid(model.l1(x))\n    h2 = F.sigmoid(model.l2(h1))\n    y = model.l3(h2)\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)\n```\n\n#\u5b66\u7fd2\n\u5168\u4f53\u306e\u6d41\u308c\u3068\u3057\u3066\u306f\u3001\n1. \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\n2. \u9806\u4f1d\u642c\n3. \u8aa4\u5dee\u9006\u4f1d\u64ad\n4. \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n\u3068\u3044\u3046\u611f\u3058\u3067\u3059\u3002\uff08\u3042\u307e\u308a\u81ea\u4fe1\u306a\u3044\u2026\uff09\n\n\u5404epoch\u3067\u3001\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u8aa4\u5dee\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u8aa4\u5dee\u3092\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002\n\u307e\u305f\u3001\u5206\u985e\u554f\u984c\u306a\u306e\u3067\u5206\u985e\u6b63\u89e3\u7387`accuracy`\u3082\u305d\u308c\u305e\u308c\u8a08\u7b97\u3057\u307e\u3059\u3002\n\n```\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n# Learning loop\nfor epoch in six.moves.range(1, n_epoch + 1):\n\n    print 'epoch', epoch\n\n    # training\n    perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n    sum_train_loss     = 0.0\n    sum_train_accuracy = 0.0\n    for i in six.moves.range(0, N, batchsize):\n\n        #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n        t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n\n        model.zerograds()            # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n        loss, acc = forward(x, t)    # \u9806\u4f1d\u642c\n        sum_train_loss      += float(cuda.to_cpu(loss.data)) * len(t)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n        sum_train_accuracy  += float(cuda.to_cpu(acc.data )) * len(t)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n        loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n        optimizer.update()           # \u6700\u9069\u5316\n\n    print('train mean loss={}, accuracy={}'.format(\n        sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n\n\n    # evaluation\n    sum_test_loss     = 0.0\n    sum_test_accuracy = 0.0\n    for i in six.moves.range(0, N_test, batchsize):\n\n        # all test data\n        x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n        t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n\n        loss, acc = forward(x, t, train=False)\n\n        sum_test_loss     += float(cuda.to_cpu(loss.data)) * len(t)\n        sum_test_accuracy += float(cuda.to_cpu(acc.data))  * len(t)\n\n    print(' test mean loss={}, accuracy={}'.format(\n        sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n\n#model\u3068optimizer\u3092\u4fdd\u5b58\nprint 'save the model'\nserializers.save_npz('pn_classifier_ffnn.model', model)\nprint 'save the optimizer'\nserializers.save_npz('pn_classifier_ffnn.state', optimizer)\n```\n\n#\u5b9f\u884c\u7d50\u679c\n\u6700\u7d42\u7684\u306a\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u5206\u985e\u6b63\u89e3\u7387\u306f`accuracy=0.716875001788`\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u3057\u304b\u3057\u3001\u5b66\u7fd2\u304c\u9032\u3080\u306b\u3064\u308c\u3066\u30c6\u30b9\u30c8\u8aa4\u5dee\u304c\u5897\u52a0\u3057\u3066\u304a\u308a\u3001\u904e\u5b66\u7fd2\u304c\u8d77\u304d\u3066\u3044\u307e\u3059...\n\n\u304a\u305d\u3089\u304f\u3066\u304d\u3068\u3046\u306b\u30e2\u30c7\u30eb\u3092\u7d44\u3093\u3060\u3053\u3068\u304c\u539f\u56e0\u3067\u3057\u3087\u3046\u3002\n\n```\n>python train.py --gpu 1 --data input.dat --units 1000\nvocab size: 4442\nepoch 1\ntrain mean loss=0.746377664579, accuracy=0.554684912523\n test mean loss=0.622971419245, accuracy=0.706875003874\nepoch 2\ntrain mean loss=0.50845754933, accuracy=0.759408453399\n test mean loss=0.503996372223, accuracy=0.761249992996\nepoch 3\ntrain mean loss=0.386604680468, accuracy=0.826067760105\n test mean loss=0.506066314876, accuracy=0.769374992698\nepoch 4\ntrain mean loss=0.301527346359, accuracy=0.870433726909\n test mean loss=0.553729468957, accuracy=0.774999994785\nepoch 5\ntrain mean loss=0.264981631757, accuracy=0.889085094432\n test mean loss=0.599407823756, accuracy=0.766874998808\nepoch 6\ntrain mean loss=0.231274759588, accuracy=0.901114668847\n test mean loss=0.68350501731, accuracy=0.755625002086\n\n...\n\nepoch 95\ntrain mean loss=0.0158744356008, accuracy=0.993598945303\n test mean loss=5.08019682765, accuracy=0.717499997467\nepoch 96\ntrain mean loss=0.0149783944279, accuracy=0.994261124581\n test mean loss=5.30629962683, accuracy=0.723749995232\nepoch 97\ntrain mean loss=0.00772037562047, accuracy=0.997351288256\n test mean loss=5.49559159577, accuracy=0.720624998212\nepoch 98\ntrain mean loss=0.00569957431572, accuracy=0.99834455516\n test mean loss=5.67661693692, accuracy=0.716875001788\nepoch 99\ntrain mean loss=0.00772406136085, accuracy=0.997240925267\n test mean loss=5.63734056056, accuracy=0.720000002533\nepoch 100\ntrain mean loss=0.0125463016702, accuracy=0.995916569395\n test mean loss=5.23713605106, accuracy=0.716875001788\nsave the model\nsave the optimizer\n\n```\n\n#\u304a\u308f\u308a\u306b\nChainer\u3092\u4f7f\u3063\u3066\u3001\u6587\u66f8\u5206\u985e\u306e\u305f\u3081\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\u904e\u5b66\u7fd2\u304c\u8d77\u304d\u306a\u3044\u3088\u3046\u306b\u3001\u30e2\u30c7\u30eb\u306e\u6539\u5584\u3092\u884c\u3044\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nChainer\u306e\u52c9\u5f37\u7528\u306b\u30b3\u30fc\u30c9\u3092\u898b\u3066\u307f\u305f\u3044\u65b9\u306f\u3001[\u3053\u3061\u3089](https://github.com/ichiroex/chainer-ffnn)\u304b\u3089\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002\n\n#\u53c2\u8003\u30da\u30fc\u30b8\n - [\u6df1\u5c64\u5b66\u7fd2\u3067\u30a2\u30cb\u30e1\u9854\u3092\u5206\u985e\u3059\u308b](http://qiita.com/hogefugabar/items/312707a09d29632e7288)\n - [Deep Learning \u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af Chainer \u3092\u4f7f\u3063\u305f\u753b\u50cf\u5206\u985e \u305d\u306e4](http://humansandcomputers.blogspot.jp/2015/08/auto-image-classification-w-deep-learning-framework-chainer-4.html)\n - [Chainer\u3067\u56de\u5e30\u554f\u984c\u3092\u3057\u305f](http://qiita.com/nzw0301/items/dbeb55bcd00b56b7fc87)\n", "tags": ["Chainer", "Python", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "\u6a5f\u68b0\u5b66\u7fd2", "DeepLearning"]}