{"context": "ChainerRL\u306e\u30d9\u30fc\u30bf\u7248\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3068\u306e\u3053\u3068\u3067\u3001\u65e9\u901f\u4f7f\u308f\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\u3053\u3053\u3067\u306f\u3001Quick Start Guide\u306e\u30bd\u30fc\u30b9\u3092\u53c2\u8003\u306b\u4e09\u76ee\u4e26\u3079\uff08\u25cb\u00d7\u30b2\u30fc\u30e0\uff09\u7528\u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u306f\u3058\u3081\u306bChainerRL\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\npip install chainerrl\n\ncmake\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u3082\u3057\u672a\u5c0e\u5165\u306e\u3068\u304d\u306f\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002\nbrew install cmake\n\n\u306a\u304a\u79c1\u306e\u74b0\u5883\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\nmacOS Sierra 10.12.3 (MBP Late2016)\nAnaconda3-4.1.0\nChainer 1.21.0\n\n\n\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u306e\u6e96\u5099\n\u30d7\u30ec\u30fc\u30e4\u30fc\u306e\u7a2e\u985e\uff08DQN\u3001\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3001\u4eba\u9593\u306a\u3069\uff09\u306b\u95a2\u308f\u3089\u305a\u3001\u25cb\u00d7\u30b2\u30fc\u30e0\u3092\u3059\u308b\u306b\u306f\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u306f\u3058\u3081\u306b\u4f5c\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u306a\u304a\u4eca\u56de\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u5206\u5272\u305b\u305a\u306b1\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u5168\u30bd\u30fc\u30b9\u3092\u66f8\u3044\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u5192\u982d\u306b\u5fc5\u8981\u30e9\u30a4\u30d6\u30e9\u30ea\u3082import\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\ndqn.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport numpy as np\n\n#\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\nclass Board():\n    def reset(self):\n        self.board = np.array([0] * 9, dtype=np.float32)\n        self.winner = None\n        self.missed = False\n        self.done = False\n\n    def move(self, act, turn):\n        if self.board[act] == 0:\n            self.board[act] = turn\n            self.check_winner()\n        else:\n            self.winner = turn*-1\n            self.missed = True\n            self.done = True\n\n    def check_winner(self):\n        win_conditions = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n        for cond in win_conditions:\n            if self.board[cond[0]] == self.board[cond[1]] == self.board[cond[2]]:\n                if self.board[cond[0]]!=0:\n                    self.winner=self.board[cond[0]]\n                    self.done = True\n                    return\n        if np.count_nonzero(self.board) == 9:\n            self.winner = 0\n            self.done = True\n\n    def get_empty_pos(self):\n        empties = np.where(self.board==0)[0]\n        if len(empties) > 0:\n            return np.random.choice(empties)\n        else:\n            return 0\n\n    def show(self):\n        row = \" {} | {} | {} \"\n        hr = \"\\n-----------\\n\"\n        tempboard = []\n        for i in self.board:\n            if i == 1:\n                tempboard.append(\"\u25cb\")\n            elif i == -1:\n                tempboard.append(\"\u00d7\")\n            else:\n                tempboard.append(\" \")\n        print((row + hr + row + hr + row).format(*tempboard))\n\n\n\u6a5f\u80fd\u306f\u4ee5\u4e0b\u306e5\u3064\u3002Python\u521d\u5fc3\u8005\u306b\u3064\u304d\u304a\u307c\u3064\u304b\u306a\u3044\u30bd\u30fc\u30b9\u3067\u6050\u7e2e\u3067\u3059\u304c\u3001\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u304b\u306f\u898b\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u308f\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\nreset \u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u306e\u521d\u671f\u5316\u3002\u5404\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u958b\u59cb\u524d\u306b\u5b9f\u884c\nmove \u624b\u306e\u914d\u7f6e\u306e\u5b9f\u884c\u3002\u914d\u7f6e\u5f8c\u306b\u52dd\u6557\u5224\u5b9a\u3084\u30df\u30b9\uff08\u7f6e\u3051\u306a\u3044\u30de\u30b9\u3078\u306e\u914d\u7f6e\uff09\u3001\u30b2\u30fc\u30e0\u7d42\u4e86\u3092\u5224\u5b9a\ncheck_winner \u52dd\u5229\u5224\u5b9a\nget_empty_pos \u914d\u7f6e\u53ef\u80fd\u306a\u30de\u30b9\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u3046\u3061\u4e00\u3064\u3092\u30e9\u30f3\u30c0\u30e0\u3067\u53d6\u5f97\u3002\u5f8c\u8ff0\u3057\u307e\u3059\u304c\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3055\u305b\u308b\u6642\u306b\u4f7f\u3044\u307e\u3059\nshow \u30dc\u30fc\u30c9\u306e\u72b6\u614b\u3092\u8868\u793a\u3057\u307e\u3059\u3002\u4eba\u9593\u3068\u306e\u5bfe\u6226\u7528\u3067\u3059\n\n\nExplorer\u6642\u306b\u4f7f\u7528\u3059\u308b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u306e\u6e96\u5099\n\u5c40\u6240\u89e3\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u3001\u305f\u307e\u306b\u5192\u967a\u3055\u305b\u308b\u306e\u304c\u3088\u3044\u305d\u3046\u3067\u3059\u3057\u3001Quickstart\u3082\u305d\u306e\u3088\u3046\u306a\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u306e\u3067\u3001\u3053\u3053\u3067\u3082\u305d\u308c\u306b\u306a\u3089\u3044\u307e\u3059\u3002Quickstart\u3067\u306fgym\u306e\u305d\u308c\u3092\u5229\u7528\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u3053\u3053\u3067\u306f\u81ea\u4f5c\u3059\u308b\u307b\u304b\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u672b\u5c3e\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\ndqn.py\n#explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\nclass RandomActor:\n    def __init__(self, board):\n        self.board = board\n        self.random_count = 0\n    def random_action_func(self):\n        self.random_count += 1\n        return self.board.get_empty_pos()\n\n\nrandom_action_func\u304c\u3053\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5927\u5207\u306a\u3068\u3053\u308d\u3067\u3059\u3002\u5148\u307b\u3069\u4f5c\u6210\u3057\u305fBoard\u306eget_empty_pos\u3092\u547c\u3073\u51fa\u3057\u3066\u3001\u914d\u7f6e\u53ef\u80fd\u306a\u30de\u30b9\u3092\u53d6\u5f97\u3057\u547c\u3073\u51fa\u3057\u5143\u306b\u8fd4\u5374\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u3042\u3068\u3067\u7d71\u8a08\u60c5\u5831\u3068\u3057\u3066\u3069\u306e\u7a0b\u5ea6\u3053\u306e\u95a2\u6570\u304c\u4f7f\u7528\u3055\u308c\u305f\u304b\uff08DQN\u304c\u8003\u3048\u305f\u624b\u3067\u306a\u304f\u30e9\u30f3\u30c0\u30e0\u3067\u8fd4\u3057\u305f\u304b\uff09\u3092\u628a\u63e1\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30a4\u30f3\u30af\u30ea\u30e1\u30f3\u30c8\u3057\u307e\u3059\u3002\u3053\u306e\u7a0b\u5ea6\u306e\u3082\u306e\u3092\u306a\u305c\u308f\u3056\u308f\u3056\u5225\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u305f\u304b\uff1f\u306b\u3064\u3044\u3066\u306f\u3053\u306e\u3042\u3068\u3067\u8aac\u660e\u3057\u307e\u3059\u3002\n\nQ-function\u306e\u6e96\u5099\nDQN\u3055\u305b\u308b\u4e0a\u3067\u306e\u672c\u4e38\u3067\u3042\u308a\u3001ChainerRL\u306e\u51fa\u756a\u3067\u3059\u3002\n\ndqn.py\n#Q\u95a2\u6570\nclass QFunction(chainer.Chain):\n    def __init__(self, obs_size, n_actions, n_hidden_channels=81):\n        super().__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_hidden_channels),\n            l3=L.Linear(n_hidden_channels, n_actions))\n    def __call__(self, x, test=False):\n        #-1\u3092\u6271\u3046\u306e\u3067leaky_relu\u3068\u3057\u305f\n        h = F.leaky_relu(self.l0(x))\n        h = F.leaky_relu(self.l1(h))\n        h = F.leaky_relu(self.l2(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l3(h))\n\n\n\u30fb\u30fb\u30fb\u4ee5\u4e0a\u3067\u3059\u3002\u3061\u3087\u3063\u3068\u62cd\u5b50\u629c\u3051\u3059\u308b\u307b\u3069\u30b7\u30f3\u30d7\u30eb\u3067\u3059\u306d\u3002\u666e\u901a\u306bNN\u3092\u5b9a\u7fa9\u3059\u308b\u306e\u3068\u307b\u3068\u3093\u3069\u5909\u308f\u308a\u3042\u308a\u307e\u305b\u3093\u3002\n\n\u74b0\u5883\u3068Agent\u306e\u6e96\u5099\n\u306a\u3093\u3068\u3001\u5468\u8fba\u306b\u4f5c\u308a\u8fbc\u3080\u3079\u304d\u3082\u306e\u306f\u3082\u3046\u3067\u304d\u3042\u304c\u3063\u305f\u306e\u3067\u3001\u3042\u3068\u306f\u74b0\u5883\u3068Agent\u3092\u6e96\u5099\u3057\u3066\u30b2\u30fc\u30e0\u306e\u9032\u884c\u3092\u4f5c\u308a\u8fbc\u3080\u306e\u307f\u3067\u3059\u3002\u307e\u305a\u306f\u74b0\u5883\u3068Agent\u304b\u3089\u3002\n\ndqn.py\n# \u30dc\u30fc\u30c9\u306e\u6e96\u5099\nb = Board()\n# explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u6e96\u5099\nra = RandomActor(b)\n# \u74b0\u5883\u3068\u884c\u52d5\u306e\u6b21\u5143\u6570\nobs_size = 9\nn_actions = 9\n# Q-function\u3068\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\nq_func = QFunction(obs_size, n_actions)\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n# \u5831\u916c\u306e\u5272\u5f15\u7387\ngamma = 0.95\n# Epsilon-greedy\u3092\u4f7f\u3063\u3066\u305f\u307e\u306b\u5192\u967a\u300250000\u30b9\u30c6\u30c3\u30d7\u3067end_epsilon\u3068\u306a\u308b\nexplorer = chainerrl.explorers.LinearDecayEpsilonGreedy(\n    start_epsilon=1.0, end_epsilon=0.3, decay_steps=50000, random_action_func=ra.random_action_func)\n# Experience Replay\u3068\u3044\u3046DQN\u3067\u7528\u3044\u308b\u5b66\u7fd2\u624b\u6cd5\u3067\u4f7f\u3046\u30d0\u30c3\u30d5\u30a1\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n# Agent\u306e\u751f\u6210\uff08replay_buffer\u7b49\u3092\u5171\u6709\u3059\u308b2\u3064\uff09\nagent_p1 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\nagent_p2 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n\n\n\u3055\u3066\u3001Epsilon-greedy\u306e\u3068\u3053\u308d\u3067\u3001\u5148\u307b\u3069\u4f5c\u3063\u305fRandomActor\u304c\u767b\u5834\u3057\u307e\u3059\u3002\u5192\u967a\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u95a2\u6570\u3078\u306e\u53c2\u7167\u3092explorer\u306b\u4e88\u3081\u6e21\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u305d\u306e\u95a2\u6570\u3078\u5f15\u6570\u3092\u6e21\u305b\u306a\u3044\u3063\u307d\u3044\uff1f\u306e\u3067\u3001\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u305fRandomActor\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30e1\u30f3\u30d0\u5909\u6570\u306b\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u3078\u306e\u53c2\u7167\u3092\u6e21\u3057\u3066\u304a\u304d\u3001explorer\u306e\u5185\u90e8\u51e6\u7406\u3067\u306f\u5f15\u6570\u306a\u3057\u306brandom_action_func\u3092\u547c\u3093\u3067\u3082\u3089\u3063\u3066\u5927\u4e08\u592b\u306b\u3057\u3066\u307f\u305f\u6b21\u7b2c\u3067\u3059\u3002\u3082\u3063\u3068\u30b9\u30de\u30fc\u30c8\u306a\u3084\u308a\u65b9\u304c\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3001\u3080\u3057\u308d\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u3002\n\u307e\u305f\u3001\u03b5-greedy\u306e\u3084\u308a\u65b9\u3092\u3001\u30b3\u30f3\u30b9\u30bf\u30f3\u30c8\u306a\u5024\u3068\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u5f90\u3005\u306b\u6e1b\u3089\u3057\u3066\u3044\u304f\u65b9\u5f0f\uff08LinearDecayEpsilonGreedy\uff09\u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u521d1.0\uff1d\u5e38\u306b\u30e9\u30f3\u30c0\u30e0\u304b\u3089\u306f\u3058\u3081\u3066\u300150000\u30b9\u30c6\u30c3\u30d7\u304b\u3051\u3066\u6700\u7d42\u7684\u306b0.3\u307e\u3067\u6e1b\u3089\u3057\u307e\u3059\u3002\u3053\u306e\u6570\u5b57\u3082\u59a5\u5f53\u304b\u3069\u3046\u304b\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u3044\u308d\u3044\u308d\u5909\u3048\u306a\u304c\u3089\u3084\u308b\u3068\u3044\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\nAgent\u306foptimizer\u3084replay_buffer\u3092\u5171\u6709\u3059\u308bP1\u3068P2\u3092\u751f\u6210\u3057\u4e92\u3044\u306b\u6226\u308f\u305b\u307e\u3059\u3002\n\n\u30b2\u30fc\u30e0\u9032\u884c\u90e8\u5206\u306e\u4f5c\u6210\n\u3082\u3046\u5b9f\u884c\u3057\u305f\u304f\u3066\u3060\u3044\u3076\u3046\u305a\u3046\u305a\u3057\u3066\u3044\u308b\u3053\u3068\u3068\u304a\u5bdf\u3057\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3092\u8ffd\u8a18\u3059\u308c\u3070\u5b9f\u884c\u3067\u304d\u307e\u3059\u306e\u3067\u3001\u3082\u3046\u3061\u3087\u3063\u3068\u30ac\u30de\u30f3\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\ndqn.py\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 20000\n#\u30ab\u30a6\u30f3\u30bf\u306e\u5ba3\u8a00\nmiss = 0\nwin = 0\ndraw = 0\n#\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u7e70\u308a\u8fd4\u3057\u5b9f\u884c\nfor i in range(1, n_episodes + 1):\n    b.reset()\n    reward = 0\n    agents = [agent_p1, agent_p2]\n    turn = np.random.choice([0, 1])\n    last_state = None\n    while not b.done:\n        #\u914d\u7f6e\u30de\u30b9\u53d6\u5f97\n        action = agents[turn].act_and_train(b.board.copy(), reward)\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n        #\u914d\u7f6e\u306e\u7d50\u679c\u3001\u7d42\u4e86\u6642\u306b\u306f\u5831\u916c\u3068\u30ab\u30a6\u30f3\u30bf\u306b\u5024\u3092\u30bb\u30c3\u30c8\u3057\u3066\u5b66\u7fd2\n        if b.done == True:\n            if b.winner == 1:\n                reward = 1\n                win += 1\n            elif b.winner == 0:\n                draw += 1\n            else:\n                reward = -1\n            if b.missed is True:\n                miss += 1\n            #\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\n            agents[turn].stop_episode_and_train(b.board.copy(), reward, True)\n            #\u76f8\u624b\u3082\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\u3002\u76f8\u624b\u306e\u30df\u30b9\u306f\u52dd\u5229\u3068\u3057\u3066\u5b66\u7fd2\u3057\u306a\u3044\u3088\u3046\u306b\n            if agents[1 if turn == 0 else 0].last_state is not None and b.missed is False:\n                #\u524d\u306e\u30bf\u30fc\u30f3\u3067\u3068\u3063\u3066\u304a\u3044\u305flast_state\u3092action\u5b9f\u884c\u5f8c\u306e\u72b6\u614b\u3068\u3057\u3066\u6e21\u3059\n                agents[1 if turn == 0 else 0].stop_episode_and_train(last_state, reward*-1, True)\n        else:\n            #\u5b66\u7fd2\u7528\u306b\u30bf\u30fc\u30f3\u6700\u5f8c\u306e\u72b6\u614b\u3092\u9000\u907f\n            last_state = b.board.copy()\n            #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n            b.board = b.board * -1\n            #\u30bf\u30fc\u30f3\u3092\u5207\u308a\u66ff\u3048\n            turn = 1 if turn == 0 else 0\n\n    #\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u9032\u6357\u8868\u793a\n    if i % 100 == 0:\n        print(\"episode:\", i, \" / rnd:\", ra.random_count, \" / miss:\", miss, \" / win:\", win, \" / draw:\", draw, \" / statistics:\", agent_p1.get_statistics(), \" / epsilon:\", agent_p1.explorer.epsilon)\n        #\u30ab\u30a6\u30f3\u30bf\u306e\u521d\u671f\u5316\n        miss = 0\n        win = 0\n        draw = 0\n        ra.random_count = 0\n    if i % 10000 == 0:\n        # 10000\u30a8\u30d4\u30bd\u30fc\u30c9\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n        agent_p1.save(\"result_\" + str(i))\n\nprint(\"Training finished.\")\n\n\n20000\u30b2\u30fc\u30e0\u3092\u7e70\u308a\u8fd4\u3059for\u6587\u3068\u30b2\u30fc\u30e0\u5185\u306e\u30bf\u30fc\u30f3\u3092\u7e70\u308a\u8fd4\u3059while\u6587\u306e\u5165\u308c\u5b50\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30dd\u30a4\u30f3\u30c8\u3068\u3057\u3066\u306f\u3001\u5148\u653b\u3082\u5f8c\u653b\u3082Agent\u81ea\u8eab\u3067\u3059\u3002\u3053\u306e\u30b2\u30fc\u30e0\u3067\u306f\u5185\u90e8\u7684\u306b\u25cb\u00d7\u306e\u5909\u308f\u308a\u306b\u81ea\u8eab\u306e\u624b\u30921\u3001\u76f8\u624b\u306e\u624b\u3092-1\u3068\u3057\u3066\u30dc\u30fc\u30c9\u306b\u914d\u7f6e\u3057\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u5148\u653b\u5f8c\u653b\u3069\u3061\u3089\u306e\u74b0\u5883\u30fb\u30a2\u30af\u30b7\u30e7\u30f3\u3082\u5b66\u7fd2\u3057\u305f\u3044\u306e\u3067\u3001\u30b2\u30fc\u30e0\u9032\u884c\u5185\u3067\u7b26\u53f7\u3092\u51fa\u3057\u5206\u3051\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5e38\u306b\u30dc\u30fc\u30c9\u306b\u306f1\u3092\u914d\u7f6e\u3059\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n\n\u5f53\u7136\u305d\u306e\u307e\u307e\u3060\u3068\u30dc\u30fc\u30c9\u304c1\u3060\u3089\u3051\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u306e\u3067\u3001\u30bf\u30fc\u30f3\u4ea4\u4ee3\u6642\u306b\u30dc\u30fc\u30c9\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3055\u305b\u3066\u3044\u307e\u3059\u3002\n        #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n        else:\n            b.board = b.board * -1\n\n\u305d\u3057\u3066\u6700\u5f8c\u306b\u3001\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3082ChainerRL\u304c\u4f5c\u3063\u3066\u304f\u308c\u308b\u3088\u3046\u306a\u306e\u3067\u300110000\u30a8\u30d4\u30bd\u30fc\u30c9\u6bce\u306b\u30a8\u30d4\u30bd\u30fc\u30c9\u6570\u3092\u672b\u5c3e\u306b\u4ed8\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u5c65\u6b74\u3092\u4fdd\u5b58\u3057\u3066\u3044\u304f\u3088\u3046\u306b\u3057\u307e\u3057\u305f\u3002\u540c\u3058experience\u3067\u5b66\u7fd2\u3055\u305b\u3066\u308b\u306e\u3067\u3001\u4fdd\u5b58\u3059\u308b\u306e\u306fagent_p1\u306e\u307f\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u5b66\u7fd2\u306e\u5b9f\u884c\n\u305d\u308c\u3067\u306f\u3001\u3044\u3056\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u30fb\u30fb\u30fb\uff01\n\u306f\u3058\u3081\u306e\u3046\u3061\u306fepsilon\u306e\u5024\u304c\u5927\u304d\u3044\u306e\u3067\u3001\u307b\u3068\u3093\u3069\u304c\u30e9\u30f3\u30c0\u30e0\u6253\u3061\uff08rnd\u306e\u5024\u304c\u30e9\u30f3\u30c0\u30e0\u3067\u6253\u3063\u305f\u56de\u6570\uff09\u3068\u306a\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u30df\u30b9\u3082\u5c11\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u5f90\u3005\u306b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u304c\u6e1b\u308b\u3068DQN\u3055\u3093\u304c\u81ea\u3089\u8003\u3048\u305f\u624b\u3067\u6253\u3064\u6a5f\u4f1a\u304c\u5897\u3048\u308b\u305f\u3081\u4e00\u6642\u7684\u306b\u30df\u30b9\u3082\u5897\u3048\u3066\u304d\u307e\u3059\u304c\u3001\u5b66\u7fd2\u304c\u9032\u3080\u3068\u305d\u308c\u3082\u53ce\u675f\u3057\u306615000\u56de\u3092\u8d85\u3048\u308b\u3068\u307b\u307c1\u6841\u524d\u534a\u306b\u306a\u308a\u307e\u3057\u305f\u3002\nepisode: 100  / rnd: 761  / miss: 1  / win: 85  / draw: 14  / statistics: [('average_q', 0.11951273068342624), ('average_loss', 0.09235552993858538)]  / epsilon: 0.994778\nepisode: 200  / rnd: 722  / miss: 3  / win: 85  / draw: 12  / statistics: [('average_q', 0.35500590929140996), ('average_loss', 0.12790488153218765)]  / epsilon: 0.9895\nepisode: 300  / rnd: 756  / miss: 6  / win: 82  / draw: 12  / statistics: [('average_q', 0.6269444783473722), ('average_loss', 0.12164947750267516)]  / epsilon: 0.984278\n\u3000\uff1a\uff08\u4e2d\u7565\uff09\nepisode: 19800  / rnd: 212  / miss: 1  / win: 69  / draw: 30  / statistics: [('average_q', 0.49387913595157096), ('average_loss', 0.07891365175610675)]  / epsilon: 0.3\nepisode: 19900  / rnd: 229  / miss: 1  / win: 61  / draw: 38  / statistics: [('average_q', 0.49195677296191365), ('average_loss', 0.07796313042393459)]  / epsilon: 0.3\nepisode: 20000  / rnd: 216  / miss: 0  / win: 70  / draw: 30  / statistics: [('average_q', 0.509864846571749), ('average_loss', 0.07866546801090374)]  / epsilon: 0.3\nTraining finished.\n\n\n\u3044\u3056\u81ea\u5206\u3068\u5bfe\u6226\uff01\n\u30df\u30b9\u305b\u305a\u6253\u3066\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3057\u3001\u305f\u307e\u306b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3057\u3066\u3044\u306a\u304c\u3089\u3082\u7d50\u69cbDraw\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u5f37\u3055\u3092\u78ba\u304b\u3081\u308b\u3079\u304f\u81ea\u5206\u3068\u5bfe\u6226\u3057\u3066\u307f\u307e\u3059\u3002\n\nHumanPlayer\u306e\u4f5c\u6210\n\u307e\u305a\u306f\u4eba\u9593\u304c\u6253\u3066\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3068\u3057\u3066\u3001HumanPlayer\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\ndqn.py\n#\u4eba\u9593\u306e\u30d7\u30ec\u30fc\u30e4\u30fc\nclass HumanPlayer:\n    def act(self, board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Please enter 1-9: \")\n                act = int(act)\n                if act >= 1 and act <= 9 and board[act-1] == 0:\n                    valid = True\n                    return act-1\n                else:\n                    print (\"Invalid move\")\n            except Exception as e:\n                    print (act +  \" is invalid\")\n\n\n\n\u5bfe\u4eba\u30b2\u30fc\u30e0\u9032\u884c\u90e8\u5206\u306e\u4f5c\u6210\n\u9032\u884c\u90e8\u5206\u3067\u3059\u3002DQN\u306e\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f1\u3001\u4eba\u9593\u306f-1\u306b\u306a\u308b\u3088\u3046\u306b\u56fa\u5b9a\u3057\u3064\u3064\u3001\u5148\u653b\u30fb\u5f8c\u653b\u306f\u30a8\u30d4\u30bd\u30fc\u30c9\u958b\u59cb\u524d\u306b\u300cDQN\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u5148\u653b\u304b\u3069\u3046\u304b\u300d\u3068\u3044\u3046\u3053\u3068\u3092\u6c7a\u5b9a\u3057\u3066\u521d\u56de\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b/\u3057\u306a\u3044\u3092\u5236\u5fa1\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u95a2\u4fc2\u3067\u3001\u5148\u653b\u30fb\u5f8c\u653b\u306b\u95a2\u308f\u3089\u305a\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f\u5e38\u306b\u25cb\u3067\u4eba\u9593\u306f\u5e38\u306b\u00d7\u3067\u3059\u3002\n\ndqn.py\n#\u691c\u8a3c\nhuman_player = HumanPlayer()\nfor i in range(10):\n    b.reset()\n    dqn_first = np.random.choice([True, False])\n    while not b.done:\n        #DQN\n        if dqn_first or np.count_nonzero(b.board) > 0:\n            b.show()\n            action = agent_p1.act(b.board.copy())\n            b.move(action, 1)\n            if b.done == True:\n                if b.winner == 1:\n                    print(\"DQN Win\")\n                elif b.winner == 0:\n                    print(\"Draw\")\n                else:\n                    print(\"DQN Missed\")\n                agent_p1.stop_episode()\n                continue\n        #\u4eba\u9593\n        b.show()\n        action = human_player.act(b.board.copy())\n        b.move(action, -1)\n        if b.done == True:\n            if b.winner == -1:\n                print(\"HUMAN Win\")\n            elif b.winner == 0:\n                print(\"Draw\")\n            agent_p1.stop_episode()\n\nprint(\"Test finished.\")\n\n\n\u30dd\u30a4\u30f3\u30c8\u306f\u3001agent\u306f\u3053\u3053\u3067\u306f\u5b66\u7fd2\u3057\u306a\u3044\u306e\u3067\u3001act()\u304a\u3088\u3073stop_episode()\u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u70b9\u3067\u3057\u3087\u3046\u304b\u3002\u3053\u308c\u3082Quickstart\u901a\u308a\u3067\u3059\u3002\n\u3055\u3066\u3001\u3053\u308c\u3067\u5bfe\u6226\u306e\u6e96\u5099\u306f\u6574\u3044\u307e\u3057\u305f\u304c\u3001\u518d\u5ea62\u4e07\u56de\u5b66\u7fd2\u3055\u305b\u308b\u306e\u3082\u4e0d\u6bdb\u306a\u306e\u3067\u4fdd\u5b58\u3057\u305fAgent\u3092\u8aad\u307f\u8fbc\u3080\u3088\u3046\u306b\u3057\u307e\u3059\u3002\u672c\u5f53\u306fdqn.py\u306e\u5b9f\u884c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5b66\u7fd2\u3059\u308b/\u65e2\u5b58\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u3092\u5207\u308a\u66ff\u3048\u308b\u306e\u304c\u30b9\u30de\u30fc\u30c8\u3067\u3059\u304c\u3001\u65e9\u304f\u30d7\u30ec\u30a4\u3057\u305f\u3044\u306e\u3067\u4ee5\u4e0b\u306e\u901a\u308a\u5b66\u7fd2\u30a8\u30d4\u30bd\u30fc\u30c9\u6570\u30920\u306b\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u51e6\u7406\u3092\u30b9\u30ad\u30c3\u30d7\u3057\u307e\u3059\u3002\n\ndqn.py\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 0\n\n\n\u305d\u3057\u3066\u3001\u5b66\u7fd2\u51e6\u7406\u306e\u5b8c\u4e86\u5f8c\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u8ffd\u52a0\u3057\u3066\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\ndqn.py\nprint(\"Training finished.\")\n\nagent_p1.load(\"result_20000\")  #\u2190\u3053\u308c\u3092\u8ffd\u52a0\n\n\n\u6e96\u5099\u304c\u3067\u304d\u305f\u3089\u3001\u3044\u3056\u5bfe\u6226\u3067\u3059\uff01\nTraining finished.\n   |   |   \n-----------\n   |   |   \n-----------\n   |   |   \n   |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   |   \nPlease enter 1-9: 1\n \u00d7 |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   |   \n \u00d7 |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   | \u25cb \nPlease enter 1-9: 8\n\n\u5bfe\u6226\u3067\u304d\u307e\u3059\u306d\uff01\u3084\u3063\u305f\u30fc\uff01\uff01\n\n\u304a\u308f\u308a\u306b\nDQN\u3082Python\u3082\u304b\u3058\u308a\u305f\u3066\u306e\u79c1\u306b\u304a\u4ed8\u304d\u5408\u3044\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\u300c\u30eb\u30fc\u30eb\u3092\u6559\u3048\u306a\u304f\u3066\u3082\u307b\u307c\u9593\u9055\u3044\u306a\u304f\u30fb\u5b9a\u77f3\u901a\u308a\u6253\u3066\u308b\u300d\u3068\u3044\u3046\u3068\u3053\u308d\u307e\u3067\u6210\u9577\u3057\u3066\u304f\u308c\u305f\u306e\u306f\u306a\u304b\u306a\u304b\u5b09\u3057\u3044\u3082\u306e\u3067\u3059\u3002\n\u3057\u304b\u3082\u3001Chainer\u3092\u305d\u306e\u307e\u307e\u5229\u7528\u3057\u3066DQN\u3092\u5b9f\u88c5\u3059\u308b\u3088\u308a\u3001\u306f\u308b\u304b\u306b\u30b9\u30c3\u30ad\u30ea\u3068\u3057\u307e\u3057\u305f\u3002ChainerRL\u3059\u3054\u3044\uff01\uff01\u898b\u901a\u3057\u304c\u3088\u304f\u306a\u3063\u305f\u3053\u3068\u3067\u3001\u3044\u308d\u3044\u308d\u6539\u826f\u3057\u3088\u3046\u3068\u3057\u3066\u30d0\u30b0\u3092\u6df7\u5728\u3055\u305b\u308b\u30fb\u30fb\u30fb\u3068\u3044\u3063\u305f\u3053\u3068\u304c\u9632\u3052\u305d\u3046\u3067\u3059\u3002\n\u9593\u9055\u3063\u3066\u3044\u305f\u308a\u300c\u672c\u6765\u3053\u3046\u3059\u3079\u304d\u300d\u300c\u3053\u3046\u3057\u305f\u65b9\u304c\u5b66\u7fd2\u304c\u9032\u3080\u300d\u300c\u3053\u308c\u3060\u3068\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u3088\u300d\u307f\u305f\u3044\u306a\u3082\u306e\u304c\u5c71\u307b\u3069\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3001\u3044\u308d\u3044\u308d\u3054\u6307\u6458\u3092\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\u4f55\u5352\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\u7279\u306b\u8ab2\u984c\u306b\u601d\u3063\u3066\u3044\u308b\u306e\u306f\u3001Agent\u306e\u6253\u3061\u65b9\u304c\u307b\u307c\u6bce\u56de\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u3053\u3068\u3067\u3059\u3002\u3082\u3063\u3068\u5192\u967a\u3055\u305b\u306a\u3051\u308c\u3070\u3044\u3051\u306a\u3044\u306e\u304b\u306a\uff1f35\u4e07\u30a8\u30d4\u30bd\u30fc\u30c9\u5b66\u7fd2\u3055\u305b\u308b\u3068\u3001\u5b9a\u77f3\u901a\u308a\u6253\u3064\u306e\u3067\u5f37\u304f\u3001\u03b5\u30920\u306b\u3059\u308b\u3068\u307b\u307c\u6bce\u56deDraw\u3068\u306a\u308b\u306e\u3067\u3001\u826f\u3044\u3053\u3068\u3067\u306f\u3042\u308b\u306e\u3067\u3059\u304c\u3002\u3002\u306a\u304a15\u4e07\u301c20\u4e07\u30a8\u30d4\u30bd\u30fc\u30c9\u304b\u3089\u306f\u7d50\u679c\u3082loss\u3082\u4e00\u5b9a\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n\u30bd\u30fc\u30b9\u5168\u4f53\n\u4e00\u5fdc\u3001\u30bd\u30fc\u30b9\u306e\u5168\u4f53\u3092\u63b2\u8f09\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u74b0\u5883\u304c\u63c3\u3063\u3066\u3044\u308c\u3070\u30b3\u30d4\u30da\u3057\u3066\u3059\u3050\u306b\u52d5\u304b\u305b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\ndqn.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport numpy as np\n\n#\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\nclass Board():\n    def reset(self):\n        self.board = np.array([0] * 9, dtype=np.float32)\n        self.winner = None\n        self.missed = False\n        self.done = False\n\n    def move(self, act, turn):\n        if self.board[act] == 0:\n            self.board[act] = turn\n            self.check_winner()\n        else:\n            self.winner = turn*-1\n            self.missed = True\n            self.done = True\n\n    def check_winner(self):\n        win_conditions = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n        for cond in win_conditions:\n            if self.board[cond[0]] == self.board[cond[1]] == self.board[cond[2]]:\n                if self.board[cond[0]]!=0:\n                    self.winner=self.board[cond[0]]\n                    self.done = True\n                    return\n        if np.count_nonzero(self.board) == 9:\n            self.winner = 0\n            self.done = True\n\n    def get_empty_pos(self):\n        empties = np.where(self.board==0)[0]\n        if len(empties) > 0:\n            return np.random.choice(empties)\n        else:\n            return 0\n\n    def show(self):\n        row = \" {} | {} | {} \"\n        hr = \"\\n-----------\\n\"\n        tempboard = []\n        for i in self.board:\n            if i == 1:\n                tempboard.append(\"\u25cb\")\n            elif i == -1:\n                tempboard.append(\"\u00d7\")\n            else:\n                tempboard.append(\" \")\n        print((row + hr + row + hr + row).format(*tempboard))\n\n#explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\nclass RandomActor:\n    def __init__(self, board):\n        self.board = board\n        self.random_count = 0\n    def random_action_func(self):\n        self.random_count += 1\n        return self.board.get_empty_pos()\n\n#Q\u95a2\u6570\nclass QFunction(chainer.Chain):\n    def __init__(self, obs_size, n_actions, n_hidden_channels=81):\n        super().__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_hidden_channels),\n            l3=L.Linear(n_hidden_channels, n_actions))\n    def __call__(self, x, test=False):\n        #-1\u3092\u6271\u3046\u306e\u3067leaky_relu\u3068\u3057\u305f\n        h = F.leaky_relu(self.l0(x))\n        h = F.leaky_relu(self.l1(h))\n        h = F.leaky_relu(self.l2(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l3(h))\n\n# \u30dc\u30fc\u30c9\u306e\u6e96\u5099\nb = Board()\n# explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u6e96\u5099\nra = RandomActor(b)\n# \u74b0\u5883\u3068\u884c\u52d5\u306e\u6b21\u5143\u6570\nobs_size = 9\nn_actions = 9\n# Q-function\u3068\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\nq_func = QFunction(obs_size, n_actions)\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n# \u5831\u916c\u306e\u5272\u5f15\u7387\ngamma = 0.95\n# Epsilon-greedy\u3092\u4f7f\u3063\u3066\u305f\u307e\u306b\u5192\u967a\u300250000\u30b9\u30c6\u30c3\u30d7\u3067end_epsilon\u3068\u306a\u308b\nexplorer = chainerrl.explorers.LinearDecayEpsilonGreedy(\n    start_epsilon=1.0, end_epsilon=0.3, decay_steps=50000, random_action_func=ra.random_action_func)\n# Experience Replay\u3068\u3044\u3046DQN\u3067\u7528\u3044\u308b\u5b66\u7fd2\u624b\u6cd5\u3067\u4f7f\u3046\u30d0\u30c3\u30d5\u30a1\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n# Agent\u306e\u751f\u6210\uff08replay_buffer\u7b49\u3092\u5171\u6709\u3059\u308b2\u3064\uff09\nagent_p1 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\nagent_p2 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 20000\n#\u30ab\u30a6\u30f3\u30bf\u306e\u5ba3\u8a00\nmiss = 0\nwin = 0\ndraw = 0\n#\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u7e70\u308a\u8fd4\u3057\u5b9f\u884c\nfor i in range(1, n_episodes + 1):\n    b.reset()\n    reward = 0\n    agents = [agent_p1, agent_p2]\n    turn = np.random.choice([0, 1])\n    last_state = None\n    while not b.done:\n        #\u914d\u7f6e\u30de\u30b9\u53d6\u5f97\n        action = agents[turn].act_and_train(b.board.copy(), reward)\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n        #\u914d\u7f6e\u306e\u7d50\u679c\u3001\u7d42\u4e86\u6642\u306b\u306f\u5831\u916c\u3068\u30ab\u30a6\u30f3\u30bf\u306b\u5024\u3092\u30bb\u30c3\u30c8\u3057\u3066\u5b66\u7fd2\n        if b.done == True:\n            if b.winner == 1:\n                reward = 1\n                win += 1\n            elif b.winner == 0:\n                draw += 1\n            else:\n                reward = -1\n            if b.missed is True:\n                miss += 1\n            #\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\n            agents[turn].stop_episode_and_train(b.board.copy(), reward, True)\n            #\u76f8\u624b\u3082\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\u3002\u76f8\u624b\u306e\u30df\u30b9\u306f\u52dd\u5229\u3068\u3057\u3066\u5b66\u7fd2\u3057\u306a\u3044\u3088\u3046\u306b\n            if agents[1 if turn == 0 else 0].last_state is not None and b.missed is False:\n                #\u524d\u306e\u30bf\u30fc\u30f3\u3067\u3068\u3063\u3066\u304a\u3044\u305flast_state\u3092action\u5b9f\u884c\u5f8c\u306e\u72b6\u614b\u3068\u3057\u3066\u6e21\u3059\n                agents[1 if turn == 0 else 0].stop_episode_and_train(last_state, reward*-1, True)\n        else:\n            #\u5b66\u7fd2\u7528\u306b\u30bf\u30fc\u30f3\u6700\u5f8c\u306e\u72b6\u614b\u3092\u9000\u907f\n            last_state = b.board.copy()\n            #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n            b.board = b.board * -1\n            #\u30bf\u30fc\u30f3\u3092\u5207\u308a\u66ff\u3048\n            turn = 1 if turn == 0 else 0\n\n    #\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u9032\u6357\u8868\u793a\n    if i % 100 == 0:\n        print(\"episode:\", i, \" / rnd:\", ra.random_count, \" / miss:\", miss, \" / win:\", win, \" / draw:\", draw, \" / statistics:\", agent_p1.get_statistics(), \" / epsilon:\", agent_p1.explorer.epsilon)\n        #\u30ab\u30a6\u30f3\u30bf\u306e\u521d\u671f\u5316\n        miss = 0\n        win = 0\n        draw = 0\n        ra.random_count = 0\n    if i % 10000 == 0:\n        # 10000\u30a8\u30d4\u30bd\u30fc\u30c9\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n        agent_p1.save(\"result_\" + str(i))\n\nprint(\"Training finished.\")\n\n#\u4eba\u9593\u306e\u30d7\u30ec\u30fc\u30e4\u30fc\nclass HumanPlayer:\n    def act(self, board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Please enter 1-9: \")\n                act = int(act)\n                if act >= 1 and act <= 9 and board[act-1] == 0:\n                    valid = True\n                    return act-1\n                else:\n                    print(\"Invalid move\")\n            except Exception as e:\n                print(act +  \" is invalid\")\n\n#\u691c\u8a3c\nhuman_player = HumanPlayer()\nfor i in range(10):\n    b.reset()\n    dqn_first = np.random.choice([True, False])\n    while not b.done:\n        #DQN\n        if dqn_first or np.count_nonzero(b.board) > 0:\n            b.show()\n            action = agent_p1.act(b.board.copy())\n            b.move(action, 1)\n            if b.done == True:\n                if b.winner == 1:\n                    print(\"DQN Win\")\n                elif b.winner == 0:\n                    print(\"Draw\")\n                else:\n                    print(\"DQN Missed\")\n                agent_p1.stop_episode()\n                continue\n        #\u4eba\u9593\n        b.show()\n        action = human_player.act(b.board.copy())\n        b.move(action, -1)\n        if b.done == True:\n            if b.winner == -1:\n                print(\"HUMAN Win\")\n            elif b.winner == 0:\n                print(\"Draw\")\n            agent_p1.stop_episode()\n\nprint(\"Test finished.\")\n\n\n\n\u53c2\u8003\u30b5\u30a4\u30c8\n\nChainerRL\nChainer\u3067DQN\u3002\u5f37\u5316\u5b66\u7fd2\u3092\u4e09\u76ee\u4e26\u3079\u3067\u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3002\uff08Deep Q Network\u3001Q-Learning\u3001\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\uff09\nDQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f\n\n[ChainerRL\u306e\u30d9\u30fc\u30bf\u7248](https://github.com/pfnet/chainerrl)\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3068\u306e\u3053\u3068\u3067\u3001\u65e9\u901f\u4f7f\u308f\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\u3053\u3053\u3067\u306f\u3001[Quick Start Guide](https://github.com/pfnet/chainerrl/blob/master/examples/quickstart/quickstart.ipynb)\u306e\u30bd\u30fc\u30b9\u3092\u53c2\u8003\u306b\u4e09\u76ee\u4e26\u3079\uff08\u25cb\u00d7\u30b2\u30fc\u30e0\uff09\u7528\u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u306f\u3058\u3081\u306bChainerRL\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\n```\npip install chainerrl\n```\n\ncmake\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u3082\u3057\u672a\u5c0e\u5165\u306e\u3068\u304d\u306f\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n\n```\nbrew install cmake\n```\n\n\u306a\u304a\u79c1\u306e\u74b0\u5883\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n- macOS Sierra 10.12.3 (MBP Late2016)\n- Anaconda3-4.1.0\n- Chainer 1.21.0\n\n# \u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u306e\u6e96\u5099\n\u30d7\u30ec\u30fc\u30e4\u30fc\u306e\u7a2e\u985e\uff08DQN\u3001\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3001\u4eba\u9593\u306a\u3069\uff09\u306b\u95a2\u308f\u3089\u305a\u3001\u25cb\u00d7\u30b2\u30fc\u30e0\u3092\u3059\u308b\u306b\u306f\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u306f\u3058\u3081\u306b\u4f5c\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u306a\u304a\u4eca\u56de\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u5206\u5272\u305b\u305a\u306b1\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u5168\u30bd\u30fc\u30b9\u3092\u66f8\u3044\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u5192\u982d\u306b\u5fc5\u8981\u30e9\u30a4\u30d6\u30e9\u30ea\u3082import\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```Python:dqn.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport numpy as np\n\n#\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\nclass Board():\n    def reset(self):\n        self.board = np.array([0] * 9, dtype=np.float32)\n        self.winner = None\n        self.missed = False\n        self.done = False\n\n    def move(self, act, turn):\n        if self.board[act] == 0:\n            self.board[act] = turn\n            self.check_winner()\n        else:\n            self.winner = turn*-1\n            self.missed = True\n            self.done = True\n\n    def check_winner(self):\n        win_conditions = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n        for cond in win_conditions:\n            if self.board[cond[0]] == self.board[cond[1]] == self.board[cond[2]]:\n                if self.board[cond[0]]!=0:\n                    self.winner=self.board[cond[0]]\n                    self.done = True\n                    return\n        if np.count_nonzero(self.board) == 9:\n            self.winner = 0\n            self.done = True\n\n    def get_empty_pos(self):\n        empties = np.where(self.board==0)[0]\n        if len(empties) > 0:\n            return np.random.choice(empties)\n        else:\n            return 0\n    \n    def show(self):\n        row = \" {} | {} | {} \"\n        hr = \"\\n-----------\\n\"\n        tempboard = []\n        for i in self.board:\n            if i == 1:\n                tempboard.append(\"\u25cb\")\n            elif i == -1:\n                tempboard.append(\"\u00d7\")\n            else:\n                tempboard.append(\" \")\n        print((row + hr + row + hr + row).format(*tempboard))\n```\n\n\u6a5f\u80fd\u306f\u4ee5\u4e0b\u306e5\u3064\u3002Python\u521d\u5fc3\u8005\u306b\u3064\u304d\u304a\u307c\u3064\u304b\u306a\u3044\u30bd\u30fc\u30b9\u3067\u6050\u7e2e\u3067\u3059\u304c\u3001\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u304b\u306f\u898b\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u308f\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n- reset \u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u306e\u521d\u671f\u5316\u3002\u5404\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u958b\u59cb\u524d\u306b\u5b9f\u884c\n- move \u624b\u306e\u914d\u7f6e\u306e\u5b9f\u884c\u3002\u914d\u7f6e\u5f8c\u306b\u52dd\u6557\u5224\u5b9a\u3084\u30df\u30b9\uff08\u7f6e\u3051\u306a\u3044\u30de\u30b9\u3078\u306e\u914d\u7f6e\uff09\u3001\u30b2\u30fc\u30e0\u7d42\u4e86\u3092\u5224\u5b9a\n- check_winner \u52dd\u5229\u5224\u5b9a\n- get_empty_pos \u914d\u7f6e\u53ef\u80fd\u306a\u30de\u30b9\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u3046\u3061\u4e00\u3064\u3092\u30e9\u30f3\u30c0\u30e0\u3067\u53d6\u5f97\u3002\u5f8c\u8ff0\u3057\u307e\u3059\u304c\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3055\u305b\u308b\u6642\u306b\u4f7f\u3044\u307e\u3059\n- show \u30dc\u30fc\u30c9\u306e\u72b6\u614b\u3092\u8868\u793a\u3057\u307e\u3059\u3002\u4eba\u9593\u3068\u306e\u5bfe\u6226\u7528\u3067\u3059\n\n# Explorer\u6642\u306b\u4f7f\u7528\u3059\u308b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u306e\u6e96\u5099\n\u5c40\u6240\u89e3\u306b\u9665\u3089\u306a\u3044\u3088\u3046\u3001\u305f\u307e\u306b\u5192\u967a\u3055\u305b\u308b\u306e\u304c\u3088\u3044\u305d\u3046\u3067\u3059\u3057\u3001Quickstart\u3082\u305d\u306e\u3088\u3046\u306a\u5b9f\u88c5\u306b\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u306e\u3067\u3001\u3053\u3053\u3067\u3082\u305d\u308c\u306b\u306a\u3089\u3044\u307e\u3059\u3002Quickstart\u3067\u306fgym\u306e\u305d\u308c\u3092\u5229\u7528\u3057\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u3053\u3053\u3067\u306f\u81ea\u4f5c\u3059\u308b\u307b\u304b\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u672b\u5c3e\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n```Python:dqn.py\n#explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\nclass RandomActor:\n    def __init__(self, board):\n        self.board = board\n        self.random_count = 0\n    def random_action_func(self):\n        self.random_count += 1\n        return self.board.get_empty_pos()\n```\n\nrandom_action_func\u304c\u3053\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5927\u5207\u306a\u3068\u3053\u308d\u3067\u3059\u3002\u5148\u307b\u3069\u4f5c\u6210\u3057\u305fBoard\u306eget_empty_pos\u3092\u547c\u3073\u51fa\u3057\u3066\u3001\u914d\u7f6e\u53ef\u80fd\u306a\u30de\u30b9\u3092\u53d6\u5f97\u3057\u547c\u3073\u51fa\u3057\u5143\u306b\u8fd4\u5374\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u3042\u3068\u3067\u7d71\u8a08\u60c5\u5831\u3068\u3057\u3066\u3069\u306e\u7a0b\u5ea6\u3053\u306e\u95a2\u6570\u304c\u4f7f\u7528\u3055\u308c\u305f\u304b\uff08DQN\u304c\u8003\u3048\u305f\u624b\u3067\u306a\u304f\u30e9\u30f3\u30c0\u30e0\u3067\u8fd4\u3057\u305f\u304b\uff09\u3092\u628a\u63e1\u3067\u304d\u308b\u3088\u3046\u306b\u3001\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30a4\u30f3\u30af\u30ea\u30e1\u30f3\u30c8\u3057\u307e\u3059\u3002\u3053\u306e\u7a0b\u5ea6\u306e\u3082\u306e\u3092\u306a\u305c\u308f\u3056\u308f\u3056\u5225\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u305f\u304b\uff1f\u306b\u3064\u3044\u3066\u306f\u3053\u306e\u3042\u3068\u3067\u8aac\u660e\u3057\u307e\u3059\u3002\n\n# Q-function\u306e\u6e96\u5099\n\nDQN\u3055\u305b\u308b\u4e0a\u3067\u306e\u672c\u4e38\u3067\u3042\u308a\u3001ChainerRL\u306e\u51fa\u756a\u3067\u3059\u3002\n\n```Python:dqn.py\n#Q\u95a2\u6570\nclass QFunction(chainer.Chain):\n    def __init__(self, obs_size, n_actions, n_hidden_channels=81):\n        super().__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_hidden_channels),\n            l3=L.Linear(n_hidden_channels, n_actions))\n    def __call__(self, x, test=False):\n        #-1\u3092\u6271\u3046\u306e\u3067leaky_relu\u3068\u3057\u305f\n        h = F.leaky_relu(self.l0(x))\n        h = F.leaky_relu(self.l1(h))\n        h = F.leaky_relu(self.l2(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l3(h))\n```\n\n\u30fb\u30fb\u30fb\u4ee5\u4e0a\u3067\u3059\u3002\u3061\u3087\u3063\u3068\u62cd\u5b50\u629c\u3051\u3059\u308b\u307b\u3069\u30b7\u30f3\u30d7\u30eb\u3067\u3059\u306d\u3002\u666e\u901a\u306bNN\u3092\u5b9a\u7fa9\u3059\u308b\u306e\u3068\u307b\u3068\u3093\u3069\u5909\u308f\u308a\u3042\u308a\u307e\u305b\u3093\u3002\n\n# \u74b0\u5883\u3068Agent\u306e\u6e96\u5099\n\n\u306a\u3093\u3068\u3001\u5468\u8fba\u306b\u4f5c\u308a\u8fbc\u3080\u3079\u304d\u3082\u306e\u306f\u3082\u3046\u3067\u304d\u3042\u304c\u3063\u305f\u306e\u3067\u3001\u3042\u3068\u306f\u74b0\u5883\u3068Agent\u3092\u6e96\u5099\u3057\u3066\u30b2\u30fc\u30e0\u306e\u9032\u884c\u3092\u4f5c\u308a\u8fbc\u3080\u306e\u307f\u3067\u3059\u3002\u307e\u305a\u306f\u74b0\u5883\u3068Agent\u304b\u3089\u3002\n\n```Python:dqn.py\n# \u30dc\u30fc\u30c9\u306e\u6e96\u5099\nb = Board()\n# explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u6e96\u5099\nra = RandomActor(b)\n# \u74b0\u5883\u3068\u884c\u52d5\u306e\u6b21\u5143\u6570\nobs_size = 9\nn_actions = 9\n# Q-function\u3068\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\nq_func = QFunction(obs_size, n_actions)\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n# \u5831\u916c\u306e\u5272\u5f15\u7387\ngamma = 0.95\n# Epsilon-greedy\u3092\u4f7f\u3063\u3066\u305f\u307e\u306b\u5192\u967a\u300250000\u30b9\u30c6\u30c3\u30d7\u3067end_epsilon\u3068\u306a\u308b\nexplorer = chainerrl.explorers.LinearDecayEpsilonGreedy(\n    start_epsilon=1.0, end_epsilon=0.3, decay_steps=50000, random_action_func=ra.random_action_func)\n# Experience Replay\u3068\u3044\u3046DQN\u3067\u7528\u3044\u308b\u5b66\u7fd2\u624b\u6cd5\u3067\u4f7f\u3046\u30d0\u30c3\u30d5\u30a1\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n# Agent\u306e\u751f\u6210\uff08replay_buffer\u7b49\u3092\u5171\u6709\u3059\u308b2\u3064\uff09\nagent_p1 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\nagent_p2 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n```\n\n\u3055\u3066\u3001Epsilon-greedy\u306e\u3068\u3053\u308d\u3067\u3001\u5148\u307b\u3069\u4f5c\u3063\u305fRandomActor\u304c\u767b\u5834\u3057\u307e\u3059\u3002\u5192\u967a\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u95a2\u6570\u3078\u306e\u53c2\u7167\u3092explorer\u306b\u4e88\u3081\u6e21\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u305d\u306e\u95a2\u6570\u3078\u5f15\u6570\u3092\u6e21\u305b\u306a\u3044\u3063\u307d\u3044\uff1f\u306e\u3067\u3001\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u305fRandomActor\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30e1\u30f3\u30d0\u5909\u6570\u306b\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\u3078\u306e\u53c2\u7167\u3092\u6e21\u3057\u3066\u304a\u304d\u3001explorer\u306e\u5185\u90e8\u51e6\u7406\u3067\u306f\u5f15\u6570\u306a\u3057\u306brandom_action_func\u3092\u547c\u3093\u3067\u3082\u3089\u3063\u3066\u5927\u4e08\u592b\u306b\u3057\u3066\u307f\u305f\u6b21\u7b2c\u3067\u3059\u3002\u3082\u3063\u3068\u30b9\u30de\u30fc\u30c8\u306a\u3084\u308a\u65b9\u304c\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3001\u3080\u3057\u308d\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u3002\n\n\u307e\u305f\u3001\u03b5-greedy\u306e\u3084\u308a\u65b9\u3092\u3001\u30b3\u30f3\u30b9\u30bf\u30f3\u30c8\u306a\u5024\u3068\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u5f90\u3005\u306b\u6e1b\u3089\u3057\u3066\u3044\u304f\u65b9\u5f0f\uff08LinearDecayEpsilonGreedy\uff09\u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u521d1.0\uff1d\u5e38\u306b\u30e9\u30f3\u30c0\u30e0\u304b\u3089\u306f\u3058\u3081\u3066\u300150000\u30b9\u30c6\u30c3\u30d7\u304b\u3051\u3066\u6700\u7d42\u7684\u306b0.3\u307e\u3067\u6e1b\u3089\u3057\u307e\u3059\u3002\u3053\u306e\u6570\u5b57\u3082\u59a5\u5f53\u304b\u3069\u3046\u304b\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u3044\u308d\u3044\u308d\u5909\u3048\u306a\u304c\u3089\u3084\u308b\u3068\u3044\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\nAgent\u306foptimizer\u3084replay_buffer\u3092\u5171\u6709\u3059\u308bP1\u3068P2\u3092\u751f\u6210\u3057\u4e92\u3044\u306b\u6226\u308f\u305b\u307e\u3059\u3002\n\n# \u30b2\u30fc\u30e0\u9032\u884c\u90e8\u5206\u306e\u4f5c\u6210\n\n\u3082\u3046\u5b9f\u884c\u3057\u305f\u304f\u3066\u3060\u3044\u3076\u3046\u305a\u3046\u305a\u3057\u3066\u3044\u308b\u3053\u3068\u3068\u304a\u5bdf\u3057\u3057\u307e\u3059\u304c\u3001\u3053\u308c\u3092\u8ffd\u8a18\u3059\u308c\u3070\u5b9f\u884c\u3067\u304d\u307e\u3059\u306e\u3067\u3001\u3082\u3046\u3061\u3087\u3063\u3068\u30ac\u30de\u30f3\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n```Python:dqn.py\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 20000\n#\u30ab\u30a6\u30f3\u30bf\u306e\u5ba3\u8a00\nmiss = 0\nwin = 0\ndraw = 0\n#\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u7e70\u308a\u8fd4\u3057\u5b9f\u884c\nfor i in range(1, n_episodes + 1):\n    b.reset()\n    reward = 0\n    agents = [agent_p1, agent_p2]\n    turn = np.random.choice([0, 1])\n    last_state = None\n    while not b.done:\n        #\u914d\u7f6e\u30de\u30b9\u53d6\u5f97\n        action = agents[turn].act_and_train(b.board.copy(), reward)\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n        #\u914d\u7f6e\u306e\u7d50\u679c\u3001\u7d42\u4e86\u6642\u306b\u306f\u5831\u916c\u3068\u30ab\u30a6\u30f3\u30bf\u306b\u5024\u3092\u30bb\u30c3\u30c8\u3057\u3066\u5b66\u7fd2\n        if b.done == True:\n            if b.winner == 1:\n                reward = 1\n                win += 1\n            elif b.winner == 0:\n                draw += 1\n            else:\n                reward = -1\n            if b.missed is True:\n                miss += 1\n            #\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\n            agents[turn].stop_episode_and_train(b.board.copy(), reward, True)\n            #\u76f8\u624b\u3082\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\u3002\u76f8\u624b\u306e\u30df\u30b9\u306f\u52dd\u5229\u3068\u3057\u3066\u5b66\u7fd2\u3057\u306a\u3044\u3088\u3046\u306b\n            if agents[1 if turn == 0 else 0].last_state is not None and b.missed is False:\n                #\u524d\u306e\u30bf\u30fc\u30f3\u3067\u3068\u3063\u3066\u304a\u3044\u305flast_state\u3092action\u5b9f\u884c\u5f8c\u306e\u72b6\u614b\u3068\u3057\u3066\u6e21\u3059\n                agents[1 if turn == 0 else 0].stop_episode_and_train(last_state, reward*-1, True)\n        else:\n            #\u5b66\u7fd2\u7528\u306b\u30bf\u30fc\u30f3\u6700\u5f8c\u306e\u72b6\u614b\u3092\u9000\u907f\n            last_state = b.board.copy()\n            #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n            b.board = b.board * -1\n            #\u30bf\u30fc\u30f3\u3092\u5207\u308a\u66ff\u3048\n            turn = 1 if turn == 0 else 0\n\n    #\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u9032\u6357\u8868\u793a\n    if i % 100 == 0:\n        print(\"episode:\", i, \" / rnd:\", ra.random_count, \" / miss:\", miss, \" / win:\", win, \" / draw:\", draw, \" / statistics:\", agent_p1.get_statistics(), \" / epsilon:\", agent_p1.explorer.epsilon)\n        #\u30ab\u30a6\u30f3\u30bf\u306e\u521d\u671f\u5316\n        miss = 0\n        win = 0\n        draw = 0\n        ra.random_count = 0\n    if i % 10000 == 0:\n        # 10000\u30a8\u30d4\u30bd\u30fc\u30c9\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n        agent_p1.save(\"result_\" + str(i))\n\nprint(\"Training finished.\")\n```\n\n20000\u30b2\u30fc\u30e0\u3092\u7e70\u308a\u8fd4\u3059for\u6587\u3068\u30b2\u30fc\u30e0\u5185\u306e\u30bf\u30fc\u30f3\u3092\u7e70\u308a\u8fd4\u3059while\u6587\u306e\u5165\u308c\u5b50\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30dd\u30a4\u30f3\u30c8\u3068\u3057\u3066\u306f\u3001\u5148\u653b\u3082\u5f8c\u653b\u3082Agent\u81ea\u8eab\u3067\u3059\u3002\u3053\u306e\u30b2\u30fc\u30e0\u3067\u306f\u5185\u90e8\u7684\u306b\u25cb\u00d7\u306e\u5909\u308f\u308a\u306b\u81ea\u8eab\u306e\u624b\u30921\u3001\u76f8\u624b\u306e\u624b\u3092-1\u3068\u3057\u3066\u30dc\u30fc\u30c9\u306b\u914d\u7f6e\u3057\u3066\u3044\u304d\u307e\u3059\u304c\u3001\u5148\u653b\u5f8c\u653b\u3069\u3061\u3089\u306e\u74b0\u5883\u30fb\u30a2\u30af\u30b7\u30e7\u30f3\u3082\u5b66\u7fd2\u3057\u305f\u3044\u306e\u3067\u3001\u30b2\u30fc\u30e0\u9032\u884c\u5185\u3067\u7b26\u53f7\u3092\u51fa\u3057\u5206\u3051\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u5e38\u306b\u30dc\u30fc\u30c9\u306b\u306f1\u3092\u914d\u7f6e\u3059\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n\n```Python\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n```\n\n\u5f53\u7136\u305d\u306e\u307e\u307e\u3060\u3068\u30dc\u30fc\u30c9\u304c1\u3060\u3089\u3051\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u306e\u3067\u3001\u30bf\u30fc\u30f3\u4ea4\u4ee3\u6642\u306b\u30dc\u30fc\u30c9\u306e\u7b26\u53f7\u3092\u53cd\u8ee2\u3055\u305b\u3066\u3044\u307e\u3059\u3002\n\n```Python\n        #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n        else:\n            b.board = b.board * -1\n```\n\n\u305d\u3057\u3066\u6700\u5f8c\u306b\u3001\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3082ChainerRL\u304c\u4f5c\u3063\u3066\u304f\u308c\u308b\u3088\u3046\u306a\u306e\u3067\u300110000\u30a8\u30d4\u30bd\u30fc\u30c9\u6bce\u306b\u30a8\u30d4\u30bd\u30fc\u30c9\u6570\u3092\u672b\u5c3e\u306b\u4ed8\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u5c65\u6b74\u3092\u4fdd\u5b58\u3057\u3066\u3044\u304f\u3088\u3046\u306b\u3057\u307e\u3057\u305f\u3002\u540c\u3058experience\u3067\u5b66\u7fd2\u3055\u305b\u3066\u308b\u306e\u3067\u3001\u4fdd\u5b58\u3059\u308b\u306e\u306fagent_p1\u306e\u307f\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n# \u5b66\u7fd2\u306e\u5b9f\u884c\n\n\u305d\u308c\u3067\u306f\u3001\u3044\u3056\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u30fb\u30fb\u30fb\uff01\n\u306f\u3058\u3081\u306e\u3046\u3061\u306fepsilon\u306e\u5024\u304c\u5927\u304d\u3044\u306e\u3067\u3001\u307b\u3068\u3093\u3069\u304c\u30e9\u30f3\u30c0\u30e0\u6253\u3061\uff08rnd\u306e\u5024\u304c\u30e9\u30f3\u30c0\u30e0\u3067\u6253\u3063\u305f\u56de\u6570\uff09\u3068\u306a\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u30df\u30b9\u3082\u5c11\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u5f90\u3005\u306b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u304c\u6e1b\u308b\u3068DQN\u3055\u3093\u304c\u81ea\u3089\u8003\u3048\u305f\u624b\u3067\u6253\u3064\u6a5f\u4f1a\u304c\u5897\u3048\u308b\u305f\u3081\u4e00\u6642\u7684\u306b\u30df\u30b9\u3082\u5897\u3048\u3066\u304d\u307e\u3059\u304c\u3001\u5b66\u7fd2\u304c\u9032\u3080\u3068\u305d\u308c\u3082\u53ce\u675f\u3057\u306615000\u56de\u3092\u8d85\u3048\u308b\u3068\u307b\u307c1\u6841\u524d\u534a\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n```\nepisode: 100  / rnd: 761  / miss: 1  / win: 85  / draw: 14  / statistics: [('average_q', 0.11951273068342624), ('average_loss', 0.09235552993858538)]  / epsilon: 0.994778\nepisode: 200  / rnd: 722  / miss: 3  / win: 85  / draw: 12  / statistics: [('average_q', 0.35500590929140996), ('average_loss', 0.12790488153218765)]  / epsilon: 0.9895\nepisode: 300  / rnd: 756  / miss: 6  / win: 82  / draw: 12  / statistics: [('average_q', 0.6269444783473722), ('average_loss', 0.12164947750267516)]  / epsilon: 0.984278\n\u3000\uff1a\uff08\u4e2d\u7565\uff09\nepisode: 19800  / rnd: 212  / miss: 1  / win: 69  / draw: 30  / statistics: [('average_q', 0.49387913595157096), ('average_loss', 0.07891365175610675)]  / epsilon: 0.3\nepisode: 19900  / rnd: 229  / miss: 1  / win: 61  / draw: 38  / statistics: [('average_q', 0.49195677296191365), ('average_loss', 0.07796313042393459)]  / epsilon: 0.3\nepisode: 20000  / rnd: 216  / miss: 0  / win: 70  / draw: 30  / statistics: [('average_q', 0.509864846571749), ('average_loss', 0.07866546801090374)]  / epsilon: 0.3\nTraining finished.\n```\n\n# \u3044\u3056\u81ea\u5206\u3068\u5bfe\u6226\uff01\n\n\u30df\u30b9\u305b\u305a\u6253\u3066\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3057\u3001\u305f\u307e\u306b\u30e9\u30f3\u30c0\u30e0\u6253\u3061\u3057\u3066\u3044\u306a\u304c\u3089\u3082\u7d50\u69cbDraw\u306b\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u5f37\u3055\u3092\u78ba\u304b\u3081\u308b\u3079\u304f\u81ea\u5206\u3068\u5bfe\u6226\u3057\u3066\u307f\u307e\u3059\u3002\n\n## HumanPlayer\u306e\u4f5c\u6210\n\n\u307e\u305a\u306f\u4eba\u9593\u304c\u6253\u3066\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30a4\u30b9\u3068\u3057\u3066\u3001HumanPlayer\u306a\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n```Python:dqn.py\n#\u4eba\u9593\u306e\u30d7\u30ec\u30fc\u30e4\u30fc\nclass HumanPlayer:\n    def act(self, board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Please enter 1-9: \")\n                act = int(act)\n                if act >= 1 and act <= 9 and board[act-1] == 0:\n                    valid = True\n                    return act-1\n                else:\n                    print (\"Invalid move\")\n            except Exception as e:\n                    print (act +  \" is invalid\")\n```\n\n## \u5bfe\u4eba\u30b2\u30fc\u30e0\u9032\u884c\u90e8\u5206\u306e\u4f5c\u6210\n\n\u9032\u884c\u90e8\u5206\u3067\u3059\u3002DQN\u306e\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f1\u3001\u4eba\u9593\u306f-1\u306b\u306a\u308b\u3088\u3046\u306b\u56fa\u5b9a\u3057\u3064\u3064\u3001\u5148\u653b\u30fb\u5f8c\u653b\u306f\u30a8\u30d4\u30bd\u30fc\u30c9\u958b\u59cb\u524d\u306b\u300cDQN\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u5148\u653b\u304b\u3069\u3046\u304b\u300d\u3068\u3044\u3046\u3053\u3068\u3092\u6c7a\u5b9a\u3057\u3066\u521d\u56de\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b/\u3057\u306a\u3044\u3092\u5236\u5fa1\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u95a2\u4fc2\u3067\u3001\u5148\u653b\u30fb\u5f8c\u653b\u306b\u95a2\u308f\u3089\u305a\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306f\u5e38\u306b\u25cb\u3067\u4eba\u9593\u306f\u5e38\u306b\u00d7\u3067\u3059\u3002\n\n```Python:dqn.py\n#\u691c\u8a3c\nhuman_player = HumanPlayer()\nfor i in range(10):\n    b.reset()\n    dqn_first = np.random.choice([True, False])\n    while not b.done:\n        #DQN\n        if dqn_first or np.count_nonzero(b.board) > 0:\n            b.show()\n            action = agent_p1.act(b.board.copy())\n            b.move(action, 1)\n            if b.done == True:\n                if b.winner == 1:\n                    print(\"DQN Win\")\n                elif b.winner == 0:\n                    print(\"Draw\")\n                else:\n                    print(\"DQN Missed\")\n                agent_p1.stop_episode()\n                continue\n        #\u4eba\u9593\n        b.show()\n        action = human_player.act(b.board.copy())\n        b.move(action, -1)\n        if b.done == True:\n            if b.winner == -1:\n                print(\"HUMAN Win\")\n            elif b.winner == 0:\n                print(\"Draw\")\n            agent_p1.stop_episode()\n\nprint(\"Test finished.\")\n```\n\n\u30dd\u30a4\u30f3\u30c8\u306f\u3001agent\u306f\u3053\u3053\u3067\u306f\u5b66\u7fd2\u3057\u306a\u3044\u306e\u3067\u3001act()\u304a\u3088\u3073stop_episode()\u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u70b9\u3067\u3057\u3087\u3046\u304b\u3002\u3053\u308c\u3082Quickstart\u901a\u308a\u3067\u3059\u3002\n\n\u3055\u3066\u3001\u3053\u308c\u3067\u5bfe\u6226\u306e\u6e96\u5099\u306f\u6574\u3044\u307e\u3057\u305f\u304c\u3001\u518d\u5ea62\u4e07\u56de\u5b66\u7fd2\u3055\u305b\u308b\u306e\u3082\u4e0d\u6bdb\u306a\u306e\u3067\u4fdd\u5b58\u3057\u305fAgent\u3092\u8aad\u307f\u8fbc\u3080\u3088\u3046\u306b\u3057\u307e\u3059\u3002\u672c\u5f53\u306fdqn.py\u306e\u5b9f\u884c\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5b66\u7fd2\u3059\u308b/\u65e2\u5b58\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u3092\u5207\u308a\u66ff\u3048\u308b\u306e\u304c\u30b9\u30de\u30fc\u30c8\u3067\u3059\u304c\u3001\u65e9\u304f\u30d7\u30ec\u30a4\u3057\u305f\u3044\u306e\u3067\u4ee5\u4e0b\u306e\u901a\u308a\u5b66\u7fd2\u30a8\u30d4\u30bd\u30fc\u30c9\u6570\u30920\u306b\u3059\u308b\u3053\u3068\u3067\u5b66\u7fd2\u51e6\u7406\u3092\u30b9\u30ad\u30c3\u30d7\u3057\u307e\u3059\u3002\n\n```Python:dqn.py\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 0\n```\n\n\u305d\u3057\u3066\u3001\u5b66\u7fd2\u51e6\u7406\u306e\u5b8c\u4e86\u5f8c\u306b\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u8ffd\u52a0\u3057\u3066\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\n```Python:dqn.py\nprint(\"Training finished.\")\n\nagent_p1.load(\"result_20000\")  #\u2190\u3053\u308c\u3092\u8ffd\u52a0\n```\n\n\u6e96\u5099\u304c\u3067\u304d\u305f\u3089\u3001\u3044\u3056\u5bfe\u6226\u3067\u3059\uff01\n\n```\nTraining finished.\n   |   |   \n-----------\n   |   |   \n-----------\n   |   |   \n   |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   |   \nPlease enter 1-9: 1\n \u00d7 |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   |   \n \u00d7 |   |   \n-----------\n   |   |   \n-----------\n \u25cb |   | \u25cb \nPlease enter 1-9: 8\n```\n\n\u5bfe\u6226\u3067\u304d\u307e\u3059\u306d\uff01\u3084\u3063\u305f\u30fc\uff01\uff01\n\n\n# \u304a\u308f\u308a\u306b\n\nDQN\u3082Python\u3082\u304b\u3058\u308a\u305f\u3066\u306e\u79c1\u306b\u304a\u4ed8\u304d\u5408\u3044\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\u300c\u30eb\u30fc\u30eb\u3092\u6559\u3048\u306a\u304f\u3066\u3082\u307b\u307c\u9593\u9055\u3044\u306a\u304f\u30fb\u5b9a\u77f3\u901a\u308a\u6253\u3066\u308b\u300d\u3068\u3044\u3046\u3068\u3053\u308d\u307e\u3067\u6210\u9577\u3057\u3066\u304f\u308c\u305f\u306e\u306f\u306a\u304b\u306a\u304b\u5b09\u3057\u3044\u3082\u306e\u3067\u3059\u3002\n\u3057\u304b\u3082\u3001Chainer\u3092\u305d\u306e\u307e\u307e\u5229\u7528\u3057\u3066DQN\u3092\u5b9f\u88c5\u3059\u308b\u3088\u308a\u3001\u306f\u308b\u304b\u306b\u30b9\u30c3\u30ad\u30ea\u3068\u3057\u307e\u3057\u305f\u3002ChainerRL\u3059\u3054\u3044\uff01\uff01\u898b\u901a\u3057\u304c\u3088\u304f\u306a\u3063\u305f\u3053\u3068\u3067\u3001\u3044\u308d\u3044\u308d\u6539\u826f\u3057\u3088\u3046\u3068\u3057\u3066\u30d0\u30b0\u3092\u6df7\u5728\u3055\u305b\u308b\u30fb\u30fb\u30fb\u3068\u3044\u3063\u305f\u3053\u3068\u304c\u9632\u3052\u305d\u3046\u3067\u3059\u3002\n\n\u9593\u9055\u3063\u3066\u3044\u305f\u308a\u300c\u672c\u6765\u3053\u3046\u3059\u3079\u304d\u300d\u300c\u3053\u3046\u3057\u305f\u65b9\u304c\u5b66\u7fd2\u304c\u9032\u3080\u300d\u300c\u3053\u308c\u3060\u3068\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u3088\u300d\u307f\u305f\u3044\u306a\u3082\u306e\u304c\u5c71\u307b\u3069\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3001\u3044\u308d\u3044\u308d\u3054\u6307\u6458\u3092\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\u4f55\u5352\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\n\u7279\u306b\u8ab2\u984c\u306b\u601d\u3063\u3066\u3044\u308b\u306e\u306f\u3001Agent\u306e\u6253\u3061\u65b9\u304c\u307b\u307c\u6bce\u56de\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u3053\u3068\u3067\u3059\u3002\u3082\u3063\u3068\u5192\u967a\u3055\u305b\u306a\u3051\u308c\u3070\u3044\u3051\u306a\u3044\u306e\u304b\u306a\uff1f35\u4e07\u30a8\u30d4\u30bd\u30fc\u30c9\u5b66\u7fd2\u3055\u305b\u308b\u3068\u3001\u5b9a\u77f3\u901a\u308a\u6253\u3064\u306e\u3067\u5f37\u304f\u3001\u03b5\u30920\u306b\u3059\u308b\u3068\u307b\u307c\u6bce\u56deDraw\u3068\u306a\u308b\u306e\u3067\u3001\u826f\u3044\u3053\u3068\u3067\u306f\u3042\u308b\u306e\u3067\u3059\u304c\u3002\u3002\u306a\u304a15\u4e07\u301c20\u4e07\u30a8\u30d4\u30bd\u30fc\u30c9\u304b\u3089\u306f\u7d50\u679c\u3082loss\u3082\u4e00\u5b9a\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n\n# \u30bd\u30fc\u30b9\u5168\u4f53\n\n\u4e00\u5fdc\u3001\u30bd\u30fc\u30b9\u306e\u5168\u4f53\u3092\u63b2\u8f09\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u74b0\u5883\u304c\u63c3\u3063\u3066\u3044\u308c\u3070\u30b3\u30d4\u30da\u3057\u3066\u3059\u3050\u306b\u52d5\u304b\u305b\u308b\u3068\u601d\u3044\u307e\u3059\u3002\n\n```Python:dqn.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport numpy as np\n\n#\u30b2\u30fc\u30e0\u30dc\u30fc\u30c9\nclass Board():\n    def reset(self):\n        self.board = np.array([0] * 9, dtype=np.float32)\n        self.winner = None\n        self.missed = False\n        self.done = False\n\n    def move(self, act, turn):\n        if self.board[act] == 0:\n            self.board[act] = turn\n            self.check_winner()\n        else:\n            self.winner = turn*-1\n            self.missed = True\n            self.done = True\n\n    def check_winner(self):\n        win_conditions = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n        for cond in win_conditions:\n            if self.board[cond[0]] == self.board[cond[1]] == self.board[cond[2]]:\n                if self.board[cond[0]]!=0:\n                    self.winner=self.board[cond[0]]\n                    self.done = True\n                    return\n        if np.count_nonzero(self.board) == 9:\n            self.winner = 0\n            self.done = True\n\n    def get_empty_pos(self):\n        empties = np.where(self.board==0)[0]\n        if len(empties) > 0:\n            return np.random.choice(empties)\n        else:\n            return 0\n    \n    def show(self):\n        row = \" {} | {} | {} \"\n        hr = \"\\n-----------\\n\"\n        tempboard = []\n        for i in self.board:\n            if i == 1:\n                tempboard.append(\"\u25cb\")\n            elif i == -1:\n                tempboard.append(\"\u00d7\")\n            else:\n                tempboard.append(\" \")\n        print((row + hr + row + hr + row).format(*tempboard))\n\n#explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\nclass RandomActor:\n    def __init__(self, board):\n        self.board = board\n        self.random_count = 0\n    def random_action_func(self):\n        self.random_count += 1\n        return self.board.get_empty_pos()\n\n#Q\u95a2\u6570\nclass QFunction(chainer.Chain):\n    def __init__(self, obs_size, n_actions, n_hidden_channels=81):\n        super().__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_hidden_channels),\n            l3=L.Linear(n_hidden_channels, n_actions))\n    def __call__(self, x, test=False):\n        #-1\u3092\u6271\u3046\u306e\u3067leaky_relu\u3068\u3057\u305f\n        h = F.leaky_relu(self.l0(x))\n        h = F.leaky_relu(self.l1(h))\n        h = F.leaky_relu(self.l2(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l3(h))\n\n# \u30dc\u30fc\u30c9\u306e\u6e96\u5099\nb = Board()\n# explorer\u7528\u306e\u30e9\u30f3\u30c0\u30e0\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u6e96\u5099\nra = RandomActor(b)\n# \u74b0\u5883\u3068\u884c\u52d5\u306e\u6b21\u5143\u6570\nobs_size = 9\nn_actions = 9\n# Q-function\u3068\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\nq_func = QFunction(obs_size, n_actions)\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n# \u5831\u916c\u306e\u5272\u5f15\u7387\ngamma = 0.95\n# Epsilon-greedy\u3092\u4f7f\u3063\u3066\u305f\u307e\u306b\u5192\u967a\u300250000\u30b9\u30c6\u30c3\u30d7\u3067end_epsilon\u3068\u306a\u308b\nexplorer = chainerrl.explorers.LinearDecayEpsilonGreedy(\n    start_epsilon=1.0, end_epsilon=0.3, decay_steps=50000, random_action_func=ra.random_action_func)\n# Experience Replay\u3068\u3044\u3046DQN\u3067\u7528\u3044\u308b\u5b66\u7fd2\u624b\u6cd5\u3067\u4f7f\u3046\u30d0\u30c3\u30d5\u30a1\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n# Agent\u306e\u751f\u6210\uff08replay_buffer\u7b49\u3092\u5171\u6709\u3059\u308b2\u3064\uff09\nagent_p1 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\nagent_p2 = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n\n#\u5b66\u7fd2\u30b2\u30fc\u30e0\u56de\u6570\nn_episodes = 20000\n#\u30ab\u30a6\u30f3\u30bf\u306e\u5ba3\u8a00\nmiss = 0\nwin = 0\ndraw = 0\n#\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u7e70\u308a\u8fd4\u3057\u5b9f\u884c\nfor i in range(1, n_episodes + 1):\n    b.reset()\n    reward = 0\n    agents = [agent_p1, agent_p2]\n    turn = np.random.choice([0, 1])\n    last_state = None\n    while not b.done:\n        #\u914d\u7f6e\u30de\u30b9\u53d6\u5f97\n        action = agents[turn].act_and_train(b.board.copy(), reward)\n        #\u914d\u7f6e\u3092\u5b9f\u884c\n        b.move(action, 1)\n        #\u914d\u7f6e\u306e\u7d50\u679c\u3001\u7d42\u4e86\u6642\u306b\u306f\u5831\u916c\u3068\u30ab\u30a6\u30f3\u30bf\u306b\u5024\u3092\u30bb\u30c3\u30c8\u3057\u3066\u5b66\u7fd2\n        if b.done == True:\n            if b.winner == 1:\n                reward = 1\n                win += 1\n            elif b.winner == 0:\n                draw += 1\n            else:\n                reward = -1\n            if b.missed is True:\n                miss += 1\n            #\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\n            agents[turn].stop_episode_and_train(b.board.copy(), reward, True)\n            #\u76f8\u624b\u3082\u30a8\u30d4\u30bd\u30fc\u30c9\u3092\u7d42\u4e86\u3057\u3066\u5b66\u7fd2\u3002\u76f8\u624b\u306e\u30df\u30b9\u306f\u52dd\u5229\u3068\u3057\u3066\u5b66\u7fd2\u3057\u306a\u3044\u3088\u3046\u306b\n            if agents[1 if turn == 0 else 0].last_state is not None and b.missed is False:\n                #\u524d\u306e\u30bf\u30fc\u30f3\u3067\u3068\u3063\u3066\u304a\u3044\u305flast_state\u3092action\u5b9f\u884c\u5f8c\u306e\u72b6\u614b\u3068\u3057\u3066\u6e21\u3059\n                agents[1 if turn == 0 else 0].stop_episode_and_train(last_state, reward*-1, True)\n        else:\n            #\u5b66\u7fd2\u7528\u306b\u30bf\u30fc\u30f3\u6700\u5f8c\u306e\u72b6\u614b\u3092\u9000\u907f\n            last_state = b.board.copy()\n            #\u7d99\u7d9a\u306e\u3068\u304d\u306f\u76e4\u9762\u306e\u5024\u3092\u53cd\u8ee2\n            b.board = b.board * -1\n            #\u30bf\u30fc\u30f3\u3092\u5207\u308a\u66ff\u3048\n            turn = 1 if turn == 0 else 0\n\n    #\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u9032\u6357\u8868\u793a\n    if i % 100 == 0:\n        print(\"episode:\", i, \" / rnd:\", ra.random_count, \" / miss:\", miss, \" / win:\", win, \" / draw:\", draw, \" / statistics:\", agent_p1.get_statistics(), \" / epsilon:\", agent_p1.explorer.epsilon)\n        #\u30ab\u30a6\u30f3\u30bf\u306e\u521d\u671f\u5316\n        miss = 0\n        win = 0\n        draw = 0\n        ra.random_count = 0\n    if i % 10000 == 0:\n        # 10000\u30a8\u30d4\u30bd\u30fc\u30c9\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\n        agent_p1.save(\"result_\" + str(i))\n\nprint(\"Training finished.\")\n\n#\u4eba\u9593\u306e\u30d7\u30ec\u30fc\u30e4\u30fc\nclass HumanPlayer:\n    def act(self, board):\n        valid = False\n        while not valid:\n            try:\n                act = input(\"Please enter 1-9: \")\n                act = int(act)\n                if act >= 1 and act <= 9 and board[act-1] == 0:\n                    valid = True\n                    return act-1\n                else:\n                    print(\"Invalid move\")\n            except Exception as e:\n                print(act +  \" is invalid\")\n\n#\u691c\u8a3c\nhuman_player = HumanPlayer()\nfor i in range(10):\n    b.reset()\n    dqn_first = np.random.choice([True, False])\n    while not b.done:\n        #DQN\n        if dqn_first or np.count_nonzero(b.board) > 0:\n            b.show()\n            action = agent_p1.act(b.board.copy())\n            b.move(action, 1)\n            if b.done == True:\n                if b.winner == 1:\n                    print(\"DQN Win\")\n                elif b.winner == 0:\n                    print(\"Draw\")\n                else:\n                    print(\"DQN Missed\")\n                agent_p1.stop_episode()\n                continue\n        #\u4eba\u9593\n        b.show()\n        action = human_player.act(b.board.copy())\n        b.move(action, -1)\n        if b.done == True:\n            if b.winner == -1:\n                print(\"HUMAN Win\")\n            elif b.winner == 0:\n                print(\"Draw\")\n            agent_p1.stop_episode()\n\nprint(\"Test finished.\")\n```\n\n# \u53c2\u8003\u30b5\u30a4\u30c8\n\n- [ChainerRL](https://github.com/pfnet/chainerrl)\n- [Chainer\u3067DQN\u3002\u5f37\u5316\u5b66\u7fd2\u3092\u4e09\u76ee\u4e26\u3079\u3067\u3044\u308d\u3044\u308d\u8a66\u3057\u3066\u307f\u305f\u3002\uff08Deep Q Network\u3001Q-Learning\u3001\u30e2\u30f3\u30c6\u30ab\u30eb\u30ed\uff09](http://qiita.com/narisan25/items/e64a5741864d5a3b0db0)\n- [DQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)\n", "tags": ["Chainer", "DQN", "\u5f37\u5316\u5b66\u7fd2", "Python", "MachineLearning"]}