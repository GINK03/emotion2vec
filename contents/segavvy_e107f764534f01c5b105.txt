{"context": "\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015\u306e\u6311\u6226\u8a18\u9332\u3067\u3059\u3002\u74b0\u5883\u306fUbuntu 16.04 LTS \uff0b Python 3.5.2 :: Anaconda 4.1.1 (64-bit)\u3067\u3059\u3002\u904e\u53bb\u306e\u30ce\u30c3\u30af\u306e\u4e00\u89a7\u306f\u3053\u3061\u3089\u304b\u3089\u3069\u3046\u305e\u3002\n\n\u7b2c8\u7ae0: \u6a5f\u68b0\u5b66\u7fd2\n\n\u672c\u7ae0\u3067\u306f\uff0cBo Pang\u6c0f\u3068Lillian Lee\u6c0f\u304c\u516c\u958b\u3057\u3066\u3044\u308bMovie Review Data\u306esentence polarity dataset v1.0\u3092\u7528\u3044\uff0c\u6587\u3092\u80af\u5b9a\u7684\uff08\u30dd\u30b8\u30c6\u30a3\u30d6\uff09\u3082\u3057\u304f\u306f\u5426\u5b9a\u7684\uff08\u30cd\u30ac\u30c6\u30a3\u30d6\uff09\u306b\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\uff08\u6975\u6027\u5206\u6790\uff09\u306b\u53d6\u308a\u7d44\u3080\uff0e\n\n\n76. \u30e9\u30d9\u30eb\u4ed8\u3051\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u9069\u7528\u3057\uff0c\u6b63\u89e3\u306e\u30e9\u30d9\u30eb\uff0c\u4e88\u6e2c\u3055\u308c\u305f\u30e9\u30d9\u30eb\uff0c\u4e88\u6e2c\u78ba\u7387\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n\n\n\u51fa\u6765\u4e0a\u304c\u3063\u305f\u30b3\u30fc\u30c9\uff1a\n\nmain.py\n# coding: utf-8\nimport codecs\nimport snowballstemmer\nimport numpy as np\n\nfname_sentiment = 'sentiment.txt'\nfname_features = 'features.txt'\nfname_theta = 'theta.npy'\nfname_result = 'result.txt'\nfencoding = 'cp1252'        # Windows-1252\u3089\u3057\u3044\n\nstemmer = snowballstemmer.stemmer('english')\n\n# \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u30ea\u30b9\u30c8  http://xpo6.com/list-of-english-stop-words/ \u306eCSV Format\u3088\u308a\nstop_words = (\n    'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,'\n    'as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,'\n    'either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,'\n    'him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,'\n    'likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,'\n    'on,only,or,other,our,own,rather,said,say,says,she,should,since,so,'\n    'some,than,that,the,their,them,then,there,these,they,this,tis,to,too,'\n    'twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,'\n    'will,with,would,yet,you,your').lower().split(',')\n\n\ndef is_stopword(str):\n    '''\u6587\u5b57\u304c\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059\n    \u5927\u5c0f\u6587\u5b57\u306f\u540c\u4e00\u8996\u3059\u308b\n\n    \u623b\u308a\u5024\uff1a\n    \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306a\u3089True\u3001\u9055\u3046\u5834\u5408\u306fFalse\n    '''\n    return str.lower() in stop_words\n\n\ndef hypothesis(data_x, theta):\n    '''\u4eee\u8aac\u95a2\u6570\n    data_x\u306b\u5bfe\u3057\u3066\u3001theta\u3092\u4f7f\u3063\u3066data_y\u3092\u4e88\u6e2c\n\n    \u623b\u308a\u5024\uff1a\n    \u4e88\u6e2c\u5024\u306e\u884c\u5217\n    '''\n    return 1.0 / (1.0 + np.exp(-data_x.dot(theta)))\n\n\ndef extract_features(data, dict_features):\n    '''\u6587\u7ae0\u304b\u3089\u7d20\u6027\u3092\u62bd\u51fa\n    \u6587\u7ae0\u304b\u3089dict_features\u306b\u542b\u307e\u308c\u308b\u7d20\u6027\u3092\u62bd\u51fa\u3057\u3001\n    dict_features['(\u7d20\u6027)']\u306e\u4f4d\u7f6e\u30921\u306b\u3057\u305f\u884c\u5217\u3092\u8fd4\u3059\u3002\n    \u306a\u304a\u3001\u5148\u982d\u8981\u7d20\u306f\u56fa\u5b9a\u30671\u3002\u7d20\u6027\u306b\u5bfe\u5fdc\u3057\u306a\u3044\u91cd\u307f\u7528\u3002\n\n    \u623b\u308a\u5024\uff1a\n    \u5148\u982d\u8981\u7d20\u3068\u3001\u8a72\u5f53\u7d20\u6027\u306e\u4f4d\u7f6e+1\u30921\u306b\u3057\u305f\u884c\u5217\n    '''\n    data_one_x = np.zeros(len(dict_features) + 1, dtype=np.float64)\n    data_one_x[0] = 1       # \u5148\u982d\u8981\u7d20\u306f\u56fa\u5b9a\u30671\u3001\u7d20\u6027\u306b\u5bfe\u5fdc\u3057\u306a\u3044\u91cd\u307f\u7528\u3002\n\n    for word in data.split(' '):\n\n        # \u524d\u5f8c\u306e\u7a7a\u767d\u6587\u5b57\u9664\u53bb\n        word = word.strip()\n\n        # \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u9664\u53bb\n        if is_stopword(word):\n            continue\n\n        # \u30b9\u30c6\u30df\u30f3\u30b0\n        word = stemmer.stemWord(word)\n\n        # \u7d20\u6027\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u53d6\u5f97\u3001\u884c\u5217\u306e\u8a72\u5f53\u7b87\u6240\u30921\u306b\n        try:\n            data_one_x[dict_features[word]] = 1\n        except:\n            pass        # dict_features\u306b\u306a\u3044\u7d20\u6027\u306f\u7121\u8996\n\n    return data_one_x\n\n\ndef load_dict_features():\n    '''features.txt\u3092\u8aad\u307f\u8fbc\u307f\u3001\u7d20\u6027\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u8f9e\u66f8\u3092\u4f5c\u6210\n    \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u5024\u306f1\u30d9\u30fc\u30b9\u3067\u3001features.txt\u306b\u304a\u3051\u308b\u884c\u756a\u53f7\u3068\u4e00\u81f4\u3059\u308b\u3002\n\n    \u623b\u308a\u5024\uff1a\n    \u7d20\u6027\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\u3059\u308b\u8f9e\u66f8\n    '''\n    with codecs.open(fname_features, 'r', fencoding) as file_in:\n        return {line.strip(): i for i, line in enumerate(file_in, start=1)}\n\n\n# \u7d20\u6027\u8f9e\u66f8\u306e\u8aad\u307f\u8fbc\u307f\ndict_features = load_dict_features()\n\n# \u5b66\u7fd2\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\ntheta = np.load(fname_theta)\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\nwith codecs.open(fname_sentiment, 'r', fencoding) as file_in, \\\n        open(fname_result, 'w') as file_out:\n\n    for line in file_in:\n\n        # \u7d20\u6027\u62bd\u51fa\n        data_one_x = extract_features(line[3:], dict_features)\n\n        # \u4e88\u6e2c\u3001\u7d50\u679c\u51fa\u529b\n        h = hypothesis(data_one_x, theta)\n        if h > 0.5:\n            file_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '+1', h))\n        else:\n            file_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '-1', 1 - h))\n\n\n\n\u5b9f\u884c\u7d50\u679c\uff1a\n\u5b9f\u884c\u7d50\u679c\u306f\u300cresult.txt\u300d\u306b\u51fa\u529b\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u3001\u305d\u306e\u5148\u982d\u90e8\u5206\u3067\u3059\u3002\n\nresult.txt\u306e\u5148\u982d\u90e8\u5206\n-1  -1  0.84128525307739\n+1  +1  0.9092062807282129\n+1  +1  0.553085519355556\n+1  +1  0.8535668467933613\n-1  -1  0.7992886809287588\n+1  +1  0.9989116240762246\n-1  +1  0.6208624063497488\n+1  +1  0.9845368320643015\n+1  +1  0.7906871750078216\n+1  +1  0.8645613519028749\n-1  -1  0.916795585155668\n+1  +1  0.9261196491506768\n-1  -1  0.9114578616603789\n+1  +1  0.7902482704258449\n+1  -1  0.6600533200938651\n+1  -1  0.5726383205991274\n-1  -1  0.9173556809882624\n-1  -1  0.9770172038339648\n+1  +1  0.9239412556453133\n+1  -1  0.5792255523114858\n\uff08\u4ee5\u4e0b\u7565\uff09\n\n\n\u3044\u304f\u3064\u304b\u9593\u9055\u3063\u3066\u3044\u308b\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u306d...\n\u30d5\u30a1\u30a4\u30eb\u5168\u4f53\u306fGitHub\u306b\u30a2\u30c3\u30d7\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u306e\u691c\u8a3c\n\u4eca\u56de\u306f\u554f\u984c74\u306e\u51e6\u7406\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u884c\u3046\u3060\u3051\u3067\u3059\u3002\u7d50\u679c\u306e\u5206\u6790\u306f\u6b21\u306e\u554f\u984c\u3067\u884c\u3044\u307e\u3059\u3002\n\u306a\u304a\u3001\u554f\u984c78\u306b\u3082\u66f8\u304b\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u5b66\u7fd2\u306b\u4f7f\u3063\u305f\u30c7\u30fc\u30bf\u306b\u3088\u308b\u691c\u8a3c\u306fNG\u3067\u3059\u3002\u6559\u79d1\u66f8\u4e38\u6697\u8a18\u3067\u5fdc\u7528\u304c\u52b9\u304b\u306a\u3044\u3088\u3046\u306a\u5b66\u7fd2\uff08\u904e\u5b66\u7fd2\u3068\u304b\u547c\u3073\u307e\u3059\uff09\u3067\u3082\u826f\u3044\u7d50\u679c\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u305f\u3081\u3067\u3059\u3002\n\u3000\n77\u672c\u76ee\u306e\u30ce\u30c3\u30af\u306f\u4ee5\u4e0a\u3067\u3059\u3002\u8aa4\u308a\u306a\u3069\u3042\u308a\u307e\u3057\u305f\u3089\u3001\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\u3002\n\n\u5b9f\u884c\u7d50\u679c\u306b\u306f\u3001100\u672c\u30ce\u30c3\u30af\u3067\u7528\u3044\u308b\u30b3\u30fc\u30d1\u30b9\u30fb\u30c7\u30fc\u30bf\u3067\u914d\u5e03\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u304c\u542b\u307e\u308c\u307e\u3059\u3002\n\n[\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af 2015](http://www.cl.ecei.tohoku.ac.jp/nlp100/)\u306e\u6311\u6226\u8a18\u9332\u3067\u3059\u3002\u74b0\u5883\u306fUbuntu 16.04 LTS \uff0b Python 3.5.2 \\:\\: Anaconda 4.1.1 (64-bit)\u3067\u3059\u3002\u904e\u53bb\u306e\u30ce\u30c3\u30af\u306e\u4e00\u89a7\u306f[\u3053\u3061\u3089](http://qiita.com/segavvy/items)\u304b\u3089\u3069\u3046\u305e\u3002\n\n## \u7b2c8\u7ae0: \u6a5f\u68b0\u5b66\u7fd2\n>\u672c\u7ae0\u3067\u306f\uff0cBo Pang\u6c0f\u3068Lillian Lee\u6c0f\u304c\u516c\u958b\u3057\u3066\u3044\u308bMovie Review Data\u306esentence polarity dataset v1.0\u3092\u7528\u3044\uff0c\u6587\u3092\u80af\u5b9a\u7684\uff08\u30dd\u30b8\u30c6\u30a3\u30d6\uff09\u3082\u3057\u304f\u306f\u5426\u5b9a\u7684\uff08\u30cd\u30ac\u30c6\u30a3\u30d6\uff09\u306b\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\uff08\u6975\u6027\u5206\u6790\uff09\u306b\u53d6\u308a\u7d44\u3080\uff0e\n\n###76. \u30e9\u30d9\u30eb\u4ed8\u3051\n>\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u9069\u7528\u3057\uff0c\u6b63\u89e3\u306e\u30e9\u30d9\u30eb\uff0c\u4e88\u6e2c\u3055\u308c\u305f\u30e9\u30d9\u30eb\uff0c\u4e88\u6e2c\u78ba\u7387\u3092\u30bf\u30d6\u533a\u5207\u308a\u5f62\u5f0f\u3067\u51fa\u529b\u305b\u3088\uff0e\n\n####\u51fa\u6765\u4e0a\u304c\u3063\u305f\u30b3\u30fc\u30c9\uff1a\n\n```python:main.py\n# coding: utf-8\nimport codecs\nimport snowballstemmer\nimport numpy as np\n\nfname_sentiment = 'sentiment.txt'\nfname_features = 'features.txt'\nfname_theta = 'theta.npy'\nfname_result = 'result.txt'\nfencoding = 'cp1252'\t\t# Windows-1252\u3089\u3057\u3044\n\nstemmer = snowballstemmer.stemmer('english')\n\n# \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u30ea\u30b9\u30c8\t http://xpo6.com/list-of-english-stop-words/ \u306eCSV Format\u3088\u308a\nstop_words = (\n\t'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,'\n\t'as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,'\n\t'either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,'\n\t'him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,'\n\t'likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,'\n\t'on,only,or,other,our,own,rather,said,say,says,she,should,since,so,'\n\t'some,than,that,the,their,them,then,there,these,they,this,tis,to,too,'\n\t'twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,'\n\t'will,with,would,yet,you,your').lower().split(',')\n\n\ndef is_stopword(str):\n\t'''\u6587\u5b57\u304c\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u304b\u3069\u3046\u304b\u3092\u8fd4\u3059\n\t\u5927\u5c0f\u6587\u5b57\u306f\u540c\u4e00\u8996\u3059\u308b\n\n\t\u623b\u308a\u5024\uff1a\n\t\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306a\u3089True\u3001\u9055\u3046\u5834\u5408\u306fFalse\n\t'''\n\treturn str.lower() in stop_words\n\n\ndef hypothesis(data_x, theta):\n\t'''\u4eee\u8aac\u95a2\u6570\n\tdata_x\u306b\u5bfe\u3057\u3066\u3001theta\u3092\u4f7f\u3063\u3066data_y\u3092\u4e88\u6e2c\n\n\t\u623b\u308a\u5024\uff1a\n\t\u4e88\u6e2c\u5024\u306e\u884c\u5217\n\t'''\n\treturn 1.0 / (1.0 + np.exp(-data_x.dot(theta)))\n\n\ndef extract_features(data, dict_features):\n\t'''\u6587\u7ae0\u304b\u3089\u7d20\u6027\u3092\u62bd\u51fa\n\t\u6587\u7ae0\u304b\u3089dict_features\u306b\u542b\u307e\u308c\u308b\u7d20\u6027\u3092\u62bd\u51fa\u3057\u3001\n\tdict_features['(\u7d20\u6027)']\u306e\u4f4d\u7f6e\u30921\u306b\u3057\u305f\u884c\u5217\u3092\u8fd4\u3059\u3002\n\t\u306a\u304a\u3001\u5148\u982d\u8981\u7d20\u306f\u56fa\u5b9a\u30671\u3002\u7d20\u6027\u306b\u5bfe\u5fdc\u3057\u306a\u3044\u91cd\u307f\u7528\u3002\n\n\t\u623b\u308a\u5024\uff1a\n\t\u5148\u982d\u8981\u7d20\u3068\u3001\u8a72\u5f53\u7d20\u6027\u306e\u4f4d\u7f6e+1\u30921\u306b\u3057\u305f\u884c\u5217\n\t'''\n\tdata_one_x = np.zeros(len(dict_features) + 1, dtype=np.float64)\n\tdata_one_x[0] = 1\t\t# \u5148\u982d\u8981\u7d20\u306f\u56fa\u5b9a\u30671\u3001\u7d20\u6027\u306b\u5bfe\u5fdc\u3057\u306a\u3044\u91cd\u307f\u7528\u3002\n\n\tfor word in data.split(' '):\n\n\t\t# \u524d\u5f8c\u306e\u7a7a\u767d\u6587\u5b57\u9664\u53bb\n\t\tword = word.strip()\n\n\t\t# \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u9664\u53bb\n\t\tif is_stopword(word):\n\t\t\tcontinue\n\n\t\t# \u30b9\u30c6\u30df\u30f3\u30b0\n\t\tword = stemmer.stemWord(word)\n\n\t\t# \u7d20\u6027\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u53d6\u5f97\u3001\u884c\u5217\u306e\u8a72\u5f53\u7b87\u6240\u30921\u306b\n\t\ttry:\n\t\t\tdata_one_x[dict_features[word]] = 1\n\t\texcept:\n\t\t\tpass\t\t# dict_features\u306b\u306a\u3044\u7d20\u6027\u306f\u7121\u8996\n\n\treturn data_one_x\n\n\ndef load_dict_features():\n\t'''features.txt\u3092\u8aad\u307f\u8fbc\u307f\u3001\u7d20\u6027\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u8f9e\u66f8\u3092\u4f5c\u6210\n\t\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u5024\u306f1\u30d9\u30fc\u30b9\u3067\u3001features.txt\u306b\u304a\u3051\u308b\u884c\u756a\u53f7\u3068\u4e00\u81f4\u3059\u308b\u3002\n\n\t\u623b\u308a\u5024\uff1a\n\t\u7d20\u6027\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\u3059\u308b\u8f9e\u66f8\n\t'''\n\twith codecs.open(fname_features, 'r', fencoding) as file_in:\n\t\treturn {line.strip(): i for i, line in enumerate(file_in, start=1)}\n\n\n# \u7d20\u6027\u8f9e\u66f8\u306e\u8aad\u307f\u8fbc\u307f\ndict_features = load_dict_features()\n\n# \u5b66\u7fd2\u7d50\u679c\u306e\u8aad\u307f\u8fbc\u307f\ntheta = np.load(fname_theta)\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\nwith codecs.open(fname_sentiment, 'r', fencoding) as file_in, \\\n\t\topen(fname_result, 'w') as file_out:\n\n\tfor line in file_in:\n\n\t\t# \u7d20\u6027\u62bd\u51fa\n\t\tdata_one_x = extract_features(line[3:], dict_features)\n\n\t\t# \u4e88\u6e2c\u3001\u7d50\u679c\u51fa\u529b\n\t\th = hypothesis(data_one_x, theta)\n\t\tif h > 0.5:\n\t\t\tfile_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '+1', h))\n\t\telse:\n\t\t\tfile_out.write('{}\\t{}\\t{}\\n'.format(line[0:2], '-1', 1 - h))\n```\n\n####\u5b9f\u884c\u7d50\u679c\uff1a\n\n\u5b9f\u884c\u7d50\u679c\u306f\u300cresult.txt\u300d\u306b\u51fa\u529b\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u3001\u305d\u306e\u5148\u982d\u90e8\u5206\u3067\u3059\u3002\n\n```console:result.txt\u306e\u5148\u982d\u90e8\u5206\n-1\t-1\t0.84128525307739\n+1\t+1\t0.9092062807282129\n+1\t+1\t0.553085519355556\n+1\t+1\t0.8535668467933613\n-1\t-1\t0.7992886809287588\n+1\t+1\t0.9989116240762246\n-1\t+1\t0.6208624063497488\n+1\t+1\t0.9845368320643015\n+1\t+1\t0.7906871750078216\n+1\t+1\t0.8645613519028749\n-1\t-1\t0.916795585155668\n+1\t+1\t0.9261196491506768\n-1\t-1\t0.9114578616603789\n+1\t+1\t0.7902482704258449\n+1\t-1\t0.6600533200938651\n+1\t-1\t0.5726383205991274\n-1\t-1\t0.9173556809882624\n-1\t-1\t0.9770172038339648\n+1\t+1\t0.9239412556453133\n+1\t-1\t0.5792255523114858\n\uff08\u4ee5\u4e0b\u7565\uff09\n```\n\n\u3044\u304f\u3064\u304b\u9593\u9055\u3063\u3066\u3044\u308b\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u306d...\n\u30d5\u30a1\u30a4\u30eb\u5168\u4f53\u306f[GitHub](https://github.com/segavvy/nlp100_Python/tree/master/76)\u306b\u30a2\u30c3\u30d7\u3057\u3066\u3044\u307e\u3059\u3002\n\n###\u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u306e\u691c\u8a3c\n\n\u4eca\u56de\u306f[\u554f\u984c74](http://qiita.com/segavvy/items/8a46a74e7a88df89051d)\u306e\u51e6\u7406\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u884c\u3046\u3060\u3051\u3067\u3059\u3002\u7d50\u679c\u306e\u5206\u6790\u306f\u6b21\u306e\u554f\u984c\u3067\u884c\u3044\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u554f\u984c78\u306b\u3082\u66f8\u304b\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u3001\u5b66\u7fd2\u306b\u4f7f\u3063\u305f\u30c7\u30fc\u30bf\u306b\u3088\u308b\u691c\u8a3c\u306fNG\u3067\u3059\u3002\u6559\u79d1\u66f8\u4e38\u6697\u8a18\u3067\u5fdc\u7528\u304c\u52b9\u304b\u306a\u3044\u3088\u3046\u306a\u5b66\u7fd2\uff08\u904e\u5b66\u7fd2\u3068\u304b\u547c\u3073\u307e\u3059\uff09\u3067\u3082\u826f\u3044\u7d50\u679c\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u305f\u3081\u3067\u3059\u3002\n\n\u3000\n77\u672c\u76ee\u306e\u30ce\u30c3\u30af\u306f\u4ee5\u4e0a\u3067\u3059\u3002\u8aa4\u308a\u306a\u3069\u3042\u308a\u307e\u3057\u305f\u3089\u3001\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\u3002\n<hr>\n\n*\u5b9f\u884c\u7d50\u679c\u306b\u306f\u3001[100\u672c\u30ce\u30c3\u30af\u3067\u7528\u3044\u308b\u30b3\u30fc\u30d1\u30b9\u30fb\u30c7\u30fc\u30bf](http://www.cl.ecei.tohoku.ac.jp/nlp100/#data)\u3067\u914d\u5e03\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u304c\u542b\u307e\u308c\u307e\u3059\u3002*\n", "tags": ["\u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af", "\u6a5f\u68b0\u5b66\u7fd2", "Python"]}