{"context": "\n\nGnosy \u30aa\u30d5\u30a3\u30b7\u30e3\u30eb\u30fb\u30d6\u30ed\u30b0 \u306b Chainer\u3067\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u884c\u3046 GPU\u74b0\u5883\u7528 \u30b3\u30fc\u30c9\u4f8b \u304c \u30a2\u30c3\u30d7\u3055\u308c\u3066\u3044\u305f\u306e\u3067\u3001CPU\u74b0\u5883\u3067\u52d5\u304f\u3088\u3046\u306b\u4e00\u90e8 \u66f8\u304d\u63db\u3048\u3066\u5b9f\u884c\u3057\u3066\u307f\u305f\u3002\n\n\nHatena Blog Gunosy\u30c7\u30fc\u30bf\u5206\u6790\u30d6\u30ed\u30b0\uff082016-07-28\uff09\u300cChainer 1.11.0 \u3067\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a66\u3057\u3066\u307f\u308b\u300d\n\n\n\u3010 CPU\u5b9f\u884c\u7528\u306b\u4e00\u90e8\u3001\u4fee\u6b63\u3057\u305f\u30b3\u30fc\u30c9 \u3011\n\n\ngunosy_blog_train_mnist_cnn.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import training\nfrom chainer.training import extensions\n\n\n# Network definition\nclass CNN(chainer.Chain):\n\n    def __init__(self, train=True):\n        super(CNN, self).__init__(\n            conv1=L.Convolution2D(1, 32, 5),\n            conv2=L.Convolution2D(32, 64, 5),\n            l1=L.Linear(1024, 10),\n        )\n        self.train = train\n\n    def __call__(self, x):\n        h = F.max_pooling_2d(F.relu(self.conv1(x)), 2)\n        h = F.max_pooling_2d(F.relu(self.conv2(h)), 2)\n        return self.l1(h)\n\n\ndef main():\n    # Set up a neural network to train\n    model = L.Classifier(CNN())\n#    chainer.cuda.get_device(0).use()\n#    model.to_gpu()\n    # Setup an optimizer\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(model)\n\n    # Load the MNIST dataset\n    train, test = chainer.datasets.get_mnist(ndim=3)\n    train_iter = chainer.iterators.SerialIterator(train, batch_size=100)\n    test_iter = chainer.iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)\n\n    # Set up a trainer\n#    updater = training.StandardUpdater(train_iter, optimizer, device=0)\n# For use of this program on CPU environment, change num of parameter \"device\" to be -1 from 0(zero)\n    updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n    trainer = training.Trainer(updater, (5, 'epoch'), out='result')\n\n#    trainer.extend(extensions.Evaluator(test_iter, model, device=0))\n# For use of this program on CPU environment, change num of parameter \"device\" to be -1 from 0(zero)\n    trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport( ['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy']))\n    trainer.extend(extensions.ProgressBar())\n\n    # Run the training\n    trainer.run()\n\nif __name__ == '__main__':\n    main()\n\n\n\n\u3010 \u5b9f\u884c\u7d50\u679c \u3011\n\n\nTerminal\nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ emacs gunosy_blog_train_mnist_cnn.py \nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ time python gunosy_blog_train_mnist_cnn.py \nepoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n1           0.181777    0.054226              0.947283       0.9832                    \n2           0.0506084   0.0434985             0.984417       0.9862                    \n3           0.0347024   0.0387978             0.989317       0.9874                    \n4           0.0254395   0.0360393             0.992          0.9882                    \n5           0.0199348   0.0337876             0.99415        0.9891                    \n\nreal    76m21.714s\nuser    73m44.136s\nsys 5m23.219s\nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ \n\n\n\n\n\n\u81ea\u5206\u3067\u66f8\u3044\u305f\u624b\u66f8\u304d\u6570\u5b57\u3092\u5224\u5225\u3057\u3066\u307f\u308b\n\n\uff08 \u4f5c\u6210\u4e2d \uff09\n\n\uff08 \u53c2\u8003 \uff09\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u30ea\u30b5\u30a4\u30ba\n\nHatena Blog \u8a18\u9332\u76ee\u9332\uff082016-04-04\uff09\u300c\u4eca\u66f4\u3060\u3051\u3069CNN\u52d5\u304b\u3057\u3066\u307f\u308b\u300d\n\n\nimg = cv2.imread(\"/data/img/train/\"+str(i)+\"/\"+files[img_index])\n           resized_img = cv2.resize(img, (50, 50)) # \u753b\u50cf\u3092\u7e2e\u5c0f\n           x_train.append(np.transpose(resized_img,(2,0,1))/255.0)\n\n\n\nHatena Blog StatsFragments\uff082015-07-05\uff09\u300cChainer \u3067 Deep Learning: model zoo \u3067 R-CNN \u3084\u308a\u305f\u3044\u300d\n\n\ndef get_image(url):\n   \"\"\"URL\u304b\u3089\u753b\u50cf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\"\"\"\n   import urllib\n   import StringIO\n   import PIL.Image as Image\n   return >Image.open(StringIO.StringIO(urllib.urlopen(url).read())).convert(\"RGB\")\n\ndef resize(image):\n   \"\"\"\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\"\"\"\n   import PIL.Image as Image\n   return image.resize((256, 256), Image.ANTIALIAS)\n\n\n\n\u307e\u3055\u304b\u308a\u52df\u96c6\u4e2d [Chainer] Deep Learning \u3067\u30a2\u30cb\u30e1 (\u3086\u308b\u3086\u308a) \u30ad\u30e3\u30e9\u30af\u30bf\u30fc\u3092\u8b58\u5225\u3059\u308b(2015/11/4\u5c11\u3057\u8ffd\u8a18)\n\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u53ce\u96c6\u306f\uff0cWeb\u30b5\u30a4\u30c8\u306e\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u306b\u3088\u3063\u3066\u534a\u81ea\u52d5\u534a\u624b\u52d5\u3067\u884c\u3044\u307e\u3057\u305f\uff0e\n\u3086\u308b\u3086\u308a\u30ad\u30e3\u30e9\u4ee5\u5916\u306e\u9854\u753b\u50cf\u306f27000\u679a\u96c6\u3081\u307e\u3057\u305f\uff0e\n\u3086\u308b\u3086\u308a\u3054\u3089\u304f\u90e8\u30ad\u30e3\u30e9\u306e\u753b\u50cf\u306f300\u679a\u305a\u3064\u96c6\u3081\uff0c\u56de\u8ee2\uff0c\u8f1d\u5ea6\u5909\u5316\u30671200\u679a\u305a\u3064\u306b\u6c34\u5897\u3057\u3057\u307e\u3057\u305f\uff0e\n\u96c6\u3081\u305f\u753b\u50cf\u306f50 \u00d7 50 pixels\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u307e\u3057\u305f\uff0e\n\n\nHatena Blog \u753b\u50cf\u51e6\u7406\u3068\u304b\u6a5f\u68b0\u5b66\u7fd2\u3068\u304b\uff082016-06-16\uff09\u300cchainer\u306eimagenet\u30b5\u30f3\u30d7\u30eb\u3092opencv\u3092\u4f7f\u3063\u3066\u52d5\u753b\u306b\u9069\u7528\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u300d\n\n\n\n####__Gnosy \u30aa\u30d5\u30a3\u30b7\u30e3\u30eb\u30fb\u30d6\u30ed\u30b0 \u306b Chainer\u3067\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u884c\u3046 GPU\u74b0\u5883\u7528 \u30b3\u30fc\u30c9\u4f8b \u304c \u30a2\u30c3\u30d7\u3055\u308c\u3066\u3044\u305f\u306e\u3067\u3001CPU\u74b0\u5883\u3067\u52d5\u304f\u3088\u3046\u306b\u4e00\u90e8 \u66f8\u304d\u63db\u3048\u3066\u5b9f\u884c\u3057\u3066\u307f\u305f\u3002__\n\n#### [Hatena Blog Gunosy\u30c7\u30fc\u30bf\u5206\u6790\u30d6\u30ed\u30b0\uff082016-07-28\uff09\u300cChainer 1.11.0 \u3067\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a66\u3057\u3066\u307f\u308b\u300d](http://data.gunosy.io)\n\n\n\n###__\u3010 CPU\u5b9f\u884c\u7528\u306b\u4e00\u90e8\u3001\u4fee\u6b63\u3057\u305f\u30b3\u30fc\u30c9 \u3011__\n\n```{Python:gunosy_blog_train_mnist_cnn.py}\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import training\nfrom chainer.training import extensions\n\n\n# Network definition\nclass CNN(chainer.Chain):\n\n    def __init__(self, train=True):\n        super(CNN, self).__init__(\n            conv1=L.Convolution2D(1, 32, 5),\n            conv2=L.Convolution2D(32, 64, 5),\n            l1=L.Linear(1024, 10),\n        )\n        self.train = train\n\n    def __call__(self, x):\n        h = F.max_pooling_2d(F.relu(self.conv1(x)), 2)\n        h = F.max_pooling_2d(F.relu(self.conv2(h)), 2)\n        return self.l1(h)\n\n\ndef main():\n    # Set up a neural network to train\n    model = L.Classifier(CNN())\n#    chainer.cuda.get_device(0).use()\n#    model.to_gpu()\n    # Setup an optimizer\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(model)\n    \n    # Load the MNIST dataset\n    train, test = chainer.datasets.get_mnist(ndim=3)\n    train_iter = chainer.iterators.SerialIterator(train, batch_size=100)\n    test_iter = chainer.iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)\n    \n    # Set up a trainer\n#    updater = training.StandardUpdater(train_iter, optimizer, device=0)\n# For use of this program on CPU environment, change num of parameter \"device\" to be -1 from 0(zero)\n    updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n    trainer = training.Trainer(updater, (5, 'epoch'), out='result')\n    \n#    trainer.extend(extensions.Evaluator(test_iter, model, device=0))\n# For use of this program on CPU environment, change num of parameter \"device\" to be -1 from 0(zero)\n    trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport( ['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy']))\n    trainer.extend(extensions.ProgressBar())\n    \n    # Run the training\n    trainer.run()\n\nif __name__ == '__main__':\n    main()\n```\n\n\n###__\u3010 \u5b9f\u884c\u7d50\u679c \u3011__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ emacs gunosy_blog_train_mnist_cnn.py \nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ time python gunosy_blog_train_mnist_cnn.py \nepoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n1           0.181777    0.054226              0.947283       0.9832                    \n2           0.0506084   0.0434985             0.984417       0.9862                    \n3           0.0347024   0.0387978             0.989317       0.9874                    \n4           0.0254395   0.0360393             0.992          0.9882                    \n5           0.0199348   0.0337876             0.99415        0.9891                    \n\nreal\t76m21.714s\nuser\t73m44.136s\nsys\t5m23.219s\nHirofumiYashima-no-MacBook:mnist hirofumiyashima$ \n```\n\n<img width=\"1041\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2016-07-29 21.45.21.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7aeb4569-95f1-782d-9f15-04423190c29c.png\">\n\n\n___\n\n###__\u81ea\u5206\u3067\u66f8\u3044\u305f\u624b\u66f8\u304d\u6570\u5b57\u3092\u5224\u5225\u3057\u3066\u307f\u308b__\n\n\uff08 \u4f5c\u6210\u4e2d \uff09\n\n___\n\n__\uff08 \u53c2\u8003 \uff09\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u30ea\u30b5\u30a4\u30ba__\n\n* [Hatena Blog \u8a18\u9332\u76ee\u9332\uff082016-04-04\uff09\u300c\u4eca\u66f4\u3060\u3051\u3069CNN\u52d5\u304b\u3057\u3066\u307f\u308b\u300d](http://kazz.hateblo.jp/entry/2016/04/04/234021)\n\n>```\n>img = cv2.imread(\"/data/img/train/\"+str(i)+\"/\"+files[img_index])\n>            resized_img = cv2.resize(img, (50, 50)) # \u753b\u50cf\u3092\u7e2e\u5c0f\n>            x_train.append(np.transpose(resized_img,(2,0,1))/255.0)\n>```\n\n\n* [Hatena Blog StatsFragments\uff082015-07-05\uff09\u300cChainer \u3067 Deep Learning: model zoo \u3067 R-CNN \u3084\u308a\u305f\u3044\u300d](http://sinhrks.hatenablog.com/entry/2015/07/05/224745)\n\n>```\n>def get_image(url):\n>    \"\"\"URL\u304b\u3089\u753b\u50cf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\"\"\"\n>    import urllib\n>    import StringIO\n>    import PIL.Image as Image\n>    return >Image.open(StringIO.StringIO(urllib.urlopen(url).read())).convert(\"RGB\")\n>\n>def resize(image):\n>    \"\"\"\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\"\"\"\n>    import PIL.Image as Image\n>    return image.resize((256, 256), Image.ANTIALIAS)\n>```\n\n* [\u307e\u3055\u304b\u308a\u52df\u96c6\u4e2d [Chainer] Deep Learning \u3067\u30a2\u30cb\u30e1 (\u3086\u308b\u3086\u308a) \u30ad\u30e3\u30e9\u30af\u30bf\u30fc\u3092\u8b58\u5225\u3059\u308b(2015/11/4\u5c11\u3057\u8ffd\u8a18)](http://wtkw.net/tech-blog/?p=34)\n\n> \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u53ce\u96c6\u306f\uff0cWeb\u30b5\u30a4\u30c8\u306e\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u306b\u3088\u3063\u3066\u534a\u81ea\u52d5\u534a\u624b\u52d5\u3067\u884c\u3044\u307e\u3057\u305f\uff0e\n>\u3086\u308b\u3086\u308a\u30ad\u30e3\u30e9\u4ee5\u5916\u306e\u9854\u753b\u50cf\u306f27000\u679a\u96c6\u3081\u307e\u3057\u305f\uff0e\n>\u3086\u308b\u3086\u308a\u3054\u3089\u304f\u90e8\u30ad\u30e3\u30e9\u306e\u753b\u50cf\u306f300\u679a\u305a\u3064\u96c6\u3081\uff0c\u56de\u8ee2\uff0c\u8f1d\u5ea6\u5909\u5316\u30671200\u679a\u305a\u3064\u306b\u6c34\u5897\u3057\u3057\u307e\u3057\u305f\uff0e\n>\u96c6\u3081\u305f\u753b\u50cf\u306f50 \u00d7 50 pixels\u306b\u30ea\u30b5\u30a4\u30ba\u3057\u307e\u3057\u305f\uff0e\n\n* [Hatena Blog \u753b\u50cf\u51e6\u7406\u3068\u304b\u6a5f\u68b0\u5b66\u7fd2\u3068\u304b\uff082016-06-16\uff09\u300cchainer\u306eimagenet\u30b5\u30f3\u30d7\u30eb\u3092opencv\u3092\u4f7f\u3063\u3066\u52d5\u753b\u306b\u9069\u7528\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u300d](http://hiro2o2.hatenablog.jp/entry/2016/06/16/205058)\n\n___\n", "tags": ["DeepLearning", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "Chainer", "\u753b\u50cf\u51e6\u7406", "\u753b\u50cf\u8a8d\u8b58"]}