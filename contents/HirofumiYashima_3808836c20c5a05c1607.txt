{"context": "\n\n\uff08 \u53c2\u7167\u3057\u305f\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \uff09\n\n\ngithub.com/DingKe/qrnn/blob/master/imbd_qrnn.py\nHirofumiYashima Qiita\u8a18\u4e8b \u300cQuasi-Recurrent Neural Network\uff08QRNN\uff09\u306e GitHub\u65e2\u5b58\u30b9\u30af\u30ea\u30d7\u30c8 \u3067\u3001NLP\u30c7\u30fc\u30bf\u4ee5\u5916 \u306e \u6570\u5024\u6642\u7cfb\u5217\u30c7\u30fc\u30bf \u306e \uff12\u30af\u30e9\u30b9\u5206\u985e\u30bf\u30b9\u30af \u304c \u89e3\u3051\u308b\u306f\u305a \u306a \u4ef6\u300d\n\n\n\nPython3 Jupyter notebook\u74b0\u5883 \u3067 \u5b9f\u884c\n\n\n\uff08 \u7d50\u679c \uff09\n\nsin\u66f2\u7dda \u306e 10\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3057\u3066\u307f\u305f\u3089\u3001\u4e88\u6e2c\u7d50\u679c \u304c \u3069\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf \u3067\u3082 \u5168\u3066\u540c\u3058\u6570\u5024 \u306b \u306a\u3063\u305f\u4ef6\n\n\uff08 \u30c7\u30d0\u30c3\u30af\u4e2d \uff09\n\n\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u3068 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3068\u3082\u306b\u3001\nmodel.fit(X, Y)\n\u306e X \u306b \u6e21\u3059 \u8aac\u660e\u5909\u6570 \u306f\u3001\n100\u500b\u306e\u6570\u5024\u914d\u5217\uff081\u5468\u671f\u5206\u306esin\u5024\uff09\n\u3067\u3059\u304c\u3001\nX \u306b\u306f\u3001\u6bce\u56de\u3001\n\u671f\u9996\u6642\u70b9 \u3068 \u671f\u672b\u6642\u70b9 \u3092 1\u6642\u70b9\u305a\u3064\u30b7\u30d5\u30c8\u3055\u305b\u305f\u300c100\u500b\u306e\u6570\u5024\u914d\u5217\u300d\n\u304c\u5165\u308a\u307e\u3059\u3002\n\u3064\u307e\u308a\u3001\n X \u3068 Y \u306b \u5b66\u7fd2\u7528 \u306e  \u5404\uff08X, Y) \u30da\u30a2\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u30eb\u30fc\u30d7\u51e6\u7406 \u3067 \u6e21\u3059\u969b\u3001\n\u96a3\u308a\u5408\u3046 X \u306f\u3001\u671f\u9996\u3068\u671f\u672b\u306e2\u6642\u70b9\u4ee5\u5916\u306e\u6570\u5024\u306f\u3001\u3059\u3079\u3066\u540c\u3058\u6570\u5024\u304c\u5165\u308a\u307e\u3059\u3002\n\u305d\u306e\u305f\u3081\u3001\n\u300c100\u500b\u306e\u6570\u5024\u914d\u5217\u300d\u3092 100\u3088\u308a\u5c0f\u3055\u3044\u6b21\u5143 \u306b \u6b21\u5143\u5727\u7e2e\u3055\u305b\u3066 embed \u3059\u308b\u3068\u3001\nX \u306f \u307b\u307d\u540c\u3058\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b \u306a\u3063\u3066\u3057\u307e\u3046\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3068\u3057\u3066 \u65b0\u3057\u3044 X \u3092 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb \u306b \u5165\u529b\u3059\u308b \u3068\u3001 \u3069\u306e X \u3082 embed \u3055\u308c\u305f\u6bb5\u968e \u3067 \u540c\u3058 \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b\u5909\u63db\u3055\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u540c\u3058 \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u304b\u3089 \u4e88\u6e2c predict \u3055\u308c\u308b Y \u306e \u63a8\u5b9a\u5024 \u3082 \u540c\u3058\u5024 \u306b \u306a\u308b \u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\nembed\u5c64 \u3092\u4f7f\u308f\u305a\u306b\u3001 Dense\u5c64 \u3092 \u5165\u529b\u5c64 \u306b \u3059\u308b\u3068\u826f\u3044\u304b\u3002\nDense\u5c64 \u306f \u5168\u7d50\u5408\u5c64 \u3067\u3001\u5168\u7d50\u5408\u5c64\u91cd\u307f\u6f14\u7b97\u3092\u884c\u3063\u305f\u7d50\u679c \u3092 relu \u306a\u3069\u306e\u6d3b\u6027\u5316\u95a2\u6570 \u3067 \n\u51fa\u529b\u3059\u308b\u5c64 \u3067\u3042\u308b\u305f\u3081\u3001\u5165\u529b\u30c7\u30fc\u30bf X \u306e \u6570\u5024 \u304c \u5909\u308f\u3063\u3066\u3057\u307e\u3044\u3001\u305d\u306e\u307e\u307e\u306e\u5f62 \u3067 \u6e21\u305b\u306a\u3044\u3002\n\u30fb https://keras.io/ja/layers/core/\n\u305d\u3053\u3067\u3001\u5165\u529b\u30c7\u30fc\u30bf X \u3092 \u305d\u306e\u307e\u307e\u306e\u5f62 \u3067 QRNN\u5c64 \u306b \u6e21\u3059 \u305f\u3081\u306b\u3001InputLayer\u5c64 \u3092 \u5165\u529b\u5c64 \u3068\u3057\u3066 \u5b9a\u7fa9\u3059\u308b \u3068 \u826f\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\nfrom keras.layers import InputLayer\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(784,)))\n\uff08 \u53c2\u8003 \uff09\n\u30fb \uff3bobsproth\u3055\u3093 Qiita\u8a18\u4e8b \u300cKeras\u306eSequential\u30e2\u30c7\u30eb\u3067InputLayer\u3092\u660e\u793a\u7684\u306b\u8ffd\u52a0\u3059\u308b\u300d\uff3d(http://qiita.com/obsproth/items/cb352dccda36e2fde8ff)\n\nsin\u66f2\u7dda \u306e \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\n\n\nPython3 Jupyter notebook\n%matplotlib inline\n\n\n\nyukiB\u3055\u3093 Qiita\u8a18\u4e8b \u3092 \u4e00\u90e8\u6539\u826f\u3057\u3066\u3001\u5b66\u7fd2\u7528 \uff06 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u751f\u6210\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f\n\n\nPython3 Jupyter notebook\ndef get_n_sequence_dataset_and_m_peripd_later_data(data, n_prev = 100, m = 1):  \n   \"\"\"\n   data should be pd.DataFrame()\n   \"\"\"\n\n   docX, docY = [], []\n   for i in range(len(data)-n_prev*m):\n\n       docX.append(data.iloc[i:i+n_prev].as_matrix())\n       docY.append(data.iloc[i+n_prev+(m-1)].as_matrix())\n\n   alsX = np.array(docX)\n   alsY = np.array(docY)\n\n   return alsX, alsY\n\n\n\nPython3 Jupyter notebook\ndef create_train_data_and_test_data(df, test_size=0.1, n_prev = 100, m = 1):  \n   \"\"\"\n   This just splits data to training and testing parts\n   \"\"\"\n   ntrn = round(len(df) * (1 - test_size))\n   ntrn = int(ntrn)\n   X_train, y_train = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[0:ntrn], n_prev, m)\n   X_test, y_test = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[ntrn:], n_prev, m)\n\n   return (X_train, y_train), (X_test, y_test)\n\n\n\nPython3 Jupyter notebook\nimport pandas as pd\nimport numpy as np\nimport math\nimport random\nimport seaborn as sns\n\n\n\n\u6b63\u5f26(sin)\u66f2\u7dda \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u4f5c\u6210\n\n\nPython3 Jupyter notebook\n# \u4e71\u6570\u306e\u7a2e\u3068\u3057\u3066\u30010\u3092\u6e21\u3059\nrandom.seed(0)\nrandom_factor = 0.05\n# sin\u66f2\u7dda 1\u5468\u671f\u3042\u305f\u308a\u306e\u30b9\u30c6\u30c3\u30d7\u6570\nsteps_per_cycle = 100\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 1000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12ce86be0>\n\n\n\nPython3 Jupyter notebook\n# 1\u5468\u671f\uff1d100\u6642\u70b9\nprint(steps_per_cycle)\n\n\n100\n\n\n\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09110\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210\n\n\nPython3 Jupyter notebook\n# 10\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e10\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=10)  \n\n\n\nPython3 Jupyter notebook\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  88501\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  88501\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  8500\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  8500\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\nPython3 Jupyter notebook\n# \u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u30011.5\u5468\u671f\u5206\uff1d150\u6642\u70b9 \u306e \u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n# \u76ee\u7684\u5909\u6570\uff08\u88ab\u8aac\u660e\u5909\u6570\uff09\u3068\u3057\u3066\u3001\uff11\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n\n\n\nPython3 Jupyter notebook\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n\n\n\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.03443538]\n [ 0.08851131]\n [ 0.11744915]\n [ 0.16364786]\n [ 0.24978178]\n [ 0.29996187]\n [ 0.3943597 ]\n [ 0.40790129]\n [ 0.47970153]\n [ 0.54284827]]\n\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.50809246]\n============================================\n\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n[[ 0.07660351]\n [ 0.09925078]\n [ 0.14849661]\n [ 0.20115338]\n [ 0.33418164]\n [ 0.41373467]\n [ 0.41169616]\n [ 0.45590469]\n [ 0.56993065]\n [ 0.58197538]]\n\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.58328127]\n\n\nPython3 Jupyter notebook\nfrom matplotlib import pyplot as plt\nplt.plot(X_train[0])\n\n\n[<matplotlib.lines.Line2D at 0x12f9e3a58>]\n\n\n\nPython3 Jupyter notebook\nplt.plot(X_test[0])\n\n\n[<matplotlib.lines.Line2D at 0x12c8246d8>]\n\n\n\nhttps://github.com/DingKe/qrnn/blob/master/imbd_qrnn.py\nhttp://qiita.com/HirofumiYashima/items/f4109a3c7bd67ccd829e\n\n\nGitHub \u6240\u53ce \u306e qrnn.py \u3092 \u5168\u30b3\u30fc\u30c9\u30d9\u30bf\u6253\u3061 \u3067 \u53d6\u5f97\n\n\n\nPython3 Jupyter notebook\n# https://github.com/DingKe/qrnn/blob/master/qrnn.py\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras import activations, initializations, regularizers, constraints\nfrom keras.layers import Layer, InputSpec\n\nfrom keras.utils.np_utils import conv_output_length\n\nimport theano\nimport theano.tensor as T\n\n\ndef _dropout(x, level, noise_shape=None, seed=None):\n    x = K.dropout(x, level, noise_shape, seed)\n    x *= (1. - level) # compensate for the scaling by the dropout\n    return x\n\n\nclass QRNN(Layer):\n    '''Qausi RNN\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n    # References\n        - [Qausi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n    '''\n    def __init__(self, output_dim, window_size=2,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 unroll=False, subsample_length=1,\n                 init='uniform', activation='tanh',\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None, \n                 dropout=0, weights=None,\n                 bias=True, input_dim=None, input_length=None,\n                 **kwargs):\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n\n        self.output_dim = output_dim\n        self.window_size = window_size\n        self.subsample = (subsample_length, 1)\n\n        self.bias = bias\n        self.init = initializations.get(init)\n        self.activation = activations.get(activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.dropout = dropout\n        if self.dropout is not None and 0. < self.dropout < 1.:\n            self.uses_learning_phase = True\n        self.initial_weights = weights\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(QRNN, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        input_dim = input_shape[2]\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.W_shape = (self.window_size, 1, input_dim, self.output_dim)\n\n        self.W_z = self.init(self.W_shape, name='{}_W_z'.format(self.name))\n        self.W_f = self.init(self.W_shape, name='{}_W_f'.format(self.name))\n        self.W_o = self.init(self.W_shape, name='{}_W_o'.format(self.name))\n        self.trainable_weights = [self.W_z, self.W_f, self.W_o]\n        self.W = K.concatenate([self.W_z, self.W_f, self.W_o], 1) \n\n        if self.bias:\n            self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n            self.b_f = K.zeros((self.output_dim,), name='{}_b_f'.format(self.name))\n            self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n            self.trainable_weights += [self.b_z, self.b_f, self.b_o]\n            self.b = K.concatenate([self.b_z, self.b_f, self.b_o])\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n        if self.bias and self.b_regularizer:\n            self.b_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        self.constraints = {}\n        if self.W_constraint:\n            self.constraints[self.W] = self.W_constraint\n        if self.bias and self.b_constraint:\n            self.constraints[self.b] = self.b_constraint\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def get_output_shape_for(self, input_shape):\n        length = input_shape[1]\n        if length:\n            length = conv_output_length(length + self.window_size - 1,\n                                        self.window_size,\n                                        'valid',\n                                        self.subsample[0])\n        if self.return_sequences:\n            return (input_shape[0], length, self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def compute_mask(self, input, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def get_initial_states(self, x):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.output_dim])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def call(self, x, mask=None):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = self.input_spec[0].shape\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(x)\n        constants = self.get_constants(x)\n        preprocessed_input = self.preprocess_input(x)\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                            initial_states,\n                                            go_backwards=self.go_backwards,\n                                            mask=mask,\n                                            constants=constants)\n        if self.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.states[i], states[i]))\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def preprocess_input(self, x):\n        if self.bias:\n            weights = zip(self.trainable_weights[0:3], self.trainable_weights[3:])\n        else:\n            weights = self.trainable_weights\n\n        if self.window_size > 1:\n            x = K.asymmetric_temporal_padding(x, self.window_size-1, 0)\n        x = K.expand_dims(x, 2)  # add a dummy dimension\n\n        # z, f, o\n        outputs = []\n        for param in weights:\n            if self.bias:\n               W, b = param\n            else:\n               W = param\n            output = K.conv2d(x, W, strides=self.subsample,\n                              border_mode='valid',\n                              dim_ordering='tf')\n            output = K.squeeze(output, 2)  # remove the dummy dimension\n            if self.bias:\n                output += K.reshape(b, (1, 1, self.output_dim))\n\n            outputs.append(output)\n\n        if self.dropout is not None and 0. < self.dropout < 1.:\n            f = K.sigmoid(outputs[1])\n            outputs[1] = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f)\n\n        return K.concatenate(outputs, 2)\n\n    def step(self, input, states):\n        prev_output = states[0]\n\n        z = input[:, :self.output_dim]\n        f = input[:, self.output_dim:2 * self.output_dim]\n        o = input[:, 2 * self.output_dim:]\n\n        z = self.activation(z)\n        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n        o = K.sigmoid(o)\n\n        output = f * prev_output + (1 - f) * z\n        output = o * output\n\n        return output, [output]\n\n    def get_constants(self, x):\n        constants = []\n        return constants\n\n    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': self.init.__name__,\n                  'window_size': self.window_size,\n                  'subsample_length': self.subsample[0],\n                  'activation': self.activation.__name__,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n                  'bias': self.bias,\n                  'input_dim': self.input_dim,\n                  'input_length': self.input_length}\n        base_config = super(QRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\n\nqrnn/imbd_qrnn.py \u3092 sin\u66f2\u7dda \u306e \u56de\u5e30\u4e88\u6e2c\u30bf\u30b9\u30af\u7528 \u306b \u66f8\u304d\u5909\u3048\u308b\u3002\n\n\nPython3 Jupyter notebook\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.datasets import imdb\n\n\n\nPython3 Jupyter notebook\n#max_features = 20000 # \u5165\u529b\u5024\u306b\u542b\u307e\u308c\u308b\u6570\u5024\u306e\u6700\u5927\u5024\n# ( \u53c2\u8003 \uff09 https://keras.io/ja/layers/embeddings/\n#\n#maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n#batch_size = 32 \n\nprint('Build model...')\nmodel = Sequential()\n# \u5165\u529b\u30c7\u30fc\u30bf\uff08\u8981\u7d20\u6570\uff1a150\uff09 \u3092 128\u6b21\u5143 \u306e \u30d9\u30af\u30c8\u30eb \u306b \u60c5\u5831\u5727\u7e2e\u3059\u308b\nmodel.add(Embedding(20000, 128, input_length=150, dropout=0.2))\n#model.add(QRNN(128, window_size=3, dropout=0.2, \n#               W_regularizer=l2(1e-4), b_regularizer=l2(1e-4), \n#               W_constraint=maxnorm(10), b_constraint=maxnorm(10)))\n\n# 128\u6b21\u5143 \u3092 QRNN\u5c64 \u3067 64\u6b21\u5143 \u306e \u30d9\u30af\u30c8\u30eb \u306b \u60c5\u5831\u5727\u7e2e\u3059\u308b\nmodel.add(QRNN(64, window_size=3, dropout=0.2, \n               W_regularizer=l2(1e-4), b_regularizer=l2(1e-4), \n               W_constraint=maxnorm(10), b_constraint=maxnorm(10)))\n#model.add(Dense(1))\nmodel.add(Dense(1))\n#model.add(Activation('sigmoid'))\nmodel.add(Activation(\"linear\"))  \n\n\nBuild model...\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/topology.py:379: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n  warnings.warn('The `regularizers` property of layers/models '\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/regularizers.py:18: UserWarning: The `set_param` method on regularizers is deprecated. It no longer does anything, and it will be removed after 06/2017.\n  warnings.warn('The `set_param` method on regularizers is deprecated. '\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/topology.py:371: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n  warnings.warn('The `regularizers` property of '\n\n\nPython3 Jupyter notebook\n# try using different optimizers and different optimizer configs\n\nmodel.compile(loss=\"mean_squared_error\",\n              optimizer=\"rmsprop\")\n\n#model.compile(loss='binary_crossentropy',\n#              optimizer='adam',\n#              metrics=['accuracy'])\n\n\n\nPython3 Jupyter notebook\nprint('Train...')\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel.fit(X_train[0:steps_per_cycle],\n          y_train[0:steps_per_cycle],\n          batch_size=int(len(X_train)/6),\n          nb_epoch=10,\n          validation_data=(X_test, y_test))\n\n#model_history = model.fit(X_train[0:steps_per_cycle], \n#                          y_train[0:steps_per_cycle], \n#                          batch_size=int(len(X_train)/6), \n#                          nb_epoch=10, \n#                          validation_split=0.05,\n#                          callbacks=[early_stopping])\n\n\nTrain...\n\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n<ipython-input-90-494e02375660> in <module>()\n      8           batch_size=int(len(X_train)/6),\n      9           nb_epoch=10,\n---> 10           validation_data=(X_test, y_test))\n     11 \n     12 #model_history = model.fit(X_train[0:steps_per_cycle],\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/models.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\n    670                               class_weight=class_weight,\n    671                               sample_weight=sample_weight,\n--> 672                               initial_epoch=initial_epoch)\n    673 \n    674     def evaluate(self, x, y, batch_size=32, verbose=1,\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\n   1114             class_weight=class_weight,\n   1115             check_batch_axis=False,\n-> 1116             batch_size=batch_size)\n   1117         # prepare validation data\n   1118         if validation_data:\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\n   1027                                    self.internal_input_shapes,\n   1028                                    check_batch_axis=False,\n-> 1029                                    exception_prefix='model input')\n   1030         y = standardize_input_data(y, self.output_names,\n   1031                                    output_shapes,\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n    110                                  ' to have ' + str(len(shapes[i])) +\n    111                                  ' dimensions, but got array with shape ' +\n--> 112                                  str(array.shape))\n    113             for j, (dim, ref_dim) in enumerate(zip(array.shape, shapes[i])):\n    114                 if not j and not check_batch_axis:\n\n\nValueError: Error when checking model input: expected embedding_input_5 to have 2 dimensions, but got array with shape (100, 150, 1)\n\n\n\u5165\u529b\u30c7\u30fc\u30bf\u306e\u6b21\u5143(shape\uff09\u9593\u9055\u3044\n\n\n\u5165\u529b\u30c7\u30fc\u30bf\u306e\u6b21\u5143(shape\uff09 \u3092 \u78ba\u8a8d\n\n\nPython3 Jupyter notebook\n# 1.5\u5468\u671f\u5206(=150\u8981\u7d20\uff09\u306e\u30c7\u30fc\u30bf \u304c\u3001\u30b5\u30f3\u30d7\u30eb\u6570 100 (=1\u5468\u671f\u5206 \u203b\u76ee\u7684\u5909\u6570\u3092\uff11\u5468\u671f\u5206\u3001\u78ba\u4fdd\u3059\u308b\u305f\u3081\uff09 \u751f\u6210\u3057\u3066\u3044\u308b\u3002\nX_train[0:steps_per_cycle].shape\n\n\n(100, 150, 1)\n\n\nPython3 Jupyter notebook\nX_train[0:steps_per_cycle].reshape(100, 150)\n\n\narray([[ 0.03443538,  0.08851131,  0.11744915, ...,  0.15404703,\n         0.0837174 ,  0.09574251],\n       [ 0.08851131,  0.11744915,  0.16364786, ...,  0.0837174 ,\n         0.09574251, -0.0172632 ],\n       [ 0.11744915,  0.16364786,  0.24978178, ...,  0.09574251,\n        -0.0172632 , -0.10926813],\n       ..., \n       [-0.22822898, -0.11413612, -0.06414337, ...,  0.35406377,\n         0.32352641,  0.21250816],\n       [-0.11413612, -0.06414337,  0.01301437, ...,  0.32352641,\n         0.21250816,  0.20910005],\n       [-0.06414337,  0.01301437,  0.09718597, ...,  0.21250816,\n         0.20910005,  0.17293252]])\n\n\nPython3 Jupyter notebook\nX_train[0:steps_per_cycle].reshape(100, 150).shape\n\n\n(100, 150)\n\n\nPython3 Jupyter notebook\ny_train[0:steps_per_cycle].shape\n\n\n(100, 1)\n\n\nPython3 Jupyter notebook\nprint('Train...')\n\nX_train = X_train[0:steps_per_cycle].reshape(100, 150)\ny_train = y_train[0:steps_per_cycle]\n\nX_test = X_test[0:steps_per_cycle].reshape(100, 150)\ny_test = y_test[0:steps_per_cycle]\n\nmodel_history = model.fit(X_train,\n                          y_train,\n                          batch_size=int(len(X_train)/6),\n                          nb_epoch=20,\n                          validation_data=(X_test, y_test)\n                          )\n\n\nTrain...\nTrain on 100 samples, validate on 100 samples\nEpoch 1/20\n100/100 [==============================] - 5s - loss: 0.5111 - val_loss: 0.4999\nEpoch 2/20\n100/100 [==============================] - 3s - loss: 0.5059 - val_loss: 0.5001\nEpoch 3/20\n100/100 [==============================] - 3s - loss: 0.5044 - val_loss: 0.5022\nEpoch 4/20\n100/100 [==============================] - 3s - loss: 0.5040 - val_loss: 0.4999\nEpoch 5/20\n100/100 [==============================] - 4s - loss: 0.5051 - val_loss: 0.5001\nEpoch 6/20\n100/100 [==============================] - 3s - loss: 0.5057 - val_loss: 0.4999\nEpoch 7/20\n100/100 [==============================] - 3s - loss: 0.5043 - val_loss: 0.5001\nEpoch 8/20\n100/100 [==============================] - 3s - loss: 0.5054 - val_loss: 0.4999\nEpoch 9/20\n100/100 [==============================] - 3s - loss: 0.5025 - val_loss: 0.5006\nEpoch 10/20\n100/100 [==============================] - 6s - loss: 0.5054 - val_loss: 0.5014\nEpoch 11/20\n100/100 [==============================] - 5s - loss: 0.5016 - val_loss: 0.4999\nEpoch 12/20\n100/100 [==============================] - 3s - loss: 0.5060 - val_loss: 0.5001\nEpoch 13/20\n100/100 [==============================] - 5s - loss: 0.5070 - val_loss: 0.5052\nEpoch 14/20\n100/100 [==============================] - 6s - loss: 0.5058 - val_loss: 0.5004\nEpoch 15/20\n100/100 [==============================] - 6s - loss: 0.5031 - val_loss: 0.5008\nEpoch 16/20\n100/100 [==============================] - 4s - loss: 0.5047 - val_loss: 0.5008\nEpoch 17/20\n100/100 [==============================] - 4s - loss: 0.5039 - val_loss: 0.5000\nEpoch 18/20\n100/100 [==============================] - 5s - loss: 0.5031 - val_loss: 0.5011\nEpoch 19/20\n100/100 [==============================] - 3s - loss: 0.5041 - val_loss: 0.5004\nEpoch 20/20\n100/100 [==============================] - 6s - loss: 0.5053 - val_loss: 0.5000\n\n\n/Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n\n\nPython3 Jupyter notebook\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n\n\n\n\nloss\u304c\u4e0a\u4e0b\u3092\u7e70\u308a\u8fd4\u3057\u3066\u3044\u308b\u3002\n\n\n\u6b63\u3057\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\n\n\nPython3 Jupyter notebook\nmodel.summary()\n\n\n\nembedding\u5c64\u3067\u3001150\u6b21\u5143\uff08sin\u66f2\u7dda 1.5\u5468\u671f\uff1d150\u6642\u70b9\u30c7\u30fc\u30bf\uff09 \u3092 128\u6b21\u5143 \u306b \u5727\u7e2e\u3057\u3066\u3044\u308b\nQRNN\u5c64\u3067\u3001128\u6b21\u5143 \u3092 64\u6b21\u5143 \u306b \u5727\u7e2e\u3057\u3066\u3044\u308b\n\n____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    embedding_5 (Embedding)          (None, 150, 128)      2560000     embedding_input_5[0][0]          \n    ____________________________________________________________________________________________________\n    qrnn_9 (QRNN)                    (None, 64)            73920       embedding_5[0][0]                \n    ____________________________________________________________________________________________________\n    dense_6 (Dense)                  (None, 1)             65          qrnn_9[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_5 (Activation)        (None, 1)             0           dense_6[0][0]                    \n    ====================================================================================================\n    Total params: 2,633,985\n    Trainable params: 2,633,985\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n\n\nPython3 Jupyter notebook\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0810\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\n# X_test = X_test[0:steps_per_cycle].reshape(100, 150) \u3092 \u5b9f\u65bd\u6e08\u307f\u3002\n\npredicted_10_ahead = model.predict(X_test) \nlen(predicted_10_ahead)\n\n\n100\n\n\n\u4e88\u6e2c\u5024\u304c\u5168\u3066\u540c\u3058\u5024\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u3002\n\n\nPython3 Jupyter notebook\nfrom pprint import pprint\npprint(predicted_10_ahead[0:10])\n\n\narray([[ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085],\n       [ 0.01007085]], dtype=float32)\n\n\nPython3 Jupyter notebook\ndataf =  pd.DataFrame(predicted_10_ahead)\ndataf.columns = [\"predict\"]\ndataf[\"true_value(observed_value)\"] = y_test\ndataf.plot()\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x12ff09eb8>\n\n\n###__\uff08 \u53c2\u7167\u3057\u305f\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \uff09__\n\n* [github.com/DingKe/qrnn/blob/master/imbd_qrnn.py](https://github.com/DingKe/qrnn/blob/master/imbd_qrnn.py)\n\n* [HirofumiYashima Qiita\u8a18\u4e8b \u300cQuasi-Recurrent Neural Network\uff08QRNN\uff09\u306e GitHub\u65e2\u5b58\u30b9\u30af\u30ea\u30d7\u30c8 \u3067\u3001NLP\u30c7\u30fc\u30bf\u4ee5\u5916 \u306e \u6570\u5024\u6642\u7cfb\u5217\u30c7\u30fc\u30bf \u306e \uff12\u30af\u30e9\u30b9\u5206\u985e\u30bf\u30b9\u30af \u304c \u89e3\u3051\u308b\u306f\u305a \u306a \u4ef6\u300d](http://qiita.com/HirofumiYashima/items/f4109a3c7bd67ccd829e)\n\n___\n\n##__Python3 Jupyter notebook\u74b0\u5883 \u3067 \u5b9f\u884c__\n\n###__\uff08 \u7d50\u679c \uff09__\n\nsin\u66f2\u7dda \u306e 10\u6642\u70b9\u5148 \u3092 \u4e88\u6e2c\u3057\u3066\u307f\u305f\u3089\u3001\u4e88\u6e2c\u7d50\u679c \u304c \u3069\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf \u3067\u3082 \u5168\u3066\u540c\u3058\u6570\u5024 \u306b \u306a\u3063\u305f\u4ef6\n\n###__\uff08 \u30c7\u30d0\u30c3\u30af\u4e2d \uff09__\n\n\n\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf \u3068 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3068\u3082\u306b\u3001\nmodel.fit(X, Y)\n\u306e X \u306b \u6e21\u3059 \u8aac\u660e\u5909\u6570 \u306f\u3001\n100\u500b\u306e\u6570\u5024\u914d\u5217\uff081\u5468\u671f\u5206\u306esin\u5024\uff09\n\u3067\u3059\u304c\u3001\nX \u306b\u306f\u3001\u6bce\u56de\u3001\n\u671f\u9996\u6642\u70b9 \u3068 \u671f\u672b\u6642\u70b9 \u3092 1\u6642\u70b9\u305a\u3064\u30b7\u30d5\u30c8\u3055\u305b\u305f\u300c100\u500b\u306e\u6570\u5024\u914d\u5217\u300d\n\u304c\u5165\u308a\u307e\u3059\u3002\n\n\u3064\u307e\u308a\u3001\n X \u3068 Y \u306b \u5b66\u7fd2\u7528 \u306e  \u5404\uff08X, Y) \u30da\u30a2\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u30eb\u30fc\u30d7\u51e6\u7406 \u3067 \u6e21\u3059\u969b\u3001\n\u96a3\u308a\u5408\u3046 X \u306f\u3001\u671f\u9996\u3068\u671f\u672b\u306e2\u6642\u70b9\u4ee5\u5916\u306e\u6570\u5024\u306f\u3001\u3059\u3079\u3066\u540c\u3058\u6570\u5024\u304c\u5165\u308a\u307e\u3059\u3002\n\n\u305d\u306e\u305f\u3081\u3001\n\u300c100\u500b\u306e\u6570\u5024\u914d\u5217\u300d\u3092 100\u3088\u308a\u5c0f\u3055\u3044\u6b21\u5143 \u306b \u6b21\u5143\u5727\u7e2e\u3055\u305b\u3066 embed \u3059\u308b\u3068\u3001\nX \u306f \u307b\u307d\u540c\u3058\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b \u306a\u3063\u3066\u3057\u307e\u3046\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\n\u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u3068\u3057\u3066 \u65b0\u3057\u3044 X \u3092 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb \u306b \u5165\u529b\u3059\u308b \u3068\u3001 \u3069\u306e X \u3082 embed \u3055\u308c\u305f\u6bb5\u968e \u3067 \u540c\u3058 \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b\u5909\u63db\u3055\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u540c\u3058 \u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u304b\u3089 \u4e88\u6e2c predict \u3055\u308c\u308b Y \u306e \u63a8\u5b9a\u5024 \u3082 \u540c\u3058\u5024 \u306b \u306a\u308b \u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\nembed\u5c64 \u3092\u4f7f\u308f\u305a\u306b\u3001 Dense\u5c64 \u3092 \u5165\u529b\u5c64 \u306b \u3059\u308b\u3068\u826f\u3044\u304b\u3002\n\nDense\u5c64 \u306f \u5168\u7d50\u5408\u5c64 \u3067\u3001\u5168\u7d50\u5408\u5c64\u91cd\u307f\u6f14\u7b97\u3092\u884c\u3063\u305f\u7d50\u679c \u3092 relu \u306a\u3069\u306e\u6d3b\u6027\u5316\u95a2\u6570 \u3067 \n\u51fa\u529b\u3059\u308b\u5c64 \u3067\u3042\u308b\u305f\u3081\u3001\u5165\u529b\u30c7\u30fc\u30bf X \u306e \u6570\u5024 \u304c \u5909\u308f\u3063\u3066\u3057\u307e\u3044\u3001\u305d\u306e\u307e\u307e\u306e\u5f62 \u3067 \u6e21\u305b\u306a\u3044\u3002\n\n\u30fb https://keras.io/ja/layers/core/\n\n\u305d\u3053\u3067\u3001\u5165\u529b\u30c7\u30fc\u30bf X \u3092 \u305d\u306e\u307e\u307e\u306e\u5f62 \u3067 QRNN\u5c64 \u306b \u6e21\u3059 \u305f\u3081\u306b\u3001InputLayer\u5c64 \u3092 \u5165\u529b\u5c64 \u3068\u3057\u3066 \u5b9a\u7fa9\u3059\u308b \u3068 \u826f\u3044\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\nfrom keras.layers import InputLayer\n\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(784,)))\n\n__\uff08 \u53c2\u8003 \uff09__\n\n\u30fb \uff3bobsproth\u3055\u3093 Qiita\u8a18\u4e8b \u300cKeras\u306eSequential\u30e2\u30c7\u30eb\u3067InputLayer\u3092\u660e\u793a\u7684\u306b\u8ffd\u52a0\u3059\u308b\u300d\uff3d(http://qiita.com/obsproth/items/cb352dccda36e2fde8ff)\n\n##__sin\u66f2\u7dda \u306e \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\n%matplotlib inline\n```\n\n####__yukiB\u3055\u3093 Qiita\u8a18\u4e8b \u3092 \u4e00\u90e8\u6539\u826f\u3057\u3066\u3001\u5b66\u7fd2\u7528 \uff06 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u751f\u6210\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f__\n\n```{python:Python3 Jupyter notebook}\ndef get_n_sequence_dataset_and_m_peripd_later_data(data, n_prev = 100, m = 1):  \n   \"\"\"\n   data should be pd.DataFrame()\n   \"\"\"\n\n   docX, docY = [], []\n   for i in range(len(data)-n_prev*m):\n\n       docX.append(data.iloc[i:i+n_prev].as_matrix())\n       docY.append(data.iloc[i+n_prev+(m-1)].as_matrix())\n\n   alsX = np.array(docX)\n   alsY = np.array(docY)\n\n   return alsX, alsY\n```\n\n\n```{python:Python3 Jupyter notebook}\ndef create_train_data_and_test_data(df, test_size=0.1, n_prev = 100, m = 1):  \n   \"\"\"\n   This just splits data to training and testing parts\n   \"\"\"\n   ntrn = round(len(df) * (1 - test_size))\n   ntrn = int(ntrn)\n   X_train, y_train = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[0:ntrn], n_prev, m)\n   X_test, y_test = get_n_sequence_dataset_and_m_peripd_later_data(df.iloc[ntrn:], n_prev, m)\n\n   return (X_train, y_train), (X_test, y_test)\n```\n\n\n```{python:Python3 Jupyter notebook}\nimport pandas as pd\nimport numpy as np\nimport math\nimport random\nimport seaborn as sns\n```\n\n####__\u6b63\u5f26(sin)\u66f2\u7dda \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3092 \u4f5c\u6210__\n\n```{python:Python3 Jupyter notebook}\n# \u4e71\u6570\u306e\u7a2e\u3068\u3057\u3066\u30010\u3092\u6e21\u3059\nrandom.seed(0)\nrandom_factor = 0.05\n# sin\u66f2\u7dda 1\u5468\u671f\u3042\u305f\u308a\u306e\u30b9\u30c6\u30c3\u30d7\u6570\nsteps_per_cycle = 100\n# \u751f\u6210\u3059\u308b\u5468\u671f\u6570\nnumber_of_cycles = 1000\n\n# \u533a\u9593 -1.0 \u301c +1.0 \u306e \u4e00\u69d8\u4e71\u6570 \u4ed8\u304d \u306e sin\u66f2\u7dda \ndf = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[\"t\"])\ndf[\"sin_t\"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)\n                                               + random.uniform(-1.0, +1.0) * random_factor))\n# 2\u5468\u671f\u5206\u3001\u63cf\u753b\ndf[[\"sin_t\"]].head(steps_per_cycle * 2).plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12ce86be0>\n\n\n\n\n![output_4_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/4ae8440b-c6f2-fbb2-93e3-84cbec274c2f.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\n# 1\u5468\u671f\uff1d100\u6642\u70b9\nprint(steps_per_cycle)\n```\n\n    100\n\n####__\uff08\u8aac\u660e\u5909\u6570\uff09100\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf \u3068 \uff08\u76ee\u7684\u5909\u6570\uff09110\u6642\u70b9\u76ee\u306e\u30c7\u30fc\u30bf \u306e \u30da\u30a2 \u3092 \u751f\u6210__\n\n```{python:Python3 Jupyter notebook}\n# 10\u6642\u70b9\u5148\u306e\u5024\u3092\u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u7528\u53ca\u3073\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n# \u76f4\u8fd1 1.5\u5468\u671f\u5206\u306e\u5024\u63a8\u79fb\u30d1\u30bf\u30fc\u30f3 \u3092\u624b\u639b\u304b\u308a\u3000\u306b\u3001\u305d\u306e10\u6642\u70b9\u5148 \u306e\uff08\uff11\u70b9\u306e\uff09\u5024 \u3092 \u4e88\u6e2c\u3059\u308b\u56de\u5e30\u30e2\u30c7\u30eb\n(X_train, y_train), (X_test, y_test) = create_train_data_and_test_data(df[[\"sin_t\"]], \n                                                                        n_prev=int(steps_per_cycle*1.5),\n                                                                        m=10)  \n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4ef6\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_train))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_train))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(X_test))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570: \", len(y_test))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  88501\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  88501\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  8500\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570:  8500\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\u8981\u7d20\u6570\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_train[0]))\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_train[0]))\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(X_test[0]))\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", len(y_test[0]))\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  150\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  1\n\n\n\n```{python:Python3 Jupyter notebook}\n# \u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u30011.5\u5468\u671f\u5206\uff1d150\u6642\u70b9 \u306e \u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n# \u76ee\u7684\u5909\u6570\uff08\u88ab\u8aac\u660e\u5909\u6570\uff09\u3068\u3057\u3066\u3001\uff11\u6642\u70b9\u5206\u306e\u30c7\u30fc\u30bf\u304c\u5165\u3063\u3066\u3044\u308b\u3002\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u306e\uff11\u90e8\u3092\u78ba\u8a8d\nprint(\"\u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_train[0][0:10])\nprint(\"\u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_train[0])\nprint(\"============================================\")\nprint(\"\u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \")\nprint(X_test[0][0:10])\nprint(\"\u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \", y_test[0])\n```\n\n    \u5b66\u7fd2\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.03443538]\n     [ 0.08851131]\n     [ 0.11744915]\n     [ 0.16364786]\n     [ 0.24978178]\n     [ 0.29996187]\n     [ 0.3943597 ]\n     [ 0.40790129]\n     [ 0.47970153]\n     [ 0.54284827]]\n    \u5b66\u7fd2\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.50809246]\n    ============================================\n    \u691c\u8a3c\u7528 \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570: \n    [[ 0.07660351]\n     [ 0.09925078]\n     [ 0.14849661]\n     [ 0.20115338]\n     [ 0.33418164]\n     [ 0.41373467]\n     [ 0.41169616]\n     [ 0.45590469]\n     [ 0.56993065]\n     [ 0.58197538]]\n    \u691c\u8a3c\u7528 \u76ee\u7684\u5909\u6570\u30c7\u30fc\u30bf\u8981\u7d20\u6570:  [-0.58328127]\n\n\n\n```{python:Python3 Jupyter notebook}\nfrom matplotlib import pyplot as plt\nplt.plot(X_train[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x12f9e3a58>]\n\n\n\n\n![output_11_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/ce336e4d-ad03-382c-3de5-d8515c9032ee.png)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nplt.plot(X_test[0])\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x12c8246d8>]\n\n\n\n\n![output_12_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/4e022793-d1c3-599e-ee6f-21cf1a001281.png)\n\n\n\n* https://github.com/DingKe/qrnn/blob/master/imbd_qrnn.py\n* http://qiita.com/HirofumiYashima/items/f4109a3c7bd67ccd829e\n\n###__GitHub \u6240\u53ce \u306e qrnn.py \u3092 \u5168\u30b3\u30fc\u30c9\u30d9\u30bf\u6253\u3061 \u3067 \u53d6\u5f97__\n\n<img width=\"1244\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-03-12 21.39.17.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/4f2b2aa7-28e9-34b8-19a1-9eb8fc77a3b7.png\">\n\n\n```{python:Python3 Jupyter notebook}\n# https://github.com/DingKe/qrnn/blob/master/qrnn.py\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras import activations, initializations, regularizers, constraints\nfrom keras.layers import Layer, InputSpec\n\nfrom keras.utils.np_utils import conv_output_length\n\nimport theano\nimport theano.tensor as T\n\n\ndef _dropout(x, level, noise_shape=None, seed=None):\n    x = K.dropout(x, level, noise_shape, seed)\n    x *= (1. - level) # compensate for the scaling by the dropout\n    return x\n\n\nclass QRNN(Layer):\n    '''Qausi RNN\n    # Arguments\n        output_dim: dimension of the internal projections and the final output.\n    # References\n        - [Qausi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n    '''\n    def __init__(self, output_dim, window_size=2,\n                 return_sequences=False, go_backwards=False, stateful=False,\n                 unroll=False, subsample_length=1,\n                 init='uniform', activation='tanh',\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None, \n                 dropout=0, weights=None,\n                 bias=True, input_dim=None, input_length=None,\n                 **kwargs):\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n\n        self.output_dim = output_dim\n        self.window_size = window_size\n        self.subsample = (subsample_length, 1)\n\n        self.bias = bias\n        self.init = initializations.get(init)\n        self.activation = activations.get(activation)\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.dropout = dropout\n        if self.dropout is not None and 0. < self.dropout < 1.:\n            self.uses_learning_phase = True\n        self.initial_weights = weights\n\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(QRNN, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        if self.stateful:\n            self.reset_states()\n        else:\n            # initial states: all-zero tensor of shape (output_dim)\n            self.states = [None]\n\n        input_dim = input_shape[2]\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.W_shape = (self.window_size, 1, input_dim, self.output_dim)\n\n        self.W_z = self.init(self.W_shape, name='{}_W_z'.format(self.name))\n        self.W_f = self.init(self.W_shape, name='{}_W_f'.format(self.name))\n        self.W_o = self.init(self.W_shape, name='{}_W_o'.format(self.name))\n        self.trainable_weights = [self.W_z, self.W_f, self.W_o]\n        self.W = K.concatenate([self.W_z, self.W_f, self.W_o], 1) \n\n        if self.bias:\n            self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n            self.b_f = K.zeros((self.output_dim,), name='{}_b_f'.format(self.name))\n            self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n            self.trainable_weights += [self.b_z, self.b_f, self.b_o]\n            self.b = K.concatenate([self.b_z, self.b_f, self.b_o])\n\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n        if self.bias and self.b_regularizer:\n            self.b_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        self.constraints = {}\n        if self.W_constraint:\n            self.constraints[self.W] = self.W_constraint\n        if self.bias and self.b_constraint:\n            self.constraints[self.b] = self.b_constraint\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def get_output_shape_for(self, input_shape):\n        length = input_shape[1]\n        if length:\n            length = conv_output_length(length + self.window_size - 1,\n                                        self.window_size,\n                                        'valid',\n                                        self.subsample[0])\n        if self.return_sequences:\n            return (input_shape[0], length, self.output_dim)\n        else:\n            return (input_shape[0], self.output_dim)\n\n    def compute_mask(self, input, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def get_initial_states(self, x):\n        # build an all-zero tensor of shape (samples, output_dim)\n        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.output_dim])  # (samples, output_dim)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def reset_states(self):\n        assert self.stateful, 'Layer must be stateful.'\n        input_shape = self.input_spec[0].shape\n        if not input_shape[0]:\n            raise Exception('If a RNN is stateful, a complete ' +\n                            'input_shape must be provided (including batch size).')\n        if hasattr(self, 'states'):\n            K.set_value(self.states[0],\n                        np.zeros((input_shape[0], self.output_dim)))\n        else:\n            self.states = [K.zeros((input_shape[0], self.output_dim))]\n\n    def call(self, x, mask=None):\n        # input shape: (nb_samples, time (padded with zeros), input_dim)\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec with a complete input shape.\n        input_shape = self.input_spec[0].shape\n        if self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(x)\n        constants = self.get_constants(x)\n        preprocessed_input = self.preprocess_input(x)\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n                                            initial_states,\n                                            go_backwards=self.go_backwards,\n                                            mask=mask,\n                                            constants=constants)\n        if self.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.states[i], states[i]))\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def preprocess_input(self, x):\n        if self.bias:\n            weights = zip(self.trainable_weights[0:3], self.trainable_weights[3:])\n        else:\n            weights = self.trainable_weights\n\n        if self.window_size > 1:\n            x = K.asymmetric_temporal_padding(x, self.window_size-1, 0)\n        x = K.expand_dims(x, 2)  # add a dummy dimension\n\n        # z, f, o\n        outputs = []\n        for param in weights:\n            if self.bias:\n               W, b = param\n            else:\n               W = param\n            output = K.conv2d(x, W, strides=self.subsample,\n                              border_mode='valid',\n                              dim_ordering='tf')\n            output = K.squeeze(output, 2)  # remove the dummy dimension\n            if self.bias:\n                output += K.reshape(b, (1, 1, self.output_dim))\n\n            outputs.append(output)\n\n        if self.dropout is not None and 0. < self.dropout < 1.:\n            f = K.sigmoid(outputs[1])\n            outputs[1] = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f)\n\n        return K.concatenate(outputs, 2)\n\n    def step(self, input, states):\n        prev_output = states[0]\n\n        z = input[:, :self.output_dim]\n        f = input[:, self.output_dim:2 * self.output_dim]\n        o = input[:, 2 * self.output_dim:]\n\n        z = self.activation(z)\n        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n        o = K.sigmoid(o)\n\n        output = f * prev_output + (1 - f) * z\n        output = o * output\n\n        return output, [output]\n\n    def get_constants(self, x):\n        constants = []\n        return constants\n\n    def get_config(self):\n        config = {'output_dim': self.output_dim,\n                  'init': self.init.__name__,\n                  'window_size': self.window_size,\n                  'subsample_length': self.subsample[0],\n                  'activation': self.activation.__name__,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n                  'bias': self.bias,\n                  'input_dim': self.input_dim,\n                  'input_length': self.input_length}\n        base_config = super(QRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```\n\n###__qrnn/imbd_qrnn.py \u3092 sin\u66f2\u7dda \u306e \u56de\u5e30\u4e88\u6e2c\u30bf\u30b9\u30af\u7528 \u306b \u66f8\u304d\u5909\u3048\u308b\u3002__\n\n```{python:Python3 Jupyter notebook}\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.datasets import imdb\n```\n\n\n```{python:Python3 Jupyter notebook}\n#max_features = 20000 # \u5165\u529b\u5024\u306b\u542b\u307e\u308c\u308b\u6570\u5024\u306e\u6700\u5927\u5024\n# ( \u53c2\u8003 \uff09 https://keras.io/ja/layers/embeddings/\n#\n#maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n#batch_size = 32 \n\nprint('Build model...')\nmodel = Sequential()\n# \u5165\u529b\u30c7\u30fc\u30bf\uff08\u8981\u7d20\u6570\uff1a150\uff09 \u3092 128\u6b21\u5143 \u306e \u30d9\u30af\u30c8\u30eb \u306b \u60c5\u5831\u5727\u7e2e\u3059\u308b\nmodel.add(Embedding(20000, 128, input_length=150, dropout=0.2))\n#model.add(QRNN(128, window_size=3, dropout=0.2, \n#               W_regularizer=l2(1e-4), b_regularizer=l2(1e-4), \n#               W_constraint=maxnorm(10), b_constraint=maxnorm(10)))\n\n# 128\u6b21\u5143 \u3092 QRNN\u5c64 \u3067 64\u6b21\u5143 \u306e \u30d9\u30af\u30c8\u30eb \u306b \u60c5\u5831\u5727\u7e2e\u3059\u308b\nmodel.add(QRNN(64, window_size=3, dropout=0.2, \n               W_regularizer=l2(1e-4), b_regularizer=l2(1e-4), \n               W_constraint=maxnorm(10), b_constraint=maxnorm(10)))\n#model.add(Dense(1))\nmodel.add(Dense(1))\n#model.add(Activation('sigmoid'))\nmodel.add(Activation(\"linear\"))  \n```\n\n    Build model...\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/topology.py:379: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n      warnings.warn('The `regularizers` property of layers/models '\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/regularizers.py:18: UserWarning: The `set_param` method on regularizers is deprecated. It no longer does anything, and it will be removed after 06/2017.\n      warnings.warn('The `set_param` method on regularizers is deprecated. '\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/topology.py:371: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n      warnings.warn('The `regularizers` property of '\n\n\n\n```{python:Python3 Jupyter notebook}\n# try using different optimizers and different optimizer configs\n\nmodel.compile(loss=\"mean_squared_error\",\n              optimizer=\"rmsprop\")\n\n#model.compile(loss='binary_crossentropy',\n#              optimizer='adam',\n#              metrics=['accuracy'])\n```\n\n\n```{python:Python3 Jupyter notebook}\nprint('Train...')\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=0)\n\nmodel.fit(X_train[0:steps_per_cycle],\n          y_train[0:steps_per_cycle],\n          batch_size=int(len(X_train)/6),\n          nb_epoch=10,\n          validation_data=(X_test, y_test))\n          \n#model_history = model.fit(X_train[0:steps_per_cycle], \n#                          y_train[0:steps_per_cycle], \n#                          batch_size=int(len(X_train)/6), \n#                          nb_epoch=10, \n#                          validation_split=0.05,\n#                          callbacks=[early_stopping])\n```\n\n    Train...\n\n\n\n    ---------------------------------------------------------------------------\n\n    ValueError                                Traceback (most recent call last)\n\n    <ipython-input-90-494e02375660> in <module>()\n          8           batch_size=int(len(X_train)/6),\n          9           nb_epoch=10,\n    ---> 10           validation_data=(X_test, y_test))\n         11 \n         12 #model_history = model.fit(X_train[0:steps_per_cycle],\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/models.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\n        670                               class_weight=class_weight,\n        671                               sample_weight=sample_weight,\n    --> 672                               initial_epoch=initial_epoch)\n        673 \n        674     def evaluate(self, x, y, batch_size=32, verbose=1,\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\n       1114             class_weight=class_weight,\n       1115             check_batch_axis=False,\n    -> 1116             batch_size=batch_size)\n       1117         # prepare validation data\n       1118         if validation_data:\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\n       1027                                    self.internal_input_shapes,\n       1028                                    check_batch_axis=False,\n    -> 1029                                    exception_prefix='model input')\n       1030         y = standardize_input_data(y, self.output_names,\n       1031                                    output_shapes,\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/keras/engine/training.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n        110                                  ' to have ' + str(len(shapes[i])) +\n        111                                  ' dimensions, but got array with shape ' +\n    --> 112                                  str(array.shape))\n        113             for j, (dim, ref_dim) in enumerate(zip(array.shape, shapes[i])):\n        114                 if not j and not check_batch_axis:\n\n\n    ValueError: Error when checking model input: expected embedding_input_5 to have 2 dimensions, but got array with shape (100, 150, 1)\n\n###__\u5165\u529b\u30c7\u30fc\u30bf\u306e\u6b21\u5143(shape\uff09\u9593\u9055\u3044__\n\n###__\u5165\u529b\u30c7\u30fc\u30bf\u306e\u6b21\u5143(shape\uff09 \u3092 \u78ba\u8a8d__\n\n\n```{python:Python3 Jupyter notebook}\n# 1.5\u5468\u671f\u5206(=150\u8981\u7d20\uff09\u306e\u30c7\u30fc\u30bf \u304c\u3001\u30b5\u30f3\u30d7\u30eb\u6570 100 (=1\u5468\u671f\u5206 \u203b\u76ee\u7684\u5909\u6570\u3092\uff11\u5468\u671f\u5206\u3001\u78ba\u4fdd\u3059\u308b\u305f\u3081\uff09 \u751f\u6210\u3057\u3066\u3044\u308b\u3002\nX_train[0:steps_per_cycle].shape\n```\n\n\n\n\n    (100, 150, 1)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nX_train[0:steps_per_cycle].reshape(100, 150)\n```\n\n\n\n\n    array([[ 0.03443538,  0.08851131,  0.11744915, ...,  0.15404703,\n             0.0837174 ,  0.09574251],\n           [ 0.08851131,  0.11744915,  0.16364786, ...,  0.0837174 ,\n             0.09574251, -0.0172632 ],\n           [ 0.11744915,  0.16364786,  0.24978178, ...,  0.09574251,\n            -0.0172632 , -0.10926813],\n           ..., \n           [-0.22822898, -0.11413612, -0.06414337, ...,  0.35406377,\n             0.32352641,  0.21250816],\n           [-0.11413612, -0.06414337,  0.01301437, ...,  0.32352641,\n             0.21250816,  0.20910005],\n           [-0.06414337,  0.01301437,  0.09718597, ...,  0.21250816,\n             0.20910005,  0.17293252]])\n\n\n\n\n```{python:Python3 Jupyter notebook}\nX_train[0:steps_per_cycle].reshape(100, 150).shape\n```\n\n\n\n\n    (100, 150)\n\n\n\n\n```{python:Python3 Jupyter notebook}\ny_train[0:steps_per_cycle].shape\n```\n\n\n\n\n    (100, 1)\n\n\n\n\n```{python:Python3 Jupyter notebook}\nprint('Train...')\n\nX_train = X_train[0:steps_per_cycle].reshape(100, 150)\ny_train = y_train[0:steps_per_cycle]\n\nX_test = X_test[0:steps_per_cycle].reshape(100, 150)\ny_test = y_test[0:steps_per_cycle]\n\nmodel_history = model.fit(X_train,\n                          y_train,\n                          batch_size=int(len(X_train)/6),\n                          nb_epoch=20,\n                          validation_data=(X_test, y_test)\n                          )\n```\n\n    Train...\n    Train on 100 samples, validate on 100 samples\n    Epoch 1/20\n    100/100 [==============================] - 5s - loss: 0.5111 - val_loss: 0.4999\n    Epoch 2/20\n    100/100 [==============================] - 3s - loss: 0.5059 - val_loss: 0.5001\n    Epoch 3/20\n    100/100 [==============================] - 3s - loss: 0.5044 - val_loss: 0.5022\n    Epoch 4/20\n    100/100 [==============================] - 3s - loss: 0.5040 - val_loss: 0.4999\n    Epoch 5/20\n    100/100 [==============================] - 4s - loss: 0.5051 - val_loss: 0.5001\n    Epoch 6/20\n    100/100 [==============================] - 3s - loss: 0.5057 - val_loss: 0.4999\n    Epoch 7/20\n    100/100 [==============================] - 3s - loss: 0.5043 - val_loss: 0.5001\n    Epoch 8/20\n    100/100 [==============================] - 3s - loss: 0.5054 - val_loss: 0.4999\n    Epoch 9/20\n    100/100 [==============================] - 3s - loss: 0.5025 - val_loss: 0.5006\n    Epoch 10/20\n    100/100 [==============================] - 6s - loss: 0.5054 - val_loss: 0.5014\n    Epoch 11/20\n    100/100 [==============================] - 5s - loss: 0.5016 - val_loss: 0.4999\n    Epoch 12/20\n    100/100 [==============================] - 3s - loss: 0.5060 - val_loss: 0.5001\n    Epoch 13/20\n    100/100 [==============================] - 5s - loss: 0.5070 - val_loss: 0.5052\n    Epoch 14/20\n    100/100 [==============================] - 6s - loss: 0.5058 - val_loss: 0.5004\n    Epoch 15/20\n    100/100 [==============================] - 6s - loss: 0.5031 - val_loss: 0.5008\n    Epoch 16/20\n    100/100 [==============================] - 4s - loss: 0.5047 - val_loss: 0.5008\n    Epoch 17/20\n    100/100 [==============================] - 4s - loss: 0.5039 - val_loss: 0.5000\n    Epoch 18/20\n    100/100 [==============================] - 5s - loss: 0.5031 - val_loss: 0.5011\n    Epoch 19/20\n    100/100 [==============================] - 3s - loss: 0.5041 - val_loss: 0.5004\n    Epoch 20/20\n    100/100 [==============================] - 6s - loss: 0.5053 - val_loss: 0.5000\n\n\n    /Users/hirofumiyashima/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n      \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n\n\n\n```{python:Python3 Jupyter notebook}\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nnb_epoch = len(loss)\n\nplt.plot(range(nb_epoch), loss, marker='.', label='loss')\nplt.plot(range(nb_epoch), val_loss, marker='.', label='val_loss')\n\nplt.legend(loc='best', fontsize=10)\nplt.grid()\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()\n```\n\n\n![output_25_0.png](https://qiita-image-store.s3.amazonaws.com/0/43487/974153fb-e014-7144-2f32-500c4430bd0d.png)\n\n\n\n####__loss\u304c\u4e0a\u4e0b\u3092\u7e70\u308a\u8fd4\u3057\u3066\u3044\u308b\u3002__\n####__\u6b63\u3057\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002__\n\n\n```{python:Python3 Jupyter notebook}\nmodel.summary()\n```\n\n* embedding\u5c64\u3067\u3001150\u6b21\u5143\uff08sin\u66f2\u7dda 1.5\u5468\u671f\uff1d150\u6642\u70b9\u30c7\u30fc\u30bf\uff09 \u3092 128\u6b21\u5143 \u306b \u5727\u7e2e\u3057\u3066\u3044\u308b\n* QRNN\u5c64\u3067\u3001128\u6b21\u5143 \u3092 64\u6b21\u5143 \u306b \u5727\u7e2e\u3057\u3066\u3044\u308b\n\n```{text:}\n____________________________________________________________________________________________________\n    Layer (type)                     Output Shape          Param #     Connected to                     \n    ====================================================================================================\n    embedding_5 (Embedding)          (None, 150, 128)      2560000     embedding_input_5[0][0]          \n    ____________________________________________________________________________________________________\n    qrnn_9 (QRNN)                    (None, 64)            73920       embedding_5[0][0]                \n    ____________________________________________________________________________________________________\n    dense_6 (Dense)                  (None, 1)             65          qrnn_9[0][0]                     \n    ____________________________________________________________________________________________________\n    activation_5 (Activation)        (None, 1)             0           dense_6[0][0]                    \n    ====================================================================================================\n    Total params: 2,633,985\n    Trainable params: 2,633,985\n    Non-trainable params: 0\n    ____________________________________________________________________________________________________\n```\n\n\n```{python:Python3 Jupyter notebook}\n# \u691c\u8a3c\u7528\u30c7\u30fc\u30bf \u306e \u8aac\u660e\u5909\u6570\u30c7\u30fc\u30bf\uff08\u76f4\u8fd11.5\u5468\u671f\u5206\u306e\u30c7\u30fc\u30bf\uff09\n# \u3092 \u5165\u529b\u3057\u3066\u3001\u51fa\u529b\u7d50\u679c\uff0810\u6642\u70b9\u5148\u306esin\u5024\u4e88\u6e2c\u5024\uff09\u3092\u51fa\u529b\u3059\u308b \n\n# X_test = X_test[0:steps_per_cycle].reshape(100, 150) \u3092 \u5b9f\u65bd\u6e08\u307f\u3002\n\npredicted_10_ahead = model.predict(X_test) \nlen(predicted_10_ahead)\n```\n\n\n\n\n    100\n\n\n##__\u4e88\u6e2c\u5024\u304c\u5168\u3066\u540c\u3058\u5024\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u305f\u3002__\n\n```{python:Python3 Jupyter notebook}\nfrom pprint import pprint\npprint(predicted_10_ahead[0:10])\n```\n\n    array([[ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085],\n           [ 0.01007085]], dtype=float32)\n\n\n\n```{python:Python3 Jupyter notebook}\ndataf =  pd.DataFrame(predicted_10_ahead)\ndataf.columns = [\"predict\"]\ndataf[\"true_value(observed_value)\"] = y_test\ndataf.plot()\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x12ff09eb8>\n\n\n\n\n![output_30_1.png](https://qiita-image-store.s3.amazonaws.com/0/43487/288aeeba-6eab-6a3f-f59c-49a4a56b6946.png)\n", "tags": ["Keras", "QRNN", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "\u6642\u7cfb\u5217\u89e3\u6790"]}