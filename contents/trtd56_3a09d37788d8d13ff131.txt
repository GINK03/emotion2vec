{"context": "\n\n\u306f\u3058\u3081\u306b\n\u6df1\u5c64\u5b66\u7fd2\u3068\u5f37\u5316\u5b66\u7fd2\u3092\u7d44\u307f\u5408\u308f\u305b\u305f Deep Q Network\u3001\u901a\u79f0DQN\u3067OpenAI Gym\u306eClassic control\u3092\u89e3\u304f\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306f\u305d\u306e\u5b9f\u88c5\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\nDQN\u81ea\u4f53\u306b\u3064\u3044\u3066\u306f\n\nDQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f\n\u30bc\u30ed\u304b\u3089Deep\u307e\u3067\u5b66\u3076\u5f37\u5316\u5b66\u7fd2\n\n\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u308f\u304b\u308a\u3084\u3059\u304f\u3001\u79c1\u3082\u3053\u3061\u3089\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587\u3084GitHub\u306e\u30b3\u30fc\u30c9\u3092\u53c2\u8003\u306b\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\u5f37\u5316\u5b66\u7fd2\u3084DQN\u306e\u7406\u8ad6\u3092\u77e5\u308a\u305f\u3044\u65b9\u306f\u3053\u3061\u3089\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002\n\nDQN\"\u3082\u3069\u304d\"\uff1f\nDeep Q Network\u3068\u3044\u3046\u540d\u524d\u304b\u3089\u3082\u5206\u304b\u308b\u901a\u308a\u3001DQN\u306f\u5f37\u5316\u5b66\u7fd2\u306e1\u3064\u3067\u3042\u308bQ\u5b66\u7fd2\u3092\u591a\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u95a2\u6570\u8fd1\u4f3c\u3057\u307e\u3059\u3002\n\u305d\u308c\u306b\u52a0\u3048\u3001\u4e0b\u8a18\u306e3\u3064\u306e\u624b\u6cd5\u3092\u53d6\u308a\u5165\u308c\u3066\u521d\u3081\u3066DQN\u3068\u547c\u3079\u308b\u307f\u305f\u3044\u3067\u3059\u3002\n\nExperience Replay\nFixed Target Q-Network\n\u5831\u916c\u306eClipping\n\n\u4eca\u56de\u79c1\u304c\u5b9f\u88c5\u3057\u305f\u624b\u6cd5\u306f1,2\u306e\u307f\u3067\u30013\u306e\u5831\u916c\u306eClipping\u306f\u884c\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3064\u307e\u308a\u6b63\u78ba\u306b\u8a00\u3046\u3068DQN\u3067\u306f\u306a\u3044\u306e\u3067\u3059\u3002\u306a\u306e\u3067DQN\"\u3082\u3069\u304d\"\u3068\u984c\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\nOpenAI Gym\u306b\u3064\u3044\u3066\n\u5f37\u5316\u5b66\u7fd2\u306e\u305f\u3081\u306e\u74b0\u5883\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u308b\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3067\u3059\u3002\npython\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u306a\u3063\u3066\u304a\u308a\u3001\n$ pip install gym\n\n\u3067\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n\u8a73\u3057\u304f\u306f\u516c\u5f0f\u30b5\u30a4\u30c8\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\n\n\u5b9f\u88c5\n\u3067\u306f\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u306e\u7d39\u4ecb\u3092\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3053\u3053\u3067\u304a\u898b\u305b\u3059\u308b\u30b3\u30fc\u30c9\u306f\u7701\u7565\u3057\u3066\u3044\u308b\u90e8\u5206\u3082\u3042\u308b\u306e\u3067\u3001\u5168\u4f53\u306f\u3053\u3061\u3089\u304b\u3089\u3054\u78ba\u8a8d\u4e0b\u3055\u3044\u3002\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\nChainer\u3092\u4f7f\u3063\u3066\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n100\u30e6\u30cb\u30c3\u30c8\u304c3\u5c64\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306fLeaky ReLU\u306b\u3057\u307e\u3057\u305f\u3002\nclass Neuralnet(Chain):\n\n    def __init__(self, n_in, n_out):\n        super(Neuralnet, self).__init__(\n            L1 = L.Linear(n_in, 100),\n            L2 = L.Linear(100, 100),\n            L3 = L.Linear(100, 100),\n            Q_value = L.Linear(100, n_out, initialW=np.zeros((n_out, 100), dtype=np.float32))\n        )\n\n    def Q_func(self, x):\n        h = F.leaky_relu(self.L1(x))\n        h = F.leaky_relu(self.L2(h))\n        h = F.leaky_relu(self.L3(h))\n        h = self.Q_value(h)\n        return h\n\n\nAgent\n\u5f37\u5316\u5b66\u7fd2\u306e\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u90e8\u5206\u306e\u5b9f\u88c5\u3067\u3059\u3002\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\n\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u4e0a\u3067\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\n\u5148\u307b\u3069\u7d39\u4ecb\u3057\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3082\u8aad\u307f\u8fbc\u3093\u3060\u72b6\u614b\u6570\u3068\u884c\u52d5\u6570\u306b\u5408\u308f\u305b\u3066\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u3057\u3066Fixed Target Q-Network\u306e\u305f\u3081\u3001\u4f5c\u6210\u3057\u305fQ\u95a2\u6570\u3092deepcopy\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u3064\u307e\u308aQ\u95a2\u6570\u304c2\u3064\u306b\u306a\u308b\u308f\u3051\u3067\u3059\u306d\u3002\u521d\u3081\u306f\u3053\u306e\u90e8\u5206\u304c\u7406\u89e3\u3067\u304d\u305a\u82e6\u52b4\u3057\u307e\u3057\u305f\u30fb\u30fb\u30fb\nclass Agent():\n\n    def __init__(self, n_st, n_act, seed):\n        self.n_act = n_act\n        self.model = Neuralnet(n_st, n_act)\n        self.target_model = copy.deepcopy(self.model)\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.memory = deque()\n        self.loss = 0\n        self.step = 0\n        self.gamma = 0.99 # \u5272\u5f15\u7387\n        self.mem_size = 1000 # Experience Replay\u306e\u305f\u3081\u306b\u899a\u3048\u3066\u304a\u304f\u7d4c\u9a13\u306e\u6570\n        self.batch_size = 100 # Experience Replay\u306e\u969b\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u306e\u5927\u304d\u3055\n        self.train_freq = 10 # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u9593\u9694\n        self.target_update_freq = 20 # \u30bf\u30fc\u30b2\u30c3\u30c8\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u540c\u671f\u9593\u9694\n        # \u03b5-greedy\n        self.epsilon = 1 # \u03b5\u306e\u521d\u671f\u5024\n        self.epsilon_decay = 0.005 # \u03b5\u306e\u6e1b\u8870\u5024\n        self.epsilon_min = 0 # \u03b5\u306e\u6700\u5c0f\u5024\n        self.exploration = 1000 # \u03b5\u3092\u6e1b\u8870\u3057\u59cb\u3081\u308b\u307e\u3067\u306e\u30b9\u30c6\u30c3\u30d7\u6570(\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u30fc\u304c\u8caf\u307e\u308b\u307e\u3067)\n\n\n\u7d4c\u9a13\u306e\u84c4\u7a4d\nExperience Replay\u306e\u70ba\n\n\u72b6\u614b: st\n\u884c\u52d5: act\n\u5831\u916c: r\n\u6b21\u306e\u72b6\u614b: st_dash\n\u30a8\u30d4\u30bd\u30fc\u30c9\u7d42\u4e86\u306e\u6709\u7121: ep_end\n\n\u306e5\u3064\u306e\u8981\u7d20\u3092\u7d4c\u9a13\u3068\u3057\u3066\u30bf\u30d7\u30eb\u306b\u3057\u3066memory\u306b\u4fdd\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u521d\u306b\u5b9a\u7fa9\u3057\u305fmemory\u30b5\u30a4\u30ba\u3092\u8d85\u3048\u308b\u3068\u5148\u306b\u5165\u308c\u305f\u3082\u306e\u304b\u3089\u30c8\u30b3\u30ed\u30c6\u30f3\u5f0f\u306b\u6368\u3066\u3089\u308c\u308b\u5f62\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u6700\u521dmemory\u306f\u305f\u3060\u306e\u30ea\u30b9\u30c8\u306b\u3057\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u4e21\u7aef\u306eappend\u3084pop\u3092\u62d8\u675f\u3067\u884c\u3048\u308bdeque\u306a\u308b\u3082\u306e\u304c\u5b58\u5728\u3059\u308b\u3068\u805e\u3044\u305f\u306e\u3067\u3001\u305d\u3061\u3089\u3092\u5229\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\ndef stock_experience(self, st, act, r, st_dash, ep_end):\n    self.memory.append((st, act, r, st_dash, ep_end))\n    if len(self.memory) > self.mem_size:\n        self.memory.popleft()\n\n\nExperience Replay\nDQN\u3067\u91cd\u8981\u306a\u624b\u6cd5\u306e1\u3064\u3067\u3042\u308bExperience Replay\u306e\u5b9f\u88c5\u90e8\u5206\u3067\u3059\u3002\n\u8caf\u3081\u3066\u304d\u305fmemory\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3001\u5b9a\u7fa9\u3057\u305f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u5207\u308a\u51fa\u3057\u3066\u5b66\u7fd2\u3057\u3066\u3044\u304d\u307e\u3059\u3002\ndef suffle_memory(self):\n    mem = np.array(self.memory)\n    return np.random.permutation(mem)\n\ndef parse_batch(self, batch):\n    st, act, r, st_dash, ep_end = [], [], [], [], []\n    for i in xrange(self.batch_size):\n        st.append(batch[i][0])\n        act.append(batch[i][1])\n        r.append(batch[i][2])\n        st_dash.append(batch[i][3])\n        ep_end.append(batch[i][4])\n    st = np.array(st, dtype=np.float32)\n    act = np.array(act, dtype=np.int8)\n    r = np.array(r, dtype=np.float32)\n    st_dash = np.array(st_dash, dtype=np.float32)\n    ep_end = np.array(ep_end, dtype=np.bool)\n    return st, act, r, st_dash, ep_end\n\ndef experience_replay(self):\n    mem = self.suffle_memory()\n    perm = np.array(xrange(len(mem)))\n    for start in perm[::self.batch_size]:\n        index = perm[start:start+self.batch_size]\n        batch = mem[index]\n        st, act, r, st_d, ep_end = self.parse_batch(batch)\n        self.model.zerograds()\n        loss = self.forward(st, act, r, st_d, ep_end)\n        loss.backward()\n        self.optimizer.update()\n\n\nQ\u95a2\u6570\u66f4\u65b0\u90e8\u5206\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u305fQ\u95a2\u6570\u306e\u66f4\u65b0\u90e8\u5206\u3067\u3059\u3002\n\u6b21\u306e\u72b6\u614b(st_dash)\u306eQ\u5024\u306e\u6700\u5927\u5024\u3092\u8a08\u7b97\u3059\u308b\u90e8\u5206\u3067\u306f\u30b3\u30d4\u30fc\u3057\u305fQ\u95a2\u6570(self.target_model.Q_func)\u3092\u4f7f\u3046\u3068\u3053\u308d\u304c\u91cd\u8981\u3067\u3059\u3002\ndef forward(self, st, act, r, st_dash, ep_end):\n    s = Variable(st)\n    s_dash = Variable(st_dash)\n    Q = self.model.Q_func(s)\n    tmp = self.target_model.Q_func(s_dash)\n    tmp = list(map(np.max, tmp.data))\n    max_Q_dash = np.asanyarray(tmp, dtype=np.float32)\n    target = np.asanyarray(copy.deepcopy(Q.data), dtype=np.float32)\n    for i in xrange(self.batch_size):\n        target[i, act[i]] = r[i] + (self.gamma * max_Q_dash[i]) * (not ep_end[i])\n    loss = F.mean_squared_error(Q, Variable(target))\n    return loss\n\n\u3053\u3053\u3067loss\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u3001Q\u5024\u3068target\u306e\u5dee\u3092-1\uff5e1\u306b\u30af\u30ea\u30c3\u30d7\u3059\u308b\u3068\u5b66\u7fd2\u304c\u65e9\u304f\u306a\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u52c9\u5f37\u4e0d\u8db3\u3067\u7406\u8ad6\u304c\u7406\u89e3\u3067\u304d\u306a\u304b\u3063\u305f\u70ba\u5b9f\u88c5\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f(\u304f\u3084\u3057\u3044...\n\n\u884c\u52d5\u3092\u8fd4\u3059\n\u5b66\u7fd2\u3057\u305fQ\u95a2\u6570\u306b\u5f93\u3063\u3066\u5165\u529b\u3055\u308c\u305f\u72b6\u614b\u306e\u6642\u306b\u53d6\u308b\u3079\u304d\u884c\u52d5\u3092\u8fd4\u3059\u90e8\u5206\u3067\u3059\u3002\u884c\u52d5\u9078\u629e\u306e\u624b\u6cd5\u306f\u03b5-greedy\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\ndef get_action(self, st):\n    if np.random.rand() < self.epsilon:\n        return np.random.randint(0, self.n_act)\n    else:\n        s = Variable(st)\n        Q = self.model.Q_func(s)\n        Q = Q.data[0]\n        a = np.argmax(Q)\n        return np.asarray(a, dtype=np.int8)\n\n\n\u5b66\u7fd2\u3092\u9032\u3081\u308b\nmemory\u304c\u5341\u5206\u306b\u6e9c\u307e\u3063\u305f\u3089\u5b66\u7fd2\u3092\u9032\u3081\u308b\u90e8\u5206\u3067\u3059\u3002\n\u6bce\u56destep\u3092\u523b\u3093\u3067\u304a\u308a\u3001\u4e00\u5b9a\u5468\u671f\u3067target\u7528\u306eQ\u95a2\u6570\u3092\u540c\u671f\u3057\u3066\u3044\u307e\u3059\u3002\n\u307e\u305f\u3001\u3042\u308b\u7a0b\u5ea6\u63a2\u7d22\u3092\u7d42\u3048\u308b\u3068\u6bcestep\u6bce\u306b\u03b5\u304c\u6e1b\u8870\u3057\u3066\u3044\u304d\u307e\u3059\u3002\ndef reduce_epsilon(self):\n    if self.epsilon > self.epsilon_min and self.exploration < self.step:\n        self.epsilon -= self.epsilon_decay\n\ndef train(self):\n    if len(self.memory) >= self.mem_size:\n        if self.step % self.train_freq == 0:\n            self.experience_replay()\n            self.reduce_epsilon()\n        if self.step % self.target_update_freq == 0:\n            self.target_model = copy.deepcopy(self.model)\n    self.step += 1\n\n\n\u5b9f\u884c\u90e8\u5206\nClassic control\u306e\u74b0\u5883\u540d\u3092\u5165\u308c\u308b\u3068\u72b6\u614b\u6570\u3084\u884c\u52d5\u6570\u3092\u52dd\u624b\u306b\u5224\u65ad\u3057\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\n\u3061\u3087\u3063\u3068\u3054\u3061\u3083\u3054\u3061\u3083\u3057\u3066\u9006\u306b\u308f\u304b\u308a\u3065\u3089\u304f\u306a\u3063\u3061\u3083\u3063\u305f\u304b\u3082\u3057\u308c\u307e\u305b\u3093^^;\ndef main(env_name):\n    env = gym.make(env_name)\n    view_path = \"./video/\" + env_name\n\n    n_st = env.observation_space.shape[0]\n    if type(env.action_space) == gym.spaces.discrete.Discrete:\n        # CartPole-v0, Acrobot-v0, MountainCar-v0\n        n_act = env.action_space.n\n        action_list = range(0, n_act)\n    elif type(env.action_space) == gym.spaces.box.Box:\n        # Pendulum-v0\n        action_list = [np.array([a]) for a in [-2.0, 2.0]]\n        n_act = len(action_list)\n\n    agent = Agent(n_st, n_act, seed)\n\n    env.monitor.start(view_path, video_callable=None, force=True, seed=seed)\n    for i_episode in xrange(1000):\n        observation = env.reset()\n        for t in xrange(200):\n            env.render()\n            state = observation.astype(np.float32).reshape((1,n_st))\n            act_i = agent.get_action(state)\n            action = action_list[act_i]\n            observation, reward, ep_end, _ = env.step(action)\n            state_dash = observation.astype(np.float32).reshape((1,n_st))\n            agent.stock_experience(state, act_i, reward, state_dash, ep_end)\n            agent.train()\n            if ep_end:\n                break\n    env.monitor.close()\n\n\n\n\u5b9f\u884c\u7d50\u679c\nOpenAI Gym\u306b\u7d50\u679c\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\n\nCartPole-v0\nAcrobot-v0\nMountainCar-v0\nPendulum-v0\n\nAcrobot\u3084Pendulum\u306f\u306a\u304b\u306a\u304b\u826f\u3044\u7d50\u679c\u3060\u3068\u601d\u3044\u307e\u3059\u304c\u3001CartPole\u304c\u5fae\u5999\u3067\u3059\u306d\u3002\ntarget\u7528Q\u95a2\u6570\u306e\u66f4\u65b0\u983b\u5ea6\u3084\u03b5\u306e\u6e1b\u8870\u306e\u5927\u304d\u3055\u3001\u6700\u9069\u5316\u624b\u6cd5\u306a\u3069\u306b\u3088\u3063\u3066\u7d50\u679c\u306f\u8272\u3005\u3068\u5909\u308f\u3063\u3066\u304f\u308b\u307f\u305f\u3044\u3067\u3059\u3002\u9762\u767d\u3044\uff01\n\n\u304a\u308f\u308a\u306b\n\u4eca\u5f8c\u306fAtari\u306e\u30b2\u30fc\u30e0\u3067\u8a66\u3057\u305f\u3044\u3067\u3059\u3002\u305d\u306e\u6642\u306f\u5831\u916c\u306eClipping\u3082\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u305d\u3046\u3067\u3059\u306d\u3002\u6b63\u898f\u5316\u3084DropOut\u3068\u304b\u3082\u8003\u616e\u3057\u305f\u307b\u3046\u304c\u3044\u3044\u306e\u304b\u306a\uff1f\n# \u306f\u3058\u3081\u306b\n\n\u6df1\u5c64\u5b66\u7fd2\u3068\u5f37\u5316\u5b66\u7fd2\u3092\u7d44\u307f\u5408\u308f\u305b\u305f Deep Q Network\u3001\u901a\u79f0DQN\u3067OpenAI Gym\u306eClassic control\u3092\u89e3\u304f\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306f\u305d\u306e\u5b9f\u88c5\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nDQN\u81ea\u4f53\u306b\u3064\u3044\u3066\u306f\n\n- [DQN\u306e\u751f\u3044\u7acb\u3061\u3000\uff0b\u3000Deep Q-Network\u3092Chainer\u3067\u66f8\u3044\u305f](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)\n- [\u30bc\u30ed\u304b\u3089Deep\u307e\u3067\u5b66\u3076\u5f37\u5316\u5b66\u7fd2](http://qiita.com/icoxfog417/items/242439ecd1a477ece312)\n\n\u306e\u8a18\u4e8b\u304c\u3068\u3066\u3082\u308f\u304b\u308a\u3084\u3059\u304f\u3001\u79c1\u3082\u3053\u3061\u3089\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u308b\u8ad6\u6587\u3084GitHub\u306e\u30b3\u30fc\u30c9\u3092\u53c2\u8003\u306b\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n\u5f37\u5316\u5b66\u7fd2\u3084DQN\u306e\u7406\u8ad6\u3092\u77e5\u308a\u305f\u3044\u65b9\u306f\u3053\u3061\u3089\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002\n\n## DQN\"\u3082\u3069\u304d\"\uff1f\n\nDeep Q Network\u3068\u3044\u3046\u540d\u524d\u304b\u3089\u3082\u5206\u304b\u308b\u901a\u308a\u3001DQN\u306f\u5f37\u5316\u5b66\u7fd2\u306e1\u3064\u3067\u3042\u308bQ\u5b66\u7fd2\u3092\u591a\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u95a2\u6570\u8fd1\u4f3c\u3057\u307e\u3059\u3002\n\u305d\u308c\u306b\u52a0\u3048\u3001\u4e0b\u8a18\u306e3\u3064\u306e\u624b\u6cd5\u3092\u53d6\u308a\u5165\u308c\u3066\u521d\u3081\u3066DQN\u3068\u547c\u3079\u308b\u307f\u305f\u3044\u3067\u3059\u3002\n\n0. Experience Replay\n0. Fixed Target Q-Network\n0. \u5831\u916c\u306eClipping\n\n\u4eca\u56de\u79c1\u304c\u5b9f\u88c5\u3057\u305f\u624b\u6cd5\u306f1,2\u306e\u307f\u3067\u30013\u306e\u5831\u916c\u306eClipping\u306f\u884c\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3064\u307e\u308a\u6b63\u78ba\u306b\u8a00\u3046\u3068DQN\u3067\u306f\u306a\u3044\u306e\u3067\u3059\u3002\u306a\u306e\u3067DQN\"\u3082\u3069\u304d\"\u3068\u984c\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n## OpenAI Gym\u306b\u3064\u3044\u3066\n\n\u5f37\u5316\u5b66\u7fd2\u306e\u305f\u3081\u306e\u74b0\u5883\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u308b\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3067\u3059\u3002\npython\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u306a\u3063\u3066\u304a\u308a\u3001\n\n~~~\n$ pip install gym\n~~~\n\n\u3067\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002\n\u8a73\u3057\u304f\u306f[\u516c\u5f0f\u30b5\u30a4\u30c8](https://gym.openai.com/)\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\n\n# \u5b9f\u88c5\n\n\u3067\u306f\u5b9f\u88c5\u3057\u305f\u30b3\u30fc\u30c9\u306e\u7d39\u4ecb\u3092\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u3053\u3053\u3067\u304a\u898b\u305b\u3059\u308b\u30b3\u30fc\u30c9\u306f\u7701\u7565\u3057\u3066\u3044\u308b\u90e8\u5206\u3082\u3042\u308b\u306e\u3067\u3001\u5168\u4f53\u306f[\u3053\u3061\u3089](https://github.com/trtd56/ClassicControl)\u304b\u3089\u3054\u78ba\u8a8d\u4e0b\u3055\u3044\u3002\n\n## \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\nChainer\u3092\u4f7f\u3063\u3066\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\n100\u30e6\u30cb\u30c3\u30c8\u304c3\u5c64\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306fLeaky ReLU\u306b\u3057\u307e\u3057\u305f\u3002\n\n~~~python\nclass Neuralnet(Chain):\n\n    def __init__(self, n_in, n_out):\n        super(Neuralnet, self).__init__(\n            L1 = L.Linear(n_in, 100),\n            L2 = L.Linear(100, 100),\n            L3 = L.Linear(100, 100),\n            Q_value = L.Linear(100, n_out, initialW=np.zeros((n_out, 100), dtype=np.float32))\n        )\n\n    def Q_func(self, x):\n        h = F.leaky_relu(self.L1(x))\n        h = F.leaky_relu(self.L2(h))\n        h = F.leaky_relu(self.L3(h))\n        h = self.Q_value(h)\n        return h\n~~~\n\n## Agent\n\n\u5f37\u5316\u5b66\u7fd2\u306e\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u90e8\u5206\u306e\u5b9f\u88c5\u3067\u3059\u3002\n\n### \u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\n\n\u5f37\u5316\u5b66\u7fd2\u3092\u884c\u3046\u4e0a\u3067\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\n\u5148\u307b\u3069\u7d39\u4ecb\u3057\u305f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3082\u8aad\u307f\u8fbc\u3093\u3060\u72b6\u614b\u6570\u3068\u884c\u52d5\u6570\u306b\u5408\u308f\u305b\u3066\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u3057\u3066Fixed Target Q-Network\u306e\u305f\u3081\u3001\u4f5c\u6210\u3057\u305fQ\u95a2\u6570\u3092deepcopy\u3057\u3066\u304a\u304d\u307e\u3059\u3002\u3064\u307e\u308aQ\u95a2\u6570\u304c2\u3064\u306b\u306a\u308b\u308f\u3051\u3067\u3059\u306d\u3002\u521d\u3081\u306f\u3053\u306e\u90e8\u5206\u304c\u7406\u89e3\u3067\u304d\u305a\u82e6\u52b4\u3057\u307e\u3057\u305f\u30fb\u30fb\u30fb\n\n~~~python\nclass Agent():\n\n    def __init__(self, n_st, n_act, seed):\n        self.n_act = n_act\n        self.model = Neuralnet(n_st, n_act)\n        self.target_model = copy.deepcopy(self.model)\n        self.optimizer = optimizers.Adam()\n        self.optimizer.setup(self.model)\n        self.memory = deque()\n        self.loss = 0\n        self.step = 0\n        self.gamma = 0.99 # \u5272\u5f15\u7387\n        self.mem_size = 1000 # Experience Replay\u306e\u305f\u3081\u306b\u899a\u3048\u3066\u304a\u304f\u7d4c\u9a13\u306e\u6570\n        self.batch_size = 100 # Experience Replay\u306e\u969b\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u306e\u5927\u304d\u3055\n        self.train_freq = 10 # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u9593\u9694\n        self.target_update_freq = 20 # \u30bf\u30fc\u30b2\u30c3\u30c8\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u540c\u671f\u9593\u9694\n        # \u03b5-greedy\n        self.epsilon = 1 # \u03b5\u306e\u521d\u671f\u5024\n        self.epsilon_decay = 0.005 # \u03b5\u306e\u6e1b\u8870\u5024\n        self.epsilon_min = 0 # \u03b5\u306e\u6700\u5c0f\u5024\n        self.exploration = 1000 # \u03b5\u3092\u6e1b\u8870\u3057\u59cb\u3081\u308b\u307e\u3067\u306e\u30b9\u30c6\u30c3\u30d7\u6570(\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u30fc\u304c\u8caf\u307e\u308b\u307e\u3067)\n~~~\n\n### \u7d4c\u9a13\u306e\u84c4\u7a4d\n\nExperience Replay\u306e\u70ba\n\n0. \u72b6\u614b: st\n1. \u884c\u52d5: act\n2. \u5831\u916c: r\n3. \u6b21\u306e\u72b6\u614b: st_dash\n4. \u30a8\u30d4\u30bd\u30fc\u30c9\u7d42\u4e86\u306e\u6709\u7121: ep_end\n\n\u306e5\u3064\u306e\u8981\u7d20\u3092\u7d4c\u9a13\u3068\u3057\u3066\u30bf\u30d7\u30eb\u306b\u3057\u3066memory\u306b\u4fdd\u5b58\u3057\u3066\u3044\u307e\u3059\u3002\u6700\u521d\u306b\u5b9a\u7fa9\u3057\u305fmemory\u30b5\u30a4\u30ba\u3092\u8d85\u3048\u308b\u3068\u5148\u306b\u5165\u308c\u305f\u3082\u306e\u304b\u3089\u30c8\u30b3\u30ed\u30c6\u30f3\u5f0f\u306b\u6368\u3066\u3089\u308c\u308b\u5f62\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u6700\u521dmemory\u306f\u305f\u3060\u306e\u30ea\u30b9\u30c8\u306b\u3057\u3066\u3044\u305f\u306e\u3067\u3059\u304c\u3001\u4e21\u7aef\u306eappend\u3084pop\u3092\u62d8\u675f\u3067\u884c\u3048\u308bdeque\u306a\u308b\u3082\u306e\u304c\u5b58\u5728\u3059\u308b\u3068\u805e\u3044\u305f\u306e\u3067\u3001\u305d\u3061\u3089\u3092\u5229\u7528\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n~~~python\ndef stock_experience(self, st, act, r, st_dash, ep_end):\n    self.memory.append((st, act, r, st_dash, ep_end))\n    if len(self.memory) > self.mem_size:\n        self.memory.popleft()\n~~~\n\n### Experience Replay\n\nDQN\u3067\u91cd\u8981\u306a\u624b\u6cd5\u306e1\u3064\u3067\u3042\u308bExperience Replay\u306e\u5b9f\u88c5\u90e8\u5206\u3067\u3059\u3002\n\u8caf\u3081\u3066\u304d\u305fmemory\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u3001\u5b9a\u7fa9\u3057\u305f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u5207\u308a\u51fa\u3057\u3066\u5b66\u7fd2\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n~~~python\ndef suffle_memory(self):\n    mem = np.array(self.memory)\n    return np.random.permutation(mem)\n\ndef parse_batch(self, batch):\n    st, act, r, st_dash, ep_end = [], [], [], [], []\n    for i in xrange(self.batch_size):\n        st.append(batch[i][0])\n        act.append(batch[i][1])\n        r.append(batch[i][2])\n        st_dash.append(batch[i][3])\n        ep_end.append(batch[i][4])\n    st = np.array(st, dtype=np.float32)\n    act = np.array(act, dtype=np.int8)\n    r = np.array(r, dtype=np.float32)\n    st_dash = np.array(st_dash, dtype=np.float32)\n    ep_end = np.array(ep_end, dtype=np.bool)\n    return st, act, r, st_dash, ep_end\n\ndef experience_replay(self):\n    mem = self.suffle_memory()\n    perm = np.array(xrange(len(mem)))\n    for start in perm[::self.batch_size]:\n        index = perm[start:start+self.batch_size]\n        batch = mem[index]\n        st, act, r, st_d, ep_end = self.parse_batch(batch)\n        self.model.zerograds()\n        loss = self.forward(st, act, r, st_d, ep_end)\n        loss.backward()\n        self.optimizer.update()\n~~~\n\n### Q\u95a2\u6570\u66f4\u65b0\u90e8\u5206\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u305fQ\u95a2\u6570\u306e\u66f4\u65b0\u90e8\u5206\u3067\u3059\u3002\n\u6b21\u306e\u72b6\u614b(st_dash)\u306eQ\u5024\u306e\u6700\u5927\u5024\u3092\u8a08\u7b97\u3059\u308b\u90e8\u5206\u3067\u306f\u30b3\u30d4\u30fc\u3057\u305fQ\u95a2\u6570(self.target_model.Q_func)\u3092\u4f7f\u3046\u3068\u3053\u308d\u304c\u91cd\u8981\u3067\u3059\u3002\n\n~~~python\ndef forward(self, st, act, r, st_dash, ep_end):\n    s = Variable(st)\n    s_dash = Variable(st_dash)\n    Q = self.model.Q_func(s)\n    tmp = self.target_model.Q_func(s_dash)\n    tmp = list(map(np.max, tmp.data))\n    max_Q_dash = np.asanyarray(tmp, dtype=np.float32)\n    target = np.asanyarray(copy.deepcopy(Q.data), dtype=np.float32)\n    for i in xrange(self.batch_size):\n        target[i, act[i]] = r[i] + (self.gamma * max_Q_dash[i]) * (not ep_end[i])\n    loss = F.mean_squared_error(Q, Variable(target))\n    return loss\n~~~\n\n\u3053\u3053\u3067loss\u3092\u8a08\u7b97\u3059\u308b\u969b\u306b\u3001Q\u5024\u3068target\u306e\u5dee\u3092-1\uff5e1\u306b\u30af\u30ea\u30c3\u30d7\u3059\u308b\u3068\u5b66\u7fd2\u304c\u65e9\u304f\u306a\u308b\u3088\u3046\u3067\u3059\u304c\u3001\u52c9\u5f37\u4e0d\u8db3\u3067\u7406\u8ad6\u304c\u7406\u89e3\u3067\u304d\u306a\u304b\u3063\u305f\u70ba\u5b9f\u88c5\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f(\u304f\u3084\u3057\u3044...\n\n### \u884c\u52d5\u3092\u8fd4\u3059\n\n\u5b66\u7fd2\u3057\u305fQ\u95a2\u6570\u306b\u5f93\u3063\u3066\u5165\u529b\u3055\u308c\u305f\u72b6\u614b\u306e\u6642\u306b\u53d6\u308b\u3079\u304d\u884c\u52d5\u3092\u8fd4\u3059\u90e8\u5206\u3067\u3059\u3002\u884c\u52d5\u9078\u629e\u306e\u624b\u6cd5\u306f\u03b5-greedy\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\n\n~~~python\ndef get_action(self, st):\n    if np.random.rand() < self.epsilon:\n        return np.random.randint(0, self.n_act)\n    else:\n        s = Variable(st)\n        Q = self.model.Q_func(s)\n        Q = Q.data[0]\n        a = np.argmax(Q)\n        return np.asarray(a, dtype=np.int8)\n~~~\n\n\n### \u5b66\u7fd2\u3092\u9032\u3081\u308b\n\nmemory\u304c\u5341\u5206\u306b\u6e9c\u307e\u3063\u305f\u3089\u5b66\u7fd2\u3092\u9032\u3081\u308b\u90e8\u5206\u3067\u3059\u3002\n\u6bce\u56destep\u3092\u523b\u3093\u3067\u304a\u308a\u3001\u4e00\u5b9a\u5468\u671f\u3067target\u7528\u306eQ\u95a2\u6570\u3092\u540c\u671f\u3057\u3066\u3044\u307e\u3059\u3002\n\u307e\u305f\u3001\u3042\u308b\u7a0b\u5ea6\u63a2\u7d22\u3092\u7d42\u3048\u308b\u3068\u6bcestep\u6bce\u306b\u03b5\u304c\u6e1b\u8870\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n~~~python\ndef reduce_epsilon(self):\n    if self.epsilon > self.epsilon_min and self.exploration < self.step:\n        self.epsilon -= self.epsilon_decay\n\ndef train(self):\n    if len(self.memory) >= self.mem_size:\n        if self.step % self.train_freq == 0:\n            self.experience_replay()\n            self.reduce_epsilon()\n        if self.step % self.target_update_freq == 0:\n            self.target_model = copy.deepcopy(self.model)\n    self.step += 1\n~~~\n\n## \u5b9f\u884c\u90e8\u5206\n\nClassic control\u306e\u74b0\u5883\u540d\u3092\u5165\u308c\u308b\u3068\u72b6\u614b\u6570\u3084\u884c\u52d5\u6570\u3092\u52dd\u624b\u306b\u5224\u65ad\u3057\u3066\u304f\u308c\u308b\u3088\u3046\u306b\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\n\u3061\u3087\u3063\u3068\u3054\u3061\u3083\u3054\u3061\u3083\u3057\u3066\u9006\u306b\u308f\u304b\u308a\u3065\u3089\u304f\u306a\u3063\u3061\u3083\u3063\u305f\u304b\u3082\u3057\u308c\u307e\u305b\u3093^^;\n\n~~~python\ndef main(env_name):\n    env = gym.make(env_name)\n    view_path = \"./video/\" + env_name\n\n    n_st = env.observation_space.shape[0]\n    if type(env.action_space) == gym.spaces.discrete.Discrete:\n        # CartPole-v0, Acrobot-v0, MountainCar-v0\n        n_act = env.action_space.n\n        action_list = range(0, n_act)\n    elif type(env.action_space) == gym.spaces.box.Box:\n        # Pendulum-v0\n        action_list = [np.array([a]) for a in [-2.0, 2.0]]\n        n_act = len(action_list)\n\n    agent = Agent(n_st, n_act, seed)\n\n    env.monitor.start(view_path, video_callable=None, force=True, seed=seed)\n    for i_episode in xrange(1000):\n        observation = env.reset()\n        for t in xrange(200):\n            env.render()\n            state = observation.astype(np.float32).reshape((1,n_st))\n            act_i = agent.get_action(state)\n            action = action_list[act_i]\n            observation, reward, ep_end, _ = env.step(action)\n            state_dash = observation.astype(np.float32).reshape((1,n_st))\n            agent.stock_experience(state, act_i, reward, state_dash, ep_end)\n            agent.train()\n            if ep_end:\n                break\n    env.monitor.close()\n\n~~~\n\n## \u5b9f\u884c\u7d50\u679c\n\nOpenAI Gym\u306b\u7d50\u679c\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\n\n- [CartPole-v0](https://gym.openai.com/evaluations/eval_ODj0t6gLRli1ig9GU3uVXQ)\n- [Acrobot-v0](https://gym.openai.com/evaluations/eval_pw1zeARxWwgmBxWV3Q)\n- [MountainCar-v0](https://gym.openai.com/evaluations/eval_V6j3Dq6R1Wr4pBSuS39g)\n- [Pendulum-v0](https://gym.openai.com/evaluations/eval_2Ij5nswES8OdSTI5vCskvw)\n\nAcrobot\u3084Pendulum\u306f\u306a\u304b\u306a\u304b\u826f\u3044\u7d50\u679c\u3060\u3068\u601d\u3044\u307e\u3059\u304c\u3001CartPole\u304c\u5fae\u5999\u3067\u3059\u306d\u3002\ntarget\u7528Q\u95a2\u6570\u306e\u66f4\u65b0\u983b\u5ea6\u3084\u03b5\u306e\u6e1b\u8870\u306e\u5927\u304d\u3055\u3001\u6700\u9069\u5316\u624b\u6cd5\u306a\u3069\u306b\u3088\u3063\u3066\u7d50\u679c\u306f\u8272\u3005\u3068\u5909\u308f\u3063\u3066\u304f\u308b\u307f\u305f\u3044\u3067\u3059\u3002\u9762\u767d\u3044\uff01\n\n# \u304a\u308f\u308a\u306b\n\u4eca\u5f8c\u306fAtari\u306e\u30b2\u30fc\u30e0\u3067\u8a66\u3057\u305f\u3044\u3067\u3059\u3002\u305d\u306e\u6642\u306f\u5831\u916c\u306eClipping\u3082\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u305d\u3046\u3067\u3059\u306d\u3002\u6b63\u898f\u5316\u3084DropOut\u3068\u304b\u3082\u8003\u616e\u3057\u305f\u307b\u3046\u304c\u3044\u3044\u306e\u304b\u306a\uff1f\n", "tags": ["Python", "DeepLearning", "Chainer", "DQN"]}