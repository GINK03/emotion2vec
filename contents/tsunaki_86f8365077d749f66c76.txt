{"context": "\n\n\u306f\u3058\u3081\u306b\n\u7279\u306bFujitsu Advent Calendar 2016\u306b\u3061\u306a\u3093\u3060\u5185\u5bb9\u3067\u306a\u304f\u3001\u81ea\u5206\u306e\u8da3\u5473\uff08\u7d99\u7d9a\u4e2d\u306e\u958b\u767a\u306e\u7d9a\u304d\uff09\u3092\u66f8\u304d\u307e\u3059\u3002\n\u203b \u5f53\u8a18\u4e8b\u306f\u4f1a\u793e\u3092\u4ee3\u8868\u3059\u308b\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u500b\u4eba\u306e\u610f\u898b\u3067\u3059\u3002\n\uff08\u672c\u696d\u306f\u65b0\u898f\u4e8b\u696d\u7acb\u3061\u4e0a\u3052\u306a\u3069\u3067\u958b\u767a\u8077\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff09\n\n\u7d99\u7d9a\u4e2d\u306e\u958b\u767a\n[\u958b\u767a\u3057\u3066\u3044\u308bSIVA\u306b\u3064\u3044\u3066]\n\uff3bfacebook\uff3d\nhttps://www.facebook.com/AIkeiba/\n\uff3bTwitter\uff3d\nhttps://twitter.com/Siva_keiba\n\u968f\u6642\u5b9f\u6cc1\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u3044\u3044\u306d\uff01\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\n\u4eca\u56de\u306e\u3084\u308b\u3053\u3068\n\u524d\u56de\u306e\u7d9a\u304d\u3002\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\uff08\u6df1\u5c64\u5b66\u7fd2\uff09\u306b\u3088\u308b\u4e88\u6e2c\u306b\u3064\u3044\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u307e\u3059\u3002\n\u59cb\u3081\u308b\u306b\u3042\u305f\u308a\u5fc5\u8981\u306a\u30b9\u30ad\u30eb\u306f\u3053\u3061\u3089\u3092\u53c2\u8003\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u4eca\u56de\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\nTensorFlow:https://www.tensorflow.org/\n\u524d\u56de\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u8a18\u4e8b\u306a\u3069\u3092\u53c2\u8003\u306b\u3057\u3066\u3082\u3089\u3048\u3070\u3068\u601d\u3044\u307e\u3059\u3002\nPython\u306e\u74b0\u5883\u69cb\u7bc9\u304b\u3089TensorFlow\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n\u4e88\u6e2c\u30e2\u30c7\u30eb\u4f5c\u6210\u3068\u5b9f\u9a13(\u524d\u3068\u540c\u3058\u3067\u3059)\n\u4ee5\u4e0b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6e96\u5099\u3057\u3066\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\n\n\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u306f\u4ee5\u4e0b\u306eCSV\uff08JRA-VAN\u3088\u308a\uff09\n\n\n\n\u30e9\u30d9\u30eb\u540d\n\u8aac\u660e\n\n\n\n\n\u958b\u50ac\u65e5\nyyyy-mm-dd\n\n\n\u7af6\u99ac\u5834\n\n\n\n\u30ec\u30fc\u30b9\u756a\u53f7\n\n\n\n\u30ec\u30fc\u30b9\u540d\n\n\n\n\u30b3\u30fc\u30b9\n\u30c0\u30fc\u30c8\n\n\n\u5468\u56de\n\u30c0\u30fc\u30c8\u304b\u829d\u306e\u5834\u5408\u3001\u53f3\u56de\u308a\u306a\u3089\u300c\u53f3\u300d\u3001\u5de6\u56de\u308a\u306a\u3089\u300c\u5de6\u300d\n\n\n\u8ddd\u96e2\n[m]\n\n\n\u99ac\u5834\u72b6\u614b\n\u826f\uff08\u308a\u3087\u3046\uff09\n\n\n\u8cde\u91d1\n[\u4e07\u5186]\n\n\n\u982d\u6570\n\n\n\n\u7740\u9806\n\n\n\n\u67a0\u756a\n\n\n\n\u99ac\u756a\n\n\n\n\u99ac\u540d\n\n\n\n\u6027\u5225\n\n\n\n\u5e74\u9f62\n\n\n\n\u9a0e\u624b\n\n\n\n\u30bf\u30a4\u30e0\n[s]\n\n\n\u7740\u5dee\n\u524d\u7740\u306e\u99ac\u3068\u306e\u5dee\u306e\u3053\u3068,\u30af\u30d3,\n\n\n\u901a\u904e\u9806\n\n\n\n\u4e0a\u308a3F\n\u30e9\u30b9\u30c8600m\u306e\u30bf\u30a4\u30e0[s]\n\n\n\u65a5\u91cf\n[kg]\n\n\n\u99ac\u4f53\u91cd\n[kg]\n\n\n\u5897\u6e1b\n\u524d\u30ec\u30fc\u30b9\u304b\u3089\u306e\u99ac\u4f53\u91cd\u5909\u5316[kg]\n\n\n\u4eba\u6c17\nodds\u306e\u964d\u9806\u306e\u756a\u53f7\n\n\n\u30aa\u30c3\u30ba\n\n\n\n\u30d6\u30ea\u30f3\u30ab\u30fc\n\u30d6\u30ea\u30f3\u30ab\u30fc(\u76ee\u96a0\u3057)\u3042\u308a\u306e\u5834\u5408\u3001\u300cB\u300d\n\n\n\u8abf\u6559\u5e2b\n\n\n\n\u8abf\u6559\u30b3\u30e1\u30f3\u30c8\n\n\n\n\u8abf\u6559\u8a55\u4fa1\n\n\n\n\n\u203b\u30d6\u30ea\u30f3\u30ab\u30fc\u3001\u8abf\u6559\u5e2b\u3001\u8abf\u6559\u30b3\u30e1\u30f3\u30c8\u4ee5\u5916\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3057\u3066\u307e\u3059\u3002\n\n\u4e88\u6e2c\u30d7\u30ed\u30b0\u30e9\u30e0(trainNN.py)\n\u30c7\u30fc\u30bf\u3068\u5408\u308f\u305b\u3066Git\u306b\u3042\u3052\u3068\u304d\u307e\u3057\u305f\u3002\n# -*- coding: utf-8 -*-\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn import preprocessing\n\nclass TrainNN:\n\n  def __init__(self) : \n    # Parameters\n    self.learning_rate = 0.01     # \u5b66\u7fd2\u7387 \u9ad8\u3044\u3068cost\u306e\u53ce\u675f\u304c\u65e9\u307e\u308b\n    self.training_epochs = 50     # \u5b66\u7fd2\u5168\u4f53\u3092\u3053\u306e\u30a8\u30dd\u30c3\u30af\u6570\u3067\u533a\u5207\u308a\u3001\u533a\u5207\u308a\u3054\u3068\u306bcost\u3092\u8868\u793a\u3059\u308b\n\n    self.batch_choice = 500\n    self.batch_size = 0           # \u5b66\u7fd21\u56de\u3054\u3068( sess.run()\u3054\u3068 )\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u3044\u304f\u3064\u5229\u7528\u3059\u308b\u304b\n    self.display_step = 1         # 1\u306a\u3089\u6bce\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306bcost\u3092\u8868\u793a\n    self.train_size = 500         # \u5168\u30c7\u30fc\u30bf\u306e\u4e2d\u3067\u3044\u304f\u3064\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u56de\u3059\u304b\n    self.step_size =  500         # \u4f55\u30b9\u30c6\u30c3\u30d7\u5b66\u7fd2\u3059\u308b\u304b\n\n    # Network Parameters\n    self.n_hidden_1 = 64      # \u96a0\u308c\u5c641\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u6570\n    self.n_hidden_2 = 64      # \u96a0\u308c\u5c642\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u6570\n\n    self.n_input = 27          # \u4e0e\u3048\u308b\u5909\u6570\u306e\u6570\n    self.n_classes = 0        # \u5206\u985e\u3059\u308b\u30af\u30e9\u30b9\u306e\u6570\n\n  def load_csv(self):\n    file_name = \"data/jra_race_resultNN.csv\"\n    df = pd.read_csv(file_name)\n    ## \u6587\u5b57\u5217\u306e\u6570\u5024\u5316\n    labelEncoder = preprocessing.LabelEncoder()\n    df['area_name'] = labelEncoder.fit_transform(df['area_name'])\n    df['race_name'] = labelEncoder.fit_transform(df['race_name'])\n    df['track'] = labelEncoder.fit_transform(df['track'])\n    df['run_direction'] = labelEncoder.fit_transform(df['run_direction'])\n    df['track_condition'] = labelEncoder.fit_transform(df['track_condition'])\n    df['horse_name'] = labelEncoder.fit_transform(df['horse_name'])\n    df['horse_sex'] = labelEncoder.fit_transform(df['horse_sex'])\n    df['jockey_name'] = labelEncoder.fit_transform(df['jockey_name'])\n    df['margin'] = labelEncoder.fit_transform(df['margin'])\n    df['is_blinkers'] = labelEncoder.fit_transform(df['is_blinkers'])\n    df['trainer_name'] = labelEncoder.fit_transform(df['trainer_name'])\n    df['comments_by_trainer'] = labelEncoder.fit_transform(df['comments_by_trainer'])\n    df['evaluation_by_trainer'] = labelEncoder.fit_transform(df['evaluation_by_trainer'])\n    df['dhorse_weight'] = labelEncoder.fit_transform(df['dhorse_weight'])\n    x_np = np.array(df[['area_name', 'race_number', 'race_name', 'track', 'run_direction',\n                       'distance', 'track_condition', 'purse', 'heads_count', \n                       'post_position', 'horse_number', 'horse_name', 'horse_sex', 'horse_age', \n                       'jockey_name', 'time', 'margin', 'time3F', \n                       'load_weight', 'horse_weight', 'dhorse_weight', 'odds_order', \n                       'odds', 'is_blinkers', 'trainer_name', 'comments_by_trainer', \n                        'evaluation_by_trainer'\n    ]].fillna(0))\n    # \u7d50\u679c\n    d = df[['finish_order']].to_dict('record')\n\n    self.vectorizer = DictVectorizer(sparse=False)\n    y_np = self.vectorizer.fit_transform(d)\n    self.n_classes = len(self.vectorizer.get_feature_names())\n\n    # \u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\n    [self.x_train, self.x_test] = np.vsplit(x_np, [self.train_size]) \n    [self.y_train, self.y_test] = np.vsplit(y_np, [self.train_size])\n    self.batch_size = self.train_size\n\n  # Create model\n  def multilayer_perceptron(self, x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n\n  def train(self) :\n    # tf Graph input\n    x = tf.placeholder(\"float\", [None, self.n_input])\n    y = tf.placeholder(\"float\", [None, self.n_classes])\n\n    # Store layers weight & bias\n    weights = {\n      'h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1]), name=\"h1\"),\n      'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2]), name=\"h2\"),\n      'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes]), name=\"wout\")\n    }\n    # \u30d0\u30a4\u30a2\u30b9\u306e\u8a2d\u5b9a\n    biases = {\n      'b1': tf.Variable(tf.random_normal([self.n_hidden_1]), name=\"b1\"),\n      'b2': tf.Variable(tf.random_normal([self.n_hidden_2]), name=\"b2\"),\n      'out': tf.Variable(tf.random_normal([self.n_classes]), name=\"bout\")\n    }\n\n    # Construct model\n    pred = self.multilayer_perceptron(x, weights, biases)\n\n    # Define loss and optimizer\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    # Logistic Regression  AdamOptimizer GradientDescentOptimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost)\n\n    # Initializing the variables\n    init = tf.initialize_all_variables()\n    saver = tf.train.Saver()\n    # Launch the graph\n\n    with tf.Session() as sess:\n      tf.scalar_summary(\"cost\", cost)\n      tf.scalar_summary(\"accuracy\", accuracy)\n      merged = tf.merge_all_summaries()\n      writer = tf.train.SummaryWriter(\"logs/tensorflow_log\", sess.graph_def)    \n\n      sess.run(init)\n      # Training cycle\n      for epoch in range(self.training_epochs):\n        avg_cost = 0.\n        # Loop over step_size\n        for i in range(self.step_size):\n          # \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089 batch_size \u3067\u6307\u5b9a\u3057\u305f\u6570\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u5f97\n          ind = np.random.choice(self.batch_size, self.batch_choice)\n          x_train_batch = self.x_train[ind]\n          y_train_batch = self.y_train[ind]\n          # Run optimization op (backprop) and loss op (to get loss value)\n          _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch})\n          avg_cost += c / self.step_size\n\n        # Display logs per epoch step\n        if epoch % self.display_step == 0:\n          print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n          # Compute average loss\n          summary_str, acc = sess.run([merged, accuracy], feed_dict={x: x_train_batch, y: y_train_batch})\n          writer.add_summary(summary_str, epoch)\n          print \"Accuracy:\", acc\n          ## model\u306e\u4fdd\u5b58\n          # name_model_file = 'model_epoch_' + str(epoch+1) + '.ckpt'\n          # save_path = saver.save(sess, 'model/tensorflow/' + name_model_file)\n\n      # Test model\n      correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n      # Calculate accuracy\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n      print \"Accuracy:\", accuracy.eval(session=sess, feed_dict={x: self.x_test, y: self.y_test})\n\nif __name__ == \"__main__\":\n  trainNN = TrainNN()\n  trainNN.load_csv()\n  trainNN.train()\n\n\n\u203b python\u304a\u3088\u3073\u30c7TensorFlow\u306b\u3064\u3044\u3066\u306f\u52c9\u5f37\u4e2d\u3067\u3059\u306e\u3067\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u8aa4\u308a\u304c\u3042\u308b\u304b\u3082\u3067\u3059\u3002\n\n\u7d50\u679c\n$ python trainNN.py \nEpoch: 0001 cost= 28999.539070313\nAccuracy: 0.183333\nEpoch: 0002 cost= 10636.817809570\nAccuracy: 0.146667\nEpoch: 0003 cost= 8187.893877930\nAccuracy: 0.216667\nEpoch: 0004 cost= 6673.761091797\nAccuracy: 0.183333\nEpoch: 0005 cost= 5364.944535645\nAccuracy: 0.196667\nEpoch: 0006 cost= 4300.876705078\nAccuracy: 0.176667\nEpoch: 0007 cost= 3560.373814697\nAccuracy: 0.27\nEpoch: 0008 cost= 2843.689293701\nAccuracy: 0.203333\nEpoch: 0009 cost= 1605.616168819\nAccuracy: 0.1\nEpoch: 0010 cost= 21.059406254\nAccuracy: 0.0666667\nEpoch: 0011 cost= 9.320877672\nAccuracy: 0.0566667\nEpoch: 0012 cost= 5.621849694\nAccuracy: 0.0866667\nEpoch: 0013 cost= 4.749191205\nAccuracy: 0.07\nEpoch: 0014 cost= 4.069611630\nAccuracy: 0.0733333\nEpoch: 0015 cost= 3.946254434\nAccuracy: 0.0566667\nEpoch: 0016 cost= 3.253856863\nAccuracy: 0.0433333\nEpoch: 0017 cost= 3.309734127\nAccuracy: 0.07\nEpoch: 0018 cost= 2.978426178\nAccuracy: 0.0566667\nEpoch: 0019 cost= 2.970437231\nAccuracy: 0.0533333\nEpoch: 0020 cost= 2.999267270\nAccuracy: 0.0566667\nEpoch: 0021 cost= 3.828851234\nAccuracy: 0.0766667\nEpoch: 0022 cost= 2.859027134\nAccuracy: 0.0666667\nEpoch: 0023 cost= 2.799751988\nAccuracy: 0.08\nEpoch: 0024 cost= 2.799972694\nAccuracy: 0.06\nEpoch: 0025 cost= 2.799448245\nAccuracy: 0.0633333\nEpoch: 0026 cost= 2.799542419\nAccuracy: 0.06\nEpoch: 0027 cost= 2.798464981\nAccuracy: 0.0933333\nEpoch: 0028 cost= 2.799431955\nAccuracy: 0.07\nEpoch: 0029 cost= 2.798901843\nAccuracy: 0.05\nEpoch: 0030 cost= 2.797275106\nAccuracy: 0.0366667\nEpoch: 0031 cost= 2.799933175\nAccuracy: 0.0566667\nEpoch: 0032 cost= 2.799303545\nAccuracy: 0.07\nEpoch: 0033 cost= 2.799922926\nAccuracy: 0.0533333\nEpoch: 0034 cost= 2.799450584\nAccuracy: 0.0833333\nEpoch: 0035 cost= 2.800814153\nAccuracy: 0.08\nEpoch: 0036 cost= 2.799030569\nAccuracy: 0.0733333\nEpoch: 0037 cost= 2.799764482\nAccuracy: 0.0566667\nEpoch: 0038 cost= 2.799920460\nAccuracy: 0.0833333\nEpoch: 0039 cost= 2.799320694\nAccuracy: 0.09\nEpoch: 0040 cost= 2.800959777\nAccuracy: 0.0766667\nEpoch: 0041 cost= 2.799612596\nAccuracy: 0.09\nEpoch: 0042 cost= 2.800021134\nAccuracy: 0.0666667\nEpoch: 0043 cost= 2.799918224\nAccuracy: 0.0666667\nEpoch: 0044 cost= 2.799846961\nAccuracy: 0.06\nEpoch: 0045 cost= 2.799782973\nAccuracy: 0.06\nEpoch: 0046 cost= 2.798343555\nAccuracy: 0.0633333\nEpoch: 0047 cost= 2.800401002\nAccuracy: 0.0633333\nEpoch: 0048 cost= 2.799323093\nAccuracy: 0.06\nEpoch: 0049 cost= 2.799231728\nAccuracy: 0.0733333\nEpoch: 0050 cost= 2.800211056\nAccuracy: 0.0633333\nAccuracy: 0.0697074\n\n$ tensorboard --logdir=logs/\n\n\n\ntensorboard\n\n\n0.6%\u3002\u3002\u3002\u5168\u304f\u3042\u305f\u308a\u307e\u305b\u3093\u306d\uff08\u7b11\uff09\n\u203b \u3082\u3061\u308d\u3093\u3067\u3059\u304cSIVA\u3067\u958b\u767a\u3057\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u5168\u304f\u9055\u3044\u307e\u3059\u3002\n\u3082\u3046\u5c11\u3057\u52c9\u5f37\u3057\u3066\u307e\u305f\u66f8\u304d\u307e\u3059\uff01\n\n\u3055\u3044\u3054\u306b\uff08\u3054\u53c2\u8003\uff09\nSIVA\u306b\u3064\u3044\u3066\u306f\u7684\u4e2d\u7387\u306f\u7d0440%\u3067\u56de\u53ce\u7387\u306f\u5b89\u5b9a\u3057\u3066100%\u524d\u5f8c\u3067\u3059\u3002\n\n[\u958b\u767a\u3057\u3066\u3044\u308bSIVA\u306b\u3064\u3044\u3066]\n\uff3bfacebook\uff3d\nhttps://www.facebook.com/AIkeiba/\n\uff3bTwitter\uff3d\nhttps://twitter.com/Siva_keiba\n\u968f\u6642\u5b9f\u6cc1\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u3044\u3044\u306d\uff01\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n## \u306f\u3058\u3081\u306b\n\u7279\u306bFujitsu Advent Calendar 2016\u306b\u3061\u306a\u3093\u3060\u5185\u5bb9\u3067\u306a\u304f\u3001\u81ea\u5206\u306e\u8da3\u5473\uff08\u7d99\u7d9a\u4e2d\u306e\u958b\u767a\u306e\u7d9a\u304d\uff09\u3092\u66f8\u304d\u307e\u3059\u3002\n\u203b \u5f53\u8a18\u4e8b\u306f\u4f1a\u793e\u3092\u4ee3\u8868\u3059\u308b\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u500b\u4eba\u306e\u610f\u898b\u3067\u3059\u3002\n\uff08\u672c\u696d\u306f\u65b0\u898f\u4e8b\u696d\u7acb\u3061\u4e0a\u3052\u306a\u3069\u3067\u958b\u767a\u8077\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff09\n\n### \u7d99\u7d9a\u4e2d\u306e\u958b\u767a\n[\u958b\u767a\u3057\u3066\u3044\u308bSIVA\u306b\u3064\u3044\u3066]\n\uff3bfacebook\uff3d\nhttps://www.facebook.com/AIkeiba/\n\uff3bTwitter\uff3d\nhttps://twitter.com/Siva_keiba\n\u968f\u6642\u5b9f\u6cc1\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u3044\u3044\u306d\uff01\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\n## \u4eca\u56de\u306e\u3084\u308b\u3053\u3068\n[\u524d\u56de](http://qiita.com/tsunaki/items/82879dbe3cb7a6217eb0)\u306e\u7d9a\u304d\u3002\n\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\uff08\u6df1\u5c64\u5b66\u7fd2\uff09\u306b\u3088\u308b\u4e88\u6e2c\u306b\u3064\u3044\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u307e\u3059\u3002\n\n\u59cb\u3081\u308b\u306b\u3042\u305f\u308a\u5fc5\u8981\u306a\u30b9\u30ad\u30eb\u306f[\u3053\u3061\u3089](http://qiita.com/tsunaki/items/d440fa06f763b0bf17f7)\u3092\u53c2\u8003\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n## \u4eca\u56de\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\n\nTensorFlow:https://www.tensorflow.org/\n[\u524d\u56de](http://qiita.com/tsunaki/items/d440fa06f763b0bf17f7)\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u8a18\u4e8b\u306a\u3069\u3092\u53c2\u8003\u306b\u3057\u3066\u3082\u3089\u3048\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n[Python\u306e\u74b0\u5883\u69cb\u7bc9\u304b\u3089TensorFlow\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb](http://qiita.com/bohemian916/items/4f0736dcea932a162d9e)\n  \n## \u4e88\u6e2c\u30e2\u30c7\u30eb\u4f5c\u6210\u3068\u5b9f\u9a13(\u524d\u3068\u540c\u3058\u3067\u3059)\n\n\u4ee5\u4e0b\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u6e96\u5099\u3057\u3066\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\n###\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u306f\u4ee5\u4e0b\u306eCSV\uff08JRA-VAN\u3088\u308a\uff09\n\n| \u30e9\u30d9\u30eb\u540d    | \u8aac\u660e |\n| :-----------|:--------------|:--------------|\n|\u958b\u50ac\u65e5|yyyy-mm-dd|\n|\u7af6\u99ac\u5834|\n|\u30ec\u30fc\u30b9\u756a\u53f7|\n|\u30ec\u30fc\u30b9\u540d|\n|\u30b3\u30fc\u30b9|\u30c0\u30fc\u30c8|\n|\u5468\u56de|\u30c0\u30fc\u30c8\u304b\u829d\u306e\u5834\u5408\u3001\u53f3\u56de\u308a\u306a\u3089\u300c\u53f3\u300d\u3001\u5de6\u56de\u308a\u306a\u3089\u300c\u5de6\u300d|\n|\u8ddd\u96e2|[m]|\n|\u99ac\u5834\u72b6\u614b|\u826f\uff08\u308a\u3087\u3046\uff09|\n|\u8cde\u91d1|[\u4e07\u5186]|\n|\u982d\u6570|\n|\u7740\u9806|\n|\u67a0\u756a|\n|\u99ac\u756a|\n|\u99ac\u540d|\n|\u6027\u5225|\n|\u5e74\u9f62|\n|\u9a0e\u624b|\n|\u30bf\u30a4\u30e0|[s]|\n|\u7740\u5dee|\u524d\u7740\u306e\u99ac\u3068\u306e\u5dee\u306e\u3053\u3068,\u30af\u30d3,|\u5927\u5dee|\n|\u901a\u904e\u9806|\n|\u4e0a\u308a3F|\u30e9\u30b9\u30c8600m\u306e\u30bf\u30a4\u30e0[s]|\n|\u65a5\u91cf|[kg]|\n|\u99ac\u4f53\u91cd|[kg]|\n|\u5897\u6e1b|\u524d\u30ec\u30fc\u30b9\u304b\u3089\u306e\u99ac\u4f53\u91cd\u5909\u5316[kg]|\n|\u4eba\u6c17|odds\u306e\u964d\u9806\u306e\u756a\u53f7|\n|\u30aa\u30c3\u30ba|\n|\u30d6\u30ea\u30f3\u30ab\u30fc|\u30d6\u30ea\u30f3\u30ab\u30fc(\u76ee\u96a0\u3057)\u3042\u308a\u306e\u5834\u5408\u3001\u300cB\u300d|\n|\u8abf\u6559\u5e2b|\n|\u8abf\u6559\u30b3\u30e1\u30f3\u30c8|\n|\u8abf\u6559\u8a55\u4fa1|\n\n\n\u203b\u30d6\u30ea\u30f3\u30ab\u30fc\u3001\u8abf\u6559\u5e2b\u3001\u8abf\u6559\u30b3\u30e1\u30f3\u30c8\u4ee5\u5916\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3057\u3066\u307e\u3059\u3002\n\n###\u4e88\u6e2c\u30d7\u30ed\u30b0\u30e9\u30e0(trainNN.py)\n\n\u30c7\u30fc\u30bf\u3068\u5408\u308f\u305b\u3066[Git](https://github.com/tsunaki00/horse_racing)\u306b\u3042\u3052\u3068\u304d\u307e\u3057\u305f\u3002\n\n```python\n# -*- coding: utf-8 -*-\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn import preprocessing\n\nclass TrainNN:\n\n  def __init__(self) : \n    # Parameters\n    self.learning_rate = 0.01     # \u5b66\u7fd2\u7387 \u9ad8\u3044\u3068cost\u306e\u53ce\u675f\u304c\u65e9\u307e\u308b\n    self.training_epochs = 50     # \u5b66\u7fd2\u5168\u4f53\u3092\u3053\u306e\u30a8\u30dd\u30c3\u30af\u6570\u3067\u533a\u5207\u308a\u3001\u533a\u5207\u308a\u3054\u3068\u306bcost\u3092\u8868\u793a\u3059\u308b\n\n    self.batch_choice = 500\n    self.batch_size = 0           # \u5b66\u7fd21\u56de\u3054\u3068( sess.run()\u3054\u3068 )\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u3044\u304f\u3064\u5229\u7528\u3059\u308b\u304b\n    self.display_step = 1         # 1\u306a\u3089\u6bce\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306bcost\u3092\u8868\u793a\n    self.train_size = 500         # \u5168\u30c7\u30fc\u30bf\u306e\u4e2d\u3067\u3044\u304f\u3064\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u56de\u3059\u304b\n    self.step_size =  500         # \u4f55\u30b9\u30c6\u30c3\u30d7\u5b66\u7fd2\u3059\u308b\u304b\n\n    # Network Parameters\n    self.n_hidden_1 = 64      # \u96a0\u308c\u5c641\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u6570\n    self.n_hidden_2 = 64      # \u96a0\u308c\u5c642\u306e\u30e6\u30cb\u30c3\u30c8\u306e\u6570\n\n    self.n_input = 27          # \u4e0e\u3048\u308b\u5909\u6570\u306e\u6570\n    self.n_classes = 0        # \u5206\u985e\u3059\u308b\u30af\u30e9\u30b9\u306e\u6570\n\n  def load_csv(self):\n    file_name = \"data/jra_race_resultNN.csv\"\n    df = pd.read_csv(file_name)\n    ## \u6587\u5b57\u5217\u306e\u6570\u5024\u5316\n    labelEncoder = preprocessing.LabelEncoder()\n    df['area_name'] = labelEncoder.fit_transform(df['area_name'])\n    df['race_name'] = labelEncoder.fit_transform(df['race_name'])\n    df['track'] = labelEncoder.fit_transform(df['track'])\n    df['run_direction'] = labelEncoder.fit_transform(df['run_direction'])\n    df['track_condition'] = labelEncoder.fit_transform(df['track_condition'])\n    df['horse_name'] = labelEncoder.fit_transform(df['horse_name'])\n    df['horse_sex'] = labelEncoder.fit_transform(df['horse_sex'])\n    df['jockey_name'] = labelEncoder.fit_transform(df['jockey_name'])\n    df['margin'] = labelEncoder.fit_transform(df['margin'])\n    df['is_blinkers'] = labelEncoder.fit_transform(df['is_blinkers'])\n    df['trainer_name'] = labelEncoder.fit_transform(df['trainer_name'])\n    df['comments_by_trainer'] = labelEncoder.fit_transform(df['comments_by_trainer'])\n    df['evaluation_by_trainer'] = labelEncoder.fit_transform(df['evaluation_by_trainer'])\n    df['dhorse_weight'] = labelEncoder.fit_transform(df['dhorse_weight'])\n    x_np = np.array(df[['area_name', 'race_number', 'race_name', 'track', 'run_direction',\n                       'distance', 'track_condition', 'purse', 'heads_count', \n                       'post_position', 'horse_number', 'horse_name', 'horse_sex', 'horse_age', \n                       'jockey_name', 'time', 'margin', 'time3F', \n                       'load_weight', 'horse_weight', 'dhorse_weight', 'odds_order', \n                       'odds', 'is_blinkers', 'trainer_name', 'comments_by_trainer', \n                        'evaluation_by_trainer'\n    ]].fillna(0))\n    # \u7d50\u679c\n    d = df[['finish_order']].to_dict('record')\n\n    self.vectorizer = DictVectorizer(sparse=False)\n    y_np = self.vectorizer.fit_transform(d)\n    self.n_classes = len(self.vectorizer.get_feature_names())\n\n    # \u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\n    [self.x_train, self.x_test] = np.vsplit(x_np, [self.train_size]) \n    [self.y_train, self.y_test] = np.vsplit(y_np, [self.train_size])\n    self.batch_size = self.train_size\n\n  # Create model\n  def multilayer_perceptron(self, x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n\n  def train(self) :\n    # tf Graph input\n    x = tf.placeholder(\"float\", [None, self.n_input])\n    y = tf.placeholder(\"float\", [None, self.n_classes])\n    \n    # Store layers weight & bias\n    weights = {\n      'h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1]), name=\"h1\"),\n      'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2]), name=\"h2\"),\n      'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes]), name=\"wout\")\n    }\n    # \u30d0\u30a4\u30a2\u30b9\u306e\u8a2d\u5b9a\n    biases = {\n      'b1': tf.Variable(tf.random_normal([self.n_hidden_1]), name=\"b1\"),\n      'b2': tf.Variable(tf.random_normal([self.n_hidden_2]), name=\"b2\"),\n      'out': tf.Variable(tf.random_normal([self.n_classes]), name=\"bout\")\n    }\n\n    # Construct model\n    pred = self.multilayer_perceptron(x, weights, biases)\n\n    # Define loss and optimizer\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    # Logistic Regression  AdamOptimizer GradientDescentOptimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost)\n\n    # Initializing the variables\n    init = tf.initialize_all_variables()\n    saver = tf.train.Saver()\n    # Launch the graph\n    \n    with tf.Session() as sess:\n      tf.scalar_summary(\"cost\", cost)\n      tf.scalar_summary(\"accuracy\", accuracy)\n      merged = tf.merge_all_summaries()\n      writer = tf.train.SummaryWriter(\"logs/tensorflow_log\", sess.graph_def)    \n\n      sess.run(init)\n      # Training cycle\n      for epoch in range(self.training_epochs):\n        avg_cost = 0.\n        # Loop over step_size\n        for i in range(self.step_size):\n          # \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089 batch_size \u3067\u6307\u5b9a\u3057\u305f\u6570\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u5f97\n          ind = np.random.choice(self.batch_size, self.batch_choice)\n          x_train_batch = self.x_train[ind]\n          y_train_batch = self.y_train[ind]\n          # Run optimization op (backprop) and loss op (to get loss value)\n          _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch})\n          avg_cost += c / self.step_size\n\n        # Display logs per epoch step\n        if epoch % self.display_step == 0:\n          print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n          # Compute average loss\n          summary_str, acc = sess.run([merged, accuracy], feed_dict={x: x_train_batch, y: y_train_batch})\n          writer.add_summary(summary_str, epoch)\n          print \"Accuracy:\", acc\n          ## model\u306e\u4fdd\u5b58\n          # name_model_file = 'model_epoch_' + str(epoch+1) + '.ckpt'\n          # save_path = saver.save(sess, 'model/tensorflow/' + name_model_file)\n            \n      # Test model\n      correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n      # Calculate accuracy\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n      print \"Accuracy:\", accuracy.eval(session=sess, feed_dict={x: self.x_test, y: self.y_test})\n\nif __name__ == \"__main__\":\n  trainNN = TrainNN()\n  trainNN.load_csv()\n  trainNN.train()\n  \n```\n\u203b python\u304a\u3088\u3073\u30c7TensorFlow\u306b\u3064\u3044\u3066\u306f\u52c9\u5f37\u4e2d\u3067\u3059\u306e\u3067\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u8aa4\u308a\u304c\u3042\u308b\u304b\u3082\u3067\u3059\u3002\n\n### \u7d50\u679c\n\n```\n$ python trainNN.py \nEpoch: 0001 cost= 28999.539070313\nAccuracy: 0.183333\nEpoch: 0002 cost= 10636.817809570\nAccuracy: 0.146667\nEpoch: 0003 cost= 8187.893877930\nAccuracy: 0.216667\nEpoch: 0004 cost= 6673.761091797\nAccuracy: 0.183333\nEpoch: 0005 cost= 5364.944535645\nAccuracy: 0.196667\nEpoch: 0006 cost= 4300.876705078\nAccuracy: 0.176667\nEpoch: 0007 cost= 3560.373814697\nAccuracy: 0.27\nEpoch: 0008 cost= 2843.689293701\nAccuracy: 0.203333\nEpoch: 0009 cost= 1605.616168819\nAccuracy: 0.1\nEpoch: 0010 cost= 21.059406254\nAccuracy: 0.0666667\nEpoch: 0011 cost= 9.320877672\nAccuracy: 0.0566667\nEpoch: 0012 cost= 5.621849694\nAccuracy: 0.0866667\nEpoch: 0013 cost= 4.749191205\nAccuracy: 0.07\nEpoch: 0014 cost= 4.069611630\nAccuracy: 0.0733333\nEpoch: 0015 cost= 3.946254434\nAccuracy: 0.0566667\nEpoch: 0016 cost= 3.253856863\nAccuracy: 0.0433333\nEpoch: 0017 cost= 3.309734127\nAccuracy: 0.07\nEpoch: 0018 cost= 2.978426178\nAccuracy: 0.0566667\nEpoch: 0019 cost= 2.970437231\nAccuracy: 0.0533333\nEpoch: 0020 cost= 2.999267270\nAccuracy: 0.0566667\nEpoch: 0021 cost= 3.828851234\nAccuracy: 0.0766667\nEpoch: 0022 cost= 2.859027134\nAccuracy: 0.0666667\nEpoch: 0023 cost= 2.799751988\nAccuracy: 0.08\nEpoch: 0024 cost= 2.799972694\nAccuracy: 0.06\nEpoch: 0025 cost= 2.799448245\nAccuracy: 0.0633333\nEpoch: 0026 cost= 2.799542419\nAccuracy: 0.06\nEpoch: 0027 cost= 2.798464981\nAccuracy: 0.0933333\nEpoch: 0028 cost= 2.799431955\nAccuracy: 0.07\nEpoch: 0029 cost= 2.798901843\nAccuracy: 0.05\nEpoch: 0030 cost= 2.797275106\nAccuracy: 0.0366667\nEpoch: 0031 cost= 2.799933175\nAccuracy: 0.0566667\nEpoch: 0032 cost= 2.799303545\nAccuracy: 0.07\nEpoch: 0033 cost= 2.799922926\nAccuracy: 0.0533333\nEpoch: 0034 cost= 2.799450584\nAccuracy: 0.0833333\nEpoch: 0035 cost= 2.800814153\nAccuracy: 0.08\nEpoch: 0036 cost= 2.799030569\nAccuracy: 0.0733333\nEpoch: 0037 cost= 2.799764482\nAccuracy: 0.0566667\nEpoch: 0038 cost= 2.799920460\nAccuracy: 0.0833333\nEpoch: 0039 cost= 2.799320694\nAccuracy: 0.09\nEpoch: 0040 cost= 2.800959777\nAccuracy: 0.0766667\nEpoch: 0041 cost= 2.799612596\nAccuracy: 0.09\nEpoch: 0042 cost= 2.800021134\nAccuracy: 0.0666667\nEpoch: 0043 cost= 2.799918224\nAccuracy: 0.0666667\nEpoch: 0044 cost= 2.799846961\nAccuracy: 0.06\nEpoch: 0045 cost= 2.799782973\nAccuracy: 0.06\nEpoch: 0046 cost= 2.798343555\nAccuracy: 0.0633333\nEpoch: 0047 cost= 2.800401002\nAccuracy: 0.0633333\nEpoch: 0048 cost= 2.799323093\nAccuracy: 0.06\nEpoch: 0049 cost= 2.799231728\nAccuracy: 0.0733333\nEpoch: 0050 cost= 2.800211056\nAccuracy: 0.0633333\nAccuracy: 0.0697074\n\n$ tensorboard --logdir=logs/\n\n```\n\n### tensorboard\n\n![Screen Shot 0028-12-17 at 23.12.08.png](https://qiita-image-store.s3.amazonaws.com/0/52867/5dd7ef60-e4eb-931f-7d97-eea89440f085.png \"Screen Shot 0028-12-17 at 23.12.08.png\")\n\n![Screen Shot 0028-12-17 at 23.13.23.png](https://qiita-image-store.s3.amazonaws.com/0/52867/973e4d8d-b707-ac27-bfbe-50e06daae352.png \"Screen Shot 0028-12-17 at 23.13.23.png\")\n\n\n0.6%\u3002\u3002\u3002\u5168\u304f\u3042\u305f\u308a\u307e\u305b\u3093\u306d\uff08\u7b11\uff09\n\u203b \u3082\u3061\u308d\u3093\u3067\u3059\u304cSIVA\u3067\u958b\u767a\u3057\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306f\u5168\u304f\u9055\u3044\u307e\u3059\u3002\n\u3082\u3046\u5c11\u3057\u52c9\u5f37\u3057\u3066\u307e\u305f\u66f8\u304d\u307e\u3059\uff01\n\n## \u3055\u3044\u3054\u306b\uff08\u3054\u53c2\u8003\uff09\nSIVA\u306b\u3064\u3044\u3066\u306f\u7684\u4e2d\u7387\u306f\u7d0440%\u3067\u56de\u53ce\u7387\u306f\u5b89\u5b9a\u3057\u3066100%\u524d\u5f8c\u3067\u3059\u3002\n\n![CzMZW9OUkAAab_1.jpg](https://qiita-image-store.s3.amazonaws.com/0/52867/96bd7971-8f2c-58ac-e6b7-fb8f36fa7adc.jpeg \"CzMZW9OUkAAab_1.jpg\")\n\n[\u958b\u767a\u3057\u3066\u3044\u308bSIVA\u306b\u3064\u3044\u3066]\n\uff3bfacebook\uff3d\nhttps://www.facebook.com/AIkeiba/\n\uff3bTwitter\uff3d\nhttps://twitter.com/Siva_keiba\n\u968f\u6642\u5b9f\u6cc1\u3057\u3066\u3044\u304d\u307e\u3059\u306e\u3067\u3001\u3044\u3044\u306d\uff01\u30d5\u30a9\u30ed\u30fc\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n", "tags": ["Python", "TensorFlow", "\u6a5f\u68b0\u5b66\u7fd2", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "\u4eba\u5de5\u77e5\u80fd"]}