{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n\n\n\u95a2\u9023\u3000http://qiita.com/7of9/items/b364d897b95476a30754\nsine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias\u3092\u3082\u3068\u306b\u81ea\u5206\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u73fe\u3057\u3066\u51fa\u529b\u3092\u8a08\u7b97\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\nhttp://qiita.com/7of9/items/b52684b0df64b6561a48\n\u306e\u7d9a\u304d\nhttp://qiita.com/7of9/items/7e45a69c822900a80c67\n\u306b\u304a\u3044\u3066sine curve\u306e\u5b66\u7fd2\u5931\u6557\u3092\u88dc\u6b63\u3067\u304d\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u518d\u5ea6model_variables.npy\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060weight\u3068bias\u304b\u3089sine curve\u3092\u4f5c\u3063\u3066\u307f\u308b\u3002\n\ncode v0.3\n\nv0.2 : applyActFnc\u3092\u8ffd\u52a0\u3057\u3066\u3001sigmoid / linear\u306e\u5207\u308a\u66ff\u3048\u306b\u5bfe\u5fdc\nv0.3 \n\n\ncalc_sigmoid()\u306e\u9593\u9055\u3044\u4fee\u6b63\n\u30c7\u30d0\u30c3\u30b0\u7528\u51fa\u529b\u95a2\u6570\u3092\u8ffd\u52a0\u3002\u30c7\u30d0\u30c3\u30b0\u4e0d\u8981\u6642\u306b\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3059\u308b\n\n\n\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u6642\u306e\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u3001no operation\u306b\u76f8\u5f53\u3059\u308b pass\u3092\u4f7f\u7528 : link\n\n\n\n\n\n\n\nreproduce_sine.py\n'''\nv0.3 Dec. 11, 2016\n    - add output_debugPrint()\n    - fix bug > calc_sigmoid() was using positive for exp()\nv0.2 Dec. 10, 2016\n    - calc_conv() takes [applyActFnc] argument\nv0.1 Dec. 10, 2016\n    - add calc_sigmoid()\n    - add fully_connected network\n    - add input data for sine curve\n=== [read_model_var.py] branched to [reproduce_sine.py] ===\n\nv0.4 Dec. 10, 2016\n    - add 2x2 network example\nv0.3 Dec. 07, 2016\n    - calc_conv() > add bias\nv0.2 Dec. 07, 2016\n    - fix calc_conv() treating src as a list\nv0.1 Dec. 07, 2016\n    - add calc_conv()\n'''\n\nimport numpy as np\nimport math\nimport sys\n\nmodel_var = np.load('model_variables.npy')\n\n\n# to ON/OFF debug print at one place\ndef output_debugPrint(str): \n#   print(str)\n    pass # no operation\n\noutput_debugPrint( (\"all shape:\",(model_var.shape)) )\n\ndef calc_sigmoid(x):\n    return 1.0 / (1.0 + math.exp(-x))\n\ndef calc_conv(src, weight, bias, applyActFnc):\n    wgt = weight.shape\n#   print wgt # debug\n    #conv = list(range(bias.size))\n    conv = [0.0] * bias.size\n    # weight\n    for idx1 in range(wgt[0]):\n        for idx2 in range(wgt[1]):\n            conv[idx2] = conv[idx2] + src[idx1] * weight[idx1,idx2]\n    # bias\n    for idx2 in range(wgt[1]):\n        conv[idx2] = conv[idx2] + bias[idx2]\n    # activation function\n    if applyActFnc:\n        for idx2 in range(wgt[1]):\n            conv[idx2] = calc_sigmoid(conv[idx2])\n\n    return conv # return list\n\ninpdata = np.linspace(0, 1, 30).astype(float).tolist()\n\n\nfor din in inpdata:\n    # input layer (7 node)\n    inlist = [ din ]\n    outdata = calc_conv(inlist, model_var[0], model_var[1], applyActFnc=True)\n    # hidden layer 1 (7 node)\n    outdata = calc_conv(outdata, model_var[2], model_var[3], applyActFnc=True)\n    # hidden layer 2 (7 node)\n    outdata = calc_conv(outdata, model_var[4], model_var[5], applyActFnc=True)\n    # output layer (1 node)\n    outdata = calc_conv(outdata, model_var[6], model_var[7], applyActFnc=False)\n    dout = outdata[0] # ouput is 1 node\n    print '%.3f, %.3f' % (din,dout)\n\n\n\n\n\u5b9f\u884c\n$ python reproduce_sine.py  > res.reprod_sine\n\n\n\nJupyter\u8868\u793a\n\u4ee5\u4e0b\u3092\u6bd4\u8f03\u3057\u305f\u3002\n\nTensorFlow\u306eprediction\u306e\u7d50\u679c\nmodel_variables.npy\u304b\u3089\u306eweight,bias\u3092\u4f7f\u3063\u3066\u8a08\u7b97\n\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('res.161210_1958.cut', delimiter=',')\ninp1 = data1[:,0]\nout1 = data1[:,1]\ndata2 = np.loadtxt('res.reprod_sine', delimiter=',')\ninp2 = data2[:,0]\nout2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\nax1.scatter(inp1, out1, label='TensorFlow prediction')\nax2.scatter(inp2, out2, label='from model_var.npy')\n\n#ax1.set_title('First line plot')\nax1.set_xlabel('x')\nax1.set_ylabel('sine(x) prediction')\nax1.grid(True)\nax1.legend()\nax1.set_xlim([0,1.0])\n\nax2.set_xlabel('x')\nax2.set_ylabel('sine(x) reproduced')\nax2.grid(True)\nax2.legend()\nax2.set_xlim([0,1.0])\n\nfig.show()\n\nhttps://www.quora.com/In-the-movie-Aladdin-what-were-the-three-wishes\n\nWhen he (Alladin) first rubs the lamp. The Genie appears,\n\n\u30a4\u30c7\u3088sine curve\u3002\n\n\u3082\u3046\u3061\u3087\u3063\u3068\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n```\n\n\u95a2\u9023\u3000http://qiita.com/7of9/items/b364d897b95476a30754\n\n[sine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias](http://qiita.com/7of9/items/f7b2e0eeea3b7fdc632c)\u3092\u3082\u3068\u306b\u81ea\u5206\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u73fe\u3057\u3066\u51fa\u529b\u3092\u8a08\u7b97\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\n\nhttp://qiita.com/7of9/items/b52684b0df64b6561a48\n\u306e\u7d9a\u304d\n\nhttp://qiita.com/7of9/items/7e45a69c822900a80c67\n\u306b\u304a\u3044\u3066sine curve\u306e\u5b66\u7fd2\u5931\u6557\u3092\u88dc\u6b63\u3067\u304d\u305f\u3088\u3046\u306a\u306e\u3067\u3001\u518d\u5ea6model_variables.npy\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3060weight\u3068bias\u304b\u3089sine curve\u3092\u4f5c\u3063\u3066\u307f\u308b\u3002\n\n## code v0.3\n\n- v0.2 : applyActFnc\u3092\u8ffd\u52a0\u3057\u3066\u3001sigmoid / linear\u306e\u5207\u308a\u66ff\u3048\u306b\u5bfe\u5fdc\n- v0.3 \n  - calc_sigmoid()\u306e\u9593\u9055\u3044\u4fee\u6b63\n  - \u30c7\u30d0\u30c3\u30b0\u7528\u51fa\u529b\u95a2\u6570\u3092\u8ffd\u52a0\u3002\u30c7\u30d0\u30c3\u30b0\u4e0d\u8981\u6642\u306b\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3059\u308b\n     - \u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u6642\u306e\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u3001no operation\u306b\u76f8\u5f53\u3059\u308b pass\u3092\u4f7f\u7528 : [link](http://stackoverflow.com/questions/690622/whats-a-standard-way-to-do-a-no-op-in-python)\n\n```reproduce_sine.py\n'''\nv0.3 Dec. 11, 2016\n\t- add output_debugPrint()\n\t- fix bug > calc_sigmoid() was using positive for exp()\nv0.2 Dec. 10, 2016\n\t- calc_conv() takes [applyActFnc] argument\nv0.1 Dec. 10, 2016\n\t- add calc_sigmoid()\n\t- add fully_connected network\n\t- add input data for sine curve\n=== [read_model_var.py] branched to [reproduce_sine.py] ===\n\nv0.4 Dec. 10, 2016\n\t- add 2x2 network example\nv0.3 Dec. 07, 2016\n\t- calc_conv() > add bias\nv0.2 Dec. 07, 2016\n\t- fix calc_conv() treating src as a list\nv0.1 Dec. 07, 2016\n\t- add calc_conv()\n'''\n\nimport numpy as np\nimport math\nimport sys\n\nmodel_var = np.load('model_variables.npy')\n\n\n# to ON/OFF debug print at one place\ndef output_debugPrint(str): \n#\tprint(str)\n\tpass # no operation\n\noutput_debugPrint( (\"all shape:\",(model_var.shape)) )\n\ndef calc_sigmoid(x):\n\treturn 1.0 / (1.0 + math.exp(-x))\n\ndef calc_conv(src, weight, bias, applyActFnc):\n\twgt = weight.shape\n#\tprint wgt # debug\n\t#conv = list(range(bias.size))\n\tconv = [0.0] * bias.size\n\t# weight\n\tfor idx1 in range(wgt[0]):\n\t\tfor idx2 in range(wgt[1]):\n\t\t\tconv[idx2] = conv[idx2] + src[idx1] * weight[idx1,idx2]\n\t# bias\n\tfor idx2 in range(wgt[1]):\n\t\tconv[idx2] = conv[idx2] + bias[idx2]\n\t# activation function\n\tif applyActFnc:\n\t\tfor idx2 in range(wgt[1]):\n\t\t\tconv[idx2] = calc_sigmoid(conv[idx2])\n\n\treturn conv # return list\n\ninpdata = np.linspace(0, 1, 30).astype(float).tolist()\n\n\nfor din in inpdata:\n\t# input layer (7 node)\n\tinlist = [ din ]\n\toutdata = calc_conv(inlist, model_var[0], model_var[1], applyActFnc=True)\n\t# hidden layer 1 (7 node)\n\toutdata = calc_conv(outdata, model_var[2], model_var[3], applyActFnc=True)\n\t# hidden layer 2 (7 node)\n\toutdata = calc_conv(outdata, model_var[4], model_var[5], applyActFnc=True)\n\t# output layer (1 node)\n\toutdata = calc_conv(outdata, model_var[6], model_var[7], applyActFnc=False)\n\tdout = outdata[0] # ouput is 1 node\n\tprint '%.3f, %.3f' % (din,dout)\n\n```\n\n```txt:\u5b9f\u884c\n$ python reproduce_sine.py  > res.reprod_sine\n```\n\n### Jupyter\u8868\u793a\n\n\u4ee5\u4e0b\u3092\u6bd4\u8f03\u3057\u305f\u3002\n\n- TensorFlow\u306eprediction\u306e\u7d50\u679c\n- model_variables.npy\u304b\u3089\u306eweight,bias\u3092\u4f7f\u3063\u3066\u8a08\u7b97\n\n```py\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('res.161210_1958.cut', delimiter=',')\ninp1 = data1[:,0]\nout1 = data1[:,1]\ndata2 = np.loadtxt('res.reprod_sine', delimiter=',')\ninp2 = data2[:,0]\nout2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\n\nax1.scatter(inp1, out1, label='TensorFlow prediction')\nax2.scatter(inp2, out2, label='from model_var.npy')\n\n#ax1.set_title('First line plot')\nax1.set_xlabel('x')\nax1.set_ylabel('sine(x) prediction')\nax1.grid(True)\nax1.legend()\nax1.set_xlim([0,1.0])\n\nax2.set_xlabel('x')\nax2.set_ylabel('sine(x) reproduced')\nax2.grid(True)\nax2.legend()\nax2.set_xlim([0,1.0])\n\nfig.show()\n```\n\nhttps://www.quora.com/In-the-movie-Aladdin-what-were-the-three-wishes\n> When he (Alladin) first rubs the lamp. The Genie appears,\n\n\u30a4\u30c7\u3088sine curve\u3002\n\n![qiita.png](https://qiita-image-store.s3.amazonaws.com/0/32870/780f0f76-7093-316f-1aec-59ab9f9941d0.png)\n\n\u3082\u3046\u3061\u3087\u3063\u3068\u3002\n\n", "tags": ["borgWarp", "Python"]}