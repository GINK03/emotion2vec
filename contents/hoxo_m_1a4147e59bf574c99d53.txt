{"context": " More than 1 year has passed since last update.\nSpark 1.5.0 \u3067\u30d0\u30b0\u4fee\u6b63\u3055\u308c\u305f\u3088\u3046\u3067\u3059\u30022015/09/09\n\n\u4eca\u307e\u3067\u30d5\u30a1\u30a4\u30eb\u3067\u6271\u3063\u3066\u3044\u305f\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u3059\u3079\u3066 Parquet \u306b\u3057\u3066 SparkR \u3067\u96c6\u8a08\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u3057\u304b\u3057\u3001DataFrame \u306b java.sql.Timestamp \u7531\u6765\u306e\u30ab\u30e9\u30e0\u304c\u4e00\u3064\u3067\u3082\u3042\u308b\u3068\u3001SparkR \u304c\u30a8\u30e9\u30fc\u3092\u51fa\u3057\u307e\u3059\u3002\n\u3061\u3087\u3063\u3068\u518d\u73fe\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u306f\u3001Scala \u3067 java.sql.Timestamp \u306e\u30ab\u30e9\u30e0\u3060\u3051\u304b\u3089\u306a\u308b DataFrame \u3092\u4f5c\u308a\u3001Parquet \u3067\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\nspark-shell\n$ spark-shell\nscala> import java.sql.Timestamp\nscala> case class TimestampTest(timestamp: Timestamp)\nscala> import sqlContext.implicits._\nscala> val rdd = sc.parallelize(1 to 3).map(i => TimestampTest(Timestamp.valueOf(\"2015-01-01 00:00:0\" + i)))\nscala> val dataframe = rdd.toDF\nscala> dataframe.write.parquet(\"timestamp-test.parquet\")\n\n\n\u3053\u308c\u3092 R \u5074\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3067 DataFrame \u306b\u623b\u3057\u307e\u3059\u3002\n\nR\nlibrary(SparkR)\n\nsc <- sparkR.init(master=\"local\")\nsqlContext <- sparkRSQL.init(sc)\n\ndf <- parquetFile(sqlContext, \"timestamp-test.parquet\")\nprint(df)\n\n\n\n\u7d50\u679c\nDataFrame[timestamp:timestamp]\n\n\n\u3061\u3083\u3093\u3068 DataFrame \u3068\u3057\u3066\u8aad\u307f\u8fbc\u3081\u305f\u3088\u3046\u3067\u3059\u3002\n\u3057\u304b\u3057\u3001\u3053\u306e DataFrame \u306b\u5bfe\u3057\u3066 head() \u3092\u3057\u3066\u307f\u308b\u3068\u3001\n\nR\nhead(df)\n\n\nError in as.data.frame.default(x[[i]], optional = TRUE) : \n  cannot coerce class \"\"jobj\"\" to a data.frame\n\n\u30a8\u30e9\u30fc\u304c\u51fa\u307e\u3059\u3002\n\u539f\u56e0\u306f\u3001SparkR \u3067\u306f timestamp \u578b\u3068\u3057\u3066 java.sql.Time \u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304a\u308a\u3001java.sql.Timestamp \u306f Java \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u305d\u306e\u307e\u307e\u4fdd\u6301\u3055\u308c\u3066\u3057\u307e\u3046\u305f\u3081\u306e\u3088\u3046\u3067\u3059\u3002\n\u3057\u304b\u3057\u3001Parquet \u306f timestamp \u578b\u3068\u3057\u3066 java.sql.Timestamp \u3057\u304b\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304a\u3089\u305a\u3001java.sql.Time \u3067\u306f\u4fdd\u5b58\u3067\u304d\u307e\u305b\u3093\u3002\n\u3053\u308c\u306f\u56f0\u308a\u307e\u3057\u305f\u3002\n\u3057\u3087\u3046\u304c\u306a\u3044\u306e\u3067\u3001SparkR \u3092\u9b54\u6539\u9020\u3057\u3066 collect() \u95a2\u6570\u306e\u4e2d\u8eab\u3092\u3001java.sql.Timestamp \u3092\u898b\u3064\u3051\u305f\u3089 POSIXct \u3078\u5909\u63db\u3059\u308b\u3088\u3046\u306b\u66f8\u304d\u63db\u3048\u3066\u3057\u307e\u3044\u307e\u3057\u3087\u3046\u3002\n\u3053\u308c\u306b\u3088\u3063\u3066\u3001head() \u3084 take() \u306a\u3069\u306e\u95a2\u9023\u3059\u308b\u95a2\u6570\u306e\u6319\u52d5\u3082\u5909\u308f\u308a\u307e\u3059\u3002\n\nR\n# Override SparkR::collect --------------------------------------------------------  \ncollect <- function(x, stringsAsFactors = FALSE) {\n  # listCols is a list of raw vectors, one per column\n  listCols <- SparkR:::callJStatic(\"org.apache.spark.sql.api.r.SQLUtils\", \"dfToCols\", x@sdf)\n  cols <- lapply(listCols, function(col) {\n    objRaw <- rawConnection(col)\n    numRows <- SparkR:::readInt(objRaw)\n    col <- SparkR:::readCol(objRaw, numRows)\n    close(objRaw)\n    ### begin added area to read Timestamp ###\n    if(is.list(col) && length(col) > 0) {\n      obj <- col[[1]]\n      class <- SparkR:::callJMethod(obj, \"getClass\")\n      class_name <- SparkR:::callJMethod(class, \"getName\")\n      if(class_name == \"java.sql.Timestamp\") {\n        times <- lapply(col, function(x) {\n          SparkR:::callJMethod(x, \"getTime\")\n        })\n        times <- unlist(times, use.names = FALSE) / 1000\n        col <- as.POSIXct(times / 1000, origin = \"1970-01-01\")\n      }\n    }\n    ### end added area ###\n    col\n  })\n  names(cols) <- columns(x)\n  do.call(cbind.data.frame, list(cols, stringsAsFactors = stringsAsFactors))\n}\nSparkR_env <- asNamespace(\"SparkR\")\nenvironment(collect) <- SparkR_env\ninvisible(eval(setMethod(\"collect\", signature(x = \"DataFrame\"), collect), SparkR_env))\n\n\n\u305d\u308c\u3067\u306f\u518d\u3073 head() \u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\nR\nhead(df)\n\n\n\n\u7d50\u679c\n            timestamp\n1 2015-01-01 00:00:01\n2 2015-01-01 00:00:02\n3 2015-01-01 00:00:03\n\n\n\u3067\u304d\u307e\u3057\u305f\uff01\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001\u3053\u306e\u6a5f\u80fd\u3092 SparkRext \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\nSparkRext - SparkR extension for closer to dplyr\n\nEnjoy!\n\n\u53c2\u8003\n\n\u65e2\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u5185\u306eS4\u30e1\u30bd\u30c3\u30c9\u306e\u66f8\u304d\u63db\u3048\n\n> Spark 1.5.0 \u3067\u30d0\u30b0\u4fee\u6b63\u3055\u308c\u305f\u3088\u3046\u3067\u3059\u30022015/09/09\n\n\u4eca\u307e\u3067\u30d5\u30a1\u30a4\u30eb\u3067\u6271\u3063\u3066\u3044\u305f\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u3059\u3079\u3066 Parquet \u306b\u3057\u3066 SparkR \u3067\u96c6\u8a08\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u3057\u304b\u3057\u3001DataFrame \u306b java.sql.Timestamp \u7531\u6765\u306e\u30ab\u30e9\u30e0\u304c\u4e00\u3064\u3067\u3082\u3042\u308b\u3068\u3001SparkR \u304c\u30a8\u30e9\u30fc\u3092\u51fa\u3057\u307e\u3059\u3002\n\n\u3061\u3087\u3063\u3068\u518d\u73fe\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u307e\u305a\u306f\u3001Scala \u3067 java.sql.Timestamp \u306e\u30ab\u30e9\u30e0\u3060\u3051\u304b\u3089\u306a\u308b DataFrame \u3092\u4f5c\u308a\u3001Parquet \u3067\u4fdd\u5b58\u3057\u307e\u3059\u3002\n\n```scala:spark-shell\n$ spark-shell\nscala> import java.sql.Timestamp\nscala> case class TimestampTest(timestamp: Timestamp)\nscala> import sqlContext.implicits._\nscala> val rdd = sc.parallelize(1 to 3).map(i => TimestampTest(Timestamp.valueOf(\"2015-01-01 00:00:0\" + i)))\nscala> val dataframe = rdd.toDF\nscala> dataframe.write.parquet(\"timestamp-test.parquet\")\n```\n\n\u3053\u308c\u3092 R \u5074\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3067 DataFrame \u306b\u623b\u3057\u307e\u3059\u3002\n\n```r:R\nlibrary(SparkR)\n\nsc <- sparkR.init(master=\"local\")\nsqlContext <- sparkRSQL.init(sc)\n\ndf <- parquetFile(sqlContext, \"timestamp-test.parquet\")\nprint(df)\n```\n\n```:\u7d50\u679c\nDataFrame[timestamp:timestamp]\n```\n\n\u3061\u3083\u3093\u3068 DataFrame \u3068\u3057\u3066\u8aad\u307f\u8fbc\u3081\u305f\u3088\u3046\u3067\u3059\u3002\n\n\u3057\u304b\u3057\u3001\u3053\u306e DataFrame \u306b\u5bfe\u3057\u3066 `head()` \u3092\u3057\u3066\u307f\u308b\u3068\u3001\n\n```r:R\nhead(df)\n```\n\n```\nError in as.data.frame.default(x[[i]], optional = TRUE) : \n  cannot coerce class \"\"jobj\"\" to a data.frame\n```\n\n\u30a8\u30e9\u30fc\u304c\u51fa\u307e\u3059\u3002\n\n\u539f\u56e0\u306f\u3001SparkR \u3067\u306f timestamp \u578b\u3068\u3057\u3066 java.sql.Time \u306e\u307f\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304a\u308a\u3001java.sql.Timestamp \u306f Java \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u305d\u306e\u307e\u307e\u4fdd\u6301\u3055\u308c\u3066\u3057\u307e\u3046\u305f\u3081\u306e\u3088\u3046\u3067\u3059\u3002\n\u3057\u304b\u3057\u3001Parquet \u306f timestamp \u578b\u3068\u3057\u3066 java.sql.Timestamp \u3057\u304b\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u304a\u3089\u305a\u3001java.sql.Time \u3067\u306f\u4fdd\u5b58\u3067\u304d\u307e\u305b\u3093\u3002\n\n\u3053\u308c\u306f\u56f0\u308a\u307e\u3057\u305f\u3002\n\n\u3057\u3087\u3046\u304c\u306a\u3044\u306e\u3067\u3001SparkR \u3092\u9b54\u6539\u9020\u3057\u3066 `collect()` \u95a2\u6570\u306e\u4e2d\u8eab\u3092\u3001java.sql.Timestamp \u3092\u898b\u3064\u3051\u305f\u3089 POSIXct \u3078\u5909\u63db\u3059\u308b\u3088\u3046\u306b\u66f8\u304d\u63db\u3048\u3066\u3057\u307e\u3044\u307e\u3057\u3087\u3046\u3002\n\u3053\u308c\u306b\u3088\u3063\u3066\u3001`head()` \u3084 `take()` \u306a\u3069\u306e\u95a2\u9023\u3059\u308b\u95a2\u6570\u306e\u6319\u52d5\u3082\u5909\u308f\u308a\u307e\u3059\u3002\n\n```r:R\n# Override SparkR::collect --------------------------------------------------------  \ncollect <- function(x, stringsAsFactors = FALSE) {\n  # listCols is a list of raw vectors, one per column\n  listCols <- SparkR:::callJStatic(\"org.apache.spark.sql.api.r.SQLUtils\", \"dfToCols\", x@sdf)\n  cols <- lapply(listCols, function(col) {\n    objRaw <- rawConnection(col)\n    numRows <- SparkR:::readInt(objRaw)\n    col <- SparkR:::readCol(objRaw, numRows)\n    close(objRaw)\n    ### begin added area to read Timestamp ###\n    if(is.list(col) && length(col) > 0) {\n      obj <- col[[1]]\n      class <- SparkR:::callJMethod(obj, \"getClass\")\n      class_name <- SparkR:::callJMethod(class, \"getName\")\n      if(class_name == \"java.sql.Timestamp\") {\n        times <- lapply(col, function(x) {\n          SparkR:::callJMethod(x, \"getTime\")\n        })\n        times <- unlist(times, use.names = FALSE) / 1000\n        col <- as.POSIXct(times / 1000, origin = \"1970-01-01\")\n      }\n    }\n    ### end added area ###\n    col\n  })\n  names(cols) <- columns(x)\n  do.call(cbind.data.frame, list(cols, stringsAsFactors = stringsAsFactors))\n}\nSparkR_env <- asNamespace(\"SparkR\")\nenvironment(collect) <- SparkR_env\ninvisible(eval(setMethod(\"collect\", signature(x = \"DataFrame\"), collect), SparkR_env))\n```\n\n\u305d\u308c\u3067\u306f\u518d\u3073 `head()` \u3092\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n```r:R\nhead(df)\n```\n\n```:\u7d50\u679c\n            timestamp\n1 2015-01-01 00:00:01\n2 2015-01-01 00:00:02\n3 2015-01-01 00:00:03\n```\n\n\u3067\u304d\u307e\u3057\u305f\uff01\n\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001\u3053\u306e\u6a5f\u80fd\u3092 SparkRext \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u8ffd\u52a0\u3057\u307e\u3057\u305f\u3002\n\n- [SparkRext - SparkR extension for closer to dplyr](https://github.com/hoxo-m/SparkRext)\n\nEnjoy!\n\n## \u53c2\u8003\n\n- [\u65e2\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u5185\u306eS4\u30e1\u30bd\u30c3\u30c9\u306e\u66f8\u304d\u63db\u3048](http://qiita.com/kohske/items/f406f66f75fb972a32ea)\n\n", "tags": ["R", "Spark"]}