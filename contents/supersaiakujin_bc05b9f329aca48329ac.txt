{"context": " More than 1 year has passed since last update.\u3000Google\u304cDeep Learning\u7528\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30afTensorFlow\u3092\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u5316\u3057\u3066\u516c\u958b\u3057\u307e\u3057\u305f\u306d(2015/11/9)\u3002\u3044\u308d\u3093\u306a\u4eba\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u3068\u304b\u3001Tutorial\u3092\u5b9f\u884c\u3057\u3066\u307f\u305f\u3068\u3044\u3046\u8a18\u4e8b\u3084\u30d6\u30ed\u30b0\u3092\u3059\u3067\u306b\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002\u51fa\u9045\u308c\u305f\u611f\u306f\u3042\u308a\u307e\u3059\u304c\u305b\u3063\u304b\u304f\u306a\u306e\u3067TensorFlow\u3092\u3044\u308d\u3044\u308d\u3044\u3058\u3063\u3066\u307f\u307e\u3057\u305f\u3002\u305f\u3060Tutorial\u3092\u5b9f\u884c\u3059\u308b\u3060\u3051\u3067\u306f\u9762\u767d\u304f\u306a\u3044\u306e\u3067\u540c\u3058\u3082\u306e\u3092Chainer\u3067\u3082\u66f8\u3044\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002Chainer\u3082\u307e\u3060\u59cb\u3081\u305f\u3070\u304b\u308a\u306a\u306e\u3067\u5168\u304f\u540c\u3058\u3067\u306f\u306a\u3044\u3067\u3059\u304c\u305d\u306e\u3078\u3093\u306f\u3054\u52d8\u5f01\u304f\u3060\u3055\u3044\u3002\n\u3000\u306a\u305cChainer\u306a\u306e\u304b\u3068\u3044\u3046\u3068\u3001Caffe\u3084Theano\u304c\u9762\u5012\u304f\u3055\u305d\u3046\u306b\u898b\u3048\u305f\u306e\u306b\u5bfe\u3057\u3001Chainer\u306f\u30b3\u30fc\u30c9\u304c\u30b9\u30de\u30fc\u30c8\u306b\u611f\u3058\u305f\u304b\u3089\u3067\u3059\u3002\u500b\u4eba\u7684\u306a\u611f\u60f3\u3067\u3059\u304c\u30fb\u30fb\u30fb\u3002\u3042\u3068Chainer\u3067\u306fRNN\u304c\u66f8\u3051\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u307b\u304b\u304c\u3067\u304d\u306a\u3044\u3082\u306e\u304c\u3067\u304d\u308b\u3068\u8a00\u308f\u308c\u308b\u3068\u306a\u3093\u3060\u304b\u826f\u3055\u305d\u3046\u306b\u611f\u3058\u3067\u3057\u307e\u3046\u306e\u306f\u4eba\u9593\u306e\u6027\u3067\u3057\u3087\u3046\u304b\u3002\n\u3000\u4eca\u56de\u306fTensorFlow\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u95a2\u3057\u3066\u306f\u66f8\u304d\u307e\u305b\u3093\u3002\u7406\u7531\u306f\u306a\u3093\u304b\u8272\u3005\u8a66\u884c\u932f\u8aa4\u3057\u305f\u306e\u3067\u3059\u304c\u3059\u3063\u304b\u308a\u5fd8\u308c\u3066\u3057\u307e\u3063\u305f\u304b\u3089\u3067\u3059\u3002\u3059\u3044\u307e\u305b\u3093\u3002\n\nDeep MNIST for Experts\n\u3000\u4eca\u56de\u8a66\u3057\u305f\u306e\u306f\u300cDeep MNIST for Experts\u300d\u3067\u3059\u3002\u8a73\u7d30\u306fTensorFlow\u306eTutorial\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e0b\u306e\u65b9\u306b\u30b3\u30fc\u30c9\u3092\u8f09\u305b\u3066\u3044\u307e\u3059\u304c\u3001\u3061\u3087\u3063\u3068\u3060\u3051\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30fc\u30c9\u304b\u3089\u5909\u66f4\u3057\u307e\u3057\u305f\u3002GPU\u7248\u306eTensorFlow\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u81ea\u5206\u306e\u74b0\u5883\u304cGeForce GTX970\u3060\u3063\u305f\u306e\u3067\u3001\u5b66\u7fd2\u5f8c\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u7cbe\u5ea6\u3092\u6e2c\u308b\u3068\u3053\u308d\u3067GPU\u306e\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u304f\u3066Out of Memory\u3067\u4f8b\u5916\u3092\u5410\u3044\u3066\u3046\u307e\u304f\u52d5\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u3044\u3063\u307a\u3093\u306b\u4f7f\u304a\u3046\u3068\u3057\u305f\u304b\u3089\u306e\u3088\u3046\u3067\u3059\u3002\u591a\u5206Google\u306fTITAN X\u3068\u304bTesla\u3068\u304b\u4f7f\u3063\u3066\u308b\u304b\u3089\u6c17\u306b\u3082\u3057\u306a\u304b\u3063\u305f\u306e\u3067\u3057\u3087\u3046\u3002\n\nprint \"test accuracy %g\"%accuracy.eval(feed_dict={\n   x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n\n\u3057\u304b\u305f\u306a\u3044\u306e\u3067\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u7cbe\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\n\ntacc = 0\ntbatchsize = 1000\nfor i in range(0,N_test,tbatchsize):\n   acc = accuracy.eval(feed_dict={\n       x: mnist.test.images[i:i+tbatchsize], y_: mnist.test.labels[i:i+tbatchsize], keep_prob: 1.0})\n   print \"test accuracy %d = %g\" % (i//tbatchsize, acc)\n   tacc += acc * tbatchsize\ntacc /= N_test\nprint \"test accuracy = %g\" % tacc    \n\n\nDeep MNIST for Experts\u306e\u69cb\u9020\n\u3000TensorFlow\u306etutorial\u306eDeep MNIST for Experts\u306e\u69cb\u9020\u306f\u3044\u308f\u3086\u308bConvolutional Neural Network\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002CNN\u306e\u8aac\u660e\u306f\u4e16\u306e\u4e2d\u306b\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3053\u3053\u3067\u306f\u3057\u306a\u3044\u3067\u3059\u304c\u3001\u4eca\u56de\u306eTutorial\u306eCNN\u306e\u69cb\u9020\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059(Fig.1)\u3002\u5148\u982d\u304cchannel\u6570\u3067\u7d9a\u3044\u3066Map\u306e\u6a2a\uff58\u7e26\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002convolution\u306b\u3088\u3063\u3066channel\u6570\u304c\u5909\u5316\u3057\u3001pooling\u306b\u3088\u308aMap\u306e\u30b5\u30a4\u30ba\u304c\u5909\u5316\u3057\u307e\u3059\u3002\u305f\u3060\u3057convolution\u306e\u3068\u3053\u308d\u3067padding\u3092\u5165\u308c\u306a\u3044\u3068Map\u306e\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002padding\u306fkernel size\u306e\u534a\u5206\u3092\u5165\u308c\u3066\u3042\u3052\u308c\u3070\u3044\u3044\u3067\u3059\u3002\n1x28x28 -> 32x28x28 -> 32x14x14 -> 64x14x14 -> 64x7x7 -> 1024 -> 10\nFig.1\n\u4ee5\u4e0b\u304cTensorFlow\u3067Network\u304c\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u3068\u3053\u308d\u3067\u3059\u3002\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1,28,28,1])    \nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n....\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n\u3053\u308c\u3092chainer\u3067\u66f8\u3044\u305f\u306e\u304c\u4ee5\u4e0b\u3067\u3059\u3002\n\nmodel = chainer.FunctionSet(conv1=F.Convolution2D(1,32,5,pad=2),\n                           conv2=F.Convolution2D(32,64,5,pad=2),\n                           fl3=F.Linear(7*7*64,1024),\n                           fl4=F.Linear(1024,10))\n...\ndef forward(x_data, y_data, train=True):\n   x, t = chainer.Variable(x_data), chainer.Variable(y_data)\n   h_conv1 = F.relu(model.conv1(x))\n   h_pool1 = F.max_pooling_2d(h_conv1, 2)\nh_conv2 = F.relu(model.conv2(h_pool1))\n   h_pool2 = F.max_pooling_2d(h_conv2, 2)\nh_fc1 = F.relu(model.fl3(h_pool2))\n   h_fc1_drop = F.dropout(h_fc1, ratio=0.5,train=train)\ny = model.fl4(h_fc1_drop)\nreturn F.softmax_cross_entropy(y, t), F.accuracy(y, t) \n\n\u53b3\u5bc6\u306b\u306f\u4ee5\u4e0b\u306e\u90e8\u5206\u304c\u540c\u3058\u3067\u306f\u306a\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3067\u3059\u3002\n\nTensorFlow\u3067\u306f\u91cd\u307fW\u3068\u30d0\u30a4\u30a2\u30b9b\u3092random\u306a\u5024\u3068\u4e00\u5b9a\u5024\u3067\u521d\u671f\u5316\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\ntruncated_normal\u306fTensorFlow\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3088\u308b\u3068\u5e73\u5747\u304b\u3089\u6a19\u6e96\u504f\u5dee\u306e\uff12\u500d\u4ee5\u4e0a\u96e2\u308c\u305f\u3068\u3053\u308d\u306e\u983b\u5ea6\u304c\uff10\u3068\u306a\u308b\u3088\u3046\u306a\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046Random\u306a\u5024\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\nconstant\u306f\u5b9a\u6570\u306eTensor\u3092\u4f5c\u308b\u95a2\u6570\u3067\u3059\u3002\n\ndef weight_variable(shape):\n   initial = tf.truncated_normal(shape, stddev=0.1)\n   return tf.Variable(initial)\ndef bias_variable(shape):\n   initial = tf.constant(0.1, shape = shape)\n   return tf.Variable(initial)\n\n2. \u3042\u3068TensorFlow\u306epadding='SAME'\u306e\u610f\u5473\u304c\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3068\u308a\u3042\u3048\u305a\u4e00\u65e6\u6c17\u306b\u3057\u306a\u3044\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002 \n\uff08\u8ffd\u8a18\uff1a\u30b3\u30e1\u30f3\u30c8\u306b\u3088\u308a\u60c5\u5831\u3092\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002SAME\u306f\u5165\u529b\u3068\u51fa\u529b\u306e\u30b5\u30a4\u30ba\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u5165\u529b\u306b0 padding\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3057\u305f\u3002\uff09\n\ndef conv2d(x, W):\n   return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\ndef max_pool_2x2(x):\n   return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n\n\u5b9f\u884c\u7d50\u679c\n\u4ee5\u4e0b\u306f\u7cbe\u5ea6\u3068\u5b9f\u884c\u6642\u9593\u3092\u6bd4\u8f03\u3057\u305f\u7d50\u679c\u3067\u3059\u3002\u5b9f\u884c\u6642\u9593\u306fTensorFlow\u3068Chainer\u4ee5\u5916\u306e\u90e8\u5206\u304c\u5168\u304f\u4e00\u7dd2\u3067\u306f\u306a\u3044\u305f\u3081\u53b3\u5bc6\u306a\u6bd4\u8f03\u306b\u306f\u306a\u3063\u3066\u3044\u306a\u3044\u3067\u3059\u3002\u30c7\u30fc\u30bf\u3092\u6e21\u3059\u3068\u304d\u306e\u5f62\u5f0f\u304c\u7570\u306a\u308b\u306e\u3067Chainer\u306e\u65b9\u306e\u30b3\u30fc\u30c9\u3067\u306f\u4f59\u8a08\u306a\u51e6\u7406\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u3002\n\u3000\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u4e0b\u306e\u8868\u306b\u307e\u3068\u3081\u307e\u3057\u305f\u3002TensorFlow,Chainer\u3068\u3082\u7cbe\u5ea6\u306f\u540c\u3058\u3067\u3057\u305f\u3002\u305d\u3082\u305d\u3082MNIST\u306f\u554f\u984c\u304c\u7c21\u5358\u306a\u306e\u3067Deep Learning\u3067\u306a\u304f\u3066\u3082\u9ad8\u3044\u7cbe\u5ea6\u304c\u51fa\u308b\u305d\u3046\u3067\u3059\u3002\u5b9f\u884c\u6642\u9593\u306fChainer\u306e\u65b9\u304c\u901f\u304b\u3063\u305f\u306e\u3067\u3059\u3002TensorFlow\u306e\u65b9\u304c\u901f\u3044\u304b\u306a\u3068\u601d\u3063\u3066\u3044\u305f\u306e\u306b\u3061\u3087\u3063\u3068\u610f\u5916\u3067\u3057\u305f\u3002\u3069\u3053\u304b\u3067TensorFlow\u306b\u306f\u3042\u308b\u51e6\u7406\u304cChainer\u306b\u306f\u7121\u3044\u3068\u304b\u3044\u3046\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u3059\u3002\u3053\u3093\u306a\u3082\u3093\u304b\u306a\u7a0b\u5ea6\u306b\u7559\u3081\u3066\u304a\u3044\u3066\u3082\u3089\u3048\u308b\u3068\u3044\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u3002\n\n\n\n\n\u7cbe\u5ea6\n\u6642\u9593\n\n\n\n\nTensorFlow\n99.2%\n249sec\n\n\nChainer\n99.2%\n213sec\n\n\n\n\u5b9f\u884c\u74b0\u5883\n\u30fbOS Linux Ubuntu 14.04 LTE\n\u30fbCPU Core i5-4460 3.2GHz\n\u30fbMemory 16GB\n\u30fbGeForce GTX 970\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\ntensorflow_mnist.py\n# -*- coding: utf-8 -*-\nimport input_data\nprint 'load MNIST dataset'\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nimport time\n\nx = tf.placeholder(\"float\", [None, 784])\ny_ = tf.placeholder(\"float\", [None, 10])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# normal distribution\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape = shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n# 1x28x28 -> 32x28x28 -> 32x14x14\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1,28,28,1])    \n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# 32x14x14 -> 64x7x7\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# 64x7x7 -> 1024\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# Readout\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\nsess.run(tf.initialize_all_variables())\n\n# training\nN = len(mnist.train.images)\nN_test = len(mnist.test.images)\nn_epoch = 20000\nbatchsize = 50\nstart_time = time.clock()\nfor i in range(n_epoch):\n  batch = mnist.train.next_batch(batchsize)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print \"step %d, training accuracy %g\"%(i, train_accuracy)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\ntacc = 0\ntbatchsize = 1000\nfor i in range(0,N_test,tbatchsize):\n    acc = accuracy.eval(feed_dict={\n        x: mnist.test.images[i:i+tbatchsize], y_: mnist.test.labels[i:i+tbatchsize], keep_prob: 1.0})\n    print \"test accuracy %d = %g\" % (i//tbatchsize, acc)\n    tacc += acc * tbatchsize\ntacc /= N_test\nprint \"test accuracy = %g\" % tacc    \n\nend_time = time.clock()\nprint end_time - start_time\n\n\n\nchainer_mnist.py\n# -*- coding: utf-8 -*-\nfrom chainer import cuda\nfrom chainer import optimizers\nimport chainer\nimport chainer.functions as F\nimport input_data\nimport numpy as np\nimport time\n\ngpu_flag = 0\n\nif gpu_flag >= 0:\n    cuda.check_cuda_available()\n\nxp = cuda.cupy if gpu_flag >= 0 else np\n\nprint 'load MNIST dataset'\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\nmodel = chainer.FunctionSet(conv1=F.Convolution2D(1,32,5,pad=2),\n                            conv2=F.Convolution2D(32,64,5,pad=2),\n                            fl3=F.Linear(7*7*64,1024),\n                            fl4=F.Linear(1024,10))\n\nif gpu_flag >= 0:\n    cuda.get_device(gpu_flag).use()\n    model.to_gpu()   \n\ndef forward(x_data, y_data, train=True):\n    x, t = chainer.Variable(x_data), chainer.Variable(y_data)\n    h_conv1 = F.relu(model.conv1(x))\n    h_pool1 = F.max_pooling_2d(h_conv1, 2)\n\n    h_conv2 = F.relu(model.conv2(h_pool1))\n    h_pool2 = F.max_pooling_2d(h_conv2, 2)\n\n    h_fc1 = F.relu(model.fl3(h_pool2))\n    h_fc1_drop = F.dropout(h_fc1, ratio=0.5,train=train)\n\n    y = model.fl4(h_fc1_drop)\n\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)                            \n\n\n# setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\nn_epoch = 20000\nbatchsize=50\n\nN = len(mnist.train.images)\nN_test = len(mnist.test.images)\n\nstart_time = time.clock()\nfor i in range(n_epoch):        \n    x_batch, y_batch = mnist.train.next_batch(batchsize)\n    x_batch = x_batch.astype(np.float32)\n    x_batch = xp.asarray(x_batch.reshape(batchsize,1,28,28))\n    y_batch = xp.asarray(np.argmax(y_batch, axis=1).astype(np.int32))\n\n    if i%100 == 0:\n        loss, acc = forward(x_batch, y_batch, train=False)\n        print 'step {0} training accuracy {1}'.format(i, float(acc.data))\n\n    optimizer.zero_grads()\n    loss, acc = forward(x_batch, y_batch)\n    loss.backward()\n    optimizer.update()\n\nsum_accuracy = 0\nsum_loss = 0\ntbatchsize=1000\nfor i in range(0, N_test, tbatchsize):\n    x_batch = mnist.test.images[i:i+tbatchsize].astype(np.float32)\n    y_batch = mnist.test.labels[i:i+tbatchsize]\n    x_batch = xp.asarray(x_batch.reshape(tbatchsize,1,28,28))\n    y_batch = xp.asarray(np.argmax(y_batch, axis=1).astype(np.int32))\n\n\n    loss, acc = forward(x_batch, y_batch, train=False)\n\n    sum_loss += float(loss.data)*tbatchsize\n    sum_accuracy += float(acc.data)*tbatchsize\n    print \"test accuracy %d = %g\" % (i//tbatchsize, float(acc.data))\n\nprint 'test accuracy = {0}'.format(sum_accuracy/N_test)\n\nend_time = time.clock()\nprint end_time - start_time\n\n\n\n\u3000Google\u304cDeep Learning\u7528\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af[TensorFlow](http://tensorflow.org/)\u3092\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u5316\u3057\u3066\u516c\u958b\u3057\u307e\u3057\u305f\u306d(2015/11/9)\u3002\u3044\u308d\u3093\u306a\u4eba\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u305f\u3068\u304b\u3001Tutorial\u3092\u5b9f\u884c\u3057\u3066\u307f\u305f\u3068\u3044\u3046\u8a18\u4e8b\u3084\u30d6\u30ed\u30b0\u3092\u3059\u3067\u306b\u66f8\u304b\u308c\u3066\u3044\u307e\u3059\u3002\u51fa\u9045\u308c\u305f\u611f\u306f\u3042\u308a\u307e\u3059\u304c\u305b\u3063\u304b\u304f\u306a\u306e\u3067TensorFlow\u3092\u3044\u308d\u3044\u308d\u3044\u3058\u3063\u3066\u307f\u307e\u3057\u305f\u3002\u305f\u3060Tutorial\u3092\u5b9f\u884c\u3059\u308b\u3060\u3051\u3067\u306f\u9762\u767d\u304f\u306a\u3044\u306e\u3067\u540c\u3058\u3082\u306e\u3092[Chainer](http://chainer.org/)\u3067\u3082\u66f8\u3044\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002Chainer\u3082\u307e\u3060\u59cb\u3081\u305f\u3070\u304b\u308a\u306a\u306e\u3067\u5168\u304f\u540c\u3058\u3067\u306f\u306a\u3044\u3067\u3059\u304c\u305d\u306e\u3078\u3093\u306f\u3054\u52d8\u5f01\u304f\u3060\u3055\u3044\u3002\n\u3000\u306a\u305cChainer\u306a\u306e\u304b\u3068\u3044\u3046\u3068\u3001Caffe\u3084Theano\u304c\u9762\u5012\u304f\u3055\u305d\u3046\u306b\u898b\u3048\u305f\u306e\u306b\u5bfe\u3057\u3001Chainer\u306f\u30b3\u30fc\u30c9\u304c\u30b9\u30de\u30fc\u30c8\u306b\u611f\u3058\u305f\u304b\u3089\u3067\u3059\u3002\u500b\u4eba\u7684\u306a\u611f\u60f3\u3067\u3059\u304c\u30fb\u30fb\u30fb\u3002\u3042\u3068Chainer\u3067\u306fRNN\u304c\u66f8\u3051\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u307b\u304b\u304c\u3067\u304d\u306a\u3044\u3082\u306e\u304c\u3067\u304d\u308b\u3068\u8a00\u308f\u308c\u308b\u3068\u306a\u3093\u3060\u304b\u826f\u3055\u305d\u3046\u306b\u611f\u3058\u3067\u3057\u307e\u3046\u306e\u306f\u4eba\u9593\u306e\u6027\u3067\u3057\u3087\u3046\u304b\u3002\n\u3000\u4eca\u56de\u306fTensorFlow\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306b\u95a2\u3057\u3066\u306f\u66f8\u304d\u307e\u305b\u3093\u3002\u7406\u7531\u306f\u306a\u3093\u304b\u8272\u3005\u8a66\u884c\u932f\u8aa4\u3057\u305f\u306e\u3067\u3059\u304c\u3059\u3063\u304b\u308a\u5fd8\u308c\u3066\u3057\u307e\u3063\u305f\u304b\u3089\u3067\u3059\u3002\u3059\u3044\u307e\u305b\u3093\u3002\n\n## Deep MNIST for Experts\n\n\u3000\u4eca\u56de\u8a66\u3057\u305f\u306e\u306f\u300cDeep MNIST for Experts\u300d\u3067\u3059\u3002\u8a73\u7d30\u306fTensorFlow\u306e[Tutorial](http://tensorflow.org/tutorials/mnist/pros/index.md)\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e0b\u306e\u65b9\u306b\u30b3\u30fc\u30c9\u3092\u8f09\u305b\u3066\u3044\u307e\u3059\u304c\u3001\u3061\u3087\u3063\u3068\u3060\u3051\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30fc\u30c9\u304b\u3089\u5909\u66f4\u3057\u307e\u3057\u305f\u3002GPU\u7248\u306eTensorFlow\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u81ea\u5206\u306e\u74b0\u5883\u304cGeForce GTX970\u3060\u3063\u305f\u306e\u3067\u3001\u5b66\u7fd2\u5f8c\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u7cbe\u5ea6\u3092\u6e2c\u308b\u3068\u3053\u308d\u3067GPU\u306e\u30e1\u30e2\u30ea\u304c\u8db3\u308a\u306a\u304f\u3066Out of Memory\u3067\u4f8b\u5916\u3092\u5410\u3044\u3066\u3046\u307e\u304f\u52d5\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u3044\u3063\u307a\u3093\u306b\u4f7f\u304a\u3046\u3068\u3057\u305f\u304b\u3089\u306e\u3088\u3046\u3067\u3059\u3002\u591a\u5206Google\u306fTITAN X\u3068\u304bTesla\u3068\u304b\u4f7f\u3063\u3066\u308b\u304b\u3089\u6c17\u306b\u3082\u3057\u306a\u304b\u3063\u305f\u306e\u3067\u3057\u3087\u3046\u3002\n\n>print \"test accuracy %g\"%accuracy.eval(feed_dict={\n>    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n\n\u3057\u304b\u305f\u306a\u3044\u306e\u3067\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u7cbe\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\n\n>tacc = 0\n>tbatchsize = 1000\n>for i in range(0,N_test,tbatchsize):\n>    acc = accuracy.eval(feed_dict={\n>        x: mnist.test.images[i:i+tbatchsize], y_: mnist.test.labels[i:i+tbatchsize], keep_prob: 1.0})\n>    print \"test accuracy %d = %g\" % (i//tbatchsize, acc)\n>    tacc += acc * tbatchsize\n>tacc /= N_test\n>print \"test accuracy = %g\" % tacc    \n\n## Deep MNIST for Experts\u306e\u69cb\u9020\n\u3000TensorFlow\u306etutorial\u306eDeep MNIST for Experts\u306e\u69cb\u9020\u306f\u3044\u308f\u3086\u308bConvolutional Neural Network\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002CNN\u306e\u8aac\u660e\u306f\u4e16\u306e\u4e2d\u306b\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3053\u3053\u3067\u306f\u3057\u306a\u3044\u3067\u3059\u304c\u3001\u4eca\u56de\u306eTutorial\u306eCNN\u306e\u69cb\u9020\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u3067\u3059(Fig.1)\u3002\u5148\u982d\u304cchannel\u6570\u3067\u7d9a\u3044\u3066Map\u306e\u6a2a\uff58\u7e26\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002convolution\u306b\u3088\u3063\u3066channel\u6570\u304c\u5909\u5316\u3057\u3001pooling\u306b\u3088\u308aMap\u306e\u30b5\u30a4\u30ba\u304c\u5909\u5316\u3057\u307e\u3059\u3002\u305f\u3060\u3057convolution\u306e\u3068\u3053\u308d\u3067padding\u3092\u5165\u308c\u306a\u3044\u3068Map\u306e\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002padding\u306fkernel size\u306e\u534a\u5206\u3092\u5165\u308c\u3066\u3042\u3052\u308c\u3070\u3044\u3044\u3067\u3059\u3002\n\n1x28x28 -> 32x28x28 -> 32x14x14 -> 64x14x14 -> 64x7x7 -> 1024 -> 10\nFig.1\n\n\u4ee5\u4e0b\u304cTensorFlow\u3067Network\u304c\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u3068\u3053\u308d\u3067\u3059\u3002\n\n>\n>W_conv1 = weight_variable([5, 5, 1, 32])\n>b_conv1 = bias_variable([32])\n>x_image = tf.reshape(x, [-1,28,28,1])    \n>\n>h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n>h_pool1 = max_pool_2x2(h_conv1)\n>\n> ....\n>\n>W_fc2 = weight_variable([1024, 10])\n>b_fc2 = bias_variable([10])\n>\n>y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n\u3053\u308c\u3092chainer\u3067\u66f8\u3044\u305f\u306e\u304c\u4ee5\u4e0b\u3067\u3059\u3002\n>model = chainer.FunctionSet(conv1=F.Convolution2D(1,32,5,pad=2),\n>                            conv2=F.Convolution2D(32,64,5,pad=2),\n>                            fl3=F.Linear(7*7*64,1024),\n>                            fl4=F.Linear(1024,10))\n>\n> ...\n>\n>def forward(x_data, y_data, train=True):\n>    x, t = chainer.Variable(x_data), chainer.Variable(y_data)\n>    h_conv1 = F.relu(model.conv1(x))\n>    h_pool1 = F.max_pooling_2d(h_conv1, 2)\n>    \n>    h_conv2 = F.relu(model.conv2(h_pool1))\n>    h_pool2 = F.max_pooling_2d(h_conv2, 2)\n>\n>    h_fc1 = F.relu(model.fl3(h_pool2))\n>    h_fc1_drop = F.dropout(h_fc1, ratio=0.5,train=train)\n>    \n>    y = model.fl4(h_fc1_drop)\n>\n>    return F.softmax_cross_entropy(y, t), F.accuracy(y, t) \n\n\n\u53b3\u5bc6\u306b\u306f\u4ee5\u4e0b\u306e\u90e8\u5206\u304c\u540c\u3058\u3067\u306f\u306a\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u3067\u3059\u3002\n\n1. TensorFlow\u3067\u306f\u91cd\u307fW\u3068\u30d0\u30a4\u30a2\u30b9b\u3092random\u306a\u5024\u3068\u4e00\u5b9a\u5024\u3067\u521d\u671f\u5316\u3057\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n \n[truncated_normal](http://www.tensorflow.org/api_docs/python/constant_op.md#truncated_normal)\u306fTensorFlow\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3088\u308b\u3068\u5e73\u5747\u304b\u3089\u6a19\u6e96\u504f\u5dee\u306e\uff12\u500d\u4ee5\u4e0a\u96e2\u308c\u305f\u3068\u3053\u308d\u306e\u983b\u5ea6\u304c\uff10\u3068\u306a\u308b\u3088\u3046\u306a\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046Random\u306a\u5024\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002\n[constant](http://www.tensorflow.org/api_docs/python/constant_op.md#constant)\u306f\u5b9a\u6570\u306eTensor\u3092\u4f5c\u308b\u95a2\u6570\u3067\u3059\u3002\n>def weight_variable(shape):\n>    initial = tf.truncated_normal(shape, stddev=0.1)\n>    return tf.Variable(initial)\n>    \n>def bias_variable(shape):\n>    initial = tf.constant(0.1, shape = shape)\n>    return tf.Variable(initial)\n\n ~~2. \u3042\u3068TensorFlow\u306epadding='SAME'\u306e\u610f\u5473\u304c\u308f\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3068\u308a\u3042\u3048\u305a\u4e00\u65e6\u6c17\u306b\u3057\u306a\u3044\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002~~ \n\uff08\u8ffd\u8a18\uff1a\u30b3\u30e1\u30f3\u30c8\u306b\u3088\u308a\u60c5\u5831\u3092\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002SAME\u306f\u5165\u529b\u3068\u51fa\u529b\u306e\u30b5\u30a4\u30ba\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u5165\u529b\u306b0 padding\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3067\u3057\u305f\u3002\uff09\n   \n>def conv2d(x, W):\n>    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n>def max_pool_2x2(x):\n>    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n## \u5b9f\u884c\u7d50\u679c\n \u4ee5\u4e0b\u306f\u7cbe\u5ea6\u3068\u5b9f\u884c\u6642\u9593\u3092\u6bd4\u8f03\u3057\u305f\u7d50\u679c\u3067\u3059\u3002\u5b9f\u884c\u6642\u9593\u306fTensorFlow\u3068Chainer\u4ee5\u5916\u306e\u90e8\u5206\u304c\u5168\u304f\u4e00\u7dd2\u3067\u306f\u306a\u3044\u305f\u3081\u53b3\u5bc6\u306a\u6bd4\u8f03\u306b\u306f\u306a\u3063\u3066\u3044\u306a\u3044\u3067\u3059\u3002\u30c7\u30fc\u30bf\u3092\u6e21\u3059\u3068\u304d\u306e\u5f62\u5f0f\u304c\u7570\u306a\u308b\u306e\u3067Chainer\u306e\u65b9\u306e\u30b3\u30fc\u30c9\u3067\u306f\u4f59\u8a08\u306a\u51e6\u7406\u304c\u5165\u3063\u3066\u3044\u307e\u3059\u3002\n\u3000\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u4e0b\u306e\u8868\u306b\u307e\u3068\u3081\u307e\u3057\u305f\u3002TensorFlow,Chainer\u3068\u3082\u7cbe\u5ea6\u306f\u540c\u3058\u3067\u3057\u305f\u3002\u305d\u3082\u305d\u3082MNIST\u306f\u554f\u984c\u304c\u7c21\u5358\u306a\u306e\u3067Deep Learning\u3067\u306a\u304f\u3066\u3082\u9ad8\u3044\u7cbe\u5ea6\u304c\u51fa\u308b\u305d\u3046\u3067\u3059\u3002\u5b9f\u884c\u6642\u9593\u306fChainer\u306e\u65b9\u304c\u901f\u304b\u3063\u305f\u306e\u3067\u3059\u3002TensorFlow\u306e\u65b9\u304c\u901f\u3044\u304b\u306a\u3068\u601d\u3063\u3066\u3044\u305f\u306e\u306b\u3061\u3087\u3063\u3068\u610f\u5916\u3067\u3057\u305f\u3002\u3069\u3053\u304b\u3067TensorFlow\u306b\u306f\u3042\u308b\u51e6\u7406\u304cChainer\u306b\u306f\u7121\u3044\u3068\u304b\u3044\u3046\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u3059\u3002\u3053\u3093\u306a\u3082\u3093\u304b\u306a\u7a0b\u5ea6\u306b\u7559\u3081\u3066\u304a\u3044\u3066\u3082\u3089\u3048\u308b\u3068\u3044\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\u3002\n\n\n|            |  \u7cbe\u5ea6 |  \u6642\u9593  |\n|------------|:-----:|:------:|\n| TensorFlow | 99.2% | 249sec |\n|   Chainer  | 99.2% | 213sec |\n\n\u5b9f\u884c\u74b0\u5883\n\u30fbOS Linux Ubuntu 14.04 LTE\n\u30fbCPU Core i5-4460 3.2GHz\n\u30fbMemory 16GB\n\u30fbGeForce GTX 970\n\n## \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n\n```lang:tensorflow_mnist.py\n# -*- coding: utf-8 -*-\nimport input_data\nprint 'load MNIST dataset'\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nimport time\n\nx = tf.placeholder(\"float\", [None, 784])\ny_ = tf.placeholder(\"float\", [None, 10])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# normal distribution\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n    \ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape = shape)\n    return tf.Variable(initial)\n    \ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n    \n# 1x28x28 -> 32x28x28 -> 32x14x14\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1,28,28,1])    \n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# 32x14x14 -> 64x7x7\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# 64x7x7 -> 1024\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# Readout\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\nsess.run(tf.initialize_all_variables())\n\n# training\nN = len(mnist.train.images)\nN_test = len(mnist.test.images)\nn_epoch = 20000\nbatchsize = 50\nstart_time = time.clock()\nfor i in range(n_epoch):\n  batch = mnist.train.next_batch(batchsize)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print \"step %d, training accuracy %g\"%(i, train_accuracy)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n  \ntacc = 0\ntbatchsize = 1000\nfor i in range(0,N_test,tbatchsize):\n    acc = accuracy.eval(feed_dict={\n        x: mnist.test.images[i:i+tbatchsize], y_: mnist.test.labels[i:i+tbatchsize], keep_prob: 1.0})\n    print \"test accuracy %d = %g\" % (i//tbatchsize, acc)\n    tacc += acc * tbatchsize\ntacc /= N_test\nprint \"test accuracy = %g\" % tacc    \n\nend_time = time.clock()\nprint end_time - start_time\n```\n```lang:chainer_mnist.py\n# -*- coding: utf-8 -*-\nfrom chainer import cuda\nfrom chainer import optimizers\nimport chainer\nimport chainer.functions as F\nimport input_data\nimport numpy as np\nimport time\n\ngpu_flag = 0\n\nif gpu_flag >= 0:\n    cuda.check_cuda_available()\n    \nxp = cuda.cupy if gpu_flag >= 0 else np\n\nprint 'load MNIST dataset'\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\nmodel = chainer.FunctionSet(conv1=F.Convolution2D(1,32,5,pad=2),\n                            conv2=F.Convolution2D(32,64,5,pad=2),\n                            fl3=F.Linear(7*7*64,1024),\n                            fl4=F.Linear(1024,10))\n\nif gpu_flag >= 0:\n    cuda.get_device(gpu_flag).use()\n    model.to_gpu()   \n                            \ndef forward(x_data, y_data, train=True):\n    x, t = chainer.Variable(x_data), chainer.Variable(y_data)\n    h_conv1 = F.relu(model.conv1(x))\n    h_pool1 = F.max_pooling_2d(h_conv1, 2)\n    \n    h_conv2 = F.relu(model.conv2(h_pool1))\n    h_pool2 = F.max_pooling_2d(h_conv2, 2)\n\n    h_fc1 = F.relu(model.fl3(h_pool2))\n    h_fc1_drop = F.dropout(h_fc1, ratio=0.5,train=train)\n    \n    y = model.fl4(h_fc1_drop)\n\n    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)                            \n    \n\n# setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\nn_epoch = 20000\nbatchsize=50\n\nN = len(mnist.train.images)\nN_test = len(mnist.test.images)\n\nstart_time = time.clock()\nfor i in range(n_epoch):        \n    x_batch, y_batch = mnist.train.next_batch(batchsize)\n    x_batch = x_batch.astype(np.float32)\n    x_batch = xp.asarray(x_batch.reshape(batchsize,1,28,28))\n    y_batch = xp.asarray(np.argmax(y_batch, axis=1).astype(np.int32))\n            \n    if i%100 == 0:\n        loss, acc = forward(x_batch, y_batch, train=False)\n        print 'step {0} training accuracy {1}'.format(i, float(acc.data))\n\n    optimizer.zero_grads()\n    loss, acc = forward(x_batch, y_batch)\n    loss.backward()\n    optimizer.update()\n    \nsum_accuracy = 0\nsum_loss = 0\ntbatchsize=1000\nfor i in range(0, N_test, tbatchsize):\n    x_batch = mnist.test.images[i:i+tbatchsize].astype(np.float32)\n    y_batch = mnist.test.labels[i:i+tbatchsize]\n    x_batch = xp.asarray(x_batch.reshape(tbatchsize,1,28,28))\n    y_batch = xp.asarray(np.argmax(y_batch, axis=1).astype(np.int32))\n\n\n    loss, acc = forward(x_batch, y_batch, train=False)\n\n    sum_loss += float(loss.data)*tbatchsize\n    sum_accuracy += float(acc.data)*tbatchsize\n    print \"test accuracy %d = %g\" % (i//tbatchsize, float(acc.data))\n    \nprint 'test accuracy = {0}'.format(sum_accuracy/N_test)\n\nend_time = time.clock()\nprint end_time - start_time\n\n```\n", "tags": ["TensorFlow", "Chainer", "DeepLearning"]}