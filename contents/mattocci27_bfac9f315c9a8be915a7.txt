{"context": "\n\n\u80cc\u666f\nR\u3092\u4f7f\u3046\u4eba\u305f\u3061\u306f\u3001\u56de\u5e30\u3092\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u308c\u306b\u3082\u95a2\u308f\u3089\u305a\u3001R\u306eMXnet\u306e\u56de\u5e30\u306b\u95a2\u3059\u308b\u8cc7\u6599\u306f\u3001\u65e5\u672c\u8a9e\u3001\u82f1\u8a9e\u3092\u554f\u308f\u305a\u3001\u3068\u3066\u3082\u5c11\u306a\u3044\u6c17\u304c\u3057\u307e\u3059\u3002\u516c\u5f0f\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u3082\u6700\u4f4e\u9650\u306e\u4f8b\u3057\u304b\u306e\u3063\u3066\u3044\u307e\u305b\u3093\uff08R\u3068MXnet\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u3044\u3046\u306e\u304c\u3001Deep learning\u306e\u9078\u629e\u80a2\u3068\u3057\u3066\u6b63\u3057\u304f\u306a\u3044\u306e\u304b\u3082\uff1f\uff09\u3002\u305d\u3053\u3067\u3001\u7c21\u5358\u306a\u4f8b\u984c\u3092\u901a\u3058\u3066\u3001R\u306eMXnet\u306e\u56de\u5e30\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u4eca\u56de\u3042\u3064\u304b\u3046\u30c7\u30fc\u30bf\u306f\u3001Sin\u30ab\u30fc\u30d6\u306b\u6b63\u898f\u5206\u5e03\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u30c7\u30fc\u30bf\u3082\u306e\u3067\u3059\u3002\u3053\u308c\u3092R\u306eMXNet\u3067\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3057\u305f\u3002\n(\u672c\u30da\u30fc\u30b8\u306f\u521d\u5fc3\u8005\u304c\u66f8\u3044\u3066\u308b\u306e\u3067\u3001\u8aa4\u308a\u3092\u542b\u3080\u304b\u3082\u3057\u308c\u307e\u305b\u3093)\n\n\n\u30c7\u30fc\u30bf\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001\u672a\u77e5\u306e\u975e\u7dda\u5f62\u95a2\u6570f(x)\uff08\u5b9f\u969b\u306fsin\uff09\u306b\u5e73\u57470\u3067\u6a19\u6e96\u504f\u5dee0.5\u3067\u6b63\u898f\u5206\u5e03\u3059\u308b\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u3082\u306e\u3092\u3001\u30c7\u30fc\u30bf\u3068\u3057\u3066\u8003\u3048\u307e\u3059\u3002\ny = f(x) + \\epsilon\\\\\n\n\\epsilon \\sim Normal(0, 0.5) \ny=f(x)+\u03f5\u03f5\u223cNormal(0,0.5)y=f(x)+\u03f5\u03f5\u223cNormal(0,0.5){y = f(x) + \\epsilon\\\\\n\n\\epsilon \\sim Normal(0, 0.5) \n}\nset.seed(1)\nn_sample <- 100\nxx <- seq(-5, 5, length = n_sample)\nmu <- 2 + sin(xx)\nyy <- rnorm(n_sample, mu, 0.5)\n\n\n\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\nDeep learning\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u3044\u304d\u307e\u3059\u3002\u4e2d\u9593\u5c64\u306f\uff13\u5c64\u3067\u3001\u30ce\u30fc\u30c9\u306e\u6570\u306fBaysian Optimization\u306b\u3088\u308a\u6c7a\u3081\u3057\u305f\u3002\u3053\u306e\u8a18\u4e8b\u306a\u3069\u304c\u53c2\u8003\u306b\u308a\u307e\u3059\u3002\nlibrary(mxnet)\n\ndata <- mx.symbol.Variable(\"data\")\n\nfc1 <- mx.symbol.FullyConnected(data, name = \"fc1\", num_hidden = 100)\nact1 <- mx.symbol.Activation(fc1, name = \"relu1\", act_type = \"relu\")\ndrop1 <- mx.symbol.Dropout(act1, p = 0.2)\n\nfc2 <- mx.symbol.FullyConnected(act1, name = \"fc2\", num_hidden = 49)\nact2 <- mx.symbol.Activation(fc2, name = \"relu2\", act_type = \"relu\")\ndrop2 <- mx.symbol.Dropout(act2, p = 0.2)\n\nfc3 <- mx.symbol.FullyConnected(drop2, name = \"fc3\", num_hidden = 53)\nact3 <- mx.symbol.Activation(fc3, name = \"relu3\", act_type = \"relu\")\ndrop3 <- mx.symbol.Dropout(act3, p = 0.2)\n\nfc4 <- mx.symbol.FullyConnected(drop3, num_hidden = 1)\n\noutput <- mx.symbol.LinearRegressionOutput(fc4)\n\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\uff11\n\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3044\u304d\u307e\u3059\u3002Learning rate\u3082Baysian Optimization\u3067\u5f97\u3089\u308c\u305f\u5024\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u4e88\u6e2c\u529b\u306e\u9ad8\u3044\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u305f\u3081\u306b\u306f\u3001\u672c\u6765\u306ftrain\u3068test\u306e\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\u3079\u304d\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u7c21\u6613\u5316\u306e\u305f\u3081\u306b\u3001\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3068\u3053\u308d\u307e\u3067\u3092\u76ee\u7684\u3068\u3057\u3066\u308b\u306e\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u30c7\u30fc\u30bf\u306f\u6b63\u898f\u5316\u3055\u305b\u305a\u3001\u305d\u306e\u307e\u307e\u306e\u5024\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\nmx.set.seed(1)\n\nmodel <- mx.model.FeedForward.create(output, X = xx, y = yy,\n                                       ctx = mx.cpu(),\n                                       num.round = 124,\n                                       array.batch.size = 100,\n                                       initializer = mx.init.uniform(0.01),\n                                       learning.rate = 0.0789,\n                                       momentum = 0.9,\n                                       eval.metric = mx.metric.rmse)\n\nStart training with 1 devices\n[1] Train-rmse=NaN\n[2] Train-rmse=2.23326795346586\n[3] Train-rmse=2.08504108125293\n[4] Train-rmse=1.82124038961198\n[5] Train-rmse=1.48954090785246\n[6] Train-rmse=1.15752385720034\n[7] Train-rmse=0.923669173895963\n#...#\n\n[123] Train-rmse=0.875656856417879\n[124] Train-rmse=0.875656267602696\n\n\n\u5b66\u7fd2\u3059\u308b\u3054\u3068\u306b\u3001RMSE\u306f\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u306f\u5168\u304f\u4e0a\u624b\u304f\u3044\u3063\u3066\u3044\u307e\u305b\u3093\u3002\u4e0b\u306e\u56f3\u306e\u3088\u3046\u306blocal minima(\u3053\u306e\u5834\u5408\u3001y\u306e\u5e73\u5747\u5024)\u306b\u5f15\u3063\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002\u4ed6\u306e\u30c7\u30fc\u30bf\u3067\u8a66\u3057\u3066\u3082\u3001\u540c\u3058\u3088\u3046\u306a\u5931\u6557\u3092\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002\n\n\u3053\u3053\u3067\u3001\u4e00\u756a\u306e\u30dd\u30a4\u30f3\u30c8\u306f\u3001initializer\u306e\u5024\u3067\u3059\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001initializer = mx.init.uniform(0.01)\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u3092\u5c11\u3057\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b2\ninitializer = mx.init.uniform(0.2)\u3068\u5909\u66f4\u3057\u3066\u3001\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u307e\u3059\u3002Baysian Optimization\u3082\u3001mx.init.uniform(0.2)\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3067\u3001\u521d\u671f\u5024\u3092uniform(\u22120.2,0.2)uniform(\u22120.2,0.2)uniform(-0.2, 0.2)\u304b\u3089\u3001\u3068\u3063\u3066\u304f\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\nmodel <- mx.model.FeedForward.create(output, X = xx, y = yy,\n                                       ctx = mx.cpu(),\n                                       num.round = 124,\n                                       array.batch.size = 100,\n                                       initializer=mx.init.uniform(0.2),\n                                       learning.rate = 0.0789,\n                                       momentum = 0.9,\n                                       eval.metric=mx.metric.rmse)\n\nStart training with 1 devices\n[1] Train-rmse=NaN\n[2] Train-rmse=2.12165792635148\n[3] Train-rmse=1.65953289558761\n[4] Train-rmse=1.1639932568447\n[5] Train-rmse=1.53508849050074\n[6] Train-rmse=1.45345311053074\n[7] Train-rmse=0.977683196174199\n#...#\n[123] Train-rmse=0.467904513556867\n[124] Train-rmse=0.443160963247551\n\n\u4eca\u5ea6\u306f\u3001local minima\u306b\u5f15\u3063\u304b\u304b\u308b\u3053\u3068\u306a\u304f\u3001\u305d\u308c\u306a\u308a\u306b\u4e0a\u624b\u304f\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u4e88\u6e2c\u306e\u30e9\u30a4\u30f3\u306f\u3001x>3\u03c0/2x>3\u03c0/2x > 3\\pi/2\u306e\u3042\u305f\u308a\u3067\u3082\u6e1b\u5c11\u95a2\u6570\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u306f\u3001\u3053\u308c\u304c\u9650\u754c\u304b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n\n\n\u5b66\u7fd2\u3057\u3066\u3044\u304f\u69d8\u5b50\n\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u3001\u5b66\u7fd2\u3057\u3066\u3044\u304f\u69d8\u5b50\u306egif\u3082\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n\u307e\u3068\u3081\n\u8abf\u6574\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u591a\u3044\u3067\u3059\u3002\u307e\u305f\u3001\u521d\u671f\u5024\u3092\u6c7a\u3081\u308b\u78ba\u7387\u5206\u5e03\u306b\u95a2\u3059\u308b\u8aac\u660e\u304c\u3042\u307e\u308a\u898b\u5f53\u305f\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u3053\u3053\u3092\u8a2d\u5b9a\u3057\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3053\u3068\u306b\u6c17\u3065\u304f\u306e\u306b\u3001\u304b\u306a\u308a\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u3068\u308a\u3042\u3048\u305a\u3001R\u3067\u3082Deep learning\u3067\u56de\u5e30\u304c\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n\n\u74b0\u5883\nsessionInfo()\nR version 3.3.2 (2016-10-31)\nPlatform: x86_64-pc-[uploading-0](...)\nlinux-gnu (64-bit)\nRunning under: Ubuntu 14.04.5 LTS\n\n\n\u53c2\u8003\n\nA Gentle Introduction to Artificial Neural Networks\n\nDeep Learning \u5165\u9580\uff08\uff12\uff09 - Chainer\u3067\u81ea\u4f5c\u306e\u975e\u7dda\u5f62\u56de\u5e30\u3092\u8a66\u305d\u3046 -\nDevelop a Neural Network with MXNet in Five Minutes\nDeep Learning\u3067\u904a\u3076(2): \u30aa\u30f3\u30e9\u30a4\u30f3\u30cb\u30e5\u30fc\u30b9\u4eba\u6c17\u5ea6\uff0b\u30d9\u30a4\u30ba\u6700\u9069\u5316\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\n\n\n\u304a\u307e\u3051\uff1a\u5168\u4f53\u306e\u30b3\u30fc\u30c9\n\nlibrary(mxnet)\nlibrary(rBayesianOptimization)\nlibrary(dplyr)\nlibrary(Metrics)\nlibrary(ggplot2)\nlibrary(cowplot)\n\n#\u30c7\u30fc\u30bf\u306e\u6e96\u5099\nset.seed(1)\nn_sample <- 100\nxx <- seq(-5, 5, length = n_sample)\nmu <- 2 + sin(xx)\nyy <- rnorm(n_sample, mu, 0.5)\n\n\n# Bayesian Optimization\nmxnet_holdout_bayes <- function(unit1, unit2, unit3, num_r, learn_r){\n     data <- mx.symbol.Variable(\"data\")\n     fc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=unit1)\n     act1 <- mx.symbol.Activation(fc1, name=\"relu1\", act_type=\"relu\")\n     drop1 <- mx.symbol.Dropout(act1, p=0.2)\n     fc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=unit2)\n     act2 <- mx.symbol.Activation(fc2, name=\"relu2\", act_type=\"relu\")\n     drop2 <- mx.symbol.Dropout(act2, p=0.2)\n     fc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\", num_hidden=unit3)\n     act3 <- mx.symbol.Activation(fc3, name=\"relu3\", act_type=\"relu\")\n     drop3 <- mx.symbol.Dropout(act3, p=0.2)\n     fc4 <- mx.symbol.FullyConnected(drop3, name=\"fc4\", num_hidden=1)\n     output <- mx.symbol.LinearRegressionOutput(fc4, name=\"linreg\")\n     devices <- mx.cpu()\n\n     mx.set.seed(1)\n     model <- mx.model.FeedForward.create(output, X=xx,\n      y=yy,\n       ctx=devices, num.round=num_r, array.batch.size=100,\n       learning.rate=learn_r, momentum=0.9,\n       eval.metric=mx.metric.rmse,\n       initializer=mx.init.uniform(0.2),\n       epoch.end.callback=mx.callback.log.train.metric(20),\n      verbose=FALSE)\n\n     preds <- predict(model, data.matrix(xx), array.layout='rowmajor')\n     holdout_score <- rmse(preds, yy)\n     list(Score=-holdout_score, Pred=-holdout_score)\n }\n\n# Bayesian Optimization\u306e\u5b9f\u884c\nopt_res <- BayesianOptimization(mxnet_holdout_bayes,\n                               bounds=list(unit1=c(1L,100L),\n                                           unit2=c(1L,100L),\n                                           unit3=c(1L,100L),\n                                           num_r=c(10L,150L),\n                                           learn_r=c(1e-5,1e-1)),\n                               init_points=50, n_iter=1, acq='ucb', kappa=2.576, eps=0.0, verbose=TRUE)\n# Best Parameters Found:\n# Round = 51      unit1 = 100.0000        unit2 = 49.0000 unit3 = 53.0000 num_r = 124.0000        learn_r = 0\n# .0789   Value = -0.4390\n\n# \u30e2\u30c7\u30eb\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\",\n num_hidden=opt_res$Best_Par[1]))\nact1 <- mx.symbol.Activation(fc1, name=\"relu1\", act_type=\"relu\")\ndrop1 <- mx.symbol.Dropout(act1, p=0.2)\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\",\n num_hidden=opt_res$Best_Par[2]))\nact2 <- mx.symbol.Activation(fc2, name=\"relu2\", act_type=\"relu\")\ndrop2 <- mx.symbol.Dropout(act2, p=0.2)\nfc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\",\n num_hidden=opt_res$Best_Par[3])\nact3 <- mx.symbol.Activation(fc3, name=\"relu3\", act_type=\"relu\")\ndrop3 <- mx.symbol.Dropout(act3, p=0.2)\nfc4 <- mx.symbol.FullyConnected(drop2, name=\"fc4\", num_hidden=1)\noutput <- mx.symbol.LinearRegressionOutput(fc4, name=\"linreg\")\n\n\n# ggplot\u3067\u306e\u30e1\u30e2\u30ea\u8abf\u6574\u7528\u95a2\u6570\nfmt_dcimals <- function(x) format(x, nsmall = 2, scientific = FALSE)\n\n# \u6210\u529f\u4e00\u679a\u5206\u306e\u4f5c\u56f3\npdf(\"good.pdf\", width = 3.5, height = 3.5)\n  mx.set.seed(1)\n  model <- mx.model.FeedForward.create(output, X=xx, y=yy,\n                                         ctx=mx.cpu(),\n                                         num.round=opt_res$Best_Par[4],\n                                         array.batch.size=100,\n                                         initializer=mx.init.uniform(0.2),\n                                         learning.rate=opt_res$Best_Par[5],\n                                         momentum=0.9,\n                                         eval.metric=mx.metric.rmse)\n\n   preds <-  predict(model, data.matrix(xx))\n\n  fig_dat <- data_frame(xx, yy, mu, preds = preds[1,]) %>%\n   tidyr::gather(cat, val, 2:4) %>%\n   mutate(cat = factor(cat, levels = c(\"mu\", \"yy\", \"preds\"))) %>%\n   mutate(cat = factor(cat, labels = c(\"True relationship\", \"Target\", \"Network output\"))) %>%\n   mutate(cat2 = ifelse(cat == \"Network output\", \"Network output\", \"Target and True relationship\") %>% as.factor)\n\n  p1 <- ggplot(fig_dat %>% arrange(val), aes(x = xx, y = val)) +\n   geom_point(data = fig_dat %>% filter(cat == \"Target\"), aes(colour = cat2)) +\n   geom_line(data = fig_dat %>% filter(cat != \"Target\"), aes(colour = cat2)) +\n   guides(colour = guide_legend(title = NULL,\n   order = 1)) +\n   labs(x = \"x\", y = \"y\") +\n   theme_bw() +\n   theme(legend.position = c(0.6, 0.85),\n         legend.background = element_rect(colour = \"black\",\n           linetype = \"solid\", size = 0.1)) +\n   scale_y_continuous(limits = c(-0.5, 7), labels=fmt_dcimals)\n  p1\ndev.off()\n\n\n\n# 130\u679a\u5206\u306e\u4f5c\u56f3\nmse_v <- NULL\nn <- 3\nhues <- seq(15, 375, length=n+1)\ncols_hex <- sort(hcl(h=hues, l=65, c=100)[1:n])\n\nfor (i in 1:130){\n  mx.set.seed(1)\n  model <- mx.model.FeedForward.create(output, X=xx, y=yy,\n                                         ctx=mx.cpu(),\n                                         num.round=i,\n                                         array.batch.size=100,\n                                         initializer=mx.init.uniform(0.2),\n                                         learning.rate=0.0789,\n                                         momentum=0.9,\n                                         eval.metric=mx.metric.rmse)\n\n   preds <-  predict(model, data.matrix(xx))\n   mse_v[i] <- (preds - yy)^2 %>% mean\n\n   if (i < 10) i2 <- paste(\"00\", i, sep = \"\")  else if (i < 100) i2 <- paste(0, i, sep = \"\") else i2 <- i\n   pdf(paste(i2, \"_fig.pdf\", sep = \"\"), width = 7, height = 3.5)\n   fig_dat <- data_frame(xx, yy, mu, preds = preds[1,]) %>%\n    tidyr::gather(cat, val, 2:4) %>%\n    mutate(cat = factor(cat, levels = c(\"mu\", \"yy\", \"preds\"))) %>%\n    mutate(cat = factor(cat, labels = c(\"True relationship\", \"Target\", \"Network output\"))) %>%\n    mutate(cat2 = ifelse(cat == \"Network output\", \"Network output\", \"Target and True relationship\") %>% as.factor)\n\n   p1 <- ggplot(fig_dat %>% arrange(val), aes(x = xx, y = val)) +\n    geom_point(data = fig_dat %>% filter(cat == \"Target\"), aes(colour = cat2)) +\n    geom_line(data = fig_dat %>% filter(cat != \"Target\"), aes(colour = cat2)) +\n    guides(colour = guide_legend(title = NULL,\n    order = 1)) +\n    labs(title = paste(\"Iteration =\", i), x = \"x\", y = \"y\") +\n    theme_bw() +\n    theme(legend.position = c(0.6, 0.85),\n          legend.background = element_rect(colour = \"black\",\n            linetype = \"solid\", size = 0.1)) +\n    scale_y_continuous(limits = c(-0.5, 7), labels=fmt_dcimals) +\n    scale_x_continuous(labels=fmt_dcimals)\n\n\n    if (i != 1) fig_dat2 <- data_frame(MSE = mse_v, Iteration = 1:i) else fig_dat2 <- data_frame(MSE = 0, Iteration = 1:i)\n\n    if (i < 20) ylim_max <- 6 else if (i < 50) ylim_max <- 2 else if (i < 75) ylim_max <- 1 else ylim_max <- 0.5\n    if (i < 20) xlim_max <- 50 else  if (i < 80) xlim_max <- 100 else xlim_max <- 130\n\n    p2 <- ggplot(fig_dat2, aes(x = Iteration, y = MSE)) +\n      geom_path(colour = cols_hex[2]) +\n      labs(title = \"\", x = \"Iterations\", y = \"Mean square error\") +\n      xlim(0, xlim_max) +\n      theme_bw() +\n      scale_y_continuous(limits = c(0, ylim_max), labels=fmt_dcimals)\n\n    p3 <- plot_grid(p1, p2, align = \"hv\")\n    print(p3)\n\n   dev.off()\n}\n\n#\u80cc\u666f\nR\u3092\u4f7f\u3046\u4eba\u305f\u3061\u306f\u3001\u56de\u5e30\u3092\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u305d\u308c\u306b\u3082\u95a2\u308f\u3089\u305a\u3001R\u306eMXnet\u306e\u56de\u5e30\u306b\u95a2\u3059\u308b\u8cc7\u6599\u306f\u3001\u65e5\u672c\u8a9e\u3001\u82f1\u8a9e\u3092\u554f\u308f\u305a\u3001\u3068\u3066\u3082\u5c11\u306a\u3044\u6c17\u304c\u3057\u307e\u3059\u3002\u516c\u5f0f\u306e[\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb](http://mxnet.io/tutorials/r/fiveMinutesNeuralNetwork.html)\u306b\u3082\u6700\u4f4e\u9650\u306e\u4f8b\u3057\u304b\u306e\u3063\u3066\u3044\u307e\u305b\u3093\uff08R\u3068MXnet\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3068\u3044\u3046\u306e\u304c\u3001Deep learning\u306e\u9078\u629e\u80a2\u3068\u3057\u3066\u6b63\u3057\u304f\u306a\u3044\u306e\u304b\u3082\uff1f\uff09\u3002\u305d\u3053\u3067\u3001\u7c21\u5358\u306a\u4f8b\u984c\u3092\u901a\u3058\u3066\u3001R\u306eMXnet\u306e\u56de\u5e30\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u4eca\u56de\u3042\u3064\u304b\u3046\u30c7\u30fc\u30bf\u306f\u3001Sin\u30ab\u30fc\u30d6\u306b\u6b63\u898f\u5206\u5e03\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u30c7\u30fc\u30bf\u3082\u306e\u3067\u3059\u3002\u3053\u308c\u3092R\u306eMXNet\u3067\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u3066\u307f\u308b\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u307e\u3057\u305f\u3002\n\n(\u672c\u30da\u30fc\u30b8\u306f\u521d\u5fc3\u8005\u304c\u66f8\u3044\u3066\u308b\u306e\u3067\u3001\u8aa4\u308a\u3092\u542b\u3080\u304b\u3082\u3057\u308c\u307e\u305b\u3093)\n\n\n<img width=\"400\" alt=\"init.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/152374/601aa064-ab39-c95f-742a-298eec41211e.png\">\n\n#\u30c7\u30fc\u30bf\n\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001\u672a\u77e5\u306e\u975e\u7dda\u5f62\u95a2\u6570f(x)\uff08\u5b9f\u969b\u306fsin\uff09\u306b\u5e73\u57470\u3067\u6a19\u6e96\u504f\u5dee0.5\u3067\u6b63\u898f\u5206\u5e03\u3059\u308b\u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u3082\u306e\u3092\u3001\u30c7\u30fc\u30bf\u3068\u3057\u3066\u8003\u3048\u307e\u3059\u3002\n\n\n```math\ny = f(x) + \\epsilon\\\\\n\n\\epsilon \\sim Normal(0, 0.5) \n```\n```R\nset.seed(1)\nn_sample <- 100\nxx <- seq(-5, 5, length = n_sample)\nmu <- 2 + sin(xx)\nyy <- rnorm(n_sample, mu, 0.5)\n```\n\n#\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\nDeep learning\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u3044\u304d\u307e\u3059\u3002\u4e2d\u9593\u5c64\u306f\uff13\u5c64\u3067\u3001\u30ce\u30fc\u30c9\u306e\u6570\u306f[Baysian Optimization](https://cran.r-project.org/web/packages/rBayesianOptimization/index.html)\u306b\u3088\u308a\u6c7a\u3081\u3057\u305f\u3002\u3053\u306e[\u8a18\u4e8b](http://tjo.hatenablog.com/entry/2016/07/21/190000)\u306a\u3069\u304c\u53c2\u8003\u306b\u308a\u307e\u3059\u3002\n\n```R\nlibrary(mxnet)\n\ndata <- mx.symbol.Variable(\"data\")\n\nfc1 <- mx.symbol.FullyConnected(data, name = \"fc1\", num_hidden = 100)\nact1 <- mx.symbol.Activation(fc1, name = \"relu1\", act_type = \"relu\")\ndrop1 <- mx.symbol.Dropout(act1, p = 0.2)\n\nfc2 <- mx.symbol.FullyConnected(act1, name = \"fc2\", num_hidden = 49)\nact2 <- mx.symbol.Activation(fc2, name = \"relu2\", act_type = \"relu\")\ndrop2 <- mx.symbol.Dropout(act2, p = 0.2)\n\nfc3 <- mx.symbol.FullyConnected(drop2, name = \"fc3\", num_hidden = 53)\nact3 <- mx.symbol.Activation(fc3, name = \"relu3\", act_type = \"relu\")\ndrop3 <- mx.symbol.Dropout(act3, p = 0.2)\n\nfc4 <- mx.symbol.FullyConnected(drop3, num_hidden = 1)\n\noutput <- mx.symbol.LinearRegressionOutput(fc4)\n```\n\n#\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\uff11\n\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3044\u304d\u307e\u3059\u3002Learning rate\u3082Baysian Optimization\u3067\u5f97\u3089\u308c\u305f\u5024\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u4e88\u6e2c\u529b\u306e\u9ad8\u3044\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u305f\u3081\u306b\u306f\u3001\u672c\u6765\u306ftrain\u3068test\u306e\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\u3079\u304d\u3067\u3059\u304c\u3001\u4eca\u56de\u306f\u7c21\u6613\u5316\u306e\u305f\u3081\u306b\u3001\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3068\u3053\u308d\u307e\u3067\u3092\u76ee\u7684\u3068\u3057\u3066\u308b\u306e\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u30c7\u30fc\u30bf\u306f\u6b63\u898f\u5316\u3055\u305b\u305a\u3001\u305d\u306e\u307e\u307e\u306e\u5024\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\n\n```R\nmx.set.seed(1)\n\nmodel <- mx.model.FeedForward.create(output, X = xx, y = yy,\n                                       ctx = mx.cpu(),\n                                       num.round = 124,\n                                       array.batch.size = 100,\n                                       initializer = mx.init.uniform(0.01),\n                                       learning.rate = 0.0789,\n                                       momentum = 0.9,\n                                       eval.metric = mx.metric.rmse)\n\nStart training with 1 devices\n[1] Train-rmse=NaN\n[2] Train-rmse=2.23326795346586\n[3] Train-rmse=2.08504108125293\n[4] Train-rmse=1.82124038961198\n[5] Train-rmse=1.48954090785246\n[6] Train-rmse=1.15752385720034\n[7] Train-rmse=0.923669173895963\n#...#\n\n[123] Train-rmse=0.875656856417879\n[124] Train-rmse=0.875656267602696\n\n```\n\u5b66\u7fd2\u3059\u308b\u3054\u3068\u306b\u3001RMSE\u306f\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u306f\u5168\u304f\u4e0a\u624b\u304f\u3044\u3063\u3066\u3044\u307e\u305b\u3093\u3002\u4e0b\u306e\u56f3\u306e\u3088\u3046\u306blocal minima(\u3053\u306e\u5834\u5408\u3001y\u306e\u5e73\u5747\u5024)\u306b\u5f15\u3063\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002\u4ed6\u306e\u30c7\u30fc\u30bf\u3067\u8a66\u3057\u3066\u3082\u3001\u540c\u3058\u3088\u3046\u306a\u5931\u6557\u3092\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002\n\n<img width=\"400\" alt=\"bad.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/152374/0e5aae46-a984-e4ae-9391-c2634703c83d.png\">\n\n\n\u3053\u3053\u3067\u3001\u4e00\u756a\u306e\u30dd\u30a4\u30f3\u30c8\u306f\u3001`initializer`\u306e\u5024\u3067\u3059\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001`initializer = mx.init.uniform(0.01)`\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u3092\u5c11\u3057\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n#\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b2\n`initializer = mx.init.uniform(0.2)`\u3068\u5909\u66f4\u3057\u3066\u3001\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u307e\u3059\u3002Baysian Optimization\u3082\u3001`mx.init.uniform(0.2)`\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3067\u3001\u521d\u671f\u5024\u3092$uniform(-0.2, 0.2)$\u304b\u3089\u3001\u3068\u3063\u3066\u304f\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\n```R\nmodel <- mx.model.FeedForward.create(output, X = xx, y = yy,\n                                       ctx = mx.cpu(),\n                                       num.round = 124,\n                                       array.batch.size = 100,\n                                       initializer=mx.init.uniform(0.2),\n                                       learning.rate = 0.0789,\n                                       momentum = 0.9,\n                                       eval.metric=mx.metric.rmse)\n\nStart training with 1 devices\n[1] Train-rmse=NaN\n[2] Train-rmse=2.12165792635148\n[3] Train-rmse=1.65953289558761\n[4] Train-rmse=1.1639932568447\n[5] Train-rmse=1.53508849050074\n[6] Train-rmse=1.45345311053074\n[7] Train-rmse=0.977683196174199\n#...#\n[123] Train-rmse=0.467904513556867\n[124] Train-rmse=0.443160963247551\n```\n\u4eca\u5ea6\u306f\u3001local minima\u306b\u5f15\u3063\u304b\u304b\u308b\u3053\u3068\u306a\u304f\u3001\u305d\u308c\u306a\u308a\u306b\u4e0a\u624b\u304f\u30d5\u30a3\u30c3\u30c8\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u4e88\u6e2c\u306e\u30e9\u30a4\u30f3\u306f\u3001$x > 3\\pi/2$\u306e\u3042\u305f\u308a\u3067\u3082\u6e1b\u5c11\u95a2\u6570\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u306f\u3001\u3053\u308c\u304c\u9650\u754c\u304b\u3068\u601d\u308f\u308c\u307e\u3059\u3002\n\n<img width=\"400\" alt=\"good.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/152374/27a89380-b81a-2b14-2db0-843e4b3cd077.png\">\n\n\n#\u5b66\u7fd2\u3057\u3066\u3044\u304f\u69d8\u5b50\n\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u3001\u5b66\u7fd2\u3057\u3066\u3044\u304f\u69d8\u5b50\u306egif\u3082\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n![moge1_130.gif](https://qiita-image-store.s3.amazonaws.com/0/152374/d4abaebe-c61d-ba6c-cfd1-b1df4dc2596d.gif)\n\n#\u307e\u3068\u3081\n\u8abf\u6574\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u591a\u3044\u3067\u3059\u3002\u307e\u305f\u3001\u521d\u671f\u5024\u3092\u6c7a\u3081\u308b\u78ba\u7387\u5206\u5e03\u306b\u95a2\u3059\u308b\u8aac\u660e\u304c\u3042\u307e\u308a\u898b\u5f53\u305f\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u3053\u3053\u3092\u8a2d\u5b9a\u3057\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3053\u3068\u306b\u6c17\u3065\u304f\u306e\u306b\u3001\u304b\u306a\u308a\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u3068\u308a\u3042\u3048\u305a\u3001R\u3067\u3082Deep learning\u3067\u56de\u5e30\u304c\u3067\u304d\u305d\u3046\u3067\u3059\u3002\n\n\n#\u74b0\u5883\n```R:\nsessionInfo()\nR version 3.3.2 (2016-10-31)\nPlatform: x86_64-pc-[uploading-0](...)\nlinux-gnu (64-bit)\nRunning under: Ubuntu 14.04.5 LTS\n```\n\n#\u53c2\u8003\n* [A Gentle Introduction to Artificial Neural Networks\n](https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/)\n* [Deep Learning \u5165\u9580\uff08\uff12\uff09 - Chainer\u3067\u81ea\u4f5c\u306e\u975e\u7dda\u5f62\u56de\u5e30\u3092\u8a66\u305d\u3046 -](http://qiita.com/carat_yoshizaki/items/bfe559d1bdd434be03ed)\n* [Develop a Neural Network with MXNet in Five Minutes](http://mxnet.io/tutorials/r/fiveMinutesNeuralNetwork.html)\n* [Deep Learning\u3067\u904a\u3076(2): \u30aa\u30f3\u30e9\u30a4\u30f3\u30cb\u30e5\u30fc\u30b9\u4eba\u6c17\u5ea6\uff0b\u30d9\u30a4\u30ba\u6700\u9069\u5316\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0] (http://tjo.hatenablog.com/entry/2016/07/21/190000)\n\n#\u304a\u307e\u3051\uff1a\u5168\u4f53\u306e\u30b3\u30fc\u30c9\n```R\n\nlibrary(mxnet)\nlibrary(rBayesianOptimization)\nlibrary(dplyr)\nlibrary(Metrics)\nlibrary(ggplot2)\nlibrary(cowplot)\n\n#\u30c7\u30fc\u30bf\u306e\u6e96\u5099\nset.seed(1)\nn_sample <- 100\nxx <- seq(-5, 5, length = n_sample)\nmu <- 2 + sin(xx)\nyy <- rnorm(n_sample, mu, 0.5)\n\n\n# Bayesian Optimization\nmxnet_holdout_bayes <- function(unit1, unit2, unit3, num_r, learn_r){\n     data <- mx.symbol.Variable(\"data\")\n     fc1 <- mx.symbol.FullyConnected(data, name=\"fc1\", num_hidden=unit1)\n     act1 <- mx.symbol.Activation(fc1, name=\"relu1\", act_type=\"relu\")\n     drop1 <- mx.symbol.Dropout(act1, p=0.2)\n     fc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\", num_hidden=unit2)\n     act2 <- mx.symbol.Activation(fc2, name=\"relu2\", act_type=\"relu\")\n     drop2 <- mx.symbol.Dropout(act2, p=0.2)\n     fc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\", num_hidden=unit3)\n     act3 <- mx.symbol.Activation(fc3, name=\"relu3\", act_type=\"relu\")\n     drop3 <- mx.symbol.Dropout(act3, p=0.2)\n     fc4 <- mx.symbol.FullyConnected(drop3, name=\"fc4\", num_hidden=1)\n     output <- mx.symbol.LinearRegressionOutput(fc4, name=\"linreg\")\n     devices <- mx.cpu()\n\n     mx.set.seed(1)\n     model <- mx.model.FeedForward.create(output, X=xx,\n      y=yy,\n       ctx=devices, num.round=num_r, array.batch.size=100,\n       learning.rate=learn_r, momentum=0.9,\n       eval.metric=mx.metric.rmse,\n       initializer=mx.init.uniform(0.2),\n       epoch.end.callback=mx.callback.log.train.metric(20),\n      verbose=FALSE)\n\n     preds <- predict(model, data.matrix(xx), array.layout='rowmajor')\n     holdout_score <- rmse(preds, yy)\n     list(Score=-holdout_score, Pred=-holdout_score)\n }\n\n# Bayesian Optimization\u306e\u5b9f\u884c\nopt_res <- BayesianOptimization(mxnet_holdout_bayes,\n                               bounds=list(unit1=c(1L,100L),\n                                           unit2=c(1L,100L),\n                                           unit3=c(1L,100L),\n                                           num_r=c(10L,150L),\n                                           learn_r=c(1e-5,1e-1)),\n                               init_points=50, n_iter=1, acq='ucb', kappa=2.576, eps=0.0, verbose=TRUE)\n# Best Parameters Found:\n# Round = 51      unit1 = 100.0000        unit2 = 49.0000 unit3 = 53.0000 num_r = 124.0000        learn_r = 0\n# .0789   Value = -0.4390\n\n# \u30e2\u30c7\u30eb\ndata <- mx.symbol.Variable(\"data\")\nfc1 <- mx.symbol.FullyConnected(data, name=\"fc1\",\n num_hidden=opt_res$Best_Par[1]))\nact1 <- mx.symbol.Activation(fc1, name=\"relu1\", act_type=\"relu\")\ndrop1 <- mx.symbol.Dropout(act1, p=0.2)\nfc2 <- mx.symbol.FullyConnected(drop1, name=\"fc2\",\n num_hidden=opt_res$Best_Par[2]))\nact2 <- mx.symbol.Activation(fc2, name=\"relu2\", act_type=\"relu\")\ndrop2 <- mx.symbol.Dropout(act2, p=0.2)\nfc3 <- mx.symbol.FullyConnected(drop2, name=\"fc3\",\n num_hidden=opt_res$Best_Par[3])\nact3 <- mx.symbol.Activation(fc3, name=\"relu3\", act_type=\"relu\")\ndrop3 <- mx.symbol.Dropout(act3, p=0.2)\nfc4 <- mx.symbol.FullyConnected(drop2, name=\"fc4\", num_hidden=1)\noutput <- mx.symbol.LinearRegressionOutput(fc4, name=\"linreg\")\n\n\n# ggplot\u3067\u306e\u30e1\u30e2\u30ea\u8abf\u6574\u7528\u95a2\u6570\nfmt_dcimals <- function(x) format(x, nsmall = 2, scientific = FALSE)\n\n# \u6210\u529f\u4e00\u679a\u5206\u306e\u4f5c\u56f3\npdf(\"good.pdf\", width = 3.5, height = 3.5)\n  mx.set.seed(1)\n  model <- mx.model.FeedForward.create(output, X=xx, y=yy,\n                                         ctx=mx.cpu(),\n                                         num.round=opt_res$Best_Par[4],\n                                         array.batch.size=100,\n                                         initializer=mx.init.uniform(0.2),\n                                         learning.rate=opt_res$Best_Par[5],\n                                         momentum=0.9,\n                                         eval.metric=mx.metric.rmse)\n\n   preds <-  predict(model, data.matrix(xx))\n\n  fig_dat <- data_frame(xx, yy, mu, preds = preds[1,]) %>%\n   tidyr::gather(cat, val, 2:4) %>%\n   mutate(cat = factor(cat, levels = c(\"mu\", \"yy\", \"preds\"))) %>%\n   mutate(cat = factor(cat, labels = c(\"True relationship\", \"Target\", \"Network output\"))) %>%\n   mutate(cat2 = ifelse(cat == \"Network output\", \"Network output\", \"Target and True relationship\") %>% as.factor)\n\n  p1 <- ggplot(fig_dat %>% arrange(val), aes(x = xx, y = val)) +\n   geom_point(data = fig_dat %>% filter(cat == \"Target\"), aes(colour = cat2)) +\n   geom_line(data = fig_dat %>% filter(cat != \"Target\"), aes(colour = cat2)) +\n   guides(colour = guide_legend(title = NULL,\n   order = 1)) +\n   labs(x = \"x\", y = \"y\") +\n   theme_bw() +\n   theme(legend.position = c(0.6, 0.85),\n         legend.background = element_rect(colour = \"black\",\n           linetype = \"solid\", size = 0.1)) +\n   scale_y_continuous(limits = c(-0.5, 7), labels=fmt_dcimals)\n  p1\ndev.off()\n\n\n\n# 130\u679a\u5206\u306e\u4f5c\u56f3\nmse_v <- NULL\nn <- 3\nhues <- seq(15, 375, length=n+1)\ncols_hex <- sort(hcl(h=hues, l=65, c=100)[1:n])\n\nfor (i in 1:130){\n  mx.set.seed(1)\n  model <- mx.model.FeedForward.create(output, X=xx, y=yy,\n                                         ctx=mx.cpu(),\n                                         num.round=i,\n                                         array.batch.size=100,\n                                         initializer=mx.init.uniform(0.2),\n                                         learning.rate=0.0789,\n                                         momentum=0.9,\n                                         eval.metric=mx.metric.rmse)\n\n   preds <-  predict(model, data.matrix(xx))\n   mse_v[i] <- (preds - yy)^2 %>% mean\n\n   if (i < 10) i2 <- paste(\"00\", i, sep = \"\")  else if (i < 100) i2 <- paste(0, i, sep = \"\") else i2 <- i\n   pdf(paste(i2, \"_fig.pdf\", sep = \"\"), width = 7, height = 3.5)\n   fig_dat <- data_frame(xx, yy, mu, preds = preds[1,]) %>%\n    tidyr::gather(cat, val, 2:4) %>%\n    mutate(cat = factor(cat, levels = c(\"mu\", \"yy\", \"preds\"))) %>%\n    mutate(cat = factor(cat, labels = c(\"True relationship\", \"Target\", \"Network output\"))) %>%\n    mutate(cat2 = ifelse(cat == \"Network output\", \"Network output\", \"Target and True relationship\") %>% as.factor)\n\n   p1 <- ggplot(fig_dat %>% arrange(val), aes(x = xx, y = val)) +\n    geom_point(data = fig_dat %>% filter(cat == \"Target\"), aes(colour = cat2)) +\n    geom_line(data = fig_dat %>% filter(cat != \"Target\"), aes(colour = cat2)) +\n    guides(colour = guide_legend(title = NULL,\n    order = 1)) +\n    labs(title = paste(\"Iteration =\", i), x = \"x\", y = \"y\") +\n    theme_bw() +\n    theme(legend.position = c(0.6, 0.85),\n          legend.background = element_rect(colour = \"black\",\n            linetype = \"solid\", size = 0.1)) +\n    scale_y_continuous(limits = c(-0.5, 7), labels=fmt_dcimals) +\n    scale_x_continuous(labels=fmt_dcimals)\n\n\n    if (i != 1) fig_dat2 <- data_frame(MSE = mse_v, Iteration = 1:i) else fig_dat2 <- data_frame(MSE = 0, Iteration = 1:i)\n\n    if (i < 20) ylim_max <- 6 else if (i < 50) ylim_max <- 2 else if (i < 75) ylim_max <- 1 else ylim_max <- 0.5\n    if (i < 20) xlim_max <- 50 else  if (i < 80) xlim_max <- 100 else xlim_max <- 130\n\n    p2 <- ggplot(fig_dat2, aes(x = Iteration, y = MSE)) +\n      geom_path(colour = cols_hex[2]) +\n      labs(title = \"\", x = \"Iterations\", y = \"Mean square error\") +\n      xlim(0, xlim_max) +\n      theme_bw() +\n      scale_y_continuous(limits = c(0, ylim_max), labels=fmt_dcimals)\n\n    p3 <- plot_grid(p1, p2, align = \"hv\")\n    print(p3)\n\n   dev.off()\n}\n```\n", "tags": ["R", "MXNet", "DeepLearning", "ggplot2", "MachineLearning"]}