{"context": "\uff11\u5e74\u4ee5\u4e0a\u3082\u524d\u3067\u3059\u304c\u3001edX\u306eScalable Machine learning\u3092\u53d7\u8b1b\u3057\u3066\u3044\u305f\u6642\u306e\u30b3\u30fc\u30c9\u304c\u51fa\u3066\u304d\u305f\u306e\u3067\u3001\u898b\u76f4\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u3053\u306e\u8b1b\u5ea7\u306e\u30cd\u30bf\u304c\u3001Kaggle\u306eCriteo\u306e\u30b3\u30f3\u30da\u306e\u30c7\u30fc\u30bf\n\u3092\u57fa\u306b\u3057\u305fCTR\u4e88\u6e2c\u3067\u3057\u3066\u3001One-hot-encoding\u3084hushing\u3057\u305f\u9ad8\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u3092\nMllib\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb\u3067\u4e88\u6e2c\u3092\u3059\u308b\u3068\u3044\u3063\u305f\u3082\u306e\u3067\u3059\u3002\u4eca\u56de\u306f\u4e88\u6e2c\u7cbe\u5ea6\u305d\u306e\u3082\u306e\u3088\u308a\u3082\u3001\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u3067\u6b21\u5143\u5727\u7e2e\u3057\u305f\u3082\u306e\u304c\u3001OHE\u5316\u3057\u305f\u30c7\u30fc\u30bf\u3068\u6bd4\u3079\u3066\u3069\u306e\u7a0b\u5ea6\u4e88\u6e2c\u7cbe\u5ea6\u306b\u5dee\u304c\u51fa\u308b\u306e\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002\nSpark\u3082\u304b\u306a\u308a\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3060\u3063\u305f\u306e\u3067\u3001\u4eca\u306a\u3089\u307e\u305f\u9055\u3046\u5b9f\u88c5\u306e\u4ed5\u65b9\u304c\u3042\u308b\u3088\u3046\u306a\u6c17\u3082\u3057\u307e\u3059\u304c\u3001\u51fa\u6765\u5408\u3044\u306e\u6a5f\u80fd\u306b\u983c\u3089\u305a\u306b\u5b9f\u88c5\u3057\u305f\u3053\u3068\u3067\u7406\u89e3\u304c\u6df1\u307e\u3063\u305f\u8a18\u61b6\u304c\u3042\u308b\u306e\u3067\u3001\u305d\u306e\u307e\u3093\u307e\u3002\u8ffd\u52a0\u6a5f\u80fd\u306e\u52c9\u5f37\u517c\u306d\u305f\u6bd4\u8f03\u306f\u307e\u305f\u5225\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u884c\u3044\u305f\u3044\u306a\u3068\u601d\u3044\u307e\u3059\u3002 Spark\u306fVirtualbox\u4e0a\u306b\u305f\u3066\u305fUbuntu\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3082\u306e\u3092\u30b7\u30f3\u30b0\u30eb\u30ce\u30fc\u30c9\u3067\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u304a\u304a\u307e\u304b\u306a\u624b\u9806\n\u2460\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n \u5143\u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u3001\u8a55\u4fa1\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u305d\u308c\u305e\u308c\u5206\u5272\n\u2461\u5143\u30c7\u30fc\u30bf\u306bOHE\u3092\u9069\u7528\u3002\u3053\u3053\u3067\u7279\u5fb4\u91cf\u306f20\u4e07\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002\n\u2462OHE\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u5206\u6790(model1)\n\u2463\u30cf\u30c3\u30b7\u30e5\u30c8\u30ea\u30c3\u30af\u3067\u6b21\u5143\u91cf\u306e\u524a\u6e1b(\u304a\u3088\u305d3,000\u7a0b\u5ea6)\n\u2464\u30cf\u30c3\u30b7\u30e5\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u5206\u6790\n -Mllib\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306e\u30e2\u30c7\u30eb(model2)\n -\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3057\u305f\u30e2\u30c7\u30eb(model3)\n\u2465Loglos\u306b\u3088\u308b\u30e2\u30c7\u30eb\u8a55\u4fa1\n -OHE\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model1) vs Hushing\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model2)\n -Hushing\u30c7\u30fc\u30bf\u30fb\u30c7\u30d5\u30a9\u30eb\u30c8\u30e2\u30c7\u30eb(model2) vs Hushing\u30c7\u30fc\u30bf\u30fbGS\u30e2\u30c7\u30eb(model3)\n\n\u30b3\u30fc\u30c9\n\u307e\u305a\u306f\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5206\u5272\u3001\u773a\u3081\u3066\u307f\u307e\u3059\u3002\nimport numpy as np\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\nrawData = (sc.textFile(\"dac_sample.txt\").map(lambda x: x.replace('\\t', ',')))\n\nOneSample = rawData.take(1)\nprint OneSample\n\n\nweights = [.8, .1, .1]\nseed = 42\nrawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n\n#Cache each datasets as it is used repeatedly\nrawTrainData.cache()\nrawValidationData.cache()\nrawTestData.cache()\n\nnTrain = rawTrainData.count()\nnVal = rawValidationData.count()\nnTest = rawTestData.count()\nprint nTrain, nVal, nTest, nTrain + nVal + nTest\n\n\n[u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n79911 10075 10014 100000\n\nOHE\u3092\u5206\u5272\u3057\u305f\u30c7\u30fc\u30bf\u306b\u5f53\u3066\u307e\u3059\u3002\ndef createOneHotDict(inputData):\n    OHEDict = (inputData\n               .flatMap(lambda x: x)\n               .distinct()\n               .zipWithIndex()\n               .collectAsMap())\n    return OHEDict\n\n\ndef parsePoint(point):\n    items = point.split(',')\n    return [(i, item) for i, item in enumerate(items[1:])]\n\ndef oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n    sizeList = [OHEDict[f] for f in rawFeats if f in OHEDict]\n    sortedSizeList = sorted(sizeList)\n    valueList = [1 for f in rawFeats if f in OHEDict ]\n    return SparseVector(numOHEFeats, sortedSizeList, valueList)\n\n\ndef parseOHEPoint(point, OHEDict, numOHEFeats):\n    parsedPoints = parsePoint(point)\n    items = point.split(',')\n    label = items[0]\n    features = oneHotEncoding(parsedPoints, OHEDict, numOHEFeats)\n    return LabeledPoint(label, features)\n\nparsedFeat = rawTrainData.map(parsePoint)\nctrOHEDict = createOneHotDict(parsedFeat)\nnumCtrOHEFeats = len(ctrOHEDict.keys())\n\n\nOHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\nOHEValidationData = rawValidationData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\nOHETestData = rawTestData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\n\nprint ('Feature size after OHE:\\n\\tNumber of features = {0}'\n       .format(numCtrOHEFeats))\n\nprint OHETrainData.take(1)\n\nFeature size after OHE:\n    Number of features = 233286\n[LabeledPoint(0.0, (233286,[386,3077,6799,8264,8862,11800,12802,16125,17551,18566,29331,33132,39525,55794,61786,81396,82659,93573,96929,100677,109699,110646,112132,120260,128596,132397,132803,140620,160675,185498,190370,191146,195925,202664,204273,206055,222737,225958,229942],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n\nOHE\u5f8c\u306e\u30c7\u30fc\u30bf\u306b\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u5b9f\u65bd\u3002\u3068\u308a\u3042\u3048\u305a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u8a2d\u5b9a\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u307e\u307e\u3084\u3063\u3066\u307f\u307e\u3059\u3002\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\n\nnumIters = 100\nstepSize = 1.0\nregParam = 0.01\nregType = 'l2'\nincludeIntercept = True\n\nmodel0 = LogisticRegressionWithSGD.train(data = OHETrainData, \n                                         iterations = numIters, \n                                         step = stepSize, \n                                         regParam = regParam,\n                                         regType = regType,\n                                         intercept = includeIntercept)\n\n#Compute loglos\nfrom math import log, exp \n\ndef computeLogLoss(p, y):\n    epsilon = 10e-12\n    if y == 1:\n        return -log(epsilon + p) if p == 0 else -log(p)\n    elif y == 0:\n        return -log(1 - p + epsilon) if p == 1 else -log(1 - p)\n\ndef getP(x, w, intercept):\n    rawPrediction = x.dot(w) + intercept\n\n    # Bound the raw prediction value\n    rawPrediction = min(rawPrediction, 20)\n    rawPrediction = max(rawPrediction, -20)\n    return 1.0 / (1.0 + exp(-rawPrediction))\n\ndef evaluateResults(model, data):\n    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept), x.label)).sum() / data.count()\n\nlogLossValLR0 = evaluateResults(model0, OHEValidationData)\nprint ('Validation Logloss for model1:\\n\\t = {0}'\n       .format(logLossValLR0))\n\n\u7d20\u3046\u3069\u3093\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3069\u3053\u307e\u3067\u4e88\u6e2c\u3067\u304d\u3066\u308b\u304b\u3001ROC\u66f2\u7dda\u3092\u63cf\u3044\u3066\u307f\u307e\u3059\u3002\u307c\u3061\u307c\u3061\u306e\u7cbe\u5ea6\u3067\u3067\u304d\u3066\u308b\u3088\u3046\u3067\u3057\u305f\u3002\nlabelsAndScores = OHEValidationData.map(lambda lp: (lp.label, getP(lp.features, model0.weights, model0.intercept)))\nlabelsAndWeights = labelsAndScores.collect()\nlabelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\nlabelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n\nlength = labelsByWeight.size\ntruePositives = labelsByWeight.cumsum()\nnumPositive = truePositives[-1]\nfalsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n\ntruePositiveRate = truePositives / numPositive\nfalsePositiveRate = falsePositives / (length - numPositive)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig = plt.figure()\nax = plt.subplot(111)\n\nax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\nax.set_ylabel('True Positive Rate (Sensitivity)')\nax.set_xlabel('False Positive Rate (1 - Specificity)')\nplt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\nplt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)\n\n\n\u6b21\u306b\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30cf\u30c3\u30b7\u30e5\u30c8\u30ea\u30c3\u30af\u3092\u5f53\u3066\u307e\u3059\u3002\nOHE\u5f8c\u306e\u6b21\u5143\u6570\u306f20\u4e07\u7a0b\u5ea6\u3001\u4eca\u56de\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u306e\u30d0\u30b1\u30c3\u30c8\u6570\u30922\u306e15\u4e57\u306732768\u500b\u3068\u3057\u3066\u307e\u3059\u3002\nfrom collections import defaultdict\nimport hashlib\n\ndef hashFunction(numBuckets, rawFeats, printMapping=False):\n    mapping = {}\n    for ind, category in rawFeats:\n        featureString = category + str(ind)\n        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n    sparseFeatures = defaultdict(float)\n    for bucket in mapping.values():\n        sparseFeatures[bucket] += 1.0\n    return dict(sparseFeatures)\n\n\ndef parseHashPoint(point, numBuckets):\n    parsedPoints = parsePoint(point)\n    items = point.split(',')\n    label = items[0]\n    features = hashFunction(numBuckets, parsedPoints, printMapping=False)\n    return LabeledPoint(label, SparseVector(numBuckets, features))\n\nnumBucketsCTR = 2 ** 15\nhashTrainData = rawTrainData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashTrainData.cache()\nhashValidationData = rawValidationData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashValidationData.cache()\nhashTestData = rawTestData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashTestData.cache()\n\nprint ('Feature size after hushing:\\n\\tNumber of features = {0}'\n       .format(numBucketsCTR))\n\nprint hashTrainData.take(1)\n\nFeature size after hushing:\n    Number of features = 32768\n[LabeledPoint(0.0, (32768,[1305,2883,3807,4814,4866,4913,6952,7117,9985,10316,11512,11722,12365,13893,14735,15816,16198,17761,19274,21604,22256,22563,22785,24855,25202,25533,25721,26487,26656,27668,28211,29152,29402,29873,30039,31484,32493,32708],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n\n\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u3092\u3042\u3066\u305f\u5f8c\u306e\u30c7\u30fc\u30bf\u306b\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u5206\u6790\u3002\n\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306e\u3082\u306e\u3068\u3001\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u3057\u305f\u3082\u306e\u3068\u4e8c\u3064\u30e2\u30c7\u30eb\u3092\u4f5c\u308a\u307e\u3059\u3002\n\u307e\u305a\u306f\u7d20\u3046\u3069\u3093\u306e\u65b9\u304b\u3089\u3002\nnumIters = 100\nstepSize = 1.0\nregParam = 0.01\nregType = 'l2'\nincludeIntercept = True\n\nmodel1 = LogisticRegressionWithSGD.train(data = hashTrainData, \n                                         iterations = numIters, \n                                         step = stepSize, \n                                         regParam = regParam,\n                                         regType = regType,\n                                         intercept = includeIntercept)\n\nlogLossValLR1 = evaluateResults(model1, hashValidationData)\nprint ('Validation Logloss for model1:\\n\\t = {0}'\n       .format(logLossValLR1))\n\nValidation Logloss for model1:\n     = 0.482691122185\n\n\u6b21\u306b\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3092\u5b9f\u65bd\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u3081\u305f\u5f8c\u306b\u30e2\u30c7\u30eb\u69cb\u7bc9\u3057\u307e\u3059\u3002\nnumIters = 500\nstepSizes = [0.1, 0.5, 1, 3, 5, 7]\nregParams = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\nregType = 'l2'\nincludeIntercept = True\n\n\n# Initialize variables using values from initial model training\nbestModel = None\nbestLogLoss = 1e10\nlogLoss = np.zeros((len(stepSizes),len(regParams)))\n\nfor i in xrange(len(stepSizes)):\n    for j in xrange(len(regParams)):\n        model = LogisticRegressionWithSGD.train(data = hashTrainData, \n                                                iterations = numIters, \n                                                step = stepSizes[i], \n                                                regParam = regParams[j],\n                                                regType = regType,\n                                                intercept = includeIntercept)\n\n        logLoss[i, j] = evaluateResults(model, hashValidationData)\n\n        if (logLoss[i, j] < bestLogLoss):\n            bestModel = model\n            bestStepsize = stepSizes[i]\n            bestParams = regParams[j]\n            bestLogLoss = logLoss[i,j]\n\n\nprint ('best parameters:\\n\\tBest Stepsize = {0:.3f}\\n\\tBest Rarams = {1:.3f}'\n       .format(bestStepsize, bestParams))\n\nprint bestParams,logLoss\n\n%matplotlib inline\nfrom matplotlib.colors import LinearSegmentedColormap\n\n\nnumRows, numCols = len(stepSizes), len(regParams)\nlogLoss = np.array(logLoss)\nlogLoss.shape = (numRows, numCols)\n\nfig = plt.figure()\nax = plt.subplot(111)\n\n\nax.set_xticklabels(regParams), ax.set_yticklabels(stepSizes)\nax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Step Size')\n\ncolors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\nimage = plt.imshow(logLoss,interpolation='nearest', aspect='auto',\n                    cmap = colors)\npass\n\nbest parameters:\n    Best Stepsize = 7.000\n    Best Rarams = 0.000\n1e-07 [[ 0.51910013  0.51910015  0.51910029  0.51910175  0.51911634  0.51926454]\n [ 0.48589729  0.4858974   0.48589847  0.48590921  0.48605802  0.48715278]\n [ 0.47476165  0.47476184  0.47476375  0.47478289  0.4750353   0.47731296]\n [ 0.46130496  0.46130542  0.46131004  0.46137622  0.46208485  0.46799094]\n [ 0.45587263  0.45587339  0.45588094  0.45600577  0.45715016  0.465792  ]\n [ 0.45268179  0.45268281  0.45270706  0.45286577  0.45438559  0.46488834]]\n\n\n\u6700\u671f\u306b\u4e0b\u8a18\u306e\u8ef8\u306b\u3088\u308bLoglos\u306b\u3088\u308b\u30e2\u30c7\u30eb\u8a55\u4fa1\u3067\u3059\u3002\n -OHE\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model1) vs Hushing\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model2)\n -Hushing\u30c7\u30fc\u30bf\u30fb\u30c7\u30d5\u30a9\u30eb\u30c8\u30e2\u30c7\u30eb(model2) vs Hushing\u30c7\u30fc\u30bf\u30fbGS\u30e2\u30c7\u30eb(model3)\nLogloss\u3067\u306e\u6bd4\u8f03\u3067\u306f\u3001\u6b21\u5143\u91cf\u3092\u5727\u7e2e\u3057\u305fHushing\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u3082\u4e88\u6e2c\u7cbe\u5ea6\u304c\u843d\u3061\u3066\u3044\u307e\u305b\u3093\u306d\u3001\u9762\u767d\u3044\u3067\u3059\u3002\nlogLossTestLR0 = evaluateResults(model0, OHETestData)\nlogLossTestLR1 = evaluateResults(model1, hashTestData)\nlogLossTestLRbest = evaluateResults(bestModel, hashTestData)\n\nprint ('OHECoding & Hashed Features Test Logloss:\\n\\tOHEModel = {0:.3f}\\n\\thashModel = {1:.3f}'\n       .format(logLossTestLR0, logLossTestLR1))\n\nprint ('Hashed Features Validation Test Logloss:\\n\\tBaseModel = {0:.3f}\\n\\tBestModel = {1:.3f}'\n       .format(logLossTestLR1, logLossTestLRbest))\n\nOHECoding & Hashed Features Test Logloss:\n    OHEModel = 0.490\n    hashModel = 0.490\nHashed Features Validation Test Logloss:\n    BaseModel = 0.490\n    BestModel = 0.460\n\n\uff11\u5e74\u4ee5\u4e0a\u3082\u524d\u3067\u3059\u304c\u3001[edX\u306eScalable Machine learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/courseware/082cd67bd37e4576871b9845583455b0/)\u3092\u53d7\u8b1b\u3057\u3066\u3044\u305f\u6642\u306e\u30b3\u30fc\u30c9\u304c\u51fa\u3066\u304d\u305f\u306e\u3067\u3001\u898b\u76f4\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u3053\u306e\u8b1b\u5ea7\u306e\u30cd\u30bf\u304c\u3001[Kaggle\u306eCriteo\u306e\u30b3\u30f3\u30da](https://www.kaggle.com/c/criteo-display-ad-challenge)\u306e\u30c7\u30fc\u30bf\n\u3092\u57fa\u306b\u3057\u305fCTR\u4e88\u6e2c\u3067\u3057\u3066\u3001One-hot-encoding\u3084hushing\u3057\u305f\u9ad8\u6b21\u5143\u306e\u30c7\u30fc\u30bf\u3092\n[Mllib\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u30e2\u30c7\u30eb](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionModel)\u3067\u4e88\u6e2c\u3092\u3059\u308b\u3068\u3044\u3063\u305f\u3082\u306e\u3067\u3059\u3002\u4eca\u56de\u306f\u4e88\u6e2c\u7cbe\u5ea6\u305d\u306e\u3082\u306e\u3088\u308a\u3082\u3001\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u3067\u6b21\u5143\u5727\u7e2e\u3057\u305f\u3082\u306e\u304c\u3001OHE\u5316\u3057\u305f\u30c7\u30fc\u30bf\u3068\u6bd4\u3079\u3066\u3069\u306e\u7a0b\u5ea6\u4e88\u6e2c\u7cbe\u5ea6\u306b\u5dee\u304c\u51fa\u308b\u306e\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002\n\n\nSpark\u3082\u304b\u306a\u308a\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3060\u3063\u305f\u306e\u3067\u3001\u4eca\u306a\u3089\u307e\u305f\u9055\u3046\u5b9f\u88c5\u306e\u4ed5\u65b9\u304c\u3042\u308b\u3088\u3046\u306a\u6c17\u3082\u3057\u307e\u3059\u304c\u3001\u51fa\u6765\u5408\u3044\u306e\u6a5f\u80fd\u306b\u983c\u3089\u305a\u306b\u5b9f\u88c5\u3057\u305f\u3053\u3068\u3067\u7406\u89e3\u304c\u6df1\u307e\u3063\u305f\u8a18\u61b6\u304c\u3042\u308b\u306e\u3067\u3001\u305d\u306e\u307e\u3093\u307e\u3002\u8ffd\u52a0\u6a5f\u80fd\u306e\u52c9\u5f37\u517c\u306d\u305f\u6bd4\u8f03\u306f\u307e\u305f\u5225\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u884c\u3044\u305f\u3044\u306a\u3068\u601d\u3044\u307e\u3059\u3002 Spark\u306fVirtualbox\u4e0a\u306b\u305f\u3066\u305fUbuntu\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3082\u306e\u3092\u30b7\u30f3\u30b0\u30eb\u30ce\u30fc\u30c9\u3067\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002\n\n#### \u304a\u304a\u307e\u304b\u306a\u624b\u9806\n\u2460\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n \u5143\u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u3001\u8a55\u4fa1\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u305d\u308c\u305e\u308c\u5206\u5272\n\u2461\u5143\u30c7\u30fc\u30bf\u306bOHE\u3092\u9069\u7528\u3002\u3053\u3053\u3067\u7279\u5fb4\u91cf\u306f20\u4e07\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002\n\u2462OHE\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u5206\u6790(model1)\n\u2463\u30cf\u30c3\u30b7\u30e5\u30c8\u30ea\u30c3\u30af\u3067\u6b21\u5143\u91cf\u306e\u524a\u6e1b(\u304a\u3088\u305d3,000\u7a0b\u5ea6)\n\u2464\u30cf\u30c3\u30b7\u30e5\u5f8c\u306e\u30c7\u30fc\u30bf\u3067\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u5206\u6790\n -Mllib\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306e\u30e2\u30c7\u30eb(model2)\n -\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u3057\u305f\u30e2\u30c7\u30eb(model3)\n\u2465Loglos\u306b\u3088\u308b\u30e2\u30c7\u30eb\u8a55\u4fa1\n -OHE\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model1) vs Hushing\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model2)\n -Hushing\u30c7\u30fc\u30bf\u30fb\u30c7\u30d5\u30a9\u30eb\u30c8\u30e2\u30c7\u30eb(model2) vs Hushing\u30c7\u30fc\u30bf\u30fbGS\u30e2\u30c7\u30eb(model3)\n\n\n#### \u30b3\u30fc\u30c9\n\u307e\u305a\u306f\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5206\u5272\u3001\u773a\u3081\u3066\u307f\u307e\u3059\u3002\n\n```python\nimport numpy as np\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\nrawData = (sc.textFile(\"dac_sample.txt\").map(lambda x: x.replace('\\t', ',')))\n\nOneSample = rawData.take(1)\nprint OneSample\n\n\nweights = [.8, .1, .1]\nseed = 42\nrawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)\n\n#Cache each datasets as it is used repeatedly\nrawTrainData.cache()\nrawValidationData.cache()\nrawTestData.cache()\n\nnTrain = rawTrainData.count()\nnVal = rawValidationData.count()\nnTest = rawTestData.count()\nprint nTrain, nVal, nTest, nTrain + nVal + nTest\n\n\n[u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n79911 10075 10014 100000\n```\n\n\nOHE\u3092\u5206\u5272\u3057\u305f\u30c7\u30fc\u30bf\u306b\u5f53\u3066\u307e\u3059\u3002\n\n```python\ndef createOneHotDict(inputData):\n    OHEDict = (inputData\n               .flatMap(lambda x: x)\n               .distinct()\n               .zipWithIndex()\n               .collectAsMap())\n    return OHEDict\n\n\ndef parsePoint(point):\n    items = point.split(',')\n    return [(i, item) for i, item in enumerate(items[1:])]\n    \ndef oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n    sizeList = [OHEDict[f] for f in rawFeats if f in OHEDict]\n    sortedSizeList = sorted(sizeList)\n    valueList = [1 for f in rawFeats if f in OHEDict ]\n    return SparseVector(numOHEFeats, sortedSizeList, valueList)\n\n\ndef parseOHEPoint(point, OHEDict, numOHEFeats):\n    parsedPoints = parsePoint(point)\n    items = point.split(',')\n    label = items[0]\n    features = oneHotEncoding(parsedPoints, OHEDict, numOHEFeats)\n    return LabeledPoint(label, features)\n\nparsedFeat = rawTrainData.map(parsePoint)\nctrOHEDict = createOneHotDict(parsedFeat)\nnumCtrOHEFeats = len(ctrOHEDict.keys())\n\n\nOHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\nOHEValidationData = rawValidationData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\nOHETestData = rawTestData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)).cache()\n\nprint ('Feature size after OHE:\\n\\tNumber of features = {0}'\n       .format(numCtrOHEFeats))\n\nprint OHETrainData.take(1)\n\nFeature size after OHE:\n\tNumber of features = 233286\n[LabeledPoint(0.0, (233286,[386,3077,6799,8264,8862,11800,12802,16125,17551,18566,29331,33132,39525,55794,61786,81396,82659,93573,96929,100677,109699,110646,112132,120260,128596,132397,132803,140620,160675,185498,190370,191146,195925,202664,204273,206055,222737,225958,229942],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n```\n \nOHE\u5f8c\u306e\u30c7\u30fc\u30bf\u306b\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u5b9f\u65bd\u3002\u3068\u308a\u3042\u3048\u305a\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u8a2d\u5b9a\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u307e\u307e\u3084\u3063\u3066\u307f\u307e\u3059\u3002\n\n```python\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\n\nnumIters = 100\nstepSize = 1.0\nregParam = 0.01\nregType = 'l2'\nincludeIntercept = True\n\nmodel0 = LogisticRegressionWithSGD.train(data = OHETrainData, \n                                         iterations = numIters, \n                                         step = stepSize, \n                                         regParam = regParam,\n                                         regType = regType,\n                                         intercept = includeIntercept)\n\n#Compute loglos\nfrom math import log, exp \n\ndef computeLogLoss(p, y):\n    epsilon = 10e-12\n    if y == 1:\n        return -log(epsilon + p) if p == 0 else -log(p)\n    elif y == 0:\n        return -log(1 - p + epsilon) if p == 1 else -log(1 - p)\n\ndef getP(x, w, intercept):\n    rawPrediction = x.dot(w) + intercept\n\n    # Bound the raw prediction value\n    rawPrediction = min(rawPrediction, 20)\n    rawPrediction = max(rawPrediction, -20)\n    return 1.0 / (1.0 + exp(-rawPrediction))\n\ndef evaluateResults(model, data):\n    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept), x.label)).sum() / data.count()\n\nlogLossValLR0 = evaluateResults(model0, OHEValidationData)\nprint ('Validation Logloss for model1:\\n\\t = {0}'\n       .format(logLossValLR0))\n```\n \n\u7d20\u3046\u3069\u3093\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3069\u3053\u307e\u3067\u4e88\u6e2c\u3067\u304d\u3066\u308b\u304b\u3001ROC\u66f2\u7dda\u3092\u63cf\u3044\u3066\u307f\u307e\u3059\u3002\u307c\u3061\u307c\u3061\u306e\u7cbe\u5ea6\u3067\u3067\u304d\u3066\u308b\u3088\u3046\u3067\u3057\u305f\u3002\n\n\n```python\nlabelsAndScores = OHEValidationData.map(lambda lp: (lp.label, getP(lp.features, model0.weights, model0.intercept)))\nlabelsAndWeights = labelsAndScores.collect()\nlabelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\nlabelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n\nlength = labelsByWeight.size\ntruePositives = labelsByWeight.cumsum()\nnumPositive = truePositives[-1]\nfalsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n\ntruePositiveRate = truePositives / numPositive\nfalsePositiveRate = falsePositives / (length - numPositive)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig = plt.figure()\nax = plt.subplot(111)\n\nax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\nax.set_ylabel('True Positive Rate (Sensitivity)')\nax.set_xlabel('False Positive Rate (1 - Specificity)')\nplt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\nplt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)\n```\n\n![ROC.png](https://qiita-image-store.s3.amazonaws.com/0/62779/29902093-419a-aab9-d1d1-608718dfc7ef.png)\n\n\n\u6b21\u306b\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30cf\u30c3\u30b7\u30e5\u30c8\u30ea\u30c3\u30af\u3092\u5f53\u3066\u307e\u3059\u3002\nOHE\u5f8c\u306e\u6b21\u5143\u6570\u306f20\u4e07\u7a0b\u5ea6\u3001\u4eca\u56de\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u306e\u30d0\u30b1\u30c3\u30c8\u6570\u30922\u306e15\u4e57\u306732768\u500b\u3068\u3057\u3066\u307e\u3059\u3002\n\n```python\nfrom collections import defaultdict\nimport hashlib\n\ndef hashFunction(numBuckets, rawFeats, printMapping=False):\n    mapping = {}\n    for ind, category in rawFeats:\n        featureString = category + str(ind)\n        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n    sparseFeatures = defaultdict(float)\n    for bucket in mapping.values():\n        sparseFeatures[bucket] += 1.0\n    return dict(sparseFeatures)\n\n\ndef parseHashPoint(point, numBuckets):\n    parsedPoints = parsePoint(point)\n    items = point.split(',')\n    label = items[0]\n    features = hashFunction(numBuckets, parsedPoints, printMapping=False)\n    return LabeledPoint(label, SparseVector(numBuckets, features))\n\nnumBucketsCTR = 2 ** 15\nhashTrainData = rawTrainData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashTrainData.cache()\nhashValidationData = rawValidationData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashValidationData.cache()\nhashTestData = rawTestData.map(lambda x: parseHashPoint(x, numBucketsCTR))\nhashTestData.cache()\n\nprint ('Feature size after hushing:\\n\\tNumber of features = {0}'\n       .format(numBucketsCTR))\n\nprint hashTrainData.take(1)\n\nFeature size after hushing:\n\tNumber of features = 32768\n[LabeledPoint(0.0, (32768,[1305,2883,3807,4814,4866,4913,6952,7117,9985,10316,11512,11722,12365,13893,14735,15816,16198,17761,19274,21604,22256,22563,22785,24855,25202,25533,25721,26487,26656,27668,28211,29152,29402,29873,30039,31484,32493,32708],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n``` \n\n\u30cf\u30c3\u30b7\u30e5\u95a2\u6570\u3092\u3042\u3066\u305f\u5f8c\u306e\u30c7\u30fc\u30bf\u306b\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u5206\u6790\u3002\n\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306e\u3082\u306e\u3068\u3001\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u3057\u305f\u3082\u306e\u3068\u4e8c\u3064\u30e2\u30c7\u30eb\u3092\u4f5c\u308a\u307e\u3059\u3002\n\n\u307e\u305a\u306f\u7d20\u3046\u3069\u3093\u306e\u65b9\u304b\u3089\u3002\n\n```python\nnumIters = 100\nstepSize = 1.0\nregParam = 0.01\nregType = 'l2'\nincludeIntercept = True\n\nmodel1 = LogisticRegressionWithSGD.train(data = hashTrainData, \n                                         iterations = numIters, \n                                         step = stepSize, \n                                         regParam = regParam,\n                                         regType = regType,\n                                         intercept = includeIntercept)\n\nlogLossValLR1 = evaluateResults(model1, hashValidationData)\nprint ('Validation Logloss for model1:\\n\\t = {0}'\n       .format(logLossValLR1))\n\nValidation Logloss for model1:\n\t = 0.482691122185\n```\n \n\u6b21\u306b\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3092\u5b9f\u65bd\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u3081\u305f\u5f8c\u306b\u30e2\u30c7\u30eb\u69cb\u7bc9\u3057\u307e\u3059\u3002\n\n```python\nnumIters = 500\nstepSizes = [0.1, 0.5, 1, 3, 5, 7]\nregParams = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\nregType = 'l2'\nincludeIntercept = True\n\n\n# Initialize variables using values from initial model training\nbestModel = None\nbestLogLoss = 1e10\nlogLoss = np.zeros((len(stepSizes),len(regParams)))\n\nfor i in xrange(len(stepSizes)):\n    for j in xrange(len(regParams)):\n        model = LogisticRegressionWithSGD.train(data = hashTrainData, \n                                                iterations = numIters, \n                                                step = stepSizes[i], \n                                                regParam = regParams[j],\n                                                regType = regType,\n                                                intercept = includeIntercept)\n        \n        logLoss[i, j] = evaluateResults(model, hashValidationData)\n        \n        if (logLoss[i, j] < bestLogLoss):\n            bestModel = model\n            bestStepsize = stepSizes[i]\n            bestParams = regParams[j]\n            bestLogLoss = logLoss[i,j]\n            \n\nprint ('best parameters:\\n\\tBest Stepsize = {0:.3f}\\n\\tBest Rarams = {1:.3f}'\n       .format(bestStepsize, bestParams))\n\nprint bestParams,logLoss\n\n%matplotlib inline\nfrom matplotlib.colors import LinearSegmentedColormap\n\n\nnumRows, numCols = len(stepSizes), len(regParams)\nlogLoss = np.array(logLoss)\nlogLoss.shape = (numRows, numCols)\n\nfig = plt.figure()\nax = plt.subplot(111)\n\n\nax.set_xticklabels(regParams), ax.set_yticklabels(stepSizes)\nax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Step Size')\n\ncolors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\nimage = plt.imshow(logLoss,interpolation='nearest', aspect='auto',\n                    cmap = colors)\npass\n\nbest parameters:\n\tBest Stepsize = 7.000\n\tBest Rarams = 0.000\n1e-07 [[ 0.51910013  0.51910015  0.51910029  0.51910175  0.51911634  0.51926454]\n [ 0.48589729  0.4858974   0.48589847  0.48590921  0.48605802  0.48715278]\n [ 0.47476165  0.47476184  0.47476375  0.47478289  0.4750353   0.47731296]\n [ 0.46130496  0.46130542  0.46131004  0.46137622  0.46208485  0.46799094]\n [ 0.45587263  0.45587339  0.45588094  0.45600577  0.45715016  0.465792  ]\n [ 0.45268179  0.45268281  0.45270706  0.45286577  0.45438559  0.46488834]]\n```\n![GridSearch.png](https://qiita-image-store.s3.amazonaws.com/0/62779/df55574c-d329-014f-1319-0fe61f2ed060.png)\n\n\n\n\u6700\u671f\u306b\u4e0b\u8a18\u306e\u8ef8\u306b\u3088\u308bLoglos\u306b\u3088\u308b\u30e2\u30c7\u30eb\u8a55\u4fa1\u3067\u3059\u3002\n -OHE\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model1) vs Hushing\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(model2)\n -Hushing\u30c7\u30fc\u30bf\u30fb\u30c7\u30d5\u30a9\u30eb\u30c8\u30e2\u30c7\u30eb(model2) vs Hushing\u30c7\u30fc\u30bf\u30fbGS\u30e2\u30c7\u30eb(model3)\n\nLogloss\u3067\u306e\u6bd4\u8f03\u3067\u306f\u3001\u6b21\u5143\u91cf\u3092\u5727\u7e2e\u3057\u305fHushing\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u3082\u4e88\u6e2c\u7cbe\u5ea6\u304c\u843d\u3061\u3066\u3044\u307e\u305b\u3093\u306d\u3001\u9762\u767d\u3044\u3067\u3059\u3002\n\n```python\nlogLossTestLR0 = evaluateResults(model0, OHETestData)\nlogLossTestLR1 = evaluateResults(model1, hashTestData)\nlogLossTestLRbest = evaluateResults(bestModel, hashTestData)\n\nprint ('OHECoding & Hashed Features Test Logloss:\\n\\tOHEModel = {0:.3f}\\n\\thashModel = {1:.3f}'\n       .format(logLossTestLR0, logLossTestLR1))\n\nprint ('Hashed Features Validation Test Logloss:\\n\\tBaseModel = {0:.3f}\\n\\tBestModel = {1:.3f}'\n       .format(logLossTestLR1, logLossTestLRbest))\n\nOHECoding & Hashed Features Test Logloss:\n\tOHEModel = 0.490\n\thashModel = 0.490\nHashed Features Validation Test Logloss:\n\tBaseModel = 0.490\n\tBestModel = 0.460\n```\n", "tags": ["Spark", "MLlib", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2", "Python"]}