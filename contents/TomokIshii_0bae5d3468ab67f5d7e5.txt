{"context": " More than 1 year has passed since last update.\u5143\u3005\uff0cDeep Learning\u7528\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\"Theano\"\u306e\u5b66\u7fd2\u3092\u9032\u3081\u3066\u3044\u305f\u304c\uff0c\u5148\u6708(2015/11)\u516c\u958b\u3055\u308c\u305f\"TensorFlow\"\u306b\u8208\u5473\u3092\u60f9\u304b\u308c\u6700\u8fd1\u306f\u305d\u3061\u3089\u3092\u30e1\u30a4\u30f3\u3067\u4f7f\u3063\u3066\u3044\u305f\uff0eTensorFlow\u3092\u89e6\u308a\u306a\u304c\u3089\uff0c\u300c\u3053\u3053\u306f\"Theano\"\u306b\u8fd1\u3044\uff0c\u3053\u3053\u306f\u304b\u306a\u308a\u9055\u3046\u300d\u3068\u611f\u3058\u3066\u3044\u305f\u304c\uff0c\u4e21\u8005\u306e\u9055\u3044\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u7c21\u5358\u306a\u30b3\u30fc\u30c9\u3092\"TensorFlow\"\u304b\u3089\u201dTheano\"\u306b\u79fb\u690d\u3057\u3066\u307f\u305f\uff0e\uff08\u666e\u901a\uff0c\u65b9\u5411\u304c\u9006\u304b\u3068\u601d\u3044\u307e\u3059\u304c\uff0e\uff09\n\n\u6982\u8981\uff1a \"Theano\" vs. \"TensorFlow\"\n\u307e\u305a\uff12\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6982\u8981\u3092\u6bd4\u8f03\u3059\u308b\uff0e\n\n\n\n\u9805\u76ee\nTheano\nTensorFlow\n\n\n\n\n\u958b\u767a\u4e3b\u4f53\n\u30a2\u30ab\u30c7\u30df\u30c3\u30af (University of Montreal)\u3000\n\u4f01\u696d (Google)\n\n\n\u516c\u958b\u5e74\u5ea6\n2010\u5e74\u3050\u3089\u3044(?)\n2015\u5e74\n\n\nTensor operation\nsupport\nsupport\n\n\nnumpy 'basic' level \u6a5f\u80fd\nsupport\nsupport\n\n\n\u81ea\u52d5\u5fae\u5206 (Graph Transformation)\nsupport\nsupport\n\n\nGPU\u6f14\u7b97\nsupport\nsupport\n\n\nGraph Visualization\nsupport (\u4e00\u5fdc\u3042\u308a\u307e\u3059\uff09\nsupport (\u3054\u5b58\u77e5 Tensorboard)\n\n\nOptimizer\nnot support (built-in\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u610f\u5473\uff09\n\u3044\u308d\u3044\u308dsupport\n\n\nNeural Network\u7cfb\u306e\u95a2\u6570\nsupport\n(\u3044\u308d\u3044\u308d) support\n\n\n\n\u6a5f\u80fd\u9762\u3067\u306f\uff0c\u4e0a\u306e\u8868\u306e\u6700\u5f8c\u306e\u90e8\u5206\u306b\u9055\u3044\u304c\u898b\u3089\u308c\u308b\uff0eTheano\u3067\u306f\u7d30\u304b\u3044\u90e8\u5206\u3092\u81ea\u524d\u3067\u7528\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u306e\u306b\u5bfe\u3057\uff0cThensorFlow\u306b\u306f\u30e9\u30a4\u30d6\u30e9\u30ea\u95a2\u6570\u3068\u3057\u3066\u521d\u3081\u304b\u3089\u3044\u308d\u3044\u308d\u6e96\u5099\u3055\u308c\u3066\u3044\u308b\uff0c\u3068\u3044\u3063\u305f\u5370\u8c61\u3067\u3042\u308b\uff0e\n\uff08Theano\u3067\u8a73\u7d30\u90e8\u304b\u3089\u30d7\u30ed\u30b0\u30e9\u30e0\u3057\u305f\u304f\u306a\u3044\u65b9\u306b\u306f\uff0cTheano\u30d9\u30fc\u30b9\u306e\u9ad8\u6a5f\u80fd\u30e9\u30a4\u30d6\u30e9\u30ea\uff0c\u4f8b\u3048\u3070 Pylearn2 \u304c\u3042\u308a\u307e\u3059\uff0e\uff09\n\n\u30b3\u30fc\u30c9\u3067\u6bd4\u8f03\uff08Neural Network\u306e\u30e2\u30c7\u30eb\u5316)\n\u3053\u3053\u3067\u306f\uff0c\"Theano\"\u3068\u201dThensorFlow\"\u3067\u306f\u307b\u3068\u3093\u3069\u540c\u3058\u3088\u3046\u306b\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\u524d\u56de\u306e\u8a18\u4e8b\u3067\u7d39\u4ecb\u3057\u305f\u591a\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046MLP(Multi-layer Perceptron)\u306e\u30b3\u30fc\u30c9\u304b\u3089\u629c\u7c8b\u3059\u308b\uff0e\nTensorFlow\u7248\n# Hidden Layer\nclass HiddenLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n\n        w_h = tf.Variable(tf.random_normal([n_in,n_out],mean=0.0,stddev=0.05))\n        b_h = tf.Variable(tf.zeros([n_out]))\n\n        self.w = w_h\n        self.b = b_h\n        self.params = [self.w, self.b]\n\n    def output(self):\n        linarg = tf.matmul(self.input, self.w) + self.b\n        self.output = tf.nn.relu(linarg)  # switch sigmoid() to relu()\n\n        return self.output\n\n# Read-out Layer\nclass ReadOutLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n\n        w_o = tf.Variable(tf.random_normal([n_in,n_out],mean=0.0,stddev=0.05))\n        b_o = tf.Variable(tf.zeros([n_out]))\n\n        self.w = w_o\n        self.b = b_o\n        self.params = [self.w, self.b]\n\n    def output(self):\n        linarg = tf.matmul(self.input, self.w) + self.b\n        self.output = tf.nn.softmax(linarg)  \n\n        return self.output\n\n\nTheano\u7248\n# Hidden Layer\nclass HiddenLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n\n        w_h = theano.shared(floatX(np.random.standard_normal([n_in, n_out])) \n                             * 0.05) \n        b_h = theano.shared(floatX(np.zeros(n_out)))\n\n        self.w = w_h\n        self.b = b_h\n        self.params = [self.w, self.b]\n\n    def output(self):\n        linarg = T.dot(self.input, self.w) + self.b\n        # self.output = T.nnet.relu(linarg)\n        self.output = T.nnet.sigmoid(linarg)\n\n        return self.output\n\n# Read-out Layer\nclass ReadOutLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n\n        w_o = theano.shared(floatX(np.random.standard_normal([n_in,n_out]))\n                             * 0.05)\n        b_o = theano.shared(floatX(np.zeros(n_out)))\n\n        self.w = w_o\n        self.b = b_o\n        self.params = [self.w, self.b]\n\n    def output(self):\n        linarg = T.dot(self.input, self.w) + self.b\n        self.output = T.nnet.softmax(linarg)  \n\n        return self.output\n\n\n\u7d30\u304b\u304f\u898b\u306a\u3051\u308c\u3070\u300c\"tf.\" \u3092\"T.\"\u306b\u5909\u3048\u305f\u3060\u3051\u300d\u306e\u9055\u3044\u3067\u3042\u308b\uff0e\u6d3b\u6027\u5316\u95a2\u6570\u306b\u3064\u3044\u3066\u306f\u5f8c\u767a\u306eTenfsorFlow\u306e\u65b9\u304c \"tf.nn.relu()\"\u7b49\u3082\u3042\u3063\u3066\u4fbf\u5229\u3068\u601d\u3063\u3066\u3044\u305f\u304c\uff0cTheano\u3067\u3082 ver.0.7.1\u304b\u3089relu() (Rectified Linear Unit) \u3092\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u3088\u3046\u3067\u3042\u308b\uff0e\uff08\u672c\u8a18\u4e8b\u3067\u4f7f\u7528\u3057\u305f\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\uff0cPython\u3000ver.2.7.10, TensorFlow ver.0.5.0, Theano ver.0.7.0 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\u307e\u305f\uff0cSoftmax\u95a2\u6570\u3082\u5f53\u7136\u306e\u3053\u3068\u306a\u304c\u3089\u4e21\u8005\u3067\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\uff0e\n\n\u30b3\u30fc\u30c9\u306e\u6bd4\u8f03\uff08\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\uff09\n\u3053\u3053\u3067\u306f\u4e21\u8005(TensorFlow vs. Theano)\u3067\u9055\u3044\u304c\u307f\u3089\u308c\u308b\uff0eTensorFlow\u3067\u306f\uff0cOptimizer\u306e\u30af\u30e9\u30b9\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u5145\u5b9f\u3057\u3066\u3044\u308b\u4e00\u65b9\uff0cTheano\u3067\u306fOptimizer\u306e\u30af\u30e9\u30b9\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u81ea\u5206\u3067\u7528\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\nTensorFlow\u7248(AdagradOptimzer\u4f7f\u7528\u4f8b)\n    # Train\n    optimizer = tf.train.AdagradOptimizer(0.01)\n    train_op = optimizer.minimize(loss)\n\n    init = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n        sess.run(init)\n        print('Training...')\n        for i in range(10001):\n            train_op.run({x: train_x, y_: train_y})\n            if i % 1000 == 0:                # echo status on screen\n                train_accuracy = accuracy.eval({x: train_x, y_: train_y})\n                print(' step, accurary = %6d: %8.3f' % (i, train_accuracy))\n\n\nTensorFlow\u3067\u306f\uff0coptimizer\u3092\u6307\u5b9a\u3057\u3066\uff0cSession\u3092\u52d5\u304b\u3059\uff0eSeesion\u306e\u4e2d\u3067Train Data\u3092Feed_dict\u306e\u5f62\u3067\u4f9b\u7d66\u3059\u308b\u3068\u3044\u3046\u6d41\u308c\uff0e\nTheano\u7248(Adagrad\u3092\u5b9f\u88c5\u3057\u305f\u4f8b)\n\u3067\u304d\u308b\u3060\u3051\u95a2\u6570\u306ecall\u306e\u4ed5\u65b9\u3092TensorFlow\u306e\u30b3\u30fc\u30c9\u306b\u8fd1\u3065\u3051\u308b\u3088\u3046\u306b\u66f8\u3044\u305f\u3064\u3082\u308a\u3060\u304c\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n# Optimizers - (GradientDescent), AdaGrad\nclass Optimizer(object):\n    def __init__(self, params, learning_rate=0.01):\n        self.lr = learning_rate\n        self.params = params\n\n    def minimize(self, loss):\n        self.gradparams = [T.grad(loss, param) for param in params]\n\nclass AdagradOptimizer(Optimizer):\n    def __init__(self, params, learning_rate=0.01, eps=1.e-6):\n        super(AdagradOptimizer, self).__init__(params, learning_rate)\n        self.eps = eps\n        self.accugrads = [theano.shared(floatX(np.zeros(t.shape.eval())),\n                          'accugrad') for t in self.params\n                         ]\n    def minimize(self, loss):\n        super(AdagradOptimizer, self).minimize(loss)\n        self.updates = OrderedDict()    # for Theano's rule\n\n        for accugrad, param, gparam in zip(\n                              self.accugrads, self.params, self.gradparams):\n            agrad = accugrad + gparam * gparam\n            dx = - (self.lr / T.sqrt(agrad + self.eps)) * gparam\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n\n\n\u3053\u308c\u3089\u3092\u4f7f\u3063\u3066\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\uff08\u5b66\u7fd2\uff09\u3092\u8d70\u3089\u305b\u308b\uff0e\n    # Train\n    myoptimizer = AdagradOptimizer(params, learning_rate=0.01, eps=1.e-8)\n    one_update = myoptimizer.minimize(loss)\n\n    # Compile ... define theano.function\n    train_model = theano.function(\n        inputs=[],\n        outputs=[loss, accuracy],\n        updates=one_update,\n        givens=[(x, strain_x), (y_, strain_y)],\n        allow_input_downcast=True\n    )\n\n    n_epochs = 10001\n    epoch = 0\n\n    while (epoch < n_epochs):\n        epoch += 1\n        loss, accu = train_model()\n        if epoch % 1000 == 0:\n            print('epoch[%5d] : cost =%8.4f, accyracy =%8.4f' % (epoch, loss, accu))\n\n\n\u3053\u306e\u3088\u3046\u306b\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\u306e\u90e8\u5206\u306b\u304a\u3044\u3066\uff0c\nTensorFlow\u3067\u306f...\n\u5909\u6570\u3092\u521d\u671f\u5316\u3057\u305f\u5f8c\uff0cSession\u3092\u8d70\u3089\u305b\u308b\uff0e\u8a13\u7df4\u30c7\u30fc\u30bf\u306f\uff0c\u30bb\u30c3\u30b7\u30e7\u30f3\u4e2d\u306b op.run( {Feed_dict})\u306e\u3088\u3046\u306a\u5f62\u3067\u4e0e\u3048\u308b\uff0e\nTheano\u3067\u306f...\ntheano.function()\u306e\u3068\u3053\u308d\u3067\uff08\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4f9b\u7d66\u3092\u542b\u3081\uff09\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u306e\u6d41\u308c\u3092\u5b9a\u7fa9\u3059\u308b\uff0e\u5b9a\u7fa9\u3057\u305f\u95a2\u6570(theano.function)\u3092\u7528\u3044\u3066\uff0c\u53cd\u5fa9\u5b66\u7fd2\u8a08\u7b97\u3092\u5b9f\u884c\u3059\u308b\uff0c\n\u3068\u3044\u3046\u9055\u3044\u304c\u898b\u3089\u308c\u308b\uff0e\n\u201dTheano\"\u306e\u5b66\u7fd2\u3092\u59cb\u3081\u305f\u5f53\u521d\u306f\uff0c\u3053\u306e theano.function() \u306e\u4f7f\u3044\u65b9\u306b\u60a9\u3093\u3060\u8a18\u61b6\u304c\u3042\u308b\u304c\uff0c\u4e0a\u8a18\u306e\u3088\u3046\u306b\"TensorFlow\"\u3068\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3067\uff0ctheano.function() \u3078\u306e\u7406\u89e3\u304c\u6df1\u307e\u3063\u305f\uff0e\uff08\u9055\u3046\u898b\u65b9\u3092\u3059\u308c\u3070\uff0ctheano.function() \u306b\u3064\u3044\u3066\u304d\u3061\u3093\u3068\u7406\u89e3\u3057\u3066\u3044\u308c\u3070\uff0cTheano\u3092\u3046\u307e\u304f\u4f7f\u3044\u3053\u306a\u305b\u308b\u3068\u601d\u3044\u307e\u3059\uff0e\uff09\n\n\u307e\u3068\u3081\u3068\u611f\u60f3\n\u540c\u3058\u76ee\u7684\u3067\u4f7f\u7528\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\u306e\u3067\uff0c\u4f3c\u3066\u3044\u308b\u3068\u3053\u308d\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\uff0e\u6a5f\u80fd\u9762\u3067\u306f\uff0cTensorFlow\u306e\u65b9\u304c\u5145\u5b9f\u3057\u3066\u3044\u308b\u306e\u3067\uff0cTheno\u30b3\u30fc\u30c9\u3092ThensorFlow\u5411\u3051\u306b\u79fb\u690d\u3059\u308b\u306e\u306f\u5bb9\u6613\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u308f\u308c\u307e\u3059\uff0e\u4f46\u3057\uff0cOptimizer\u3084\u65b0\u3057\u3044\u95a2\u6570\u3082\u4e00\u5ea6\uff0c\u5b9f\u88c5\u3057\u3066\u3044\u308c\u3070\u305d\u308c\u3092\u4f7f\u3044\u307e\u308f\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\uff0cTheano\u304c\u4e0d\u5229\u3068\u3044\u3046\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\uff08\u5148\u4eba\u306e\u30b3\u30fc\u30c9\u3082\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3057\uff0c\u30a2\u30c9\u30aa\u30f3\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3082\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\uff0e\uff09\n\u307e\u3060MLP\u306e\u30b3\u30fc\u30c9\u3092\u898b\u305f\u3060\u3051\u3067\uff0c\u3088\u308a\u8907\u96d1\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u8a66\u3057\u3066\u3044\u307e\u305b\u3093\u304c\uff0c\u3069\u3061\u3089\u3082\u975e\u5e38\u306b\u53ef\u80fd\u6027\u306e\u3042\u308b\u30c4\u30fc\u30eb\u3068\u601d\u308f\u308c\u307e\u3059\uff0e\uff08Chainer\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3059\u304c\u306a\u304b\u306a\u304b\u624b\u304c\u3064\u304c\u305a... \uff09\n\n\uff08\u8ffd\u8a18\uff09Keras \u306b\u3064\u3044\u3066\u306e\u60c5\u5831\n\u672c\u8a18\u4e8b\u3092\u66f8\u304f\u306b\u3042\u305f\u308aNeural Network Library \"Keras\"\u306b\u3064\u3044\u3066\u30b5\u30a4\u30c8\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\uff0c\u201dTheano\"\u3068\u7d44\u307f\u5408\u308f\u305b\u3066\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u52a0\u3048\u3066\uff0c \u201dTensorFlow\"\u3068\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u4f7f\u3046\u3082\u306e\u3082\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\uff0e\uff08\u5f8c\u3067\u8abf\u3079\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\uff0e\uff09\nKeras: Deep Learning library for Theano and TensorFlow\n\n\u53c2\u8003\u6587\u732e (web site)\n\nTensorFlow Documentation : https://www.tensorflow.org/\n\nTheano Documentation : http://deeplearning.net/software/theano/index.html\n\ngradient-optimizers 0.0.4 : https://pypi.python.org/pypi/gradient-optimizers/0.0.4\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u3092\u8272\u3005\u8a66\u3057\u3066\u307f\u308b- Qiita\nhttp://qiita.com/hogefugabar/items/1d4f6c905d0edbc71af2\n\nTensorFlow\u306eMLP\u30b3\u30fc\u30c9\u3067\"Wine\"\u3092\u5206\u985e - Qiita\nhttp://qiita.com/TomokIshii/items/2cab778a3192d561a1ef\n\nKeras Documentation : http://keras.io/\n\n\n\n\u5143\u3005\uff0cDeep Learning\u7528\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\"Theano\"\u306e\u5b66\u7fd2\u3092\u9032\u3081\u3066\u3044\u305f\u304c\uff0c\u5148\u6708(2015/11)\u516c\u958b\u3055\u308c\u305f\"TensorFlow\"\u306b\u8208\u5473\u3092\u60f9\u304b\u308c\u6700\u8fd1\u306f\u305d\u3061\u3089\u3092\u30e1\u30a4\u30f3\u3067\u4f7f\u3063\u3066\u3044\u305f\uff0eTensorFlow\u3092\u89e6\u308a\u306a\u304c\u3089\uff0c\u300c\u3053\u3053\u306f\"Theano\"\u306b\u8fd1\u3044\uff0c\u3053\u3053\u306f\u304b\u306a\u308a\u9055\u3046\u300d\u3068\u611f\u3058\u3066\u3044\u305f\u304c\uff0c\u4e21\u8005\u306e\u9055\u3044\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u7c21\u5358\u306a\u30b3\u30fc\u30c9\u3092\"TensorFlow\"\u304b\u3089\u201dTheano\"\u306b\u79fb\u690d\u3057\u3066\u307f\u305f\uff0e\uff08\u666e\u901a\uff0c\u65b9\u5411\u304c\u9006\u304b\u3068\u601d\u3044\u307e\u3059\u304c\uff0e\uff09\n\n## \u6982\u8981\uff1a \"Theano\" vs. \"TensorFlow\"\n\n\u307e\u305a\uff12\u3064\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306e\u6982\u8981\u3092\u6bd4\u8f03\u3059\u308b\uff0e\n\n|    \u9805\u76ee   |    Theano  |    TensorFlow   |\n|:--------:|:-----------:|:----------------:|\n| \u958b\u767a\u4e3b\u4f53 | \u30a2\u30ab\u30c7\u30df\u30c3\u30af (University of Montreal)\u3000| \u4f01\u696d (Google)\n| \u516c\u958b\u5e74\u5ea6 | 2010\u5e74\u3050\u3089\u3044(?)   |  2015\u5e74\n| Tensor operation |   support  |  support |\n| numpy 'basic' level \u6a5f\u80fd | support  | support |\n|  \u81ea\u52d5\u5fae\u5206 (Graph Transformation)  | support | support |\n| GPU\u6f14\u7b97 |  support   |   support |\n| Graph Visualization |  support (\u4e00\u5fdc\u3042\u308a\u307e\u3059\uff09|  support (\u3054\u5b58\u77e5 Tensorboard) |\n| Optimizer | not support (built-in\u3067\u306f\u306a\u3044\u3068\u3044\u3046\u610f\u5473\uff09 | \u3044\u308d\u3044\u308dsupport |\n| Neural Network\u7cfb\u306e\u95a2\u6570 | support | (\u3044\u308d\u3044\u308d) support |\n\n\u6a5f\u80fd\u9762\u3067\u306f\uff0c\u4e0a\u306e\u8868\u306e\u6700\u5f8c\u306e\u90e8\u5206\u306b\u9055\u3044\u304c\u898b\u3089\u308c\u308b\uff0eTheano\u3067\u306f\u7d30\u304b\u3044\u90e8\u5206\u3092\u81ea\u524d\u3067\u7528\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u306e\u306b\u5bfe\u3057\uff0cThensorFlow\u306b\u306f\u30e9\u30a4\u30d6\u30e9\u30ea\u95a2\u6570\u3068\u3057\u3066\u521d\u3081\u304b\u3089\u3044\u308d\u3044\u308d\u6e96\u5099\u3055\u308c\u3066\u3044\u308b\uff0c\u3068\u3044\u3063\u305f\u5370\u8c61\u3067\u3042\u308b\uff0e  \n\uff08Theano\u3067\u8a73\u7d30\u90e8\u304b\u3089\u30d7\u30ed\u30b0\u30e9\u30e0\u3057\u305f\u304f\u306a\u3044\u65b9\u306b\u306f\uff0cTheano\u30d9\u30fc\u30b9\u306e\u9ad8\u6a5f\u80fd\u30e9\u30a4\u30d6\u30e9\u30ea\uff0c\u4f8b\u3048\u3070 Pylearn2 \u304c\u3042\u308a\u307e\u3059\uff0e\uff09\n\n## \u30b3\u30fc\u30c9\u3067\u6bd4\u8f03\uff08Neural Network\u306e\u30e2\u30c7\u30eb\u5316)\n\n\u3053\u3053\u3067\u306f\uff0c\"Theano\"\u3068\u201dThensorFlow\"\u3067\u306f\u307b\u3068\u3093\u3069\u540c\u3058\u3088\u3046\u306b\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b\uff0e[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/TomokIshii/items/2cab778a3192d561a1ef)\u3067\u7d39\u4ecb\u3057\u305f\u591a\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046MLP(Multi-layer Perceptron)\u306e\u30b3\u30fc\u30c9\u304b\u3089\u629c\u7c8b\u3059\u308b\uff0e\n\n**TensorFlow\u7248**\n\n```py\n# Hidden Layer\nclass HiddenLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n    \n        w_h = tf.Variable(tf.random_normal([n_in,n_out],mean=0.0,stddev=0.05))\n        b_h = tf.Variable(tf.zeros([n_out]))\n     \n        self.w = w_h\n        self.b = b_h\n        self.params = [self.w, self.b]\n    \n    def output(self):\n        linarg = tf.matmul(self.input, self.w) + self.b\n        self.output = tf.nn.relu(linarg)  # switch sigmoid() to relu()\n        \n        return self.output\n\n# Read-out Layer\nclass ReadOutLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n        \n        w_o = tf.Variable(tf.random_normal([n_in,n_out],mean=0.0,stddev=0.05))\n        b_o = tf.Variable(tf.zeros([n_out]))\n       \n        self.w = w_o\n        self.b = b_o\n        self.params = [self.w, self.b]\n    \n    def output(self):\n        linarg = tf.matmul(self.input, self.w) + self.b\n        self.output = tf.nn.softmax(linarg)  \n\n        return self.output\n\n```\n\n**Theano\u7248**\n\n```py\n# Hidden Layer\nclass HiddenLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n    \n        w_h = theano.shared(floatX(np.random.standard_normal([n_in, n_out])) \n                             * 0.05) \n        b_h = theano.shared(floatX(np.zeros(n_out)))\n   \n        self.w = w_h\n        self.b = b_h\n        self.params = [self.w, self.b]\n    \n    def output(self):\n        linarg = T.dot(self.input, self.w) + self.b\n        # self.output = T.nnet.relu(linarg)\n        self.output = T.nnet.sigmoid(linarg)\n        \n        return self.output\n\n# Read-out Layer\nclass ReadOutLayer(object):\n    def __init__(self, input, n_in, n_out):\n        self.input = input\n        \n        w_o = theano.shared(floatX(np.random.standard_normal([n_in,n_out]))\n                             * 0.05)\n        b_o = theano.shared(floatX(np.zeros(n_out)))\n       \n        self.w = w_o\n        self.b = b_o\n        self.params = [self.w, self.b]\n    \n    def output(self):\n        linarg = T.dot(self.input, self.w) + self.b\n        self.output = T.nnet.softmax(linarg)  \n\n        return self.output\n\n```\n\n\u7d30\u304b\u304f\u898b\u306a\u3051\u308c\u3070\u300c\"tf.\" \u3092\"T.\"\u306b\u5909\u3048\u305f\u3060\u3051\u300d\u306e\u9055\u3044\u3067\u3042\u308b\uff0e\u6d3b\u6027\u5316\u95a2\u6570\u306b\u3064\u3044\u3066\u306f\u5f8c\u767a\u306eTenfsorFlow\u306e\u65b9\u304c \"tf.nn.relu()\"\u7b49\u3082\u3042\u3063\u3066\u4fbf\u5229\u3068\u601d\u3063\u3066\u3044\u305f\u304c\uff0cTheano\u3067\u3082 ver.0.7.1\u304b\u3089relu() (Rectified Linear Unit) \u3092\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u3088\u3046\u3067\u3042\u308b\uff0e\uff08\u672c\u8a18\u4e8b\u3067\u4f7f\u7528\u3057\u305f\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\uff0cPython\u3000ver.2.7.10, TensorFlow ver.0.5.0, Theano ver.0.7.0 \u306b\u306a\u308a\u307e\u3059\uff0e\uff09\n\n\u307e\u305f\uff0cSoftmax\u95a2\u6570\u3082\u5f53\u7136\u306e\u3053\u3068\u306a\u304c\u3089\u4e21\u8005\u3067\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\uff0e\n\n## \u30b3\u30fc\u30c9\u306e\u6bd4\u8f03\uff08\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\uff09\n\n\u3053\u3053\u3067\u306f\u4e21\u8005(TensorFlow vs. Theano)\u3067\u9055\u3044\u304c\u307f\u3089\u308c\u308b\uff0eTensorFlow\u3067\u306f\uff0cOptimizer\u306e\u30af\u30e9\u30b9\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u5145\u5b9f\u3057\u3066\u3044\u308b\u4e00\u65b9\uff0cTheano\u3067\u306fOptimizer\u306e\u30af\u30e9\u30b9\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u81ea\u5206\u3067\u7528\u610f\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\uff0e\n\n**TensorFlow\u7248(AdagradOptimzer\u4f7f\u7528\u4f8b)**\n\n```py\n    # Train\n    optimizer = tf.train.AdagradOptimizer(0.01)\n    train_op = optimizer.minimize(loss)\n    \n    init = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n        sess.run(init)\n        print('Training...')\n        for i in range(10001):\n            train_op.run({x: train_x, y_: train_y})\n            if i % 1000 == 0:                # echo status on screen\n                train_accuracy = accuracy.eval({x: train_x, y_: train_y})\n                print(' step, accurary = %6d: %8.3f' % (i, train_accuracy))\n\n```\n\nTensorFlow\u3067\u306f\uff0coptimizer\u3092\u6307\u5b9a\u3057\u3066\uff0cSession\u3092\u52d5\u304b\u3059\uff0eSeesion\u306e\u4e2d\u3067Train Data\u3092Feed_dict\u306e\u5f62\u3067\u4f9b\u7d66\u3059\u308b\u3068\u3044\u3046\u6d41\u308c\uff0e\n\n**Theano\u7248(Adagrad\u3092\u5b9f\u88c5\u3057\u305f\u4f8b)**\n\u3067\u304d\u308b\u3060\u3051\u95a2\u6570\u306ecall\u306e\u4ed5\u65b9\u3092TensorFlow\u306e\u30b3\u30fc\u30c9\u306b\u8fd1\u3065\u3051\u308b\u3088\u3046\u306b\u66f8\u3044\u305f\u3064\u3082\u308a\u3060\u304c\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u305f\uff0e\n\n```py\n# Optimizers - (GradientDescent), AdaGrad\nclass Optimizer(object):\n    def __init__(self, params, learning_rate=0.01):\n        self.lr = learning_rate\n        self.params = params\n       \n    def minimize(self, loss):\n        self.gradparams = [T.grad(loss, param) for param in params]\n        \nclass AdagradOptimizer(Optimizer):\n    def __init__(self, params, learning_rate=0.01, eps=1.e-6):\n        super(AdagradOptimizer, self).__init__(params, learning_rate)\n        self.eps = eps\n        self.accugrads = [theano.shared(floatX(np.zeros(t.shape.eval())),\n                          'accugrad') for t in self.params\n                         ]\n    def minimize(self, loss):\n        super(AdagradOptimizer, self).minimize(loss)\n        self.updates = OrderedDict()    # for Theano's rule\n\n        for accugrad, param, gparam in zip(\n                              self.accugrads, self.params, self.gradparams):\n            agrad = accugrad + gparam * gparam\n            dx = - (self.lr / T.sqrt(agrad + self.eps)) * gparam\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n\n```\n\n\u3053\u308c\u3089\u3092\u4f7f\u3063\u3066\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\uff08\u5b66\u7fd2\uff09\u3092\u8d70\u3089\u305b\u308b\uff0e\n\n```py\n    # Train\n    myoptimizer = AdagradOptimizer(params, learning_rate=0.01, eps=1.e-8)\n    one_update = myoptimizer.minimize(loss)\n    \n    # Compile ... define theano.function\n    train_model = theano.function(\n        inputs=[],\n        outputs=[loss, accuracy],\n        updates=one_update,\n        givens=[(x, strain_x), (y_, strain_y)],\n        allow_input_downcast=True\n    )\n\n    n_epochs = 10001\n    epoch = 0\n    \n    while (epoch < n_epochs):\n        epoch += 1\n        loss, accu = train_model()\n        if epoch % 1000 == 0:\n            print('epoch[%5d] : cost =%8.4f, accyracy =%8.4f' % (epoch, loss, accu))\n\n```\n\n\u3053\u306e\u3088\u3046\u306b\u6700\u9069\u5316\u30d7\u30ed\u30bb\u30b9\u306e\u90e8\u5206\u306b\u304a\u3044\u3066\uff0c\n**TensorFlow\u3067\u306f...**\n\u5909\u6570\u3092\u521d\u671f\u5316\u3057\u305f\u5f8c\uff0cSession\u3092\u8d70\u3089\u305b\u308b\uff0e\u8a13\u7df4\u30c7\u30fc\u30bf\u306f\uff0c\u30bb\u30c3\u30b7\u30e7\u30f3\u4e2d\u306b op.run( {Feed_dict})\u306e\u3088\u3046\u306a\u5f62\u3067\u4e0e\u3048\u308b\uff0e  \n**Theano\u3067\u306f...**\ntheano.function()\u306e\u3068\u3053\u308d\u3067\uff08\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4f9b\u7d66\u3092\u542b\u3081\uff09\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u306e\u6d41\u308c\u3092\u5b9a\u7fa9\u3059\u308b\uff0e\u5b9a\u7fa9\u3057\u305f\u95a2\u6570(theano.function)\u3092\u7528\u3044\u3066\uff0c\u53cd\u5fa9\u5b66\u7fd2\u8a08\u7b97\u3092\u5b9f\u884c\u3059\u308b\uff0c\n\u3068\u3044\u3046\u9055\u3044\u304c\u898b\u3089\u308c\u308b\uff0e\n\n\u201dTheano\"\u306e\u5b66\u7fd2\u3092\u59cb\u3081\u305f\u5f53\u521d\u306f\uff0c\u3053\u306e theano.function() \u306e\u4f7f\u3044\u65b9\u306b\u60a9\u3093\u3060\u8a18\u61b6\u304c\u3042\u308b\u304c\uff0c\u4e0a\u8a18\u306e\u3088\u3046\u306b\"TensorFlow\"\u3068\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3067\uff0ctheano.function() \u3078\u306e\u7406\u89e3\u304c\u6df1\u307e\u3063\u305f\uff0e\uff08\u9055\u3046\u898b\u65b9\u3092\u3059\u308c\u3070\uff0ctheano.function() \u306b\u3064\u3044\u3066\u304d\u3061\u3093\u3068\u7406\u89e3\u3057\u3066\u3044\u308c\u3070\uff0cTheano\u3092\u3046\u307e\u304f\u4f7f\u3044\u3053\u306a\u305b\u308b\u3068\u601d\u3044\u307e\u3059\uff0e\uff09\n\n## \u307e\u3068\u3081\u3068\u611f\u60f3\n\u540c\u3058\u76ee\u7684\u3067\u4f7f\u7528\u3055\u308c\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\u306e\u3067\uff0c\u4f3c\u3066\u3044\u308b\u3068\u3053\u308d\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\uff0e\u6a5f\u80fd\u9762\u3067\u306f\uff0cTensorFlow\u306e\u65b9\u304c\u5145\u5b9f\u3057\u3066\u3044\u308b\u306e\u3067\uff0cTheno\u30b3\u30fc\u30c9\u3092ThensorFlow\u5411\u3051\u306b\u79fb\u690d\u3059\u308b\u306e\u306f\u5bb9\u6613\u3067\u306f\u306a\u3044\u304b\u3068\u601d\u308f\u308c\u307e\u3059\uff0e\u4f46\u3057\uff0cOptimizer\u3084\u65b0\u3057\u3044\u95a2\u6570\u3082\u4e00\u5ea6\uff0c\u5b9f\u88c5\u3057\u3066\u3044\u308c\u3070\u305d\u308c\u3092\u4f7f\u3044\u307e\u308f\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\uff0cTheano\u304c\u4e0d\u5229\u3068\u3044\u3046\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\uff08\u5148\u4eba\u306e\u30b3\u30fc\u30c9\u3082\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3057\uff0c\u30a2\u30c9\u30aa\u30f3\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3082\u305f\u304f\u3055\u3093\u3042\u308a\u307e\u3059\uff0e\uff09\n\n\u307e\u3060MLP\u306e\u30b3\u30fc\u30c9\u3092\u898b\u305f\u3060\u3051\u3067\uff0c\u3088\u308a\u8907\u96d1\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u3092\u8a66\u3057\u3066\u3044\u307e\u305b\u3093\u304c\uff0c\u3069\u3061\u3089\u3082\u975e\u5e38\u306b\u53ef\u80fd\u6027\u306e\u3042\u308b\u30c4\u30fc\u30eb\u3068\u601d\u308f\u308c\u307e\u3059\uff0e\uff08Chainer\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u306e\u3067\u3059\u304c\u306a\u304b\u306a\u304b\u624b\u304c\u3064\u304c\u305a... \uff09\n\n## \uff08\u8ffd\u8a18\uff09Keras \u306b\u3064\u3044\u3066\u306e\u60c5\u5831\n\u672c\u8a18\u4e8b\u3092\u66f8\u304f\u306b\u3042\u305f\u308aNeural Network Library \"Keras\"\u306b\u3064\u3044\u3066\u30b5\u30a4\u30c8\u3092\u78ba\u8a8d\u3057\u305f\u3068\u3053\u308d\uff0c\u201dTheano\"\u3068\u7d44\u307f\u5408\u308f\u305b\u3066\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u52a0\u3048\u3066\uff0c \u201dTensorFlow\"\u3068\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u4f7f\u3046\u3082\u306e\u3082\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\uff0e\uff08\u5f8c\u3067\u8abf\u3079\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059\uff0e\uff09\n\n[Keras: Deep Learning library for Theano and TensorFlow](http://keras.io/)\n\n\n## \u53c2\u8003\u6587\u732e (web site)\n- TensorFlow Documentation : [https://www.tensorflow.org/](https://www.tensorflow.org/)\n- Theano Documentation : http://deeplearning.net/software/theano/index.html\n- gradient-optimizers 0.0.4 : https://pypi.python.org/pypi/gradient-optimizers/0.0.4\n- \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u5b66\u7fd2\u4fc2\u6570\u6700\u9069\u5316\u3092\u8272\u3005\u8a66\u3057\u3066\u307f\u308b- Qiita\n[http://qiita.com/hogefugabar/items/1d4f6c905d0edbc71af2](http://qiita.com/hogefugabar/items/1d4f6c905d0edbc71af2)  \n- TensorFlow\u306eMLP\u30b3\u30fc\u30c9\u3067\"Wine\"\u3092\u5206\u985e - Qiita  \n[http://qiita.com/TomokIshii/items/2cab778a3192d561a1ef](http://qiita.com/TomokIshii/items/2cab778a3192d561a1ef)\n- Keras Documentation : http://keras.io/\n\n\n\n", "tags": ["Python", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2", "TensorFlow", "Theano"]}