{"context": "\n\nPython\u3067URL\u30ea\u30b9\u30c8\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\u4ee5\u4e0b\u306e\u8a18\u4e8b\u3067\u7279\u5b9a\u306eWEB\u30b5\u30a4\u30c8\u3092\u30af\u30ed\u30fc\u30eb\u3057\u3066URL\u306e\u30ea\u30b9\u30c8\u304c\u3067\u304d\u3066\u304d\u305f\u306e\u3067\u3001\u305d\u308c\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\nBeautifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\uff08\u9023\u756a\u30da\u30fc\u30b8\uff09\nBeautifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\uff08\u968e\u5c64\u5316\u30da\u30fc\u30b8\uff09\n\n\u30bd\u30fc\u30b9\n\nsimple_downloader.py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport requests, os\n\nheaders = { 'User-Agent' : 'Mozilla/5.0' }\ncwd = os.getcwd()\nresult_dir = cwd + '/download/'\nlist_file = cwd + '/list.txt'\ndone_file = 'done.txt'\nfail_file = 'fail.txt'\n\ndef fetchImage(url):\n    path_relative = url.replace('http://', '').replace('https://', '')\n    try:\n        res = requests.get(url, headers = headers)\n        image = res.content\n        paths = os.path.split(path_relative)[0].split('/')\n        path_current = result_dir\n        for path in paths:\n            path_current += path + '/'\n            if not os.path.exists(path_current):\n                os.mkdir(path_current)\n        with open('{result_dir}{path_relative}'.format(result_dir = result_dir, path_relative = path_relative), 'wb') as f:\n            f.write(image)\n    except:\n        return False\n    return True\n\ndef getUrl():\n    result = ''\n    with open(list_file, 'r') as f:\n        url_list = f.read().split('\\n')\n    result = url_list.pop(0)\n    with open(list_file, 'w') as f:\n        f.write('\\n'.join(url_list))\n    return result\n\ndef saveUrl(file_name, url):\n    with open(file_name, 'a') as f:\n        f.write(url + '\\n')\n\ndef download():\n    url = getUrl()\n    while url != '':\n        if fetchImage(url):\n            saveUrl(done_file, url)\n            print('done ' + url)\n        else:\n            saveUrl(fail_file, url)\n            print('fail ' + url)\n        url = getUrl()\n\ndownload()\n\n\n\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\npython\u3067\u7279\u5b9a\u306eURL\u304b\u3089\u753b\u50cf\u3092\u4e00\u62ec\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u4fee\u6b63\u7248\n[Python]\u30d5\u30a1\u30a4\u30eb/\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u64cd\u4f5c\n# Python\u3067URL\u30ea\u30b9\u30c8\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n\u4ee5\u4e0b\u306e\u8a18\u4e8b\u3067\u7279\u5b9a\u306eWEB\u30b5\u30a4\u30c8\u3092\u30af\u30ed\u30fc\u30eb\u3057\u3066URL\u306e\u30ea\u30b9\u30c8\u304c\u3067\u304d\u3066\u304d\u305f\u306e\u3067\u3001\u305d\u308c\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\n[BeautifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\uff08\u9023\u756a\u30da\u30fc\u30b8\uff09](http://qiita.com/shngt/items/d4fa9c4231aab09145d9)\n\n[BeautifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\uff08\u968e\u5c64\u5316\u30da\u30fc\u30b8\uff09](http://qiita.com/shngt/items/3a32e5cc439d5277f7af)\n\n## \u30bd\u30fc\u30b9\n\n```python:simple_downloader.py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport requests, os\n\nheaders = { 'User-Agent' : 'Mozilla/5.0' }\ncwd = os.getcwd()\nresult_dir = cwd + '/download/'\nlist_file = cwd + '/list.txt'\ndone_file = 'done.txt'\nfail_file = 'fail.txt'\n\ndef fetchImage(url):\n    path_relative = url.replace('http://', '').replace('https://', '')\n    try:\n        res = requests.get(url, headers = headers)\n        image = res.content\n        paths = os.path.split(path_relative)[0].split('/')\n        path_current = result_dir\n        for path in paths:\n            path_current += path + '/'\n            if not os.path.exists(path_current):\n                os.mkdir(path_current)\n        with open('{result_dir}{path_relative}'.format(result_dir = result_dir, path_relative = path_relative), 'wb') as f:\n            f.write(image)\n    except:\n        return False\n    return True\n\ndef getUrl():\n    result = ''\n    with open(list_file, 'r') as f:\n        url_list = f.read().split('\\n')\n    result = url_list.pop(0)\n    with open(list_file, 'w') as f:\n        f.write('\\n'.join(url_list))\n    return result\n\ndef saveUrl(file_name, url):\n    with open(file_name, 'a') as f:\n        f.write(url + '\\n')\n\ndef download():\n    url = getUrl()\n    while url != '':\n        if fetchImage(url):\n            saveUrl(done_file, url)\n            print('done ' + url)\n        else:\n            saveUrl(fail_file, url)\n            print('fail ' + url)\n        url = getUrl()\n\ndownload()\n```\n\n## \u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\n\n[python\u3067\u7279\u5b9a\u306eURL\u304b\u3089\u753b\u50cf\u3092\u4e00\u62ec\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u4fee\u6b63\u7248](http://qiita.com/fantm21/items/8b963a0c973057f0984a)\n[[Python]\u30d5\u30a1\u30a4\u30eb/\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u64cd\u4f5c](http://qiita.com/supersaiakujin/items/12451cd2b8315fe7d054)\n", "tags": ["scraping", "Python"]}