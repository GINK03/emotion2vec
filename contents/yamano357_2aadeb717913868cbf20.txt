{"context": "\n\n\u6982\u8981\n\u3000\u4ee5\u524d\u306b{tensorflow}\u306ePython\u30e9\u30a4\u30d6\u30e9\u30ea\u3092import\u3059\u308b\u95a2\u6570\u3092\u7528\u3044\u3066R\u304b\u3089Keras\u3092\u547c\u3073\u51fa\u3057\u3001\u77ed\u6b4c\u751f\u6210\u3092\u8a66\u307f\u307e\u3057\u305f\u3002\n\u3000\u3057\u304b\u3057\u306a\u304c\u3089\u3001\u8a66\u3057\u305f\u3082\u306e\u306e\u77e5\u898b\u304c\u8db3\u3089\u305a\u3001\u6e80\u8db3\u304c\u3044\u304f\u7d50\u679c\u304c\u51fa\u305b\u305f\u3068\u306f\u8a00\u3048\u307e\u305b\u3093\u3002\n\u3000\u305d\u3053\u3067\u4eca\u56de\u306fKeras\u306e\u304a\u52c9\u5f37\u3092\u517c\u306d\u3066\u3001Keras\u306eexample\u306b\u3042\u308b\u753b\u50cf\u751f\u6210(VAE; variational autoencoder)\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092R\u3067\u66f8\u304d\u63db\u3048\u3001\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u3053\u3046\u3057\u3066\u5f97\u305f\u77e5\u898b\u3092\u6d3b\u7528\u3057\u3066\u3001\u3088\u308a\u3053\u306a\u308c\u305f\u77ed\u6b4c\u751f\u6210\u3092\u76ee\u6307\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nKeras Documentation\nThis script demonstrates how to build a variational autoencoder with Keras\n\n\u3000\u52a0\u3048\u3066\u4eca\u56de\u306f\u3001Python\u30e9\u30a4\u30d6\u30e9\u30ea\u3092import\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u3057\u3066{reticulate}\u3092\u7528\u3044\u307e\u3057\u305f\u3002\u3053\u3061\u3089\u306f{tensorflow}\u304b\u3089Python\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u8aad\u307f\u8fbc\u3080\u6a5f\u80fd\u304c\u5206\u3051\u3089\u308c\u305f\u3082\u306e\u3067\u3001{tensorflow}\u3067\u3082{reticulate}\u3092\u4f7f\u3046\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u3000\u3055\u3089\u306b{reticulate}\u306b\u306fR\u4e0a\u3067Python\u3092\u53c2\u7167\u3059\u308b\u305f\u3081\u306e\u69d8\u3005\u306a\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u3066\u304a\u308a\u3001\u4ee5\u524d\u306f\u3067\u304d\u306a\u304b\u3063\u305f\u3053\u3068\uff08\u4f8b\u3048\u3070\u30011\u8981\u7d20\u306eR\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u3068\u3057\u3066\u30a2\u30b5\u30a4\u30f3\u3057\u3088\u3046\u3068\u3059\u308b\u3068Python\u306e\u30ea\u30b9\u30c8\u306b\u5909\u63db\u3055\u308c\u3066\u3057\u307e\u3044\u3001\u30a8\u30e9\u30fc\u306b\u306a\u3063\u3066\u3044\u305f\uff09\u304c\u53ef\u80fd\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\nR Interface to Python \n\n\u3000\u306a\u304a\u3001\u672c\u8a18\u4e8b\u3067\u306f\u300cKeras\u3092\u7528\u3044\u305fVAE\u306ePython\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3001R\u3067\u5b9f\u884c\u3059\u308b\u306b\u306f\u3069\u3046\u3044\u3046\u98a8\u306b\u8a18\u8ff0\u3059\u308b\u304b\u300d\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\u304a\u308a\u307e\u3059\u3002VAE\u306e\u7406\u8ad6\u306b\u3064\u3044\u3066\u306f\u300c\u53c2\u8003\u300d\u306b\u3042\u308b\u30ea\u30f3\u30af\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\n\u5b9a\u7fa9\u90e8\n\n\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u8aad\u307f\u8fbc\u307f\n\n\u30e9\u30a4\u30d6\u30ea\u8aad\u307f\u8fbc\u307f\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(DiagrammeR)\n\n# Keras\u3092R\u4e0a\u3067\u4f7f\u3048\u308b\u3088\u3046\u306b{reticulate}\u306eimport\u95a2\u6570\u3067\u8aad\u307f\u8fbc\u307f\nkeras <- reticulate::import(module = \"keras\")\n# \u30e2\u30c7\u30eb\u3092dot\u306b\u5909\u63db\u3059\u308bKeras\u95a2\u6570\u3092\u5225\u540d\u3067\u5b9a\u7fa9\uff08\u95a2\u6570\u540d\u304c\u9577\u304f\u306a\u308a\u3059\u304e\u3066\u8996\u8a8d\u6027\u304c\u60aa\u304b\u3063\u305f\u305f\u3081\uff09\nmodel_dot <- keras$utils$visualize_util$model_to_dot\n\n\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6e96\u5099\n\nMNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f\nmnist_d <- keras$datasets$mnist$load_data()\n# \u753b\u50cf\u30c7\u30fc\u30bf\u3092[0, 1]\u306b\u5909\u63db\nx_train <- array(\n  data = mnist_d[[1]][[1]] / 255,\n  dim = c(nrow(x = mnist_d[[1]][[1]]), cumprod(x = dim(x = mnist_d[[1]][[1]])[-1])[2])\n)\nx_test <- array(\n  data = mnist_d[[2]][[1]] / 255,\n  dim = c(nrow(x = mnist_d[[2]][[1]]), cumprod(x = dim(x = mnist_d[[2]][[1]])[-1])[2])\n)\n\n\n\n\u5b9a\u6570\u3068\u95a2\u6570\u306e\u5b9a\u7fa9\n\u3000R\u304b\u3089Keras(Python)\u306b\u6574\u6570\u5024\u3092\u6e21\u3059\u5834\u5408\u306f\u660e\u793a\u7684\u306b\u6574\u6570\u5024\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b\u5fc5\u8981\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u6570\u5024\u306e\u5f8c\u308d\u306b\u300cL\u300d\u3092\u3064\u3051\u3066\u6574\u6570\u5024\u3068\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u8907\u6570\u307e\u3068\u3081\u3066\u6574\u6570\u5316\u3057\u305f\u3044\u3068\u304d\u306fas.integer\u3092\u7528\u3044\u3066\u3082\u3044\u3044\u3067\u3059\u3002\n\n\u5b9a\u6570\u5b9a\u7fa9\n# \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\nBATCH_SIZE <- 100L\nORIGINAL_DIM <- 784L\nLATENT_DIM <- 2L\nINTERMEDIATE_DIM <- 256L\nEPSILON_STD <- 1.0\n\n# Epoch\u6570\nNB_EPOCH <- 100L\n\n\n\n\u3000\u30c6\u30f3\u30bd\u30eb\u540c\u58eb\u306e\u8a08\u7b97\u3092R\u4e0a\u3067\u5b9a\u7fa9\u3059\u308b\u3068\u578b\u5909\u63db\u3084backend\u306e\u7af6\u5408\u306a\u3069\u304c\u5384\u4ecb\u3060\u3063\u305f\u306e\u3067\u3001\u3053\u3053\u3067\u306fPython\u95a2\u6570\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002\n\u3000\u640d\u5931\u95a2\u6570\u3078\u306e\u8907\u6570\u5f15\u6570\u306e\u6e21\u3057\u65b9\u306f\u4e0b\u8a18\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n- Passing additional arguments to objective function\n\n\u95a2\u6570\u5b9a\u7fa9\npy_def <- reticulate::py_run_string(\n  code = \"\nimport keras.backend as K\nfrom keras import objectives\ndef sampling(args):\n  z_mean, z_log_var = args\n  epsilon = K.random_normal(shape = (batch_size, latent_dim), mean = 0., std = epsilon_std)\n  return z_mean + K.exp(z_log_var / 2) * epsilon\n\ndef vae_loss(z_mean, z_log_var):\n  def loss(x, x_decoded_mean):\n    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n    return xent_loss + kl_loss\n  return loss\n  \",\n  convert = FALSE\n)\n\n# Python\u5074\u306e\u74b0\u5883\u306b\u5b9a\u6570\u5b9a\u7fa9\npy_def$batch_size <- BATCH_SIZE\npy_def$original_dim <- ORIGINAL_DIM\npy_def$latent_dim <- LATENT_DIM\npy_def$epsilon_std <- EPSILON_STD\n\n\n\n\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3068\u5b66\u7fd2\n\u3000\u30b9\u30af\u30ea\u30d7\u30c8\u4f8b\u3092\u53c2\u8003\u306b\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3092\u7c9b\u3005\u3068\u884c\u3044\u307e\u3059\u3002\n\u3000\u30e1\u30bd\u30c3\u30c9\u547c\u3073\u51fa\u3057\u306b\u300c$__call__\u300d\u3092\u7528\u3044\u3066\u304a\u308a\u3001\u8996\u8a8d\u6027\u304c\u3068\u3066\u3082\u60aa\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3092\u3046\u307e\u304f\u3067\u304d\u308b\u3068\u4f7f\u3044\u3084\u3059\u304f\u3066\u3044\u3044\u3067\u3059\u304c\u3001\u5bfe\u5fdc\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u8981\u8abf\u67fb\u3067\u3059\u3002\n\nThis script demonstrates how to build a variational autoencoder with Keras\n\n\nVariational AutoEncoder\n\nVAE\u30e2\u30c7\u30eb\u5b9a\u7fa9\nx <- keras$layers$Input(batch_shape = list(BATCH_SIZE, ORIGINAL_DIM))\nh <- keras$layers$Dense(output_dim = INTERMEDIATE_DIM, activation = \"relu\")$`__call__`(x = x)\n\nz_mean <- keras$layers$Dense(output_dim = LATENT_DIM)$`__call__`(x = h)\nz_log_var <- keras$layers$Dense(output_dim = LATENT_DIM)$`__call__`(x = h)\n\n# \u524d\u8ff0\u306e\u5b9a\u7fa9\u90e8\u3067\u5b9a\u7fa9\u3057\u305fPython\u95a2\u6570\u3092Lambda\u30ec\u30a4\u30e4\u30fc\u3067\u4f7f\u7528\u3059\u308b\u95a2\u6570\u306b\u6307\u5b9a\u3059\u308b\nz <- keras$layers$Lambda(py_def$sampling, output_shape = list(LATENT_DIM))$`__call__`(list(z_mean, z_log_var))\n\ndecoder_h <- keras$layers$Dense(output_dim = INTERMEDIATE_DIM, activation = \"relu\")\nh_decoded <- decoder_h$`__call__`(z)\ndecoder_mean <- keras$layers$Dense(output_dim = ORIGINAL_DIM, activation = \"sigmoid\")\nx_decoded_mean <- decoder_mean$`__call__`(h_decoded)\n\nvae <- keras$models$Model(input = x, output = x_decoded_mean)\nvae$compile(optimizer = \"rmsprop\", loss = py_def$vae_loss(z_mean, z_log_var), metrics = list(\"mse\"))\n\n# VAE\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> vae$summary()\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (100, 784)            0                                            \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (100, 256)            200960      input_1[0][0]                    \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n____________________________________________________________________________________________________\ndense_3 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n____________________________________________________________________________________________________\nlambda_1 (Lambda)                (100, 2)              0           dense_2[0][0]                    \n                                                                   dense_3[0][0]                    \n____________________________________________________________________________________________________\ndense_4 (Dense)                  (100, 256)            768         lambda_1[0][0]                   \n____________________________________________________________________________________________________\ndense_5 (Dense)                  (100, 784)            201488      dense_4[0][0]                    \n====================================================================================================\nTotal params: 404,244\nTrainable params: 404,244\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# Keras\u306e\u95a2\u6570\u3092\u7528\u3044\u3066\u30e2\u30c7\u30eb\u3092dot\u5316\u3057\u305f\u5f8c\u306b{DiagrammeR}\u3067\u53ef\u8996\u5316\n# VAE\u30e2\u30c7\u30eb\u69cb\u6210\n> DiagrammeR::grViz(diagram = model_dot(model = vae, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n\n\n\n\nVAE\u30e2\u30c7\u30eb\u5b66\u7fd2\n# callback\u95a2\u6570\u3067Tensorboard\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3002\u5f15\u6570\u306b\u306f\u30ed\u30b0\u306e\u51fa\u529b\u5148\u3092\u6307\u5b9a\ntb <- keras$callbacks$TensorBoard(log_dir = \"./tensor_board/\")\n\n# callbacks\u5f15\u6570\u306b\u5148\u306eTensorboard\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u52a0\u3048\u305fR\u306elist\u3092\u6307\u5b9a\u3057\u3066\u5b66\u7fd2\u3092\u958b\u59cb\n> vae_hist <- vae$fit(\n  x = x_train, y = x_train, shuffle = TRUE, nb_epoch = NB_EPOCH, batch_size = BATCH_SIZE,\n  verbose = 1L, callbacks = list(tb)\n)\nEpoch 1/100\n60000/60000 [==============================] - 12s - loss: 191.4784 - mean_squared_error: 0.0577    \nEpoch 2/100\n60000/60000 [==============================] - 18s - loss: 170.3066 - mean_squared_error: 0.0508    \nEpoch 3/100\n60000/60000 [==============================] - 15s - loss: 166.8076 - mean_squared_error: 0.0493    \nEpoch 4/100\n60000/60000 [==============================] - 15s - loss: 164.6561 - mean_squared_error: 0.0483    \nEpoch 5/100\n60000/60000 [==============================] - 14s - loss: 163.0909 - mean_squared_error: 0.0475    \nEpoch 6/100\n60000/60000 [==============================] - 13s - loss: 161.8344 - mean_squared_error: 0.0469    \nEpoch 7/100\n60000/60000 [==============================] - 12s - loss: 160.6972 - mean_squared_error: 0.0463    \nEpoch 8/100\n60000/60000 [==============================] - 14s - loss: 159.6432 - mean_squared_error: 0.0458    \nEpoch 9/100\n60000/60000 [==============================] - 13s - loss: 158.6979 - mean_squared_error: 0.0453    \nEpoch 10/100\n60000/60000 [==============================] - 14s - loss: 157.8867 - mean_squared_error: 0.0449    \nEpoch 11/100\n60000/60000 [==============================] - 13s - loss: 157.1553 - mean_squared_error: 0.0445    \nEpoch 12/100\n60000/60000 [==============================] - 14s - loss: 156.5216 - mean_squared_error: 0.0442    \nEpoch 13/100\n60000/60000 [==============================] - 14s - loss: 155.9646 - mean_squared_error: 0.0440    \nEpoch 14/100\n60000/60000 [==============================] - 14s - loss: 155.4928 - mean_squared_error: 0.0437    \nEpoch 15/100\n60000/60000 [==============================] - 14s - loss: 155.0626 - mean_squared_error: 0.0435    \nEpoch 16/100\n60000/60000 [==============================] - 15s - loss: 154.6859 - mean_squared_error: 0.0433    \nEpoch 17/100\n60000/60000 [==============================] - 14s - loss: 154.3239 - mean_squared_error: 0.0431    \nEpoch 18/100\n60000/60000 [==============================] - 13s - loss: 154.0035 - mean_squared_error: 0.0430    \nEpoch 19/100\n60000/60000 [==============================] - 13s - loss: 153.6581 - mean_squared_error: 0.0428    \nEpoch 20/100\n60000/60000 [==============================] - 13s - loss: 153.3801 - mean_squared_error: 0.0427    \nEpoch 21/100\n60000/60000 [==============================] - 14s - loss: 153.1392 - mean_squared_error: 0.0426    \nEpoch 22/100\n60000/60000 [==============================] - 15s - loss: 152.8732 - mean_squared_error: 0.0425    \nEpoch 23/100\n60000/60000 [==============================] - 14s - loss: 152.6677 - mean_squared_error: 0.0423    \nEpoch 24/100\n60000/60000 [==============================] - 13s - loss: 152.4509 - mean_squared_error: 0.0422    \nEpoch 25/100\n60000/60000 [==============================] - 13s - loss: 152.2459 - mean_squared_error: 0.0421    \nEpoch 26/100\n60000/60000 [==============================] - 14s - loss: 152.0889 - mean_squared_error: 0.0421    \nEpoch 27/100\n60000/60000 [==============================] - 13s - loss: 151.8911 - mean_squared_error: 0.0420    \nEpoch 28/100\n60000/60000 [==============================] - 12s - loss: 151.7171 - mean_squared_error: 0.0419    \nEpoch 29/100\n60000/60000 [==============================] - 12s - loss: 151.5549 - mean_squared_error: 0.0418    \nEpoch 30/100\n60000/60000 [==============================] - 12s - loss: 151.4016 - mean_squared_error: 0.0417    \nEpoch 31/100\n60000/60000 [==============================] - 12s - loss: 151.2778 - mean_squared_error: 0.0417    \nEpoch 32/100\n60000/60000 [==============================] - 12s - loss: 151.1159 - mean_squared_error: 0.0416    \nEpoch 33/100\n60000/60000 [==============================] - 12s - loss: 151.0022 - mean_squared_error: 0.0415    \nEpoch 34/100\n60000/60000 [==============================] - 12s - loss: 150.8492 - mean_squared_error: 0.0414    \nEpoch 35/100\n60000/60000 [==============================] - 13s - loss: 150.7255 - mean_squared_error: 0.0414    \nEpoch 36/100\n60000/60000 [==============================] - 15s - loss: 150.5891 - mean_squared_error: 0.0413    \nEpoch 37/100\n60000/60000 [==============================] - 14s - loss: 150.5213 - mean_squared_error: 0.0413    \nEpoch 38/100\n60000/60000 [==============================] - 14s - loss: 150.3956 - mean_squared_error: 0.0412    \nEpoch 39/100\n60000/60000 [==============================] - 16s - loss: 150.2792 - mean_squared_error: 0.0412    \nEpoch 40/100\n60000/60000 [==============================] - 15s - loss: 150.1885 - mean_squared_error: 0.0411    \nEpoch 41/100\n60000/60000 [==============================] - 15s - loss: 150.0715 - mean_squared_error: 0.0411    \nEpoch 42/100\n60000/60000 [==============================] - 13s - loss: 149.9864 - mean_squared_error: 0.0410    \nEpoch 43/100\n60000/60000 [==============================] - 13s - loss: 149.8985 - mean_squared_error: 0.0410    \nEpoch 44/100\n60000/60000 [==============================] - 13s - loss: 149.7982 - mean_squared_error: 0.0409    \nEpoch 45/100\n60000/60000 [==============================] - 14s - loss: 149.7250 - mean_squared_error: 0.0409    \nEpoch 46/100\n60000/60000 [==============================] - 12s - loss: 149.6136 - mean_squared_error: 0.0408    \nEpoch 47/100\n60000/60000 [==============================] - 13s - loss: 149.5333 - mean_squared_error: 0.0408    \nEpoch 48/100\n60000/60000 [==============================] - 13s - loss: 149.4482 - mean_squared_error: 0.0407    \nEpoch 49/100\n60000/60000 [==============================] - 15s - loss: 149.3567 - mean_squared_error: 0.0407    \nEpoch 50/100\n60000/60000 [==============================] - 14s - loss: 149.2792 - mean_squared_error: 0.0407    \nEpoch 51/100\n60000/60000 [==============================] - 14s - loss: 149.2187 - mean_squared_error: 0.0406    \nEpoch 52/100\n60000/60000 [==============================] - 15s - loss: 149.1258 - mean_squared_error: 0.0406    \nEpoch 53/100\n60000/60000 [==============================] - 14s - loss: 149.0511 - mean_squared_error: 0.0405    \nEpoch 54/100\n60000/60000 [==============================] - 13s - loss: 148.9863 - mean_squared_error: 0.0405    \nEpoch 55/100\n60000/60000 [==============================] - 14s - loss: 148.9004 - mean_squared_error: 0.0405    \nEpoch 56/100\n60000/60000 [==============================] - 14s - loss: 148.8643 - mean_squared_error: 0.0404    \nEpoch 57/100\n60000/60000 [==============================] - 12s - loss: 148.7888 - mean_squared_error: 0.0404    \nEpoch 58/100\n60000/60000 [==============================] - 12s - loss: 148.6941 - mean_squared_error: 0.0404    \nEpoch 59/100\n60000/60000 [==============================] - 12s - loss: 148.6297 - mean_squared_error: 0.0403    \nEpoch 60/100\n60000/60000 [==============================] - 12s - loss: 148.5614 - mean_squared_error: 0.0403    \nEpoch 61/100\n60000/60000 [==============================] - 12s - loss: 148.5167 - mean_squared_error: 0.0403    \nEpoch 62/100\n60000/60000 [==============================] - 12s - loss: 148.4498 - mean_squared_error: 0.0402    \nEpoch 63/100\n60000/60000 [==============================] - 12s - loss: 148.3782 - mean_squared_error: 0.0402    \nEpoch 64/100\n60000/60000 [==============================] - 12s - loss: 148.3564 - mean_squared_error: 0.0402    \nEpoch 65/100\n60000/60000 [==============================] - 12s - loss: 148.2764 - mean_squared_error: 0.0401    \nEpoch 66/100\n60000/60000 [==============================] - 12s - loss: 148.2129 - mean_squared_error: 0.0401    \nEpoch 67/100\n60000/60000 [==============================] - 12s - loss: 148.1271 - mean_squared_error: 0.0401    \nEpoch 68/100\n60000/60000 [==============================] - 12s - loss: 148.1235 - mean_squared_error: 0.0401    \nEpoch 69/100\n60000/60000 [==============================] - 12s - loss: 148.0465 - mean_squared_error: 0.0400    \nEpoch 70/100\n60000/60000 [==============================] - 12s - loss: 148.0112 - mean_squared_error: 0.0400    \nEpoch 71/100\n60000/60000 [==============================] - 12s - loss: 147.9145 - mean_squared_error: 0.0400    \nEpoch 72/100\n60000/60000 [==============================] - 12s - loss: 147.8853 - mean_squared_error: 0.0399    \nEpoch 73/100\n60000/60000 [==============================] - 12s - loss: 147.8278 - mean_squared_error: 0.0399    \nEpoch 74/100\n60000/60000 [==============================] - 12s - loss: 147.7964 - mean_squared_error: 0.0399    \nEpoch 75/100\n60000/60000 [==============================] - 12s - loss: 147.7580 - mean_squared_error: 0.0399    \nEpoch 76/100\n60000/60000 [==============================] - 12s - loss: 147.6748 - mean_squared_error: 0.0398    \nEpoch 77/100\n60000/60000 [==============================] - 12s - loss: 147.6575 - mean_squared_error: 0.0398    \nEpoch 78/100\n60000/60000 [==============================] - 12s - loss: 147.5867 - mean_squared_error: 0.0398    \nEpoch 79/100\n60000/60000 [==============================] - 12s - loss: 147.5454 - mean_squared_error: 0.0398    \nEpoch 80/100\n60000/60000 [==============================] - 12s - loss: 147.5190 - mean_squared_error: 0.0398    \nEpoch 81/100\n60000/60000 [==============================] - 12s - loss: 147.4637 - mean_squared_error: 0.0397    \nEpoch 82/100\n60000/60000 [==============================] - 12s - loss: 147.4142 - mean_squared_error: 0.0397    \nEpoch 83/100\n60000/60000 [==============================] - 12s - loss: 147.3433 - mean_squared_error: 0.0397    \nEpoch 84/100\n60000/60000 [==============================] - 12s - loss: 147.3191 - mean_squared_error: 0.0397    \nEpoch 85/100\n60000/60000 [==============================] - 12s - loss: 147.3064 - mean_squared_error: 0.0396    \nEpoch 86/100\n60000/60000 [==============================] - 12s - loss: 147.2359 - mean_squared_error: 0.0396    \nEpoch 87/100\n60000/60000 [==============================] - 12s - loss: 147.2098 - mean_squared_error: 0.0396    \nEpoch 88/100\n60000/60000 [==============================] - 12s - loss: 147.1685 - mean_squared_error: 0.0396    \nEpoch 89/100\n60000/60000 [==============================] - 12s - loss: 147.0960 - mean_squared_error: 0.0395    \nEpoch 90/100\n60000/60000 [==============================] - 12s - loss: 147.0898 - mean_squared_error: 0.0395    \nEpoch 91/100\n60000/60000 [==============================] - 12s - loss: 147.0344 - mean_squared_error: 0.0395    \nEpoch 92/100\n60000/60000 [==============================] - 12s - loss: 146.9816 - mean_squared_error: 0.0395    \nEpoch 93/100\n60000/60000 [==============================] - 12s - loss: 146.9709 - mean_squared_error: 0.0395    \nEpoch 94/100\n60000/60000 [==============================] - 12s - loss: 146.9162 - mean_squared_error: 0.0394    \nEpoch 95/100\n60000/60000 [==============================] - 12s - loss: 146.8808 - mean_squared_error: 0.0394    \nEpoch 96/100\n60000/60000 [==============================] - 14s - loss: 146.8675 - mean_squared_error: 0.0394    \nEpoch 97/100\n60000/60000 [==============================] - 14s - loss: 146.7880 - mean_squared_error: 0.0394    \nEpoch 98/100\n60000/60000 [==============================] - 13s - loss: 146.7733 - mean_squared_error: 0.0394    \nEpoch 99/100\n60000/60000 [==============================] - 14s - loss: 146.7276 - mean_squared_error: 0.0394    \nEpoch 100/100\n60000/60000 [==============================] - 13s - loss: 146.7409 - mean_squared_error: 0.0394    \n\n# MSE\u3068Loss\u306e\u7d4c\u904e\u3092\u30d7\u30ed\u30c3\u30c8\n> dplyr::data_frame(\n  mse = unlist(vae_hist$history$mse),\n  loss = unlist(vae_hist$history$loss)\n) %>% \n  tibble::rownames_to_column(var = \"epoch\") %>% \n  dplyr::mutate(epoch = as.integer(x = epoch)) %>% \n  tidyr::gather(key = var, value = loss, -epoch) %>% \n  ggplot2::ggplot(data = ., mapping = ggplot2::aes(x = epoch, y = loss, group = var, colour = var)) +\n  ggplot2::geom_line() + \n  ggplot2::facet_wrap(facets = ~ var, nrow = 2, scales = \"free\")\n\n\n\n\u3000\u308f\u308a\u3068\u53ce\u675f\u3057\u3066\u3044\u305d\u3046\u3067\u3059\u304c\u3001\u3082\u3046\u5c11\u3057Epoch\u3092\u56de\u3057\u3066\u3082\u3088\u3055\u305d\u3046\u3067\u3059\u306d\u3002\n\nTensorboard\n\u3000\u5b66\u7fd2\u6642\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u306bTensorboard\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u52a0\u3048\u3066\u3044\u305f\u306e\u3067\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u304b\u3089\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067Tensorboard\u306b\u3088\u308b\u5b66\u7fd2\u7d4c\u904e\u306e\u53ef\u8996\u5316\u3067\u304d\u307e\u3059\u3002\n\nTensorboard\u306e\u8d77\u52d5\n# tensorboard\u30b3\u30de\u30f3\u30c9\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308c\u3070\u4e0b\u8a18\u3067OK\n$ tensorboard --logdir=~/tensor_board/\n\n# \u81ea\u524d\u306eMac\u74b0\u5883\u3067\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306btensorboard.py\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u53e9\u3044\u3066\u3044\u308b\n$ python /usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=~/tensor_board/\n\n\n\u3000Tensorboard\u306e\u8d77\u52d5\u5f8c\u306f\u300chttp://localhost:6006\u300d\uff08EC2\u4e0a\u3067\u8d77\u52d5\u3057\u3066\u3044\u308b\u5834\u5408\u306flocalhost\u3067\u306f\u306a\u304f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9IP\u3067\u78ba\u8a8d\u53ef\u80fd\u3002\u305f\u3060\u3057\u3001\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30b0\u30eb\u30fc\u30d7\u306e\u30eb\u30fc\u30eb\u30676006\u304c\u7a7a\u3044\u3066\u3044\u308b\u3053\u3068\uff09\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3053\u3068\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u753b\u9762\u3067\u5b66\u7fd2\u72b6\u6cc1\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\n\nEncorder\n\nencorder\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9\nencoder <- keras$models$Model(input = x, output = z_mean)\nx_test_encoded <- encoder$predict(x = x_test, batch_size = BATCH_SIZE)\n\n# encorder\u306e\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> encoder$summary()\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (100, 784)            0                                            \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (100, 256)            200960      input_1[0][0]                    \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n====================================================================================================\nTotal params: 201,474\nTrainable params: 201,474\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# encorder\u306e\u30e2\u30c7\u30eb\u69cb\u6210\u3092\u53ef\u8996\u5316\n> DiagrammeR::grViz(diagram = model_dot(model = encoder, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n\n\n\n\nGenerator\n\ngenerator\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9\ndecoder_input <- keras$layers$Input(shape = list(LATENT_DIM))\nh_decoded_ <- decoder_h$`__call__`(decoder_input)\nx_decoded_mean_ <- decoder_mean$`__call__`(h_decoded_)\ngenerator <- keras$models$Model(input = decoder_input, output = x_decoded_mean_)\n\n# generator\u306e\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> generator$summary()\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_2 (InputLayer)             (None, 2)             0                                            \n____________________________________________________________________________________________________\ndense_4 (Dense)                  multiple              768         input_2[0][0]                    \n____________________________________________________________________________________________________\ndense_5 (Dense)                  multiple              201488      dense_4[1][0]                    \n====================================================================================================\nTotal params: 202,256\nTrainable params: 202,256\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# generator\u306e\u30e2\u30c7\u30eb\u69cb\u6210\u3092\u53ef\u8996\u5316\n> DiagrammeR::grViz(diagram = model_dot(model = generator, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n\n\n\n\n\u753b\u50cf\u751f\u6210\u90e8\n\u3000generator\u306b\u4e71\u6570\u5024\u3092\u4e0e\u3048\u3001\u6570\u5024\u753b\u50cf\u3092\u8868\u3059\u884c\u5217\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n\u753b\u50cf\u751f\u6210\n# \u751f\u6210\u7528\u74b0\u5883\u306e\u8a2d\u5b9a\nn <- 15\ndigit_size <- 28\n\ngrid_x <- qnorm(p = seq(from = 0.05, to = 0.95, length.out = n), mean = 0, sd = 1)\ngrid_y <- qnorm(p = seq(from = 0.05, to = 0.95, length.out = n), mean = 0, sd = 1)\n\n# Python\u306e\u30d3\u30eb\u30c8\u30a4\u30f3\u95a2\u6570\u3092\u4f7f\u3046\u305f\u3081\u306breticulate::import_builtins\u3092\u5229\u7528\nm <- reticulate::import_builtins(convert = TRUE)\n# \u4e0b\u8a18\u306e\u3088\u3046\u306b\u3057\u3066Python\u306e\u95a2\u6570\u3092\u4f7f\u3046\u3053\u3068\u3082\u53ef\u80fd\nii <- reticulate::iterate(x = m$enumerate(grid_x))\njj <- reticulate::iterate(x = m$enumerate(grid_x))\n\npl <- apply(\n  X = expand.grid(sapply(X = ii, FUN = \"[[\", 2), sapply(X = jj, FUN = \"[[\", 2)),\n  MARGIN = 1,\n  FUN = function(ii) {\n\u3000\u3000 # \u4e71\u6570\u304b\u3089\u753b\u50cf\u751f\u6210\n    x_decoded <- generator$predict(x = matrix(data = ii, nrow = 1, ncol = 2))\n    return(\n      grid::rasterGrob(\n        image = matrix(data = x_decoded[1, ], nrow = digit_size, ncol = digit_size, byrow = FALSE),\n        interpolate = FALSE\n      )\n    )\n  }\n)\nml <- gridExtra::marrangeGrob(grobs = pl, nrow = n, ncol = n)\n# ggplot2::ggsave(filename = \"vae.png\", plot = ml, device = \"png\")\n\n\n\n\u3000\u6570\u5b57\u753b\u50cf\u304c\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u305d\u3046\u3067\u3059\u3002\n\n\u307e\u3068\u3081\n\u3000Python\u306e\u95a2\u6570\u5b9a\u7fa9\u3092\u4e00\u90e8\u5229\u7528\u3057\u307e\u3057\u305f\u304c\u3001VAE\u306b\u3088\u308b\u6570\u5b57\u753b\u50cf\u751f\u6210\u306eexample\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304d\u63db\u3048\u3001R\u4e0a\u3067\u753b\u50cf\u751f\u6210\u3067\u304d\u308b\u307e\u3067\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002Keras\u306e\u304a\u52c9\u5f37\u304c\u9032\u3093\u3060\u3053\u3068\u306b\u3088\u308a\u3001\u4ee5\u524d\u306b\u884c\u3063\u305f\u77ed\u6b4c\u751f\u6210\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u3092\u4e0a\u3052\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\u3000\u307e\u305f\u3001\u5f53\u521d\u306fDCGAN\u3092\u8a66\u3057\u3066\u307f\u3088\u3046\u3068\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u304c\u53b3\u3057\u3044\u70b9\u3068Keras\u306eexample\u304c\u602a\u3057\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u8ae6\u3081\u307e\u3057\u305f\uff08\u4f55\u5ea6\u8a66\u3057\u3066\u3082\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\uff09\u3002\n\u3000\u751f\u6210\u30e2\u30c7\u30eb\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u5fdc\u7528\u306e\u5e45\u304c\u3042\u308a\u305d\u3046\u306a\u306e\u3067\u3001\u3082\u3046\u5c11\u3057\u8a73\u3057\u304f\u89e6\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u53c2\u8003\n\n\u8ad6\u6587\u7d39\u4ecb Semi-supervised Learning with Deep Generative Models\n\u732b\u3067\u3082\u5206\u304b\u308bVariational AutoEncoder\nKeras\u3067Variational AutoEncoder\nVariational Autoencoder\uff08VAE\uff09\u3067\u751f\u6210\u30e2\u30c7\u30eb\nVAE\u304b\u3089CVAE with keras\nVariational Auto Encoder\nVAE\u3068GAN\u3092\u6d3b\u7528\u3057\u305f\u30d5\u30a1\u30c3\u30b7\u30e7\u30f3\u30a2\u30a4\u30c6\u30e0\u691c\u7d22\u30b7\u30b9\u30c6\u30e0\nKeras\u3067\u5b66\u3076Autoencoder\nKeras Generative Adversarial Networks\nKeras\u3067DCGAN\u66f8\u304f\n\u306f\u3058\u3081\u3066\u306eGAN\nMNIST Generative Adversarial Model in Keras\nUnrolled Generative Adversarial Networks [arXiv:1611.02163]\nTrainable = False isn't freezing weights\nPython object instantiation fails for classes defined in an R session\nTrying to use scikit-learn from R\n\n\n\u5b9f\u884c\u74b0\u5883\n\nR\u5b9f\u884c\u74b0\u5883\n> devtools::session_info()\nSession info ----------------------------------------------------------------------------------------------------\n setting  value                       \n version  R version 3.3.2 (2016-10-31)\n system   x86_64, darwin15.6.0        \n ui       RStudio (1.0.136)           \n language (EN)                        \n collate  ja_JP.UTF-8                 \n tz       Asia/Tokyo                  \n date     2017-02-26                  \n\nPackages --------------------------------------------------------------------------------------------------------\n package     * version date       source                             \n assertthat    0.1     2013-12-06 CRAN (R 3.3.2)                     \n colorspace    1.3-2   2016-12-14 CRAN (R 3.3.2)                     \n DBI           0.5-1   2016-09-10 CRAN (R 3.3.2)                     \n devtools      1.12.0  2016-12-05 CRAN (R 3.3.2)                     \n DiagrammeR  * 0.8.4   2016-07-17 CRAN (R 3.3.2)                     \n digest        0.6.10  2016-08-02 CRAN (R 3.3.2)                     \n dplyr       * 0.5.0   2016-06-24 CRAN (R 3.3.2)                     \n ggplot2     * 2.2.0   2016-11-11 CRAN (R 3.3.2)                     \n gridExtra     2.2.1   2016-02-29 CRAN (R 3.3.2)                     \n gtable        0.2.0   2016-02-26 CRAN (R 3.3.2)                     \n htmltools     0.3.5   2016-03-21 CRAN (R 3.3.2)                     \n htmlwidgets   0.8     2016-11-09 CRAN (R 3.3.2)                     \n igraph        1.0.1   2015-06-26 CRAN (R 3.3.2)                     \n influenceR    0.1.0   2015-09-03 cran (@0.1.0)                      \n jsonlite      1.1     2016-09-14 CRAN (R 3.3.2)                     \n labeling      0.3     2014-08-23 CRAN (R 3.3.2)                     \n lazyeval      0.2.0   2016-06-12 CRAN (R 3.3.2)                     \n magrittr      1.5     2014-11-22 CRAN (R 3.3.2)                     \n memoise       1.0.0   2016-01-29 CRAN (R 3.3.2)                     \n munsell       0.4.3   2016-02-13 CRAN (R 3.3.2)                     \n plyr          1.8.4   2016-06-08 CRAN (R 3.3.2)                     \n purrr       * 0.2.2   2016-06-18 CRAN (R 3.3.2)                     \n R6            2.2.0   2016-10-05 CRAN (R 3.3.2)                     \n Rcpp          0.12.8  2016-11-17 CRAN (R 3.3.2)                     \n readr       * 1.0.0   2016-08-03 CRAN (R 3.3.2)                     \n reticulate  * 0.6.0   2017-02-17 Github (rstudio/reticulate@519fa07)\n RevoUtils     10.0.2  2016-11-22 local                              \n rstudioapi    0.6     2016-06-27 CRAN (R 3.3.2)                     \n scales        0.4.1   2016-11-09 CRAN (R 3.3.2)                     \n stringi       1.1.2   2016-10-01 CRAN (R 3.3.2)                     \n stringr       1.1.0   2016-08-19 CRAN (R 3.3.2)                     \n tibble      * 1.2     2016-08-26 CRAN (R 3.3.2)                     \n tidyr       * 0.6.0   2016-08-12 CRAN (R 3.3.2)                     \n tidyverse   * 1.0.0   2016-09-09 CRAN (R 3.3.2)                     \n visNetwork    1.0.2   2016-10-05 CRAN (R 3.3.2)                     \n withr         1.0.2   2016-06-20 CRAN (R 3.3.2)                     \n yaml          2.1.14  2016-11-12 CRAN (R 3.3.2)          \n\n\n\nPython\u5b9f\u884c\u74b0\u5883\n$ python --version\nPython 2.7.12\n\n$ pip list --format=columns | grep -e \"tensorflow\" -e \"Keras\"\nKeras                              1.2.2       \ntensorflow                         1.0.0\n\n\n# \u6982\u8981\n\n\u3000\u4ee5\u524d\u306b[{tensorflow}\u306ePython\u30e9\u30a4\u30d6\u30e9\u30ea\u3092import\u3059\u308b\u95a2\u6570\u3092\u7528\u3044\u3066R\u304b\u3089Keras\u3092\u547c\u3073\u51fa\u3057\u3001\u77ed\u6b4c\u751f\u6210\u3092\u8a66\u307f\u307e\u3057\u305f](http://qiita.com/yamano357/items/27bb3d39dc8047c46dba)\u3002\n\u3000\u3057\u304b\u3057\u306a\u304c\u3089\u3001\u8a66\u3057\u305f\u3082\u306e\u306e\u77e5\u898b\u304c\u8db3\u3089\u305a\u3001\u6e80\u8db3\u304c\u3044\u304f\u7d50\u679c\u304c\u51fa\u305b\u305f\u3068\u306f\u8a00\u3048\u307e\u305b\u3093\u3002\n\u3000\u305d\u3053\u3067\u4eca\u56de\u306fKeras\u306e\u304a\u52c9\u5f37\u3092\u517c\u306d\u3066\u3001Keras\u306eexample\u306b\u3042\u308b\u753b\u50cf\u751f\u6210(VAE; variational autoencoder)\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092R\u3067\u66f8\u304d\u63db\u3048\u3001\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u3053\u3046\u3057\u3066\u5f97\u305f\u77e5\u898b\u3092\u6d3b\u7528\u3057\u3066\u3001\u3088\u308a\u3053\u306a\u308c\u305f\u77ed\u6b4c\u751f\u6210\u3092\u76ee\u6307\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n- [Keras Documentation](https://keras.io)\n- [This script demonstrates how to build a variational autoencoder with Keras](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py)\n\n\u3000\u52a0\u3048\u3066\u4eca\u56de\u306f\u3001Python\u30e9\u30a4\u30d6\u30e9\u30ea\u3092import\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u3057\u3066{reticulate}\u3092\u7528\u3044\u307e\u3057\u305f\u3002\u3053\u3061\u3089\u306f{tensorflow}\u304b\u3089Python\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u8aad\u307f\u8fbc\u3080\u6a5f\u80fd\u304c\u5206\u3051\u3089\u308c\u305f\u3082\u306e\u3067\u3001{tensorflow}\u3067\u3082{reticulate}\u3092\u4f7f\u3046\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\u3000\u3055\u3089\u306b{reticulate}\u306b\u306fR\u4e0a\u3067Python\u3092\u53c2\u7167\u3059\u308b\u305f\u3081\u306e\u69d8\u3005\u306a\u6a5f\u80fd\u304c\u8ffd\u52a0\u3055\u308c\u3066\u304a\u308a\u3001\u4ee5\u524d\u306f\u3067\u304d\u306a\u304b\u3063\u305f\u3053\u3068\uff08\u4f8b\u3048\u3070\u30011\u8981\u7d20\u306eR\u30d9\u30af\u30c8\u30eb\u3092\u30b9\u30ab\u30e9\u30fc\u3068\u3057\u3066\u30a2\u30b5\u30a4\u30f3\u3057\u3088\u3046\u3068\u3059\u308b\u3068Python\u306e\u30ea\u30b9\u30c8\u306b\u5909\u63db\u3055\u308c\u3066\u3057\u307e\u3044\u3001\u30a8\u30e9\u30fc\u306b\u306a\u3063\u3066\u3044\u305f\uff09\u304c\u53ef\u80fd\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n- [R Interface to Python](https://github.com/rstudio/reticulate) \n\n\n\u3000\u306a\u304a\u3001\u672c\u8a18\u4e8b\u3067\u306f\u300cKeras\u3092\u7528\u3044\u305fVAE\u306ePython\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3001R\u3067\u5b9f\u884c\u3059\u308b\u306b\u306f\u3069\u3046\u3044\u3046\u98a8\u306b\u8a18\u8ff0\u3059\u308b\u304b\u300d\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\u304a\u308a\u307e\u3059\u3002VAE\u306e\u7406\u8ad6\u306b\u3064\u3044\u3066\u306f\u300c\u53c2\u8003\u300d\u306b\u3042\u308b\u30ea\u30f3\u30af\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\n\n# \u5b9a\u7fa9\u90e8\n\n## \u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u8aad\u307f\u8fbc\u307f\n\n```{r:\u30e9\u30a4\u30d6\u30ea\u8aad\u307f\u8fbc\u307f}\nlibrary(tidyverse)\nlibrary(reticulate)\nlibrary(DiagrammeR)\n\n# Keras\u3092R\u4e0a\u3067\u4f7f\u3048\u308b\u3088\u3046\u306b{reticulate}\u306eimport\u95a2\u6570\u3067\u8aad\u307f\u8fbc\u307f\nkeras <- reticulate::import(module = \"keras\")\n# \u30e2\u30c7\u30eb\u3092dot\u306b\u5909\u63db\u3059\u308bKeras\u95a2\u6570\u3092\u5225\u540d\u3067\u5b9a\u7fa9\uff08\u95a2\u6570\u540d\u304c\u9577\u304f\u306a\u308a\u3059\u304e\u3066\u8996\u8a8d\u6027\u304c\u60aa\u304b\u3063\u305f\u305f\u3081\uff09\nmodel_dot <- keras$utils$visualize_util$model_to_dot\n```\n\n## \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6e96\u5099\n```{r:MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f}\nmnist_d <- keras$datasets$mnist$load_data()\n# \u753b\u50cf\u30c7\u30fc\u30bf\u3092[0, 1]\u306b\u5909\u63db\nx_train <- array(\n  data = mnist_d[[1]][[1]] / 255,\n  dim = c(nrow(x = mnist_d[[1]][[1]]), cumprod(x = dim(x = mnist_d[[1]][[1]])[-1])[2])\n)\nx_test <- array(\n  data = mnist_d[[2]][[1]] / 255,\n  dim = c(nrow(x = mnist_d[[2]][[1]]), cumprod(x = dim(x = mnist_d[[2]][[1]])[-1])[2])\n)\n```\n\n## \u5b9a\u6570\u3068\u95a2\u6570\u306e\u5b9a\u7fa9\n\n\u3000R\u304b\u3089Keras(Python)\u306b\u6574\u6570\u5024\u3092\u6e21\u3059\u5834\u5408\u306f\u660e\u793a\u7684\u306b\u6574\u6570\u5024\u3068\u3057\u3066\u5b9a\u7fa9\u3059\u308b\u5fc5\u8981\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u6570\u5024\u306e\u5f8c\u308d\u306b\u300cL\u300d\u3092\u3064\u3051\u3066\u6574\u6570\u5024\u3068\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u8907\u6570\u307e\u3068\u3081\u3066\u6574\u6570\u5316\u3057\u305f\u3044\u3068\u304d\u306f`as.integer`\u3092\u7528\u3044\u3066\u3082\u3044\u3044\u3067\u3059\u3002\n\n```{r:\u5b9a\u6570\u5b9a\u7fa9}\n# \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\nBATCH_SIZE <- 100L\nORIGINAL_DIM <- 784L\nLATENT_DIM <- 2L\nINTERMEDIATE_DIM <- 256L\nEPSILON_STD <- 1.0\n\n# Epoch\u6570\nNB_EPOCH <- 100L\n```\n\n---\n\n\u3000\u30c6\u30f3\u30bd\u30eb\u540c\u58eb\u306e\u8a08\u7b97\u3092R\u4e0a\u3067\u5b9a\u7fa9\u3059\u308b\u3068\u578b\u5909\u63db\u3084backend\u306e\u7af6\u5408\u306a\u3069\u304c\u5384\u4ecb\u3060\u3063\u305f\u306e\u3067\u3001\u3053\u3053\u3067\u306fPython\u95a2\u6570\u3068\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002\n\u3000\u640d\u5931\u95a2\u6570\u3078\u306e\u8907\u6570\u5f15\u6570\u306e\u6e21\u3057\u65b9\u306f\u4e0b\u8a18\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n- [Passing additional arguments to objective function](https://github.com/fchollet/keras/issues/2121)\n\n```{r:\u95a2\u6570\u5b9a\u7fa9}\npy_def <- reticulate::py_run_string(\n  code = \"\nimport keras.backend as K\nfrom keras import objectives\ndef sampling(args):\n  z_mean, z_log_var = args\n  epsilon = K.random_normal(shape = (batch_size, latent_dim), mean = 0., std = epsilon_std)\n  return z_mean + K.exp(z_log_var / 2) * epsilon\n  \ndef vae_loss(z_mean, z_log_var):\n  def loss(x, x_decoded_mean):\n    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n    return xent_loss + kl_loss\n  return loss\n  \",\n  convert = FALSE\n)\n\n# Python\u5074\u306e\u74b0\u5883\u306b\u5b9a\u6570\u5b9a\u7fa9\npy_def$batch_size <- BATCH_SIZE\npy_def$original_dim <- ORIGINAL_DIM\npy_def$latent_dim <- LATENT_DIM\npy_def$epsilon_std <- EPSILON_STD\n```\n\n\n# \u30e2\u30c7\u30eb\u5b9a\u7fa9\u3068\u5b66\u7fd2\n\u3000\u30b9\u30af\u30ea\u30d7\u30c8\u4f8b\u3092\u53c2\u8003\u306b\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u3092\u7c9b\u3005\u3068\u884c\u3044\u307e\u3059\u3002\n\u3000\u30e1\u30bd\u30c3\u30c9\u547c\u3073\u51fa\u3057\u306b\u300c$`__call__`\u300d\u3092\u7528\u3044\u3066\u304a\u308a\u3001\u8996\u8a8d\u6027\u304c\u3068\u3066\u3082\u60aa\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u3053\u3092\u3046\u307e\u304f\u3067\u304d\u308b\u3068\u4f7f\u3044\u3084\u3059\u304f\u3066\u3044\u3044\u3067\u3059\u304c\u3001\u5bfe\u5fdc\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u8981\u8abf\u67fb\u3067\u3059\u3002\n\n- [This script demonstrates how to build a variational autoencoder with Keras](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py)\n\n## Variational AutoEncoder \n```{r:VAE\u30e2\u30c7\u30eb\u5b9a\u7fa9}\nx <- keras$layers$Input(batch_shape = list(BATCH_SIZE, ORIGINAL_DIM))\nh <- keras$layers$Dense(output_dim = INTERMEDIATE_DIM, activation = \"relu\")$`__call__`(x = x)\n\nz_mean <- keras$layers$Dense(output_dim = LATENT_DIM)$`__call__`(x = h)\nz_log_var <- keras$layers$Dense(output_dim = LATENT_DIM)$`__call__`(x = h)\n\n# \u524d\u8ff0\u306e\u5b9a\u7fa9\u90e8\u3067\u5b9a\u7fa9\u3057\u305fPython\u95a2\u6570\u3092Lambda\u30ec\u30a4\u30e4\u30fc\u3067\u4f7f\u7528\u3059\u308b\u95a2\u6570\u306b\u6307\u5b9a\u3059\u308b\nz <- keras$layers$Lambda(py_def$sampling, output_shape = list(LATENT_DIM))$`__call__`(list(z_mean, z_log_var))\n\ndecoder_h <- keras$layers$Dense(output_dim = INTERMEDIATE_DIM, activation = \"relu\")\nh_decoded <- decoder_h$`__call__`(z)\ndecoder_mean <- keras$layers$Dense(output_dim = ORIGINAL_DIM, activation = \"sigmoid\")\nx_decoded_mean <- decoder_mean$`__call__`(h_decoded)\n\nvae <- keras$models$Model(input = x, output = x_decoded_mean)\nvae$compile(optimizer = \"rmsprop\", loss = py_def$vae_loss(z_mean, z_log_var), metrics = list(\"mse\"))\n\n# VAE\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> vae$summary()\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (100, 784)            0                                            \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (100, 256)            200960      input_1[0][0]                    \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n____________________________________________________________________________________________________\ndense_3 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n____________________________________________________________________________________________________\nlambda_1 (Lambda)                (100, 2)              0           dense_2[0][0]                    \n                                                                   dense_3[0][0]                    \n____________________________________________________________________________________________________\ndense_4 (Dense)                  (100, 256)            768         lambda_1[0][0]                   \n____________________________________________________________________________________________________\ndense_5 (Dense)                  (100, 784)            201488      dense_4[0][0]                    \n====================================================================================================\nTotal params: 404,244\nTrainable params: 404,244\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# Keras\u306e\u95a2\u6570\u3092\u7528\u3044\u3066\u30e2\u30c7\u30eb\u3092dot\u5316\u3057\u305f\u5f8c\u306b{DiagrammeR}\u3067\u53ef\u8996\u5316\n# VAE\u30e2\u30c7\u30eb\u69cb\u6210\n> DiagrammeR::grViz(diagram = model_dot(model = vae, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n```\n\n<img width=\"599\" alt=\"vae-model.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/99957/3e132b95-7de1-d633-071a-ba3475e35eac.png\">\n\n\n```{r:VAE\u30e2\u30c7\u30eb\u5b66\u7fd2}\n# callback\u95a2\u6570\u3067Tensorboard\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3002\u5f15\u6570\u306b\u306f\u30ed\u30b0\u306e\u51fa\u529b\u5148\u3092\u6307\u5b9a\ntb <- keras$callbacks$TensorBoard(log_dir = \"./tensor_board/\")\n\n# callbacks\u5f15\u6570\u306b\u5148\u306eTensorboard\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u52a0\u3048\u305fR\u306elist\u3092\u6307\u5b9a\u3057\u3066\u5b66\u7fd2\u3092\u958b\u59cb\n> vae_hist <- vae$fit(\n  x = x_train, y = x_train, shuffle = TRUE, nb_epoch = NB_EPOCH, batch_size = BATCH_SIZE,\n  verbose = 1L, callbacks = list(tb)\n)\nEpoch 1/100\n60000/60000 [==============================] - 12s - loss: 191.4784 - mean_squared_error: 0.0577    \nEpoch 2/100\n60000/60000 [==============================] - 18s - loss: 170.3066 - mean_squared_error: 0.0508    \nEpoch 3/100\n60000/60000 [==============================] - 15s - loss: 166.8076 - mean_squared_error: 0.0493    \nEpoch 4/100\n60000/60000 [==============================] - 15s - loss: 164.6561 - mean_squared_error: 0.0483    \nEpoch 5/100\n60000/60000 [==============================] - 14s - loss: 163.0909 - mean_squared_error: 0.0475    \nEpoch 6/100\n60000/60000 [==============================] - 13s - loss: 161.8344 - mean_squared_error: 0.0469    \nEpoch 7/100\n60000/60000 [==============================] - 12s - loss: 160.6972 - mean_squared_error: 0.0463    \nEpoch 8/100\n60000/60000 [==============================] - 14s - loss: 159.6432 - mean_squared_error: 0.0458    \nEpoch 9/100\n60000/60000 [==============================] - 13s - loss: 158.6979 - mean_squared_error: 0.0453    \nEpoch 10/100\n60000/60000 [==============================] - 14s - loss: 157.8867 - mean_squared_error: 0.0449    \nEpoch 11/100\n60000/60000 [==============================] - 13s - loss: 157.1553 - mean_squared_error: 0.0445    \nEpoch 12/100\n60000/60000 [==============================] - 14s - loss: 156.5216 - mean_squared_error: 0.0442    \nEpoch 13/100\n60000/60000 [==============================] - 14s - loss: 155.9646 - mean_squared_error: 0.0440    \nEpoch 14/100\n60000/60000 [==============================] - 14s - loss: 155.4928 - mean_squared_error: 0.0437    \nEpoch 15/100\n60000/60000 [==============================] - 14s - loss: 155.0626 - mean_squared_error: 0.0435    \nEpoch 16/100\n60000/60000 [==============================] - 15s - loss: 154.6859 - mean_squared_error: 0.0433    \nEpoch 17/100\n60000/60000 [==============================] - 14s - loss: 154.3239 - mean_squared_error: 0.0431    \nEpoch 18/100\n60000/60000 [==============================] - 13s - loss: 154.0035 - mean_squared_error: 0.0430    \nEpoch 19/100\n60000/60000 [==============================] - 13s - loss: 153.6581 - mean_squared_error: 0.0428    \nEpoch 20/100\n60000/60000 [==============================] - 13s - loss: 153.3801 - mean_squared_error: 0.0427    \nEpoch 21/100\n60000/60000 [==============================] - 14s - loss: 153.1392 - mean_squared_error: 0.0426    \nEpoch 22/100\n60000/60000 [==============================] - 15s - loss: 152.8732 - mean_squared_error: 0.0425    \nEpoch 23/100\n60000/60000 [==============================] - 14s - loss: 152.6677 - mean_squared_error: 0.0423    \nEpoch 24/100\n60000/60000 [==============================] - 13s - loss: 152.4509 - mean_squared_error: 0.0422    \nEpoch 25/100\n60000/60000 [==============================] - 13s - loss: 152.2459 - mean_squared_error: 0.0421    \nEpoch 26/100\n60000/60000 [==============================] - 14s - loss: 152.0889 - mean_squared_error: 0.0421    \nEpoch 27/100\n60000/60000 [==============================] - 13s - loss: 151.8911 - mean_squared_error: 0.0420    \nEpoch 28/100\n60000/60000 [==============================] - 12s - loss: 151.7171 - mean_squared_error: 0.0419    \nEpoch 29/100\n60000/60000 [==============================] - 12s - loss: 151.5549 - mean_squared_error: 0.0418    \nEpoch 30/100\n60000/60000 [==============================] - 12s - loss: 151.4016 - mean_squared_error: 0.0417    \nEpoch 31/100\n60000/60000 [==============================] - 12s - loss: 151.2778 - mean_squared_error: 0.0417    \nEpoch 32/100\n60000/60000 [==============================] - 12s - loss: 151.1159 - mean_squared_error: 0.0416    \nEpoch 33/100\n60000/60000 [==============================] - 12s - loss: 151.0022 - mean_squared_error: 0.0415    \nEpoch 34/100\n60000/60000 [==============================] - 12s - loss: 150.8492 - mean_squared_error: 0.0414    \nEpoch 35/100\n60000/60000 [==============================] - 13s - loss: 150.7255 - mean_squared_error: 0.0414    \nEpoch 36/100\n60000/60000 [==============================] - 15s - loss: 150.5891 - mean_squared_error: 0.0413    \nEpoch 37/100\n60000/60000 [==============================] - 14s - loss: 150.5213 - mean_squared_error: 0.0413    \nEpoch 38/100\n60000/60000 [==============================] - 14s - loss: 150.3956 - mean_squared_error: 0.0412    \nEpoch 39/100\n60000/60000 [==============================] - 16s - loss: 150.2792 - mean_squared_error: 0.0412    \nEpoch 40/100\n60000/60000 [==============================] - 15s - loss: 150.1885 - mean_squared_error: 0.0411    \nEpoch 41/100\n60000/60000 [==============================] - 15s - loss: 150.0715 - mean_squared_error: 0.0411    \nEpoch 42/100\n60000/60000 [==============================] - 13s - loss: 149.9864 - mean_squared_error: 0.0410    \nEpoch 43/100\n60000/60000 [==============================] - 13s - loss: 149.8985 - mean_squared_error: 0.0410    \nEpoch 44/100\n60000/60000 [==============================] - 13s - loss: 149.7982 - mean_squared_error: 0.0409    \nEpoch 45/100\n60000/60000 [==============================] - 14s - loss: 149.7250 - mean_squared_error: 0.0409    \nEpoch 46/100\n60000/60000 [==============================] - 12s - loss: 149.6136 - mean_squared_error: 0.0408    \nEpoch 47/100\n60000/60000 [==============================] - 13s - loss: 149.5333 - mean_squared_error: 0.0408    \nEpoch 48/100\n60000/60000 [==============================] - 13s - loss: 149.4482 - mean_squared_error: 0.0407    \nEpoch 49/100\n60000/60000 [==============================] - 15s - loss: 149.3567 - mean_squared_error: 0.0407    \nEpoch 50/100\n60000/60000 [==============================] - 14s - loss: 149.2792 - mean_squared_error: 0.0407    \nEpoch 51/100\n60000/60000 [==============================] - 14s - loss: 149.2187 - mean_squared_error: 0.0406    \nEpoch 52/100\n60000/60000 [==============================] - 15s - loss: 149.1258 - mean_squared_error: 0.0406    \nEpoch 53/100\n60000/60000 [==============================] - 14s - loss: 149.0511 - mean_squared_error: 0.0405    \nEpoch 54/100\n60000/60000 [==============================] - 13s - loss: 148.9863 - mean_squared_error: 0.0405    \nEpoch 55/100\n60000/60000 [==============================] - 14s - loss: 148.9004 - mean_squared_error: 0.0405    \nEpoch 56/100\n60000/60000 [==============================] - 14s - loss: 148.8643 - mean_squared_error: 0.0404    \nEpoch 57/100\n60000/60000 [==============================] - 12s - loss: 148.7888 - mean_squared_error: 0.0404    \nEpoch 58/100\n60000/60000 [==============================] - 12s - loss: 148.6941 - mean_squared_error: 0.0404    \nEpoch 59/100\n60000/60000 [==============================] - 12s - loss: 148.6297 - mean_squared_error: 0.0403    \nEpoch 60/100\n60000/60000 [==============================] - 12s - loss: 148.5614 - mean_squared_error: 0.0403    \nEpoch 61/100\n60000/60000 [==============================] - 12s - loss: 148.5167 - mean_squared_error: 0.0403    \nEpoch 62/100\n60000/60000 [==============================] - 12s - loss: 148.4498 - mean_squared_error: 0.0402    \nEpoch 63/100\n60000/60000 [==============================] - 12s - loss: 148.3782 - mean_squared_error: 0.0402    \nEpoch 64/100\n60000/60000 [==============================] - 12s - loss: 148.3564 - mean_squared_error: 0.0402    \nEpoch 65/100\n60000/60000 [==============================] - 12s - loss: 148.2764 - mean_squared_error: 0.0401    \nEpoch 66/100\n60000/60000 [==============================] - 12s - loss: 148.2129 - mean_squared_error: 0.0401    \nEpoch 67/100\n60000/60000 [==============================] - 12s - loss: 148.1271 - mean_squared_error: 0.0401    \nEpoch 68/100\n60000/60000 [==============================] - 12s - loss: 148.1235 - mean_squared_error: 0.0401    \nEpoch 69/100\n60000/60000 [==============================] - 12s - loss: 148.0465 - mean_squared_error: 0.0400    \nEpoch 70/100\n60000/60000 [==============================] - 12s - loss: 148.0112 - mean_squared_error: 0.0400    \nEpoch 71/100\n60000/60000 [==============================] - 12s - loss: 147.9145 - mean_squared_error: 0.0400    \nEpoch 72/100\n60000/60000 [==============================] - 12s - loss: 147.8853 - mean_squared_error: 0.0399    \nEpoch 73/100\n60000/60000 [==============================] - 12s - loss: 147.8278 - mean_squared_error: 0.0399    \nEpoch 74/100\n60000/60000 [==============================] - 12s - loss: 147.7964 - mean_squared_error: 0.0399    \nEpoch 75/100\n60000/60000 [==============================] - 12s - loss: 147.7580 - mean_squared_error: 0.0399    \nEpoch 76/100\n60000/60000 [==============================] - 12s - loss: 147.6748 - mean_squared_error: 0.0398    \nEpoch 77/100\n60000/60000 [==============================] - 12s - loss: 147.6575 - mean_squared_error: 0.0398    \nEpoch 78/100\n60000/60000 [==============================] - 12s - loss: 147.5867 - mean_squared_error: 0.0398    \nEpoch 79/100\n60000/60000 [==============================] - 12s - loss: 147.5454 - mean_squared_error: 0.0398    \nEpoch 80/100\n60000/60000 [==============================] - 12s - loss: 147.5190 - mean_squared_error: 0.0398    \nEpoch 81/100\n60000/60000 [==============================] - 12s - loss: 147.4637 - mean_squared_error: 0.0397    \nEpoch 82/100\n60000/60000 [==============================] - 12s - loss: 147.4142 - mean_squared_error: 0.0397    \nEpoch 83/100\n60000/60000 [==============================] - 12s - loss: 147.3433 - mean_squared_error: 0.0397    \nEpoch 84/100\n60000/60000 [==============================] - 12s - loss: 147.3191 - mean_squared_error: 0.0397    \nEpoch 85/100\n60000/60000 [==============================] - 12s - loss: 147.3064 - mean_squared_error: 0.0396    \nEpoch 86/100\n60000/60000 [==============================] - 12s - loss: 147.2359 - mean_squared_error: 0.0396    \nEpoch 87/100\n60000/60000 [==============================] - 12s - loss: 147.2098 - mean_squared_error: 0.0396    \nEpoch 88/100\n60000/60000 [==============================] - 12s - loss: 147.1685 - mean_squared_error: 0.0396    \nEpoch 89/100\n60000/60000 [==============================] - 12s - loss: 147.0960 - mean_squared_error: 0.0395    \nEpoch 90/100\n60000/60000 [==============================] - 12s - loss: 147.0898 - mean_squared_error: 0.0395    \nEpoch 91/100\n60000/60000 [==============================] - 12s - loss: 147.0344 - mean_squared_error: 0.0395    \nEpoch 92/100\n60000/60000 [==============================] - 12s - loss: 146.9816 - mean_squared_error: 0.0395    \nEpoch 93/100\n60000/60000 [==============================] - 12s - loss: 146.9709 - mean_squared_error: 0.0395    \nEpoch 94/100\n60000/60000 [==============================] - 12s - loss: 146.9162 - mean_squared_error: 0.0394    \nEpoch 95/100\n60000/60000 [==============================] - 12s - loss: 146.8808 - mean_squared_error: 0.0394    \nEpoch 96/100\n60000/60000 [==============================] - 14s - loss: 146.8675 - mean_squared_error: 0.0394    \nEpoch 97/100\n60000/60000 [==============================] - 14s - loss: 146.7880 - mean_squared_error: 0.0394    \nEpoch 98/100\n60000/60000 [==============================] - 13s - loss: 146.7733 - mean_squared_error: 0.0394    \nEpoch 99/100\n60000/60000 [==============================] - 14s - loss: 146.7276 - mean_squared_error: 0.0394    \nEpoch 100/100\n60000/60000 [==============================] - 13s - loss: 146.7409 - mean_squared_error: 0.0394    \n\n# MSE\u3068Loss\u306e\u7d4c\u904e\u3092\u30d7\u30ed\u30c3\u30c8\n> dplyr::data_frame(\n  mse = unlist(vae_hist$history$mse),\n  loss = unlist(vae_hist$history$loss)\n) %>% \n  tibble::rownames_to_column(var = \"epoch\") %>% \n  dplyr::mutate(epoch = as.integer(x = epoch)) %>% \n  tidyr::gather(key = var, value = loss, -epoch) %>% \n  ggplot2::ggplot(data = ., mapping = ggplot2::aes(x = epoch, y = loss, group = var, colour = var)) +\n  ggplot2::geom_line() + \n  ggplot2::facet_wrap(facets = ~ var, nrow = 2, scales = \"free\")\n```\n\n![mse-loss.png](https://qiita-image-store.s3.amazonaws.com/0/99957/8d1e4631-adce-0c30-1809-3561cf8e6f40.png)\n\n\u3000\u308f\u308a\u3068\u53ce\u675f\u3057\u3066\u3044\u305d\u3046\u3067\u3059\u304c\u3001\u3082\u3046\u5c11\u3057Epoch\u3092\u56de\u3057\u3066\u3082\u3088\u3055\u305d\u3046\u3067\u3059\u306d\u3002\n\n\n## Tensorboard\n\u3000\u5b66\u7fd2\u6642\u306e\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u306bTensorboard\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u52a0\u3048\u3066\u3044\u305f\u306e\u3067\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u304b\u3089\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067Tensorboard\u306b\u3088\u308b\u5b66\u7fd2\u7d4c\u904e\u306e\u53ef\u8996\u5316\u3067\u304d\u307e\u3059\u3002\n\n```{shell:Tensorboard\u306e\u8d77\u52d5}\n# tensorboard\u30b3\u30de\u30f3\u30c9\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308c\u3070\u4e0b\u8a18\u3067OK\n$ tensorboard --logdir=~/tensor_board/\n\n# \u81ea\u524d\u306eMac\u74b0\u5883\u3067\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306btensorboard.py\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u53e9\u3044\u3066\u3044\u308b\n$ python /usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=~/tensor_board/\n```\n\n\u3000Tensorboard\u306e\u8d77\u52d5\u5f8c\u306f\u300c`http://localhost:6006`\u300d\uff08EC2\u4e0a\u3067\u8d77\u52d5\u3057\u3066\u3044\u308b\u5834\u5408\u306flocalhost\u3067\u306f\u306a\u304f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9IP\u3067\u78ba\u8a8d\u53ef\u80fd\u3002\u305f\u3060\u3057\u3001\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30b0\u30eb\u30fc\u30d7\u306e\u30eb\u30fc\u30eb\u30676006\u304c\u7a7a\u3044\u3066\u3044\u308b\u3053\u3068\uff09\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3053\u3068\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u753b\u9762\u3067\u5b66\u7fd2\u72b6\u6cc1\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\n\n<img width=\"1266\" alt=\"tensorboard.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/99957/4629b760-960e-b3a0-ac0b-b06a54f44a25.png\">\n\n## Encorder\n```{R:encorder\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9}\nencoder <- keras$models$Model(input = x, output = z_mean)\nx_test_encoded <- encoder$predict(x = x_test, batch_size = BATCH_SIZE)\n\n# encorder\u306e\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> encoder$summary()\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_1 (InputLayer)             (100, 784)            0                                            \n____________________________________________________________________________________________________\ndense_1 (Dense)                  (100, 256)            200960      input_1[0][0]                    \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (100, 2)              514         dense_1[0][0]                    \n====================================================================================================\nTotal params: 201,474\nTrainable params: 201,474\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# encorder\u306e\u30e2\u30c7\u30eb\u69cb\u6210\u3092\u53ef\u8996\u5316\n> DiagrammeR::grViz(diagram = model_dot(model = encoder, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n```\n\n<img width=\"604\" alt=\"encoder-model.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/99957/af49ee61-a5fa-5508-4d32-16484660d2ab.png\">\n\n\n## Generator\n```{R:generator\u306e\u30e2\u30c7\u30eb\u5b9a\u7fa9}\ndecoder_input <- keras$layers$Input(shape = list(LATENT_DIM))\nh_decoded_ <- decoder_h$`__call__`(decoder_input)\nx_decoded_mean_ <- decoder_mean$`__call__`(h_decoded_)\ngenerator <- keras$models$Model(input = decoder_input, output = x_decoded_mean_)\n\n# generator\u306e\u30e2\u30c7\u30eb\u306e\u8981\u7d04\u51fa\u529b\n> generator$summary()\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ninput_2 (InputLayer)             (None, 2)             0                                            \n____________________________________________________________________________________________________\ndense_4 (Dense)                  multiple              768         input_2[0][0]                    \n____________________________________________________________________________________________________\ndense_5 (Dense)                  multiple              201488      dense_4[1][0]                    \n====================================================================================================\nTotal params: 202,256\nTrainable params: 202,256\nNon-trainable params: 0\n____________________________________________________________________________________________________\n\n# generator\u306e\u30e2\u30c7\u30eb\u69cb\u6210\u3092\u53ef\u8996\u5316\n> DiagrammeR::grViz(diagram = model_dot(model = generator, show_shapes = TRUE)$create(prog = \"dot\", format = \"dot\"))\n```\n\n<img width=\"654\" alt=\"generator-model.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/99957/d453e700-4c50-a471-b53c-ef49716bf911.png\">\n\n\n# \u753b\u50cf\u751f\u6210\u90e8\n\u3000generator\u306b\u4e71\u6570\u5024\u3092\u4e0e\u3048\u3001\u6570\u5024\u753b\u50cf\u3092\u8868\u3059\u884c\u5217\u3092\u751f\u6210\u3057\u307e\u3059\u3002\n\n```{R:\u753b\u50cf\u751f\u6210}\n# \u751f\u6210\u7528\u74b0\u5883\u306e\u8a2d\u5b9a\nn <- 15\ndigit_size <- 28\n\ngrid_x <- qnorm(p = seq(from = 0.05, to = 0.95, length.out = n), mean = 0, sd = 1)\ngrid_y <- qnorm(p = seq(from = 0.05, to = 0.95, length.out = n), mean = 0, sd = 1)\n\n# Python\u306e\u30d3\u30eb\u30c8\u30a4\u30f3\u95a2\u6570\u3092\u4f7f\u3046\u305f\u3081\u306breticulate::import_builtins\u3092\u5229\u7528\nm <- reticulate::import_builtins(convert = TRUE)\n# \u4e0b\u8a18\u306e\u3088\u3046\u306b\u3057\u3066Python\u306e\u95a2\u6570\u3092\u4f7f\u3046\u3053\u3068\u3082\u53ef\u80fd\nii <- reticulate::iterate(x = m$enumerate(grid_x))\njj <- reticulate::iterate(x = m$enumerate(grid_x))\n\npl <- apply(\n  X = expand.grid(sapply(X = ii, FUN = \"[[\", 2), sapply(X = jj, FUN = \"[[\", 2)),\n  MARGIN = 1,\n  FUN = function(ii) {\n\u3000\u3000 # \u4e71\u6570\u304b\u3089\u753b\u50cf\u751f\u6210\n    x_decoded <- generator$predict(x = matrix(data = ii, nrow = 1, ncol = 2))\n    return(\n      grid::rasterGrob(\n        image = matrix(data = x_decoded[1, ], nrow = digit_size, ncol = digit_size, byrow = FALSE),\n        interpolate = FALSE\n      )\n    )\n  }\n)\nml <- gridExtra::marrangeGrob(grobs = pl, nrow = n, ncol = n)\n# ggplot2::ggsave(filename = \"vae.png\", plot = ml, device = \"png\")\n```\n\n![vae.png](https://qiita-image-store.s3.amazonaws.com/0/99957/e1a86551-bc0c-6f79-d671-bda10ff867b3.png)\n\n\u3000\u6570\u5b57\u753b\u50cf\u304c\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u305d\u3046\u3067\u3059\u3002\n\n\n# \u307e\u3068\u3081\n\u3000Python\u306e\u95a2\u6570\u5b9a\u7fa9\u3092\u4e00\u90e8\u5229\u7528\u3057\u307e\u3057\u305f\u304c\u3001VAE\u306b\u3088\u308b\u6570\u5b57\u753b\u50cf\u751f\u6210\u306eexample\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304d\u63db\u3048\u3001R\u4e0a\u3067\u753b\u50cf\u751f\u6210\u3067\u304d\u308b\u307e\u3067\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002Keras\u306e\u304a\u52c9\u5f37\u304c\u9032\u3093\u3060\u3053\u3068\u306b\u3088\u308a\u3001\u4ee5\u524d\u306b\u884c\u3063\u305f\u77ed\u6b4c\u751f\u6210\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u3092\u4e0a\u3052\u3089\u308c\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\n\u3000\u307e\u305f\u3001\u5f53\u521d\u306fDCGAN\u3092\u8a66\u3057\u3066\u307f\u3088\u3046\u3068\u3057\u305f\u306e\u3067\u3059\u304c\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u304c\u53b3\u3057\u3044\u70b9\u3068Keras\u306eexample\u304c\u602a\u3057\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u8ae6\u3081\u307e\u3057\u305f\uff08\u4f55\u5ea6\u8a66\u3057\u3066\u3082\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\uff09\u3002\n\u3000\u751f\u6210\u30e2\u30c7\u30eb\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306f\u5fdc\u7528\u306e\u5e45\u304c\u3042\u308a\u305d\u3046\u306a\u306e\u3067\u3001\u3082\u3046\u5c11\u3057\u8a73\u3057\u304f\u89e6\u3063\u3066\u307f\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\n\n\n# \u53c2\u8003\n- [\u8ad6\u6587\u7d39\u4ecb Semi-supervised Learning with Deep Generative Models](https://www.slideshare.net/beam2d/semisupervised-learning-with-deep-generative-models)\n- [\u732b\u3067\u3082\u5206\u304b\u308bVariational AutoEncoder](https://www.slideshare.net/ssusere55c63/variational-autoencoder-64515581)\n- [Keras\u3067Variational AutoEncoder](http://sh-tatsuno.com/blog/index.php/2016/07/30/variationalautoencoder/)\n- [Variational Autoencoder\uff08VAE\uff09\u3067\u751f\u6210\u30e2\u30c7\u30eb](http://qiita.com/RyotaKatoh/items/b353d14a5d7c3edc0f3e)\n- [VAE\u304b\u3089CVAE with keras](http://ralo23.hatenablog.com/entry/2016/08/26/163556)\n- [Variational Auto Encoder](http://nzw0301.github.io/notes/vae.pdf)\n- [VAE\u3068GAN\u3092\u6d3b\u7528\u3057\u305f\u30d5\u30a1\u30c3\u30b7\u30e7\u30f3\u30a2\u30a4\u30c6\u30e0\u691c\u7d22\u30b7\u30b9\u30c6\u30e0](http://tech.vasily.jp/entry/retrieval_using_vae_gan)\n- [Keras\u3067\u5b66\u3076Autoencoder](https://elix-tech.github.io/ja/2016/07/17/autoencoder.html)\n- [Keras Generative Adversarial Networks](https://github.com/bstriner/keras-adversarial)\n- [Keras\u3067DCGAN\u66f8\u304f](http://qiita.com/t-ae/items/236457c29ba85a7579d5)\n- [\u306f\u3058\u3081\u3066\u306eGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html)\n- [MNIST Generative Adversarial Model in Keras](http://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html)\n- [Unrolled Generative Adversarial Networks [arXiv:1611.02163]](http://musyoku.github.io/2017/01/29/Unrolled-Generative-Adversarial-Networks/)\n- [Trainable = False isn't freezing weights](https://github.com/fchollet/keras/issues/4674)\n- [Python object instantiation fails for classes defined in an R session](https://github.com/rstudio/reticulate/issues/4)\n- [Trying to use scikit-learn from R](https://github.com/dfalbel/rsk)\n\n\n## \u5b9f\u884c\u74b0\u5883\n```{r:R\u5b9f\u884c\u74b0\u5883}\n> devtools::session_info()\nSession info ----------------------------------------------------------------------------------------------------\n setting  value                       \n version  R version 3.3.2 (2016-10-31)\n system   x86_64, darwin15.6.0        \n ui       RStudio (1.0.136)           \n language (EN)                        \n collate  ja_JP.UTF-8                 \n tz       Asia/Tokyo                  \n date     2017-02-26                  \n\nPackages --------------------------------------------------------------------------------------------------------\n package     * version date       source                             \n assertthat    0.1     2013-12-06 CRAN (R 3.3.2)                     \n colorspace    1.3-2   2016-12-14 CRAN (R 3.3.2)                     \n DBI           0.5-1   2016-09-10 CRAN (R 3.3.2)                     \n devtools      1.12.0  2016-12-05 CRAN (R 3.3.2)                     \n DiagrammeR  * 0.8.4   2016-07-17 CRAN (R 3.3.2)                     \n digest        0.6.10  2016-08-02 CRAN (R 3.3.2)                     \n dplyr       * 0.5.0   2016-06-24 CRAN (R 3.3.2)                     \n ggplot2     * 2.2.0   2016-11-11 CRAN (R 3.3.2)                     \n gridExtra     2.2.1   2016-02-29 CRAN (R 3.3.2)                     \n gtable        0.2.0   2016-02-26 CRAN (R 3.3.2)                     \n htmltools     0.3.5   2016-03-21 CRAN (R 3.3.2)                     \n htmlwidgets   0.8     2016-11-09 CRAN (R 3.3.2)                     \n igraph        1.0.1   2015-06-26 CRAN (R 3.3.2)                     \n influenceR    0.1.0   2015-09-03 cran (@0.1.0)                      \n jsonlite      1.1     2016-09-14 CRAN (R 3.3.2)                     \n labeling      0.3     2014-08-23 CRAN (R 3.3.2)                     \n lazyeval      0.2.0   2016-06-12 CRAN (R 3.3.2)                     \n magrittr      1.5     2014-11-22 CRAN (R 3.3.2)                     \n memoise       1.0.0   2016-01-29 CRAN (R 3.3.2)                     \n munsell       0.4.3   2016-02-13 CRAN (R 3.3.2)                     \n plyr          1.8.4   2016-06-08 CRAN (R 3.3.2)                     \n purrr       * 0.2.2   2016-06-18 CRAN (R 3.3.2)                     \n R6            2.2.0   2016-10-05 CRAN (R 3.3.2)                     \n Rcpp          0.12.8  2016-11-17 CRAN (R 3.3.2)                     \n readr       * 1.0.0   2016-08-03 CRAN (R 3.3.2)                     \n reticulate  * 0.6.0   2017-02-17 Github (rstudio/reticulate@519fa07)\n RevoUtils     10.0.2  2016-11-22 local                              \n rstudioapi    0.6     2016-06-27 CRAN (R 3.3.2)                     \n scales        0.4.1   2016-11-09 CRAN (R 3.3.2)                     \n stringi       1.1.2   2016-10-01 CRAN (R 3.3.2)                     \n stringr       1.1.0   2016-08-19 CRAN (R 3.3.2)                     \n tibble      * 1.2     2016-08-26 CRAN (R 3.3.2)                     \n tidyr       * 0.6.0   2016-08-12 CRAN (R 3.3.2)                     \n tidyverse   * 1.0.0   2016-09-09 CRAN (R 3.3.2)                     \n visNetwork    1.0.2   2016-10-05 CRAN (R 3.3.2)                     \n withr         1.0.2   2016-06-20 CRAN (R 3.3.2)                     \n yaml          2.1.14  2016-11-12 CRAN (R 3.3.2)          \n```\n\n```{shell:Python\u5b9f\u884c\u74b0\u5883}\n$ python --version\nPython 2.7.12\n\n$ pip list --format=columns | grep -e \"tensorflow\" -e \"Keras\"\nKeras                              1.2.2       \ntensorflow                         1.0.0\n```\n", "tags": ["R", "Keras", "DeepLearning"]}