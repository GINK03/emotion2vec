{"context": "\n\n\u4ee5\u4e0b\u3001\u9818\u57df\u5225\u306b\u3001\u5165\u9580\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8 \u3078\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5f35\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n\n\u53c2\u8003\u66f8\u7c4d\n\n\nBryan O'Sullivan\u307b\u304b (\u8457)\u30fb\u5c71\u4e0b \u4f38\u592b (\u7ffb\u8a33) \u300eReal World Haskell\u2015\u5b9f\u6226\u3067\u5b66\u3076\u95a2\u6570\u578b\u8a00\u8a9e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u300f \u5927\u578b\u672c, 2009/10/26\n\n\n\nNishant Shukla (\u8457) Haskell Data Analysis Cookbook,  (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af \u2013 2014/6/25\n\n\n\nJames Church (\u8457) Learning Haskell Data,  (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af \u2013 2015/5/28\n\n\n\nPavel Ryzhov (\u8457),  Haskell Financial Data Modeling and Predictive Analytics, (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af, 2013/10/25\n\n\n\uff08\u3000Amazon \u306e\u300c\u4e2d\u8eab\u3092\u898b\u3066\u307f\u308b\u300d\u3088\u308a \u76ee\u6b21 \u3092 \u4e00\u90e8 \u8ee2\u8f09 \uff09\n\n\n\n\uff08 \u5168\u822c\u53c2\u8003 \uff09\n\n\nQuora Is Haskell a good fit for machine learning problems? Why? Or why not? \nstackoverflow Machine learning in OCaml or Haskell? \n\n\n\n\u3010 \u30d5\u30a1\u30a4\u30eb\u5165\u51fa\u529b \u3011\n\n\n\u30a6\u30a9\u30fc\u30af\u30b9\u30eb\u30fc Haskell > \u5165\u51fa\u529b \u300c\u5165\u51fa\u529b\u300d\n\nHatena::Diary (2010/12/11) \u300csirocco \u306e\u66f8\u3044\u3066\u3082\u3059\u3050\u306b\u5fd8\u308c\u308b\u30e1\u30e2\u300d* Functional Programming \u304a\u6c17\u697d Haskell \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u5165\u9580 \u300c\u30d5\u30a1\u30a4\u30eb\u5165\u51fa\u529b\u300d\n\n7shi\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/11) \u300cHaskell IO\u30e2\u30ca\u30c9 \u8d85\u5165\u9580\u300d\n\n\n\n\u3010 \u30b0\u30e9\u30d5\u63cf\u753b \u3011\n\n\ntapioka\u3055\u3093 Qiita\u8a18\u4e8b (2014/09/03) \u300cHaskell\u3067\u30b0\u30e9\u30d5\u63cf\u753b\u300d\n\u6368\u3066\u3089\u308c\u305f\u30d6\u30ed\u30b0 (2011/12/09) \u300cHaskell Chart \u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u30b0\u30e9\u30d5\u63cf\u753b\u300d\nHatena Blog \u7de0\u601d\u4e4b\u8a18 (2011/11/12) \u300cHaskell\u3067\u30b0\u30e9\u30d5\u3092\u66f8\u304f\u306b\u306f\u300d\n\n\n\n\u3010 \u7dda\u5f62\u56de\u5e30\u30fb\u7d71\u8a08\u89e3\u6790\u30fb\u591a\u5909\u91cf\u89e3\u6790 \u3011\n\n\nHatena Blog \u308a\u3093\u3054\u304c\u3067\u3066\u3044\u308b (2014/04/11) \u300cHaskell\u3067\u30c7\u30fc\u30bf\u51e6\u7406\u304c\u3057\u305f\u3044\uff01\u300d\nHatena::Diary mmitou\u306e\u65e5\u8a18 (2012/11/07) \u300cPRML\uff13\u7ae0\u306e\u7dda\u5f62\u56de\u5e30\u3092\u5b9f\u88c5\u3057\u305f\u300d\n\n\uff08 \u4e0a\u8a18 \u300cmmitou\u306e\u65e5\u8a18\u300d \u3088\u308a \u30b3\u30fc\u30c9\u3092\u4e00\u90e8\u629c\u7c8b\u3057\u3066\u8ee2\u8f09 \uff09\n\n\n\u30b3\u30fc\u30c9\uff08\u4e00\u90e8\u629c\u7c8b)\n-- \u5ea7\u6a19\u3092\u6587\u5b57\u5217\u306b\u3059\u308b\npointsToString :: (Show a) => [a] -> [a] -> String\npointsToString xs ys =\n  concat $\n  zipWith (\\x y -> show x ++ \" \" ++ show y ++ \"\\n\") xs ys\n\n-- \u5ea7\u6a19\u3092\u6587\u5b57\u5217\u306b\u3059\u308b\nto3dPosString xs ts = concat $\n                      zipWith (\\(x,y) z ->    (show x) ++ \" \"\n                                           ++ (show y) ++ \" \"\n                                           ++ (show z) ++ \"\\n\")\n                      xs ts\n\n-- \u30ac\u30a6\u30b9\u57fa\u5e95\u95a2\u6570\ngaussianBasis s mu x = exp $ (- (x - mu) ** 2.0) / s\n\n-- \u591a\u9805\u5f0f\u57fa\u5e95\u95a2\u6570\npolynomialBasis n x = x ** n\n\n-- \u4e8c\u6b21\u5143\u7528\u591a\u9805\u5f0f\u57fa\u5e95\u95a2\u6570\n-- \u591a\u6b21\u5143\u306e\u5834\u5408\u306b\u3053\u306e\u3088\u3046\u306a\u57fa\u5e95\u95a2\u6570\u3067\u3088\u3044\u306e\u304b\u3069\u3046\u304b\u306f\u78ba\u8a8d\u3057\u3066\u3044\u306a\u3044\u3002\npolynomialBasis' n (x,y) = x ** n + y ** n\n\n-- \u57fa\u5e95\u95a2\u6570\u306e\u914d\u5217\u3068\u5165\u529b\u5909\u6570x\u304b\u3089\u8a08\u753b\u884c\u5217\u03a6\u3092\u8a08\u7b97\u3059\u308b\ndesignMatrix basiss xs = (n >< m) [f x | x <- xs, f <- basiss]\n where\n   n = length xs\n   m = length basiss\n\n-- \u6b63\u5247\u5316\u4fc2\u6570\u3001\u8a08\u753b\u884c\u5217\u3068\u76ee\u6a19\u5909\u6570\u306e\u914d\u5217\u304b\u3089\n-- \u57fa\u5e95\u95a2\u6570\u306e\u7dda\u5f62\u7d50\u5408\u3067\u7528\u3044\u308b\u30d1\u30e9\u30e1\u30fc\u30bf w \u3092\u8a08\u7b97\u3059\u308b\nparameters lambda phi ts = w\n where\n   -- \u914d\u5217\u3092\u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\n   t       = (length ts >< 1) ts \n   m       = cols phi\n   -- (lambda * \u5358\u4f4d\u884c\u5217) \u3092\u8a08\u7b97\u3059\u308b\n   lambdaI = fromLists $ map (map (*lambda)) (toLists $ ident m)\n   -- \u8a08\u753b\u884c\u5217\u306e\u8ee2\u7f6e\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\n   phiT = trans phi\n   -- w = ((\u03bb I + (\u03a6^T)\u03a6)^(-1))(\u03a6^T)t <= PRML P.143 (3.28)\u5f0f\u3088\u308a\n   w = (inv (lambdaI + phiT `multiply` phi))\n       `multiply` phiT `multiply` t\n\n-- \u7dda\u5f62\u56de\u5e30\n-- \u6b63\u5247\u5316\u4fc2\u6570, \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8981\u7d20\u6570, \u57fa\u5e95\u95a2\u6570, \u5165\u529b\u5024xs,\u76ee\u6a19\u5024ts\u304b\u3089\u3001\n-- \u7dda\u5f62\u56de\u5e30\u306b\u3088\u3063\u3066\u63a8\u5b9a\u3055\u308c\u305f\u95a2\u6570f\u3092\u8fd4\u3059\u3002\nlinearRegression lambda m basis xs ts = f\n where\n   -- \u57fa\u5e95\u95a2\u6570\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u308b\n   basiss = (\\_ -> 1.0) : (map (basis . fromIntegral) [1..m])\n   -- \u8a08\u753b\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\n   dm = designMatrix basiss xs\n   -- \u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30ebw\u3092\u8a08\u7b97\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u306b\u3059\u308b\n   ws = concat . toLists $ parameters lambda dm ts\n   -- \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u63a8\u6e2c\u3055\u308c\u308b\u95a2\u6570\u3092\u8fd4\u3059\n   f  = \\x -> sum $ zipWith (*) ws $ map (\\g -> g x) basics\n\n\n\n\n\n\u751f\u614b\u7cfb \u6355\u98df\u30fb\u88ab\u6355\u98df\uff08\u30e9\u30c8\u30ab\u30fb\u30dc\u30eb\u30c6\u30e9\uff09\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\n\n\n\u6368\u3066\u3089\u308c\u305f\u30d6\u30ed\u30b0 \uff082011/11/04\uff09\u300cLotka-Volterra \u30e2\u30c7\u30eb\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u300d\n\n\n\u89e3\u6790\u7684\u306b\u89e3\u304f\u3053\u3068\u304c\u56f0\u96e3\u3067\u3042\u308b\u305f\u3081\uff0c\u8a08\u7b97\u6a5f\u306b\u3088\u308b\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\u306e\u304c\u666e\u901a\u3067\u3059\u3002\n\u3053\u308c\u3092 Haskell \u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\ndata Fractional t => Model t = Lv t t t t \ndata Fractional t => N t = N { prey :: t, predator :: t }\n\ndelta dt (Lv a b c d) (N { prey = x, predator = y }) =\n   N { prey = x * (a - b * y) * dt,\n       predator = -y * (c - d * x) * dt }\nmove n dn = N { prey = prey n + prey dn,\n                predator = predator n + predator dn }\nmean d1 d2 = N { prey = (prey d1 + prey d2) / 2,\n                 predator = (predator d1 + predator d2) / 2 }\nnext dt param (t, n) = let d1 = delta dt param n\n                           d2 = delta dt param $ move n d1\n                      in (t + dt, move n $ mean d1 d2)\nlv dt param n0 = iterate (next dt param) (0, n0)\n\n\n\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092 (a,b,c,d)=(2.0,0.1,1.0,0.07)\uff0c\n\u521d\u671f\u5024\u3092 (x,y)=(120,10) \u3068\u3057\u3066\uff0c \nt \u3092 0.02 \u523b\u307f\u3067 1,200 \u56de\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u305f[A]\u3002\n\ntake 1200 $ lv 0.02 (Lv 2.0 0.1 1.0 0.07) (N { prey = 120, predator = 10 })\n\n\n\u4e0a\u8a18\u306e\u95a2\u6570\u305d\u306e\u307e\u307e\u3067\u306f\u51fa\u529b\u3067\u304d\u307e\u305b\u3093\u304c\uff0c\u9069\u5f53\u306b CSV \u3067\u51fa\u529b\u3057\u3066 R \u3067\u30b0\u30e9\u30d5\u5316\u3057\u305f\u306e\u304c\u4ee5\u4e0b\u306e\u56f3\u3067\u3059\u3002\n\uff08 \u30b0\u30e9\u30d5\u7701\u7565 \uff09\n\n\n\n\u3010 \u6a5f\u68b0\u5b66\u7fd2 \u3011\n\n\nnebutalab\u3055\u3093 Qiita\u8a18\u4e8b (2013/12/16) \u300cHaskell\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaHLearn\u300d\n\n\n\n\u3010 \u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8 \u3011\n\n\nHatena Blog bitterharvest's diary (2014/08/27) \u300cHaskell\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u300d\n\n\n\u305d\u3053\u3067\u3001Haskell\u306e\u4e16\u754c\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3069\u306e\u7a0b\u5ea6\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u306e\u304b\u3092\u8abf\u3079\u308b\u3053\u3068\u306b\u3057\u305f\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3044\u304f\u3064\u304b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3060\u304c\u3001\u79c1\u304c\u898b\u3064\u3051\u305f\u306e\u306f\u3001A functional approach to Neural Network \u3067\u3042\u308b\u3002\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7406\u8ad6\u7684\u80cc\u666f\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u306e\u6587\u732e\u306b\u3082\u7c21\u5358\u306b\u66f8\u3044\u3066\u3042\u308b\u3057\u3001\u8aac\u660e\u66f8\u3082\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3001\u3053\u3053\u3067\u306f\u3001\u89e6\u308c\u306a\u3044\u3002\n\n\n\nA Functional Approach to Neural Networks\n\n\n\n\n\nmhwombat/backprop-example\n\n\nAn example of back-propagation implemented in Haskell.\nAn example of back-propagation implemented in Haskell. >\nThis code is intended to accompany the article A Functional Approach to Neural Networks by Amy de Buitl\u00e9ir, Michael Russell, Mark Daly at \nhttp://themonadreader.files.wordpress.com/2013/03/issue214.pdf.\n\n\u307e\u305f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u884c\u5217\u8a08\u7b97\u3092\u884c\u3046\u306e\u3067\u3001hmatrix \u3082 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\nhmatrix \u306f C\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3059\u308b\u306e\u3067\u3001gsl-lapack-windows.zip\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3001\u89e3\u51cd\u3057\u3001\u3042\u308b\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u7d0d\u3081\u308b\u3002\u3053\u3053\u3067\u306f\u3001C:\\gsl\u306b\u3059\u308b\u3002\u3053\u306e\u6642\u3001\u74b0\u5883\u5909\u6570path\u306b\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u66f8\u304d\u8fbc\u307f\u3001\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002\nhmatrix \u306e \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u6b21\u306e\u3088\u3046\u306b\u884c\u3046\u3002\n\ncabal install hmatrix --extra-lib-dir=C:\\gsl --extra-include-dir=C:\\gsl\n\n\n\u5f8c\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u308a\u3001\n\ncabal install\nbackdrop-example\n\n\n\u3068\u3059\u308b\u3002\n\n\n\u697d\u306f\u865a\u306b\u51fa\u305a\uff1f (2013/05/03) \u300cNN\u3068Haskell\u306e\u52c9\u5f37\uff1aAND \u3068 OR\u300d\n\u697d\u306f\u865a\u306b\u51fa\u305a\uff1f (2013/05/20) \u300cNN\u3068Haskell\u306e\u52c9\u5f37\uff1aEX-OR\uff08\u4fee\u6b63\u7248\uff09\u300d\n\n\n\n\u3010 \u30c7\u30a3\u30fc\u30d7\u30fb\u30e9\u30fc\u30cb\u30f3\u30b0 \u3011\n\n\nhackage, deeplearning-hs: Deep Learning in Haskell\n\n\uff08 \u4e0a\u8a18 \u30d1\u30c3\u30b1\u30fc\u30b8 \u306e GitHub\u30ea\u30dd\u30b8\u30c8\u30ea \uff09\n\najtulloch/dnngraph\n\n\nA DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph\n\n\nGet a Brain\n\n\nLet\u2019s build a neural network from scratch. Our artificial brain should run on just the core Haskell system.\nWe follow the free online book Neural Networks and Deep Learning by Michael Nielsen. We will train a network to recognize handwritten digits, specifically those in the MNIST database of handwritten digits.\n\n\n\n\nTeglor Deep Learning Libraries by Language\n\n\nHaskell\nDNNGraph is a deep neural network model generation DSL in Haskell.\n\n\nDNNGraph\n\n\n(GitHub) ajtulloch/dnngraph\n\n\nA DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph\n\uff08 \u4e2d\u7565 \uff09\nBuilding\nMake sure that you have Python 2 and protoc from Protocol Buffers installed. Then run\n\n\nTerminal\n$ cabal install hprotoc\n$ ./lens_proto.sh # generate code from protocol buffers\n$ cabal install\n\n\n\nDSL Examples\nAlexNet\n\n\nGHCi\nimport           Control.Lens\nimport           Control.Monad\n\nimport           NN.DSL\nimport           NN.Examples.ImageNet\nimport           NN.Graph\n\nalexTrain = train & cropSize' 227 & batchSize' 256 & mirror' True\nalexTest = test & cropSize' 227 & batchSize' 50 & mirror' False\n\nalexLrn = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\nalexConv = conv & param' alexMult & weightFillerC' (gaussian 0.01) & biasFillerC' zero\nalexIP n = ip n & param' alexMult & weightFillerIP' (gaussian 0.005) & biasFillerIP' (constant 0.1)\nalexPool = maxPool & sizeP' 3\n\nalexMult = [def & lrMult' 1 & decayMult' 1, -- weights\n            def & lrMult' 2 & decayMult' 0] -- biases\n\n-- |Model\nconv1 = alexConv & numOutputC' 96 & kernelSizeC' 11 & strideC' 4\nconv2 = alexConv & numOutputC' 256 & padC' 2 & kernelSizeC' 5 & groupC' 2\nconv3 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3\nconv4 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\nconv5 = alexConv & numOutputC' 256 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\n\nalexNet = do\n -- Set up the model\n  (input', representation) <-\n      sequential [\n           -- Convolutional Layers\n           conv1, relu, alexLrn, alexPool & strideP' 3,\n           conv2, relu, alexLrn, alexPool & strideP' 2,\n           conv3, relu,\n           conv4, relu,\n           conv5, relu, alexPool & strideP' 2,\n           -- FC Layers\n           alexIP 4096, relu, dropout 0.5,\n           alexIP 4096, relu, dropout 0.5,\n           alexIP 1000 & weightFillerIP' (gaussian 0.01) & biasFillerIP' zero]\n\n forM_ [alexTrain, alexTest] $ attach (To input')\n forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\n\n\n\nGoogLeNet\n\n\nGHCi\nmodule NN.Examples.GoogLeNet where\n\nimport           Gen.Caffe.FillerParameter       as FP\nimport           Gen.Caffe.InnerProductParameter as IP\nimport           Gen.Caffe.LayerParameter        as LP\n\nimport           Control.Lens\nimport           Control.Monad\nimport           Data.Sequence                   (singleton)\nimport           Data.Word\n\nimport           NN\nimport           NN.Examples.ImageNet\n\n\ngoogleTrain = train & mirror' True & batchSize' 32 & cropSize' 224\ngoogleTest = test & mirror' False & batchSize' 50 & cropSize' 224\n\ngoogleMult = [def & lrMult' 1 & decayMult' 1, -- weights\n              def & lrMult' 2 & decayMult' 0] -- biases\ngoogleConv = conv & param' googleMult & biasFillerC' (constant 0.2)\ngoogleLRN = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\ngooglePool = maxPool & sizeP' 3 & strideP' 2\ngoogleIP n = ip n & param' googleMult\n\nconv1 = googleConv & numOutputC' 64 & padC' 3 & kernelSizeC' 7 & strideC' 2 & weightFillerC' (xavier 0.1)\nconv2 = googleConv & numOutputC' 192 & padC' 1 & kernelSizeC' 3 & weightFillerC' (xavier 0.03)\n\ntopPool = avgPool & sizeP' 7 & strideP' 1\ntopFc = googleIP 1000 & biasFillerIP' (constant 0) & weightFillerIP' (xavier 0.0)\n        -- Weird, but in Caffe replication\n        & _inner_product_param._Just.IP._weight_filler._Just._std .~ Nothing\n\ndata Inception = Inception {_1x1, _3x3reduce, _3x3, _5x5reduce, _5x5, _poolProj :: Word32}\n\ninception :: Node -> Inception -> NetBuilder Node\ninception input Inception{..} = do\n  columns' <- mapM sequential columns\n  concat'' <- layer' concat'\n  forM_ columns' $ \\(bottom, top) -> do\n                                  input >-> bottom\n                                  top >-> concat''\n return concat''\n   where\n      columns = [\n       [googleConv & numOutputC' _1x1  & kernelSizeC' 1 & weightFillerC' (xavier 0.03), relu],\n       [googleConv & numOutputC' _3x3reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.09), relu, googleConv & numOutputC' _3x3 & kernelSizeC' 3 & weightFillerC' (xavier 0.03) & padC' 1, relu],\n       [googleConv & numOutputC' _5x5reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.2), relu, googleConv & numOutputC' _5x5 & kernelSizeC' 5 & weightFillerC' (xavier 0.03) & padC' 2, relu],\n       [maxPool& sizeP' 3 & strideP' 3 & padP' 1, googleConv & numOutputC' _poolProj & kernelSizeC' 1 & weightFillerC' (xavier 0.1), relu]]\n\nintermediateClassifier :: Node -> NetBuilder ()\nintermediateClassifier source = do\n  (input, representation) <- sequential [pool1, conv1', relu, fc1, relu, dropout 0.7, fc2]\n  source >-> input\n\n forM_ [accuracy 1, accuracy 5, softmax & _loss_weight <>~ singleton 0.3] $ attach (From representation)\n    where\n      pool1 = avgPool & sizeP' 5 & strideP' 3\n      conv1' = googleConv & numOutputC' 128 & kernelSizeC' 1 & weightFillerC' (xavier 0.08)\n      fc1 = googleIP 1024 & weightFillerIP' (xavier 0.02) & biasFillerIP' (constant 0.2)\n      fc2 = googleIP 1000 & weightFillerIP' (xavier 0.0009765625) & biasFillerIP' (constant 0)\n\n-- What to do at each row in the inner column?\ndata Row = I Inception | Classifier | MaxPool\n\ninsertRow :: Node -> Row -> NetBuilder Node\ninsertRow input (I inceptor) = inception input inceptor\ninsertRow input Classifier = do\n intermediateClassifier input\n return input\ninsertRow input MaxPool = do\n node <- layer' googlePool\n input >-> node\n return node\n\ngoogLeNet :: NetBuilder ()\ngoogLeNet = do\n  (input, initial) <- sequential [conv1, relu, >>googlePool, googleLRN, conv2, relu, googleLRN, >>googlePool]\n\n top <- foldM insertRow initial [\n             I $ Inception 64 96 128 16 32 32,\n             I $ Inception 128 128 192 32 96 64,\n             MaxPool,\n             I $ Inception 192 96 208 16 48 64,\n             Classifier,\n             I $ Inception 150 112 224 24 64 64,\n             I $ Inception 128 128 256 24 64 64,\n             I $ Inception 112 144 288 32 64 64,\n             Classifier,\n             I $ Inception 256 160 320 32 128 128,\n             MaxPool,\n             I $ Inception 256 160 320 32 128 128,\n             I $ Inception 384 192 384 48 128 128]\n\n (_, representation) <- with top >- sequential [topPool, dropout 0.4, topFc]\n\n forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\n forM_ [googleTrain, googleTest] $ attach (To input)\n\nmain :: IO ()\nmain = cli googLeNet\n\n\n\n\n\n\uff08GitHub\uff09tensorflow/haskell\n\n\nHaskell bindings for TensorFlow https://tensorflow.github.io/haskell/\u2026\n\n\n\u4ee5\u4e0b\u306e\u30d6\u30ed\u30b0\u306f\u3001CNN\u3092\u30b9\u30af\u30e9\u30c3\u30c1\u3067\u30bc\u30ed\u304b\u3089\u72ec\u81ea\u5b9f\u88c5\u3092\u8a66\u307f\u3066\u3044\u308b\u3002\n\nCNN\u306e\u89e3\u8aac\u3092\u542b\u3081\u3066\u3001\u5206\u304b\u308a\u3084\u3059\u3044\u3002\n\u6700\u5f8c\u306b\u5b8c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u30e1\u30e2\u30ea\u3092\u5927\u91cf\u306b\u6d88\u8cbb\u3059\u308b\u3053\u3068\u304c\u5224\u660e\u3057\u305f\u3053\u3068\u304b\u3089\u3001\u30b3\u30fc\u30c9\u6700\u9069\u5316\u306e\u5fc5\u8981\u304c\u8ff0\u3079\u3089\u308c\u3066\u3001\u300c\u5b8c\u6210\uff1f\u300d\u3068\u7591\u554f\u7b26\u3092\u4ed8\u3057\u305f\u3068\u306e\u3053\u3068\u3002\n\nHatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/08/30\uff09\u300cDeepLearning(1): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0a)\u300d\nHatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/09/10\uff09\u300cDeepLearning(2): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0b)\u300d\nHatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/11/08)\u300cDeepLearning(3): \u305d\u3057\u3066\u9006\u4f1d\u64ad\uff08\u3067\u3082\u5168\u7d50\u5408\u5c64\u307e\u3067\uff09\u300d\nHatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082017/01/09)\u300cDeepLearning(4): CNN\u306e\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d\n\n\n3-2. \u30e1\u30e2\u30ea\u30ea\u30fc\u30af\uff01\uff1f\n\u3055\u3066\u3001\u8868\u984c\u306b\u300c\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d\u3068\u30af\u30a8\u30b9\u30c1\u30e7\u30f3\u3092\u4ed8\u3051\u305f\u7406\u7531\u304c\u3053\u308c\u3060\u3002 epoch\u6570\u3092200\u306b\u4e0b\u3052\u305f\u7406\u7531\u3082\u3002\n\u3084\u3063\u3068\u3067\u304d\u305f\u3068\u601d\u3063\u3066epoch\u6570500\u3067\u5b9f\u884c\u3057\u3066\u307f\u308b\u3068\u306a\u304b\u306a\u304b\u7d42\u308f\u3089\u306a\u3044\u3002\n\u958b\u59cb\u6642\u306f1 epoch\u306e\u51e6\u7406\u306b\u7d042\u79d2\u304b\u304b\u3063\u305f\u306e\u3067\u5358\u7d14\u8a08\u7b97\u3067\u306f1000\u79d2\u306b\u306a\u308b\u304c 30\u5206\u7d4c\u3063\u3066\u3082350\u7a0b\u5ea6\u3002\n\u304a\u304b\u3057\u3044\u3068\u601d\u3063\u3066top\u30b3\u30de\u30f3\u30c9\u3067\u30d7\u30ed\u30bb\u30b9\u3092\u898b\u3066 \u307f\u305f\u3089\u30e1\u30e2\u30ea\u309210GB\u3082\u98df\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u3050\u505c\u6b62\u3057\u305f\uff01\n\u6c17\u3092\u53d6\u308a\u76f4\u3057\u3066\u3082\u3046\u4e00\u5ea6\u5b9f\u884c\u3057\u305f\u3068\u304d\u306etop\u30b3\u30de\u30f3\u30c9\u306e\u51fa\u529b\u304c\u3053\u308c\u3002\ntrain \u30d7\u30ed\u30bb\u30b9\u304c\u5927\u91cf\u306e\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3057\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308b\u3002\u3053\u308c\u306f\u7e70\u308a\u8fd4\u3057\u56de\u6570\u304c \u3060\u3044\u305f\u3044180\u3050\u3089\u3044\u306e\u3068\u304d\u306e\u72b6\u614b\u3002\n\n\uff08 Qiita\u8a18\u4e8b\u7248 \u3082 \u3042\u308b \u6a21\u69d8 \uff09\n\neijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(1): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0a)\u300d\neijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(2): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0b)\u300d \neijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(3): \u305d\u3057\u3066\u9006\u4f1d\u64ad\uff08\u3067\u3082\u5168\u7d50\u5408\u5c64\u307e\u3067\uff09\u300d\neijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(4): CNN\u306e\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d\n\n\n\n\u3010 \u81ea\u7136\u8a00\u8a9e\u89e3\u6790 \u3011\n\n\nHackage mecab package\n\n\nmmitou\u306e\u65e5\u8a18 (2012-11-02) \u300c\u5f62\u614b\u7d20\u89e3\u6790\u30a8\u30f3\u30b8\u30f3MeCab\u3092Fedora17\u3067Haskell\u304b\u3089\u4f7f\u3046\u969b\u306e\u30e1\u30e2\u300d\n\n\nyum install *mecab*\necho \"\u30e1\u30ab\u30d6\u306f\u7f8e\u5473\u3057\u304f\u3066\u5065\u5eb7\u306b\u3082\u3044\u3044\u3088\u3002\" | mecab \n\nHaskell\u306eMeCab\u30d0\u30a4\u30f3\u30c7\u30a3\u30f3\u30b0 Text.MeCab \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\ncabal\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\ncabal update\ncabal install mecab\n\n\n\uff08 \u4e2d\u7565 \uff09\n\n\nHaskell\nimport Text.MeCab\nimport Control.Monad\n\ntoString :: Text.MeCab.Node String -> String\ntoString node = foldr (\\x acc-> x ++ \" \" ++ acc) [] attrs\n where\n   id   = (show $ nodeId node)\n   stat = case nodeStat node of\n     BOS -> \"BOS\"\n     EOS -> \"EOS\"\n     _   -> (nodeSurface node) ++ \" \" ++ (show $ nodeRlength node)\n   feature = nodeFeature node\n   rcAttr  = show $ nodeRcAttr node\n   lcAttr  = show $ nodeLcAttr node\n   posid   = show $ nodePosid  node\n   ctype   = show $ nodeCharType node\n   stat'   = show $ nodeStat   node\n   isbest  = show $ nodeIsBest node\n   alpha   = show $ nodeAlpha  node\n   beta    = show $ nodeBeta   node\n   prob    = show $ nodeProb   node\n   cost    = show $ nodeCost   node\n   attrs = [id, stat, feature]\n   --attrs = [id, stat, feature, rcAttr, lcAttr, posid, ctype,\n   --         stat', isbest, alpha, beta, prob, cost]\n\ntest = do\n let input = \"\u592a\u90ce\u306f\u6b21\u90ce\u304c\u6301\u3063\u3066\u3044\u308b\u672c\u3092\u82b1\u5b50\u306b\u6e21\u3057\u305f\u3002\"\n mecab  <- new2 \"\"\n -- Gets tagged result in string format.\n result <- parse mecab input\n putStrLn $ \"INPUT : \" ++ input\n putStrLn $ \"RESULT: \" ++ result\n\n -- Gets N best results in string format.\n nbresult <- parseNBest mecab 3 input\n putStrLn $ \"NBEST: \" ++ nbresult\n\n -- Gets N best results in sequence.\n parseNBestInit mecab input\n forM_ [0..1] $ \\n -> do\n   nextResult <- next mecab\n   case nextResult of\n     Just nr -> putStrLn $ (show n) ++ \":\" ++ nr\n     _       -> return ()\n\n -- Gets Node object.\n nodes <- parseToNode mecab input\n mapM_ (putStrLn . toString) nodes\n\n return ()\n\nmain = test\n\n\n\n\nHackage cabocha package\n\n\n\u3059\u3050\u306b\u5fd8\u308c\u308b\u8133\u307f\u305d\u306e\u305f\u3081\u306e\u30e1\u30e2 (2008/07/10) \u300cHaskell \u3067\u7c21\u5358\u306a\u30c6\u30ad\u30b9\u30c8\u51e6\u7406 - \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\u300d\neuphonictechnologies\u2019s diary (2014/08/03) \u300cHaskell\u3067\u3082\u306e\u3059\u3054\u304f\u7c21\u5358\u306a\u30b9\u30da\u30eb\u4fee\u6b63\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u3063\u3066\u307f\u308b - \u305d\u306e1.5:\u6587\u5b57\u5217\u51e6\u7406\u3092Text\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3088\u3046\u306b\u3059\u308b\u300d\nHaskell\u306e\u99c4\u76ee\u306a\u4f7f\u3044\u65b9 \u6587\u5b57\u5217\u51e6\u8a00\u8a9e\u9593\u7406\u6bd4\u8f03\u8868\n\n\n__Hackage chatter package\n\n\nchatter is a collection of simple Natural Language Processing algorithms.\nChatter supports:\n\nPart of speech tagging with Averaged Perceptrons. Based on the Python implementation by Matthew Honnibal: (http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/) See NLP.POS for the details of part-of-speech tagging with chatter.\nPhrasal Chunking (also with an Averaged Perceptron) to identify arbitrary chunks based on training data.\nDocument similarity; A cosine-based similarity measure, and TF-IDF calculations, are available in the NLP.Similarity.VectorSim module.\nInformation Extraction patterns via (http://www.haskell.org/haskellwiki/Parsec/) Parsec\n\n\n\nTF-IDF \u306b\u3088\u308b \u6587\u66f8\u5206\u985e\n\n\nbgamari/tf-idf\n\n\nA simple implementation of Term Frequency-Inverse Document Frequency (tf-idf) in Haskell\n\n\nandrevdm/TextClassify\n\n\nDemo Haskell TfIdf text classifier - code for blog post http://www.andrevdm.com/posts/2016-09\u2026\nA haskell tf-idf implementation. This implemntation is for the \"Haskell text classification using Tf-Idf\" blog post.\n\n\n\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u4ee5\u4e0b\u306eBlog\u8a18\u4e8b\n\nAndre's Blog Haskell text classification using Tf-Idf\n\n\n\nmarkandrus/TF-IDF\n\n\nFrequentist Document Classification\n\n\nv97ug/TFIDF Haskell\u3067\u3001TFIDF\u306e\u5b9f\u88c5\u3092\u3059\u308b\nShachi H Kumar , Document Search and Ranking using Haskell , CMPS 203 Project Report\n\n\n\n\nLSA ( Latent Semantic Analysis )\n\n\nViktor Stanchev's Blog, Latent Semantic Analysis in Haskell, Posted on April 4, 2012 by vikstrous\n\n\n\uff08 \u4e0a\u8a18 Blog \u6240\u53ce\u306e\u30b3\u30fc\u30c9 \uff09\nimport Data.List\nimport Data.Char\nimport Numeric.LinearAlgebra\n\ntitles = [\"The Neatest Little Guide to Stock Market Investing\",\n          \"Investing For Dummies, 4th Edition\",\n          \"The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns\",\n          \"The Little Book of Value Investing\",\n          \"Value Investing: From Graham to Buffett and Beyond\",\n          \"Rich Dad's Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!\",\n          \"Investing in Real Estate, 5th Edition\",\n          \"Stock Investing For Dummies\",\n          \"Rich Dad's Advisors: The ABC's of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss\" ]\n\nstopwords = [\"and\",\"edition\",\"for\",\"in\",\"little\",\"of\",\"the\",\"to\"]\n\ntext = unlines titles\n\ndocs :: [[String]]\ndocs = map (filter (not . (`elem` stopwords))) $      -- stopwords filter\n        map words $\n          lines $\n            filter (\\x -> isAlpha x || isSpace x) $   -- discard everything except alpha and space characters\n              map toLower text                        -- lowercase the input\n\ntf :: [([Char], Int)]\ntf = filter (\\(_,f) -> f>1) $ map (\\l@(x:xs) -> (x,length l)) . group . sort $ concat docs -- remove words that appear only once\n\ndoc_freq :: Int -> [Char] -> Int\ndoc_freq d t = length (filter (==t) (docs !! d))\n\nmat :: Matrix Double\nmat = buildMatrix (length tf) (length docs) ( \\(term, doc) ->\n          let occurances = fromIntegral $ doc_freq doc $ fst $ tf !! term -- occurance count\n              docLength = genericLength $ docs !! doc                     -- words per doc\n              numDocs = genericLength docs                                -- number of docs\n              commonness = fromIntegral $ snd $ tf !! term                -- number of docs this word occurs in\n          in (occurances / docLength * log (numDocs / commonness))\n      )\n\ncompress k m = u_k  sigma_k  v_k where\n    (u,sigma,v) = fullSVD m                         -- get SVD\n    sigma_k = (takeColumns k . takeRows k) sigma    -- keep k values of \u03a3\n    u_k = takeColumns k u                           -- keep k columns of U\n    v_k = takeRows k $ trans v                      -- keep k rows of v\n\nreduce_dim k m = v_k where\n        (u,sigma,v) = fullSVD m                         -- mapping of documents to concept space\n        v_k = takeRows k $ trans v                      -- keep \n\n\n\n\u3010 DB\u64cd\u4f5c \u3011\n\n\nHatena Blog Creatable a => a -> IO b_ (2014/06/21) \u300cHaskell-rerational-record\u3067DB\u64cd\u4f5c\u3059\u308b\u306e\u304c\u697d\u3057\u3059\u304e\u308b\u4ef6\u301c\u305d\u306e\uff11\u301c\u300d_\nAmazon, O'REILLY, Developing Web Applications With Haskell and Yesod\nliquid_amber\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/04) \u300cDatabase.Esqueleto\u5165\u9580\u300d\n\n\n\nFPGA\n\n\nC\u03bbaSH From Haskell to Hardware\n\n\nWhy use C\u03bbaSH\nC\u03bbaSH (pronounced \u2018clash\u2019) is a functional hardware description language that borrows both its syntax and semantics from the functional programming language Haskell. It provides a familiar structural design approach to both combinational and synchronous sequential circuits. The C\u03bbaSH compiler transforms these high-level descriptions to low-level synthesizable VHDL, Verilog, or SystemVerilog.\n\n\n\n\u3010 Web \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30fb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af \u3011\n\n\nTECHSCOREBLOG (2013/06/11) \u300cYesod\u5165\u9580 (1) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304b\u3089\u8d77\u52d5\u307e\u3067\u300d\nWeb \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30fb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af Yesod\nmaoe\u306e\u30d6\u30ed\u30b0 (2013/09/10) \u300cHaskell\u3067WebApp\u306e\u958b\u767a\u306b\u5fc5\u8981\u306aN\u500b\u306e\u3053\u3068\u300d\ngogotanaka\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/02) \u300c\u3010Mac OS X 10.10\u3011Haskell + Yesod + Heroku \u3067 web\u30a2\u30d7\u30ea 1/3\u300d\nHatena Blog \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u82b8\u8853\u8ad6 (2014/03/19) \u300cHaskell\u306eYesod\u3067Web\u30a2\u30d7\u30ea\u958b\u767a\u5165\u9580 (1)\u300d\n\n### __\u4ee5\u4e0b\u3001\u9818\u57df\u5225\u306b\u3001\u5165\u9580\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8 \u3078\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5f35\u3063\u3066\u307f\u307e\u3057\u305f\u3002__\n\n\n##__\u53c2\u8003\u66f8\u7c4d__\n\n* [Bryan O'Sullivan\u307b\u304b (\u8457)\u30fb\u5c71\u4e0b \u4f38\u592b (\u7ffb\u8a33) \u300eReal World Haskell\u2015\u5b9f\u6226\u3067\u5b66\u3076\u95a2\u6570\u578b\u8a00\u8a9e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u300f \u5927\u578b\u672c, 2009/10/26](https://www.amazon.co.jp/Real-World-Haskell\u2015\u5b9f\u6226\u3067\u5b66\u3076\u95a2\u6570\u578b\u8a00\u8a9e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0-Bryan-OSullivan/dp/4873114233)\n\n<img width=\"274\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.50.08.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/de39ad73-a94d-9246-bc2c-30fcf4487eed.png\">\n\n* [Nishant Shukla (\u8457) _Haskell Data Analysis Cookbook_,  (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af \u2013 2014/6/25](https://www.amazon.co.jp/Haskell-Analysis-Cookbook-Nishant-Shukla/dp/1783286334)\n\n<img width=\"276\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.46.11.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/86b4d8ee-67de-0a58-67db-fab772be8529.png\">\n\n* [James Church (\u8457) _Learning Haskell Data_,  (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af \u2013 2015/5/28](https://www.amazon.co.jp/Learning-Haskell-Data-James-Church/dp/178439470X/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=ETC97Z8752G0KFR59JJ4)\n\n<img width=\"272\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.48.11.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/2e13e020-b714-bbc2-367c-b3ccb8f9302b.png\">\n\n* [Pavel Ryzhov (\u8457),  _Haskell Financial Data Modeling and Predictive Analytics_, (\u82f1\u8a9e) \u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af, 2013/10/25](https://www.amazon.co.jp/Haskell-Financial-Modeling-Predictive-Analytics/dp/1782169431/ref=pd_cp_14_1?_encoding=UTF8&psc=1&refRID=174D4PJ61E8MGRKDR9JT)\n\n<img width=\"276\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.52.21.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/889073ba-99d0-f424-43c9-8617cd5ef128.png\">\n\n__\uff08\u3000Amazon \u306e\u300c\u4e2d\u8eab\u3092\u898b\u3066\u307f\u308b\u300d\u3088\u308a \u76ee\u6b21 \u3092 \u4e00\u90e8 \u8ee2\u8f09 \uff09__\n\n<img width=\"611\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.53.22.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/6448b53b-34de-1ee8-40fe-a4b2d843aa30.png\">\n\n<img width=\"612\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.53.30.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/2aefc629-7dc1-18b5-5884-1b77cce9940a.png\">\n\n##__\uff08 \u5168\u822c\u53c2\u8003 \uff09__\n\n* [Quora _Is Haskell a good fit for machine learning problems? Why? Or why not?_ ](https://www.quora.com/Is-Haskell-a-good-fit-for-machine-learning-problems)\n\n* [stackoverflow _Machine learning in OCaml or Haskell?_ ](http://stackoverflow.com/questions/2268885/machine-learning-in-ocaml-or-haskell)\n\n___\n\n\n### __\u3010 \u30d5\u30a1\u30a4\u30eb\u5165\u51fa\u529b \u3011__\n\n* [\u30a6\u30a9\u30fc\u30af\u30b9\u30eb\u30fc Haskell > \u5165\u51fa\u529b \u300c\u5165\u51fa\u529b\u300d](http://walk.wgag.net/haskell/io.html)\n* [Hatena::Diary (2010/12/11) \u300csirocco \u306e\u66f8\u3044\u3066\u3082\u3059\u3050\u306b\u5fd8\u308c\u308b\u30e1\u30e2\u300d](http://d.hatena.ne.jp/sirocco/20101211/1292038713)* [Functional Programming \u304a\u6c17\u697d Haskell \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u5165\u9580 \u300c\u30d5\u30a1\u30a4\u30eb\u5165\u51fa\u529b\u300d](http://www.geocities.jp/m_hiroi/func/haskell19.html)\n* [7shi\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/11) \u300cHaskell IO\u30e2\u30ca\u30c9 \u8d85\u5165\u9580\u300d](http://qiita.com/7shi/items/d3d3492ddd90d47160f2)\n\n\n___\n\n### __\u3010 \u30b0\u30e9\u30d5\u63cf\u753b \u3011__\n\n* [tapioka\u3055\u3093 Qiita\u8a18\u4e8b (2014/09/03) \u300cHaskell\u3067\u30b0\u30e9\u30d5\u63cf\u753b\u300d](http://qiita.com/tapioka/items/7362e8f95a8a7596c361)\n* [\u6368\u3066\u3089\u308c\u305f\u30d6\u30ed\u30b0 (2011/12/09) \u300cHaskell Chart \u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u30b0\u30e9\u30d5\u63cf\u753b\u300d](http://blog.recyclebin.jp/archives/2309)\n* [Hatena Blog \u7de0\u601d\u4e4b\u8a18 (2011/11/12) \u300cHaskell\u3067\u30b0\u30e9\u30d5\u3092\u66f8\u304f\u306b\u306f\u300d](http://na4zagin3.hatenablog.com/entry/20111112/1321112964)\n\n___\n\n\n### __\u3010 \u7dda\u5f62\u56de\u5e30\u30fb\u7d71\u8a08\u89e3\u6790\u30fb\u591a\u5909\u91cf\u89e3\u6790 \u3011__\n\n* [Hatena Blog \u308a\u3093\u3054\u304c\u3067\u3066\u3044\u308b (2014/04/11) \u300cHaskell\u3067\u30c7\u30fc\u30bf\u51e6\u7406\u304c\u3057\u305f\u3044\uff01\u300d](http://bicycle1885.hatenablog.com/entry/2014/04/11/201734)\n\n* [Hatena::Diary mmitou\u306e\u65e5\u8a18 (2012/11/07) \u300cPRML\uff13\u7ae0\u306e\u7dda\u5f62\u56de\u5e30\u3092\u5b9f\u88c5\u3057\u305f\u300d](http://d.hatena.ne.jp/mmitou/20121107/1352289359)\n\n__\uff08 \u4e0a\u8a18 \u300cmmitou\u306e\u65e5\u8a18\u300d \u3088\u308a \u30b3\u30fc\u30c9\u3092\u4e00\u90e8\u629c\u7c8b\u3057\u3066\u8ee2\u8f09 \uff09__\n\n>```{haskell:\u30b3\u30fc\u30c9\uff08\u4e00\u90e8\u629c\u7c8b)}\n>-- \u5ea7\u6a19\u3092\u6587\u5b57\u5217\u306b\u3059\u308b\npointsToString :: (Show a) => [a] -> [a] -> String\npointsToString xs ys =\n  concat $\n  zipWith (\\x y -> show x ++ \" \" ++ show y ++ \"\\n\") xs ys\n>\n>-- \u5ea7\u6a19\u3092\u6587\u5b57\u5217\u306b\u3059\u308b\n>to3dPosString xs ts = concat $\n                      zipWith (\\(x,y) z ->    (show x) ++ \" \"\n                                           ++ (show y) ++ \" \"\n                                           ++ (show z) ++ \"\\n\")\n                      xs ts\n>\n>-- \u30ac\u30a6\u30b9\u57fa\u5e95\u95a2\u6570\ngaussianBasis s mu x = exp $ (- (x - mu) ** 2.0) / s\n>\n>-- \u591a\u9805\u5f0f\u57fa\u5e95\u95a2\u6570\n>polynomialBasis n x = x ** n\n>\n>-- \u4e8c\u6b21\u5143\u7528\u591a\u9805\u5f0f\u57fa\u5e95\u95a2\u6570\n>-- \u591a\u6b21\u5143\u306e\u5834\u5408\u306b\u3053\u306e\u3088\u3046\u306a\u57fa\u5e95\u95a2\u6570\u3067\u3088\u3044\u306e\u304b\u3069\u3046\u304b\u306f\u78ba\u8a8d\u3057\u3066\u3044\u306a\u3044\u3002\n>polynomialBasis' n (x,y) = x ** n + y ** n\n>\n>-- \u57fa\u5e95\u95a2\u6570\u306e\u914d\u5217\u3068\u5165\u529b\u5909\u6570x\u304b\u3089\u8a08\u753b\u884c\u5217\u03a6\u3092\u8a08\u7b97\u3059\u308b\n>designMatrix basiss xs = (n >< m) [f x | x <- xs, f <- basiss]\n>  where\n>    n = length xs\n>    m = length basiss\n>\n>-- \u6b63\u5247\u5316\u4fc2\u6570\u3001\u8a08\u753b\u884c\u5217\u3068\u76ee\u6a19\u5909\u6570\u306e\u914d\u5217\u304b\u3089\n>-- \u57fa\u5e95\u95a2\u6570\u306e\u7dda\u5f62\u7d50\u5408\u3067\u7528\u3044\u308b\u30d1\u30e9\u30e1\u30fc\u30bf w \u3092\u8a08\u7b97\u3059\u308b\n>parameters lambda phi ts = w\n>  where\n>    -- \u914d\u5217\u3092\u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\n>    t       = (length ts >< 1) ts \n>    m       = cols phi\n>    -- (lambda * \u5358\u4f4d\u884c\u5217) \u3092\u8a08\u7b97\u3059\u308b\n>    lambdaI = fromLists $ map (map (*lambda)) (toLists $ ident m)\n>    -- \u8a08\u753b\u884c\u5217\u306e\u8ee2\u7f6e\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\n>    phiT = trans phi\n>    -- w = ((\u03bb I + (\u03a6^T)\u03a6)^(-1))(\u03a6^T)t <= PRML P.143 (3.28)\u5f0f\u3088\u308a\n>    w = (inv (lambdaI + phiT `multiply` phi))\n>        `multiply` phiT `multiply` t\n>\n>-- \u7dda\u5f62\u56de\u5e30\n>-- \u6b63\u5247\u5316\u4fc2\u6570, \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8981\u7d20\u6570, \u57fa\u5e95\u95a2\u6570, \u5165\u529b\u5024xs,\u76ee\u6a19\u5024ts\u304b\u3089\u3001\n>-- \u7dda\u5f62\u56de\u5e30\u306b\u3088\u3063\u3066\u63a8\u5b9a\u3055\u308c\u305f\u95a2\u6570f\u3092\u8fd4\u3059\u3002\n>linearRegression lambda m basis xs ts = f\n>  where\n>    -- \u57fa\u5e95\u95a2\u6570\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u308b\n>    basiss = (\\_ -> 1.0) : (map (basis . fromIntegral) [1..m])\n>    -- \u8a08\u753b\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\n>    dm = designMatrix basiss xs\n>    -- \u30d1\u30e9\u30e1\u30fc\u30bf\u30d9\u30af\u30c8\u30ebw\u3092\u8a08\u7b97\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u306b\u3059\u308b\n>    ws = concat . toLists $ parameters lambda dm ts\n>    -- \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u63a8\u6e2c\u3055\u308c\u308b\u95a2\u6570\u3092\u8fd4\u3059\n>    f  = \\x -> sum $ zipWith (*) ws $ map (\\g -> g x) basics\n>```\n\n___\n\n###__\u751f\u614b\u7cfb \u6355\u98df\u30fb\u88ab\u6355\u98df\uff08\u30e9\u30c8\u30ab\u30fb\u30dc\u30eb\u30c6\u30e9\uff09\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3__\n\n* [\u6368\u3066\u3089\u308c\u305f\u30d6\u30ed\u30b0 \uff082011/11/04\uff09\u300cLotka-Volterra \u30e2\u30c7\u30eb\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u300d](https://blog.recyclebin.jp/archives/2202)\n\n>\u89e3\u6790\u7684\u306b\u89e3\u304f\u3053\u3068\u304c\u56f0\u96e3\u3067\u3042\u308b\u305f\u3081\uff0c\u8a08\u7b97\u6a5f\u306b\u3088\u308b\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\u306e\u304c\u666e\u901a\u3067\u3059\u3002\n>\n>\u3053\u308c\u3092 Haskell \u3067\u5b9f\u88c5\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n>\n>>```{haskell:}\n>>data Fractional t => Model t = Lv t t t t \n>>data Fractional t => N t = N { prey :: t, predator :: t }\n>>\n>>delta dt (Lv a b c d) (N { prey = x, predator = y }) =\n   N { prey = x * (a - b * y) * dt,\n       predator = -y * (c - d * x) * dt }\n>>move n dn = N { prey = prey n + prey dn,\n                predator = predator n + predator dn }\n>>mean d1 d2 = N { prey = (prey d1 + prey d2) / 2,\n                 predator = (predator d1 + predator d2) / 2 }\n>>next dt param (t, n) = let d1 = delta dt param n\n                           d2 = delta dt param $ move n d1\n>>                       in (t + dt, move n $ mean d1 d2)\nlv dt param n0 = iterate (next dt param) (0, n0)\n>>```\n>\n>\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092 (a,b,c,d)=(2.0,0.1,1.0,0.07)\uff0c\n>\u521d\u671f\u5024\u3092 (x,y)=(120,10) \u3068\u3057\u3066\uff0c \n>t \u3092 0.02 \u523b\u307f\u3067 1,200 \u56de\u8a08\u7b97\u3057\u3066\u307f\u307e\u3057\u305f[A]\u3002\n>\n>>```{haskell:}\n>>take 1200 $ lv 0.02 (Lv 2.0 0.1 1.0 0.07) (N { prey = 120, predator = 10 })\n>>```\n>\n>\u4e0a\u8a18\u306e\u95a2\u6570\u305d\u306e\u307e\u307e\u3067\u306f\u51fa\u529b\u3067\u304d\u307e\u305b\u3093\u304c\uff0c\u9069\u5f53\u306b CSV \u3067\u51fa\u529b\u3057\u3066 R \u3067\u30b0\u30e9\u30d5\u5316\u3057\u305f\u306e\u304c\u4ee5\u4e0b\u306e\u56f3\u3067\u3059\u3002\n>\n> \uff08 \u30b0\u30e9\u30d5\u7701\u7565 \uff09\n>\n\n\n___\n\n### __\u3010 \u6a5f\u68b0\u5b66\u7fd2 \u3011__\n\n* [nebutalab\u3055\u3093 Qiita\u8a18\u4e8b (2013/12/16) \u300cHaskell\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaHLearn\u300d](http://qiita.com/nebutalab/items/680203ff3d84cde7c8db)\n\n___\n\n### __\u3010 \u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8 \u3011__\n\n* [Hatena Blog bitterharvest's diary (2014/08/27) \u300cHaskell\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u300d](http://bitterharvest.hatenablog.com/entry/2014/08/27/143825)\n\n> \u305d\u3053\u3067\u3001Haskell\u306e\u4e16\u754c\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3069\u306e\u7a0b\u5ea6\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u306e\u304b\u3092\u8abf\u3079\u308b\u3053\u3068\u306b\u3057\u305f\u3002\n>\n> \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u3044\u304f\u3064\u304b\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3060\u304c\u3001\u79c1\u304c\u898b\u3064\u3051\u305f\u306e\u306f\u3001_A functional approach to Neural Network_ \u3067\u3042\u308b\u3002\n>\n> \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7406\u8ad6\u7684\u80cc\u666f\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u306e\u6587\u732e\u306b\u3082\u7c21\u5358\u306b\u66f8\u3044\u3066\u3042\u308b\u3057\u3001\u8aac\u660e\u66f8\u3082\u305f\u304f\u3055\u3093\u3042\u308b\u306e\u3067\u3001\u3053\u3053\u3067\u306f\u3001\u89e6\u308c\u306a\u3044\u3002\n\n___\n\n* [A Functional Approach to Neural Networks](https://themonadreader.files.wordpress.com/2013/03/issue214.pdf)\n\n<img width=\"597\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 1.42.07.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/5527ddef-002e-57c8-b7be-27d98f8ffb12.png\">\n\n<img width=\"596\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 1.42.13.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/92348bef-f6f9-0d92-1d6a-6a97dd9c2c76.png\">\n\n<img width=\"598\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 1.42.26.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/add7f672-b9a9-1e63-c355-7a04e204f8b0.png\">\n\n* [mhwombat/backprop-example](https://github.com/mhwombat/backprop-example)\n\n> An example of back-propagation implemented in Haskell.\n>\n> An example of back-propagation implemented in Haskell. >\n> This code is intended to accompany the article *A Functional Approach to Neural Networks* by Amy de Buitl\u00e9ir, Michael Russell, Mark Daly at \n> http://themonadreader.files.wordpress.com/2013/03/issue214.pdf.\n___\n\n> \u307e\u305f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u884c\u5217\u8a08\u7b97\u3092\u884c\u3046\u306e\u3067\u3001**hmatrix** \u3082 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\n>\n> **hmatrix** \u306f C\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3059\u308b\u306e\u3067\u3001gsl-lapack-windows.zip\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3001\u89e3\u51cd\u3057\u3001\u3042\u308b\u30d5\u30a9\u30eb\u30c0\u30fc\u306b\u7d0d\u3081\u308b\u3002\u3053\u3053\u3067\u306f\u3001C:\\gsl\u306b\u3059\u308b\u3002\u3053\u306e\u6642\u3001\u74b0\u5883\u5909\u6570path\u306b\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u66f8\u304d\u8fbc\u307f\u3001\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002\n>\n> **hmatrix** \u306e \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u6b21\u306e\u3088\u3046\u306b\u884c\u3046\u3002\n>\n>>```\n>>cabal install hmatrix --extra-lib-dir=C:\\gsl --extra-include-dir=C:\\gsl\n>>```\n>\n>\u5f8c\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u308a\u3001\n>\n>>```\n>>cabal install\n>>backdrop-example\n>>```\n>\n>\u3068\u3059\u308b\u3002\n\n* [\u697d\u306f\u865a\u306b\u51fa\u305a\uff1f (2013/05/03) \u300cNN\u3068Haskell\u306e\u52c9\u5f37\uff1aAND \u3068 OR\u300d](http://prime997.blogspot.jp/2013/05/nnand-or.html)\n* [\u697d\u306f\u865a\u306b\u51fa\u305a\uff1f (2013/05/20) \u300cNN\u3068Haskell\u306e\u52c9\u5f37\uff1aEX-OR\uff08\u4fee\u6b63\u7248\uff09\u300d](http://prime997.blogspot.jp/2013/05/nnhaskellex-or.html)\n\n\n___\n\n## __\u3010 \u30c7\u30a3\u30fc\u30d7\u30fb\u30e9\u30fc\u30cb\u30f3\u30b0 \u3011__\n\n* [hackage, _deeplearning-hs: Deep Learning in Haskell_](https://hackage.haskell.org/package/deeplearning-hs)\n\n__\uff08 \u4e0a\u8a18 \u30d1\u30c3\u30b1\u30fc\u30b8 \u306e GitHub\u30ea\u30dd\u30b8\u30c8\u30ea \uff09__\n\n* [ajtulloch/dnngraph](https://github.com/ajtulloch/dnngraph)\n\n> A DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph\n\n* [Get a Brain](https://crypto.stanford.edu/~blynn/haskell/brain.html)\n\n> Let\u2019s build a neural network from scratch. Our artificial brain should run on just the core Haskell system.\n>\n> We follow the free online book *Neural Networks and Deep Learning* by Michael Nielsen. We will train a network to recognize handwritten digits, specifically those in the MNIST database of handwritten digits.\n\n<img width=\"997\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 1.12.36.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/65e2cd7f-3754-e05a-dba4-8d6564976264.png\">\n\n<img width=\"684\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 1.12.54.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/f4ba5bbe-1d37-41f1-0acf-5f62c669a275.png\">\n\n* [Teglor _Deep Learning Libraries by Language_](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)\n\n> __Haskell__\n>\n> *DNNGraph* is a deep neural network model generation DSL in Haskell.\n\n####__*DNNGraph*__\n\n* [(GitHub) ajtulloch/dnngraph](https://github.com/ajtulloch/dnngraph)\n\n> A DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph\n>\n> \uff08 \u4e2d\u7565 \uff09\n>\n> __Building__\n>\n> Make sure that you have Python 2 and protoc from Protocol Buffers installed. Then run\n>\n>>```{bash:Terminal}\n>>$ cabal install hprotoc\n>>$ ./lens_proto.sh # generate code from protocol buffers\n>>$ cabal install\n>>```\n>\n>__DSL Examples__\n>\n>__AlexNet__\n>\n>>```{haskell:GHCi}\n>>import           Control.Lens\n>>import           Control.Monad\n>>\n>>import           NN.DSL\n>>import           NN.Examples.ImageNet\n>>import           NN.Graph\n>>\n>>alexTrain = train & cropSize' 227 & batchSize' 256 & mirror' True\n>>alexTest = test & cropSize' 227 & batchSize' 50 & mirror' False\n>>\n>>alexLrn = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\n>>alexConv = conv & param' alexMult & weightFillerC' (gaussian 0.01) & biasFillerC' zero\n>>alexIP n = ip n & param' alexMult & weightFillerIP' (gaussian 0.005) & biasFillerIP' (constant 0.1)\n>>alexPool = maxPool & sizeP' 3\n>>\n>>alexMult = [def & lrMult' 1 & decayMult' 1, -- weights\n            def & lrMult' 2 & decayMult' 0] -- biases\n>>\n>>-- |Model\n>>conv1 = alexConv & numOutputC' 96 & kernelSizeC' 11 & strideC' 4\n>>conv2 = alexConv & numOutputC' 256 & padC' 2 & kernelSizeC' 5 & groupC' 2\n>>conv3 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3\n>>conv4 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\n>>conv5 = alexConv & numOutputC' 256 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\n>>\n>>alexNet = do\n>>  -- Set up the model\n  (input', representation) <-\n      sequential [\n           -- Convolutional Layers\n           conv1, relu, alexLrn, alexPool & strideP' 3,\n           conv2, relu, alexLrn, alexPool & strideP' 2,\n           conv3, relu,\n           conv4, relu,\n           conv5, relu, alexPool & strideP' 2,\n           -- FC Layers\n           alexIP 4096, relu, dropout 0.5,\n           alexIP 4096, relu, dropout 0.5,\n           alexIP 1000 & weightFillerIP' (gaussian 0.01) & biasFillerIP' zero]\n>>\n>>  forM_ [alexTrain, alexTest] $ attach (To input')\n>>  forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\n>>```\n>\n>\n>__GoogLeNet__\n>\n>>```{haskell:GHCi}\n>>module NN.Examples.GoogLeNet where\n>>\n>>import           Gen.Caffe.FillerParameter       as FP\n>>import           Gen.Caffe.InnerProductParameter as IP\n>>import           Gen.Caffe.LayerParameter        as LP\n>>\n>>import           Control.Lens\n>>import           Control.Monad\n>>import           Data.Sequence                   (singleton)\n>>import           Data.Word\n>>\n>>import           NN\n>>import           NN.Examples.ImageNet\n>>\n>>\n>>googleTrain = train & mirror' True & batchSize' 32 & cropSize' 224\n>>googleTest = test & mirror' False & batchSize' 50 & cropSize' 224\n>>\n>>googleMult = [def & lrMult' 1 & decayMult' 1, -- weights\n              def & lrMult' 2 & decayMult' 0] -- biases\n>>googleConv = conv & param' googleMult & biasFillerC' (constant 0.2)\n>>googleLRN = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\n>>googlePool = maxPool & sizeP' 3 & strideP' 2\n>>googleIP n = ip n & param' googleMult\n>>\n>>conv1 = googleConv & numOutputC' 64 & padC' 3 & kernelSizeC' 7 & strideC' 2 & weightFillerC' (xavier 0.1)\n>>conv2 = googleConv & numOutputC' 192 & padC' 1 & kernelSizeC' 3 & weightFillerC' (xavier 0.03)\n>>\n>>topPool = avgPool & sizeP' 7 & strideP' 1\n>>topFc = googleIP 1000 & biasFillerIP' (constant 0) & weightFillerIP' (xavier 0.0)\n        -- Weird, but in Caffe replication\n        & _inner_product_param._Just.IP._weight_filler._Just._std .~ Nothing\n>>\n>>data Inception = Inception {_1x1, _3x3reduce, _3x3, _5x5reduce, _5x5, _poolProj :: Word32}\n>>\n>>inception :: Node -> Inception -> NetBuilder Node\n>>inception input Inception{..} = do\n  columns' <- mapM sequential columns\n  concat'' <- layer' concat'\n  forM_ columns' $ \\(bottom, top) -> do\n                                  input >-> bottom\n                                  top >-> concat''\n>>  return concat''\n>>    where\n      columns = [\n       [googleConv & numOutputC' _1x1  & kernelSizeC' 1 & weightFillerC' (xavier 0.03), relu],\n       [googleConv & numOutputC' _3x3reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.09), relu, googleConv & numOutputC' _3x3 & kernelSizeC' 3 & weightFillerC' (xavier 0.03) & padC' 1, relu],\n       [googleConv & numOutputC' _5x5reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.2), relu, googleConv & numOutputC' _5x5 & kernelSizeC' 5 & weightFillerC' (xavier 0.03) & padC' 2, relu],\n       [maxPool& sizeP' 3 & strideP' 3 & padP' 1, googleConv & numOutputC' _poolProj & kernelSizeC' 1 & weightFillerC' (xavier 0.1), relu]]\n>>\n>>intermediateClassifier :: Node -> NetBuilder ()\nintermediateClassifier source = do\n  (input, representation) <- sequential [pool1, conv1', relu, fc1, relu, dropout 0.7, fc2]\n  source >-> input\n>>\n>>  forM_ [accuracy 1, accuracy 5, softmax & _loss_weight <>~ singleton 0.3] $ attach (From representation)\n    where\n      pool1 = avgPool & sizeP' 5 & strideP' 3\n      conv1' = googleConv & numOutputC' 128 & kernelSizeC' 1 & weightFillerC' (xavier 0.08)\n      fc1 = googleIP 1024 & weightFillerIP' (xavier 0.02) & biasFillerIP' (constant 0.2)\n      fc2 = googleIP 1000 & weightFillerIP' (xavier 0.0009765625) & biasFillerIP' (constant 0)\n>>\n>>-- What to do at each row in the inner column?\ndata Row = I Inception | Classifier | MaxPool\n>>\n>>insertRow :: Node -> Row -> NetBuilder Node\n>>insertRow input (I inceptor) = inception input inceptor\n>>insertRow input Classifier = do\n>>  intermediateClassifier input\n>>  return input\n>>insertRow input MaxPool = do\n>>  node <- layer' googlePool\n>>  input >-> node\n>>  return node\n>>\n>>googLeNet :: NetBuilder ()\n>>googLeNet = do\n  (input, initial) <- sequential [conv1, relu, >>googlePool, googleLRN, conv2, relu, googleLRN, >>googlePool]\n>>\n>>  top <- foldM insertRow initial [\n             I $ Inception 64 96 128 16 32 32,\n             I $ Inception 128 128 192 32 96 64,\n             MaxPool,\n             I $ Inception 192 96 208 16 48 64,\n             Classifier,\n             I $ Inception 150 112 224 24 64 64,\n             I $ Inception 128 128 256 24 64 64,\n             I $ Inception 112 144 288 32 64 64,\n             Classifier,\n             I $ Inception 256 160 320 32 128 128,\n             MaxPool,\n             I $ Inception 256 160 320 32 128 128,\n             I $ Inception 384 192 384 48 128 128]\n>>\n>>  (_, representation) <- with top >- sequential [topPool, dropout 0.4, topFc]\n>>\n>>  forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\n>>  forM_ [googleTrain, googleTest] $ attach (To input)\n>>\n>>main :: IO ()\n>>main = cli googLeNet\n>>```\n\n* [\uff08GitHub\uff09tensorflow/haskell](https://github.com/tensorflow/haskell)\n\n> Haskell bindings for TensorFlow https://tensorflow.github.io/haskell/\u2026\n\n####__\u4ee5\u4e0b\u306e\u30d6\u30ed\u30b0\u306f\u3001CNN\u3092\u30b9\u30af\u30e9\u30c3\u30c1\u3067\u30bc\u30ed\u304b\u3089\u72ec\u81ea\u5b9f\u88c5\u3092\u8a66\u307f\u3066\u3044\u308b\u3002__\n\n__CNN\u306e\u89e3\u8aac\u3092\u542b\u3081\u3066\u3001\u5206\u304b\u308a\u3084\u3059\u3044\u3002__\n\n__\u6700\u5f8c\u306b\u5b8c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u30e1\u30e2\u30ea\u3092\u5927\u91cf\u306b\u6d88\u8cbb\u3059\u308b\u3053\u3068\u304c\u5224\u660e\u3057\u305f\u3053\u3068\u304b\u3089\u3001\u30b3\u30fc\u30c9\u6700\u9069\u5316\u306e\u5fc5\u8981\u304c\u8ff0\u3079\u3089\u308c\u3066\u3001\u300c\u5b8c\u6210\uff1f\u300d\u3068\u7591\u554f\u7b26\u3092\u4ed8\u3057\u305f\u3068\u306e\u3053\u3068\u3002__\n\n* [Hatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/08/30\uff09\u300cDeepLearning(1): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0a)\u300d](http://eijian.hatenablog.com/entry/2016/08/30/012556)\n\n* [Hatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/09/10\uff09\u300cDeepLearning(2): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0b)\u300d](http://eijian.hatenablog.com/entry/2016/09/10/203218)\n\n* [Hatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082016/11/08)\u300cDeepLearning(3): \u305d\u3057\u3066\u9006\u4f1d\u64ad\uff08\u3067\u3082\u5168\u7d50\u5408\u5c64\u307e\u3067\uff09\u300d](http://eijian.hatenablog.com/entry/2016/11/08/023227)\n\n* [Hatena Blog Haskell\u3067\u3044\u3063\u3066\u307f\u3088\u3046 \uff082017/01/09)\u300cDeepLearning(4): CNN\u306e\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d](http://eijian.hatenablog.com/entry/2017/01/09/021103)\n\n> __3-2. \u30e1\u30e2\u30ea\u30ea\u30fc\u30af\uff01\uff1f__\n>\n> \u3055\u3066\u3001\u8868\u984c\u306b\u300c\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d\u3068\u30af\u30a8\u30b9\u30c1\u30e7\u30f3\u3092\u4ed8\u3051\u305f\u7406\u7531\u304c\u3053\u308c\u3060\u3002 epoch\u6570\u3092200\u306b\u4e0b\u3052\u305f\u7406\u7531\u3082\u3002\n>\n> \u3084\u3063\u3068\u3067\u304d\u305f\u3068\u601d\u3063\u3066epoch\u6570500\u3067\u5b9f\u884c\u3057\u3066\u307f\u308b\u3068\u306a\u304b\u306a\u304b\u7d42\u308f\u3089\u306a\u3044\u3002\n>\n>\u958b\u59cb\u6642\u306f1 epoch\u306e\u51e6\u7406\u306b\u7d042\u79d2\u304b\u304b\u3063\u305f\u306e\u3067\u5358\u7d14\u8a08\u7b97\u3067\u306f1000\u79d2\u306b\u306a\u308b\u304c 30\u5206\u7d4c\u3063\u3066\u3082350\u7a0b\u5ea6\u3002\n>\n> \u304a\u304b\u3057\u3044\u3068\u601d\u3063\u3066top\u30b3\u30de\u30f3\u30c9\u3067\u30d7\u30ed\u30bb\u30b9\u3092\u898b\u3066 \u307f\u305f\u3089\u30e1\u30e2\u30ea\u309210GB\u3082\u98df\u3063\u3066\u3044\u305f\u306e\u3067\u3059\u3050\u505c\u6b62\u3057\u305f\uff01\n>\n> \u6c17\u3092\u53d6\u308a\u76f4\u3057\u3066\u3082\u3046\u4e00\u5ea6\u5b9f\u884c\u3057\u305f\u3068\u304d\u306etop\u30b3\u30de\u30f3\u30c9\u306e\u51fa\u529b\u304c\u3053\u308c\u3002\n>\n> train \u30d7\u30ed\u30bb\u30b9\u304c\u5927\u91cf\u306e\u30e1\u30e2\u30ea\u3092\u6d88\u8cbb\u3057\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308b\u3002\u3053\u308c\u306f\u7e70\u308a\u8fd4\u3057\u56de\u6570\u304c \u3060\u3044\u305f\u3044180\u3050\u3089\u3044\u306e\u3068\u304d\u306e\u72b6\u614b\u3002\n\n__\uff08 Qiita\u8a18\u4e8b\u7248 \u3082 \u3042\u308b \u6a21\u69d8 \uff09__\n\n* [eijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(1): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0a)\u300d](http://qiita.com/eijian/items/06b1ba1276d1bfd77b93)\n* [eijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(2): \u307e\u305a\u306f\u9806\u4f1d\u64ad(\u4e0b)\u300d ](http://qiita.com/eijian/items/24d7e6aee332d59509ec)\n* [eijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(3): \u305d\u3057\u3066\u9006\u4f1d\u64ad\uff08\u3067\u3082\u5168\u7d50\u5408\u5c64\u307e\u3067\uff09\u300d](http://qiita.com/eijian/items/bfbdd3f7862ec84ce8c9)\n* [eijian\u3055\u3093 Qiita\u8a18\u4e8b \u300cDeepLearning(4): CNN\u306e\u9006\u4f1d\u64ad\u5b8c\u6210\uff1f\u300d](http://qiita.com/eijian/items/c947fb6b5e7a49858fb4)\n\n___\n\n\n## __\u3010 \u81ea\u7136\u8a00\u8a9e\u89e3\u6790 \u3011__\n\n####__[Hackage mecab package](https://hackage.haskell.org/package/mecab)__\n\n* [mmitou\u306e\u65e5\u8a18 (2012-11-02) \u300c\u5f62\u614b\u7d20\u89e3\u6790\u30a8\u30f3\u30b8\u30f3MeCab\u3092Fedora17\u3067Haskell\u304b\u3089\u4f7f\u3046\u969b\u306e\u30e1\u30e2\u300d](http://d.hatena.ne.jp/mmitou/20121102/1351853275)\n\n> ```\n> yum install *mecab*\n> echo \"\u30e1\u30ab\u30d6\u306f\u7f8e\u5473\u3057\u304f\u3066\u5065\u5eb7\u306b\u3082\u3044\u3044\u3088\u3002\" | mecab \n>```\n> \n> __Haskell\u306eMeCab\u30d0\u30a4\u30f3\u30c7\u30a3\u30f3\u30b0 Text.MeCab \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b__\n> cabal\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\n>\n> ```\n> cabal update\n> cabal install mecab\n> ```\n\n__\uff08 \u4e2d\u7565 \uff09__\n\n> ```{Haskell:Haskell}\n> import Text.MeCab\n> import Control.Monad\n>\n> toString :: Text.MeCab.Node String -> String\n> toString node = foldr (\\x acc-> x ++ \" \" ++ acc) [] attrs\n>  where\n>    id   = (show $ nodeId node)\n>    stat = case nodeStat node of\n>      BOS -> \"BOS\"\n>      EOS -> \"EOS\"\n>      _   -> (nodeSurface node) ++ \" \" ++ (show $ nodeRlength node)\n>    feature = nodeFeature node\n>    rcAttr  = show $ nodeRcAttr node\n>    lcAttr  = show $ nodeLcAttr node\n>    posid   = show $ nodePosid  node\n>    ctype   = show $ nodeCharType node\n>    stat'   = show $ nodeStat   node\n>    isbest  = show $ nodeIsBest node\n>    alpha   = show $ nodeAlpha  node\n>    beta    = show $ nodeBeta   node\n>    prob    = show $ nodeProb   node\n>    cost    = show $ nodeCost   node\n>    attrs = [id, stat, feature]\n>    --attrs = [id, stat, feature, rcAttr, lcAttr, posid, ctype,\n>    --         stat', isbest, alpha, beta, prob, cost]\n>\n> test = do\n>  let input = \"\u592a\u90ce\u306f\u6b21\u90ce\u304c\u6301\u3063\u3066\u3044\u308b\u672c\u3092\u82b1\u5b50\u306b\u6e21\u3057\u305f\u3002\"\n>  mecab  <- new2 \"\"\n>  -- Gets tagged result in string format.\n>  result <- parse mecab input\n>  putStrLn $ \"INPUT : \" ++ input\n>  putStrLn $ \"RESULT: \" ++ result\n>\n>  -- Gets N best results in string format.\n>  nbresult <- parseNBest mecab 3 input\n>  putStrLn $ \"NBEST: \" ++ nbresult\n>\n>  -- Gets N best results in sequence.\n>  parseNBestInit mecab input\n>  forM_ [0..1] $ \\n -> do\n>    nextResult <- next mecab\n>    case nextResult of\n>      Just nr -> putStrLn $ (show n) ++ \":\" ++ nr\n>      _       -> return ()\n>\n>  -- Gets Node object.\n>  nodes <- parseToNode mecab input\n>  mapM_ (putStrLn . toString) nodes\n>\n>  return ()\n>\n> main = test\n>```\n\n\n####__[Hackage cabocha package](https://hackage.haskell.org/package/cabocha)__\n\n* [\u3059\u3050\u306b\u5fd8\u308c\u308b\u8133\u307f\u305d\u306e\u305f\u3081\u306e\u30e1\u30e2 (2008/07/10) \u300cHaskell \u3067\u7c21\u5358\u306a\u30c6\u30ad\u30b9\u30c8\u51e6\u7406 - \u30c7\u30fc\u30bf\u306e\u62bd\u51fa\u300d](http://jutememo.blogspot.jp/2008/07/haskell_10.html)\n\n* [euphonictechnologies\u2019s diary (2014/08/03) \u300cHaskell\u3067\u3082\u306e\u3059\u3054\u304f\u7c21\u5358\u306a\u30b9\u30da\u30eb\u4fee\u6b63\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u3063\u3066\u307f\u308b - \u305d\u306e1.5:\u6587\u5b57\u5217\u51e6\u7406\u3092Text\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3088\u3046\u306b\u3059\u308b\u300d](http://blog.euphonictech.com/entry/2014/08/03/231519)\n\n* [Haskell\u306e\u99c4\u76ee\u306a\u4f7f\u3044\u65b9 \u6587\u5b57\u5217\u51e6\u8a00\u8a9e\u9593\u7406\u6bd4\u8f03\u8868](http://www3.atwiki.jp/nanakoso/pages/23.html)\n\n####__[Hackage chatter package](https://hackage.haskell.org/package/chatter)\n\n> chatter is a collection of simple Natural Language Processing algorithms.\n>\n>Chatter supports:\n>\n> * Part of speech tagging with Averaged Perceptrons. Based on the Python implementation by Matthew Honnibal: (http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/) See NLP.POS for the details of part-of-speech tagging with chatter.\n>\n> * Phrasal Chunking (also with an Averaged Perceptron) to identify arbitrary chunks based on training data.\n>\n> * Document similarity; A cosine-based similarity measure, and TF-IDF calculations, are available in the NLP.Similarity.VectorSim module.\n>\n> * Information Extraction patterns via (http://www.haskell.org/haskellwiki/Parsec/) Parsec\n\n###__TF-IDF \u306b\u3088\u308b \u6587\u66f8\u5206\u985e__\n\n\n* [bgamari/tf-idf](https://github.com/bgamari/tf-idf)\n\n> A simple implementation of Term Frequency-Inverse Document Frequency (tf-idf) in Haskell\n\n* [andrevdm/TextClassify](https://github.com/andrevdm/TextClassify)\n\n> Demo Haskell TfIdf text classifier - code for blog post http://www.andrevdm.com/posts/2016-09\u2026\n>\n> A haskell tf-idf implementation. This implemntation is for the \"Haskell text classification using Tf-Idf\" blog post.\n\n___\n\n__\u8a00\u53ca\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u4ee5\u4e0b\u306eBlog\u8a18\u4e8b__\n\n* [Andre's Blog Haskell text classification using Tf-Idf](http://www.andrevdm.com/posts/2016-09-21-haskell-tfidf.html)\n\n___\n\n* [markandrus/TF-IDF](https://github.com/markandrus/TF-IDF)\n\n> Frequentist Document Classification\n\n* [v97ug/TFIDF Haskell\u3067\u3001TFIDF\u306e\u5b9f\u88c5\u3092\u3059\u308b](https://github.com/v97ug/TFIDF)\n\n* [Shachi H Kumar , _Document Search and Ranking using Haskell_ , CMPS 203 Project Report](https://shachihkumar.files.wordpress.com/2014/02/document_searchrank_haskell.pdf)\n\n<img width=\"749\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.40.33.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/00fcaa90-e02c-fb88-14ae-0244b22379cf.png\">\n\n<img width=\"842\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.41.07.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/42b8b774-a083-8cef-cf51-c91065c376ac.png\">\n\n###__LSA ( *Latent Semantic Analysis* )__\n\n* [Viktor Stanchev's Blog, _Latent Semantic Analysis in Haskell_, Posted on April 4, 2012 by vikstrous](https://blog.viktorstanchev.com/2012/04/04/latent-semantic-analysis-in-haskell/)\n\n<img width=\"1238\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-01-16 0.42.15.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7d38a9b1-f4fc-4274-0f6b-9d9221ff1ec2.png\">\n\n__\uff08 \u4e0a\u8a18 Blog \u6240\u53ce\u306e\u30b3\u30fc\u30c9 \uff09__\n\n```{haskell:}\nimport Data.List\nimport Data.Char\nimport Numeric.LinearAlgebra\n \ntitles = [\"The Neatest Little Guide to Stock Market Investing\",\n          \"Investing For Dummies, 4th Edition\",\n          \"The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns\",\n          \"The Little Book of Value Investing\",\n          \"Value Investing: From Graham to Buffett and Beyond\",\n          \"Rich Dad's Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!\",\n          \"Investing in Real Estate, 5th Edition\",\n          \"Stock Investing For Dummies\",\n          \"Rich Dad's Advisors: The ABC's of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss\" ]\n \nstopwords = [\"and\",\"edition\",\"for\",\"in\",\"little\",\"of\",\"the\",\"to\"]\n \ntext = unlines titles\n \ndocs :: [[String]]\ndocs = map (filter (not . (`elem` stopwords))) $      -- stopwords filter\n        map words $\n          lines $\n            filter (\\x -> isAlpha x || isSpace x) $   -- discard everything except alpha and space characters\n              map toLower text                        -- lowercase the input\n \ntf :: [([Char], Int)]\ntf = filter (\\(_,f) -> f>1) $ map (\\l@(x:xs) -> (x,length l)) . group . sort $ concat docs -- remove words that appear only once\n \ndoc_freq :: Int -> [Char] -> Int\ndoc_freq d t = length (filter (==t) (docs !! d))\n \nmat :: Matrix Double\nmat = buildMatrix (length tf) (length docs) ( \\(term, doc) ->\n          let occurances = fromIntegral $ doc_freq doc $ fst $ tf !! term -- occurance count\n              docLength = genericLength $ docs !! doc                     -- words per doc\n              numDocs = genericLength docs                                -- number of docs\n              commonness = fromIntegral $ snd $ tf !! term                -- number of docs this word occurs in\n          in (occurances / docLength * log (numDocs / commonness))\n      )\n \ncompress k m = u_k  sigma_k  v_k where\n    (u,sigma,v) = fullSVD m                         -- get SVD\n    sigma_k = (takeColumns k . takeRows k) sigma    -- keep k values of \u03a3\n    u_k = takeColumns k u                           -- keep k columns of U\n    v_k = takeRows k $ trans v                      -- keep k rows of v\n \nreduce_dim k m = v_k where\n        (u,sigma,v) = fullSVD m                         -- mapping of documents to concept space\n        v_k = takeRows k $ trans v                      -- keep \n```\n\n\n\n___\n\n## __\u3010 DB\u64cd\u4f5c \u3011__\n\n* [Hatena Blog _Creatable a => a -> IO b__ (2014/06/21) \u300cHaskell-rerational-record\u3067DB\u64cd\u4f5c\u3059\u308b\u306e\u304c\u697d\u3057\u3059\u304e\u308b\u4ef6\u301c\u305d\u306e\uff11\u301c\u300d_](http://tune.hateblo.jp/entry/2014/06/21/161745)\n* [Amazon, O'REILLY, _Developing Web Applications With Haskell and Yesod_](http://www.amazon.co.jp/exec/obidos/ASIN/1449316972/m12i0b-22/)\n* [liquid_amber\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/04) \u300cDatabase.Esqueleto\u5165\u9580\u300d](http://qiita.com/liquid_amber/items/d711c38ff6497d5a0c1d)\n\n___\n\n##__FPGA__\n\n* [C\u03bbaSH _From Haskell to Hardware_](http://www.clash-lang.org)\n\n> __Why use C\u03bbaSH__\n>\n>C\u03bbaSH (pronounced \u2018clash\u2019) is a functional hardware description language that borrows both its syntax and semantics from the functional programming language Haskell. It provides a familiar structural design approach to both combinational and synchronous sequential circuits. The C\u03bbaSH compiler transforms these high-level descriptions to low-level synthesizable VHDL, Verilog, or SystemVerilog.\n\n\n\n___\n\n## __\u3010 Web \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30fb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af \u3011__\n\n* [TECHSCOREBLOG (2013/06/11) \u300cYesod\u5165\u9580 (1) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304b\u3089\u8d77\u52d5\u307e\u3067\u300d](http://www.techscore.com/blog/2013/06/11/yesod%E5%85%A5%E9%96%80-1-%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E3%81%8B%E3%82%89%E8%B5%B7%E5%8B%95%E3%81%BE%E3%81%A7/)\n\n* [Web \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30fb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af Yesod](http://www.mew.org/~kazu/material/2012-yesod.pdf)\n\n\n* [maoe\u306e\u30d6\u30ed\u30b0 (2013/09/10) \u300cHaskell\u3067WebApp\u306e\u958b\u767a\u306b\u5fc5\u8981\u306aN\u500b\u306e\u3053\u3068\u300d](http://maoe.hatenadiary.jp/entry/2013/09/10/182914)\n* [gogotanaka\u3055\u3093 Qiita\u8a18\u4e8b (2014/12/02) \u300c\u3010Mac OS X 10.10\u3011Haskell + Yesod + Heroku \u3067 web\u30a2\u30d7\u30ea 1/3\u300d](http://qiita.com/gogotanaka/items/a9a2483c37935acf568a)\n* [Hatena Blog \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u82b8\u8853\u8ad6 (2014/03/19) \u300cHaskell\u306eYesod\u3067Web\u30a2\u30d7\u30ea\u958b\u767a\u5165\u9580 (1)\u300d](http://demmys.hatenablog.com/entry/2014/03/19/Haskell%E3%81%A7Web%E3%82%A2%E3%83%97%E3%83%AA_(1))\n", "tags": ["Haskell", "MachineLearning", "\u6a5f\u68b0\u5b66\u7fd2", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "DeepLearning"]}