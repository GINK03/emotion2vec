{"context": "Distributed computing (Apache Hadoop, Spark, ...) Advent Calendar 2016 \u306e24\u65e5\u76ee\u3067\u3059\u3002\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001Hadoop \u30af\u30e9\u30b9\u30bf\u3092\u624b\u8efd\u306b\u624b\u5143\u306b\u69cb\u7bc9\u3057\u305f\u3044\u3068\u304d\u306b\u4fbf\u5229\u306a\u3001Apache Bigtop \u306e\u6a5f\u80fd\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nApache Bigtop \u3068\u306f\nApache Bigtop \u306f\u3001Hadoop \u3068\u305d\u306e\u5468\u8fba\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u30d3\u30eb\u30c9\u3057\u3001 deb \u3084 rpm \u3068\u3044\u3063\u305f\u5f62\u5f0f\u3067\u30d1\u30c3\u30b1\u30fc\u30b8\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u7a2e Linux \u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3078\u306e\u5c0e\u5165\u3092\u5bb9\u6613\u306b\u3059\u308b\u305f\u3081\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u3059\u3002\u305d\u308c\u4ee5\u5916\u306b\u3082\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6a5f\u80fd\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u4f5c\u6210\u3057\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5404\u30ce\u30fc\u30c9\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u3001\u9069\u5207\u306b\u8a2d\u5b9a\u3059\u308b\u305f\u3081\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\n\u30c7\u30d7\u30ed\u30a4\u7d50\u679c\u306e\u78ba\u8a8d\u3084\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30d0\u30fc\u30b8\u30e7\u30f3\u9593\u306e\u76f8\u4e92\u904b\u7528\u6027\u306e\u62c5\u4fdd\u306b\u4f7f\u308f\u308c\u308b\u3001\u7d71\u5408\u30c6\u30b9\u30c8\u30fb\u30b9\u30e2\u30fc\u30af\u30c6\u30b9\u30c8\u6a5f\u80fd\n\u30c7\u30d7\u30ed\u30a4\u3057\u305f\u30af\u30e9\u30b9\u30bf\u4e0a\u3067\u52d5\u4f5c\u3055\u305b\u308b\u305f\u3081\u306e\u30b5\u30f3\u30d7\u30eb\u30a2\u30d7\u30ea\u3084\u3001\u30a2\u30d7\u30ea\u3067\u4f7f\u7528\u3059\u308b\u696d\u52d9\u30c7\u30fc\u30bf\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\n1\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u30842\u306e\u7d71\u5408\u30c6\u30b9\u30c8\u30fb\u30b9\u30e2\u30fc\u30af\u30c6\u30b9\u30c8\u3092\u3001VirtualBox \u4e0a\u306e VM \u3084 Docker \u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3059\u308b\u6a5f\u80fd\n\n\u3053\u306e\u8a18\u4e8b\u3067\u306f\u6700\u5f8c\u306e\u3001Docker \u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3059\u308b Hadoop \u30af\u30e9\u30b9\u30bf\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u524d\u63d0\n\u52d5\u4f5c\u78ba\u8a8d\u306b\u306f\u4ee5\u4e0b\u306e\u74b0\u5883\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\n\n\u74b0\u5883: Microsoft Azure\nVirtual Machine Size: Standard DS11 v2 (2 cores, 14 GB memory)\nOS: Ubuntu Server 16.04 LTS\n\n\u307e\u305f\u3001\u52d5\u4f5c\u78ba\u8a8d\u6642\u306e Bigtop \u306e HEAD \u30ea\u30d3\u30b8\u30e7\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\nsekikn@bigtop:~/bigtop$ git rev-parse HEAD\nf4d023b4c505efbb3c5b52cb0aa7ceb9dc20cc60\n\n\n\u5358\u4e00\u30ce\u30fc\u30c9\u3067\u306e Hadoop \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\n\u8d77\u52d5\u3057\u305f\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3001\u4e8b\u524d\u6e96\u5099\u3092\u884c\u3044\u307e\u3059\u3002\n\u307e\u305a\u3001\u521d\u671f\u72b6\u614b\u3060\u3068 Docker \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u3001Docker \u672c\u4f53\u3068 docker-compose \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\u305d\u306e\u5f8c\u3001Docker \u30b5\u30fc\u30d3\u30b9\u3092\u8d77\u52d5\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u30e6\u30fc\u30b6\u3092 docker \u30b0\u30eb\u30fc\u30d7\u306b\u8ffd\u52a0\u3057\u307e\u3059 (\u8ffd\u52a0\u5f8c\u3001\u4e00\u5ea6\u30ed\u30b0\u30a2\u30a6\u30c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093).\nsekikn@bigtop:~$ sudo apt-get install docker.io docker-compose\nsekikn@bigtop:~$ sudo service docker start\nsekikn@bigtop:~$ sudo gpasswd -a sekikn docker\n\n\u307e\u305f\u3001\u5f8c\u8ff0\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u306e\u5b9f\u884c\u306b\u5fc5\u8981\u306a\u306e\u3067\u3001Ruby \u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304d\u307e\u3059\u3002\nsekikn@bigtop:~$ sudo apt-get install ruby2.3\n\n\u3053\u308c\u3067\u6e96\u5099\u304c\u6574\u3044\u307e\u3057\u305f\u306e\u3067\u3001GitHub \u306e Bigtop \u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30ed\u30fc\u30ab\u30eb\u306b clone \u3057\u3066\u304d\u307e\u3059\u3002\nsekikn@bigtop:~$ git clone https://github.com/apache/bigtop.git\n\n\u305d\u3057\u3066\u3001Docker \u7528\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30ca\u3092\u63d0\u4f9b\u3057\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u307e\u3059\nsekikn@bigtop:~$ cd bigtop/provisioner/docker\nsekikn@bigtop:~/bigtop/provisioner/docker$ ls\nconfig  config_centos7.yaml  config_debian.yaml  config_ubuntu.yaml  config.yaml  docker-compose.yml  docker-hadoop.sh  README.md\n\n\u3053\u308c\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5f79\u5272\u3092\u7c21\u5358\u306b\u8aac\u660e\u3057\u307e\u3059\u3002\n\n\nconfig \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u4f5c\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ca\u306b\u30de\u30a6\u30f3\u30c8\u3055\u308c\u3001\u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3057\u3066\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3084\u540d\u524d\u89e3\u6c7a\u306e\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\u30e6\u30fc\u30b6\u304c\u7de8\u96c6\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\nconfig_ \u304b\u3089\u59cb\u307e\u308b YAML \u30d5\u30a1\u30a4\u30eb\u306f Docker \u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u6642\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u3001\u4f7f\u7528\u3059\u308b Linux \u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306b\u30b5\u30f3\u30d7\u30eb\u304c\u4ed8\u3044\u3066\u3044\u307e\u3059 (config_centos7.yaml: CentOS 7, config_debian.yaml: Debian, config_ubuntu.yaml: Ubuntu). \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001config.yaml \u30d5\u30a1\u30a4\u30eb (CentOS 6) \u304c\u4f7f\u308f\u308c\u307e\u3059\u3002\u30e6\u30fc\u30b6\u306f\u4f7f\u3044\u305f\u3044\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u306b\u5fdc\u3058\u3066\u4f7f\u7528\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u9078\u629e\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u3092\u5909\u66f4\u3057\u307e\u3059\u3002\n\ndocker-compose.yml \u306f\u3001\u8907\u6570\u306e Docker \u30b3\u30f3\u30c6\u30ca\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3042\u308b docker-compose \u304c\u4f7f\u7528\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u7de8\u96c6\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\ndocker-hadoop.sh \u304c\u3001\u30e6\u30fc\u30b6\u304c\u5b9f\u884c\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u3059\u3002\u6307\u5b9a\u3057\u305f\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u8a18\u8ff0\u306b\u5f93\u3044\u3001docker-compose \u3068\u9023\u643a\u3057\u3066\u3001\u5fc5\u8981\u306a\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u3084\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u306a\u3069\u3092\u884c\u3044\u307e\u3059\u3002\n\nREADME.md \u306f\u3001docker-hadoop.sh \u306e\u8a73\u3057\u3044\u4f7f\u7528\u65b9\u6cd5\u304c\u66f8\u3044\u3066\u3042\u308b\u3001\u6587\u5b57\u901a\u308a\u306e README \u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\n\n\u305d\u308c\u3067\u306f\u3001\u30af\u30e9\u30b9\u30bf\u3092\u8d77\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\u4f7f\u7528\u3059\u308b\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306f -C \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u307e\u3059\u304c\u3001\u500b\u4eba\u7684\u306b Ubuntu \u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3053\u3068\u304b\u3089\u3001config_ubuntu.yaml \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u307e\u305f\u3001-c \u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u30af\u30e9\u30b9\u30bf\u306e\u8d77\u52d5\u3092\u6307\u793a\u3057\u307e\u3059\u304c\u3001\u5f15\u6570\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u30ce\u30fc\u30c9\u306e\u53f0\u6570\u3092\u53d6\u308a\u307e\u3059\u3002\u4eca\u56de\u306f\u5358\u4e00\u30ce\u30fc\u30c9\u3067\u8d77\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -C config_ubuntu.yaml -c 1\n\n(snip)\n\nNotice: Finished catalog run in 367.28 seconds\n\n\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u3001\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u3001\u3057\u3070\u3089\u304f\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002\u4eca\u56de\u306e\u74b0\u5883\u3067\u306f6\u5206\u307b\u3069\u3067\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\n\u305d\u308c\u3067\u306f\u3001\u52d5\u4f5c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u3092 -l \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -l\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nThe DOCKER_IMAGE variable is not set. Defaulting to a blank string.\n            Name                 Command     State   Ports \n----------------------------------------------------------\n20161204142027r27164_bigtop_1   /sbin/init   Up            \n\n\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u305f\u3089\u3001\u3053\u306e\u30ce\u30fc\u30c9\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e1\u53f0\u76ee\u306e\u30ce\u30fc\u30c9\u306b\u5bfe\u3057\u3001bash \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -e 1 bash\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nroot@eafb5ee278dc:/# \n\n\u7121\u4e8b\u3001\u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3067\u304d\u307e\u3057\u305f\u3002\u5c11\u3057\u72b6\u614b\u3092\u8abf\u3079\u307e\u3059\u3002\nroot@eafb5ee278dc:/# jps\n10750 JobHistoryServer\n11766 Jps\n11263 NameNode\n10414 ResourceManager\n11512 DataNode\n10967 NodeManager\n10269 WebAppProxyServer\n\nroot@eafb5ee278dc:/# hdfs dfs -ls /\nFound 7 items\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /apps\ndrwxrwxrwx   - hdfs  hadoop          0 2016-12-04 14:27 /benchmarks\ndrwxr-xr-x   - hbase hbase           0 2016-12-04 14:27 /hbase\ndrwxr-xr-x   - solr  solr            0 2016-12-04 14:27 /solr\ndrwxrwxrwt   - hdfs  hadoop          0 2016-12-04 14:27 /tmp\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /user\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /var\n\nroot@eafb5ee278dc:/# yarn jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 100 100\nNumber of Maps  = 100\nSamples per Map = 100\n\n(snip)\n\nJob Finished in 226.281 seconds\nEstimated value of Pi is 3.14080000000000000000\n\nHadoop \u306e\u4e3b\u8981\u30d7\u30ed\u30bb\u30b9\u304c\u8d77\u52d5\u3055\u308c\u3066\u304a\u308a\u3001HDFS \u3084 YARN (MRv2) \u304c\u5229\u7528\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\nWeb UI \u306b\u3082\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u3001\u30b3\u30f3\u30c6\u30ca\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\nroot@eafb5ee278dc:/# ifconfig \neth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  \n          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n(snip)\n\n\u4eca\u56de\u306f\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u3067\u4f5c\u696d\u3092\u884c\u3063\u3066\u3044\u308b\u306e\u3067\u3001SSH\u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c9\u3092\u5229\u7528\u3057\u3066\u3001\u30b3\u30f3\u30c6\u30ca\u4e0a\u3067\u958b\u304b\u308c\u3066\u3044\u308b\u30dd\u30fc\u30c8\u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u624b\u5143\u306e\u30de\u30b7\u30f3\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002xxx.xxx.xxx.xxx \u306f\u3001\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u30a2\u30c9\u30ec\u30b9\u3067\u3059\u3002\nsekikn@local:~$ ssh -N -L 50070:172.17.0.2:50070 xxx.xxx.xxx.xxx\n\n\u3053\u306e\u72b6\u614b\u3067 http://localhost:50070/ \u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b HDFS Web UI \u304c\u8868\u793a\u3055\u308c\u307e\u3057\u305f\u3002\n\n\u3053\u306e\u3088\u3046\u306b\u3001Bigtop \u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\u3068 Docker \u30b5\u30dd\u30fc\u30c8\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u3082\u306e\u306e10\u5206\u7a0b\u5ea6\u3067 Hadoop \u30af\u30e9\u30b9\u30bf\u3092\u624b\u5143\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n\n\u8907\u6570\u30ce\u30fc\u30c9\u3067\u306e Hadoop \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u3068\u3001\u8ffd\u52a0\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5c0e\u5165\n\u3055\u3066\u3001\u4e0a\u3067\u69cb\u7bc9\u3057\u305f Hadoop \u30af\u30e9\u30b9\u30bf\u306f\u3001\u30af\u30e9\u30b9\u30bf\u3068\u306f\u540d\u3070\u304b\u308a\u306e1\u53f0\u69cb\u6210\u3067\u3057\u305f\u3002\u307e\u305f\u3001jps \u306e\u7d50\u679c\u304b\u3089\u3082\u308f\u304b\u308b\u3068\u304a\u308a\u3001Hadoop \u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20 (HDFS, YARN) \u3057\u304b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002\u305d\u3053\u3067\u3001\u6b21\u306f\u30af\u30e9\u30b9\u30bf\u306e\u30ce\u30fc\u30c9\u6570\u3092\u5897\u3084\u3059\u3068\u3068\u3082\u306b\u3001Hive \u3068 Spark \u3068\u3044\u3063\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\u307e\u305a\u3001\u5148\u307b\u3069\u4f5c\u3063\u305f\u30af\u30e9\u30b9\u30bf\u3092\u3044\u3063\u305f\u3093\u7834\u68c4\u3057\u307e\u3059\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -d\n\n\u306a\u304a\u3001\u4eca\u56de\u306e\u74b0\u5883\u3067\u306f\u3001\u4e0a\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u6642\u306b\u3001\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u3044\u307e\u3057\u305f\u3002\n./docker-hadoop.sh: line 128: ./config/hosts: Permission denied\n\n\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u6642\u306b\u5404\u30b3\u30f3\u30c6\u30ca\u306b\u4ed8\u4e0e\u3055\u308c\u305f\u30db\u30b9\u30c8\u540d\u3068IP\u30a2\u30c9\u30ec\u30b9\u306e\u5bfe\u5fdc\u95a2\u4fc2\u304c\u66f8\u304d\u8fbc\u307e\u308c\u305f\u5f8c\u3001Docker \u30b3\u30f3\u30c6\u30ca\u4e0a\u306b /etc/hosts \u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8\u3055\u308c\u308b\u3053\u3068\u3067\u3001\u5404\u30ce\u30fc\u30c9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u4f7f\u308f\u308c\u307e\u3059\u3002\u524d\u56de\u306e\u8a2d\u5b9a\u304c\u6b8b\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u30c1\u30a7\u30c3\u30af\u30a2\u30a6\u30c8\u6642\u306e\u72b6\u614b (\u7a7a) \u306b\u623b\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ git status\n\n(snip)\n\n    modified:   config/hosts\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nsekikn@bigtop:~/bigtop/provisioner/docker$ git checkout config/hosts\nsekikn@bigtop:~/bigtop/provisioner/docker$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nnothing to commit, working directory clean\n\n\u305d\u3057\u3066\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306b Hive \u3068 Spark \u3092\u8ffd\u52a0\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ vi config_ubuntu.yaml # \u4ee5\u4e0b\u306e\u884c\u3092\u5909\u66f4\u3059\u308b\nsekikn@bigtop:~/bigtop/provisioner/docker$ git diff\ndiff --git a/provisioner/docker/config_ubuntu.yaml b/provisioner/docker/config_ubuntu.yaml\nindex e4ea6f3..e5fcd01 100644\n--- a/provisioner/docker/config_ubuntu.yaml\n+++ b/provisioner/docker/config_ubuntu.yaml\n@@ -23,7 +23,7 @@ boot2docker:\n\n repo: \"http://bigtop-repos.s3.amazonaws.com/releases/1.0.0/ubuntu/trusty/x86_64\"\n distro: debian\n-components: [hadoop, yarn]\n+components: [hadoop, yarn, hive, spark]\n namenode_ui_port: \"50070\"\n yarn_ui_port: \"8088\"\n hbase_ui_port: \"60010\"\n\n\u305d\u308c\u3067\u306f\u3001-C \u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u5f15\u6570\u306b\u5909\u66f4\u5f8c\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u3001-c \u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u5f15\u6570\u306b\u30ce\u30fc\u30c9\u6570 (5\u3068\u3057\u307e\u3059) \u3092\u6307\u5b9a\u3057\u3066\u3001docker-hadoop.sh \u3092\u5b9f\u884c\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -C config_ubuntu.yaml -c 5\n\n(snip)\n\nNotice: Finished catalog run in 566.56 seconds\n\n\u8d77\u52d5\u3059\u308b\u30b3\u30f3\u30c6\u30ca\u306e\u6570\u3068\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u6570\u304c\u5897\u3048\u305f\u5206\u3001\u69cb\u7bc9\u306b\u5fc5\u8981\u306a\u6642\u9593\u304c\u5c11\u3057\u4f38\u3073\u3066\u300110\u5206\u5f31\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u3067\u306f\u30015\u3064\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -l\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nThe DOCKER_IMAGE variable is not set. Defaulting to a blank string.\n            Name                 Command     State   Ports \n----------------------------------------------------------\n20161204153854r30862_bigtop_1   /sbin/init   Up            \n20161204153854r30862_bigtop_2   /sbin/init   Up            \n20161204153854r30862_bigtop_3   /sbin/init   Up            \n20161204153854r30862_bigtop_4   /sbin/init   Up            \n20161204153854r30862_bigtop_5   /sbin/init   Up            \n\n\u8907\u6570\u53f0\u69cb\u6210\u306e\u5834\u5408\u306f\u3001\u6700\u521d\u306e1\u30ce\u30fc\u30c9\u304c\u30de\u30b9\u30bf\u30fc\u3001\u305d\u308c\u4ee5\u5916\u306f\u30b9\u30ec\u30fc\u30d6\u3068\u3044\u3046\u6271\u3044\u306b\u306a\u308a\u307e\u3059\u3002\n\u305d\u308c\u3067\u306f\u30de\u30b9\u30bf\u30fc\u30b5\u30fc\u30d0\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066\u3001\u8ffd\u52a0\u3057\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u4f7f\u3048\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u306f Hive \u3067\u3059\u3002\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -e 1 bash\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nroot@3fde12cd3334:/# hive \n\n(snip)\n\nhive> create table hoge(fuga string);\nOK\nTime taken: 0.815 seconds\n\nhive> insert overwrite table hoge values ('foo'), ('bar');\nQuery ID = root_20161204155353_7f63418d-ed5d-4978-8f31-dfc76c7b80fb\nTotal jobs = 3\nLaunching Job 1 out of 3\nNumber of reduce tasks is set to 0 since there's no reduce operator\nStarting Job = job_1480866447700_0001, Tracking URL = http://3fde12cd3334.bigtop.apache.org:20888/proxy/application_1480866447700_0001/\nKill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1480866447700_0001\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n2016-12-04 15:53:19,603 Stage-1 map = 0%,  reduce = 0%\n2016-12-04 15:53:26,892 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec\nMapReduce Total cumulative CPU time: 940 msec\nEnded Job = job_1480866447700_0001\nStage-4 is selected by condition resolver.\nStage-3 is filtered out by condition resolver.\nStage-5 is filtered out by condition resolver.\nMoving data to: hdfs://3fde12cd3334.bigtop.apache.org:8020/tmp/hive/root/c0a3b0f9-e833-4e52-9bf9-f4893cc77289/hive_2016-12-04_15-53-10_742_4565711671967633591-1/-ext-10000\nLoading data to table default.hoge\nTable default.hoge stats: [numFiles=1, numRows=2, totalSize=8, rawDataSize=6]\nMapReduce Jobs Launched: \nStage-Stage-1: Map: 1   Cumulative CPU: 0.94 sec   HDFS Read: 302 HDFS Write: 76 SUCCESS\nTotal MapReduce CPU Time Spent: 940 msec\nOK\nTime taken: 17.571 seconds\n\nhive> select count(*) from hoge;\nQuery ID = root_20161204155353_a3977e8c-b08b-4e2a-b665-2a62dbf1ab92\nTotal jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nStarting Job = job_1480866447700_0002, Tracking URL = http://3fde12cd3334.bigtop.apache.org:20888/proxy/application_1480866447700_0002/\nKill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1480866447700_0002\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2016-12-04 15:53:41,241 Stage-1 map = 0%,  reduce = 0%\n2016-12-04 15:53:47,492 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.82 sec\n2016-12-04 15:53:52,679 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.8 sec\nMapReduce Total cumulative CPU time: 1 seconds 800 msec\nEnded Job = job_1480866447700_0002\nMapReduce Jobs Launched: \nStage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 1.8 sec   HDFS Read: 238 HDFS Write: 2 SUCCESS\nTotal MapReduce CPU Time Spent: 1 seconds 800 msec\nOK\n2\nTime taken: 18.505 seconds, Fetched: 1 row(s)\n\n\u554f\u984c\u306a\u304f\u4f7f\u3048\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u6b21\u306f Spark \u3067\u3059\u3002\nroot@3fde12cd3334:/# spark-shell \ntput: No value for $TERM and no -T specified\n16/12/04 16:00:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/12/04 16:00:59 INFO SecurityManager: Changing view acls to: root\n16/12/04 16:00:59 INFO SecurityManager: Changing modify acls to: root\n16/12/04 16:00:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)\n16/12/04 16:00:59 INFO HttpServer: Starting HTTP Server\n16/12/04 16:00:59 INFO Server: jetty-8.y.z-SNAPSHOT\n16/12/04 16:00:59 INFO AbstractConnector: Started SocketConnector@0.0.0.0:35650\n16/12/04 16:00:59 INFO Utils: Successfully started service 'HTTP class server' on port 35650.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.3.1\n      /_/\n\n(snip)\n\nscala> sc.parallelize(1 to 100).filter(_ % 2 == 0).count\n16/12/04 16:02:54 INFO SparkContext: Starting job: count at <console>:22\n16/12/04 16:02:54 INFO DAGScheduler: Got job 0 (count at <console>:22) with 2 output partitions (allowLocal=false)\n16/12/04 16:02:54 INFO DAGScheduler: Final stage: Stage 0(count at <console>:22)\n16/12/04 16:02:54 INFO DAGScheduler: Parents of final stage: List()\n16/12/04 16:02:54 INFO DAGScheduler: Missing parents: List()\n16/12/04 16:02:54 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[1] at filter at <console>:22), which has no missing parents\n16/12/04 16:02:54 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=0, maxMem=278302556\n16/12/04 16:02:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1776.0 B, free 265.4 MB)\n16/12/04 16:02:54 INFO MemoryStore: ensureFreeSpace(1285) called with curMem=1776, maxMem=278302556\n16/12/04 16:02:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1285.0 B, free 265.4 MB)\n16/12/04 16:02:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fde12cd3334.bigtop.apache.org:36932 (size: 1285.0 B, free: 265.4 MB)\n16/12/04 16:02:54 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0\n16/12/04 16:02:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:839\n16/12/04 16:02:54 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MapPartitionsRDD[1] at filter at <console>:22)\n16/12/04 16:02:54 INFO YarnScheduler: Adding task set 0.0 with 2 tasks\n16/12/04 16:02:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 59ee358fd33a.bigtop.apache.org, PROCESS_LOCAL, 1260 bytes)\n16/12/04 16:02:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 479b6117c03a.bigtop.apache.org, PROCESS_LOCAL, 1317 bytes)\n16/12/04 16:02:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 479b6117c03a.bigtop.apache.org:38012 (size: 1285.0 B, free: 530.3 MB)\n16/12/04 16:02:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 59ee358fd33a.bigtop.apache.org:45933 (size: 1285.0 B, free: 530.3 MB)\n16/12/04 16:02:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 677 ms on 479b6117c03a.bigtop.apache.org (1/2)\n16/12/04 16:02:55 INFO DAGScheduler: Stage 0 (count at <console>:22) finished in 0.717 s\n16/12/04 16:02:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 714 ms on 59ee358fd33a.bigtop.apache.org (2/2)\n16/12/04 16:02:55 INFO DAGScheduler: Job 0 finished: count at <console>:22, took 0.928298 s\n16/12/04 16:02:55 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \nres0: Long = 50\n\n\u3053\u3061\u3089\u3082\u554f\u984c\u306a\u3044\u3088\u3046\u3067\u3059\u3002\n\n\u307e\u3068\u3081\nHadoop \u30af\u30e9\u30b9\u30bf\u3092 Docker \u30b3\u30f3\u30c6\u30ca\u4e0a\u306b\u304a\u624b\u8efd\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3001Apache Bigtop \u306e\u6a5f\u80fd\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306e\u624b\u9806\u3067\u306f\u3001Bigtop \u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b\u516c\u958b\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u304c\u3001\u4ee3\u308f\u308a\u306b\u81ea\u4f5c\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u308a\u3001\u305d\u306e\u72b6\u614b\u3067 Bigtop \u304c\u63d0\u4f9b\u3059\u308b\u30c6\u30b9\u30c8\u7fa4\u3092\u5b9f\u884c\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u8a73\u7d30\u306f bigtop/provisioner/docker/README.md \u306a\u3069\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\u307e\u305f\u3001Docker \u304c\u5229\u7528\u3067\u304d\u306a\u3044\u74b0\u5883\u3067\u306f\u3001VirtualBox \u4e0a\u306b Vagrant \u3067\u69cb\u7bc9\u3057\u305f VM \u4e0a\u306b Hadoop \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30b9\u30bf\u30c3\u30af\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002bigtop-deploy/vm/vagrant-puppet-vm \u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002docker-hadoop.sh \u30b3\u30de\u30f3\u30c9\u306e\u4ee3\u308f\u308a\u306b vagrant up \u30b3\u30de\u30f3\u30c9\u3067 VM \u3092\u8d77\u52d5\u3059\u308b\u306a\u3069\u3001\u7d30\u304b\u3044\u9055\u3044\u306f\u3042\u308a\u307e\u3059\u304c\u3001\u6982\u306d\u4e0a\u8a18\u306e\u77e5\u8b58\u304c\u6d41\u7528\u3067\u304d\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u3042\u308b vagrantconfig.yaml \u306e\u8a18\u8ff0\u65b9\u6cd5\u306f Docker \u7248\u306e config_ \u30d5\u30a1\u30a4\u30eb\u7fa4\u3068\u540c\u69d8\u3067\u3059\u3002\n[Distributed computing (Apache Hadoop, Spark, ...) Advent Calendar 2016](http://qiita.com/advent-calendar/2016/distributed-computing) \u306e24\u65e5\u76ee\u3067\u3059\u3002\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001Hadoop \u30af\u30e9\u30b9\u30bf\u3092\u624b\u8efd\u306b\u624b\u5143\u306b\u69cb\u7bc9\u3057\u305f\u3044\u3068\u304d\u306b\u4fbf\u5229\u306a\u3001Apache Bigtop \u306e\u6a5f\u80fd\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n# Apache Bigtop \u3068\u306f\n\nApache Bigtop \u306f\u3001Hadoop \u3068\u305d\u306e\u5468\u8fba\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u30d3\u30eb\u30c9\u3057\u3001 deb \u3084 rpm \u3068\u3044\u3063\u305f\u5f62\u5f0f\u3067\u30d1\u30c3\u30b1\u30fc\u30b8\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u7a2e Linux \u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3078\u306e\u5c0e\u5165\u3092\u5bb9\u6613\u306b\u3059\u308b\u305f\u3081\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u3059\u3002\u305d\u308c\u4ee5\u5916\u306b\u3082\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u6a5f\u80fd\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\n\n1. \u4f5c\u6210\u3057\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5404\u30ce\u30fc\u30c9\u306b\u30c7\u30d7\u30ed\u30a4\u3057\u3001\u9069\u5207\u306b\u8a2d\u5b9a\u3059\u308b\u305f\u3081\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\n2. \u30c7\u30d7\u30ed\u30a4\u7d50\u679c\u306e\u78ba\u8a8d\u3084\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30d0\u30fc\u30b8\u30e7\u30f3\u9593\u306e\u76f8\u4e92\u904b\u7528\u6027\u306e\u62c5\u4fdd\u306b\u4f7f\u308f\u308c\u308b\u3001\u7d71\u5408\u30c6\u30b9\u30c8\u30fb\u30b9\u30e2\u30fc\u30af\u30c6\u30b9\u30c8\u6a5f\u80fd\n3. \u30c7\u30d7\u30ed\u30a4\u3057\u305f\u30af\u30e9\u30b9\u30bf\u4e0a\u3067\u52d5\u4f5c\u3055\u305b\u308b\u305f\u3081\u306e\u30b5\u30f3\u30d7\u30eb\u30a2\u30d7\u30ea\u3084\u3001\u30a2\u30d7\u30ea\u3067\u4f7f\u7528\u3059\u308b\u696d\u52d9\u30c7\u30fc\u30bf\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\n4. 1\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u30842\u306e\u7d71\u5408\u30c6\u30b9\u30c8\u30fb\u30b9\u30e2\u30fc\u30af\u30c6\u30b9\u30c8\u3092\u3001VirtualBox \u4e0a\u306e VM \u3084 Docker \u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3059\u308b\u6a5f\u80fd\n\n\u3053\u306e\u8a18\u4e8b\u3067\u306f\u6700\u5f8c\u306e\u3001Docker \u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3059\u308b Hadoop \u30af\u30e9\u30b9\u30bf\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\u306b\u3064\u3044\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n# \u524d\u63d0\n\n\u52d5\u4f5c\u78ba\u8a8d\u306b\u306f\u4ee5\u4e0b\u306e\u74b0\u5883\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u3002\n\n* \u74b0\u5883: Microsoft Azure\n* Virtual Machine Size: Standard DS11 v2 (2 cores, 14 GB memory)\n* OS: Ubuntu Server 16.04 LTS\n\n\u307e\u305f\u3001\u52d5\u4f5c\u78ba\u8a8d\u6642\u306e Bigtop \u306e HEAD \u30ea\u30d3\u30b8\u30e7\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n```\nsekikn@bigtop:~/bigtop$ git rev-parse HEAD\nf4d023b4c505efbb3c5b52cb0aa7ceb9dc20cc60\n```\n\n# \u5358\u4e00\u30ce\u30fc\u30c9\u3067\u306e Hadoop \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\n\n\u8d77\u52d5\u3057\u305f\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3001\u4e8b\u524d\u6e96\u5099\u3092\u884c\u3044\u307e\u3059\u3002\n\u307e\u305a\u3001\u521d\u671f\u72b6\u614b\u3060\u3068 Docker \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u306e\u3067\u3001Docker \u672c\u4f53\u3068 docker-compose \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\u305d\u306e\u5f8c\u3001Docker \u30b5\u30fc\u30d3\u30b9\u3092\u8d77\u52d5\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u30e6\u30fc\u30b6\u3092 docker \u30b0\u30eb\u30fc\u30d7\u306b\u8ffd\u52a0\u3057\u307e\u3059 (\u8ffd\u52a0\u5f8c\u3001\u4e00\u5ea6\u30ed\u30b0\u30a2\u30a6\u30c8\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093).\n\n```\nsekikn@bigtop:~$ sudo apt-get install docker.io docker-compose\nsekikn@bigtop:~$ sudo service docker start\nsekikn@bigtop:~$ sudo gpasswd -a sekikn docker\n```\n\n\u307e\u305f\u3001\u5f8c\u8ff0\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u306e\u5b9f\u884c\u306b\u5fc5\u8981\u306a\u306e\u3067\u3001Ruby \u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n```\nsekikn@bigtop:~$ sudo apt-get install ruby2.3\n```\n\n\u3053\u308c\u3067\u6e96\u5099\u304c\u6574\u3044\u307e\u3057\u305f\u306e\u3067\u3001GitHub \u306e Bigtop \u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30ed\u30fc\u30ab\u30eb\u306b clone \u3057\u3066\u304d\u307e\u3059\u3002\n\n```\nsekikn@bigtop:~$ git clone https://github.com/apache/bigtop.git\n```\n\n\u305d\u3057\u3066\u3001Docker \u7528\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30ca\u3092\u63d0\u4f9b\u3057\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u307e\u3059\n\n```\nsekikn@bigtop:~$ cd bigtop/provisioner/docker\nsekikn@bigtop:~/bigtop/provisioner/docker$ ls\nconfig  config_centos7.yaml  config_debian.yaml  config_ubuntu.yaml  config.yaml  docker-compose.yml  docker-hadoop.sh  README.md\n```\n\n\u3053\u308c\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5f79\u5272\u3092\u7c21\u5358\u306b\u8aac\u660e\u3057\u307e\u3059\u3002\n\n* `config` \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u4f5c\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ca\u306b\u30de\u30a6\u30f3\u30c8\u3055\u308c\u3001\u30b3\u30f3\u30c6\u30ca\u306b\u5bfe\u3057\u3066\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3084\u540d\u524d\u89e3\u6c7a\u306e\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\u30e6\u30fc\u30b6\u304c\u7de8\u96c6\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n* `config_` \u304b\u3089\u59cb\u307e\u308b YAML \u30d5\u30a1\u30a4\u30eb\u306f Docker \u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u6642\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u3001\u4f7f\u7528\u3059\u308b Linux \u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306b\u30b5\u30f3\u30d7\u30eb\u304c\u4ed8\u3044\u3066\u3044\u307e\u3059 (config_centos7.yaml: CentOS 7, config_debian.yaml: Debian, config_ubuntu.yaml: Ubuntu). \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001`config.yaml` \u30d5\u30a1\u30a4\u30eb (CentOS 6) \u304c\u4f7f\u308f\u308c\u307e\u3059\u3002\u30e6\u30fc\u30b6\u306f\u4f7f\u3044\u305f\u3044\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u306b\u5fdc\u3058\u3066\u4f7f\u7528\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u9078\u629e\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u3092\u5909\u66f4\u3057\u307e\u3059\u3002\n* `docker-compose.yml` \u306f\u3001\u8907\u6570\u306e Docker \u30b3\u30f3\u30c6\u30ca\u3092\u7ba1\u7406\u3059\u308b\u305f\u3081\u306e\u30c4\u30fc\u30eb\u3067\u3042\u308b docker-compose \u304c\u4f7f\u7528\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\u57fa\u672c\u7684\u306b\u306f\u7de8\u96c6\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n* `docker-hadoop.sh` \u304c\u3001\u30e6\u30fc\u30b6\u304c\u5b9f\u884c\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u3059\u3002\u6307\u5b9a\u3057\u305f\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u8a18\u8ff0\u306b\u5f93\u3044\u3001docker-compose \u3068\u9023\u643a\u3057\u3066\u3001\u5fc5\u8981\u306a\u30b3\u30f3\u30c6\u30ca\u306e\u8d77\u52d5\u3084\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u306a\u3069\u3092\u884c\u3044\u307e\u3059\u3002\n* `README.md` \u306f\u3001`docker-hadoop.sh` \u306e\u8a73\u3057\u3044\u4f7f\u7528\u65b9\u6cd5\u304c\u66f8\u3044\u3066\u3042\u308b\u3001\u6587\u5b57\u901a\u308a\u306e README \u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\n\n\u305d\u308c\u3067\u306f\u3001\u30af\u30e9\u30b9\u30bf\u3092\u8d77\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\u4f7f\u7528\u3059\u308b\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306f `-C` \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u307e\u3059\u304c\u3001\u500b\u4eba\u7684\u306b Ubuntu \u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u591a\u3044\u3053\u3068\u304b\u3089\u3001`config_ubuntu.yaml` \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u307e\u305f\u3001`-c` \u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u30af\u30e9\u30b9\u30bf\u306e\u8d77\u52d5\u3092\u6307\u793a\u3057\u307e\u3059\u304c\u3001\u5f15\u6570\u3068\u3057\u3066\u8d77\u52d5\u3059\u308b\u30ce\u30fc\u30c9\u306e\u53f0\u6570\u3092\u53d6\u308a\u307e\u3059\u3002\u4eca\u56de\u306f\u5358\u4e00\u30ce\u30fc\u30c9\u3067\u8d77\u52d5\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -C config_ubuntu.yaml -c 1\n\n(snip)\n\nNotice: Finished catalog run in 367.28 seconds\n```\n\n\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u3001\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u304c\u5b8c\u4e86\u3059\u308b\u307e\u3067\u3001\u3057\u3070\u3089\u304f\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002\u4eca\u56de\u306e\u74b0\u5883\u3067\u306f6\u5206\u307b\u3069\u3067\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\n\u305d\u308c\u3067\u306f\u3001\u52d5\u4f5c\u4e2d\u306e\u30b3\u30f3\u30c6\u30ca\u3092 `-l` \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -l\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nThe DOCKER_IMAGE variable is not set. Defaulting to a blank string.\n            Name                 Command     State   Ports \n----------------------------------------------------------\n20161204142027r27164_bigtop_1   /sbin/init   Up            \n```\n\n\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u305f\u3089\u3001\u3053\u306e\u30ce\u30fc\u30c9\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e1\u53f0\u76ee\u306e\u30ce\u30fc\u30c9\u306b\u5bfe\u3057\u3001bash \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -e 1 bash\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nroot@eafb5ee278dc:/# \n```\n\n\u7121\u4e8b\u3001\u30b3\u30f3\u30c6\u30ca\u306b\u30ed\u30b0\u30a4\u30f3\u3067\u304d\u307e\u3057\u305f\u3002\u5c11\u3057\u72b6\u614b\u3092\u8abf\u3079\u307e\u3059\u3002\n\n```\nroot@eafb5ee278dc:/# jps\n10750 JobHistoryServer\n11766 Jps\n11263 NameNode\n10414 ResourceManager\n11512 DataNode\n10967 NodeManager\n10269 WebAppProxyServer\n\nroot@eafb5ee278dc:/# hdfs dfs -ls /\nFound 7 items\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /apps\ndrwxrwxrwx   - hdfs  hadoop          0 2016-12-04 14:27 /benchmarks\ndrwxr-xr-x   - hbase hbase           0 2016-12-04 14:27 /hbase\ndrwxr-xr-x   - solr  solr            0 2016-12-04 14:27 /solr\ndrwxrwxrwt   - hdfs  hadoop          0 2016-12-04 14:27 /tmp\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /user\ndrwxr-xr-x   - hdfs  hadoop          0 2016-12-04 14:27 /var\n\nroot@eafb5ee278dc:/# yarn jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 100 100\nNumber of Maps  = 100\nSamples per Map = 100\n\n(snip)\n\nJob Finished in 226.281 seconds\nEstimated value of Pi is 3.14080000000000000000\n```\n\nHadoop \u306e\u4e3b\u8981\u30d7\u30ed\u30bb\u30b9\u304c\u8d77\u52d5\u3055\u308c\u3066\u304a\u308a\u3001HDFS \u3084 YARN (MRv2) \u304c\u5229\u7528\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\nWeb UI \u306b\u3082\u30a2\u30af\u30bb\u30b9\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u3001\u30b3\u30f3\u30c6\u30ca\u306eIP\u30a2\u30c9\u30ec\u30b9\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n\n```\nroot@eafb5ee278dc:/# ifconfig \neth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  \n          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n(snip)\n```\n\n\u4eca\u56de\u306f\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u3067\u4f5c\u696d\u3092\u884c\u3063\u3066\u3044\u308b\u306e\u3067\u3001SSH\u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c9\u3092\u5229\u7528\u3057\u3066\u3001\u30b3\u30f3\u30c6\u30ca\u4e0a\u3067\u958b\u304b\u308c\u3066\u3044\u308b\u30dd\u30fc\u30c8\u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3059\u3002\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u624b\u5143\u306e\u30de\u30b7\u30f3\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002xxx.xxx.xxx.xxx \u306f\u3001\u30af\u30e9\u30a6\u30c9\u4e0a\u306e\u30de\u30b7\u30f3\u306e\u30b0\u30ed\u30fc\u30d0\u30eb\u30a2\u30c9\u30ec\u30b9\u3067\u3059\u3002\n\n```\nsekikn@local:~$ ssh -N -L 50070:172.17.0.2:50070 xxx.xxx.xxx.xxx\n```\n\n\u3053\u306e\u72b6\u614b\u3067 http://localhost:50070/ \u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b HDFS Web UI \u304c\u8868\u793a\u3055\u308c\u307e\u3057\u305f\u3002\n\n![Screenshot from 2016-12-04 23-55-54.png](https://qiita-image-store.s3.amazonaws.com/0/106853/848e0945-dd6b-f785-d60d-5b5dca733268.png)\n\n\u3053\u306e\u3088\u3046\u306b\u3001Bigtop \u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6a5f\u80fd\u3068 Docker \u30b5\u30dd\u30fc\u30c8\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u3082\u306e\u306e10\u5206\u7a0b\u5ea6\u3067 Hadoop \u30af\u30e9\u30b9\u30bf\u3092\u624b\u5143\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3057\u305f\u3002\n\n# \u8907\u6570\u30ce\u30fc\u30c9\u3067\u306e Hadoop \u30af\u30e9\u30b9\u30bf\u69cb\u7bc9\u3068\u3001\u8ffd\u52a0\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5c0e\u5165\n\n\u3055\u3066\u3001\u4e0a\u3067\u69cb\u7bc9\u3057\u305f Hadoop \u30af\u30e9\u30b9\u30bf\u306f\u3001\u30af\u30e9\u30b9\u30bf\u3068\u306f\u540d\u3070\u304b\u308a\u306e1\u53f0\u69cb\u6210\u3067\u3057\u305f\u3002\u307e\u305f\u3001jps \u306e\u7d50\u679c\u304b\u3089\u3082\u308f\u304b\u308b\u3068\u304a\u308a\u3001Hadoop \u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20 (HDFS, YARN) \u3057\u304b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002\u305d\u3053\u3067\u3001\u6b21\u306f\u30af\u30e9\u30b9\u30bf\u306e\u30ce\u30fc\u30c9\u6570\u3092\u5897\u3084\u3059\u3068\u3068\u3082\u306b\u3001Hive \u3068 Spark \u3068\u3044\u3063\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3082\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u307e\u305a\u3001\u5148\u307b\u3069\u4f5c\u3063\u305f\u30af\u30e9\u30b9\u30bf\u3092\u3044\u3063\u305f\u3093\u7834\u68c4\u3057\u307e\u3059\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -d\n```\n\n\u306a\u304a\u3001\u4eca\u56de\u306e\u74b0\u5883\u3067\u306f\u3001\u4e0a\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u6642\u306b\u3001\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u3044\u307e\u3057\u305f\u3002\n\n```\n./docker-hadoop.sh: line 128: ./config/hosts: Permission denied\n```\n\n\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u6642\u306b\u5404\u30b3\u30f3\u30c6\u30ca\u306b\u4ed8\u4e0e\u3055\u308c\u305f\u30db\u30b9\u30c8\u540d\u3068IP\u30a2\u30c9\u30ec\u30b9\u306e\u5bfe\u5fdc\u95a2\u4fc2\u304c\u66f8\u304d\u8fbc\u307e\u308c\u305f\u5f8c\u3001Docker \u30b3\u30f3\u30c6\u30ca\u4e0a\u306b /etc/hosts \u3068\u3057\u3066\u30de\u30a6\u30f3\u30c8\u3055\u308c\u308b\u3053\u3068\u3067\u3001\u5404\u30ce\u30fc\u30c9\u306e\u540d\u524d\u89e3\u6c7a\u306b\u4f7f\u308f\u308c\u307e\u3059\u3002\u524d\u56de\u306e\u8a2d\u5b9a\u304c\u6b8b\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u30c1\u30a7\u30c3\u30af\u30a2\u30a6\u30c8\u6642\u306e\u72b6\u614b (\u7a7a) \u306b\u623b\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ git status\n\n(snip)\n\n\tmodified:   config/hosts\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nsekikn@bigtop:~/bigtop/provisioner/docker$ git checkout config/hosts\nsekikn@bigtop:~/bigtop/provisioner/docker$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nnothing to commit, working directory clean\n```\n\n\u305d\u3057\u3066\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306b Hive \u3068 Spark \u3092\u8ffd\u52a0\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ vi config_ubuntu.yaml # \u4ee5\u4e0b\u306e\u884c\u3092\u5909\u66f4\u3059\u308b\nsekikn@bigtop:~/bigtop/provisioner/docker$ git diff\ndiff --git a/provisioner/docker/config_ubuntu.yaml b/provisioner/docker/config_ubuntu.yaml\nindex e4ea6f3..e5fcd01 100644\n--- a/provisioner/docker/config_ubuntu.yaml\n+++ b/provisioner/docker/config_ubuntu.yaml\n@@ -23,7 +23,7 @@ boot2docker:\n \n repo: \"http://bigtop-repos.s3.amazonaws.com/releases/1.0.0/ubuntu/trusty/x86_64\"\n distro: debian\n-components: [hadoop, yarn]\n+components: [hadoop, yarn, hive, spark]\n namenode_ui_port: \"50070\"\n yarn_ui_port: \"8088\"\n hbase_ui_port: \"60010\"\n```\n\n\u305d\u308c\u3067\u306f\u3001`-C` \u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u5f15\u6570\u306b\u5909\u66f4\u5f8c\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u3001`-c` \u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u5f15\u6570\u306b\u30ce\u30fc\u30c9\u6570 (5\u3068\u3057\u307e\u3059) \u3092\u6307\u5b9a\u3057\u3066\u3001`docker-hadoop.sh` \u3092\u5b9f\u884c\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -C config_ubuntu.yaml -c 5\n\n(snip)\n\nNotice: Finished catalog run in 566.56 seconds\n```\n\n\u8d77\u52d5\u3059\u308b\u30b3\u30f3\u30c6\u30ca\u306e\u6570\u3068\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u6570\u304c\u5897\u3048\u305f\u5206\u3001\u69cb\u7bc9\u306b\u5fc5\u8981\u306a\u6642\u9593\u304c\u5c11\u3057\u4f38\u3073\u3066\u300110\u5206\u5f31\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u3067\u306f\u30015\u3064\u306e\u30b3\u30f3\u30c6\u30ca\u304c\u8d77\u52d5\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -l\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nThe DOCKER_IMAGE variable is not set. Defaulting to a blank string.\n            Name                 Command     State   Ports \n----------------------------------------------------------\n20161204153854r30862_bigtop_1   /sbin/init   Up            \n20161204153854r30862_bigtop_2   /sbin/init   Up            \n20161204153854r30862_bigtop_3   /sbin/init   Up            \n20161204153854r30862_bigtop_4   /sbin/init   Up            \n20161204153854r30862_bigtop_5   /sbin/init   Up            \n```\n\n\u8907\u6570\u53f0\u69cb\u6210\u306e\u5834\u5408\u306f\u3001\u6700\u521d\u306e1\u30ce\u30fc\u30c9\u304c\u30de\u30b9\u30bf\u30fc\u3001\u305d\u308c\u4ee5\u5916\u306f\u30b9\u30ec\u30fc\u30d6\u3068\u3044\u3046\u6271\u3044\u306b\u306a\u308a\u307e\u3059\u3002\n\u305d\u308c\u3067\u306f\u30de\u30b9\u30bf\u30fc\u30b5\u30fc\u30d0\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3066\u3001\u8ffd\u52a0\u3057\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u4f7f\u3048\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u3087\u3046\u3002\u307e\u305a\u306f Hive \u3067\u3059\u3002\n\n```\nsekikn@bigtop:~/bigtop/provisioner/docker$ ./docker-hadoop.sh -e 1 bash\nWARNING: The DOCKER_IMAGE variable is not set. Defaulting to a blank string.\nroot@3fde12cd3334:/# hive \n\n(snip)\n\nhive> create table hoge(fuga string);\nOK\nTime taken: 0.815 seconds\n\nhive> insert overwrite table hoge values ('foo'), ('bar');\nQuery ID = root_20161204155353_7f63418d-ed5d-4978-8f31-dfc76c7b80fb\nTotal jobs = 3\nLaunching Job 1 out of 3\nNumber of reduce tasks is set to 0 since there's no reduce operator\nStarting Job = job_1480866447700_0001, Tracking URL = http://3fde12cd3334.bigtop.apache.org:20888/proxy/application_1480866447700_0001/\nKill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1480866447700_0001\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n2016-12-04 15:53:19,603 Stage-1 map = 0%,  reduce = 0%\n2016-12-04 15:53:26,892 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec\nMapReduce Total cumulative CPU time: 940 msec\nEnded Job = job_1480866447700_0001\nStage-4 is selected by condition resolver.\nStage-3 is filtered out by condition resolver.\nStage-5 is filtered out by condition resolver.\nMoving data to: hdfs://3fde12cd3334.bigtop.apache.org:8020/tmp/hive/root/c0a3b0f9-e833-4e52-9bf9-f4893cc77289/hive_2016-12-04_15-53-10_742_4565711671967633591-1/-ext-10000\nLoading data to table default.hoge\nTable default.hoge stats: [numFiles=1, numRows=2, totalSize=8, rawDataSize=6]\nMapReduce Jobs Launched: \nStage-Stage-1: Map: 1   Cumulative CPU: 0.94 sec   HDFS Read: 302 HDFS Write: 76 SUCCESS\nTotal MapReduce CPU Time Spent: 940 msec\nOK\nTime taken: 17.571 seconds\n\nhive> select count(*) from hoge;\nQuery ID = root_20161204155353_a3977e8c-b08b-4e2a-b665-2a62dbf1ab92\nTotal jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks determined at compile time: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nStarting Job = job_1480866447700_0002, Tracking URL = http://3fde12cd3334.bigtop.apache.org:20888/proxy/application_1480866447700_0002/\nKill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1480866447700_0002\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2016-12-04 15:53:41,241 Stage-1 map = 0%,  reduce = 0%\n2016-12-04 15:53:47,492 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.82 sec\n2016-12-04 15:53:52,679 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.8 sec\nMapReduce Total cumulative CPU time: 1 seconds 800 msec\nEnded Job = job_1480866447700_0002\nMapReduce Jobs Launched: \nStage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 1.8 sec   HDFS Read: 238 HDFS Write: 2 SUCCESS\nTotal MapReduce CPU Time Spent: 1 seconds 800 msec\nOK\n2\nTime taken: 18.505 seconds, Fetched: 1 row(s)\n```\n\n\u554f\u984c\u306a\u304f\u4f7f\u3048\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\u6b21\u306f Spark \u3067\u3059\u3002\n\n```\nroot@3fde12cd3334:/# spark-shell \ntput: No value for $TERM and no -T specified\n16/12/04 16:00:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/12/04 16:00:59 INFO SecurityManager: Changing view acls to: root\n16/12/04 16:00:59 INFO SecurityManager: Changing modify acls to: root\n16/12/04 16:00:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)\n16/12/04 16:00:59 INFO HttpServer: Starting HTTP Server\n16/12/04 16:00:59 INFO Server: jetty-8.y.z-SNAPSHOT\n16/12/04 16:00:59 INFO AbstractConnector: Started SocketConnector@0.0.0.0:35650\n16/12/04 16:00:59 INFO Utils: Successfully started service 'HTTP class server' on port 35650.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.3.1\n      /_/\n\n(snip)\n\nscala> sc.parallelize(1 to 100).filter(_ % 2 == 0).count\n16/12/04 16:02:54 INFO SparkContext: Starting job: count at <console>:22\n16/12/04 16:02:54 INFO DAGScheduler: Got job 0 (count at <console>:22) with 2 output partitions (allowLocal=false)\n16/12/04 16:02:54 INFO DAGScheduler: Final stage: Stage 0(count at <console>:22)\n16/12/04 16:02:54 INFO DAGScheduler: Parents of final stage: List()\n16/12/04 16:02:54 INFO DAGScheduler: Missing parents: List()\n16/12/04 16:02:54 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[1] at filter at <console>:22), which has no missing parents\n16/12/04 16:02:54 INFO MemoryStore: ensureFreeSpace(1776) called with curMem=0, maxMem=278302556\n16/12/04 16:02:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1776.0 B, free 265.4 MB)\n16/12/04 16:02:54 INFO MemoryStore: ensureFreeSpace(1285) called with curMem=1776, maxMem=278302556\n16/12/04 16:02:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1285.0 B, free 265.4 MB)\n16/12/04 16:02:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fde12cd3334.bigtop.apache.org:36932 (size: 1285.0 B, free: 265.4 MB)\n16/12/04 16:02:54 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0\n16/12/04 16:02:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:839\n16/12/04 16:02:54 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MapPartitionsRDD[1] at filter at <console>:22)\n16/12/04 16:02:54 INFO YarnScheduler: Adding task set 0.0 with 2 tasks\n16/12/04 16:02:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 59ee358fd33a.bigtop.apache.org, PROCESS_LOCAL, 1260 bytes)\n16/12/04 16:02:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 479b6117c03a.bigtop.apache.org, PROCESS_LOCAL, 1317 bytes)\n16/12/04 16:02:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 479b6117c03a.bigtop.apache.org:38012 (size: 1285.0 B, free: 530.3 MB)\n16/12/04 16:02:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 59ee358fd33a.bigtop.apache.org:45933 (size: 1285.0 B, free: 530.3 MB)\n16/12/04 16:02:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 677 ms on 479b6117c03a.bigtop.apache.org (1/2)\n16/12/04 16:02:55 INFO DAGScheduler: Stage 0 (count at <console>:22) finished in 0.717 s\n16/12/04 16:02:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 714 ms on 59ee358fd33a.bigtop.apache.org (2/2)\n16/12/04 16:02:55 INFO DAGScheduler: Job 0 finished: count at <console>:22, took 0.928298 s\n16/12/04 16:02:55 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \nres0: Long = 50\n```\n\n\u3053\u3061\u3089\u3082\u554f\u984c\u306a\u3044\u3088\u3046\u3067\u3059\u3002\n\n# \u307e\u3068\u3081\n\nHadoop \u30af\u30e9\u30b9\u30bf\u3092 Docker \u30b3\u30f3\u30c6\u30ca\u4e0a\u306b\u304a\u624b\u8efd\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3001Apache Bigtop \u306e\u6a5f\u80fd\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306e\u624b\u9806\u3067\u306f\u3001Bigtop \u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b\u516c\u958b\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u304c\u3001\u4ee3\u308f\u308a\u306b\u81ea\u4f5c\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u308a\u3001\u305d\u306e\u72b6\u614b\u3067 Bigtop \u304c\u63d0\u4f9b\u3059\u308b\u30c6\u30b9\u30c8\u7fa4\u3092\u5b9f\u884c\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u8a73\u7d30\u306f `bigtop/provisioner/docker/README.md` \u306a\u3069\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\u307e\u305f\u3001Docker \u304c\u5229\u7528\u3067\u304d\u306a\u3044\u74b0\u5883\u3067\u306f\u3001VirtualBox \u4e0a\u306b Vagrant \u3067\u69cb\u7bc9\u3057\u305f VM \u4e0a\u306b Hadoop \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30b9\u30bf\u30c3\u30af\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002`bigtop-deploy/vm/vagrant-puppet-vm` \u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002`docker-hadoop.sh` \u30b3\u30de\u30f3\u30c9\u306e\u4ee3\u308f\u308a\u306b `vagrant up` \u30b3\u30de\u30f3\u30c9\u3067 VM \u3092\u8d77\u52d5\u3059\u308b\u306a\u3069\u3001\u7d30\u304b\u3044\u9055\u3044\u306f\u3042\u308a\u307e\u3059\u304c\u3001\u6982\u306d\u4e0a\u8a18\u306e\u77e5\u8b58\u304c\u6d41\u7528\u3067\u304d\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u3042\u308b `vagrantconfig.yaml` \u306e\u8a18\u8ff0\u65b9\u6cd5\u306f Docker \u7248\u306e `config_` \u30d5\u30a1\u30a4\u30eb\u7fa4\u3068\u540c\u69d8\u3067\u3059\u3002\n", "tags": ["hadoop", "Bigtop", "docker", "hive", "Spark"]}