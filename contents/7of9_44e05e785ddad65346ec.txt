{"context": "input:100 node, output:100 node\u3067\u306e\u95a2\u6570\u8fd1\u4f3c\u3092Neural network\u306b\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\n\u95a2\u9023\u3059\u308b\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u300cfunction approximation\u300d\u304c\u6319\u3052\u3089\u308c\u308b\u3002\n\u898b\u3064\u3051\u305f\u306e\u304c\u4ee5\u4e0b\u3002\nhttp://neuralnetworksanddeeplearning.com/chap4.html\n\u4ee5\u4e0b\u3001\u81ea\u5206\u304c\u8208\u5473\u3092\u6301\u3063\u305f\u90e8\u5206\u306e\u629c\u7c8b(Excerpt)\u3068\u88dc\u8db3\u8003\u5bdf\u3002\n\nuniversality\n\nThis result holds even if the function has many inputs, f=f(x1,\u2026,xm), and many outputs. \n\n...\n\nThis result tells us that neural networks have a kind of universality. No matter what function we want to compute, we know that there is a neural network which can do the job.\n\n\u3069\u3093\u306a\u95a2\u6570\u3067\u3042\u308d\u3046\u3068\u3082\u3001\u305d\u308c\u3092\u518d\u73fe\u3059\u308bNeural network\u304c\u3042\u308b\u3001\u3068\u306e\u3053\u3068\u3002\n\nWhat's more, this universality theorem holds even if we restrict our networks to have just a single layer intermediate between the input and the output neurons\n\nhidden layer\u304c\u5358\u5c64\u3067\u3082\u3053\u306euniversality\u3092\u6301\u3064\u3068\u306e\u3053\u3068\u3002\u84b8\u7559\u306e\u7d50\u679c\u5f97\u3089\u308c\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3082extremely powerful\u3068\u3082\u95a2\u9023\u3059\u308b\u304b\u3082\u3057\u308c\u306a\u3044(\u79c1\u898b)\u3002\nuniversality\u306b\u3064\u3044\u3066\u306f\u3001quite technical\u306a\u8aac\u660e\u306f\u3042\u3063\u305f\u3068\u306e\u3053\u3068 (\u4ee5\u4e0b)\u3002 \n\nFor instance, one of the original papers proving the result* did so using the Hahn-Banach theorem, the Riesz Representation theorem, and some Fourier analysis.\n\n\u8272\u3005\u306a\u904e\u7a0b\u306f\u95a2\u6570\u8a08\u7b97\u3068\u8003\u3048\u3089\u308c\u308b\uff08\u4ee5\u4e0b)\u3002\n\nAlmost any process you can imagine can be thought of as function computation. Consider the problem of naming a piece of music based on a short sample of the piece. \n\n...\n\nOr consider the problem of translating a Chinese text into English. \n\n...\n\nUniversality means that, in principle, neural networks can do all these things and many more.\n\n...\n\nThis limitation applies also to traditional universality theorems for models such as Boolean circuits.\n\n\u771f\u7406\u5024\u8868\u306e\u5b66\u7fd2\u306a\u3069\u3082\u53ef\u80fd\u3002\n167,926\u500b\u306e\u5b66\u7fd2\u3082\u304a\u305d\u3089\u304f\u53ef\u80fd(\u3068\u81ea\u5206\u3092\u596e\u3044\u7acb\u305f\u305b\u308b)\u3002\n\nTwo caveats\nhttp://ejje.weblio.jp/content/caveat\n\n\u624b\u7d9a\u304d\u5dee\u3057\u6b62\u3081\u901a\u544a\u3001\u8b66\u544a\u3001\u6ce8\u610f\n\n...\n\ntwo caveats to the informal statement \"a neural network can compute any function\".\n\n1\u3064\u76ee\u306ecaveat\u306f\u4ee5\u4e0b\u3002\n\nFirst, this doesn't mean that a network can be used to exactly compute any function. Rather, we can get an approximation that is as good as we want. \n\n...\n\nby using enough hidden neurons we can always find a neural network whose output g(x)g(x) satisfies |g(x)\u2212f(x)|<\u03f5, for all inputs x. \n\n\u5e0c\u671b\u7cbe\u5ea6\u03f5\u03f5\\epsilon\u307e\u3067\u306e\u8fd1\u4f3c\u306fhidden neuron\u3092\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u9054\u6210\u3067\u304d\u308b\u3002\n2\u3064\u76ee\u306ecaveat\u306f\u4ee5\u4e0b\u3002\n\nThe second caveat is that the class of functions which can be approximated in the way described are the continuous functions.\n\n\u9023\u7d9a\u95a2\u6570\u3067\u306a\u3044\u3068\u8fd1\u4f3c\u3067\u304d\u306a\u3044\u3002\nStep function\u306e\u3088\u3046\u306a\u5834\u5408\u3001Gibbs phenomenon\u306e\u3088\u3046\u306b\u306a\u308b\u306e\u3060\u308d\u3046\u304b\u3002\n...\n\nHowever, even if the function we'd really like to compute is discontinuous, it's often the case that a continuous approximation is good enough.  ... In practice, this is not usually an important limitation.\n\n...\n\nSumming up, a more precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.\n\n...\n\nIn the diagram below, click on the weight, ww, and drag the mouse a little ways to the right to increase w.\n\n\u3069\u3046\u3044\u3046\u4ed5\u639b\u3051\u304b\u308f\u304b\u3089\u306a\u3044\u304c\u3001weight\u3068bias\u3092\u5909\u66f4\u3057\u306a\u304c\u3089\u305d\u306e\u51fa\u529b\u95a2\u6570\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u3066\u9762\u767d\u3044\u3002\n\nYou'll see that as the bias increases the graph moves to the left, but its shape doesn't change.\n\n...\n\nYou'll see that as you decrease the weight, the curve broadens out.\n\n...\n\nFinally, increase the weight up past w=100. As you do, the curve gets steeper, until eventually it begins to look like a step function. \n\n...\n\nIt's actually quite a bit easier to work with step functions than general sigmoid functions.\n\n...\n\nAnd so it makes things much easier to assume that our hidden neurons are outputting step functions. \n\nsigmod function\u3067\u306a\u304fstep function\u3067\u3042\u308b\u3068\u4eee\u5b9a\u3057\u3066\u8003\u5bdf\u3057\u3066\u3044\u308b\u3002\n\nthe position of the step is proportional to b, and inversely proportional to w.\nIn fact, the step is at position s=\u2212b/w\n\nstep function\u306e\u5024\u304c\u5909\u5316\u3059\u308b\u90e8\u5206(\u30d1\u30e9\u30e1\u30fc\u30bfs)\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u3044\u308b\u3002w\u3068b\u306b\u3088\u308b\u51fa\u529b\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u304b\u3089s\u306b\u3088\u308b\u51fa\u529b\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u3092\u95b2\u89a7\u8005\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\nUp to now we've been focusing on the output from just the top hidden neuron. Let's take a look at the behavior of the entire network.\n\nhidden layer\u306e1\u3064\u306eneuron\u306e\u64cd\u4f5c\u304b\u3089\u3001hidden layer\u306e\u5168\u90e8\u306eneuron\u306es\u5024\u3068\u3001weight\u306e\u64cd\u4f5c\u306b\u3088\u308b\u51fa\u529b\u5909\u5316\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\nWhat's being plotted on the right is the weighted output w1a1+w2a2 from the hidden layer. ... These outputs are denoted with aas because they're often known as the neurons' activations.\n\n...\n\nFinally, try setting w1 to be 0.8 and w2 to be \u22120.8. You get a \"bump\" function\n\nbump function: \u30b0\u30e9\u30d5\u306e\u9014\u4e2d\u3067step up\u3057\u3066\u3001\u305d\u306e\u5f8cstep down\u3059\u308b\u3088\u3046\u306a\u5f62\u3002\n\nWe can use our bump-making trick to get two bumps, by gluing two pairs of hidden neurons together into the same network:\n\nhidden neurons\u3092\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u3001bump\u306e\u6570\u3082\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u305d\u3057\u3066\u305d\u308c\u306f\u95a2\u6570\u306e\u8fd1\u4f3c\u306b\u8ca2\u732e\u3057\u3066\u3044\u304f\u3068\u3044\u3046\u611f\u3058\u3002\n\nContrariwise, try clicking on the graph, and dragging up or down to change the height of any of the bump functions. \n\n\u30b0\u30e9\u30d5\u81ea\u4f53\u3092\u64cd\u4f5c\u3059\u308b\u3053\u3068\u3067\u3001h\u5024\u306e\u5909\u5316\u3092\u898b\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002\n\nWhat's output from the network is \u03c3(\u2211jwjaj+b) where bb is the bias on the output neuron. Is there some way we can achieve control over the actual output from the network?\nThe solution is to design a neural network whose hidden layer has a weighted output given by \u03c3\u22121\u2218f(x), where \u03c3\u22121 is just the inverse of the \u03c3\u03c3 function.\n\n\u03c3\u22121\u306f\u03c3\u22121\u03c3\u22121\\sigma^{-1}\u3002(\u30ea\u30f3\u30af\u5143\u30da\u30fc\u30b8\u3067\u306f\u304d\u3061\u3093\u3068\u8868\u793a\u3055\u308c\u3066\u3044\u308b\u3002\u3053\u3061\u3089\u3067\u306flatex\u8868\u8a18\u3067\u66f8\u304d\u306a\u304a\u3059\u6642\u9593\u7701\u7565\u306e\u305f\u3081\u624b\u629c\u304d)\u3002\n\nHow well you're doing is measured by the average deviation between the goal function and the function the network is actually computing.\n\n\u30b0\u30e9\u30d5\u306b\u3066bumps\u3092\u64cd\u4f5c\u3057\u3066\u3001\u5b9f\u969b\u306eAverage deviation\u5024\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\nIt's only a coarse approximation, but we could easily do much better, merely by increasing the number of pairs of hidden neurons, allowing more bumps.\n\nhidden neurons\u306e\u30da\u30a2\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3001\u7cbe\u5ea6\u3092\u9ad8\u3081\u3089\u308c\u308b\u3002\n\nWe could have used this procedure for any continuous function from [0,1] to [0,1].\n\n\u7bc4\u56f2\u306b\u3064\u3044\u3066[0,1]\u306e\u5236\u7d04\u304c\u3042\u308b\u306e\u304b\u306f\u672a\u6d88\u5316\u3002\n(sigmoid function\u524d\u63d0\u306e\u8a71\u306e\u305f\u3081\u306e\u3088\u3046\u3060)\n\nMany input variables\n\nSo let's address the two-input case.\n\n2\u5165\u529b\u306e\u5834\u5408\u306b\u62e1\u5f35\u3057\u3066\u3044\u308b\u3002\n\nThe difference is that now the step function is in three dimensions. \n\n2\u3064\u306einput neurons\u306b\u306a\u3063\u305f\u5834\u5408\u30012\u6b21\u5143(x,y)\u65b9\u5411\u305d\u308c\u305e\u308c\u306estep function\u3092\u64cd\u4f5c\u3059\u308b\u3088\u3046\u306a\u3053\u3068\u306b\u306a\u308b\u3068\u56f3\u793a\u3055\u308c\u3066\u3044\u308b\u3002\n\nWe can use the step functions we've just constructed to compute a three-dimensional bump function. \n\nbump function\u3082x,y\u8ef8\u305d\u308c\u305e\u308c\u306b\u306a\u308a\u3001tower function\u306b\u306a\u308b\uff08\u4ee5\u4e0b\uff09\u3002\n\nWhat we've built looks a little like a tower function:\n\n...\n\nthen we could use them to approximate arbitrary functions, just by adding up many towers of different heights, and in different locations:\n\n...\n\nLet's try gluing two such networks together, in order to compute two different tower functions. \n\ntower function\u81ea\u4f53\u3092\u8907\u6570\u6301\u3064\u3053\u3068\u3082\u53ef\u80fd\u3002\n\nIn particular, you can see that by modifying the weights in the final layer you can change the height of the output towers.\n\n...\n\nWhat about functions of more than two variables?\n\n...\n\nBy gluing together many such networks we can get as many towers as we want, and so approximate an arbitrary function of three variables. Exactly the same idea works in mm dimensions. The only change needed is to make the output bias (\u2212m+1/2)h, in order to get the right kind of sandwiching behavior to level the plateau.\n\n(-m + 1/2)h\u306e\u5236\u7d04\u306b\u3064\u3044\u3066\u306f\u672a\u6d88\u5316\u3002\n\nProblem\nsingle hidden layer\u3067\u4efb\u610f\u306e\u95a2\u6570\u3092\u8fd1\u4f3c\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\n\nto approximate a tower function which is circular in shape\n\ninput neuron\u306e\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3001rectangular in shape\u304b\u3089 circular in shape\u306b\u306a\u308b\u3068\u3044\u3046\u3053\u3068\u3060\u308d\u3046\u3002\n\nExtension beyond sigmoid neurons\n\u3053\u308c\u307e\u3067\u306e\u8b70\u8ad6\u306fsigmoid function\u306b\u57fa\u3065\u3044\u3066\u3044\u305f\u3002\u5225\u306e\u95a2\u6570\u3092\u4f7f\u3046\u3068\u3069\u3046\u304b\u3068\u3044\u3046\u3068\n\nand ultimately it becomes a very good approximation to a step function. \n\n...\n\nWhat properties does s(z)s(z) need to satisfy in order for this to work? We do need to assume that s(z)s(z) is well-defined as z\u2192\u2212\u221e and z\u2192\u221e. ... We also need to assume that these limits are different from one another. \n\n\nProblems\n\nEarlier in the book we met another type of neuron known as a rectified linear unit. Explain why such neurons don't satisfy the conditions just given for universality.\n\nReLU\u306funiversality\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3055\u306a\u3044\u3068\u306e\u3053\u3068\u3089\u3057\u3044\u3002\n\nFixing up the step functions\n\nUp to now, we've been assuming that our neurons can produce step functions exactly. \n\n...\n\na narrow window of failure, \n\n\u5b8c\u5168\u306astep function\u3067\u306f\u306a\u304f\u3001\u5024\u304c\u6025\u5cfb\u306b\u5909\u5316\u3059\u308b\u90e8\u5206\u306ba narrow window of failure\u304c\u3042\u308b\u3002\n\nNow, it's not a terrible failure. By making the weights input to the neurons big enough we can make these windows of failure as small as we like. \n\n...\n\npoints in a failure window for one approximation won't be in a failure window for the other.\n\nstep function\u3092\u8907\u6570\u4f7f\u3063\u3066overlap\u3055\u305b\u305f\u5834\u5408\u30011\u3064\u306efunction\u306efailure window\u306f\u4ed6\u306e\u3082\u306e\u306efailure window\u306b\u306a\u3089\u306a\u3044\u3001\u306a\u3069\u306e\u305f\u3081\u5f71\u97ff\u306f\u5c0f\u3055\u3044\u3068\u8457\u8005\u306f\u898b\u3066\u3044\u308b\u3088\u3046\u3060 (\u4ee5\u4e0b)\u3002\n\nWe could do even better by adding up a large number, MM, of overlapping approximations to the function \u03c3\u22121\u2218f(x)/M.\n\n\nConclusion\n\nSo the right question to ask is not whether any particular function is computable, but rather what's a good way to compute the function.\n\n...\n\nGiven this, you might wonder why we would ever be interested in deep networks, i.e., networks with many hidden layers. Can't we simply replace those networks with shallow, single hidden layer networks?\n\n...\n\nIn later chapters, we'll see evidence suggesting that deep networks do a better job than shallow networks at learning such hierarchies of knowledge.\n\n[input:100 node, output:100 node\u3067\u306e\u95a2\u6570\u8fd1\u4f3c\u3092Neural network\u306b\u5b66\u7fd2\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u3044\u308b](http://qiita.com/7of9/items/7394f4326a88500a67b9)\u3002\n\u95a2\u9023\u3059\u308b\u30ad\u30fc\u30ef\u30fc\u30c9\u3068\u3057\u3066\u300cfunction approximation\u300d\u304c\u6319\u3052\u3089\u308c\u308b\u3002\n\n\u898b\u3064\u3051\u305f\u306e\u304c\u4ee5\u4e0b\u3002\nhttp://neuralnetworksanddeeplearning.com/chap4.html\n\n\u4ee5\u4e0b\u3001\u81ea\u5206\u304c\u8208\u5473\u3092\u6301\u3063\u305f\u90e8\u5206\u306e[\u629c\u7c8b(Excerpt)](http://ejje.weblio.jp/content/%E6%8A%9C%E7%B2%8B)\u3068\u88dc\u8db3\u8003\u5bdf\u3002\n\n### universality\n\n> This result holds even if the function has many inputs, f=f(x1,\u2026,xm), and many outputs. \n\n...\n\n> This result tells us that neural networks have a kind of universality. No matter what function we want to compute, we know that there is a neural network which can do the job.\n\n\u3069\u3093\u306a\u95a2\u6570\u3067\u3042\u308d\u3046\u3068\u3082\u3001\u305d\u308c\u3092\u518d\u73fe\u3059\u308bNeural network\u304c\u3042\u308b\u3001\u3068\u306e\u3053\u3068\u3002\n\n> What's more, this universality theorem holds even if we restrict our networks to have just a single layer intermediate between the input and the output neurons\n\nhidden layer\u304c\u5358\u5c64\u3067\u3082\u3053\u306euniversality\u3092\u6301\u3064\u3068\u306e\u3053\u3068\u3002[\u84b8\u7559](http://qiita.com/HirofumiYashima/items/fc220b66d16b2d2078c4)\u306e\u7d50\u679c\u5f97\u3089\u308c\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3082extremely powerful\u3068\u3082\u95a2\u9023\u3059\u308b\u304b\u3082\u3057\u308c\u306a\u3044(\u79c1\u898b)\u3002\n\nuniversality\u306b\u3064\u3044\u3066\u306f\u3001quite technical\u306a\u8aac\u660e\u306f\u3042\u3063\u305f\u3068\u306e\u3053\u3068 (\u4ee5\u4e0b)\u3002 \n\n> For instance, one of the original papers proving the result* did so using the Hahn-Banach theorem, the Riesz Representation theorem, and some Fourier analysis.\n\n\u8272\u3005\u306a\u904e\u7a0b\u306f\u95a2\u6570\u8a08\u7b97\u3068\u8003\u3048\u3089\u308c\u308b\uff08\u4ee5\u4e0b)\u3002\n\n> Almost any process you can imagine can be thought of as function computation. Consider the problem of naming a piece of music based on a short sample of the piece. \n\n...\n\n> Or consider the problem of translating a Chinese text into English. \n\n\n...\n\n> Universality means that, in principle, neural networks can do all these things and many more.\n\n...\n\n> This limitation applies also to traditional universality theorems for models such as Boolean circuits.\n\n\u771f\u7406\u5024\u8868\u306e\u5b66\u7fd2\u306a\u3069\u3082\u53ef\u80fd\u3002\n[167,926\u500b\u306e\u5b66\u7fd2](http://qiita.com/7of9/items/6fa9552340041ee534d9)\u3082\u304a\u305d\u3089\u304f\u53ef\u80fd(\u3068\u81ea\u5206\u3092\u596e\u3044\u7acb\u305f\u305b\u308b)\u3002\n\n### Two caveats\n\nhttp://ejje.weblio.jp/content/caveat\n> \u624b\u7d9a\u304d\u5dee\u3057\u6b62\u3081\u901a\u544a\u3001\u8b66\u544a\u3001\u6ce8\u610f\n\n...\n\n> two caveats to the informal statement \"a neural network can compute any function\".\n\n1\u3064\u76ee\u306ecaveat\u306f\u4ee5\u4e0b\u3002\n\n> First, this doesn't mean that a network can be used to exactly compute any function. Rather, we can get an approximation that is as good as we want. \n\n...\n\n> by using enough hidden neurons we can always find a neural network whose output g(x)g(x) satisfies |g(x)\u2212f(x)|<\u03f5, for all inputs x. \n\n\u5e0c\u671b\u7cbe\u5ea6$\\epsilon$\u307e\u3067\u306e\u8fd1\u4f3c\u306fhidden neuron\u3092\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u9054\u6210\u3067\u304d\u308b\u3002\n\n2\u3064\u76ee\u306ecaveat\u306f\u4ee5\u4e0b\u3002\n\n> The second caveat is that the class of functions which can be approximated in the way described are the continuous functions.\n\n\u9023\u7d9a\u95a2\u6570\u3067\u306a\u3044\u3068\u8fd1\u4f3c\u3067\u304d\u306a\u3044\u3002\nStep function\u306e\u3088\u3046\u306a\u5834\u5408\u3001[Gibbs phenomenon](https://en.wikipedia.org/wiki/Gibbs_phenomenon)\u306e\u3088\u3046\u306b\u306a\u308b\u306e\u3060\u308d\u3046\u304b\u3002\n\n...\n\n> However, even if the function we'd really like to compute is discontinuous, it's often the case that a continuous approximation is good enough.  ... In practice, this is not usually an important limitation.\n\n...\n\n> Summing up, a more precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.\n\n...\n\n> In the diagram below, click on the weight, ww, and drag the mouse a little ways to the right to increase w.\n\n\u3069\u3046\u3044\u3046\u4ed5\u639b\u3051\u304b\u308f\u304b\u3089\u306a\u3044\u304c\u3001weight\u3068bias\u3092\u5909\u66f4\u3057\u306a\u304c\u3089\u305d\u306e\u51fa\u529b\u95a2\u6570\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u3066\u9762\u767d\u3044\u3002\n\n> You'll see that as the bias increases the graph moves to the left, but its shape doesn't change.\n\n...\n>  You'll see that as you decrease the weight, the curve broadens out.\n\n...\n> Finally, increase the weight up past w=100. As you do, the curve gets steeper, until eventually it begins to look like a step function. \n\n...\n> It's actually quite a bit easier to work with step functions than general sigmoid functions.\n\n...\n> And so it makes things much easier to assume that our hidden neurons are outputting step functions. \n\nsigmod function\u3067\u306a\u304fstep function\u3067\u3042\u308b\u3068\u4eee\u5b9a\u3057\u3066\u8003\u5bdf\u3057\u3066\u3044\u308b\u3002\n\n> the position of the step is proportional to b, and inversely proportional to w.\n> In fact, the step is at position s=\u2212b/w\n\nstep function\u306e\u5024\u304c\u5909\u5316\u3059\u308b\u90e8\u5206(\u30d1\u30e9\u30e1\u30fc\u30bfs)\u306b\u3064\u3044\u3066\u8003\u5bdf\u3057\u3066\u3044\u308b\u3002w\u3068b\u306b\u3088\u308b\u51fa\u529b\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u304b\u3089s\u306b\u3088\u308b\u51fa\u529b\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u3092\u95b2\u89a7\u8005\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\n> Up to now we've been focusing on the output from just the top hidden neuron. Let's take a look at the behavior of the entire network.\n\nhidden layer\u306e1\u3064\u306eneuron\u306e\u64cd\u4f5c\u304b\u3089\u3001hidden layer\u306e\u5168\u90e8\u306eneuron\u306es\u5024\u3068\u3001weight\u306e\u64cd\u4f5c\u306b\u3088\u308b\u51fa\u529b\u5909\u5316\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\n> What's being plotted on the right is the weighted output w1a1+w2a2 from the hidden layer. ... These outputs are denoted with aas because they're often known as the neurons' activations.\n\n...\n\n> Finally, try setting w1 to be 0.8 and w2 to be \u22120.8. You get a \"bump\" function\n\nbump function: \u30b0\u30e9\u30d5\u306e\u9014\u4e2d\u3067step up\u3057\u3066\u3001\u305d\u306e\u5f8cstep down\u3059\u308b\u3088\u3046\u306a\u5f62\u3002\n\n> We can use our bump-making trick to get two bumps, by gluing two pairs of hidden neurons together into the same network:\n\nhidden neurons\u3092\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u3001bump\u306e\u6570\u3082\u5897\u3084\u3057\u3066\u3044\u304f\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u305d\u3057\u3066\u305d\u308c\u306f\u95a2\u6570\u306e\u8fd1\u4f3c\u306b\u8ca2\u732e\u3057\u3066\u3044\u304f\u3068\u3044\u3046\u611f\u3058\u3002\n\n\n> Contrariwise, try clicking on the graph, and dragging up or down to change the height of any of the bump functions. \n\n\u30b0\u30e9\u30d5\u81ea\u4f53\u3092\u64cd\u4f5c\u3059\u308b\u3053\u3068\u3067\u3001h\u5024\u306e\u5909\u5316\u3092\u898b\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002\n\n> What's output from the network is \u03c3(\u2211jwjaj+b) where bb is the bias on the output neuron. Is there some way we can achieve control over the actual output from the network?\n\n> The solution is to design a neural network whose hidden layer has a weighted output given by \u03c3\u22121\u2218f(x), where \u03c3\u22121 is just the inverse of the \u03c3\u03c3 function.\n\n\u03c3\u22121\u306f$\\sigma^{-1}$\u3002(\u30ea\u30f3\u30af\u5143\u30da\u30fc\u30b8\u3067\u306f\u304d\u3061\u3093\u3068\u8868\u793a\u3055\u308c\u3066\u3044\u308b\u3002\u3053\u3061\u3089\u3067\u306flatex\u8868\u8a18\u3067\u66f8\u304d\u306a\u304a\u3059\u6642\u9593\u7701\u7565\u306e\u305f\u3081\u624b\u629c\u304d)\u3002\n\n> How well you're doing is measured by the average deviation between the goal function and the function the network is actually computing.\n\n\u30b0\u30e9\u30d5\u306b\u3066bumps\u3092\u64cd\u4f5c\u3057\u3066\u3001\u5b9f\u969b\u306eAverage deviation\u5024\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\n> It's only a coarse approximation, but we could easily do much better, merely by increasing the number of pairs of hidden neurons, allowing more bumps.\n\nhidden neurons\u306e\u30da\u30a2\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3001\u7cbe\u5ea6\u3092\u9ad8\u3081\u3089\u308c\u308b\u3002\n\n> We could have used this procedure for any continuous function from [0,1] to [0,1].\n\n~~\u7bc4\u56f2\u306b\u3064\u3044\u3066[0,1]\u306e\u5236\u7d04\u304c\u3042\u308b\u306e\u304b\u306f\u672a\u6d88\u5316\u3002~~\n(sigmoid function\u524d\u63d0\u306e\u8a71\u306e\u305f\u3081\u306e\u3088\u3046\u3060)\n\n### Many input variables\n\n> So let's address the two-input case.\n\n2\u5165\u529b\u306e\u5834\u5408\u306b\u62e1\u5f35\u3057\u3066\u3044\u308b\u3002\n\n> The difference is that now the step function is in three dimensions. \n\n2\u3064\u306einput neurons\u306b\u306a\u3063\u305f\u5834\u5408\u30012\u6b21\u5143(x,y)\u65b9\u5411\u305d\u308c\u305e\u308c\u306estep function\u3092\u64cd\u4f5c\u3059\u308b\u3088\u3046\u306a\u3053\u3068\u306b\u306a\u308b\u3068\u56f3\u793a\u3055\u308c\u3066\u3044\u308b\u3002\n\n> We can use the step functions we've just constructed to compute a three-dimensional bump function. \n\nbump function\u3082x,y\u8ef8\u305d\u308c\u305e\u308c\u306b\u306a\u308a\u3001tower function\u306b\u306a\u308b\uff08\u4ee5\u4e0b\uff09\u3002\n\n> What we've built looks a little like a tower function:\n\n...\n\n> then we could use them to approximate arbitrary functions, just by adding up many towers of different heights, and in different locations:\n\n...\n\n> Let's try gluing two such networks together, in order to compute two different tower functions. \n\ntower function\u81ea\u4f53\u3092\u8907\u6570\u6301\u3064\u3053\u3068\u3082\u53ef\u80fd\u3002\n\n> In particular, you can see that by modifying the weights in the final layer you can change the height of the output towers.\n\n...\n\n> What about functions of more than two variables?\n\n...\n\n> **By gluing together many such networks we can get as many towers as we want, and so approximate an arbitrary function of three variables. Exactly the same idea works in mm dimensions. The only change needed is to make the output bias (\u2212m+1/2)h, in order to get the right kind of sandwiching behavior to level the plateau.**\n\n(-m + 1/2)h\u306e\u5236\u7d04\u306b\u3064\u3044\u3066\u306f\u672a\u6d88\u5316\u3002\n\n### Problem\n\nsingle hidden layer\u3067\u4efb\u610f\u306e\u95a2\u6570\u3092\u8fd1\u4f3c\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\u3002\n\n> to approximate a tower function which is circular in shape\n\ninput neuron\u306e\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3001rectangular in shape\u304b\u3089 circular in shape\u306b\u306a\u308b\u3068\u3044\u3046\u3053\u3068\u3060\u308d\u3046\u3002\n\n### Extension beyond sigmoid neurons\n\n\u3053\u308c\u307e\u3067\u306e\u8b70\u8ad6\u306fsigmoid function\u306b\u57fa\u3065\u3044\u3066\u3044\u305f\u3002\u5225\u306e\u95a2\u6570\u3092\u4f7f\u3046\u3068\u3069\u3046\u304b\u3068\u3044\u3046\u3068\n\n> and ultimately it becomes a very good approximation to a step function. \n\n...\n\n> What properties does s(z)s(z) need to satisfy in order for this to work? We do need to assume that s(z)s(z) is well-defined as z\u2192\u2212\u221e and z\u2192\u221e. ... We also need to assume that these limits are different from one another. \n\n### Problems\n\n> Earlier in the book we met another type of neuron known as a rectified linear unit. Explain why such neurons don't satisfy the conditions just given for universality.\n\nReLU\u306funiversality\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3055\u306a\u3044\u3068\u306e\u3053\u3068\u3089\u3057\u3044\u3002\n\n\n### Fixing up the step functions\n\n> Up to now, we've been assuming that our neurons can produce step functions exactly. \n\n...\n\n> a narrow window of failure, \n\n\u5b8c\u5168\u306astep function\u3067\u306f\u306a\u304f\u3001\u5024\u304c\u6025\u5cfb\u306b\u5909\u5316\u3059\u308b\u90e8\u5206\u306ba narrow window of failure\u304c\u3042\u308b\u3002\n\n> Now, it's not a terrible failure. By making the weights input to the neurons big enough we can make these windows of failure as small as we like. \n\n...\n\n>  points in a failure window for one approximation won't be in a failure window for the other.\n\nstep function\u3092\u8907\u6570\u4f7f\u3063\u3066overlap\u3055\u305b\u305f\u5834\u5408\u30011\u3064\u306efunction\u306efailure window\u306f\u4ed6\u306e\u3082\u306e\u306efailure window\u306b\u306a\u3089\u306a\u3044\u3001\u306a\u3069\u306e\u305f\u3081\u5f71\u97ff\u306f\u5c0f\u3055\u3044\u3068\u8457\u8005\u306f\u898b\u3066\u3044\u308b\u3088\u3046\u3060 (\u4ee5\u4e0b)\u3002\n\n> We could do even better by adding up a large number, MM, of overlapping approximations to the function \u03c3\u22121\u2218f(x)/M.\n\n\n### Conclusion\n\n> So the right question to ask is not whether any particular function is computable, but rather what's a good way to compute the function.\n\n...\n\n> Given this, you might wonder why we would ever be interested in deep networks, i.e., networks with many hidden layers. Can't we simply replace those networks with shallow, single hidden layer networks?\n\n...\n\n> In later chapters, we'll see evidence suggesting that deep networks do a better job than shallow networks at learning such hierarchies of knowledge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "tags": ["link", "borgWarp"]}