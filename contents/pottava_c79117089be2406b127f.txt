{"context": "\n\n\u66f4\u65b0\u5c65\u6b74\n\n[2017/03/08] AWS \u516c\u5f0f Deep Learning AMI v2 \u30ea\u30ea\u30fc\u30b9\uff01\nCUDA 8.0 \u306b\u306a\u308a\u3001TensorFlow v1.0.0, MXNet v0.9.3 \u306a\u3069\n\n\nDeep Learning \u306b\u4fbf\u5229\u306a AMI\n\nNVIDIA \u63d0\u4f9b\n\n\nNVIDIA CUDA Toolkit 7.5 on Amazon Linux\nNVIDIA DIGITS 4 on Ubuntu 14.04\n\n2016/09 \u672b\u306b\u516c\u958b\u3055\u308c\u305f\u6a21\u69d8\u3002\n\u81ea\u5206\u3067 CUDA \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3053\u3068\u304c\u3042\u308b\u65b9\u306a\u3089\u308f\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001NVIDIA \u306e GPU \u30c9\u30e9\u30a4\u30d0\u304b\u3089\u59cb\u307e\u308a\u3001CUDA \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306e\u306f\u306a\u304b\u306a\u304b\u306e\u624b\u9593\u3067\u3057\u305f\u3002\uff08\u307e\u3042\u3001\u4e00\u5ea6 AMI \u3092\u4f5c\u308c\u3070\u3063\u3066\u306e\u306f\u3042\u308a\u307e\u3059\u304c\uff09\n\u3053\u306e AMI \u3092\u4f7f\u3048\u3070\u3001\u3082\u306e\u3059\u3054\u304f\u7c21\u5358\u306b\u30b5\u30fc\u30d0\u304c\u4f7f\u3044\u59cb\u3081\u3089\u308c\u307e\u3059\uff01\n\nAWS \u63d0\u4f9b\n\nDeep Learning AMI Amazon Linux Version\nDeep Learning AMI Ubuntu Version\n\nMXNet\u3001Caffe\u3001Tensorflow\u3001Theano \u305d\u3057\u3066 Torch \u306a\u3069\u4eba\u6c17\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30fb\u8a2d\u5b9a\u30fb\u30c6\u30b9\u30c8\u3055\u308c\u305f AMI\u3002\u305f\u3060\u3057\u3001\u3053\u3061\u3089\u306b\u306f CUDA \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u304b\u3064\u6771\u4eac\u30ea\u30fc\u30b8\u30e7\u30f3\u306b\u306f\u914d\u5e03\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u30fb\u30fb\u3002\uff08\u8ffd\u8a18\uff1a\u6700\u65b0\u7248\u306f CUDA \u5165\u3063\u3066\u3044\u307e\u3059\uff01\u3042\u308a\u304c\u305f\u3044\uff01\uff01\uff09\n\uff08\u304a\u8a66\u3057\u7528\u30c6\u30b9\u30c8\u30b5\u30f3\u30d7\u30eb\u304c\u5165\u3063\u3066\u3044\u305f\u308a\u3059\u308b\u3088\u3046\u3067\u3059\uff09\n[ec2-user@ip-10-0-0-10 ~]$ ls -la /home/ec2-user/src/bin\n\u5408\u8a08 40\ndrwxrwxr-x  2 ec2-user ec2-user 4096  9\u6708 20 00:48 .\ndrwxrwxr-x 14 ec2-user ec2-user 4096  9\u6708 28 20:21 ..\n-rwxr-xr-x  1 ec2-user ec2-user  350  9\u6708 20 00:12 testAll\n-rwxr-xr-x  1 ec2-user ec2-user 1207  9\u6708 20 00:12 testCaffe\n-rwxr-xr-x  1 ec2-user ec2-user 1141  9\u6708 20 00:12 testMXNet\n-rwxr-xr-x  1 ec2-user ec2-user   70  9\u6708 20 00:13 testTensorFlow\n-rwxr-xr-x  1 ec2-user ec2-user   66  9\u6708 20 00:13 testTheano\n-rwxr-xr-x  1 ec2-user ec2-user 1058  9\u6708 20 00:13 testTheanoOrTensorFlow\n-rwxr-xr-x  1 ec2-user ec2-user  861  9\u6708 20 00:13 testTorch\n-rwxr-xr-x  1 ec2-user ec2-user  210  9\u6708 20 00:13 testUtil\n\n\n\u8d77\u52d5\u3057\u3066\u307f\u308b\nNVIDIA CUDA Toolkit 7.5 on Amazon Linux \u3092\u4f7f\u3044\u3001\u4e2d\u8eab\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u8d77\u52d5\u624b\u9806\n\n1. \u30de\u30fc\u30b1\u30c3\u30c8\u30d7\u30ec\u30a4\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u627f\u8afe\n\u9069\u5207\u306a\u30ea\u30fc\u30b8\u30e7\u30f3\u3092\u9078\u629e\u3057 continue \u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\n\n\u30b9\u30dd\u30c3\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f7f\u3044\u305f\u3044\u306e\u3067\u3001\u30de\u30cb\u30e5\u30a2\u30eb\u3067\u306e\u8d77\u52d5\u3092\u9078\u629e\u3002\u753b\u9762\u306e\u5de6\u4e0b\u306b\u3042\u308b AMI \u306e ID \u3092\u63a7\u3048\u3064\u3064\u3001\uff08\u4eca\u56de\u306e AMI \u306f\u3044\u307e\u306e\u3068\u3053\u308d\u7121\u6599\u3067\u3059\u304c\uff09\u8d77\u52d5\u6642\u9593\u3042\u305f\u308a\u306e\u5229\u7528\u6599\u3092\u78ba\u8a8d\u3057\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u898f\u7d04\u306b\u5f93\u3046\u3053\u3068\u3092\u627f\u8a8d\u3057\u307e\u3059\u3002\n\n\n2. EC2 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u8d77\u52d5\n\u30b9\u30dd\u30c3\u30c8\u6599\u91d1\u3092\u8a2d\u5b9a\u3057\u3064\u3064 g2.2xlarge \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n\n\u30b9\u30dd\u30c3\u30c8\u306e\u91d1\u984d\u304c\u5e02\u5834\u3092\u4e0b\u56de\u308b\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n3. EC2 \u306b SSH\n\u8d77\u52d5\u3057\u305f\u3089\u30b5\u30fc\u30d0\u306b\u5165\u3063\u3066\u307f\u307e\u3059\u3002\nssh -i ec2.pem ec2-user@x.x.x.x\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u30c9\u30e9\u30a4\u30d0\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ nvcc -V\n\n-bash: nvcc: \u30b3\u30de\u30f3\u30c9\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\n\n\n\u306a\u308b\u307b\u3069\u3002\n\n\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ /usr/local/cuda/bin/nvcc -V\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n\n\n\u306f\u3044\u3001\u3057\u3063\u304b\u308a 7.5 \u3067\u3059\u306d\u3002AMI \u7d20\u6674\u3089\u3057\u3044\uff01\uff01\n\nNVIDIA-docker \u3082\u5165\u308c\u3068\u3053\u3046\n\n1. docker \u3092\u5165\u308c\u3066\n\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u306b\u306f\u5f93\u308f\u305a1\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067 Docker \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\nsudo yum install -y docker\nsudo service docker start\nsudo usermod -aG docker ec2-user\n\n\u4ee5\u5f8c ec2-user \u3067\u3082 docker \u30b3\u30de\u30f3\u30c9\u304c\u4f7f\u3048\u308b\u3088\u3046\u306b\u3001\u3044\u3063\u305f\u3093 SSH \u304b\u3089\u629c\u3051\u3066\u5165\u308a\u306a\u304a\u3057\u307e\u3059\u3002\n\n2. NVIDIA-docker \u3082\u5165\u308c\u308b\n\u6b8b\u5ff5\u306a\u304c\u3089 1.0.0 \u6642\u70b9\u3067\u306f\u3001\u914d\u5e03\u3055\u308c\u3066\u3044\u308b rpm \u306e\u4f9d\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u304c AmazonLinux \u3060\u3068\u30a2\u30ec\u306a\u3082\u306e\u3067\u3001\u30d0\u30a4\u30ca\u30ea\u3092\u6301\u3063\u3066\u304d\u3066\u7f6e\u304f\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\nnvidia-docker\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0/nvidia-docker_1.0.0_amd64.tar.xz\nsudo tar --strip-components=1 -C /usr/bin -xvf /tmp/nvidia-docker*.tar.xz && rm /tmp/nvidia-docker*.tar.xz\n\n\n\n3. nvidia-docker-plugin \u306e\u8d77\u52d5\n\nnvidia-docker-plugin\u306e\u8d77\u52d5\nsudo mkdir -p /run/docker/plugins/\nsudo -b nohup nvidia-docker-plugin > /tmp/nvidia-docker.log\n\n\n\u3082\u3057\u5f8c\u304b\u3089\u30b5\u30fc\u30d0\u3092\u518d\u8d77\u52d5\u3059\u308b / AMI \u3092\u3068\u308b\u3064\u3082\u308a\u3067\u3042\u308c\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u7528\u610f\u3057\u3066\u304a\u304f\u3068\u3088\u3044\u3067\u3059\u3002\n\nnvidia-docker-plugin\u306einit\u30b9\u30af\u30ea\u30d7\u30c8\u8a2d\u7f6e\nsudo sh -c \"cat << EOF > /etc/init.d/nvdocker-plugin\n#!/bin/sh\n# chkconfig: 345 98 20\n# description: NVIDIA-docker plugin\nnvidia-docker-plugin > /tmp/nvidia-docker.log\nEOF\"\nsudo chmod +x /etc/init.d/nvdocker-plugin\nsudo chkconfig --add nvdocker-plugin\n\n\nplugin \u304c\u8d77\u52d5\u3057\u305f\u3089\u3001REST API \u306e\u758e\u901a\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n\nAPI\u8d77\u52d5\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ curl 127.0.0.1:3476/v1.0/docker/cli\n\n--volume-driver=nvidia-docker --volume=nvidia_driver_352.99:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia0\n\n\n\n4. \u8d77\u52d5\u78ba\u8a8d\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f NVIDIA-docker \u3067 nvidia-smi \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\nDocker\u3092\u4f7f\u3063\u305fnvidia-smi\n[ec2-user@ip-10-0-0-10 ~]$ nvidia-docker run --rm nvidia/cuda:7.5 nvidia-smi\n\nTue Oct 11 00:38:26 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.99     Driver Version: 352.99         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\n| N/A   25C    P8    17W / 125W |     11MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\u3044\u3044\u611f\u30582\u3067\u3059\u306d\u3002\n\n\u691c\u8a3c\n\n\u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\nGPU \u3092\u63a8\u8ad6\u306b\u4f7f\u3044\u3001HTTP \u30d7\u30ed\u30c8\u30b3\u30eb\u3092\u8a71\u3059 API \u30b5\u30fc\u30d0\u3092\u4f7f\u3063\u3066\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3092\u3068\u3063\u3066\u307f\u307e\u3059\u3002\n\n1. GRE \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nNVIDIA \u793e\u304c\u30c7\u30e2\u7528\u306b\u4f5c\u3063\u305f\u3001Caffe \u3067\u306e\u63a8\u8ad6\u3092 REST API \u5316\u3057\u305f GPU REST Engine3 \u3068\u3044\u3046\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f7f\u3044\u307e\u30594\u3002\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u63a8\u8ad6\u30b5\u30fc\u30d0\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n\u30b3\u30df\u30c3\u30c8\u3092 b7fac33 \u306b\u623b\u3057\u3001CUDA7.5 \u7cfb\u306b\u3059\u308b\u5fc5\u8981\u3042\u308a\n\n\ndocker build \u306b\u306f g2.2xlarge \u3067 30 \u5206\u7a0b\u5ea6\u304b\u304b\u308a\u307e\u3059\n\n\n\u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\ncd $HOME\ngit clone https://github.com/NVIDIA/gpu-rest-engine\ncd gpu-rest-engine\ngit checkout b7fac33\ndocker build -t inference -f Dockerfile.inference_server .\nnvidia-docker run -d --name api -p 8000:8000 inference\n\n\n\n2. \u8d77\u52d5\u78ba\u8a8d\ngit \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b\u30a6\u30b5\u30ae\u306e\u753b\u50cf\u3092\u63a8\u8ad6\u30b5\u30fc\u30d0\u306b\u554f\u3044\u5408\u308f\u305b\u3066\u307f\u307e\u3059\u3002\n[ec2-user@ip-10-0-0-10 ~]$ IMAGE=\"./images/1.jpg\"\n[ec2-user@ip-10-0-0-10 ~]$ curl -XPOST --data-binary @${IMAGE} 127.0.0.1:8000/api/classify\n\n[{\"confidence\":0.9998,\"label\":\"n02328150 Angora, Angora rabbit\"},{\"confidence\":0.0001,\"label\":\"n02325366 wood rabbit, cottontail, cottontail rabbit\"},{\"confidence\":0.0001,\"label\":\"n02326432 hare\"},{\"confidence\":0.0000,\"label\":\"n02085936 Maltese dog, Maltese terrier, Maltese\"},{\"confidence\":0.0000,\"label\":\"n02342885 hamster\"}]\n\n99.98% \u306e\u78ba\u7387\u3067\u30a2\u30f3\u30b4\u30e9\u30a6\u30b5\u30ae\u3089\u3057\u3044\u3067\u3059\u3002\u52d5\u3044\u3066\u307e\u3059\u306d\u3002\n\u3053\u306e\u72b6\u614b\u3067 SecurityGroup \u3055\u3048\u89e3\u653e\u3055\u308c\u3066\u3044\u308c\u3070\u3001\u5916\u90e8\u304b\u3089\u306e\u63a8\u8ad6\u3082\u884c\u3048\u307e\u3059\u3002\u697d\u3057\u3044\u3002\n\n\u7c21\u6613\u30d9\u30f3\u30c1\u30de\u30fc\u30af\n\n\u691c\u8a3c\u7528\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u30d3\u30eb\u30c9\nrakyll/boom \u3092\u5185\u5305\u3057\u305f\u30c4\u30fc\u30eb\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002\n\n\u691c\u8a3c\u7528\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u751f\u6210\ndocker build -t client -f Dockerfile.inference_client .\n\n\n\n\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5b9f\u65bd5\n\n\ng2.2xlarge\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=1 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        258.6445 secs\n  Slowest:      0.0167 secs\n  Fastest:      0.0127 secs\n  Average:      0.0129 secs\n  Requests/sec: 77.3262\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n\n\nGRE \u306e README.md \u306b\u306f\u300c\u30b9\u30bf\u30f3\u30c9\u30a2\u30ed\u30fc\u30f3\u306a Caffe \u306a\u3089\u3060\u3044\u305f\u3044\u79d2\u9593 500 \u753b\u50cf\u51e6\u7406\u3067\u304d\u308b\u300d\u3068\u3042\u308a\u307e\u3059\u304c\u3001\u307e\u3042\u3001\u306f\u3044\u3002\n\u4e00\u65b9\u3067\u3053\u306e GRE\u3001\u30de\u30eb\u30c1 GPU \u306b\u5bfe\u5fdc\u3057\u3066\u3061\u3083\u3093\u3068\u30ea\u30af\u30a8\u30b9\u30c8\u634c\u3044\u3066\u304f\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u306a\u306e\u3067\u30fb\u30fb\n\n\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30bf\u30a4\u30d7\u5909\u66f4\nAmazon EC2 \u3067\u306f\u6700\u8fd1\u3001P2 \u7cfb\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\nNew P2 Instance Type for Amazon EC2 \u2013 Up to 16 GPUs | AWS Blog\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u305b\u3063\u304b\u304f\u306a\u306e\u3067 p2.16xlarge \u3067\u3082\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3092\u3068\u308a\u6bd4\u8f03\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u30596\u3002GPU \u305d\u306e\u3082\u306e\u3082 K80 \u3060\u3057\u3001\u4f55\u304b\u3089\u4f55\u307e\u3067\u9ad8\u30b9\u30da\u30c3\u30af\u306b\u306a\u308b\u306e\u3067\u671f\u5f85\u304c\u9ad8\u307e\u308a\u307e\u3059\u306d\uff01\n\np2.16xlarge \u3067\u518d\u53d6\u5f97\n10\u670812\u65e5\u73fe\u5728\u3001\u30b9\u30dd\u30c3\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u610f\u5473\u4e0d\u660e\u306a\u91d1\u984d\u306b\u9ad8\u9a30\u3057\u3066\u3044\u308b\u306e\u3067\u7d20\u76f4\u306b\u30aa\u30f3\u30c7\u30de\u30f3\u30c9\u3067\u8d77\u52d5\u3057\u307e\u3059\u3002\u4e0a\u8ff0\u306e\u624b\u9806\u540c\u69d8\u306b7 GRE \u307e\u3067\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u6e08\u307e\u305b8\u3001nvidia-smi \u3092\u6d41\u3057\u3066\u307f\u307e\u3059\u3002\n\nGPU\u6570\n[ec2-user@ip-10-0-0-10 ~]$ nvidia-docker run --rm nvidia/cuda:7.5 nvidia-smi | grep K80 | wc -l\n16\n\n\n\u3053\u308c\u3092\u53c2\u8003\u306b CONCURRENCY \u306e\u5024\u3092\u3088\u3057\u306a\u306b\u5909\u66f4\u3057\u3064\u3064\u3001\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u53d6\u5f97\u30b3\u30de\u30f3\u30c9\u3092\u6253\u3063\u3066\u307f\u307e\u3059\u3002\n\np2.16xlarge\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=16 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        14.1379 secs\n  Slowest:      3.9332 secs\n  Fastest:      0.0055 secs\n  Average:      0.0110 secs\n  Requests/sec: 1414.6341\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n\n\n\u304a\u30fc\u306a\u308b\u307b\u3069\u3002\n\u66f4\u306b\u3001p2 \u7cfb\u3067\u306f GPU \u6700\u9069\u5316 \u304c\u884c\u3048\u308b\u306e\u3067\u3001\u305d\u306e\u4e0a\u3067\u3082\u3046\u4e00\u5ea6\u3084\u3063\u3066\u307f\u307e\u3059\u3002\n\nGPU\u6700\u9069\u5316\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi -pm 1\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi --auto-boost-default=0\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi -ac 2505,875\n\n\n\np2.16xlarge\uff08GPU\u6700\u9069\u5316\u5f8c\uff09\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=16 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        7.9519 secs\n  Slowest:      0.0200 secs\n  Fastest:      0.0055 secs\n  Average:      0.0063 secs\n  Requests/sec: 2515.1144\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n\n\n\u305d\u3082\u305d\u3082 p2 \u7cfb\u306f\u30b9\u30da\u30c3\u30af\u7684\u306b\u5b66\u7fd2\u306b\u6700\u9069\u306a\u30b5\u30fc\u30d0\u3060\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u63a8\u8ad6\u3067\u3082\u3053\u308c\u3060\u3051\u9055\u3044\u304c\u3067\u3066\u304f\u308b\u3068\u308f\u304f\u308f\u304f\u3057\u307e\u3059\u306d\uff01\uff01\n\n\u5229\u7528\u3059\u308b GPU \u30c7\u30d0\u30a4\u30b9\u6570\u3092\u5909\u3048\u3066\u307f\u308b\nNVIDIA-docker \u306f\u3084\u308c\u308b\u5b50\u306a\u306e\u3067\u3001docker \u30b3\u30f3\u30c6\u30ca\u306b\u5f15\u304d\u6e21\u3057\u30a2\u30d7\u30ea\u3067\u5229\u7528\u3059\u308b\u30c7\u30d0\u30a4\u30b9\u3092\u81ea\u7531\u306b\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002\uff08\u5f53\u7136\u306a\u304c\u3089 docker \u30b3\u30f3\u30c6\u30ca\u304b\u3089\u306f\u6e21\u3055\u308c\u305f GPU \u3057\u304b\u898b\u3048\u307e\u305b\u3093\uff09\n\u73fe\u5728\u52d5\u3044\u3066\u3044\u308b\u30b5\u30fc\u30d3\u30b9\u3092\u4e00\u5ea6\u6b62\u3081\n\n\u30b5\u30fc\u30d3\u30b9\u306e\u505c\u6b62\ndocker stop api\ndocker rm api\n\n\n5 GPU \u306b\u5236\u9650\u3057\u3066\u8d77\u52d5\u3057\u76f4\u3057\u307e\u3059\u3002\n\nGPU\u5236\u9650\u7248\u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\nNV_GPU=0,1,2,3,4 nvidia-docker run -d --name api -p 8000:8000 inference\n\n\n\np2.16xlarge\uff08GPU\u5236\u9650\u5f8c\uff09\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=5 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        25.8224 secs\n  Slowest:      0.5705 secs\n  Fastest:      0.0055 secs\n  Average:      0.0064 secs\n  Requests/sec: 774.5201\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n\n\n\u305d\u308c\u3063\u307d\u3044\u5024\u3067\u3059\u306d\uff01\n\u304a\u308f\u308a\u3002\n\u3000\n\n\n\n\nyum update \u3067\u30ab\u30fc\u30cd\u30eb\u304c\u66f4\u65b0\u3055\u308c\u3066\u3057\u307e\u3046\u3068\u3001\u518d\u8d77\u52d5\u6642\u306b\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u304c\u8aad\u307f\u8fbc\u307e\u308c\u306a\u304f\u306a\u308b\u305f\u3081\u3002 sudo sh -c 'echo \"exclude=kernel*\" >> /etc/yum.conf' \u306a\u3069\u3067\u5bfe\u51e6\u3057\u3066\u304a\u3044\u3066\u3082\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\u00a0\u21a9\n\n\ndocker: Error response from daemon \u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u518d\u5ea6\u5b9f\u884c\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u30022\u5ea6\u76ee\u306a\u3089\u901a\u308b\u3068\u601d\u3044\u307e\u3059\u00a0\u21a9\n\n\nGo \u306e http \u30b5\u30fc\u30d0\u7d4c\u7531\u3067 Caffe \u3067\u306e\u63a8\u8ad6\u3092\u884c\u3046\u8584\u3044\u30e9\u30c3\u30d1\u30fc\u3067\u3059\u00a0\u21a9\n\n\n\u53c2\u8003: GPU REST Engine \u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3068\u52d5\u4f5c\u78ba\u8a8d\u00a0\u21a9\n\n\n\u753b\u50cf\u306e\u63a8\u8ad6\u306f\u305b\u305a\u3001CUDA \u306e kernel call \u306e\u307f\u3092\u8a08\u6e2c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3 \u3082\u540c\u68b1\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u00a0\u21a9\n\n\n\u4e0a\u9650\u7de9\u548c\u3057\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u305d\u3082\u305d\u3082\u7533\u8acb\u30d5\u30a9\u30fc\u30e0\u4e0a p2 \u7cfb\u304c\u9078\u629e\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u30b3\u30e1\u30f3\u30c8\u6b04\u3067\u300cp2 \u4f7f\u308f\u305b\u3066\uff01\u300d\u3068\u61c7\u9858\u3057\u3066\u3057\u3070\u3089\u304f\u5f85\u3061\u307e\u3057\u3087\u3046\u00a0\u21a9\n\n\ng2 \u306a\u3069\u3067\u306e AMI \u5316\u304c\u3046\u307e\u304f\u3044\u3063\u3066\u3044\u306a\u3044\u3068 p2 \u8d77\u52d5\u5f8c\u306b\u7d76\u671b\u3059\u308b\u306e\u3067\u3001\u9593\u9055\u3044\u306a\u304f\u52d5\u4f5c\u3059\u308b\u3001NVIDIA \u516c\u5f0f AMI \u304b\u3089\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3042\u3066\u3066\u3044\u304f\u624b\u9806\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u6163\u308c\u3066\u3044\u306a\u3051\u308c\u3070\u3002\u00a0\u21a9\n\n\n\u63a8\u8ad6\u30b5\u30fc\u30d0\u306f GPU \u5206\u306e\u521d\u671f\u5316\u51e6\u7406\uff08\u30e2\u30c7\u30eb\u306e\u30ed\u30fc\u30c9\u306a\u3069\uff09\u304c\u8d70\u308b\u305f\u3081\u3001\u8d77\u52d5\u307e\u3067\u3057\u3070\u3089\u304f\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002docker logs -f api \u306a\u3069\u3067 \"Starting server listening on :8000\" \u304c\u8868\u793a\u3055\u308c\u308b\u307e\u3067\u6c17\u9577\u306b\u5f85\u3061\u307e\u3059\u3002\u00a0\u21a9\n\n\n\n# \u66f4\u65b0\u5c65\u6b74\n\n- [2017/03/08] AWS \u516c\u5f0f [Deep Learning AMI v2 \u30ea\u30ea\u30fc\u30b9](https://aws.amazon.com/about-aws/whats-new/2017/03/deep-learning-ami-release-v2-0-now-available-for-amazon-linux/)\uff01  \n**CUDA 8.0** \u306b\u306a\u308a\u3001TensorFlow v1.0.0, MXNet v0.9.3 \u306a\u3069\n\n# Deep Learning \u306b\u4fbf\u5229\u306a AMI\n\n## NVIDIA \u63d0\u4f9b\n\n* [\nNVIDIA CUDA Toolkit 7.5 on Amazon Linux](https://aws.amazon.com/marketplace/pp/B01LZMLK1K)\n* [NVIDIA DIGITS 4 on Ubuntu 14.04](https://aws.amazon.com/marketplace/pp/B01LZN28VD)\n\n2016/09 \u672b\u306b\u516c\u958b\u3055\u308c\u305f\u6a21\u69d8\u3002\n\n\u81ea\u5206\u3067 CUDA \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3053\u3068\u304c\u3042\u308b\u65b9\u306a\u3089\u308f\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001NVIDIA \u306e GPU \u30c9\u30e9\u30a4\u30d0\u304b\u3089\u59cb\u307e\u308a\u3001CUDA \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u306e\u306f\u306a\u304b\u306a\u304b\u306e\u624b\u9593\u3067\u3057\u305f\u3002\uff08\u307e\u3042\u3001\u4e00\u5ea6 AMI \u3092\u4f5c\u308c\u3070\u3063\u3066\u306e\u306f\u3042\u308a\u307e\u3059\u304c\uff09\n\u3053\u306e AMI \u3092\u4f7f\u3048\u3070\u3001\u3082\u306e\u3059\u3054\u304f\u7c21\u5358\u306b\u30b5\u30fc\u30d0\u304c\u4f7f\u3044\u59cb\u3081\u3089\u308c\u307e\u3059\uff01\n\n## AWS \u63d0\u4f9b\n\n* [Deep Learning AMI Amazon Linux Version](https://aws.amazon.com/marketplace/pp/B01M0AXXQB)\n* [Deep Learning AMI Ubuntu Version](https://aws.amazon.com/marketplace/pp/B06VSPXKDX)\n\nMXNet\u3001Caffe\u3001Tensorflow\u3001Theano \u305d\u3057\u3066 Torch \u306a\u3069\u4eba\u6c17\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304c\u4e8b\u524d\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30fb\u8a2d\u5b9a\u30fb\u30c6\u30b9\u30c8\u3055\u308c\u305f AMI\u3002\u305f\u3060\u3057\u3001\u3053\u3061\u3089\u306b\u306f ~~CUDA \u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u304a\u3089\u305a~~\u3001**\u304b\u3064\u6771\u4eac\u30ea\u30fc\u30b8\u30e7\u30f3\u306b\u306f\u914d\u5e03\u3055\u308c\u3066\u3044\u307e\u305b\u3093**\u30fb\u30fb\u3002\uff08\u8ffd\u8a18\uff1a\u6700\u65b0\u7248\u306f CUDA \u5165\u3063\u3066\u3044\u307e\u3059\uff01\u3042\u308a\u304c\u305f\u3044\uff01\uff01\uff09\n\n\uff08\u304a\u8a66\u3057\u7528\u30c6\u30b9\u30c8\u30b5\u30f3\u30d7\u30eb\u304c\u5165\u3063\u3066\u3044\u305f\u308a\u3059\u308b\u3088\u3046\u3067\u3059\uff09\n\n```\n[ec2-user@ip-10-0-0-10 ~]$ ls -la /home/ec2-user/src/bin\n\u5408\u8a08 40\ndrwxrwxr-x  2 ec2-user ec2-user 4096  9\u6708 20 00:48 .\ndrwxrwxr-x 14 ec2-user ec2-user 4096  9\u6708 28 20:21 ..\n-rwxr-xr-x  1 ec2-user ec2-user  350  9\u6708 20 00:12 testAll\n-rwxr-xr-x  1 ec2-user ec2-user 1207  9\u6708 20 00:12 testCaffe\n-rwxr-xr-x  1 ec2-user ec2-user 1141  9\u6708 20 00:12 testMXNet\n-rwxr-xr-x  1 ec2-user ec2-user   70  9\u6708 20 00:13 testTensorFlow\n-rwxr-xr-x  1 ec2-user ec2-user   66  9\u6708 20 00:13 testTheano\n-rwxr-xr-x  1 ec2-user ec2-user 1058  9\u6708 20 00:13 testTheanoOrTensorFlow\n-rwxr-xr-x  1 ec2-user ec2-user  861  9\u6708 20 00:13 testTorch\n-rwxr-xr-x  1 ec2-user ec2-user  210  9\u6708 20 00:13 testUtil\n```\n\n# \u8d77\u52d5\u3057\u3066\u307f\u308b\n\nNVIDIA CUDA Toolkit 7.5 on Amazon Linux \u3092\u4f7f\u3044\u3001\u4e2d\u8eab\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n## \u8d77\u52d5\u624b\u9806\n\n### 1. \u30de\u30fc\u30b1\u30c3\u30c8\u30d7\u30ec\u30a4\u30b9\u3067\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u627f\u8afe\n\n\u9069\u5207\u306a\u30ea\u30fc\u30b8\u30e7\u30f3\u3092\u9078\u629e\u3057 `continue` \u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\n\n<img width=\"500\" alt=\"1_1.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/66457/1494ffb0-672d-fadb-2e90-477d3d4d4886.png\" width=\"500\">\n\n\u30b9\u30dd\u30c3\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f7f\u3044\u305f\u3044\u306e\u3067\u3001\u30de\u30cb\u30e5\u30a2\u30eb\u3067\u306e\u8d77\u52d5\u3092\u9078\u629e\u3002\u753b\u9762\u306e\u5de6\u4e0b\u306b\u3042\u308b AMI \u306e ID \u3092\u63a7\u3048\u3064\u3064\u3001\uff08\u4eca\u56de\u306e AMI \u306f\u3044\u307e\u306e\u3068\u3053\u308d\u7121\u6599\u3067\u3059\u304c\uff09\u8d77\u52d5\u6642\u9593\u3042\u305f\u308a\u306e\u5229\u7528\u6599\u3092\u78ba\u8a8d\u3057\u3001\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u898f\u7d04\u306b\u5f93\u3046\u3053\u3068\u3092\u627f\u8a8d\u3057\u307e\u3059\u3002\n\n<img width=\"972\" alt=\"2.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/66457/65d930d4-f6f4-be54-6b84-916e6bd73c9a.png\" width=\"500\">\n\n### 2. EC2 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u8d77\u52d5\n\n\u30b9\u30dd\u30c3\u30c8\u6599\u91d1\u3092\u8a2d\u5b9a\u3057\u3064\u3064 g2.2xlarge \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n<img width=\"500\" alt=\"1.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/66457/c2625e95-2be5-0d3e-d0c5-1a547e582d3d.png\" width=\"500\">\n\n<img width=\"500\" alt=\"2.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/66457/8505320e-91c5-186c-9171-9a21b6d689bc.png\" width=\"500\">\n\n\u30b9\u30dd\u30c3\u30c8\u306e\u91d1\u984d\u304c\u5e02\u5834\u3092\u4e0b\u56de\u308b\u3068\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n### 3. EC2 \u306b SSH\n\n\u8d77\u52d5\u3057\u305f\u3089\u30b5\u30fc\u30d0\u306b\u5165\u3063\u3066\u307f\u307e\u3059\u3002\n`ssh -i ec2.pem ec2-user@x.x.x.x`\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u30c9\u30e9\u30a4\u30d0\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3059\u3002\n\n```sh:\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ nvcc -V\n\n-bash: nvcc: \u30b3\u30de\u30f3\u30c9\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\n```\n\n\u306a\u308b\u307b\u3069\u3002\n\n```sh:\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ /usr/local/cuda/bin/nvcc -V\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n```\n\n\u306f\u3044\u3001\u3057\u3063\u304b\u308a 7.5 \u3067\u3059\u306d\u3002AMI \u7d20\u6674\u3089\u3057\u3044\uff01\uff01\n\n## NVIDIA-docker \u3082\u5165\u308c\u3068\u3053\u3046\n\n### 1. docker \u3092\u5165\u308c\u3066\n\n[\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html#install_docker) \u306b\u306f\u5f93\u308f\u305a[^9]\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067 Docker \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n\n```sh:\nsudo yum install -y docker\nsudo service docker start\nsudo usermod -aG docker ec2-user\n```\n\n\u4ee5\u5f8c ec2-user \u3067\u3082 docker \u30b3\u30de\u30f3\u30c9\u304c\u4f7f\u3048\u308b\u3088\u3046\u306b\u3001\u3044\u3063\u305f\u3093 SSH \u304b\u3089\u629c\u3051\u3066\u5165\u308a\u306a\u304a\u3057\u307e\u3059\u3002\n\n### 2. NVIDIA-docker \u3082\u5165\u308c\u308b\n\n\u6b8b\u5ff5\u306a\u304c\u3089 `1.0.0` \u6642\u70b9\u3067\u306f\u3001\u914d\u5e03\u3055\u308c\u3066\u3044\u308b rpm \u306e\u4f9d\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u304c AmazonLinux \u3060\u3068\u30a2\u30ec\u306a\u3082\u306e\u3067\u3001[\u30d0\u30a4\u30ca\u30ea\u3092\u6301\u3063\u3066\u304d\u3066\u7f6e\u304f](https://github.com/NVIDIA/nvidia-docker/wiki#other-distributions)\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n```sh:nvidia-docker\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.0/nvidia-docker_1.0.0_amd64.tar.xz\nsudo tar --strip-components=1 -C /usr/bin -xvf /tmp/nvidia-docker*.tar.xz && rm /tmp/nvidia-docker*.tar.xz\n```\n\n### 3. nvidia-docker-plugin \u306e\u8d77\u52d5\n\n```sh:nvidia-docker-plugin\u306e\u8d77\u52d5\nsudo mkdir -p /run/docker/plugins/\nsudo -b nohup nvidia-docker-plugin > /tmp/nvidia-docker.log\n```\n\n\u3082\u3057\u5f8c\u304b\u3089\u30b5\u30fc\u30d0\u3092\u518d\u8d77\u52d5\u3059\u308b / AMI \u3092\u3068\u308b\u3064\u3082\u308a\u3067\u3042\u308c\u3070\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u7528\u610f\u3057\u3066\u304a\u304f\u3068\u3088\u3044\u3067\u3059\u3002\n\n```sh:nvidia-docker-plugin\u306einit\u30b9\u30af\u30ea\u30d7\u30c8\u8a2d\u7f6e\nsudo sh -c \"cat << EOF > /etc/init.d/nvdocker-plugin\n#!/bin/sh\n# chkconfig: 345 98 20\n# description: NVIDIA-docker plugin\nnvidia-docker-plugin > /tmp/nvidia-docker.log\nEOF\"\nsudo chmod +x /etc/init.d/nvdocker-plugin\nsudo chkconfig --add nvdocker-plugin\n```\n\nplugin \u304c\u8d77\u52d5\u3057\u305f\u3089\u3001REST API \u306e\u758e\u901a\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n\n```sh:API\u8d77\u52d5\u78ba\u8a8d\n[ec2-user@ip-10-0-0-10 ~]$ curl 127.0.0.1:3476/v1.0/docker/cli\n\n--volume-driver=nvidia-docker --volume=nvidia_driver_352.99:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia0\n```\n\n### 4. \u8d77\u52d5\u78ba\u8a8d\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f NVIDIA-docker \u3067 nvidia-smi \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```sh:Docker\u3092\u4f7f\u3063\u305fnvidia-smi\n[ec2-user@ip-10-0-0-10 ~]$ nvidia-docker run --rm nvidia/cuda:7.5 nvidia-smi\n\nTue Oct 11 00:38:26 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.99     Driver Version: 352.99         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\n| N/A   25C    P8    17W / 125W |     11MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n```\n\n\u3044\u3044\u611f\u3058[^2]\u3067\u3059\u306d\u3002\n\n# \u691c\u8a3c\n\n## \u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\n\nGPU \u3092\u63a8\u8ad6\u306b\u4f7f\u3044\u3001HTTP \u30d7\u30ed\u30c8\u30b3\u30eb\u3092\u8a71\u3059 API \u30b5\u30fc\u30d0\u3092\u4f7f\u3063\u3066\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3092\u3068\u3063\u3066\u307f\u307e\u3059\u3002\n\n### 1. GRE \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nNVIDIA \u793e\u304c\u30c7\u30e2\u7528\u306b\u4f5c\u3063\u305f\u3001Caffe \u3067\u306e\u63a8\u8ad6\u3092 REST API \u5316\u3057\u305f [GPU REST Engine](https://github.com/NVIDIA/gpu-rest-engine)[^3] \u3068\u3044\u3046\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u4f7f\u3044\u307e\u3059[^4]\u3002\n\n\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u63a8\u8ad6\u30b5\u30fc\u30d0\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n* \u30b3\u30df\u30c3\u30c8\u3092 b7fac33 \u306b\u623b\u3057\u3001**CUDA7.5 \u7cfb\u306b\u3059\u308b\u5fc5\u8981\u3042\u308a**\n* `docker build` \u306b\u306f g2.2xlarge \u3067 30 \u5206\u7a0b\u5ea6\u304b\u304b\u308a\u307e\u3059\n\n```sh:\u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\ncd $HOME\ngit clone https://github.com/NVIDIA/gpu-rest-engine\ncd gpu-rest-engine\ngit checkout b7fac33\ndocker build -t inference -f Dockerfile.inference_server .\nnvidia-docker run -d --name api -p 8000:8000 inference\n```\n\n### 2. \u8d77\u52d5\u78ba\u8a8d\n\ngit \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u540c\u68b1\u3055\u308c\u3066\u3044\u308b\u30a6\u30b5\u30ae\u306e\u753b\u50cf\u3092\u63a8\u8ad6\u30b5\u30fc\u30d0\u306b\u554f\u3044\u5408\u308f\u305b\u3066\u307f\u307e\u3059\u3002\n\n```\n[ec2-user@ip-10-0-0-10 ~]$ IMAGE=\"./images/1.jpg\"\n[ec2-user@ip-10-0-0-10 ~]$ curl -XPOST --data-binary @${IMAGE} 127.0.0.1:8000/api/classify\n\n[{\"confidence\":0.9998,\"label\":\"n02328150 Angora, Angora rabbit\"},{\"confidence\":0.0001,\"label\":\"n02325366 wood rabbit, cottontail, cottontail rabbit\"},{\"confidence\":0.0001,\"label\":\"n02326432 hare\"},{\"confidence\":0.0000,\"label\":\"n02085936 Maltese dog, Maltese terrier, Maltese\"},{\"confidence\":0.0000,\"label\":\"n02342885 hamster\"}]\n```\n\n99.98% \u306e\u78ba\u7387\u3067\u30a2\u30f3\u30b4\u30e9\u30a6\u30b5\u30ae\u3089\u3057\u3044\u3067\u3059\u3002\u52d5\u3044\u3066\u307e\u3059\u306d\u3002\n\u3053\u306e\u72b6\u614b\u3067 SecurityGroup \u3055\u3048\u89e3\u653e\u3055\u308c\u3066\u3044\u308c\u3070\u3001\u5916\u90e8\u304b\u3089\u306e\u63a8\u8ad6\u3082\u884c\u3048\u307e\u3059\u3002\u697d\u3057\u3044\u3002\n\n\n## \u7c21\u6613\u30d9\u30f3\u30c1\u30de\u30fc\u30af\n\n### \u691c\u8a3c\u7528\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u30d3\u30eb\u30c9\n\n[rakyll/boom](https://github.com/rakyll/boom) \u3092\u5185\u5305\u3057\u305f\u30c4\u30fc\u30eb\u3092\u30d3\u30eb\u30c9\u3057\u307e\u3059\u3002\n\n```sh:\u691c\u8a3c\u7528\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u751f\u6210\ndocker build -t client -f Dockerfile.inference_client .\n```\n\n### \u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5b9f\u65bd[^5]\n\n```sh:g2.2xlarge\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=1 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:       \t258.6445 secs\n  Slowest:     \t0.0167 secs\n  Fastest:     \t0.0127 secs\n  Average:     \t0.0129 secs\n  Requests/sec:\t77.3262\n  Total data:  \t6880000 bytes\n  Size/request:\t344 bytes\n[...]\n```\n\nGRE \u306e README.md \u306b\u306f\u300c\u30b9\u30bf\u30f3\u30c9\u30a2\u30ed\u30fc\u30f3\u306a Caffe \u306a\u3089\u3060\u3044\u305f\u3044\u79d2\u9593 500 \u753b\u50cf\u51e6\u7406\u3067\u304d\u308b\u300d\u3068\u3042\u308a\u307e\u3059\u304c\u3001\u307e\u3042\u3001\u306f\u3044\u3002\n\u4e00\u65b9\u3067\u3053\u306e GRE\u3001\u30de\u30eb\u30c1 GPU \u306b\u5bfe\u5fdc\u3057\u3066\u3061\u3083\u3093\u3068\u30ea\u30af\u30a8\u30b9\u30c8\u634c\u3044\u3066\u304f\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u306a\u306e\u3067\u30fb\u30fb\n\n### \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30bf\u30a4\u30d7\u5909\u66f4\n\nAmazon EC2 \u3067\u306f\u6700\u8fd1\u3001P2 \u7cfb\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u307e\u3057\u305f\u3002\n[New P2 Instance Type for Amazon EC2 \u2013 Up to 16 GPUs | AWS Blog](https://aws.amazon.com/jp/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/)\n\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u305b\u3063\u304b\u304f\u306a\u306e\u3067 p2.16xlarge \u3067\u3082\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3092\u3068\u308a\u6bd4\u8f03\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3059[^6]\u3002GPU \u305d\u306e\u3082\u306e\u3082 K80 \u3060\u3057\u3001\u4f55\u304b\u3089\u4f55\u307e\u3067\u9ad8\u30b9\u30da\u30c3\u30af\u306b\u306a\u308b\u306e\u3067\u671f\u5f85\u304c\u9ad8\u307e\u308a\u307e\u3059\u306d\uff01\n\n### p2.16xlarge \u3067\u518d\u53d6\u5f97\n\n10\u670812\u65e5\u73fe\u5728\u3001\u30b9\u30dd\u30c3\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304c\u610f\u5473\u4e0d\u660e\u306a\u91d1\u984d\u306b\u9ad8\u9a30\u3057\u3066\u3044\u308b\u306e\u3067\u7d20\u76f4\u306b\u30aa\u30f3\u30c7\u30de\u30f3\u30c9\u3067\u8d77\u52d5\u3057\u307e\u3059\u3002\u4e0a\u8ff0\u306e\u624b\u9806\u540c\u69d8\u306b[^7] GRE \u307e\u3067\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u6e08\u307e\u305b[^8]\u3001nvidia-smi \u3092\u6d41\u3057\u3066\u307f\u307e\u3059\u3002\n\n```sh:GPU\u6570\n[ec2-user@ip-10-0-0-10 ~]$ nvidia-docker run --rm nvidia/cuda:7.5 nvidia-smi | grep K80 | wc -l\n16\n```\n\n\u3053\u308c\u3092\u53c2\u8003\u306b `CONCURRENCY` \u306e\u5024\u3092\u3088\u3057\u306a\u306b\u5909\u66f4\u3057\u3064\u3064\u3001\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u53d6\u5f97\u30b3\u30de\u30f3\u30c9\u3092\u6253\u3063\u3066\u307f\u307e\u3059\u3002\n\n```sh:p2.16xlarge\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=16 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        14.1379 secs\n  Slowest:      3.9332 secs\n  Fastest:      0.0055 secs\n  Average:      0.0110 secs\n  Requests/sec: 1414.6341\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n```\n\n\u304a\u30fc\u306a\u308b\u307b\u3069\u3002\n\n\u66f4\u306b\u3001p2 \u7cfb\u3067\u306f [GPU \u6700\u9069\u5316](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cluster_computing.html#optimize_gpu) \u304c\u884c\u3048\u308b\u306e\u3067\u3001\u305d\u306e\u4e0a\u3067\u3082\u3046\u4e00\u5ea6\u3084\u3063\u3066\u307f\u307e\u3059\u3002\n\n```sh:GPU\u6700\u9069\u5316\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi -pm 1\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi --auto-boost-default=0\nnvidia-docker run --rm --privileged nvidia/cuda:7.5 nvidia-smi -ac 2505,875\n```\n\n```sh:p2.16xlarge\uff08GPU\u6700\u9069\u5316\u5f8c\uff09\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=16 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        7.9519 secs\n  Slowest:      0.0200 secs\n  Fastest:      0.0055 secs\n  Average:      0.0063 secs\n  Requests/sec: 2515.1144\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n```\n\n\u305d\u3082\u305d\u3082 p2 \u7cfb\u306f\u30b9\u30da\u30c3\u30af\u7684\u306b\u5b66\u7fd2\u306b\u6700\u9069\u306a\u30b5\u30fc\u30d0\u3060\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u63a8\u8ad6\u3067\u3082\u3053\u308c\u3060\u3051\u9055\u3044\u304c\u3067\u3066\u304f\u308b\u3068\u308f\u304f\u308f\u304f\u3057\u307e\u3059\u306d\uff01\uff01\n\n### \u5229\u7528\u3059\u308b GPU \u30c7\u30d0\u30a4\u30b9\u6570\u3092\u5909\u3048\u3066\u307f\u308b\n\nNVIDIA-docker \u306f\u3084\u308c\u308b\u5b50\u306a\u306e\u3067\u3001**docker \u30b3\u30f3\u30c6\u30ca\u306b\u5f15\u304d\u6e21\u3057\u30a2\u30d7\u30ea\u3067\u5229\u7528\u3059\u308b\u30c7\u30d0\u30a4\u30b9\u3092\u81ea\u7531\u306b\u6307\u5b9a**\u3067\u304d\u307e\u3059\u3002\uff08\u5f53\u7136\u306a\u304c\u3089 docker \u30b3\u30f3\u30c6\u30ca\u304b\u3089\u306f\u6e21\u3055\u308c\u305f GPU \u3057\u304b\u898b\u3048\u307e\u305b\u3093\uff09\n\n\u73fe\u5728\u52d5\u3044\u3066\u3044\u308b\u30b5\u30fc\u30d3\u30b9\u3092\u4e00\u5ea6\u6b62\u3081\n\n```sh:\u30b5\u30fc\u30d3\u30b9\u306e\u505c\u6b62\ndocker stop api\ndocker rm api\n```\n\n5 GPU \u306b\u5236\u9650\u3057\u3066\u8d77\u52d5\u3057\u76f4\u3057\u307e\u3059\u3002\n\n```sh:GPU\u5236\u9650\u7248\u63a8\u8ad6\u30b5\u30fc\u30d0\u306e\u8d77\u52d5\nNV_GPU=0,1,2,3,4 nvidia-docker run -d --name api -p 8000:8000 inference\n```\n\n```sh:p2.16xlarge\uff08GPU\u5236\u9650\u5f8c\uff09\n[ec2-user@ip-10-0-0-10 ~]$ docker run --rm -e CONCURRENCY=5 -e REQUESTS=20000 --net=host client\n\nSummary:\n  Total:        25.8224 secs\n  Slowest:      0.5705 secs\n  Fastest:      0.0055 secs\n  Average:      0.0064 secs\n  Requests/sec: 774.5201\n  Total data:   6880000 bytes\n  Size/request: 344 bytes\n[...]\n```\n\n\u305d\u308c\u3063\u307d\u3044\u5024\u3067\u3059\u306d\uff01\n\u304a\u308f\u308a\u3002\n\n\u3000\n\n[^2]: `docker: Error response from daemon` \u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u518d\u5ea6\u5b9f\u884c\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u30022\u5ea6\u76ee\u306a\u3089\u901a\u308b\u3068\u601d\u3044\u307e\u3059\n[^3]: Go \u306e http \u30b5\u30fc\u30d0\u7d4c\u7531\u3067 Caffe \u3067\u306e\u63a8\u8ad6\u3092\u884c\u3046\u8584\u3044\u30e9\u30c3\u30d1\u30fc\u3067\u3059\n[^4]: \u53c2\u8003: [GPU REST Engine \u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3068\u52d5\u4f5c\u78ba\u8a8d](http://qiita.com/daikumatan/items/2758ebd12209622772ab)\n[^5]: \u753b\u50cf\u306e\u63a8\u8ad6\u306f\u305b\u305a\u3001[CUDA \u306e kernel call \u306e\u307f\u3092\u8a08\u6e2c\u3059\u308b\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3](https://github.com/NVIDIA/gpu-rest-engine#benchmarking-overhead-of-cuda-kernel-calls) \u3082\u540c\u68b1\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n[^6]: \u4e0a\u9650\u7de9\u548c\u3057\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044\u306e\u3067\u3059\u304c\u3001\u305d\u3082\u305d\u3082\u7533\u8acb\u30d5\u30a9\u30fc\u30e0\u4e0a p2 \u7cfb\u304c\u9078\u629e\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u30b3\u30e1\u30f3\u30c8\u6b04\u3067\u300cp2 \u4f7f\u308f\u305b\u3066\uff01\u300d\u3068\u61c7\u9858\u3057\u3066\u3057\u3070\u3089\u304f\u5f85\u3061\u307e\u3057\u3087\u3046\n[^7]: g2 \u306a\u3069\u3067\u306e AMI \u5316\u304c\u3046\u307e\u304f\u3044\u3063\u3066\u3044\u306a\u3044\u3068 p2 \u8d77\u52d5\u5f8c\u306b\u7d76\u671b\u3059\u308b\u306e\u3067\u3001\u9593\u9055\u3044\u306a\u304f\u52d5\u4f5c\u3059\u308b\u3001NVIDIA \u516c\u5f0f AMI \u304b\u3089\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3042\u3066\u3066\u3044\u304f\u624b\u9806\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u6163\u308c\u3066\u3044\u306a\u3051\u308c\u3070\u3002\n[^8]: \u63a8\u8ad6\u30b5\u30fc\u30d0\u306f GPU \u5206\u306e\u521d\u671f\u5316\u51e6\u7406\uff08\u30e2\u30c7\u30eb\u306e\u30ed\u30fc\u30c9\u306a\u3069\uff09\u304c\u8d70\u308b\u305f\u3081\u3001\u8d77\u52d5\u307e\u3067\u3057\u3070\u3089\u304f\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002`docker logs -f api` \u306a\u3069\u3067 \"Starting server listening on :8000\" \u304c\u8868\u793a\u3055\u308c\u308b\u307e\u3067\u6c17\u9577\u306b\u5f85\u3061\u307e\u3059\u3002\n[^9]: yum update \u3067\u30ab\u30fc\u30cd\u30eb\u304c\u66f4\u65b0\u3055\u308c\u3066\u3057\u307e\u3046\u3068\u3001\u518d\u8d77\u52d5\u6642\u306b\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u304c\u8aad\u307f\u8fbc\u307e\u308c\u306a\u304f\u306a\u308b\u305f\u3081\u3002 `sudo sh -c 'echo \"exclude=kernel*\" >> /etc/yum.conf'` \u306a\u3069\u3067\u5bfe\u51e6\u3057\u3066\u304a\u3044\u3066\u3082\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\n", "tags": ["AWS", "EC2", "NVIDIA", "GPU", "DeepLearning"]}