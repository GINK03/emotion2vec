{"tags": ["Caffe", "windows7"], "context": "\n\n\u6b20\u9665\u753b\u50cf\u306eCNN\u52d5\u4f5c\u30c6\u30b9\u30c8\n\n\u524d\u63d0\n\ncaffe\u306e\u5c0e\u5165\u307e\u3067\u6e08\u3093\u3067\u3044\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3002\"~/caffe-windows/Build/x64/Release\"\u306bPATH\u3092\u901a\u3059\u3002convert_imageset.exe, compute_image_mean.exe, caffe.exe\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306b\u3042\u308b\u3002\n\u6b20\u9665\u753b\u50cf\u306a\u306e\u3067\u30012\u30af\u30e9\u30b9\u5224\u5225(\u6b63\u5e38 or \u7570\u5e38)\u3002\u753b\u50cf\u306f\u5224\u5225\u5bb9\u6613\u306a\u753b\u50cf\u3092\u7528\u3044\u308b\u3002\n\u52d5\u4f5c\u30c6\u30b9\u30c8\u306a\u306e\u3067\u3001lenet\u30e2\u30c7\u30eb\u3067\u691c\u8a3c\u3002\n\n\n\u52d5\u4f5c\u74b0\u5883\nWindows 7 64bit + RAM 16GB + Cygwin(cpu\u30e2\u30fc\u30c9\u3067)\n\n\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u7f6e\ncaffe-windows/\n\u3000\u251c data/\n\u3000\u2502\u3000\u3000\u2514 mytest/\n\u3000\u2502\u3000\u3000   \u251c train.txt #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c test.txt #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c answer.txt #../../example/mytest/deploy.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mean.binaryproto #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_train_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_test_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_deploy_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c src/\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 0001.bmp\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 0002.bmp\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 (\u4ee5\u4e0b\u7701\u7565\u3002\u8a13\u7df4\u7528\uff1atrain data=>10000\u679a test data => 2000\u679a \u8a55\u4fa1\u7528:eval data => 7000\u679a\u4f5c\u6210)\n\u3000\u251c example/\n\u3000\u2502\u3000\u3000\u251c eval.sh\n\u3000\u2502\u3000\u3000\u251c train.sh\n\u3000\u2502\u3000\u3000\u2514 mytest/\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 solver.prototxt #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u306b\u4f7f\u7528\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 train_test.prototxt #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u306b\u4f7f\u7528\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 mytest_iter_100000.caffemodel #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u5f8c\u306b\u751f\u6210\u3055\u308c\u308b\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 eval.prototxt #../../example/mytest/eval.sh(\u8a55\u4fa1\u6642)\u306b\u4f7f\u7528\n\u3000\u251c Build/\n\u3000\u2502\u3000\u2514 x64/\n\u3000\u2514\u3000\u3000\u3000\u2514 Release/ #\u30d1\u30b9\u3092\u901a\u3057\u3066\u304a\u304f\u3002\n\n\n\n\u8a13\u7df4\n\n\u8a13\u7df4\u306e\u6e96\u5099\n\n\u753b\u50cf\u30c7\u30fc\u30bf(30px\u56db\u65b9)\u3092\u7528\u610f\u3057\u3066\u304a\u304f\u3002\u4eca\u56de\u306f\u3001\u30c6\u30b9\u30c8\u306e\u305f\u3081\u3001\u308f\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u7570\u5e38\u7269\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u304f\u3063\u3064\u3051\u3001\u4eba\u5de5\u7684\u306b\u4f5c\u6210\u3002\n\u753b\u50cf\u3092(\u6b63\u5e38):(\u7570\u5e38) = 1:1.1\u4f4d\u306e\u5272\u5408\u3067\u7528\u610f\u3057\u305f\u3002\u30b0\u30ec\u30fc\u753b\u50cf\u3002\n\u8a13\u7df4\u8868(train.txt, test.txt, answer.txt\u306a\u3069)\u306f\u3001\n\n12001.bmp 0\n12002.bmp 1\n12003.bmp 1\n12004.bmp 0\n12005.bmp 0\n12006.bmp 0\n12007.bmp 0\n12008.bmp 1\n12009.bmp 0\n12010.bmp 0\n(..\u4ee5\u4e0b\u7701\u7565)\n\n\u306e\u3088\u3046\u306a\u5f62\u3067\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3002\n\nsolver.prototxt\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\n\nnet: \"examples/mytest/lenet/train_test.prototxt\"\ntest_iter: 100 #NOTE that ${test_iter} * ${batch_size} = {number of test data}\ntest_interval: 500 \nbase_lr: 0.01 #adjust value\nmomentum: 0.9\nweight_decay: 0.0005\n# The learning rate policy\nlr_policy: \"inv\"\ngamma: 0.0001\npower: 0.75\n# Display every 100 iterations\ndisplay: 500\n# The maximum number of iterations\nmax_iter: 100000\n# snapshot intermediate results\nsnapshot: 5000\nsnapshot_prefix: \"examples/mytest/lenet/mytest\"\n# solver mode: CPU or GPU\nsolver_mode: CPU\n\n\ntrain_test.prototxt\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\n\nname: \"LeNet\"\nlayer {\n  name: \"mnist\"\n  type: \"Data\"\n  top: \"data\"\n  top: \"label\"\n  include {\n    phase: TRAIN\n  }\n  transform_param {\n    scale: 0.00390625 #raw_scale is 255, so, 1/255 scaling\n    mean_file: \"data/mytest/mean.binaryproto\"\n  }\n  data_param {\n    source: \"data/mytest/mytest_train_leveldb\"\n    batch_size: 100\n    backend: LEVELDB\n  }\n}\nlayer {\n  name: \"mnist\"\n  type: \"Data\"\n  top: \"data\"\n  top: \"label\"\n  include {\n    phase: TEST\n  }\n  transform_param {\n    scale: 0.00390625\n    mean_file: \"data/mytest/mean.binaryproto\"\n  }\n  data_param {\n    source: \"data/mytest/mytest_test_leveldb\"\n    batch_size: 20 # ref test_iter param of solver.prototxt.\n    backend: LEVELDB\n  }\n}\nlayer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 20\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool1\"\n  type: \"Pooling\"\n  bottom: \"conv1\"\n  top: \"pool1\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"conv2\"\n  type: \"Convolution\"\n  bottom: \"pool1\"\n  top: \"conv2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 50\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool2\"\n  type: \"Pooling\"\n  bottom: \"conv2\"\n  top: \"pool2\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"ip1\"\n  type: \"InnerProduct\"\n  bottom: \"pool2\"\n  top: \"ip1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 500\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"relu1\"\n  type: \"ReLU\"\n  bottom: \"ip1\"\n  top: \"ip1\"\n}\nlayer {\n  name: \"ip2\"\n  type: \"InnerProduct\"\n  bottom: \"ip1\"\n  top: \"ip2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 2 #NOTE that we try 2 classification(true or false), so we change num_output para from 1000 to 2\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"accuracy\"\n  type: \"Accuracy\"\n  bottom: \"ip2\"\n  bottom: \"label\"\n  top: \"accuracy\"\n  include {\n    phase: TEST\n  }\n}\nlayer {\n  name: \"loss\"\n  type: \"SoftmaxWithLoss\"\n  bottom: \"ip2\"\n  bottom: \"label\"\n  top: \"loss\"\n}\n\n\n\n\u8a13\u7df4\n\u4ee5\u4e0b\u3092terminal\u3067\u5b9f\u884c\u3002(convert_imageset\u3068\u3001compute_image_mean\u3001\u306e\u4ed5\u69d8\u306b\u3064\u3044\u3066\u306f\u3001--help\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u898b\u308c\u308b\u3002) --gray\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3001RGB\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u308b\u3002caffe-windows\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u3001\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u3002\n$ ./examples/mytest/learn.sh\nCreating leveldb...\nI1019 04:52:57.107475  9896 convert_imageset.cpp:86] A total of 10000 images.\nI1019 04:52:57.127476  9896 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_train_leveldb\nI1019 04:52:59.663730  9896 convert_imageset.cpp:144] Processed 1000 files.\nI1019 04:53:00.843848  9896 convert_imageset.cpp:144] Processed 2000 files.\nI1019 04:53:02.155979  9896 convert_imageset.cpp:144] Processed 3000 files.\nI1019 04:53:03.460110  9896 convert_imageset.cpp:144] Processed 4000 files.\nI1019 04:53:05.120276  9896 convert_imageset.cpp:144] Processed 5000 files.\nI1019 04:53:06.133378  9896 convert_imageset.cpp:144] Processed 6000 files.\nI1019 04:53:07.143478  9896 convert_imageset.cpp:144] Processed 7000 files.\nI1019 04:53:09.223686  9896 convert_imageset.cpp:144] Processed 8000 files.\nI1019 04:53:10.565820  9896 convert_imageset.cpp:144] Processed 9000 files.\nI1019 04:53:11.995964  9896 convert_imageset.cpp:144] Processed 10000 files.\nI1019 04:53:12.098973  6780 convert_imageset.cpp:86] A total of 2000 images.\nI1019 04:53:12.110975  6780 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_test_leveldb\nI1019 04:53:12.894053  6780 convert_imageset.cpp:144] Processed 1000 files.\nI1019 04:53:13.695133  6780 convert_imageset.cpp:144] Processed 2000 files.\nComputing image mean...\nDone.\nI1019 04:53:13.972162   752 caffe.cpp:179] Use CPU.\nI1019 04:53:13.973161   752 solver.cpp:48] Initializing solver from parameters:\ntest_iter: 10\ntest_interval: 500\n(..\u4e2d\u7565)\nI1019 05:16:55.221271  8968 solver.cpp:404]     Test net output #0: accuracy = 1\nI1019 05:16:55.221271  8968 solver.cpp:404]     Test net output #1: loss = 0.00398486 (* 1 = 0.00398486 loss)\nI1019 05:16:55.348284  8968 solver.cpp:228] Iteration 9500, loss = 0.000340846\nI1019 05:16:55.348284  8968 solver.cpp:244]     Train net output #0: loss = 0.000340768 (* 1 = 0.000340768 loss)\nI1019 05:16:55.348284  8968 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\nI1019 05:18:03.680117  8968 solver.cpp:454] Snapshotting to binary proto file examples/mytest/lenet/mytest_iter_10000.caffemodel\nI1019 05:18:03.696118  8968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mytest/lenet/mytest_iter_10000.solverstate\nI1019 05:18:03.783128  8968 solver.cpp:317] Iteration 10000, loss = 0.000334693\nI1019 05:18:03.783128  8968 solver.cpp:337] Iteration 10000, Testing net (#0)\nI1019 05:18:03.861135  8968 solver.cpp:404]     Test net output #0: accuracy = 1\nI1019 05:18:03.861135  8968 solver.cpp:404]     Test net output #1: loss = 0.00207814 (* 1 = 0.00207814 loss)\nI1019 05:18:03.861135  8968 solver.cpp:322] Optimization Done.\nI1019 05:18:03.861135  8968 caffe.cpp:223] Optimization Done.\n\n\n\u5224\u5225\u7c21\u6613\u306a\u753b\u50cf\u306a\u306e\u3067\u3001#0: accuracy = 1\u3067\u3059\u3002\n\ntrain.sh\u306e\u4e2d\u8eab\n\n#!/usr/bin/env sh\n# This script converts the mytest data into lmdb/leveldb format,\n# depending on the value assigned to $BACKEND.\n\nDATA=data/mytest\n#BUILD=Build/x64/Release\n\nBACKEND=\"leveldb\"\n\necho \"Creating ${BACKEND}...\"\n\nrm -rf $DATA/mytest_train_${BACKEND}\nrm -rf $DATA/mytest_test_${BACKEND}\nrm -rf $DATA/mean.binaryproto\n\n# write gray option\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/train.txt $DATA/mytest_train_${BACKEND} -backend=${BACKEND} -gray\n# write gray option\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/test.txt $DATA/mytest_test_${BACKEND} -backend=${BACKEND} -gray\n\necho \"Computing image mean...\"\n\ncompute_image_mean.exe -backend=${BACKEND} \\\n  $DATA/mytest_train_${BACKEND} $DATA/mean.binaryproto\necho \"Done.\"\n\ncaffe train --solver=examples/mytest/lenet/solver.prototxt\n\n\n\n\u8a55\u4fa1\n\u4ee5\u4e0b\u3092\u5b9f\u884c\u3002\n$ ./examples/mytest/eval.sh 7000\nCreating leveldb...\nI1019 05:20:49.808728  7768 convert_imageset.cpp:86] A total of 7000 images.\nI1019 05:20:49.819730  7768 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_deploy_leveldb\nI1019 05:20:50.637811  7768 convert_imageset.cpp:144] Processed 1000 files.\nI1019 05:20:51.431890  7768 convert_imageset.cpp:144] Processed 2000 files.\nI1019 05:20:52.224969  7768 convert_imageset.cpp:144] Processed 3000 files.\nI1019 05:20:53.003047  7768 convert_imageset.cpp:144] Processed 4000 files.\nI1019 05:20:53.798127  7768 convert_imageset.cpp:144] Processed 5000 files.\nI1019 05:20:54.590206  7768 convert_imageset.cpp:144] Processed 6000 files.\nI1019 05:20:55.389286  7768 convert_imageset.cpp:144] Processed 7000 files.\nI1019 05:20:55.488296  2020 caffe.cpp:247] Use CPU.\nI1019 05:20:55.493296  2020 net.cpp:49] Initializing net from parameters:\nname: \"LeNet\"\nstate {\n  phase: TEST\n}\nlayer {\n\n(..\u4e2d\u7565)\nI1019 05:20:15.893337  9280 caffe.cpp:276] Batch 6998, accuracy = 1\nI1019 05:20:15.893337  9280 caffe.cpp:276] Batch 6998, loss = 1.3113e-006\nI1019 05:20:15.894337  9280 caffe.cpp:276] Batch 6999, accuracy = 1\nI1019 05:20:15.894337  9280 caffe.cpp:276] Batch 6999, loss = 0.00046085\nI1019 05:20:15.894337  9280 caffe.cpp:281] Loss: 0.00181827\nI1019 05:20:15.894337  9280 caffe.cpp:293] accuracy = 1\nI1019 05:20:15.894337  9280 caffe.cpp:293] loss = 0.00181827 (* 1 = 0.00181827 loss)\n\n\nanswer.txt\u306e\u4e00\u884c\u76ee\u306e\u6b63\u89e3\u3092\u5909\u3048\u3066\u307f\u308b\u3068\u3001\u51fa\u529b\u306f\u3001\u3053\u3093\u306a\u611f\u3058\u306b\u306a\u3063\u305f\u3002\n$ ./examples/mytest/eval.sh 7000\nCreating leveldb...\nI1019 05:22:51.454891  9516 convert_imageset.cpp:86] A total of 7000 images.\nI1019 05:22:51.466892  9516 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_deploy_leveldb\nI1019 05:22:52.261972  9516 convert_imageset.cpp:144] Processed 1000 files.\nI1019 05:22:53.054051  9516 convert_imageset.cpp:144] Processed 2000 files.\nI1019 05:22:53.842130  9516 convert_imageset.cpp:144] Processed 3000 files.\nI1019 05:22:54.529199  9516 convert_imageset.cpp:144] Processed 4000 files.\nI1019 05:22:55.265272  9516 convert_imageset.cpp:144] Processed 5000 files.\nI1019 05:22:56.077353  9516 convert_imageset.cpp:144] Processed 6000 files.\nI1019 05:22:56.871433  9516 convert_imageset.cpp:144] Processed 7000 files.\nI1019 05:22:57.544500  9368 caffe.cpp:247] Use CPU.\nI1019 05:22:57.548501  9368 net.cpp:49] Initializing net from parameters:\nname: \"LeNet\"\nstate {\n  phase: TEST\n}\nlayer {\n  name: \"test0\"\n(..\u7701\u7565)\nI1019 05:22:57.598506  9368 net.cpp:219] label_test0_1_split does not need backward computation.\nI1019 05:22:57.598506  9368 net.cpp:219] test0 does not need backward computation.\nI1019 05:22:57.598506  9368 net.cpp:261] This network produces output accuracy\nI1019 05:22:57.598506  9368 net.cpp:261] This network produces output loss\nI1019 05:22:57.598506  9368 net.cpp:274] Network initialization done.\nI1019 05:22:57.604506  9368 net.cpp:752] Ignoring source layer mnist\nI1019 05:22:57.604506  9368 caffe.cpp:253] Running for 7000 iterations.\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, accuracy = 0\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, loss = 12.3964\nI1019 05:22:57.613507  9368 caffe.cpp:276] Batch 1, accuracy = 1\n(..\u4e2d\u7565)\nI1019 05:25:11.542899  9588 caffe.cpp:276] Batch 6998, accuracy = 1\nI1019 05:25:11.542899  9588 caffe.cpp:276] Batch 6998, loss = 1.3113e-006\nI1019 05:25:11.543900  9588 caffe.cpp:276] Batch 6999, accuracy = 1\nI1019 05:25:11.543900  9588 caffe.cpp:276] Batch 6999, loss = 0.00046085\nI1019 05:25:11.543900  9588 caffe.cpp:281] Loss: 0.00358918\nI1019 05:25:11.543900  9588 caffe.cpp:293] accuracy = 0.999857\nI1019 05:25:11.543900  9588 caffe.cpp:293] loss = 0.00358918 (* 1 = 0.00358918 loss)\n\n\n\u3068\u306a\u308b\u3002\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, accuracy = 0\n\n\u3068\u3042\u308b\u306e\u3067\u3001lenet\u306e\u52d5\u4f5c\u306f\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3059\u3002\n\neval.sh\u306e\u4e2d\u8eab\u306f\u4ee5\u4e0b\u3002\n\n#!/usr/bin/env sh\n# This script converts the mytest data into lmdb/leveldb format,\n# depending on the value assigned to $BACKEND.\n\nEXAMPLE=examples/mytest\nDATA=data/mytest\nBUILD=Build/x64/Release\n\n\nBACKEND=\"leveldb\"\n\necho \"Creating ${BACKEND}...\"\nrm -rf $DATA/mytest_deploy_${BACKEND}\n\n\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/answer.txt $DATA/mytest_deploy_${BACKEND} -backend=${BACKEND} -gray\n\ncaffe test -model $EXAMPLE/lenet/eval.prototxt -weights $EXAMPLE/lenet/mytest_iter_100000.caffemodel -iterations $1\n\n\neval.prototxt\u306e\u4e2d\u8eab\u306f\u4ee5\u4e0b\u3002\n\nname: \"mytest\"\nlayer {\n  name: \"data\"\n  type: \"Input\"\n  top: \"data\"\n  input_param { \n    shape: { \n        dim: 1 # the size you want to eval at a time\n        dim: 1 # number of colour channels\n        dim: 30 # width\n        dim: 30 # height\n    } \n  }\n}\nlayer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 20\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool1\"\n  type: \"Pooling\"\n  bottom: \"conv1\"\n  top: \"pool1\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"conv2\"\n  type: \"Convolution\"\n  bottom: \"pool1\"\n  top: \"conv2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 50\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool2\"\n  type: \"Pooling\"\n  bottom: \"conv2\"\n  top: \"pool2\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"ip1\"\n  type: \"InnerProduct\"\n  bottom: \"pool2\"\n  top: \"ip1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 500\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"relu1\"\n  type: \"ReLU\"\n  bottom: \"ip1\"\n  top: \"ip1\"\n}\nlayer {\n  name: \"ip2\"\n  type: \"InnerProduct\"\n  bottom: \"ip1\"\n  top: \"ip2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 2 #NOTE that we try 2 classification(true or false), so we change num_output para from 1000 to 2\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"prob\"\n  type: \"Softmax\"\n  bottom: \"ip2\"\n  top: \"prob\"\n}\n\n\n\n\u53c2\u8003\n\n\u53c2\u8003url\nhttp://tutorial.caffe.berkeleyvision.org/tutorial/interfaces.html\nhttp://qiita.com/wyamamo/items/1875561b030f7ff42617\n\ncaffe\u306e\u4ed5\u69d8\n--help\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u898b\u308c\u308b\u3088\u3002\n\nconvert_imageset\n\n  Flags from ..\\..\\tools\\convert_imageset.cpp:\n    -backend (The backend {lmdb, leveldb} for storing the result) type: string\n      default: \"lmdb\"\n    -check_size (When this option is on, check that all the datum have the same\n      size) type: bool default: false\n    -encode_type (Optional: What type should we encode the image as\n      ('png','jpg',...).) type: string default: \"\"\n    -encoded (When this option is on, the encoded image will be save in datum)\n      type: bool default: false\n    -gray (When this option is on, treat images as grayscale ones) type: bool\n      default: false\n    -resize_height (Height images are resized to) type: int32 default: 0\n    -resize_width (Width images are resized to) type: int32 default: 0\n    -shuffle (Randomly shuffle the order of images and their labels) type: bool\n      default: false\n\n\n\ncompute_image_mean\n\n  Flags from ..\\..\\tools\\compute_image_mean.cpp:\n    -backend (The backend {leveldb, lmdb} containing the images) type: string\n      default: \"lmdb\"\n\n\n\ncaffe\n\n  Flags from ..\\..\\tools\\caffe.cpp:\n    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use\n      '-gpu all' to run on all available GPUs. The effective training batch\n      size is multiplied by the number of devices.) type: string default: \"\"\n    -iterations (The number of iterations to run.) type: int32 default: 50\n    -model (The model definition protocol buffer text file.) type: string\n      default: \"\"\n    -sighup_effect (Optional; action to take when a SIGHUP signal is received:\n      snapshot, stop or none.) type: string default: \"snapshot\"\n    -sigint_effect (Optional; action to take when a SIGINT signal is received:\n      snapshot, stop or none.) type: string default: \"stop\"\n    -snapshot (Optional; the snapshot solver state to resume training.)\n      type: string default: \"\"\n    -solver (The solver definition protocol buffer text file.) type: string\n      default: \"\"\n    -weights (Optional; the pretrained weights to initialize finetuning,\n      separated by ','. Cannot be set simultaneously with snapshot.)\n      type: string default: \"\"\n\n\n# \u6b20\u9665\u753b\u50cf\u306eCNN\u52d5\u4f5c\u30c6\u30b9\u30c8\n\n## \u524d\u63d0\n+ caffe\u306e\u5c0e\u5165\u307e\u3067\u6e08\u3093\u3067\u3044\u308b\u3068\u4eee\u5b9a\u3059\u308b\u3002\"~/caffe-windows/Build/x64/Release\"\u306bPATH\u3092\u901a\u3059\u3002convert_imageset.exe, compute_image_mean.exe, caffe.exe\u306b\u3064\u3044\u3066\u306f\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306b\u3042\u308b\u3002\n+ \u6b20\u9665\u753b\u50cf\u306a\u306e\u3067\u30012\u30af\u30e9\u30b9\u5224\u5225(\u6b63\u5e38 or \u7570\u5e38)\u3002\u753b\u50cf\u306f\u5224\u5225\u5bb9\u6613\u306a\u753b\u50cf\u3092\u7528\u3044\u308b\u3002\n+ \u52d5\u4f5c\u30c6\u30b9\u30c8\u306a\u306e\u3067\u3001lenet\u30e2\u30c7\u30eb\u3067\u691c\u8a3c\u3002\n\n## \u52d5\u4f5c\u74b0\u5883\n\nWindows 7 64bit + RAM 16GB + Cygwin(cpu\u30e2\u30fc\u30c9\u3067)\n\n## \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u7f6e\n```text\ncaffe-windows/\n\u3000\u251c data/\n\u3000\u2502\u3000\u3000\u2514 mytest/\n\u3000\u2502\u3000\u3000   \u251c train.txt #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c test.txt #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c answer.txt #../../example/mytest/deploy.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mean.binaryproto #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_train_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_test_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c mytest_deploy_leveldb/ #../../example/mytest/train.sh \u3067\u4f5c\u6210\n\u3000\u2502\u3000\u3000   \u251c src/\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 0001.bmp\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 0002.bmp\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 (\u4ee5\u4e0b\u7701\u7565\u3002\u8a13\u7df4\u7528\uff1atrain data=>10000\u679a test data => 2000\u679a \u8a55\u4fa1\u7528:eval data => 7000\u679a\u4f5c\u6210)\n\u3000\u251c example/\n\u3000\u2502\u3000\u3000\u251c eval.sh\n\u3000\u2502\u3000\u3000\u251c train.sh\n\u3000\u2502\u3000\u3000\u2514 mytest/\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 solver.prototxt #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u306b\u4f7f\u7528\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 train_test.prototxt #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u306b\u4f7f\u7528\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 mytest_iter_100000.caffemodel #../../example/mytest/train.sh(\u8a13\u7df4\u6642)\u5f8c\u306b\u751f\u6210\u3055\u308c\u308b\n\u3000\u2502\u3000\u3000\u3000\u3000\u3000\u2514 eval.prototxt #../../example/mytest/eval.sh(\u8a55\u4fa1\u6642)\u306b\u4f7f\u7528\n\u3000\u251c Build/\n\u3000\u2502\u3000\u2514 x64/\n\u3000\u2514\u3000\u3000\u3000\u2514 Release/ #\u30d1\u30b9\u3092\u901a\u3057\u3066\u304a\u304f\u3002\n\n```\n\n## \u8a13\u7df4\n\n### \u8a13\u7df4\u306e\u6e96\u5099\n\n+ \u753b\u50cf\u30c7\u30fc\u30bf(30px\u56db\u65b9)\u3092\u7528\u610f\u3057\u3066\u304a\u304f\u3002\u4eca\u56de\u306f\u3001\u30c6\u30b9\u30c8\u306e\u305f\u3081\u3001\u308f\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u7570\u5e38\u7269\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u304f\u3063\u3064\u3051\u3001\u4eba\u5de5\u7684\u306b\u4f5c\u6210\u3002\n\u753b\u50cf\u3092(\u6b63\u5e38):(\u7570\u5e38) = 1:1.1\u4f4d\u306e\u5272\u5408\u3067\u7528\u610f\u3057\u305f\u3002\u30b0\u30ec\u30fc\u753b\u50cf\u3002\n\n+ \u8a13\u7df4\u8868(train.txt, test.txt, answer.txt\u306a\u3069)\u306f\u3001\n\n```text\n12001.bmp 0\n12002.bmp 1\n12003.bmp 1\n12004.bmp 0\n12005.bmp 0\n12006.bmp 0\n12007.bmp 0\n12008.bmp 1\n12009.bmp 0\n12010.bmp 0\n(..\u4ee5\u4e0b\u7701\u7565)\n```\n\u306e\u3088\u3046\u306a\u5f62\u3067\u8a18\u8ff0\u3057\u3066\u304a\u304f\u3002\n\n+ solver.prototxt\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\n\n\n```text\nnet: \"examples/mytest/lenet/train_test.prototxt\"\ntest_iter: 100 #NOTE that ${test_iter} * ${batch_size} = {number of test data}\ntest_interval: 500 \nbase_lr: 0.01 #adjust value\nmomentum: 0.9\nweight_decay: 0.0005\n# The learning rate policy\nlr_policy: \"inv\"\ngamma: 0.0001\npower: 0.75\n# Display every 100 iterations\ndisplay: 500\n# The maximum number of iterations\nmax_iter: 100000\n# snapshot intermediate results\nsnapshot: 5000\nsnapshot_prefix: \"examples/mytest/lenet/mytest\"\n# solver mode: CPU or GPU\nsolver_mode: CPU\n```\n\n+ train_test.prototxt\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\n\n```ruby\nname: \"LeNet\"\nlayer {\n  name: \"mnist\"\n  type: \"Data\"\n  top: \"data\"\n  top: \"label\"\n  include {\n    phase: TRAIN\n  }\n  transform_param {\n    scale: 0.00390625 #raw_scale is 255, so, 1/255 scaling\n\tmean_file: \"data/mytest/mean.binaryproto\"\n  }\n  data_param {\n    source: \"data/mytest/mytest_train_leveldb\"\n    batch_size: 100\n    backend: LEVELDB\n  }\n}\nlayer {\n  name: \"mnist\"\n  type: \"Data\"\n  top: \"data\"\n  top: \"label\"\n  include {\n    phase: TEST\n  }\n  transform_param {\n    scale: 0.00390625\n\tmean_file: \"data/mytest/mean.binaryproto\"\n  }\n  data_param {\n    source: \"data/mytest/mytest_test_leveldb\"\n    batch_size: 20 # ref test_iter param of solver.prototxt.\n    backend: LEVELDB\n  }\n}\nlayer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 20\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool1\"\n  type: \"Pooling\"\n  bottom: \"conv1\"\n  top: \"pool1\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"conv2\"\n  type: \"Convolution\"\n  bottom: \"pool1\"\n  top: \"conv2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 50\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool2\"\n  type: \"Pooling\"\n  bottom: \"conv2\"\n  top: \"pool2\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"ip1\"\n  type: \"InnerProduct\"\n  bottom: \"pool2\"\n  top: \"ip1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 500\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"relu1\"\n  type: \"ReLU\"\n  bottom: \"ip1\"\n  top: \"ip1\"\n}\nlayer {\n  name: \"ip2\"\n  type: \"InnerProduct\"\n  bottom: \"ip1\"\n  top: \"ip2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 2 #NOTE that we try 2 classification(true or false), so we change num_output para from 1000 to 2\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"accuracy\"\n  type: \"Accuracy\"\n  bottom: \"ip2\"\n  bottom: \"label\"\n  top: \"accuracy\"\n  include {\n    phase: TEST\n  }\n}\nlayer {\n  name: \"loss\"\n  type: \"SoftmaxWithLoss\"\n  bottom: \"ip2\"\n  bottom: \"label\"\n  top: \"loss\"\n}\n\n```\n\n### \u8a13\u7df4\n\u4ee5\u4e0b\u3092terminal\u3067\u5b9f\u884c\u3002(convert_imageset\u3068\u3001compute_image_mean\u3001\u306e\u4ed5\u69d8\u306b\u3064\u3044\u3066\u306f\u3001--help\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u898b\u308c\u308b\u3002) --gray\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3001RGB\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u308b\u3002caffe-windows\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u3001\u30b3\u30de\u30f3\u30c9\u5b9f\u884c\u3002\n\n```bash\n$ ./examples/mytest/learn.sh\nCreating leveldb...\nI1019 04:52:57.107475  9896 convert_imageset.cpp:86] A total of 10000 images.\nI1019 04:52:57.127476  9896 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_train_leveldb\nI1019 04:52:59.663730  9896 convert_imageset.cpp:144] Processed 1000 files.\nI1019 04:53:00.843848  9896 convert_imageset.cpp:144] Processed 2000 files.\nI1019 04:53:02.155979  9896 convert_imageset.cpp:144] Processed 3000 files.\nI1019 04:53:03.460110  9896 convert_imageset.cpp:144] Processed 4000 files.\nI1019 04:53:05.120276  9896 convert_imageset.cpp:144] Processed 5000 files.\nI1019 04:53:06.133378  9896 convert_imageset.cpp:144] Processed 6000 files.\nI1019 04:53:07.143478  9896 convert_imageset.cpp:144] Processed 7000 files.\nI1019 04:53:09.223686  9896 convert_imageset.cpp:144] Processed 8000 files.\nI1019 04:53:10.565820  9896 convert_imageset.cpp:144] Processed 9000 files.\nI1019 04:53:11.995964  9896 convert_imageset.cpp:144] Processed 10000 files.\nI1019 04:53:12.098973  6780 convert_imageset.cpp:86] A total of 2000 images.\nI1019 04:53:12.110975  6780 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_test_leveldb\nI1019 04:53:12.894053  6780 convert_imageset.cpp:144] Processed 1000 files.\nI1019 04:53:13.695133  6780 convert_imageset.cpp:144] Processed 2000 files.\nComputing image mean...\nDone.\nI1019 04:53:13.972162   752 caffe.cpp:179] Use CPU.\nI1019 04:53:13.973161   752 solver.cpp:48] Initializing solver from parameters:\ntest_iter: 10\ntest_interval: 500\n(..\u4e2d\u7565)\nI1019 05:16:55.221271  8968 solver.cpp:404]     Test net output #0: accuracy = 1\nI1019 05:16:55.221271  8968 solver.cpp:404]     Test net output #1: loss = 0.00398486 (* 1 = 0.00398486 loss)\nI1019 05:16:55.348284  8968 solver.cpp:228] Iteration 9500, loss = 0.000340846\nI1019 05:16:55.348284  8968 solver.cpp:244]     Train net output #0: loss = 0.000340768 (* 1 = 0.000340768 loss)\nI1019 05:16:55.348284  8968 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\nI1019 05:18:03.680117  8968 solver.cpp:454] Snapshotting to binary proto file examples/mytest/lenet/mytest_iter_10000.caffemodel\nI1019 05:18:03.696118  8968 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mytest/lenet/mytest_iter_10000.solverstate\nI1019 05:18:03.783128  8968 solver.cpp:317] Iteration 10000, loss = 0.000334693\nI1019 05:18:03.783128  8968 solver.cpp:337] Iteration 10000, Testing net (#0)\nI1019 05:18:03.861135  8968 solver.cpp:404]     Test net output #0: accuracy = 1\nI1019 05:18:03.861135  8968 solver.cpp:404]     Test net output #1: loss = 0.00207814 (* 1 = 0.00207814 loss)\nI1019 05:18:03.861135  8968 solver.cpp:322] Optimization Done.\nI1019 05:18:03.861135  8968 caffe.cpp:223] Optimization Done.\n\n```\n\u5224\u5225\u7c21\u6613\u306a\u753b\u50cf\u306a\u306e\u3067\u3001#0: accuracy = 1\u3067\u3059\u3002\n\n+ train.sh\u306e\u4e2d\u8eab\n\n```bash\n#!/usr/bin/env sh\n# This script converts the mytest data into lmdb/leveldb format,\n# depending on the value assigned to $BACKEND.\n\nDATA=data/mytest\n#BUILD=Build/x64/Release\n\nBACKEND=\"leveldb\"\n\necho \"Creating ${BACKEND}...\"\n\nrm -rf $DATA/mytest_train_${BACKEND}\nrm -rf $DATA/mytest_test_${BACKEND}\nrm -rf $DATA/mean.binaryproto\n\n# write gray option\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/train.txt $DATA/mytest_train_${BACKEND} -backend=${BACKEND} -gray\n# write gray option\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/test.txt $DATA/mytest_test_${BACKEND} -backend=${BACKEND} -gray\n  \necho \"Computing image mean...\"\n\ncompute_image_mean.exe -backend=${BACKEND} \\\n  $DATA/mytest_train_${BACKEND} $DATA/mean.binaryproto\necho \"Done.\"\n\ncaffe train --solver=examples/mytest/lenet/solver.prototxt\n\n```\n\n## \u8a55\u4fa1\n\n\n\u4ee5\u4e0b\u3092\u5b9f\u884c\u3002\n\n```bash\n$ ./examples/mytest/eval.sh 7000\nCreating leveldb...\nI1019 05:20:49.808728  7768 convert_imageset.cpp:86] A total of 7000 images.\nI1019 05:20:49.819730  7768 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_deploy_leveldb\nI1019 05:20:50.637811  7768 convert_imageset.cpp:144] Processed 1000 files.\nI1019 05:20:51.431890  7768 convert_imageset.cpp:144] Processed 2000 files.\nI1019 05:20:52.224969  7768 convert_imageset.cpp:144] Processed 3000 files.\nI1019 05:20:53.003047  7768 convert_imageset.cpp:144] Processed 4000 files.\nI1019 05:20:53.798127  7768 convert_imageset.cpp:144] Processed 5000 files.\nI1019 05:20:54.590206  7768 convert_imageset.cpp:144] Processed 6000 files.\nI1019 05:20:55.389286  7768 convert_imageset.cpp:144] Processed 7000 files.\nI1019 05:20:55.488296  2020 caffe.cpp:247] Use CPU.\nI1019 05:20:55.493296  2020 net.cpp:49] Initializing net from parameters:\nname: \"LeNet\"\nstate {\n  phase: TEST\n}\nlayer {\n\n(..\u4e2d\u7565)\nI1019 05:20:15.893337  9280 caffe.cpp:276] Batch 6998, accuracy = 1\nI1019 05:20:15.893337  9280 caffe.cpp:276] Batch 6998, loss = 1.3113e-006\nI1019 05:20:15.894337  9280 caffe.cpp:276] Batch 6999, accuracy = 1\nI1019 05:20:15.894337  9280 caffe.cpp:276] Batch 6999, loss = 0.00046085\nI1019 05:20:15.894337  9280 caffe.cpp:281] Loss: 0.00181827\nI1019 05:20:15.894337  9280 caffe.cpp:293] accuracy = 1\nI1019 05:20:15.894337  9280 caffe.cpp:293] loss = 0.00181827 (* 1 = 0.00181827 loss)\n\n```\n\nanswer.txt\u306e\u4e00\u884c\u76ee\u306e\u6b63\u89e3\u3092\u5909\u3048\u3066\u307f\u308b\u3068\u3001\u51fa\u529b\u306f\u3001\u3053\u3093\u306a\u611f\u3058\u306b\u306a\u3063\u305f\u3002\n\n```bash\n$ ./examples/mytest/eval.sh 7000\nCreating leveldb...\nI1019 05:22:51.454891  9516 convert_imageset.cpp:86] A total of 7000 images.\nI1019 05:22:51.466892  9516 db_leveldb.cpp:18] Opened leveldb data/mytest/mytest_deploy_leveldb\nI1019 05:22:52.261972  9516 convert_imageset.cpp:144] Processed 1000 files.\nI1019 05:22:53.054051  9516 convert_imageset.cpp:144] Processed 2000 files.\nI1019 05:22:53.842130  9516 convert_imageset.cpp:144] Processed 3000 files.\nI1019 05:22:54.529199  9516 convert_imageset.cpp:144] Processed 4000 files.\nI1019 05:22:55.265272  9516 convert_imageset.cpp:144] Processed 5000 files.\nI1019 05:22:56.077353  9516 convert_imageset.cpp:144] Processed 6000 files.\nI1019 05:22:56.871433  9516 convert_imageset.cpp:144] Processed 7000 files.\nI1019 05:22:57.544500  9368 caffe.cpp:247] Use CPU.\nI1019 05:22:57.548501  9368 net.cpp:49] Initializing net from parameters:\nname: \"LeNet\"\nstate {\n  phase: TEST\n}\nlayer {\n  name: \"test0\"\n(..\u7701\u7565)\nI1019 05:22:57.598506  9368 net.cpp:219] label_test0_1_split does not need backward computation.\nI1019 05:22:57.598506  9368 net.cpp:219] test0 does not need backward computation.\nI1019 05:22:57.598506  9368 net.cpp:261] This network produces output accuracy\nI1019 05:22:57.598506  9368 net.cpp:261] This network produces output loss\nI1019 05:22:57.598506  9368 net.cpp:274] Network initialization done.\nI1019 05:22:57.604506  9368 net.cpp:752] Ignoring source layer mnist\nI1019 05:22:57.604506  9368 caffe.cpp:253] Running for 7000 iterations.\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, accuracy = 0\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, loss = 12.3964\nI1019 05:22:57.613507  9368 caffe.cpp:276] Batch 1, accuracy = 1\n(..\u4e2d\u7565)\nI1019 05:25:11.542899  9588 caffe.cpp:276] Batch 6998, accuracy = 1\nI1019 05:25:11.542899  9588 caffe.cpp:276] Batch 6998, loss = 1.3113e-006\nI1019 05:25:11.543900  9588 caffe.cpp:276] Batch 6999, accuracy = 1\nI1019 05:25:11.543900  9588 caffe.cpp:276] Batch 6999, loss = 0.00046085\nI1019 05:25:11.543900  9588 caffe.cpp:281] Loss: 0.00358918\nI1019 05:25:11.543900  9588 caffe.cpp:293] accuracy = 0.999857\nI1019 05:25:11.543900  9588 caffe.cpp:293] loss = 0.00358918 (* 1 = 0.00358918 loss)\n\n```\n\n\u3068\u306a\u308b\u3002\n\n```bash\nI1019 05:22:57.608507  9368 caffe.cpp:276] Batch 0, accuracy = 0\n```\n\n\u3068\u3042\u308b\u306e\u3067\u3001lenet\u306e\u52d5\u4f5c\u306f\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3059\u3002\n\n\n\n\n+ eval.sh\u306e\u4e2d\u8eab\u306f\u4ee5\u4e0b\u3002\n\n```bash\n#!/usr/bin/env sh\n# This script converts the mytest data into lmdb/leveldb format,\n# depending on the value assigned to $BACKEND.\n\nEXAMPLE=examples/mytest\nDATA=data/mytest\nBUILD=Build/x64/Release\n\n\nBACKEND=\"leveldb\"\n\necho \"Creating ${BACKEND}...\"\nrm -rf $DATA/mytest_deploy_${BACKEND}\n\n\nconvert_imageset.exe $DATA/src/ \\\n  $DATA/answer.txt $DATA/mytest_deploy_${BACKEND} -backend=${BACKEND} -gray\n\ncaffe test -model $EXAMPLE/lenet/eval.prototxt -weights $EXAMPLE/lenet/mytest_iter_100000.caffemodel -iterations $1\n```\n\n+ eval.prototxt\u306e\u4e2d\u8eab\u306f\u4ee5\u4e0b\u3002\n\n```ruby\nname: \"mytest\"\nlayer {\n  name: \"data\"\n  type: \"Input\"\n  top: \"data\"\n  input_param { \n\tshape: { \n\t\tdim: 1 # the size you want to eval at a time\n\t\tdim: 1 # number of colour channels\n\t\tdim: 30 # width\n\t\tdim: 30 # height\n\t} \n  }\n}\nlayer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 20\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool1\"\n  type: \"Pooling\"\n  bottom: \"conv1\"\n  top: \"pool1\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"conv2\"\n  type: \"Convolution\"\n  bottom: \"pool1\"\n  top: \"conv2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 50\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"pool2\"\n  type: \"Pooling\"\n  bottom: \"conv2\"\n  top: \"pool2\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 2\n    stride: 2\n  }\n}\nlayer {\n  name: \"ip1\"\n  type: \"InnerProduct\"\n  bottom: \"pool2\"\n  top: \"ip1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 500\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"relu1\"\n  type: \"ReLU\"\n  bottom: \"ip1\"\n  top: \"ip1\"\n}\nlayer {\n  name: \"ip2\"\n  type: \"InnerProduct\"\n  bottom: \"ip1\"\n  top: \"ip2\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  inner_product_param {\n    num_output: 2 #NOTE that we try 2 classification(true or false), so we change num_output para from 1000 to 2\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\nlayer {\n  name: \"prob\"\n  type: \"Softmax\"\n  bottom: \"ip2\"\n  top: \"prob\"\n}\n\n```\n\n# \u53c2\u8003\n\n## \u53c2\u8003url\nhttp://tutorial.caffe.berkeleyvision.org/tutorial/interfaces.html\nhttp://qiita.com/wyamamo/items/1875561b030f7ff42617\n\n\n## caffe\u306e\u4ed5\u69d8\n\n--help\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u898b\u308c\u308b\u3088\u3002\n\n### convert_imageset\n>\n```text\n  Flags from ..\\..\\tools\\convert_imageset.cpp:\n    -backend (The backend {lmdb, leveldb} for storing the result) type: string\n      default: \"lmdb\"\n    -check_size (When this option is on, check that all the datum have the same\n      size) type: bool default: false\n    -encode_type (Optional: What type should we encode the image as\n      ('png','jpg',...).) type: string default: \"\"\n    -encoded (When this option is on, the encoded image will be save in datum)\n      type: bool default: false\n    -gray (When this option is on, treat images as grayscale ones) type: bool\n      default: false\n    -resize_height (Height images are resized to) type: int32 default: 0\n    -resize_width (Width images are resized to) type: int32 default: 0\n    -shuffle (Randomly shuffle the order of images and their labels) type: bool\n      default: false\n```\n\n### compute_image_mean\n\n>\n```text\n  Flags from ..\\..\\tools\\compute_image_mean.cpp:\n    -backend (The backend {leveldb, lmdb} containing the images) type: string\n      default: \"lmdb\"\n```\n\n\n### caffe\n\n>\n```text\n  Flags from ..\\..\\tools\\caffe.cpp:\n    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use\n      '-gpu all' to run on all available GPUs. The effective training batch\n      size is multiplied by the number of devices.) type: string default: \"\"\n    -iterations (The number of iterations to run.) type: int32 default: 50\n    -model (The model definition protocol buffer text file.) type: string\n      default: \"\"\n    -sighup_effect (Optional; action to take when a SIGHUP signal is received:\n      snapshot, stop or none.) type: string default: \"snapshot\"\n    -sigint_effect (Optional; action to take when a SIGINT signal is received:\n      snapshot, stop or none.) type: string default: \"stop\"\n    -snapshot (Optional; the snapshot solver state to resume training.)\n      type: string default: \"\"\n    -solver (The solver definition protocol buffer text file.) type: string\n      default: \"\"\n    -weights (Optional; the pretrained weights to initialize finetuning,\n      separated by ','. Cannot be set simultaneously with snapshot.)\n      type: string default: \"\"\n```\n"}