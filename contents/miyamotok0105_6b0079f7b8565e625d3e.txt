{"tags": ["DeepLearning", "Chainer"], "context": "\u6a5f\u68b0\u5b66\u7fd2\u3063\u3066\u3084\u308a\u3060\u3059\u3068\u4e8c\u3064\u306e\u610f\u5473\u3067\u306f\u307e\u308a\u307e\u3059\u3002\n\u71c3\u3048\u3066\u304f\u308b\u3068\u5bdd\u308c\u306a\u304f\u306a\u308a\u307e\u3059\u3002\n\u304a\u6c17\u3092\u3064\u3051\u304f\u3060\u3055\u3044\u3002\n\n\u6df1\u5c64\u5b66\u7fd2 (\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba)\u52c9\u5f37\u4f1a\n2\u7ae0\u52c9\u5f37\u4e2d\nhttp://www.slideshare.net/keiichiroumiyamoto/2-64080484\n\nChainer\u306e\u4e3b\u306a\u30af\u30e9\u30b9\n\u30fbVariable\n\u5909\u6570\u306e\u5024\u306e\u5909\u5316\u3092\u8a18\u9332\u3059\u308b\u3002\n\u30fbFunction\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7ba1\u7406\u3092\u884c\u3046\u3002\n\u30fbLink\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6301\u3064\u3002\n\u30fbChain\nLink\u3092\u307e\u3068\u3081\u305f\u3082\u306e\u3002\n\u30fbOptimizer\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u64cd\u4f5c\u3059\u308b\u3002\n\n\u521d\u5fc3\u8005\u306a\u308a\u306b\u3084\u3063\u3066\u307f\u305f\n\n\u30e6\u30cb\u30c3\u30c8\n\u30e6\u30cb\u30c3\u30c8\uff11\u500b\u3067\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\u56f3\u3092\u66f8\u304d\u305f\u304b\u3063\u305f\u3051\u3069\u3001\u975e\u5e38\u306b\u5927\u5909\u3067\u632b\u6298\u3002\n\n\uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3067\u9806\u4f1d\u64ad\n\n# -*- coding: utf-8 -*-\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import FunctionSet, Variable\nimport numpy as np\n##\u30e6\u30cb\u30c3\u30c8\u4e00\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u305f##\nmodel = FunctionSet(l1 = L.Linear(4, 1)) # # \uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3002\uff14\u3064\u306e\u5165\u529b\u3068\uff11\u3064\u306e\u51fa\u529b\u3002\nx_data = np.random.rand(1, 4) * 100 # 4\u3064\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u914d\u5217\u3092\u4f5c\u6210\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nx = Variable(x_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n\nprint(float(model.l1(x).data))\n\n\n\n\n\u9806\u4f1d\u64ad\n\u96a0\u308c\u5c64\uff11\u5c64\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3059\u3002\n784\u306e\u5165\u529b\u3067\u96a0\u308c\u5c64\u304c\uff11\uff10\uff10\uff10\u3067\u51fa\u529b\u304c\uff11\uff10\u3002\nmnist\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u3068\u3053\u308d\u304b\u3089\u629c\u304d\u53d6\u3063\u305f\u30b3\u30fc\u30c9\u306e\u307d\u3044\u3067\u3059\u3002\nsoftmax_cross_entropy\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3067\u9806\u4f1d\u64ad\u306e\u307f\u3057\u3066\u307e\u3059\u3002\n\u9006\u4f1d\u64ad\u306f\u4f55\u56de\u304b\u56de\u3057\u305f\u3044\u306e\u3067\u5f8c\u3067\u66f8\u3044\u3066\u307f\u307e\u3059\u3002\n\n\u9806\u4f1d\u64ad\nfrom chainer import FunctionSet, Variable\nimport chainer.functions as F\nimport numpy as np\n\n# \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306e\u5b9a\u7fa9\nmodel = FunctionSet(l1=F.Linear( 784, 1000),\n                    l2=F.Linear(1000, 1000),\n                    l3=F.Linear(1000, 10))\n\ndef forward(x_data, y_data):\n    x, t = Variable(x_data), Variable(y_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n\n    h1 = F.relu(model.l1(x))\n    h2 = F.relu(model.l2(h1))\n    y  = model.l3(h2)\n    return F.softmax_cross_entropy(y, t)\n\nx_data = np.random.rand(1, 784) * 100\ny_data = np.array([0])            # \u30df\u30cb\u30d0\u30c3\u30c1\u3092\u521d\u671f\u5316\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\ny_data = y_data.astype(np.int32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nloss = forward(x_data, y_data)  # \u9806\u4f1d\u64ad\n# loss.backward()                 # \u9006\u4f1d\u64ad\nprint(float((loss.data)))\n\n\n\n\n\n\u6d3b\u6027\u5316\u95a2\u6570\n\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u767a\u706b\u3055\u305b\u3066\u904a\u3093\u3067\u307f\u307e\u3057\u305f\u3002\n\u4f55\u56de\u3082\u53e9\u304f\u3068\u767a\u706b\u3057\u305f\u308a\u767a\u706b\u3057\u306a\u304b\u3063\u305f\u308a\u95a2\u6570\u306e\u9055\u3044\u304c\u3042\u3063\u3066\u304a\u3082\u3057\u308d\u3044\u3067\u3059\u3002\n\n\u6d3b\u6027\u5316\u95a2\u6570\n\n# -*- coding: utf-8 -*-\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import FunctionSet, Variable\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef test_plot(func, max):\n     xd = Variable(np.array([range(-max,max)], dtype=np.float32).T)\n     yd = func(xd)\n     plt.figure(figsize=(6,6))\n     plt.plot(xd.data,yd.data)\n     plt.show()\n\n##\u30e6\u30cb\u30c3\u30c8\u4e00\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u305f##\nmodel = FunctionSet(l1 = L.Linear(4, 1)) # # \uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3002\uff14\u3064\u306e\u5165\u529b\u3068\uff11\u3064\u306e\u51fa\u529b\u3002\nx_data = np.random.rand(1, 4) * 100 # 4\u3064\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u914d\u5217\u3092\u4f5c\u6210\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nx = Variable(x_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n\n# \u4f55\u5ea6\u3082\u5b9f\u884c\u3057\u305f\u5834\u5408\u306b\u6d3b\u6027\u5316\u95a2\u6570\u306e\u52d5\u304d\u3092\u89b3\u5bdf\u3059\u308b\u3002\n# \u6570\u5b57\u304c\u5927\u304d\u304f\u306a\u308b\u3068\u767a\u706b\u3092\u610f\u5473\u3059\u308b\u3002\nprint(float(model.l1(x).data))\nu = model.l1(x)\nz = F.sigmoid(u) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\nprint(z.data)\nz = F.tanh(u) # \u53cc\u66f2\u7dda\u6b63\u63a5\u95a2\u6570\nprint(z.data)\nz = F.relu(u) # \u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570\nprint(z.data)\n\n# \u6d3b\u6027\u5316\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3057\u3066\u307f\u308b\n# \u56f0\u3063\u305f\u6642\u306b\u304a\u99b4\u67d3\u307f\u306e\u30e9\u30f3\u30d7\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb\ntest_plot(F.relu, 10)\n# \u30b7\u30b0\u30e2\u30a4\u30c9\ntest_plot(F.sigmoid, 10)\n# \u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\ntest_plot(F.tanh, 10)\n\n\n\n\n\n\n\n\u578b\u5909\u63db\n\u6b63\u89e3\u30e9\u30d9\u30eb(\u672c\u66f8\u3060\u3068\u671b\u307e\u3057\u3044\u51fa\u529bd)\u306e\u578b\u3092\u6574\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3068\u66f8\u3044\u3066\u3042\u308a\u307e\u3057\u305f\u3002\n\u5b9f\u969b\u306b\u5909\u63db\u3092\u3057\u306a\u3044\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u3053\u3068\u304c\u4f55\u5ea6\u3082\u3042\u3063\u305f\u306e\u3067\u30e1\u30e2\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u4e0a\u306e\u65b9\u3067\u66f8\u3044\u305f[\uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3067\u9806\u4f1d\u64ad]\u3067\u3082\u30ad\u30e3\u30b9\u30c8\u3057\u306a\u3044\u3068\u30a8\u30e9\u30fc\u306b\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n\u578b\u5909\u63db\nd = d.astype(numpy.float32) # \u56de\u5e30\nd = d.astype(numpy.int32) # \u5206\u985e\n\n\n\n\u8aa4\u5dee\u95a2\u6570\ny\u3092\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3001d\u3092\u671b\u307e\u3057\u3044\u51fa\u529b\u3002\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u6b63\u89e3\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u30da\u30a2\u3067\u6e21\u3059\u3002\n\n\u8aa4\u5dee\u95a2\u6570\nloss = F.mean_squared_error(y, d) \nloss = F.softmax_cross_entropy(y, d)\n\n\n\n\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\n\u30fb\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u6574\u306e\u5c65\u6b74\n\u96a0\u308c\u5c64\u30e6\u30cb\u30c3\u30c8\u6570\u8abf\u7bc0\n\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5909\u66f4\n\u5b66\u7fd2\u56de\u6570\u306e\u5909\u66f4\n\n\u30c6\u30ad\u30b9\u30c8\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n# coding: utf-8\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom collections import defaultdict\nimport six\nimport sys\nimport chainer\nimport chainer.links as L\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport argparse\nfrom gensim import corpora, matutils\nimport MeCab\n\n\"\"\"\n\u5358\u7d14\u306aNN\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e (posi-nega)\n - \u96a0\u308c\u5c64\u306f2\u3064\n - \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306b\u306fBoW\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n @author ichiroex\n\"\"\"\n# coding: utf-8\nimport numpy as np\nfrom numpy import hstack, vsplit, hsplit, reshape\nfrom sklearn.cross_validation import train_test_split\nfrom collections import defaultdict\nimport six\nimport sys\nimport chainer\nimport chainer.links as L\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport argparse\nfrom gensim import corpora, matutils\nimport MeCab\nfrom matplotlib import pyplot as plt\n\n\n\"\"\"\n\u5358\u7d14\u306aNN\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e (posi-nega)\n - \u96a0\u308c\u5c64\u306f2\u3064\n - \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306b\u306fBoW\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n @author ichiroex\n\"\"\"\n\nclass InitData():\n\n    def __init__(self):\n        source = []\n        target = []\n        comment = []\n        word = []\n\n    def to_words(self, sentence):\n        \"\"\"\n        \u5165\u529b: '\u3059\u3079\u3066\u81ea\u5206\u306e\u307b\u3046\u3078'\n        \u51fa\u529b: tuple(['\u3059\u3079\u3066', '\u81ea\u5206', '\u306e', '\u307b\u3046', '\u3078'])\n        \"\"\"\n        tagger = MeCab.Tagger('mecabrc')  # \u5225\u306eTagger\u3092\u4f7f\u3063\u3066\u3082\u3044\u3044\n        mecab_result = tagger.parse(sentence)\n        info_of_words = mecab_result.split('\\n')\n        words = []\n        for info in info_of_words:\n            # macab\u3067\u5206\u3051\u308b\u3068\u3001\u6587\u306e\u6700\u5f8c\u306b\u2019\u2019\u304c\u3001\u305d\u306e\u624b\u524d\u306b'EOS'\u304c\u6765\u308b\n            if info == 'EOS' or info == '':\n                break\n                # info => '\u306a\\t\u52a9\u8a5e,\u7d42\u52a9\u8a5e,*,*,*,*,\u306a,\u30ca,\u30ca'\n            info_elems = info.split(',')\n            # 6\u756a\u76ee\u306b\u3001\u7121\u6d3b\u7528\u7cfb\u306e\u5358\u8a9e\u304c\u5165\u308b\u3002\u3082\u30576\u756a\u76ee\u304c'*'\u3060\u3063\u305f\u30890\u756a\u76ee\u3092\u5165\u308c\u308b\n            if info_elems[6] == '*':\n                # info_elems[0] => '\u30f4\u30a1\u30f3\u30ed\u30c3\u30b5\u30e0\\t\u540d\u8a5e'\n                words.append(info_elems[0][:-3])\n                continue\n            words.append(info_elems[6])\n        return tuple(words)\n\n    def load_data(self, fname):\n\n        source = []\n        target = []\n        comment = []\n        word = []\n        f = open(fname, \"r\")\n\n        document_list = [] #\u5404\u884c\u306b\u4e00\u6587\u66f8. \u6587\u66f8\u5185\u306e\u8981\u7d20\u306f\u5358\u8a9e\n        for l in f.readlines():\n            sample = l.strip().split(\" \", 1)        #\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092\u5206\u3051\u308b\n            # print(sample)\n            label = int(sample[0])                  #\u30e9\u30d9\u30eb\n            target.append(label)\n            # document_list.append(sample[1].split()) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n            document_list.append(self.to_words(sample[1])) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n            comment.append(sample[1])\n            word.append(self.to_words(sample[1]))\n            # print(sample[1])\n\n\n        print(document_list)\n\n        #\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n        dictionary = {}   \n        dictionary = corpora.Dictionary(document_list)\n        dictionary.filter_extremes(no_below=0, no_above=100) \n        # no_below: \u4f7f\u308f\u308c\u3066\u3044\u308b\u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n        # no_above: \u4f7f\u308f\u308c\u3066\u308b\u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n\n        #\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n        for document in document_list:\n            tmp = dictionary.doc2bow(document) #\u6587\u66f8\u3092BoW\u8868\u73fe\n            vec = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0]) \n            source.append(vec)\n\n\n        dataset = {}\n        dataset['source'] = np.array(source)\n        dataset['target'] = np.array(target)\n        dataset['comment'] = comment;\n        dataset['word'] = word;\n        print(\"------------\")\n        # print(dataset['source'])\n        # print(dataset['target'])\n        print(dataset['comment'])\n        print(\"\u5168\u30c7\u30fc\u30bf\u6570:\", len(dataset['source'])) # \u30c7\u30fc\u30bf\u306e\u6570\n        print (\"\u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u6570:\", len(dictionary.items())) # \u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u306e\u6570\n\n        return dataset, dictionary\n\n\n\n\n\nif __name__ == '__main__':\n\n    #\u5b66\u7fd2\u30b0\u30e9\u30d5\u7528\n    losses =[]\n    accuracies =[]\n\n    initdata = InitData()\n\n    #\u5f15\u6570\u306e\u8a2d\u5b9a\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu  '    , dest='gpu'        , type=int, default=0,            help='1: use gpu, 0: use cpu')\n    parser.add_argument('--data '    , dest='data'       , type=str, default='input.dat',  help='an input data file')\n    parser.add_argument('--epoch'    , dest='epoch'      , type=int, default=100,          help='number of epochs to learn')\n    parser.add_argument('--batchsize', dest='batchsize'  , type=int, default=1,           help='learning minibatch size')\n    parser.add_argument('--units'    , dest='units'      , type=int, default=100,           help='number of hidden unit')\n\n    args = parser.parse_args()\n\n    batchsize   = args.batchsize    # minibatch size\n    n_epoch     = args.epoch        # \u30a8\u30dd\u30c3\u30af\u6570(\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u56de\u6570)\n\n    # Prepare dataset\n    dataset, dictionary = initdata.load_data(args.data)\n\n    dataset['source'] = dataset['source'].astype(np.float32) #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n    dataset['target'] = dataset['target'].astype(np.int32)   #\u30e9\u30d9\u30eb\n\n    x_train, x_test, y_train, y_test, c_train, c_test, word_train, word_test = train_test_split(dataset['source'], dataset['target'], dataset['comment'], dataset['word'], test_size=0.15)\n\n    print(\"------------\")\n    print(\"x_train\", x_train)\n    print(\"x_test\", x_test)\n    # print(y_train)\n    # print(y_test)\n\n    N_test = y_test.size         # test data size\n    N = len(x_train)             # train data size\n    in_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n    print(\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570:\", N)\n\n    n_units = args.units # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n    n_label = 2          # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n    #\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n    model = chainer.Chain(l1=L.Linear(in_units, n_units),\n                          l2=L.Linear(n_units, n_units),\n                           l3=L.Linear(n_units,  n_label))\n\n    # #GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n    # if args.gpu > 0:\n        # cuda.check_cuda_available()\n        # cuda.get_device(args.gpu).use()\n        # model.to_gpu()\n        # xp = np if args.gpu <= 0 else cuda.cupy #args.gpu <= 0: use cpu, otherwise: use gpu\n\n    xp = np\n\n    batchsize = args.batchsize\n    n_epoch   = args.epoch\n\n    def forward(x, t, train=True):\n        h1 = F.relu(model.l1(x))\n        h2 = F.relu(model.l2(h1))\n        y = model.l3(h2)\n        # h1 = F.dropout(F.relu(model.l1(x)), train=True)\n        # h2 = F.dropout(F.relu(model.l2(h1)), train=True)\n        # y = model.l3(h2)\n        # h1 = F.dropout(F.relu(model.l1(x)))\n        # h2 = F.dropout(F.relu(model.l2(h1)))\n        # y = model.l3(h2)\n\n\n        # print(\"l1:\",h1.data.size)\n        # print(\"l2:\",h2.data.size)\n        # print(\"l3:\",y.data.size)\n        # print(y.data)\n        return F.softmax_cross_entropy(y, t), F.accuracy(y, t), y.data\n\n    # Setup optimizer\n    optimizer = optimizers.Adam()\n    optimizer.setup(model)\n\n    # Learning loop---------------------------------------------\n    for epoch in six.moves.range(1, n_epoch + 1):\n\n        print ('epoch', epoch)\n\n        # training---------------------------------------------\n        perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n        sum_train_loss     = 0.0\n        sum_train_accuracy = 0.0\n        for i in six.moves.range(0, N, batchsize):\n\n            #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n            x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n            t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n            c = chainer.Variable(xp.asarray(c_train[perm[i:i + batchsize]]))\n            w = chainer.Variable(xp.asarray(word_train[perm[i:i + batchsize]]))\n\n            # print(\"x_train \",x_train)\n            # print(\"x_train \",np.vsplit(x_train, 2))\n\n            model.zerograds()            # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n            loss, acc, y = forward(x, t)    # \u9806\u4f1d\u642c\n            sum_train_loss      += float(cuda.to_cpu(loss.data)) * len(t)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n            sum_train_accuracy  += float(cuda.to_cpu(acc.data )) * len(t)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n            loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n            optimizer.update()           # \u6700\u9069\u5316 \n            losses.append(loss.data) #\u8aa4\u5dee\u95a2\u6570\u30b0\u30e9\u30d5\u7528\n\n\n        print('train mean loss={}, accuracy={}'.format(\n            sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n        print(np.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n\n\n        # evaluation---------------------------------------------\n        sum_test_loss     = 0.0\n        sum_test_accuracy = 0.0\n        for i in six.moves.range(0, N_test, batchsize):\n\n            # all test data\n            x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n            t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n            c = chainer.Variable(xp.asarray(c_train[i:i + batchsize]))\n            w = chainer.Variable(xp.asarray(word_train[i:i + batchsize]))\n\n            loss, acc, y = forward(x, t, train=False)\n\n            sum_test_loss     += float(cuda.to_cpu(loss.data)) * len(t)\n            sum_test_accuracy += float(cuda.to_cpu(acc.data))  * len(t)\n            accuracies.append(acc.data) #\u6c4e\u5316\u6027\u80fd\u30b0\u30e9\u30d5\u7528\n\n        print(' test mean loss={}, accuracy={}'.format(\n            sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n        print(np.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n        print(\"-----------------------------------\")\n\n\n    #model\u3068optimizer\u3092\u4fdd\u5b58---------------------------------------------\n    print ('save the model')\n    serializers.save_npz('pn_classifier_ffnn.model', model)\n    print ('save the optimizer')\n    serializers.save_npz('pn_classifier_ffnn.state', optimizer)\n\n    plt.plot(losses, label = \"sum_train_loss\")\n    plt.plot(accuracies, label = \"sum_train_accuracy\")\n    plt.yscale('log')\n    plt.legend()\n    plt.grid(True)\n    plt.title(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.show()\n\n    # print ('load the model and optimizer')\n    # serializers.load_npz('pn_classifier_ffnn.model', model)\n    # serializers.load_npz('pn_classifier_ffnn.state', optimizer)\n\n\n\n\u307e\u3060\u554f\u984c\u3068\u3057\u3066\u6c4e\u5316\u80fd\u529b\u6e2c\u5b9a\u304c\u6b8b\u3063\u3066\u3044\u307e\u3059\u3002\n\u30fc\u30fc\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30fc\u30fc\n\u7591\u554f\u6587\u30921 \n\u666e\u901a\u306e\u6587\u30920 \n\u3010input.dat\u3011\n\n0 \u660e\u65e5\u5929\u6c17\u826f\u3051\u308c\u3070\u3044\u3044\u306d\u30fc\n0 \u5929\u6c17\u306e\u3044\u3044\u65e5\u306f\u30d0\u30fc\u30d9\u30ad\u30e5\u30fc\u3067\u3057\u3087\u3002\n0 \u6885\u96e8\u306e\u6642\u671f\u306b\u306f\u96e8\u304c\u964d\u3089\u306a\u3044\u3068\u3001\u6c34\u4e0d\u8db3\u306b\u306a\u308b\u3002\n0 \u6e29\u6696\u5316\u306e\u5f71\u97ff\u3067\u65e5\u672c\u306f\u6691\u304f\u306a\u3063\u3066\u3044\u308b\u3002\n0 \u5929\u6c17\u304c\u3044\u3044\u65e5\u306f\u30cf\u30a4\u30ad\u30f3\u30b0\u3067\u3057\u3087\u3002\n0 \u5929\u6c17\u304c\u3044\u3044\u65e5\u306f\u30a6\u30a9\u30fc\u30ad\u30f3\u30b0\u3088\u306d\u3002\n\n\n\u3088\u304f\u3042\u308b\u30a8\u30e9\u30fc\nExpect: in_types[0].ndim >= 2\nActual: 0 < 2\n\n\u5165\u529bx,d\u306e\u6b21\u5143\u6570\u304c\u3042\u3063\u3066\u3044\u306a\u3044\u3002\nActual: 0(\u5b9f\u969b\u306e\u5165\u529b\u6b21\u5143\u6570) < 2(\u671f\u5f85\u3059\u308b\u6b21\u5143\u6570)\n\uff08\u5bfe\u5fdc\u4f8b\uff09\nX = chainer.Variable(np.array(x[0], dtype=np.float32))\n \u2193\nX = chainer.Variable(np.atleast_2d(np.array(x[0], dtype=np.float32)))\n\n\u203b\u6bce\u9031\u66f4\u65b0\u4e88\u5b9a\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\nhttps://github.com/miyamotok0105/MachineLearning\n\n\u53c2\u8003\u30da\u30fc\u30b8\n\u304a\u52c9\u5f37\u3092\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\nhttp://pip-install-deeplearning.hatenadiary.jp/entry/2016/01/06/141009\n\u6a5f\u68b0\u5b66\u7fd2\u3063\u3066\u3084\u308a\u3060\u3059\u3068\u4e8c\u3064\u306e\u610f\u5473\u3067\u306f\u307e\u308a\u307e\u3059\u3002\n\u71c3\u3048\u3066\u304f\u308b\u3068\u5bdd\u308c\u306a\u304f\u306a\u308a\u307e\u3059\u3002\n\u304a\u6c17\u3092\u3064\u3051\u304f\u3060\u3055\u3044\u3002\n\n#\u6df1\u5c64\u5b66\u7fd2 (\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba)\u52c9\u5f37\u4f1a\n2\u7ae0\u52c9\u5f37\u4e2d\nhttp://www.slideshare.net/keiichiroumiyamoto/2-64080484\n\n\n#Chainer\u306e\u4e3b\u306a\u30af\u30e9\u30b9\n\u30fbVariable\n\u5909\u6570\u306e\u5024\u306e\u5909\u5316\u3092\u8a18\u9332\u3059\u308b\u3002\n\u30fbFunction\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u7ba1\u7406\u3092\u884c\u3046\u3002\n\u30fbLink\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6301\u3064\u3002\n\u30fbChain\nLink\u3092\u307e\u3068\u3081\u305f\u3082\u306e\u3002\n\u30fbOptimizer\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u64cd\u4f5c\u3059\u308b\u3002\n\n#\u521d\u5fc3\u8005\u306a\u308a\u306b\u3084\u3063\u3066\u307f\u305f\n\n\n## \u30e6\u30cb\u30c3\u30c8\n\u30e6\u30cb\u30c3\u30c8\uff11\u500b\u3067\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\u56f3\u3092\u66f8\u304d\u305f\u304b\u3063\u305f\u3051\u3069\u3001\u975e\u5e38\u306b\u5927\u5909\u3067\u632b\u6298\u3002\n\n```py:\uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3067\u9806\u4f1d\u64ad\n\n# -*- coding: utf-8 -*-\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import FunctionSet, Variable\nimport numpy as np\n##\u30e6\u30cb\u30c3\u30c8\u4e00\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u305f##\nmodel = FunctionSet(l1 = L.Linear(4, 1)) # # \uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3002\uff14\u3064\u306e\u5165\u529b\u3068\uff11\u3064\u306e\u51fa\u529b\u3002\nx_data = np.random.rand(1, 4) * 100 # 4\u3064\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u914d\u5217\u3092\u4f5c\u6210\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nx = Variable(x_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n\nprint(float(model.l1(x).data))\n\n```\n\n##\u9806\u4f1d\u64ad\n\n\n\u96a0\u308c\u5c64\uff11\u5c64\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3059\u3002\n784\u306e\u5165\u529b\u3067\u96a0\u308c\u5c64\u304c\uff11\uff10\uff10\uff10\u3067\u51fa\u529b\u304c\uff11\uff10\u3002\nmnist\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u3068\u3053\u308d\u304b\u3089\u629c\u304d\u53d6\u3063\u305f\u30b3\u30fc\u30c9\u306e\u307d\u3044\u3067\u3059\u3002\nsoftmax_cross_entropy\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3067\u9806\u4f1d\u64ad\u306e\u307f\u3057\u3066\u307e\u3059\u3002\n\u9006\u4f1d\u64ad\u306f\u4f55\u56de\u304b\u56de\u3057\u305f\u3044\u306e\u3067\u5f8c\u3067\u66f8\u3044\u3066\u307f\u307e\u3059\u3002\n\n```py:\u9806\u4f1d\u64ad\nfrom chainer import FunctionSet, Variable\nimport chainer.functions as F\nimport numpy as np\n\n# \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306e\u5b9a\u7fa9\nmodel = FunctionSet(l1=F.Linear( 784, 1000),\n                    l2=F.Linear(1000, 1000),\n                    l3=F.Linear(1000, 10))\n\ndef forward(x_data, y_data):\n    x, t = Variable(x_data), Variable(y_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n    \n    h1 = F.relu(model.l1(x))\n    h2 = F.relu(model.l2(h1))\n    y  = model.l3(h2)\n    return F.softmax_cross_entropy(y, t)\n \nx_data = np.random.rand(1, 784) * 100\ny_data = np.array([0])            # \u30df\u30cb\u30d0\u30c3\u30c1\u3092\u521d\u671f\u5316\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\ny_data = y_data.astype(np.int32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nloss = forward(x_data, y_data)  # \u9806\u4f1d\u64ad\n# loss.backward()                 # \u9006\u4f1d\u64ad\nprint(float((loss.data)))\n\n\n```\n\n\n##\u6d3b\u6027\u5316\u95a2\u6570\n\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u767a\u706b\u3055\u305b\u3066\u904a\u3093\u3067\u307f\u307e\u3057\u305f\u3002\n\u4f55\u56de\u3082\u53e9\u304f\u3068\u767a\u706b\u3057\u305f\u308a\u767a\u706b\u3057\u306a\u304b\u3063\u305f\u308a\u95a2\u6570\u306e\u9055\u3044\u304c\u3042\u3063\u3066\u304a\u3082\u3057\u308d\u3044\u3067\u3059\u3002\n\n```py:\u6d3b\u6027\u5316\u95a2\u6570\n\n# -*- coding: utf-8 -*-\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport chainer.links as L\nfrom chainer import FunctionSet, Variable\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef test_plot(func, max):\n     xd = Variable(np.array([range(-max,max)], dtype=np.float32).T)\n     yd = func(xd)\n     plt.figure(figsize=(6,6))\n     plt.plot(xd.data,yd.data)\n     plt.show()\n\n##\u30e6\u30cb\u30c3\u30c8\u4e00\u3064\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u305f##\nmodel = FunctionSet(l1 = L.Linear(4, 1)) # # \uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3002\uff14\u3064\u306e\u5165\u529b\u3068\uff11\u3064\u306e\u51fa\u529b\u3002\nx_data = np.random.rand(1, 4) * 100 # 4\u3064\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u914d\u5217\u3092\u4f5c\u6210\nx_data = x_data.astype(np.float32) # \u5909\u63db\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u3063\u305f\nx = Variable(x_data) # Variable\u306f\u30ad\u30e3\u30b9\u30c8\n\n# \u4f55\u5ea6\u3082\u5b9f\u884c\u3057\u305f\u5834\u5408\u306b\u6d3b\u6027\u5316\u95a2\u6570\u306e\u52d5\u304d\u3092\u89b3\u5bdf\u3059\u308b\u3002\n# \u6570\u5b57\u304c\u5927\u304d\u304f\u306a\u308b\u3068\u767a\u706b\u3092\u610f\u5473\u3059\u308b\u3002\nprint(float(model.l1(x).data))\nu = model.l1(x)\nz = F.sigmoid(u) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\nprint(z.data)\nz = F.tanh(u) # \u53cc\u66f2\u7dda\u6b63\u63a5\u95a2\u6570\nprint(z.data)\nz = F.relu(u) # \u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570\nprint(z.data)\n\n# \u6d3b\u6027\u5316\u95a2\u6570\u306e\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3057\u3066\u307f\u308b\n# \u56f0\u3063\u305f\u6642\u306b\u304a\u99b4\u67d3\u307f\u306e\u30e9\u30f3\u30d7\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb\ntest_plot(F.relu, 10)\n# \u30b7\u30b0\u30e2\u30a4\u30c9\ntest_plot(F.sigmoid, 10)\n# \u30bf\u30f3\u30b8\u30a7\u30f3\u30c8\ntest_plot(F.tanh, 10)\n\n\n\n\n```\n\n\n##\u578b\u5909\u63db\n\u6b63\u89e3\u30e9\u30d9\u30eb(\u672c\u66f8\u3060\u3068\u671b\u307e\u3057\u3044\u51fa\u529bd)\u306e\u578b\u3092\u6574\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3068\u66f8\u3044\u3066\u3042\u308a\u307e\u3057\u305f\u3002\n\u5b9f\u969b\u306b\u5909\u63db\u3092\u3057\u306a\u3044\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u3053\u3068\u304c\u4f55\u5ea6\u3082\u3042\u3063\u305f\u306e\u3067\u30e1\u30e2\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\u4e0a\u306e\u65b9\u3067\u66f8\u3044\u305f[\uff11\u3064\u306e\u30e6\u30cb\u30c3\u30c8\u3067\u9806\u4f1d\u64ad]\u3067\u3082\u30ad\u30e3\u30b9\u30c8\u3057\u306a\u3044\u3068\u30a8\u30e9\u30fc\u306b\u306a\u3063\u3066\u3044\u307e\u3057\u305f\u3002\n\n```py:\u578b\u5909\u63db\nd = d.astype(numpy.float32) # \u56de\u5e30\nd = d.astype(numpy.int32) # \u5206\u985e\n```\n\n\n##\u8aa4\u5dee\u95a2\u6570\ny\u3092\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3001d\u3092\u671b\u307e\u3057\u3044\u51fa\u529b\u3002\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u6b63\u89e3\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u30da\u30a2\u3067\u6e21\u3059\u3002\n\n```py:\u8aa4\u5dee\u95a2\u6570\nloss = F.mean_squared_error(y, d) \nloss = F.softmax_cross_entropy(y, d)\n```\n\n##\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\n\u30fb\u30d1\u30e9\u30e1\u30fc\u30bf\u8abf\u6574\u306e\u5c65\u6b74\n\u96a0\u308c\u5c64\u30e6\u30cb\u30c3\u30c8\u6570\u8abf\u7bc0\n\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5909\u66f4\n\u5b66\u7fd2\u56de\u6570\u306e\u5909\u66f4\n\n```py:\u30c6\u30ad\u30b9\u30c8\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n# coding: utf-8\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom collections import defaultdict\nimport six\nimport sys\nimport chainer\nimport chainer.links as L\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport argparse\nfrom gensim import corpora, matutils\nimport MeCab\n\n\"\"\"\n\u5358\u7d14\u306aNN\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e (posi-nega)\n - \u96a0\u308c\u5c64\u306f2\u3064\n - \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306b\u306fBoW\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n @author ichiroex\n\"\"\"\n# coding: utf-8\nimport numpy as np\nfrom numpy import hstack, vsplit, hsplit, reshape\nfrom sklearn.cross_validation import train_test_split\nfrom collections import defaultdict\nimport six\nimport sys\nimport chainer\nimport chainer.links as L\nfrom chainer import optimizers, cuda, serializers\nimport chainer.functions as F\nimport argparse\nfrom gensim import corpora, matutils\nimport MeCab\nfrom matplotlib import pyplot as plt\n\n\n\"\"\"\n\u5358\u7d14\u306aNN\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e (posi-nega)\n - \u96a0\u308c\u5c64\u306f2\u3064\n - \u6587\u66f8\u30d9\u30af\u30c8\u30eb\u306b\u306fBoW\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n @author ichiroex\n\"\"\"\n\nclass InitData():\n\n    def __init__(self):\n        source = []\n        target = []\n        comment = []\n        word = []\n\n    def to_words(self, sentence):\n        \"\"\"\n        \u5165\u529b: '\u3059\u3079\u3066\u81ea\u5206\u306e\u307b\u3046\u3078'\n        \u51fa\u529b: tuple(['\u3059\u3079\u3066', '\u81ea\u5206', '\u306e', '\u307b\u3046', '\u3078'])\n        \"\"\"\n        tagger = MeCab.Tagger('mecabrc')  # \u5225\u306eTagger\u3092\u4f7f\u3063\u3066\u3082\u3044\u3044\n        mecab_result = tagger.parse(sentence)\n        info_of_words = mecab_result.split('\\n')\n        words = []\n        for info in info_of_words:\n            # macab\u3067\u5206\u3051\u308b\u3068\u3001\u6587\u306e\u6700\u5f8c\u306b\u2019\u2019\u304c\u3001\u305d\u306e\u624b\u524d\u306b'EOS'\u304c\u6765\u308b\n            if info == 'EOS' or info == '':\n                break\n                # info => '\u306a\\t\u52a9\u8a5e,\u7d42\u52a9\u8a5e,*,*,*,*,\u306a,\u30ca,\u30ca'\n            info_elems = info.split(',')\n            # 6\u756a\u76ee\u306b\u3001\u7121\u6d3b\u7528\u7cfb\u306e\u5358\u8a9e\u304c\u5165\u308b\u3002\u3082\u30576\u756a\u76ee\u304c'*'\u3060\u3063\u305f\u30890\u756a\u76ee\u3092\u5165\u308c\u308b\n            if info_elems[6] == '*':\n                # info_elems[0] => '\u30f4\u30a1\u30f3\u30ed\u30c3\u30b5\u30e0\\t\u540d\u8a5e'\n                words.append(info_elems[0][:-3])\n                continue\n            words.append(info_elems[6])\n        return tuple(words)\n\n    def load_data(self, fname):\n\n        source = []\n        target = []\n        comment = []\n        word = []\n        f = open(fname, \"r\")\n\n        document_list = [] #\u5404\u884c\u306b\u4e00\u6587\u66f8. \u6587\u66f8\u5185\u306e\u8981\u7d20\u306f\u5358\u8a9e\n        for l in f.readlines():\n            sample = l.strip().split(\" \", 1)        #\u30e9\u30d9\u30eb\u3068\u5358\u8a9e\u5217\u3092\u5206\u3051\u308b\n            # print(sample)\n            label = int(sample[0])                  #\u30e9\u30d9\u30eb\n            target.append(label)\n            # document_list.append(sample[1].split()) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n            document_list.append(self.to_words(sample[1])) #\u5358\u8a9e\u5206\u5272\u3057\u3066\u6587\u66f8\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\n            comment.append(sample[1])\n            word.append(self.to_words(sample[1]))\n            # print(sample[1])\n\n\n        print(document_list)\n\n        #\u5358\u8a9e\u8f9e\u66f8\u3092\u4f5c\u6210\n        dictionary = {}   \n        dictionary = corpora.Dictionary(document_list)\n        dictionary.filter_extremes(no_below=0, no_above=100) \n        # no_below: \u4f7f\u308f\u308c\u3066\u3044\u308b\u6587\u66f8\u304cno_below\u500b\u4ee5\u4e0b\u306e\u5358\u8a9e\u3092\u7121\u8996\n        # no_above: \u4f7f\u308f\u308c\u3066\u308b\u6587\u7ae0\u306e\u5272\u5408\u304cno_above\u4ee5\u4e0a\u306e\u5834\u5408\u7121\u8996\n        \n        #\u6587\u66f8\u306e\u30d9\u30af\u30c8\u30eb\u5316\n        for document in document_list:\n            tmp = dictionary.doc2bow(document) #\u6587\u66f8\u3092BoW\u8868\u73fe\n            vec = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0]) \n            source.append(vec)\n\n\n        dataset = {}\n        dataset['source'] = np.array(source)\n        dataset['target'] = np.array(target)\n        dataset['comment'] = comment;\n        dataset['word'] = word;\n        print(\"------------\")\n        # print(dataset['source'])\n        # print(dataset['target'])\n        print(dataset['comment'])\n        print(\"\u5168\u30c7\u30fc\u30bf\u6570:\", len(dataset['source'])) # \u30c7\u30fc\u30bf\u306e\u6570\n        print (\"\u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u6570:\", len(dictionary.items())) # \u8f9e\u66f8\u306b\u767b\u9332\u3055\u308c\u305f\u5358\u8a9e\u306e\u6570\n\n        return dataset, dictionary\n\n\n\n\n\nif __name__ == '__main__':\n\n    #\u5b66\u7fd2\u30b0\u30e9\u30d5\u7528\n    losses =[]\n    accuracies =[]\n\n    initdata = InitData()\n\n    #\u5f15\u6570\u306e\u8a2d\u5b9a\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu  '    , dest='gpu'        , type=int, default=0,            help='1: use gpu, 0: use cpu')\n    parser.add_argument('--data '    , dest='data'       , type=str, default='input.dat',  help='an input data file')\n    parser.add_argument('--epoch'    , dest='epoch'      , type=int, default=100,          help='number of epochs to learn')\n    parser.add_argument('--batchsize', dest='batchsize'  , type=int, default=1,           help='learning minibatch size')\n    parser.add_argument('--units'    , dest='units'      , type=int, default=100,           help='number of hidden unit')\n\n    args = parser.parse_args()\n\n    batchsize   = args.batchsize    # minibatch size\n    n_epoch     = args.epoch        # \u30a8\u30dd\u30c3\u30af\u6570(\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u56de\u6570)\n\n    # Prepare dataset\n    dataset, dictionary = initdata.load_data(args.data)\n\n    dataset['source'] = dataset['source'].astype(np.float32) #\u6587\u66f8\u30d9\u30af\u30c8\u30eb\n    dataset['target'] = dataset['target'].astype(np.int32)   #\u30e9\u30d9\u30eb\n\n    x_train, x_test, y_train, y_test, c_train, c_test, word_train, word_test = train_test_split(dataset['source'], dataset['target'], dataset['comment'], dataset['word'], test_size=0.15)\n\n    print(\"------------\")\n    print(\"x_train\", x_train)\n    print(\"x_test\", x_test)\n    # print(y_train)\n    # print(y_test)\n\n    N_test = y_test.size         # test data size\n    N = len(x_train)             # train data size\n    in_units = x_train.shape[1]  # \u5165\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570 (\u8a9e\u5f59\u6570)\n    print(\"\u5b66\u7fd2\u30c7\u30fc\u30bf\u6570:\", N)\n\n    n_units = args.units # \u96a0\u308c\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n    n_label = 2          # \u51fa\u529b\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n\n    #\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n    model = chainer.Chain(l1=L.Linear(in_units, n_units),\n                          l2=L.Linear(n_units, n_units),\n                           l3=L.Linear(n_units,  n_label))\n\n    # #GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n    # if args.gpu > 0:\n        # cuda.check_cuda_available()\n        # cuda.get_device(args.gpu).use()\n        # model.to_gpu()\n        # xp = np if args.gpu <= 0 else cuda.cupy #args.gpu <= 0: use cpu, otherwise: use gpu\n\n    xp = np\n\n    batchsize = args.batchsize\n    n_epoch   = args.epoch\n\n    def forward(x, t, train=True):\n        h1 = F.relu(model.l1(x))\n        h2 = F.relu(model.l2(h1))\n        y = model.l3(h2)\n        # h1 = F.dropout(F.relu(model.l1(x)), train=True)\n        # h2 = F.dropout(F.relu(model.l2(h1)), train=True)\n        # y = model.l3(h2)\n        # h1 = F.dropout(F.relu(model.l1(x)))\n        # h2 = F.dropout(F.relu(model.l2(h1)))\n        # y = model.l3(h2)\n\n\n        # print(\"l1:\",h1.data.size)\n        # print(\"l2:\",h2.data.size)\n        # print(\"l3:\",y.data.size)\n        # print(y.data)\n        return F.softmax_cross_entropy(y, t), F.accuracy(y, t), y.data\n\n    # Setup optimizer\n    optimizer = optimizers.Adam()\n    optimizer.setup(model)\n\n    # Learning loop---------------------------------------------\n    for epoch in six.moves.range(1, n_epoch + 1):\n\n        print ('epoch', epoch)\n\n        # training---------------------------------------------\n        perm = np.random.permutation(N) #\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5217\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\n        sum_train_loss     = 0.0\n        sum_train_accuracy = 0.0\n        for i in six.moves.range(0, N, batchsize):\n\n            #perm \u3092\u4f7f\u3044 x_train, y_train\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u9078\u629e (\u6bce\u56de\u5bfe\u8c61\u3068\u306a\u308b\u30c7\u30fc\u30bf\u306f\u7570\u306a\u308b)\n            x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]])) #source\n            t = chainer.Variable(xp.asarray(y_train[perm[i:i + batchsize]])) #target\n            c = chainer.Variable(xp.asarray(c_train[perm[i:i + batchsize]]))\n            w = chainer.Variable(xp.asarray(word_train[perm[i:i + batchsize]]))\n\n            # print(\"x_train \",x_train)\n            # print(\"x_train \",np.vsplit(x_train, 2))\n\n            model.zerograds()            # \u52fe\u914d\u3092\u30bc\u30ed\u521d\u671f\u5316\n            loss, acc, y = forward(x, t)    # \u9806\u4f1d\u642c\n            sum_train_loss      += float(cuda.to_cpu(loss.data)) * len(t)   # \u5e73\u5747\u8aa4\u5dee\u8a08\u7b97\u7528\n            sum_train_accuracy  += float(cuda.to_cpu(acc.data )) * len(t)   # \u5e73\u5747\u6b63\u89e3\u7387\u8a08\u7b97\u7528\n            loss.backward()              # \u8aa4\u5dee\u9006\u4f1d\u64ad\n            optimizer.update()           # \u6700\u9069\u5316 \n            losses.append(loss.data) #\u8aa4\u5dee\u95a2\u6570\u30b0\u30e9\u30d5\u7528\n\n\n        print('train mean loss={}, accuracy={}'.format(\n            sum_train_loss / N, sum_train_accuracy / N)) #\u5e73\u5747\u8aa4\u5dee\n        print(np.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n        \n\n        # evaluation---------------------------------------------\n        sum_test_loss     = 0.0\n        sum_test_accuracy = 0.0\n        for i in six.moves.range(0, N_test, batchsize):\n\n            # all test data\n            x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n            t = chainer.Variable(xp.asarray(y_test[i:i + batchsize]))\n            c = chainer.Variable(xp.asarray(c_train[i:i + batchsize]))\n            w = chainer.Variable(xp.asarray(word_train[i:i + batchsize]))\n\n            loss, acc, y = forward(x, t, train=False)\n\n            sum_test_loss     += float(cuda.to_cpu(loss.data)) * len(t)\n            sum_test_accuracy += float(cuda.to_cpu(acc.data))  * len(t)\n            accuracies.append(acc.data) #\u6c4e\u5316\u6027\u80fd\u30b0\u30e9\u30d5\u7528\n\n        print(' test mean loss={}, accuracy={}'.format(\n            sum_test_loss / N_test, sum_test_accuracy / N_test)) #\u5e73\u5747\u8aa4\u5dee\n        print(np.argmax(y), y, c.data, w.data) #\u51fa\u529b\u7d50\u679c\u4e00\u89a7\n        print(\"-----------------------------------\")\n\n\n    #model\u3068optimizer\u3092\u4fdd\u5b58---------------------------------------------\n    print ('save the model')\n    serializers.save_npz('pn_classifier_ffnn.model', model)\n    print ('save the optimizer')\n    serializers.save_npz('pn_classifier_ffnn.state', optimizer)\n\n    plt.plot(losses, label = \"sum_train_loss\")\n    plt.plot(accuracies, label = \"sum_train_accuracy\")\n    plt.yscale('log')\n    plt.legend()\n    plt.grid(True)\n    plt.title(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.show()\n\n    # print ('load the model and optimizer')\n    # serializers.load_npz('pn_classifier_ffnn.model', model)\n    # serializers.load_npz('pn_classifier_ffnn.state', optimizer)\n\n```\n\n\u307e\u3060\u554f\u984c\u3068\u3057\u3066\u6c4e\u5316\u80fd\u529b\u6e2c\u5b9a\u304c\u6b8b\u3063\u3066\u3044\u307e\u3059\u3002\n\n\n\n```\n\u30fc\u30fc\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30fc\u30fc\n\u7591\u554f\u6587\u30921 \n\u666e\u901a\u306e\u6587\u30920 \n\u3010input.dat\u3011\n\n0 \u660e\u65e5\u5929\u6c17\u826f\u3051\u308c\u3070\u3044\u3044\u306d\u30fc\n0 \u5929\u6c17\u306e\u3044\u3044\u65e5\u306f\u30d0\u30fc\u30d9\u30ad\u30e5\u30fc\u3067\u3057\u3087\u3002\n0 \u6885\u96e8\u306e\u6642\u671f\u306b\u306f\u96e8\u304c\u964d\u3089\u306a\u3044\u3068\u3001\u6c34\u4e0d\u8db3\u306b\u306a\u308b\u3002\n0 \u6e29\u6696\u5316\u306e\u5f71\u97ff\u3067\u65e5\u672c\u306f\u6691\u304f\u306a\u3063\u3066\u3044\u308b\u3002\n0 \u5929\u6c17\u304c\u3044\u3044\u65e5\u306f\u30cf\u30a4\u30ad\u30f3\u30b0\u3067\u3057\u3087\u3002\n0 \u5929\u6c17\u304c\u3044\u3044\u65e5\u306f\u30a6\u30a9\u30fc\u30ad\u30f3\u30b0\u3088\u306d\u3002\n```\n\n\n\n#\u3088\u304f\u3042\u308b\u30a8\u30e9\u30fc\n\n```\nExpect: in_types[0].ndim >= 2\nActual: 0 < 2\n```\n\n\u5165\u529bx,d\u306e\u6b21\u5143\u6570\u304c\u3042\u3063\u3066\u3044\u306a\u3044\u3002\nActual: 0(\u5b9f\u969b\u306e\u5165\u529b\u6b21\u5143\u6570) < 2(\u671f\u5f85\u3059\u308b\u6b21\u5143\u6570)\n\n```\n\uff08\u5bfe\u5fdc\u4f8b\uff09\nX = chainer.Variable(np.array(x[0], dtype=np.float32))\n \u2193\nX = chainer.Variable(np.atleast_2d(np.array(x[0], dtype=np.float32)))\n```\n\n\n\n\n\n\n\n\n\u203b\u6bce\u9031\u66f4\u65b0\u4e88\u5b9a\n\n\n#\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\nhttps://github.com/miyamotok0105/MachineLearning\n#\u53c2\u8003\u30da\u30fc\u30b8\n\u304a\u52c9\u5f37\u3092\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\nhttp://pip-install-deeplearning.hatenadiary.jp/entry/2016/01/06/141009\n"}