{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n\n\n\u95a2\u9023\u3000http://qiita.com/7of9/items/b364d897b95476a30754\nsine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias\u3092\u3082\u3068\u306b\u81ea\u5206\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u73fe\u3057\u3066\u51fa\u529b\u3092\u8a08\u7b97\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\nhttp://qiita.com/7of9/items/b7d38e174d4052b74cae\n\u306e\u7d9a\u304d\u3002\nTensorFlow\u306e\u51e6\u7406\u3067\u51fa\u529b\u3057\u305fweight, bias\u3092\u8aad\u307f\u8fbc\u3093\u3060\u5225python\u30d7\u30ed\u30b0\u30e9\u30e0\u3067sine curve\u306e\u518d\u73fe\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u3002\n\ninput.csv\u751f\u6210\nhttp://qiita.com/7of9/items/b364d897b95476a30754#\u30c7\u30fc\u30bf\u751f\u6210\u90e8\n\ncode (sigmoid_onlyHidden.py)\nTensorFlow\u5074\u3067weight, bias\u306e\u51fa\u529b\u4f4d\u7f6e(model_variables = slim.get_model_variables()\u4ee5\u4e0b3\u884c)\u3092\u4e0b\u306e\u65b9\u306b\u5909\u66f4\u3057\u3066\u307f\u305f\u3002\ntry\u306e\u4e2d\u3067\u51fa\u529b\u3059\u308b\u3068\u3001\u672c\u6765\u6b32\u3057\u3044weight, bias\u3092\u51fa\u529b\u3067\u304d\u3066\u3044\u306a\u3044\u306e\u3067\u306f\u3001\u3068\u3044\u3046\u8003\u3048\u3002\n\nsigmoid_onlyHidden.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# prase CSV\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## network\nhiddens = slim.stack(input_ph, slim.fully_connected, [7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n#prediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=None, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n      inpbt, outbt = sess.run([inputs_batch, output_batch])\n      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inpbt, output_ph: outbt})\n\n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n\n#    # output to npy \n#    model_variables = slim.get_model_variables()\n#    res = sess.run(model_variables)\n#    np.save('model_variables.npy', res)\n\n  finally:\n    coord.request_stop()\n\n\n  # output to npy \n  model_variables = slim.get_model_variables()\n  res = sess.run(model_variables)\n  np.save('model_variables.npy', res)\n\n\n#output trained curve\n  print 'output' # used to separate from above lines (grep -A 200 output [outfile])\n  for loop in range(10):\n    inpbt, outbt = sess.run([inputs_batch, output_batch])\n    pred = sess.run([prediction], feed_dict={input_ph:inpbt, output_ph: outbt})\n    for din,dout in zip(inpbt, pred[0]):\n      print '%.5f,%.5f' % (din,dout)\n\n\n  coord.join(threads)\n\n\n\nmodel_variables.npy \u306e\u4f5c\u308a\u76f4\u3057\n\n\u5b9f\u884c\n$ python sigmoid_onlyHidden.py\n\n\nTensorFlow Prediction\u30d5\u30a1\u30a4\u30eb(res.161210_1958.cut)\u3092\u4f5c\u308b\u5834\u5408\u306f\u3053\u3061\u3089\n\n\u5b9f\u884c\n$python sigmoid_onlyHidden.py > res.161210_1958.org\n$grep -A 200 output res.161210_1958.org > res.161210_1958.cut\n$vi res.161210_1958.cut # (1\u884c\u76ee\u3092\u524a\u9664)\n\n\n\nsine curve\u306e\u518d\u73fe\n\nreproduce_sine.py\n'''\nv0.3 Dec. 11, 2016\n    - add output_debugPrint()\n    - fix bug > calc_sigmoid() was using positive for exp()\nv0.2 Dec. 10, 2016\n    - calc_conv() takes [applyActFnc] argument\nv0.1 Dec. 10, 2016\n    - add calc_sigmoid()\n    - add fully_connected network\n    - add input data for sine curve\n=== [read_model_var.py] branched to [reproduce_sine.py] ===\n\nv0.4 Dec. 10, 2016\n    - add 2x2 network example\nv0.3 Dec. 07, 2016\n    - calc_conv() > add bias\nv0.2 Dec. 07, 2016\n    - fix calc_conv() treating src as a list\nv0.1 Dec. 07, 2016\n    - add calc_conv()\n'''\n\nimport numpy as np\nimport math\nimport sys\n\nmodel_var = np.load('model_variables.npy')\n\n\n# to ON/OFF debug print at one place\ndef output_debugPrint(str): \n#   print(str)\n    pass # no operation\n\noutput_debugPrint( (\"all shape:\",(model_var.shape)) )\n\ndef calc_sigmoid(x):\n    return 1.0 / (1.0 + math.exp(-x))\n\ndef calc_conv(src, weight, bias, applyActFnc):\n    wgt = weight.shape\n#   print wgt # debug\n    #conv = list(range(bias.size))\n    conv = [0.0] * bias.size\n\n    # weight\n    for idx1 in range(wgt[0]):\n        for idx2 in range(wgt[1]):\n            conv[idx2] = conv[idx2] + src[idx1] * weight[idx1,idx2]\n    # bias\n    for idx2 in range(wgt[1]):\n        conv[idx2] = conv[idx2] + bias[idx2]\n    # activation function\n    if applyActFnc:\n        for idx2 in range(wgt[1]):\n            conv[idx2] = calc_sigmoid(conv[idx2])\n\n    return conv # return list\n\ninpdata = np.linspace(0, 1, 30).astype(float).tolist()\n\n#debug\nfor idx in range(8):\n    output_debugPrint(model_var[idx].shape)\n#sys.exit()\n\n\nfor din in inpdata:\n    # input layer (1 node)\n    #\n    # hidden layer 1 (7 node)\n    inlist = [ din ]\n    outdata = calc_conv(inlist, model_var[0], model_var[1], applyActFnc=True)\n    # hidden layer 2 (7 node)\n    outdata = calc_conv(outdata, model_var[2], model_var[3], applyActFnc=True)\n    # hidden layer 3 (7 node)\n    outdata = calc_conv(outdata, model_var[4], model_var[5], applyActFnc=True)\n    # output layer (1 node)\n    outdata = calc_conv(outdata, model_var[6], model_var[7], applyActFnc=False)\n    dout = outdata[0] # output is 1 node\n    print '%.3f, %.3f' % (din,dout)\n\n\n\n\u5b9f\u884c\n$ python reproduce_sine.py > res.reprod_sine\n\n\n\nJupyter\u3067\u306e\u8868\u793a\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('res.161210_1958.cut', delimiter=',')\ninp1 = data1[:,0]\nout1 = data1[:,1]\ndata2 = np.loadtxt('res.reprod_sine', delimiter=',')\ninp2 = data2[:,0]\nout2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,1,1)\n\nax1.scatter(inp1, out1, label='TensorFlow prediction', color='blue', marker='o')\nax1.scatter(inp2, out2, label='from model_var.npy', color='red',marker='x')\n\nax1.set_xlabel('x')\nax1.set_ylabel('sine(x) prediction')\nax1.grid(True)\nax1.legend()\nax1.set_xlim([0,1.0])\n\nfig.show()\n\n\nthe peggies\u306e\u300c\u30b0\u30e9\u30a4\u30c0\u30fc\u300d\u3092\u805e\u304d\u306a\u304c\u3089\u3001\u6c17\u5206\u3088\u304fsine curve\u3092\u518d\u73fe\u3067\u304d\u305f\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n```\n\n\u95a2\u9023\u3000http://qiita.com/7of9/items/b364d897b95476a30754\n\n[sine curve\u3092\u5b66\u7fd2\u3057\u305f\u6642\u306eweight\u3068bias](http://qiita.com/7of9/items/f7b2e0eeea3b7fdc632c)\u3092\u3082\u3068\u306b\u81ea\u5206\u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u518d\u73fe\u3057\u3066\u51fa\u529b\u3092\u8a08\u7b97\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u3002\n\nhttp://qiita.com/7of9/items/b7d38e174d4052b74cae\n\u306e\u7d9a\u304d\u3002\n\nTensorFlow\u306e\u51e6\u7406\u3067\u51fa\u529b\u3057\u305fweight, bias\u3092\u8aad\u307f\u8fbc\u3093\u3060\u5225python\u30d7\u30ed\u30b0\u30e9\u30e0\u3067sine curve\u306e\u518d\u73fe\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u3002\n\n### input.csv\u751f\u6210\n\nhttp://qiita.com/7of9/items/b364d897b95476a30754#\u30c7\u30fc\u30bf\u751f\u6210\u90e8\n\n### code (sigmoid_onlyHidden.py)\n\nTensorFlow\u5074\u3067weight, bias\u306e\u51fa\u529b\u4f4d\u7f6e(`model_variables = slim.get_model_variables()`\u4ee5\u4e0b3\u884c)\u3092\u4e0b\u306e\u65b9\u306b\u5909\u66f4\u3057\u3066\u307f\u305f\u3002\n\ntry\u306e\u4e2d\u3067\u51fa\u529b\u3059\u308b\u3068\u3001\u672c\u6765\u6b32\u3057\u3044weight, bias\u3092\u51fa\u529b\u3067\u304d\u3066\u3044\u306a\u3044\u306e\u3067\u306f\u3001\u3068\u3044\u3046\u8003\u3048\u3002\n\n```sigmoid_onlyHidden.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfilename_queue = tf.train.string_input_producer([\"input.csv\"])\n\n# prase CSV\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## network\nhiddens = slim.stack(input_ph, slim.fully_connected, [7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\n#prediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=None, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(30000): #[10000]\n      inpbt, outbt = sess.run([inputs_batch, output_batch])\n      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inpbt, output_ph: outbt})\n\n      if (i+1) % 100 == 0:\n        print(\"%d,%f\" % (i+1, t_loss))\n\n#    # output to npy \n#    model_variables = slim.get_model_variables()\n#    res = sess.run(model_variables)\n#    np.save('model_variables.npy', res)\n\n  finally:\n    coord.request_stop()\n\n\n  # output to npy \n  model_variables = slim.get_model_variables()\n  res = sess.run(model_variables)\n  np.save('model_variables.npy', res)\n\n\n#output trained curve\n  print 'output' # used to separate from above lines (grep -A 200 output [outfile])\n  for loop in range(10):\n    inpbt, outbt = sess.run([inputs_batch, output_batch])\n    pred = sess.run([prediction], feed_dict={input_ph:inpbt, output_ph: outbt})\n    for din,dout in zip(inpbt, pred[0]):\n      print '%.5f,%.5f' % (din,dout)\n\n\n  coord.join(threads)\n```\n\n### model_variables.npy \u306e\u4f5c\u308a\u76f4\u3057\n\n```txt:\u5b9f\u884c\n$ python sigmoid_onlyHidden.py\n```\n\nTensorFlow Prediction\u30d5\u30a1\u30a4\u30eb(res.161210_1958.cut)\u3092\u4f5c\u308b\u5834\u5408\u306f\u3053\u3061\u3089\n\n```txt:\u5b9f\u884c\n$python sigmoid_onlyHidden.py > res.161210_1958.org\n$grep -A 200 output res.161210_1958.org > res.161210_1958.cut\n$vi res.161210_1958.cut # (1\u884c\u76ee\u3092\u524a\u9664)\n```\n\n### sine curve\u306e\u518d\u73fe\n\n```reproduce_sine.py\n'''\nv0.3 Dec. 11, 2016\n\t- add output_debugPrint()\n\t- fix bug > calc_sigmoid() was using positive for exp()\nv0.2 Dec. 10, 2016\n\t- calc_conv() takes [applyActFnc] argument\nv0.1 Dec. 10, 2016\n\t- add calc_sigmoid()\n\t- add fully_connected network\n\t- add input data for sine curve\n=== [read_model_var.py] branched to [reproduce_sine.py] ===\n\nv0.4 Dec. 10, 2016\n\t- add 2x2 network example\nv0.3 Dec. 07, 2016\n\t- calc_conv() > add bias\nv0.2 Dec. 07, 2016\n\t- fix calc_conv() treating src as a list\nv0.1 Dec. 07, 2016\n\t- add calc_conv()\n'''\n\nimport numpy as np\nimport math\nimport sys\n\nmodel_var = np.load('model_variables.npy')\n\n\n# to ON/OFF debug print at one place\ndef output_debugPrint(str): \n#\tprint(str)\n\tpass # no operation\n\noutput_debugPrint( (\"all shape:\",(model_var.shape)) )\n\ndef calc_sigmoid(x):\n\treturn 1.0 / (1.0 + math.exp(-x))\n\ndef calc_conv(src, weight, bias, applyActFnc):\n\twgt = weight.shape\n#\tprint wgt # debug\n\t#conv = list(range(bias.size))\n\tconv = [0.0] * bias.size\n\n\t# weight\n\tfor idx1 in range(wgt[0]):\n\t\tfor idx2 in range(wgt[1]):\n\t\t\tconv[idx2] = conv[idx2] + src[idx1] * weight[idx1,idx2]\n\t# bias\n\tfor idx2 in range(wgt[1]):\n\t\tconv[idx2] = conv[idx2] + bias[idx2]\n\t# activation function\n\tif applyActFnc:\n\t\tfor idx2 in range(wgt[1]):\n\t\t\tconv[idx2] = calc_sigmoid(conv[idx2])\n\n\treturn conv # return list\n\ninpdata = np.linspace(0, 1, 30).astype(float).tolist()\n\n#debug\nfor idx in range(8):\n\toutput_debugPrint(model_var[idx].shape)\n#sys.exit()\n\n\nfor din in inpdata:\n\t# input layer (1 node)\n\t#\n\t# hidden layer 1 (7 node)\n\tinlist = [ din ]\n\toutdata = calc_conv(inlist, model_var[0], model_var[1], applyActFnc=True)\n\t# hidden layer 2 (7 node)\n\toutdata = calc_conv(outdata, model_var[2], model_var[3], applyActFnc=True)\n\t# hidden layer 3 (7 node)\n\toutdata = calc_conv(outdata, model_var[4], model_var[5], applyActFnc=True)\n\t# output layer (1 node)\n\toutdata = calc_conv(outdata, model_var[6], model_var[7], applyActFnc=False)\n\tdout = outdata[0] # output is 1 node\n\tprint '%.3f, %.3f' % (din,dout)\n```\n\n```txt:\u5b9f\u884c\n$ python reproduce_sine.py > res.reprod_sine\n```\n\n### Jupyter\u3067\u306e\u8868\u793a\n\n```py\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('res.161210_1958.cut', delimiter=',')\ninp1 = data1[:,0]\nout1 = data1[:,1]\ndata2 = np.loadtxt('res.reprod_sine', delimiter=',')\ninp2 = data2[:,0]\nout2 = data2[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,1,1)\n\nax1.scatter(inp1, out1, label='TensorFlow prediction', color='blue', marker='o')\nax1.scatter(inp2, out2, label='from model_var.npy', color='red',marker='x')\n\nax1.set_xlabel('x')\nax1.set_ylabel('sine(x) prediction')\nax1.grid(True)\nax1.legend()\nax1.set_xlim([0,1.0])\n\nfig.show()\n```\n\n![qiita.png](https://qiita-image-store.s3.amazonaws.com/0/32870/b4e4cdaa-e754-c797-c80e-892a57e82bb4.png)\n\nthe peggies\u306e\u300c\u30b0\u30e9\u30a4\u30c0\u30fc\u300d\u3092\u805e\u304d\u306a\u304c\u3089\u3001\u6c17\u5206\u3088\u304fsine curve\u3092\u518d\u73fe\u3067\u304d\u305f\u3002\n\n\n", "tags": ["borgWarp", "Python", "TensorFlow", "regression"]}