{"context": "Chainer\u3067\u3084\u3063\u3066\u307f\u308bDeep Q Learning - \u7acb\u3061\u4e0a\u3052\u7de8\u3067\u3001\u898b\u305f\u76ee\u307e\u3055\u306b\u4eba\u5de5\u77e5\u80fd\u3063\u307d\u3044\u3053\u3068\u3092\u3084\u3063\u3066\u3044\u305f\u306e\u3067\u771f\u4f3c\u3066\u3084\u3063\u3066\u307f\u308b\u3053\u3068\u306b\u3002\n\u3068\u306f\u3044\u3048wxPython\u3068\u304b\u4f7f\u3063\u305f\u3053\u3068\u306a\u3044\u3057\u3001\u305d\u306e\u307e\u3093\u307e\u771f\u4f3c\u308b\u3068\u96e3\u3057\u305d\u3046\u306a\u306e\u3067\u3001\u3088\u308a\u7c21\u5358\u306a\u611f\u3058\u306b\u3057\u307e\u3057\u305f\u3002\u307e\u3041\u904a\u3073\u3067\u3059\u3057\u3002\n\u6bce\u5ea6\u304a\u306a\u3058\u307f\u3001\u3042\u307e\u308a\u5c02\u9580\u7684\u306a\u3053\u3068\u306f\u826f\u304f\u5206\u304b\u3089\u305a\u96f0\u56f2\u6c17\u3067\u66f8\u3044\u3066\u3044\u308b\u306e\u3067\u3001\u5927\u5e45\u306b\u52d8\u9055\u3044\u3057\u3066\u3044\u308b\u7b87\u6240\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u6c17\u306b\u306a\u3063\u305f\u70b9\u306f\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\n\n\u76ee\u6a19\n\n\u7bb1\u306e\u4e2d\u306b\u30ea\u30f3\u30b4\uff08\u306b\u898b\u7acb\u3066\u305f\u70b9\uff09\u3092\u591a\u6570\u914d\u7f6e\u3002\n\u305d\u3053\u306b\u4eba\u5de5\u77e5\u80fd\u7684\u866b\u3092\u914d\u7f6e\u3002\n\u866b\u306f\u4e0a\u4e0b\u5de6\u53f3\u304a\u3088\u3073\u79fb\u52d5\u3057\u306a\u3044\u3053\u3068\u3092\u9078\u629e\u3067\u304d\u308b\u3002\n\u30ea\u30f3\u30b4\u3092\u98df\u3079\u308b\u4e8b\u304c\u5831\u916c\u3002\n\n\n\u8a2d\u8a08\u6982\u8981\n\n\u57fa\u672c\u7684\u306b\u306f\u524d\u56de\u4f5c\u3063\u305f\u3082\u306e\u3092\u30d9\u30fc\u30b9\u306b\u8003\u3048\u308b\u3002\n\u52d5\u304d\u307e\u308f\u308b\u3053\u3068\u3067\u5831\u916c\u3092\u5f97\u3066\u884c\u304f\u904e\u7a0b\u3092\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u5b9f\u88c5\u3059\u308b\u3002\n\u30d3\u30b8\u30e5\u30a2\u30eb\u7684\u306a\u8868\u793a\u304c\u5fc5\u8981\u306a\u306e\u3067\u3001wxPython\u3088\u308a\u7c21\u5358\u306b\u4f7f\u3048\u305d\u3046\u306amatplotlib\u3092\u4f7f\u3063\u3066\u8868\u793a\u3055\u305b\u308b\u3053\u3068\u306b\u3059\u308b\u3002\uff08\u672c\u6765\u306f\u30b0\u30e9\u30d5\u63cf\u753b\u7528\u3067\u3059\u304c\uff09\n\n\n\u74b0\u5883\nUbuntu 14.04\nTensorFlow 0.7\nGCE CPUx8 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\nPython 2.7\n\n\u5b9f\u88c5\n\u203b\u8d70\u308a\u66f8\u304d\u306e\u3088\u3046\u306b\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3057\u3066\u3001\u8272\u3005\u5909\u3048\u306a\u304c\u3089\u8a66\u3057\u3066\u3044\u305f\u306e\u3067\u3042\u307e\u308a\u30ad\u30ec\u30a4\u306a\u30b3\u30fc\u30c9\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3054\u4e86\u627f\u304f\u3060\u3055\u3044\u3002\n\u4e00\u756a\u4e0b\u306e\u65b9\u306b\u5168\u6587\u8f09\u305b\u3066\u3044\u307e\u3059\u3002\n\n\u30b0\u30e9\u30d5\ndef inference(x_ph):\n\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal([NUM_IMPUT, NUM_HIDDEN1], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN1], dtype=tf.float32), name='biases')\n        hidden1 = tf.nn.relu(tf.matmul(x_ph, weights) + biases)\n\n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN1, NUM_HIDDEN2], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN2], dtype=tf.float32), name='biases')\n        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n\n    with tf.name_scope('output'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN2, NUM_OUTPUT], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_OUTPUT], dtype=tf.float32), name='biases')\n        y = tf.matmul(hidden2, weights) + biases\n\n    return y\n\n\u96a0\u308c\u5c64\u306f2\u5c64\u3002\u30e6\u30cb\u30c3\u30c8\u65701000,500\u3002\u3053\u306e\u8fba\u306f\u9069\u5f53\u306b\u6c7a\u3081\u3066\u3044\u307e\u3059\u3002\n\u6d3b\u6027\u5316\u95a2\u6570\u306frelu\u3067\u826f\u3044\u3093\u3060\u308d\u3046\u304b\u30fb\u30fb\u30fb\n\n\u5165\u529b\u30d9\u30af\u30c8\u30eb\n    def getInputField(self, x, y):\n        side = int(math.sqrt(NUM_IMPUT))\n        rad = int(math.sqrt(NUM_IMPUT) / 2)\n        field = [0.] * NUM_IMPUT\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x - rad and apple_x <= x + rad \\\n            and apple_y >= y - rad and apple_y <= y + rad:\n                idx = side * (apple_y - y + rad) + (apple_x - x + rad)\n                field[idx] = 1.\n\n        return field\n\n\u81ea\u5206\u306e\u5468\u308a\u306e\u534a\u5f849\u30de\u30b9\u3092\u898b\u3066\u3001\u30ea\u30f3\u30b4\u304c\u3042\u308b\u7b87\u6240\u30921\u3068\u3057\u3066\u3001\u3053\u308c\u3092\u5165\u529b\u306b\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u81ea\u5206\u3092\u542b\u3081\u305f19*19\u306e\u56db\u89d2\u304c\u8996\u754c\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\u30fb\u30fb\u30fb\u3042\u3001\u6642\u9593\u306a\u304f\u306a\u3063\u3066\u304d\u305f\u3002\n\u6c17\u304c\u5411\u3044\u305f\u3089\u4eca\u5ea6\u7d30\u304b\u3044\u3068\u3053\u308d\u66f8\u304d\u307e\u3059\u3002\n\u4e0b\u8a18\u306b\u30b3\u30fc\u30c9\u3068\u5b66\u7fd2\u5f8c\u306e\u52d5\u753b\u3092\u8f09\u305b\u3066\u304a\u304d\u307e\u3059\u3002\n\u6c17\u306b\u306a\u308b\u70b9\u3001\u305d\u306e\u9055\u3046\u3093\u3058\u3083\u306d\uff1f\u3053\u3046\u3057\u305f\u65b9\u304c\u3044\u3044\u3093\u3058\u3083\u306d\uff1f\u3068\u3044\u3046\u3068\u3053\u308d\u3042\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308c\u3070\u5e78\u3044\u3067\u3059\u3002\n\n\u7d50\u679c\n\n\n\n\n\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n# -*- coding: UTF-8 -*-\n\n# import\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport random\nimport tensorflow as tf\nimport numpy as np\n\n# definition\nNUM_IMPUT = 361\nNUM_HIDDEN1 = 1000\nNUM_HIDDEN2 = 500\nNUM_OUTPUT = 5\nLEARNING_RATE = 0.1\nREPEAT_TIMES = 1000\nLOG_DIR = \"tf_log\"\nGAMMA = 0.9\nstddev = 0.01\nRANDOM_FACTOR = 0.1\nBATCH = 300\nFRAMES = 600\nMAX_X = 40\nMAX_Y = 40\nX_LIMIT = [0, MAX_X]\nY_LIMIT = [0, MAX_Y]\napple_xs = []\napple_ys = []\nNUM_CELL = 1\nNUM_APPLE = 200\n\ndef inference(x_ph):\n\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal([NUM_IMPUT, NUM_HIDDEN1], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN1], dtype=tf.float32), name='biases')\n        hidden1 = tf.nn.relu(tf.matmul(x_ph, weights) + biases)\n\n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN1, NUM_HIDDEN2], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN2], dtype=tf.float32), name='biases')\n        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n\n    with tf.name_scope('output'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN2, NUM_OUTPUT], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_OUTPUT], dtype=tf.float32), name='biases')\n        y = tf.matmul(hidden2, weights) + biases\n\n    return y\n\ndef loss(y, y_ph):\n    return tf.reduce_mean(tf.nn.l2_loss((y - y_ph)))\n\ndef optimize(loss):\n    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n    train_step = optimizer.minimize(loss)\n    return train_step\n\nclass Cell:\n    def __init__(self, training=True):\n        self.lifetime = random.randint(1, 100)\n        self.x = random.randint(0, MAX_X)\n        self.y = random.randint(0, MAX_Y)\n        self.input_history = []\n        self.reward_history = []\n        self.training = training\n\n    def getInputField(self, x, y):\n        side = int(math.sqrt(NUM_IMPUT))\n        rad = int(math.sqrt(NUM_IMPUT) / 2)\n        field = [0.] * NUM_IMPUT\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x - rad and apple_x <= x + rad \\\n            and apple_y >= y - rad and apple_y <= y + rad:\n                idx = side * (apple_y - y + rad) + (apple_x - x + rad)\n                field[idx] = 1.\n\n        return field\n\n    def getNextPositionReward(self, x, y):\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x-1 and apple_x <= x+1 and \\\n                apple_y >= y-1 and apple_y <= y+1:\n                return 1.\n\n        return 0.\n\n    def moveNextPosition(self, next_position_rewards):\n\n        if random.random() < RANDOM_FACTOR and self.training:\n            act = random.randint(0, 4)\n        else:\n            act = np.argmax(next_position_rewards)\n\n        if act == 1 and self.x < MAX_X :\n            self.x += 1\n        elif act == 2 and self.x > 0:\n            self.x -= 1\n        elif act == 3 and self.y < MAX_Y:\n            self.y += 1\n        elif act == 4 and self.y > 0:\n            self.y -= 1\n\n        for i in range(len(apple_xs)):\n            if apple_xs[i] >= self.x-1 and apple_xs[i] <= self.x+1 and \\\n                apple_ys[i] >= self.y-1 and apple_ys[i] <= self.y+1:\n                apple_xs.pop(i)\n                apple_ys.pop(i)\n                break\n\n    def action(self):\n        input0 = self.getInputField(self.x, self.y)\n        input1 = self.getInputField(self.x+1, self.y)\n        input2 = self.getInputField(self.x-1, self.y)\n        input3 = self.getInputField(self.x, self.y+1)\n        input4 = self.getInputField(self.x, self.y-1)\n\n        next_position_rewards = []\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x+1, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x-1, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y+1))\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y-1))\n\n        future_rewards_array = sess.run(y, feed_dict={x_ph: [input0, input1, input2, input3, input4]})\n        for i in range(5):\n            next_position_rewards[i] = next_position_rewards[i] + GAMMA * np.max(future_rewards_array[i])\n\n        self.input_history.append(input0)\n        self.reward_history.append(next_position_rewards)\n\n        self.moveNextPosition(next_position_rewards)\n\n\nclass World:\n    def __init__(self, training=True):\n\n        self.cells = []\n        for i in range(NUM_CELL):\n            cell = Cell(training)\n            self.cells.append(cell)\n\n        while(len(apple_xs) != 0):\n            apple_xs.pop()\n            apple_ys.pop()\n\n        for i in range(NUM_APPLE):\n            apple_xs.append(random.randint(0, MAX_X))\n            apple_ys.append(random.randint(0, MAX_Y))\n\n    def _update_plot(self, i, fig, im):\n        xs = []\n        ys = []\n        for cell in self.cells:\n            cell.action()\n            xs.append(cell.x)\n            ys.append(cell.y)\n\n        self.red.set_data(xs, ys)\n        self.yellow.set_data(apple_xs, apple_ys)\n\n    def showAnimation(self, filename=None):\n\n        self.fig =  plt.figure()\n        ax = self.fig.add_subplot(1,1,1)\n\n        ax.set_xlim(X_LIMIT)\n        ax.set_ylim(Y_LIMIT)\n\n        #addition\n        self.red, = ax.plot([], [], 'ro', lw=2)\n        self.yellow, = ax.plot([], [], 'yo', lw=2)\n\n        self.im = []\n\n        ani = animation.FuncAnimation(self.fig, self._update_plot, fargs = (self.fig, self.im),\n                                          frames = FRAMES, interval = 10, repeat = False)\n\n#         plt.show()\n\n        if filename != None:\n            ani.save(filename, writer=\"mencoder\")\n\n    def training(self):\n        for cell in self.cells:\n            cell.action()\n\nif __name__ == \"__main__\":\n\n    x_ph = tf.placeholder(tf.float32, [None, NUM_IMPUT])\n    y_ph = tf.placeholder(tf.float32, [None, NUM_OUTPUT])\n\n    y = inference(x_ph)\n    loss = loss(y, y_ph)\n    tf.scalar_summary(\"Loss\", loss)\n    train_step = optimize(loss)\n\n    sess = tf.Session()\n    summary_op = tf.merge_all_summaries()\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    summary_writer = tf.train.SummaryWriter(LOG_DIR, graph_def=sess.graph_def)\n\n    for i in range(REPEAT_TIMES):\n\n        world = World()\n        for j in range(BATCH):\n            for cell in world.cells:\n                cell.action()\n\n        for cell in world.cells:\n            sess.run(train_step, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            summary_str = sess.run(summary_op, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            summary_writer.add_summary(summary_str, i)\n            ce = sess.run(loss, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            print \"Cross Entropy: \" + str(ce)\n\n        print \"Count: \" + str(i)\n\n    world = World()\n    world.showAnimation(\"ani.mp4\")\n\n\u8aad\u3080\u3068\u5206\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u4eba\u5de5\u77e5\u80fd\u866b\u306f2\u3064\u4ee5\u4e0a\u306b\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n\u6240\u611f\n\u610f\u5916\u3068\u51e6\u7406\u304c\u91cd\u3044\u3002\u5c11\u3057\u826f\u3044\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3082\u3063\u3068\u8a08\u7b97\u306f\u901f\u3044\u3082\u306e\u304b\u3068\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u610f\u5916\u3068\u6642\u9593\u304c\u304b\u304b\u308b\u3002\u305d\u306e\u305f\u3081\u3001\u30d0\u30c3\u30c1\u3092\u77ed\u304f\u3057\u305f\u308a\u56de\u3059\u56de\u6570\u304c\u5272\u3068\u30b7\u30d3\u30a2\u306b\u306a\u308a\u3084\u3059\u3044\u3002\n\u7d50\u679c\u306f\u4e00\u5fdc\u30ea\u30f3\u30b4\u3092\u8ffd\u3063\u3066\u3044\u308b\u3088\u3046\u3060\u304c\u3001\u5468\u308a\u306b\u30ea\u30f3\u30b4\u304c\u306a\u304f\u306a\u308b\u3068\u3069\u3046\u3057\u3066\u826f\u3044\u304b\u5206\u304b\u3089\u306a\u3044\u98a8\u306a\u52d5\u304d\u306b\u306a\u308b\u3063\u307d\u3044\u3002\u30d0\u30c3\u30c1\u3092\u77ed\u3081\u306b\u30bb\u30c3\u30c8\u3057\u305f\u306e\u3067\u3001\u9577\u671f\u7684\u306a\u8996\u70b9\u3067\u306e\u52d5\u304d\u306f\u5b66\u7fd2\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3060\u308d\u3046\u304b\u3002\u8996\u754c\u306b\u306f\u5165\u3063\u3066\u3044\u308b\u30cf\u30ba\u3060\u3051\u3069\u3001\u8fd1\u304f\u306a\u3044\u3068\u53cd\u5fdc\u3057\u306a\u3044\u3088\u3046\u306b\u898b\u3048\u308b\u3002\n[Chainer\u3067\u3084\u3063\u3066\u307f\u308bDeep Q Learning - \u7acb\u3061\u4e0a\u3052\u7de8](http://qiita.com/chachay/items/5fdb7c64af68bcacf7d3)\u3067\u3001\u898b\u305f\u76ee\u307e\u3055\u306b\u4eba\u5de5\u77e5\u80fd\u3063\u307d\u3044\u3053\u3068\u3092\u3084\u3063\u3066\u3044\u305f\u306e\u3067\u771f\u4f3c\u3066\u3084\u3063\u3066\u307f\u308b\u3053\u3068\u306b\u3002\n\u3068\u306f\u3044\u3048wxPython\u3068\u304b\u4f7f\u3063\u305f\u3053\u3068\u306a\u3044\u3057\u3001\u305d\u306e\u307e\u3093\u307e\u771f\u4f3c\u308b\u3068\u96e3\u3057\u305d\u3046\u306a\u306e\u3067\u3001\u3088\u308a\u7c21\u5358\u306a\u611f\u3058\u306b\u3057\u307e\u3057\u305f\u3002\u307e\u3041\u904a\u3073\u3067\u3059\u3057\u3002\n\u6bce\u5ea6\u304a\u306a\u3058\u307f\u3001\u3042\u307e\u308a\u5c02\u9580\u7684\u306a\u3053\u3068\u306f\u826f\u304f\u5206\u304b\u3089\u305a\u96f0\u56f2\u6c17\u3067\u66f8\u3044\u3066\u3044\u308b\u306e\u3067\u3001\u5927\u5e45\u306b\u52d8\u9055\u3044\u3057\u3066\u3044\u308b\u7b87\u6240\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u6c17\u306b\u306a\u3063\u305f\u70b9\u306f\u6307\u6458\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\n\n##\u76ee\u6a19\n- \u7bb1\u306e\u4e2d\u306b\u30ea\u30f3\u30b4\uff08\u306b\u898b\u7acb\u3066\u305f\u70b9\uff09\u3092\u591a\u6570\u914d\u7f6e\u3002\n- \u305d\u3053\u306b\u4eba\u5de5\u77e5\u80fd\u7684\u866b\u3092\u914d\u7f6e\u3002\n- \u866b\u306f\u4e0a\u4e0b\u5de6\u53f3\u304a\u3088\u3073\u79fb\u52d5\u3057\u306a\u3044\u3053\u3068\u3092\u9078\u629e\u3067\u304d\u308b\u3002\n- \u30ea\u30f3\u30b4\u3092\u98df\u3079\u308b\u4e8b\u304c\u5831\u916c\u3002\n\n##\u8a2d\u8a08\u6982\u8981\n- \u57fa\u672c\u7684\u306b\u306f[\u524d\u56de](http://qiita.com/yai/items/f09b681f6a2d263ee6d6)\u4f5c\u3063\u305f\u3082\u306e\u3092\u30d9\u30fc\u30b9\u306b\u8003\u3048\u308b\u3002\n- \u52d5\u304d\u307e\u308f\u308b\u3053\u3068\u3067\u5831\u916c\u3092\u5f97\u3066\u884c\u304f\u904e\u7a0b\u3092\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3067\u5b9f\u88c5\u3059\u308b\u3002\n- \u30d3\u30b8\u30e5\u30a2\u30eb\u7684\u306a\u8868\u793a\u304c\u5fc5\u8981\u306a\u306e\u3067\u3001wxPython\u3088\u308a\u7c21\u5358\u306b\u4f7f\u3048\u305d\u3046\u306amatplotlib\u3092\u4f7f\u3063\u3066\u8868\u793a\u3055\u305b\u308b\u3053\u3068\u306b\u3059\u308b\u3002\uff08\u672c\u6765\u306f\u30b0\u30e9\u30d5\u63cf\u753b\u7528\u3067\u3059\u304c\uff09\n\n##\u74b0\u5883\nUbuntu 14.04\nTensorFlow 0.7\nGCE CPUx8 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\nPython 2.7\n\n##\u5b9f\u88c5\n\u203b\u8d70\u308a\u66f8\u304d\u306e\u3088\u3046\u306b\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3057\u3066\u3001\u8272\u3005\u5909\u3048\u306a\u304c\u3089\u8a66\u3057\u3066\u3044\u305f\u306e\u3067\u3042\u307e\u308a\u30ad\u30ec\u30a4\u306a\u30b3\u30fc\u30c9\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u3054\u4e86\u627f\u304f\u3060\u3055\u3044\u3002\n\u4e00\u756a\u4e0b\u306e\u65b9\u306b\u5168\u6587\u8f09\u305b\u3066\u3044\u307e\u3059\u3002\n###\u30b0\u30e9\u30d5\n```\ndef inference(x_ph):\n\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal([NUM_IMPUT, NUM_HIDDEN1], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN1], dtype=tf.float32), name='biases')\n        hidden1 = tf.nn.relu(tf.matmul(x_ph, weights) + biases)\n\n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN1, NUM_HIDDEN2], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN2], dtype=tf.float32), name='biases')\n        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n\n    with tf.name_scope('output'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN2, NUM_OUTPUT], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_OUTPUT], dtype=tf.float32), name='biases')\n        y = tf.matmul(hidden2, weights) + biases\n\n    return y\n```\n\u96a0\u308c\u5c64\u306f2\u5c64\u3002\u30e6\u30cb\u30c3\u30c8\u65701000,500\u3002\u3053\u306e\u8fba\u306f\u9069\u5f53\u306b\u6c7a\u3081\u3066\u3044\u307e\u3059\u3002\n\u6d3b\u6027\u5316\u95a2\u6570\u306frelu\u3067\u826f\u3044\u3093\u3060\u308d\u3046\u304b\u30fb\u30fb\u30fb\n\n###\u5165\u529b\u30d9\u30af\u30c8\u30eb\n```\n    def getInputField(self, x, y):\n        side = int(math.sqrt(NUM_IMPUT))\n        rad = int(math.sqrt(NUM_IMPUT) / 2)\n        field = [0.] * NUM_IMPUT\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x - rad and apple_x <= x + rad \\\n            and apple_y >= y - rad and apple_y <= y + rad:\n                idx = side * (apple_y - y + rad) + (apple_x - x + rad)\n                field[idx] = 1.\n\n        return field\n```\n\u81ea\u5206\u306e\u5468\u308a\u306e\u534a\u5f849\u30de\u30b9\u3092\u898b\u3066\u3001\u30ea\u30f3\u30b4\u304c\u3042\u308b\u7b87\u6240\u30921\u3068\u3057\u3066\u3001\u3053\u308c\u3092\u5165\u529b\u306b\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u81ea\u5206\u3092\u542b\u3081\u305f19*19\u306e\u56db\u89d2\u304c\u8996\u754c\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002\n\n**\u30fb\u30fb\u30fb\u3042\u3001\u6642\u9593\u306a\u304f\u306a\u3063\u3066\u304d\u305f\u3002**\n\u6c17\u304c\u5411\u3044\u305f\u3089\u4eca\u5ea6\u7d30\u304b\u3044\u3068\u3053\u308d\u66f8\u304d\u307e\u3059\u3002\n\u4e0b\u8a18\u306b\u30b3\u30fc\u30c9\u3068\u5b66\u7fd2\u5f8c\u306e\u52d5\u753b\u3092\u8f09\u305b\u3066\u304a\u304d\u307e\u3059\u3002\n\u6c17\u306b\u306a\u308b\u70b9\u3001\u305d\u306e\u9055\u3046\u3093\u3058\u3083\u306d\uff1f\u3053\u3046\u3057\u305f\u65b9\u304c\u3044\u3044\u3093\u3058\u3083\u306d\uff1f\u3068\u3044\u3046\u3068\u3053\u308d\u3042\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308c\u3070\u5e78\u3044\u3067\u3059\u3002\n\n##\u7d50\u679c\n![ani1.gif](https://qiita-image-store.s3.amazonaws.com/0/117826/76c5c9ec-9ac6-15f3-117d-410ab4d18f00.gif)\n![ani2.gif](https://qiita-image-store.s3.amazonaws.com/0/117826/d52c735a-cc4d-3b1d-280a-a9bf4d57315e.gif)\n![ani3.gif](https://qiita-image-store.s3.amazonaws.com/0/117826/e22852c6-5055-30c1-b4df-4e4807963658.gif)\n\n##\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\n```\n# -*- coding: UTF-8 -*-\n\n# import\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport random\nimport tensorflow as tf\nimport numpy as np\n\n# definition\nNUM_IMPUT = 361\nNUM_HIDDEN1 = 1000\nNUM_HIDDEN2 = 500\nNUM_OUTPUT = 5\nLEARNING_RATE = 0.1\nREPEAT_TIMES = 1000\nLOG_DIR = \"tf_log\"\nGAMMA = 0.9\nstddev = 0.01\nRANDOM_FACTOR = 0.1\nBATCH = 300\nFRAMES = 600\nMAX_X = 40\nMAX_Y = 40\nX_LIMIT = [0, MAX_X]\nY_LIMIT = [0, MAX_Y]\napple_xs = []\napple_ys = []\nNUM_CELL = 1\nNUM_APPLE = 200\n\ndef inference(x_ph):\n\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal([NUM_IMPUT, NUM_HIDDEN1], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN1], dtype=tf.float32), name='biases')\n        hidden1 = tf.nn.relu(tf.matmul(x_ph, weights) + biases)\n\n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN1, NUM_HIDDEN2], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_HIDDEN2], dtype=tf.float32), name='biases')\n        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n\n    with tf.name_scope('output'):\n        weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN2, NUM_OUTPUT], stddev=stddev), name='weights')\n        biases = tf.Variable(tf.zeros([NUM_OUTPUT], dtype=tf.float32), name='biases')\n        y = tf.matmul(hidden2, weights) + biases\n\n    return y\n\ndef loss(y, y_ph):\n    return tf.reduce_mean(tf.nn.l2_loss((y - y_ph)))\n\ndef optimize(loss):\n    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n    train_step = optimizer.minimize(loss)\n    return train_step\n\nclass Cell:\n    def __init__(self, training=True):\n        self.lifetime = random.randint(1, 100)\n        self.x = random.randint(0, MAX_X)\n        self.y = random.randint(0, MAX_Y)\n        self.input_history = []\n        self.reward_history = []\n        self.training = training\n\n    def getInputField(self, x, y):\n        side = int(math.sqrt(NUM_IMPUT))\n        rad = int(math.sqrt(NUM_IMPUT) / 2)\n        field = [0.] * NUM_IMPUT\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x - rad and apple_x <= x + rad \\\n            and apple_y >= y - rad and apple_y <= y + rad:\n                idx = side * (apple_y - y + rad) + (apple_x - x + rad)\n                field[idx] = 1.\n\n        return field\n\n    def getNextPositionReward(self, x, y):\n        for apple_x, apple_y in zip(apple_xs, apple_ys):\n            if apple_x >= x-1 and apple_x <= x+1 and \\\n                apple_y >= y-1 and apple_y <= y+1:\n                return 1.\n\n        return 0.\n\n    def moveNextPosition(self, next_position_rewards):\n\n        if random.random() < RANDOM_FACTOR and self.training:\n            act = random.randint(0, 4)\n        else:\n            act = np.argmax(next_position_rewards)\n\n        if act == 1 and self.x < MAX_X :\n            self.x += 1\n        elif act == 2 and self.x > 0:\n            self.x -= 1\n        elif act == 3 and self.y < MAX_Y:\n            self.y += 1\n        elif act == 4 and self.y > 0:\n            self.y -= 1\n\n        for i in range(len(apple_xs)):\n            if apple_xs[i] >= self.x-1 and apple_xs[i] <= self.x+1 and \\\n                apple_ys[i] >= self.y-1 and apple_ys[i] <= self.y+1:\n                apple_xs.pop(i)\n                apple_ys.pop(i)\n                break\n\n    def action(self):\n        input0 = self.getInputField(self.x, self.y)\n        input1 = self.getInputField(self.x+1, self.y)\n        input2 = self.getInputField(self.x-1, self.y)\n        input3 = self.getInputField(self.x, self.y+1)\n        input4 = self.getInputField(self.x, self.y-1)\n\n        next_position_rewards = []\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x+1, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x-1, self.y))\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y+1))\n        next_position_rewards.append(self.getNextPositionReward(self.x, self.y-1))\n\n        future_rewards_array = sess.run(y, feed_dict={x_ph: [input0, input1, input2, input3, input4]})\n        for i in range(5):\n            next_position_rewards[i] = next_position_rewards[i] + GAMMA * np.max(future_rewards_array[i])\n\n        self.input_history.append(input0)\n        self.reward_history.append(next_position_rewards)\n\n        self.moveNextPosition(next_position_rewards)\n\n\nclass World:\n    def __init__(self, training=True):\n\n        self.cells = []\n        for i in range(NUM_CELL):\n            cell = Cell(training)\n            self.cells.append(cell)\n\n        while(len(apple_xs) != 0):\n            apple_xs.pop()\n            apple_ys.pop()\n\n        for i in range(NUM_APPLE):\n            apple_xs.append(random.randint(0, MAX_X))\n            apple_ys.append(random.randint(0, MAX_Y))\n\n    def _update_plot(self, i, fig, im):\n        xs = []\n        ys = []\n        for cell in self.cells:\n            cell.action()\n            xs.append(cell.x)\n            ys.append(cell.y)\n\n        self.red.set_data(xs, ys)\n        self.yellow.set_data(apple_xs, apple_ys)\n\n    def showAnimation(self, filename=None):\n\n        self.fig =  plt.figure()\n        ax = self.fig.add_subplot(1,1,1)\n\n        ax.set_xlim(X_LIMIT)\n        ax.set_ylim(Y_LIMIT)\n\n        #addition\n        self.red, = ax.plot([], [], 'ro', lw=2)\n        self.yellow, = ax.plot([], [], 'yo', lw=2)\n\n        self.im = []\n\n        ani = animation.FuncAnimation(self.fig, self._update_plot, fargs = (self.fig, self.im),\n                                          frames = FRAMES, interval = 10, repeat = False)\n\n#         plt.show()\n\n        if filename != None:\n            ani.save(filename, writer=\"mencoder\")\n\n    def training(self):\n        for cell in self.cells:\n            cell.action()\n\nif __name__ == \"__main__\":\n\n    x_ph = tf.placeholder(tf.float32, [None, NUM_IMPUT])\n    y_ph = tf.placeholder(tf.float32, [None, NUM_OUTPUT])\n\n    y = inference(x_ph)\n    loss = loss(y, y_ph)\n    tf.scalar_summary(\"Loss\", loss)\n    train_step = optimize(loss)\n\n    sess = tf.Session()\n    summary_op = tf.merge_all_summaries()\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    summary_writer = tf.train.SummaryWriter(LOG_DIR, graph_def=sess.graph_def)\n\n    for i in range(REPEAT_TIMES):\n\n        world = World()\n        for j in range(BATCH):\n            for cell in world.cells:\n                cell.action()\n\n        for cell in world.cells:\n            sess.run(train_step, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            summary_str = sess.run(summary_op, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            summary_writer.add_summary(summary_str, i)\n            ce = sess.run(loss, feed_dict={x_ph: cell.input_history, y_ph: cell.reward_history})\n            print \"Cross Entropy: \" + str(ce)\n\n        print \"Count: \" + str(i)\n\n    world = World()\n    world.showAnimation(\"ani.mp4\")\n```\n\u8aad\u3080\u3068\u5206\u304b\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u4eba\u5de5\u77e5\u80fd\u866b\u306f2\u3064\u4ee5\u4e0a\u306b\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\n\n##\u6240\u611f\n\u610f\u5916\u3068\u51e6\u7406\u304c\u91cd\u3044\u3002\u5c11\u3057\u826f\u3044\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3082\u3063\u3068\u8a08\u7b97\u306f\u901f\u3044\u3082\u306e\u304b\u3068\u601d\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u610f\u5916\u3068\u6642\u9593\u304c\u304b\u304b\u308b\u3002\u305d\u306e\u305f\u3081\u3001\u30d0\u30c3\u30c1\u3092\u77ed\u304f\u3057\u305f\u308a\u56de\u3059\u56de\u6570\u304c\u5272\u3068\u30b7\u30d3\u30a2\u306b\u306a\u308a\u3084\u3059\u3044\u3002\n\u7d50\u679c\u306f\u4e00\u5fdc\u30ea\u30f3\u30b4\u3092\u8ffd\u3063\u3066\u3044\u308b\u3088\u3046\u3060\u304c\u3001\u5468\u308a\u306b\u30ea\u30f3\u30b4\u304c\u306a\u304f\u306a\u308b\u3068\u3069\u3046\u3057\u3066\u826f\u3044\u304b\u5206\u304b\u3089\u306a\u3044\u98a8\u306a\u52d5\u304d\u306b\u306a\u308b\u3063\u307d\u3044\u3002\u30d0\u30c3\u30c1\u3092\u77ed\u3081\u306b\u30bb\u30c3\u30c8\u3057\u305f\u306e\u3067\u3001\u9577\u671f\u7684\u306a\u8996\u70b9\u3067\u306e\u52d5\u304d\u306f\u5b66\u7fd2\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3060\u308d\u3046\u304b\u3002\u8996\u754c\u306b\u306f\u5165\u3063\u3066\u3044\u308b\u30cf\u30ba\u3060\u3051\u3069\u3001\u8fd1\u304f\u306a\u3044\u3068\u53cd\u5fdc\u3057\u306a\u3044\u3088\u3046\u306b\u898b\u3048\u308b\u3002\n", "tags": ["TensorFlow", "Python", "DeepLearning", "DQN"]}