{"context": "\n\n\u3053\u306e\u6587\u7ae0\u306b\u3064\u3044\u3066\n\u3053\u306e\u30dd\u30b9\u30c8\u306f\u3001\u5148\u65e5Hadoop\u3068S3\u3068\u3044\u3046\u30bf\u30a4\u30c8\u30eb\u3067\u66f8\u3044\u305f\u4e0b\u8a18\u306e\u3088\u3046\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u4f5c\u308b\u306b\u3042\u305f\u3063\u3066\u306eZeppelin + Spark + LLAP\u307e\u308f\u308a\u306e\u8abf\u67fb\u3068\u691c\u8a3c\u72b6\u6cc1\u3092\u81ea\u5206\u306e\u305f\u3081\u306e\u30e1\u30e2\u3068\u3057\u3066\u307e\u3068\u3081\u305f\u3082\u306e\u3067\u3059\u3002\n\n\nMotivation\nSpark\u3067S3\u4e0a\u306e\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u30ec\u30a4\u30c6\u30f3\u30b7\u3092\u3067\u304d\u308b\u3060\u3051\u4e0b\u3052\u3001\u304b\u3064S3\u306e\u30b9\u30ed\u30c3\u30c8\u30ea\u30f3\u30b0\u3092\u907f\u3051\u308b\u305f\u3081\u306b\u3067\u304d\u308b\u3060\u3051\u30a2\u30af\u30bb\u30b9\u983b\u5ea6\u3092\u4e0b\u3052\u305f\u3044\u3068\u3044\u3046\u80cc\u666f\u304b\u3089spark-llap\u3092\u8a66\u3057\u3066\u3044\u308b\u3002\nhttps://github.com/hortonworks-spark/spark-llap\nspark-llap\u306f\u5358\u7d14\u306bHiveserver2\u7d4c\u7531\u3067Hive LLAP\u306e\u5b9f\u884c\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u308f\u3051\u3067\u306f\u306a\u304f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002\n\n*\u753b\u50cf\u306fHortonworks\u306eHive\u958b\u767a\u8005\u3001Sergey Shelulkhin\u306eSlideshare\u304b\u3089\u62dd\u501f\u3002\n\u3053\u306e\u65b9\u6cd5\u306e\u30e1\u30ea\u30c3\u30c8\u306f\u3001Hiveserver2\u306b\u96c6\u7d04\u3055\u308c\u3066Thrift\u306b\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u3055\u308c\u305f\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u306e\u3067\u306f\u306a\u304f\u3001Spark\u306e\u5404Executor\u304c\u76f4\u63a5LLAP Daemon\u304b\u3089\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u3001\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u304c\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3002\n\nBuild\nREADME\u306b\u306fbuild/sbt package\u3057\u308d\u3068\u66f8\u3044\u3066\u3042\u308b\u304c\u3053\u308c\u3058\u3083\u3060\u3081\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306bbuild/sbt assembly\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\uff08\u307e\u3042\u305d\u308a\u3083\u305d\u3046\u304b\uff09\nhttps://github.com/hortonworks-spark/spark-llap/issues/9\nmaster\u30d6\u30e9\u30f3\u30c1\u306fSpark1.6\u7528\u306a\u306e\u3067\u3053\u3093\u306a\u611f\u3058\u306b\u30d3\u30eb\u30c9\u3067\u304d\u308b\u3002\nbuild/sbt -Dspark.version=1.6.2.2.5.3.0-37 -Dhadoop.version=2.7.3.2.5.3.0-37 -Dhive.version=2.1.0.2.5.3.0-37 -Dtez.version=0.8.4.2.5.3.0-37 -Drepourl=http://nexus-private.hortonworks.com:8081/nexus/content/groups/public/ clean compile assembly\nSpark2.0\u7528\u306ebranch-2.0\u3082\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u30d3\u30eb\u30c9\u3002\nbuild/sbt -Dspark.version=2.0.0.2.5.3.0-37 -Dhadoop.version=2.7.3.2.5.3.0-37 -Dhive.version=2.1.0.2.5.3.0-37 -Dtez.version=0.8.4.2.5.3.0-37 -Drepourl=http://nexus-private.hortonworks.com:8081/nexus/content/groups/public/ clean compile assembly\n\u3069\u3061\u3089\u3082\u3046\u307e\u304f\u3044\u3063\u305f\u3002Hortonworks\u306erepository\u3092\u4f7f\u308f\u306a\u304d\u3083\u3044\u3051\u306a\u3044\u304b\u3069\u3046\u304b\u306f\u30fb\u30fb\u30fb\u305f\u3081\u3057\u3066\u3044\u306a\u3044\u3002\n\n\u52d5\u304b\u3057\u3066\u307f\u308b\nREADME\u306b\u5f93\u3063\u3066\u4e0b\u8a18\u3092spark-default.conf\u306b\u8a2d\u5b9a\nspark.sql.hive.hiveserver2.url=jdbc:hive2://hiveserver2:10500\nspark.hadoop.hive.llap.daemon.service.hosts=@llap0\nspark.hadoop.hive.zookeeper.quorum=zookeeper:2181\n\nSpark Thrift Server\u4f7f\u3046\u306a\u3089\u4ed6\u306b\u3082\u8a2d\u5b9a\u304c\u5fc5\u8981\u3060\u304c\u3053\u3053\u3067\u306f\u30b9\u30ad\u30c3\u30d7\u3002\u3053\u306e\u4e0a\u3067spark-shell --jars PATH/TO/ASSEMBRY_JAR\u3067spark-shell\u3092\u8d77\u52d5\u3002\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u52d5\u3044\u3066\u304f\u308c\u305f\u3002\nimport org.apache.spark.sql.hive.llap.LlapContext\nvar llapContext = new LlapContext(sc)\n\nval sql = \"SOME HIVE QL\"\n\nllapContext.sql(sql).show\n\nSpark2\u3060\u3068\u4e0b\u8a18\u3060\u3051\u3067\u52d5\u304f\u3089\u3057\u3044\u3002\u30af\u30a8\u30ea\u306f\u901a\u308b\u3051\u3069\u3001\u5b9f\u969b\u306bLLAP\u3067\u30b8\u30e7\u30d6\u304c\u8d70\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\nspark.sql(\"SOME HIVE QL\").show\n\n\n\u3061\u3083\u3093\u3068\u52d5\u3044\u3066\u3044\u308b\u304b\u78ba\u8a8d\nSpark1\u3067\u306e\u5b9f\u884c\u306f\u3044\u308d\u3044\u308d\u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u308b\u306e\u3067\u52d5\u3044\u3066\u3044\u308b\u3063\u307d\u3044\u304c\u3001Spark2\u306e\u306e\u307b\u3046\u306f\u3044\u307e\u3044\u3061\u30c7\u30d0\u30c3\u30b0\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u3002\nSpark\u304b\u3089\u52d5\u304b\u3057\u3066\u3044\u308b\u304b\u3089\u306a\u306e\u304b\uff08\u305f\u3076\u3093\u305d\u3046\uff09\u3001Tez View\u306b\u306f\u30ed\u30b0\u306f\u51fa\u3066\u3053\u306a\u3044\u3057\u3001LLAP Web Service(http://llapnode:10502)\u306b\u3082\u306a\u3093\u306e\u60c5\u5831\u3082\u51fa\u3066\u3053\u306a\u3044\n\nZeppelin\u3067\u52d5\u304b\u3057\u3066\u307f\u308b\nZeppelin\u3067Spark Interpreter\u306b\u5bfe\u3057\u3066spark-llap\u306eassembly jar\u3092\u8db3\u3057\u3066\u3084\u3063\u305f\u3089\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3002\n\nimport org.apache.spark.sql.hive.llap.LlapContext\njava.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n    at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\n    at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\n    at org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:225)\n    at org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:215)\n    at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:458)\n    at org.apache.spark.sql.SQLContext$$anonfun$4.apply(SQLContext.scala:272)\n    at org.apache.spark.sql.SQLContext$$anonfun$4.apply(SQLContext.scala:271)\n    at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n    at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:271)\n    at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\n    at org.apache.spark.sql.hive.llap.LlapContext.<init>(LlapContext.scala:40)\n    at org.apache.spark.sql.hive.llap.LlapContext.<init>(LlapContext.scala:60)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)\n    at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)\n    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:42)\n    at $iwC$$iwC$$iwC.<init>(<console>:44)\n    at $iwC$$iwC.<init>(<console>:46)\n    at $iwC.<init>(<console>:48)\n    at <init>(<console>:50)\n    at .<init>(<console>:54)\n    at .<clinit>(<console>)\n    at .<init>(<console>:7)\n    at .<clinit>(<console>)\n    at $print(<console>)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n    at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n    at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\n    at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:717)\n    at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:928)\n    at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:871)\n    at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:864)\n    at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n    at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n    at org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n    at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n    at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n    ... 58 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n    ... 64 more\nCaused by: javax.jdo.JDOFatalInternalException: Unexpected exception caught.\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1193)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n    ... 69 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n    ... 88 more\nCaused by: org.datanucleus.exceptions.NucleusException: Error creating the MetaDataManager for API \"JDO\" : \n    at org.datanucleus.NucleusContext.getMetaDataManager(NucleusContext.java:1001)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.initialiseMetaData(JDOPersistenceManagerFactory.java:703)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.<init>(JDOPersistenceManagerFactory.java:511)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:301)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n    ... 96 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n    at org.datanucleus.NucleusContext.getMetaDataManager(NucleusContext.java:995)\n    ... 100 more\nCaused by: org.datanucleus.metadata.InvalidClassMetaDataException: Class \"org.apache.hadoop.hive.metastore.model.MConstraint\" has MetaData yet the class cant be found. Please check your CLASSPATH specifications.\n    at org.datanucleus.metadata.AbstractClassMetaData.loadClass(AbstractClassMetaData.java:579)\n    at org.datanucleus.metadata.ClassMetaData.populate(ClassMetaData.java:166)\n    at org.datanucleus.metadata.MetaDataManager$1.run(MetaDataManager.java:2918)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at org.datanucleus.metadata.MetaDataManager.populateAbstractClassMetaData(MetaDataManager.java:2912)\n    at org.datanucleus.metadata.MetaDataManager.populateFileMetaData(MetaDataManager.java:2735)\n    at org.datanucleus.api.jdo.metadata.JDOMetaDataManager.loadXMLMetaDataForClass(JDOMetaDataManager.java:786)\n    at org.datanucleus.api.jdo.metadata.JDOMetaDataManager.getMetaDataForClassInternal(JDOMetaDataManager.java:383)\n    at org.datanucleus.api.jdo.metadata.JDOMetaDataManager$MetaDataRegisterClassListener.registerClass(JDOMetaDataManager.java:207)\n    at javax.jdo.spi.JDOImplHelper.addRegisterClassListener(JDOImplHelper.java:462)\n    at org.datanucleus.api.jdo.metadata.JDOMetaDataManager.<init>(JDOMetaDataManager.java:194)\n    ... 107 more\nCaused by: org.datanucleus.exceptions.ClassNotResolvedException: Class \"org.apache.hadoop.hive.metastore.model.MConstraint\" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.\n    at org.datanucleus.ClassLoaderResolverImpl.classForName(ClassLoaderResolverImpl.java:216)\n    at org.datanucleus.ClassLoaderResolverImpl.classForName(ClassLoaderResolverImpl.java:368)\n    at org.datanucleus.metadata.AbstractClassMetaData.loadClass(AbstractClassMetaData.java:569)\n    ... 117 more\n\n\n\u3044\u307e\u306e\u3068\u3053\u308d\u306e\u7d50\u8ad6\nZeppelin\u306e\u30a8\u30e9\u30fc\u306f\u3061\u3083\u3093\u3068\u8ffd\u3048\u3070\u30b7\u30e5\u30fc\u30c8\u3067\u304d\u308b\u304b\u3082\u3060\u3051\u3069\u3001\u305d\u3053\u307e\u3067\u6642\u9593\u306f\u639b\u3051\u3089\u308c\u305d\u3046\u306b\u306a\u3044\u306e\u3067\u3001\u4e00\u65e6\u6b21\u306eHDP\u306e\u30ea\u30ea\u30fc\u30b9\u3092\u5f85\u3064\u306e\u304c\u3044\u3044\u304b\u306a\u3002LLAP\u3092GA\u306b\u3057\u3066\u3001Zeppelin\u304b\u3089\uff08\u3067\u304d\u308c\u3070Spark2\u3067\uff09\u547c\u3079\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u308c\u308b\u3068\u3053\u308d\u307e\u3067\u6574\u3063\u3066\u3044\u305f\u3089\u6700\u9ad8\u3002\n\u3068\u306f\u3044\u3044\u3064\u3064\u3001\u307e\u305f\u6642\u9593\u3092\u898b\u3064\u3051\u3066\u305f\u3081\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3046\u3002\nEMR\u306b\u53d6\u308a\u8fbc\u307e\u308c\u3066\u304f\u308c\u308b\u3068\u66f4\u306b\u3046\u308c\u3057\u3044\u306a\u3068\u3082\u601d\u3046\u3051\u3069\u3001\u3053\u308c\u306b\u306fspark-llap\u304c\u672c\u5bb6Spark\u306b\u53d6\u308a\u8fbc\u307e\u308c\u308b\u304b\u3001Hortonworks\u304b\u3089spark package\u3068\u3057\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u304b\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u3067\u304d\u306a\u3044\u306e\u3067\u3001\u307e\u3060\u307e\u3060\u6642\u9593\u306f\u304b\u304b\u308a\u305d\u3046\u3002\n## \u3053\u306e\u6587\u7ae0\u306b\u3064\u3044\u3066\n\n\u3053\u306e\u30dd\u30b9\u30c8\u306f\u3001\u5148\u65e5[Hadoop\u3068S3](http://qiita.com/imaifactory/items/6866a5c5d8cc363ace12)\u3068\u3044\u3046\u30bf\u30a4\u30c8\u30eb\u3067\u66f8\u3044\u305f\u4e0b\u8a18\u306e\u3088\u3046\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u4f5c\u308b\u306b\u3042\u305f\u3063\u3066\u306eZeppelin + Spark + LLAP\u307e\u308f\u308a\u306e\u8abf\u67fb\u3068\u691c\u8a3c\u72b6\u6cc1\u3092\u81ea\u5206\u306e\u305f\u3081\u306e\u30e1\u30e2\u3068\u3057\u3066\u307e\u3068\u3081\u305f\u3082\u306e\u3067\u3059\u3002\n\n![architecture](https://qiita-image-store.s3.amazonaws.com/0/10292/e0e40daa-688b-cd1d-7dae-4911543fe41c.png)\n\n## Motivation\n\nSpark\u3067S3\u4e0a\u306e\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b\u306b\u3042\u305f\u3063\u3066\u30ec\u30a4\u30c6\u30f3\u30b7\u3092\u3067\u304d\u308b\u3060\u3051\u4e0b\u3052\u3001\u304b\u3064S3\u306e\u30b9\u30ed\u30c3\u30c8\u30ea\u30f3\u30b0\u3092\u907f\u3051\u308b\u305f\u3081\u306b\u3067\u304d\u308b\u3060\u3051\u30a2\u30af\u30bb\u30b9\u983b\u5ea6\u3092\u4e0b\u3052\u305f\u3044\u3068\u3044\u3046\u80cc\u666f\u304b\u3089spark-llap\u3092\u8a66\u3057\u3066\u3044\u308b\u3002\n\nhttps://github.com/hortonworks-spark/spark-llap\n\nspark-llap\u306f\u5358\u7d14\u306bHiveserver2\u7d4c\u7531\u3067Hive LLAP\u306e\u5b9f\u884c\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u308f\u3051\u3067\u306f\u306a\u304f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002\n\n![Kobito.NPnFd7.png](https://qiita-image-store.s3.amazonaws.com/0/10292/fe1d37f2-4e3d-fddf-31d8-fdd4202bfdfc.png \"Kobito.NPnFd7.png\")\n\n*\u753b\u50cf\u306fHortonworks\u306eHive\u958b\u767a\u8005\u3001[Sergey Shelulkhin\u306eSlideshare](http://www.slideshare.net/HadoopSummit/llap-subsecond-analytical-queries-in-hive)\u304b\u3089\u62dd\u501f\u3002\n\n\u3053\u306e\u65b9\u6cd5\u306e\u30e1\u30ea\u30c3\u30c8\u306f\u3001Hiveserver2\u306b\u96c6\u7d04\u3055\u308c\u3066Thrift\u306b\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u3055\u308c\u305f\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u306e\u3067\u306f\u306a\u304f\u3001Spark\u306e\u5404Executor\u304c\u76f4\u63a5LLAP Daemon\u304b\u3089\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u3001\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u304c\u306a\u3044\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3002\n\n## Build\n\nREADME\u306b\u306f`build/sbt package`\u3057\u308d\u3068\u66f8\u3044\u3066\u3042\u308b\u304c\u3053\u308c\u3058\u3083\u3060\u3081\u3067\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306b`build/sbt assembly`\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\uff08\u307e\u3042\u305d\u308a\u3083\u305d\u3046\u304b\uff09\n\nhttps://github.com/hortonworks-spark/spark-llap/issues/9\n\nmaster\u30d6\u30e9\u30f3\u30c1\u306fSpark1.6\u7528\u306a\u306e\u3067\u3053\u3093\u306a\u611f\u3058\u306b\u30d3\u30eb\u30c9\u3067\u304d\u308b\u3002\n\nbuild/sbt -Dspark.version=1.6.2.2.5.3.0-37 -Dhadoop.version=2.7.3.2.5.3.0-37 -Dhive.version=2.1.0.2.5.3.0-37 -Dtez.version=0.8.4.2.5.3.0-37 -Drepourl=http://nexus-private.hortonworks.com:8081/nexus/content/groups/public/ clean compile assembly\n\nSpark2.0\u7528\u306ebranch-2.0\u3082\u3001\u4e0b\u8a18\u306e\u3088\u3046\u306a\u611f\u3058\u3067\u30d3\u30eb\u30c9\u3002\n\nbuild/sbt -Dspark.version=2.0.0.2.5.3.0-37 -Dhadoop.version=2.7.3.2.5.3.0-37 -Dhive.version=2.1.0.2.5.3.0-37 -Dtez.version=0.8.4.2.5.3.0-37 -Drepourl=http://nexus-private.hortonworks.com:8081/nexus/content/groups/public/ clean compile assembly\n\n\u3069\u3061\u3089\u3082\u3046\u307e\u304f\u3044\u3063\u305f\u3002Hortonworks\u306erepository\u3092\u4f7f\u308f\u306a\u304d\u3083\u3044\u3051\u306a\u3044\u304b\u3069\u3046\u304b\u306f\u30fb\u30fb\u30fb\u305f\u3081\u3057\u3066\u3044\u306a\u3044\u3002\n\n## \u52d5\u304b\u3057\u3066\u307f\u308b\n\nREADME\u306b\u5f93\u3063\u3066\u4e0b\u8a18\u3092spark-default.conf\u306b\u8a2d\u5b9a\n\n```\nspark.sql.hive.hiveserver2.url=jdbc:hive2://hiveserver2:10500\nspark.hadoop.hive.llap.daemon.service.hosts=@llap0\nspark.hadoop.hive.zookeeper.quorum=zookeeper:2181\n```\n\nSpark Thrift Server\u4f7f\u3046\u306a\u3089\u4ed6\u306b\u3082\u8a2d\u5b9a\u304c\u5fc5\u8981\u3060\u304c\u3053\u3053\u3067\u306f\u30b9\u30ad\u30c3\u30d7\u3002\u3053\u306e\u4e0a\u3067`spark-shell --jars PATH/TO/ASSEMBRY_JAR`\u3067spark-shell\u3092\u8d77\u52d5\u3002\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u52d5\u3044\u3066\u304f\u308c\u305f\u3002\n\n```\nimport org.apache.spark.sql.hive.llap.LlapContext\nvar llapContext = new LlapContext(sc)\n\nval sql = \"SOME HIVE QL\"\n\nllapContext.sql(sql).show\n```\n\nSpark2\u3060\u3068\u4e0b\u8a18\u3060\u3051\u3067\u52d5\u304f\u3089\u3057\u3044\u3002\u30af\u30a8\u30ea\u306f\u901a\u308b\u3051\u3069\u3001\u5b9f\u969b\u306bLLAP\u3067\u30b8\u30e7\u30d6\u304c\u8d70\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\n\n```\nspark.sql(\"SOME HIVE QL\").show\n```\n\n## \u3061\u3083\u3093\u3068\u52d5\u3044\u3066\u3044\u308b\u304b\u78ba\u8a8d\n\nSpark1\u3067\u306e\u5b9f\u884c\u306f\u3044\u308d\u3044\u308d\u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u308b\u306e\u3067\u52d5\u3044\u3066\u3044\u308b\u3063\u307d\u3044\u304c\u3001Spark2\u306e\u306e\u307b\u3046\u306f\u3044\u307e\u3044\u3061\u30c7\u30d0\u30c3\u30b0\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u3002\n\nSpark\u304b\u3089\u52d5\u304b\u3057\u3066\u3044\u308b\u304b\u3089\u306a\u306e\u304b\uff08\u305f\u3076\u3093\u305d\u3046\uff09\u3001Tez View\u306b\u306f\u30ed\u30b0\u306f\u51fa\u3066\u3053\u306a\u3044\u3057\u3001LLAP Web Service(http://llapnode:10502)\u306b\u3082\u306a\u3093\u306e\u60c5\u5831\u3082\u51fa\u3066\u3053\u306a\u3044\n\n## Zeppelin\u3067\u52d5\u304b\u3057\u3066\u307f\u308b\n\nZeppelin\u3067Spark Interpreter\u306b\u5bfe\u3057\u3066spark-llap\u306eassembly jar\u3092\u8db3\u3057\u3066\u3084\u3063\u305f\u3089\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3002\n\n```\n\nimport org.apache.spark.sql.hive.llap.LlapContext\njava.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:225)\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:215)\n\tat org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:458)\n\tat org.apache.spark.sql.SQLContext$$anonfun$4.apply(SQLContext.scala:272)\n\tat org.apache.spark.sql.SQLContext$$anonfun$4.apply(SQLContext.scala:271)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:271)\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\n\tat org.apache.spark.sql.hive.llap.LlapContext.<init>(LlapContext.scala:40)\n\tat org.apache.spark.sql.hive.llap.LlapContext.<init>(LlapContext.scala:60)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:42)\n\tat $iwC$$iwC$$iwC.<init>(<console>:44)\n\tat $iwC$$iwC.<init>(<console>:46)\n\tat $iwC.<init>(<console>:48)\n\tat <init>(<console>:50)\n\tat .<init>(<console>:54)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:717)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:928)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:871)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:864)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\t... 58 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\t... 64 more\nCaused by: javax.jdo.JDOFatalInternalException: Unexpected exception caught.\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1193)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\t... 69 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\t... 88 more\nCaused by: org.datanucleus.exceptions.NucleusException: Error creating the MetaDataManager for API \"JDO\" : \n\tat org.datanucleus.NucleusContext.getMetaDataManager(NucleusContext.java:1001)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.initialiseMetaData(JDOPersistenceManagerFactory.java:703)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.<init>(JDOPersistenceManagerFactory.java:511)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:301)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\t... 96 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n\tat org.datanucleus.NucleusContext.getMetaDataManager(NucleusContext.java:995)\n\t... 100 more\nCaused by: org.datanucleus.metadata.InvalidClassMetaDataException: Class \"org.apache.hadoop.hive.metastore.model.MConstraint\" has MetaData yet the class cant be found. Please check your CLASSPATH specifications.\n\tat org.datanucleus.metadata.AbstractClassMetaData.loadClass(AbstractClassMetaData.java:579)\n\tat org.datanucleus.metadata.ClassMetaData.populate(ClassMetaData.java:166)\n\tat org.datanucleus.metadata.MetaDataManager$1.run(MetaDataManager.java:2918)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.datanucleus.metadata.MetaDataManager.populateAbstractClassMetaData(MetaDataManager.java:2912)\n\tat org.datanucleus.metadata.MetaDataManager.populateFileMetaData(MetaDataManager.java:2735)\n\tat org.datanucleus.api.jdo.metadata.JDOMetaDataManager.loadXMLMetaDataForClass(JDOMetaDataManager.java:786)\n\tat org.datanucleus.api.jdo.metadata.JDOMetaDataManager.getMetaDataForClassInternal(JDOMetaDataManager.java:383)\n\tat org.datanucleus.api.jdo.metadata.JDOMetaDataManager$MetaDataRegisterClassListener.registerClass(JDOMetaDataManager.java:207)\n\tat javax.jdo.spi.JDOImplHelper.addRegisterClassListener(JDOImplHelper.java:462)\n\tat org.datanucleus.api.jdo.metadata.JDOMetaDataManager.<init>(JDOMetaDataManager.java:194)\n\t... 107 more\nCaused by: org.datanucleus.exceptions.ClassNotResolvedException: Class \"org.apache.hadoop.hive.metastore.model.MConstraint\" was not found in the CLASSPATH. Please check your specification and your CLASSPATH.\n\tat org.datanucleus.ClassLoaderResolverImpl.classForName(ClassLoaderResolverImpl.java:216)\n\tat org.datanucleus.ClassLoaderResolverImpl.classForName(ClassLoaderResolverImpl.java:368)\n\tat org.datanucleus.metadata.AbstractClassMetaData.loadClass(AbstractClassMetaData.java:569)\n\t... 117 more\n  ```\n\n## \u3044\u307e\u306e\u3068\u3053\u308d\u306e\u7d50\u8ad6\n\nZeppelin\u306e\u30a8\u30e9\u30fc\u306f\u3061\u3083\u3093\u3068\u8ffd\u3048\u3070\u30b7\u30e5\u30fc\u30c8\u3067\u304d\u308b\u304b\u3082\u3060\u3051\u3069\u3001\u305d\u3053\u307e\u3067\u6642\u9593\u306f\u639b\u3051\u3089\u308c\u305d\u3046\u306b\u306a\u3044\u306e\u3067\u3001\u4e00\u65e6\u6b21\u306eHDP\u306e\u30ea\u30ea\u30fc\u30b9\u3092\u5f85\u3064\u306e\u304c\u3044\u3044\u304b\u306a\u3002LLAP\u3092GA\u306b\u3057\u3066\u3001Zeppelin\u304b\u3089\uff08\u3067\u304d\u308c\u3070Spark2\u3067\uff09\u547c\u3079\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u308c\u308b\u3068\u3053\u308d\u307e\u3067\u6574\u3063\u3066\u3044\u305f\u3089\u6700\u9ad8\u3002\n\n\u3068\u306f\u3044\u3044\u3064\u3064\u3001\u307e\u305f\u6642\u9593\u3092\u898b\u3064\u3051\u3066\u305f\u3081\u3057\u3066\u307f\u3088\u3046\u3068\u601d\u3046\u3002\n\nEMR\u306b\u53d6\u308a\u8fbc\u307e\u308c\u3066\u304f\u308c\u308b\u3068\u66f4\u306b\u3046\u308c\u3057\u3044\u306a\u3068\u3082\u601d\u3046\u3051\u3069\u3001\u3053\u308c\u306b\u306fspark-llap\u304c\u672c\u5bb6Spark\u306b\u53d6\u308a\u8fbc\u307e\u308c\u308b\u304b\u3001Hortonworks\u304b\u3089spark package\u3068\u3057\u3066\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u308b\u304b\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u3067\u304d\u306a\u3044\u306e\u3067\u3001\u307e\u3060\u307e\u3060\u6642\u9593\u306f\u304b\u304b\u308a\u305d\u3046\u3002\n", "tags": ["Zeppelin", "hive", "Spark"]}