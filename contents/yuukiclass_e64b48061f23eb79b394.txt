{"tags": ["Python", "Chainer", "DeepLearning"], "context": " More than 1 year has passed since last update.\u524d\u56desin\u95a2\u6570\u3092chainer\u3092\u4f7f\u3063\u3066\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u3066\u307f\u305f\u306e\u3060\u304c\u3001\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u305f\u3081\u3044\u308d\u3044\u308d\u53ef\u8996\u5316\u3057\u306a\u304c\u3089\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u3066\u307f\u305f\u3002\u7d50\u8ad6\u304b\u3089\u8a00\u3046\u3068\u5931\u6557\u306b\u7d42\u308f\u3063\u305f\u3002\u306a\u305c\u306a\u306e\u304b\u672a\u3060\u306b\u308f\u304b\u3089\u306a\u3044\uff08\u5c40\u6240\u89e3\u306b\u9665\u3063\u305f\u304b\u6c17\u304c\u3059\u308b\u306e\u3060\u304c\u30fb\u30fb\u30fb\u30fb\uff09\u3002\u3054\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3059\u3054\u304f\u3042\u308a\u304c\u305f\u3044\u3002\n\u524d\u56de\u3068\u7570\u306a\u308b\u70b9\n-sin(x)\u3067\u306f\u306a\u304fsin(0.25x)\u306e\u8fd1\u4f3c\u3092\u76ee\u6a19\u3068\u3057\u305f\n-y\u306e\u5024\u3092\u51fa\u529b\u3055\u305b\u3066\u307f\u305f\n-\u640d\u5931\u5e73\u5747\u306e\u30b0\u30e9\u30d5\u3092\u51fa\u529b\u3057\u305f\n\nsin_NN.py\nimport numpy as np\nimport six\nimport chainer\nfrom chainer import computational_graph as c\nfrom chainer import cuda\nimport chainer.functions as F\nfrom chainer import optimizers\nimport matplotlib.pyplot as plt\nimport csv\n\n\ndef make_dateset():\n    x_train = np.arange(0,3.14*40.0,0.5)\n    y_train = np.sin(0.25 * x_train).astype(np.float32)\n\n    f = open('sin_train.csv','ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(x_train)\n    csvWriter.writerow(y_train)\n    f.close()\n\n    x_test  = np.arange(3.14*40.0,3.14 * 60.0,0.5)\n    y_test = np.sin(0.25 * x_test).astype(np.float32)\n    return x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n\n\ndef forward(x_data,y_data,train = True,pred_flag = False):\n    if pred_flag:\n        x = chainer.Variable(x_data)\n        train = False\n    else:\n        x,t = chainer.Variable(x_data),chainer.Variable(y_data)\n\n    h1 = F.dropout(F.relu(model.l1(x)),  train=train)\n    h2 = F.dropout(F.relu(model.l2(h1)),  train=train)\n    h3 = F.dropout(F.relu(model.l3(h2)),  train=train)\n    y = model.l4(h3)\n\n    if pred_flag:\n        return y\n    else:\n        return F.mean_squared_error(y,t)\n\n\nif __name__ == \"__main__\":\n\n\n    x_train,y_train,x_test,y_test = make_dateset()\n    x_train = x_train.reshape(len(x_train),1)\n    y_train = y_train.reshape(len(y_train),1)\n    x_test = x_test.reshape(len(x_test),1)\n    y_test = y_test.reshape(len(y_test),1)\n\n\n    xp = np\n\n    batchsize = 20\n    N = len(x_train)\n    N_test = len(x_test)\n    n_epoch = 500\n    n_units = 10\n\n    model = chainer.FunctionSet(l1=F.Linear(1, n_units),\n                                l2=F.Linear(n_units, n_units),\n                                l3=F.Linear(n_units, n_units),\n                                l4=F.Linear(n_units, 1))\n\n    optimizer = optimizers.Adam()\n    optimizer.setup(model.collect_parameters())\n\n\n    loss_means = []\n\n    for epoch in six.moves.range(1, n_epoch + 1):\n        print('epoch', epoch)\n\n        #train\n        perm = np.random.permutation(N)\n        sum_loss = 0\n        sum_accuracy = 0\n        for i in six.moves.range(0, N, batchsize):\n            x_batch = xp.asarray(x_train[perm[i:i + batchsize]])\n            y_batch = xp.asarray(y_train[perm[i:i + batchsize]])\n            optimizer.zero_grads()\n            loss = forward(x_batch, y_batch)\n            loss.backward()\n            optimizer.update()\n            sum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n        print \"train mean loss = \",sum_loss/N\n\n        #evaluation\n        sum_loss = 0\n        sum_accuracy = 0\n        for i in six.moves.range(0, N_test, batchsize):\n            x_batch = xp.asarray(x_test[i:i+batchsize])\n            y_batch = xp.asarray(y_test[i:i+batchsize])\n            loss = forward(x_batch, y_batch, train=False)\n            sum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n\n            ###################################################\n        if epoch == 200:\n            #predict\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n            x_pre = np.arange(3.14*80.0,3.14*120.0,0.5)\n            x_pre = x_pre.astype(np.float32)\n            y_pre = np.sin(0.25 * x_pre).astype(np.float32)\n            y_pre = y_pre.reshape(1,len(y_pre))\n            answer = []\n\n            #predict\n            for g in range(0,len(x_pre)-1):\n                xx =  np.asarray([[x_pre[g]]])\n                y1 = forward(x_data = xx,y_data = None,train = False,pred_flag=True)\n                answer.append(y1.data[0][0])\n\n            f = open('sin_pre.csv','ab')\n            csvWriter = csv.writer(f)\n            csvWriter.writerow(x_pre)\n            csvWriter.writerow(y_pre[0])\n            csvWriter.writerow(answer)\n            f.close()\n            ####################\n        print \"test mean loss = \",sum_loss/N_test\n        loss_means.append(sum_loss/N_test)\n\n    f = open('loss_means.csv','ab')\n    csvWriter = csv.writer(f)\n    csvWriter.writerow(loss_means)\n    f.close()\n\n\n-Deep Learning\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\n\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u300020\nepoch\u6570\uff08\u5b66\u7fd2\u56de\u6570\uff09\u3000500\n\u30e6\u30cb\u30c3\u30c8\u6570\u30001-20-20-1\n\u6d3b\u6027\u5316\u95a2\u6570\u3000ReLu(\u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570\uff09\n\u66f4\u65b0\u65b9\u6cd5 Adam\n\u640d\u5931\u8aa4\u5dee\u95a2\u6570\u3000\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\n\u307e\u305a\u4eca\u56de\u5b66\u7fd2\u3055\u305b\u308b\u30c7\u30fc\u30bf(train data)\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u305f\ny=sin(0.25x)\n\n0<x<3.14*40\u306e\u7bc4\u56f2\u30670.5\u305a\u3064\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f5c\u6210\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306f3.14*40.0<x<60.0\u306e\u7bc4\u56f2\u30670.5\u9593\u9694\u3067\u4f5c\u6210\u3057\u305f\n\n\u640d\u5931\u5e73\u5747\u30b0\u30e9\u30d5\nsin\u95a2\u6570\u306e\u8fd1\u4f3c\u304c\u3042\u307e\u308a\u306b\u3082\u3067\u304d\u306a\u3044\u306e\u3067\u78ba\u8a8d\u306e\u305f\u3081\u306b\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u305f\u3002\u6700\u521d\u306e\u640d\u5931\u5e73\u5747\u5024\u304c\u5927\u304d\u3059\u304e\u3066epoch13\u3042\u305f\u308a\u304b\u30890\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u304c\u5b9f\u969b\u306f0.5\u4ed8\u8fd1\u3092\u3046\u308d\u3046\u308d\u3057\u3066\u3044\u308b\u30020.5\u307e\u3067\u6e1b\u5c11\u3059\u308b\u306e\u3060\u304c\u3001\u305d\u3053\u304b\u3089\u5168\u304f\u6e1b\u5c11\u3057\u306a\u3044\u3002\u30d0\u30c3\u30c1\u6570\u30fb\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u5909\u3048\u3066\u30820.5\u4ed8\u8fd1\u3067\u6e1b\u5c11\u304c\u6b62\u307e\u3063\u3066\u3057\u307e\u3063\u305f\u3002\n\n\n\u4e88\u60f3\u30b0\u30e9\u30d5\n\u5b9f\u969b\u306b\u3069\u306e\u3088\u3046\u306a\u95a2\u6570\u306b\u306a\u308b\u304b\u51fa\u529b\u3057\u3066\u307f\u305f\u3002epoch\u304c200\u6642\n\n\u9752\u304c\u6b63\u89e3\uff08\u672c\u6765\u306e0.25*sin(x)\u95a2\u6570\uff09\u3001\u30aa\u30ec\u30f3\u30b8\u304cDeep Learning\u306e\u51fa\u529b\u95a2\u6570\u3002\u307e\u3042\u3053\u308c\u306a\u3089\u640d\u5931\u5e73\u5747\u30820.5\u306b\u306a\u308b\u308f\u306a\u3063\u3066\u611f\u3058\u3067\u3059\u3002\u5c40\u6240\u89e3\u306b\u9665\u3063\u3066\u3057\u307e\u3063\u305f\u5834\u5408\u306e\u89e3\u6c7a\u65b9\u6cd5\u3068\u3057\u3066\u306f\u5b66\u7fd2\u7387\u3092\u3044\u3058\u308c\u3070\u3088\u304b\u3063\u305f\u6c17\u304c\u3059\u308b\u3093\u3060\u3051\u3069\u307e\u3060\u307e\u3060\u52c9\u5f37\u4e2d\u306a\u306e\u3067\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u304b\u30fb\u30fb\u30fb\u30fb\u30fb\n\u3069\u306a\u305f\u304b\u52a9\u3051\u3066\u304f\u3060\u3055\u3044\uff08\u7b11\uff09\n[\u524d\u56de](http://qiita.com/yuukiclass/items/8b197efac1904248fd9c)sin\u95a2\u6570\u3092chainer\u3092\u4f7f\u3063\u3066\u8fd1\u4f3c\u3057\u3088\u3046\u3068\u3057\u3066\u307f\u305f\u306e\u3060\u304c\u3001\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u305f\u3081\u3044\u308d\u3044\u308d\u53ef\u8996\u5316\u3057\u306a\u304c\u3089\u518d\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u3066\u307f\u305f\u3002\u7d50\u8ad6\u304b\u3089\u8a00\u3046\u3068\u5931\u6557\u306b\u7d42\u308f\u3063\u305f\u3002\u306a\u305c\u306a\u306e\u304b\u672a\u3060\u306b\u308f\u304b\u3089\u306a\u3044\uff08\u5c40\u6240\u89e3\u306b\u9665\u3063\u305f\u304b\u6c17\u304c\u3059\u308b\u306e\u3060\u304c\u30fb\u30fb\u30fb\u30fb\uff09\u3002\u3054\u6307\u6458\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u3059\u3054\u304f\u3042\u308a\u304c\u305f\u3044\u3002\n\n**\u524d\u56de\u3068\u7570\u306a\u308b\u70b9**\n-sin(x)\u3067\u306f\u306a\u304fsin(0.25x)\u306e\u8fd1\u4f3c\u3092\u76ee\u6a19\u3068\u3057\u305f\n-y\u306e\u5024\u3092\u51fa\u529b\u3055\u305b\u3066\u307f\u305f\n-\u640d\u5931\u5e73\u5747\u306e\u30b0\u30e9\u30d5\u3092\u51fa\u529b\u3057\u305f\n\n\n```lang:sin_NN.py\nimport numpy as np\nimport six\nimport chainer\nfrom chainer import computational_graph as c\nfrom chainer import cuda\nimport chainer.functions as F\nfrom chainer import optimizers\nimport matplotlib.pyplot as plt\nimport csv\n\n\ndef make_dateset():\n\tx_train = np.arange(0,3.14*40.0,0.5)\n\ty_train = np.sin(0.25 * x_train).astype(np.float32)\n\n\tf = open('sin_train.csv','ab')\n\tcsvWriter = csv.writer(f)\n\tcsvWriter.writerow(x_train)\n\tcsvWriter.writerow(y_train)\n\tf.close()\n\n\tx_test  = np.arange(3.14*40.0,3.14 * 60.0,0.5)\n\ty_test = np.sin(0.25 * x_test).astype(np.float32)\n\treturn x_train.astype(np.float32),y_train.astype(np.float32),x_test.astype(np.float32),y_test.astype(np.float32)\n\n\n\t\ndef forward(x_data,y_data,train = True,pred_flag = False):\n\tif pred_flag:\n\t\tx = chainer.Variable(x_data)\n\t\ttrain = False\n\telse:\n\t\tx,t = chainer.Variable(x_data),chainer.Variable(y_data)\n\n\th1 = F.dropout(F.relu(model.l1(x)),  train=train)\n\th2 = F.dropout(F.relu(model.l2(h1)),  train=train)\n\th3 = F.dropout(F.relu(model.l3(h2)),  train=train)\n\ty = model.l4(h3)\n\n\tif pred_flag:\n\t\treturn y\n\telse:\n\t\treturn F.mean_squared_error(y,t)\n\n\nif __name__ == \"__main__\":\n\n\t\n\tx_train,y_train,x_test,y_test = make_dateset()\n\tx_train = x_train.reshape(len(x_train),1)\n\ty_train = y_train.reshape(len(y_train),1)\n\tx_test = x_test.reshape(len(x_test),1)\n\ty_test = y_test.reshape(len(y_test),1)\n\n\n\txp = np\n\t\n\tbatchsize = 20\n\tN = len(x_train)\n\tN_test = len(x_test)\n\tn_epoch = 500\n\tn_units = 10\n\n\tmodel = chainer.FunctionSet(l1=F.Linear(1, n_units),\n\t\t\t\t\t\t\t\tl2=F.Linear(n_units, n_units),\n\t\t\t\t\t\t\t\tl3=F.Linear(n_units, n_units),\n\t\t\t\t\t\t\t\tl4=F.Linear(n_units, 1))\n\n\toptimizer = optimizers.Adam()\n\toptimizer.setup(model.collect_parameters())\n\n\n\tloss_means = []\n\n\tfor epoch in six.moves.range(1, n_epoch + 1):\n\t\tprint('epoch', epoch)\n\t\t\n\t\t#train\n\t\tperm = np.random.permutation(N)\n\t\tsum_loss = 0\n\t\tsum_accuracy = 0\n\t\tfor i in six.moves.range(0, N, batchsize):\n\t\t\tx_batch = xp.asarray(x_train[perm[i:i + batchsize]])\n\t\t\ty_batch = xp.asarray(y_train[perm[i:i + batchsize]])\n\t\t\toptimizer.zero_grads()\n\t\t\tloss = forward(x_batch, y_batch)\n\t\t\tloss.backward()\n\t\t\toptimizer.update()\n\t\t\tsum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n\t\tprint \"train mean loss = \",sum_loss/N\n\n\t\t#evaluation\n\t\tsum_loss = 0\n\t\tsum_accuracy = 0\n\t\tfor i in six.moves.range(0, N_test, batchsize):\n\t\t\tx_batch = xp.asarray(x_test[i:i+batchsize])\n\t\t\ty_batch = xp.asarray(y_test[i:i+batchsize])\n\t\t\tloss = forward(x_batch, y_batch, train=False)\n\t\t\tsum_loss += float(cuda.to_cpu(loss.data)) * len(y_batch)\n\n\t\t\t###################################################\n\t\tif epoch == 200:\n\t\t\t#predict\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n\t\t\tx_pre = np.arange(3.14*80.0,3.14*120.0,0.5)\n\t\t\tx_pre = x_pre.astype(np.float32)\n\t\t\ty_pre = np.sin(0.25 * x_pre).astype(np.float32)\n\t\t\ty_pre = y_pre.reshape(1,len(y_pre))\n\t\t\tanswer = []\n\n\t\t\t#predict\n\t\t\tfor g in range(0,len(x_pre)-1):\n\t\t\t\txx =  np.asarray([[x_pre[g]]])\n\t\t\t\ty1 = forward(x_data = xx,y_data = None,train = False,pred_flag=True)\n\t\t\t\tanswer.append(y1.data[0][0])\n\n\t\t\tf = open('sin_pre.csv','ab')\n\t\t\tcsvWriter = csv.writer(f)\n\t\t\tcsvWriter.writerow(x_pre)\n\t\t\tcsvWriter.writerow(y_pre[0])\n\t\t\tcsvWriter.writerow(answer)\n\t\t\tf.close()\n\t\t\t####################\n\t\tprint \"test mean loss = \",sum_loss/N_test\n\t\tloss_means.append(sum_loss/N_test)\n\n\tf = open('loss_means.csv','ab')\n\tcsvWriter = csv.writer(f)\n\tcsvWriter.writerow(loss_means)\n\tf.close()\n```\n\n-Deep Learning\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\n\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u300020\nepoch\u6570\uff08\u5b66\u7fd2\u56de\u6570\uff09\u3000500\n\u30e6\u30cb\u30c3\u30c8\u6570\u30001-20-20-1\n\u6d3b\u6027\u5316\u95a2\u6570\u3000ReLu(\u6b63\u898f\u5316\u7dda\u5f62\u95a2\u6570\uff09\n\u66f4\u65b0\u65b9\u6cd5 Adam\n\u640d\u5931\u8aa4\u5dee\u95a2\u6570\u3000\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u95a2\u6570\n\n\n##\u5b66\u7fd2\u30c7\u30fc\u30bf\n\u307e\u305a\u4eca\u56de\u5b66\u7fd2\u3055\u305b\u308b\u30c7\u30fc\u30bf(train data)\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u305f\ny=sin(0.25x)\n![sin_train.PNG](https://qiita-image-store.s3.amazonaws.com/0/90668/eaf3b30a-a889-2738-9dab-c4a75cadf074.png)\n\n0<x<3.14*40\u306e\u7bc4\u56f2\u30670.5\u305a\u3064\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f5c\u6210\n\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306f3.14*40.0<x<60.0\u306e\u7bc4\u56f2\u30670.5\u9593\u9694\u3067\u4f5c\u6210\u3057\u305f\n\n##\u640d\u5931\u5e73\u5747\u30b0\u30e9\u30d5\nsin\u95a2\u6570\u306e\u8fd1\u4f3c\u304c\u3042\u307e\u308a\u306b\u3082\u3067\u304d\u306a\u3044\u306e\u3067\u78ba\u8a8d\u306e\u305f\u3081\u306b\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u305f\u3002\u6700\u521d\u306e\u640d\u5931\u5e73\u5747\u5024\u304c\u5927\u304d\u3059\u304e\u3066epoch13\u3042\u305f\u308a\u304b\u30890\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u304c\u5b9f\u969b\u306f0.5\u4ed8\u8fd1\u3092\u3046\u308d\u3046\u308d\u3057\u3066\u3044\u308b\u30020.5\u307e\u3067\u6e1b\u5c11\u3059\u308b\u306e\u3060\u304c\u3001\u305d\u3053\u304b\u3089\u5168\u304f\u6e1b\u5c11\u3057\u306a\u3044\u3002\u30d0\u30c3\u30c1\u6570\u30fb\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u5909\u3048\u3066\u30820.5\u4ed8\u8fd1\u3067\u6e1b\u5c11\u304c\u6b62\u307e\u3063\u3066\u3057\u307e\u3063\u305f\u3002\n![loss_mean.PNG](https://qiita-image-store.s3.amazonaws.com/0/90668/0154de09-6741-0bda-6d23-8d11c9fe5076.png)\n\n\n##\u4e88\u60f3\u30b0\u30e9\u30d5\n\u5b9f\u969b\u306b\u3069\u306e\u3088\u3046\u306a\u95a2\u6570\u306b\u306a\u308b\u304b\u51fa\u529b\u3057\u3066\u307f\u305f\u3002epoch\u304c200\u6642\n![sin_predict.PNG](https://qiita-image-store.s3.amazonaws.com/0/90668/f85ad15b-29ba-3c33-2bc6-9fa3c7b092cb.png)\n\n\u9752\u304c\u6b63\u89e3\uff08\u672c\u6765\u306e0.25*sin(x)\u95a2\u6570\uff09\u3001\u30aa\u30ec\u30f3\u30b8\u304cDeep Learning\u306e\u51fa\u529b\u95a2\u6570\u3002\u307e\u3042\u3053\u308c\u306a\u3089\u640d\u5931\u5e73\u5747\u30820.5\u306b\u306a\u308b\u308f\u306a\u3063\u3066\u611f\u3058\u3067\u3059\u3002\u5c40\u6240\u89e3\u306b\u9665\u3063\u3066\u3057\u307e\u3063\u305f\u5834\u5408\u306e\u89e3\u6c7a\u65b9\u6cd5\u3068\u3057\u3066\u306f\u5b66\u7fd2\u7387\u3092\u3044\u3058\u308c\u3070\u3088\u304b\u3063\u305f\u6c17\u304c\u3059\u308b\u3093\u3060\u3051\u3069\u307e\u3060\u307e\u3060\u52c9\u5f37\u4e2d\u306a\u306e\u3067\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u304b\u30fb\u30fb\u30fb\u30fb\u30fb\n\n\n\u3069\u306a\u305f\u304b\u52a9\u3051\u3066\u304f\u3060\u3055\u3044\uff08\u7b11\uff09\n\n\n\n\n\n"}