{"context": " More than 1 year has passed since last update.\u4eca\u56de\u306f\u3001Apache hadoop\u30d5\u30a1\u30df\u30ea\u30fc\u306eHDFS\u3092\u8a66\u3057\u3066\u307f\u305f\n\u3061\u306a\u307f\u306b\u3001HDFS\u3068\u306f\u3001\"Hadoop Distributed File System\"\u3068\u306e\u3053\u3068\u3060\u305d\u3046\u3067\u3059\u3002\n\n\u2b1b\ufe0e HDFS\u74b0\u5883\u69cb\u7bc9\n\nJDK\u74b0\u5883\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3001JAVA_HOME\u74b0\u5883\u5909\u6570\u306b\u30bb\u30c3\u30c8\u3059\u308b\n\n$ sudo apt-get install default-jdk\n$ sudo update-alternatives --list java\n/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n\n$ cd $HOME\n$ vi .profile\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport PATH=$HOME/hadoop-2.7.2/bin:$HOME/hadoop-2.7.2/sbin:$PATH\n$ source .profile\n\n\nssh\u74b0\u5883\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\n\n$ sudo apt-get install ssh\n$ sudo apt-get install rsync\n$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n\nssh localhost\u3067\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u554f\u3044\u5408\u308f\u305b\u304c\u7121\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\n$ ssh localhost\n$ exit\n\n\nHadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\n\n$ wget http://www.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz\n$ tar xfz hadoop-2.7.2.tar.gz\n$ cd hadoop-2.7.2/etc/hadoop/\n\n\nhadoop\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u4fee\u6b63\u3059\u308b\n\n\u307e\u305a\u3001core-site.xml\u3092\u7de8\u96c6\u3059\u308b\n$ vi core-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n    <property>\n        <name>fs.default.name</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n\n\u3064\u3065\u3044\u3066\u3001hdfs-site.xml\u3092\u7de8\u96c6\u3059\u308b\n$ vi hdfs-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n\n\u305d\u3057\u3066\u3001hadoop-env.sh\u3082\u7de8\u96c6\u3057\u3066\u304a\u304f\n$ vi hadoop-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n\n\nNameNode\u3092\u521d\u671f\u5316\u3059\u308b\n\n$ hdfs namenode -format\n16/03/04 03:55:40 INFO namenode.NameNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = HDFS/127.0.1.1\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 2.7.2\n\n... (snip)\n\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at HDFS/127.0.1.1\n************************************************************/\n\n\nhadoop\u3092\u8d77\u52d5\u3059\u308b\n\n$ start-dfs.sh \nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-namenode-HDFS.out\nlocalhost: starting datanode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-datanode-HDFS.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-secondarynamenode-HDFS.out\n\n\nhadoop\u30d7\u30ed\u30bb\u30b9\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b\n\n$ jps\n12767 Jps\n12656 SecondaryNameNode\n12462 DataNode\n12302 NameNode\n\n\nhadoop\u52d5\u4f5c\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\n\nWeb\u30d6\u30e9\u30a6\u30b6\u304b\u3089\u3001http://localhost:50070/ \u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3001hadoop\u52d5\u4f5c\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\n\n\n\u2b1b\ufe0e HDFS\u3092\u4f7f\u3063\u3066\u307f\u308b\n\nfoo\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u307f\u308b\n\n$ hdfs dfs -ls /\n$ hdfs dfs -mkdir /foo\n$ hdfs dfs -ls /\nFound 1 items\ndrwxr-xr-x   - tsubo supergroup          0 2016-03-04 04:11 /foo\n\n\nUNIX\u30ed\u30fc\u30ab\u30eb\u4e0a\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u3001HDFS\u4e0a\u306b\u4fdd\u7ba1\u3057\u3066\u307f\u308b\n\n$ cat /home/tsubo/test.txt\ntest data\n$ hdfs dfs -put /home/tsubo/test.txt /foo\n@HDFS:~$ hdfs dfs -ls /foo\nFound 1 items\n-rw-r--r--   1 tsubo supergroup         10 2016-03-04 04:14 /foo/test.txt\n\n\nHDFS\u4e0a\u306etest.txt\u30d5\u30a1\u30a4\u30eb\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u3066\u307f\u308b\n\n$ hdfs dfs -cat /foo/test.txt\ntest data\n\n\nWeb\u30d6\u30e9\u30a6\u30b6\u306e\u7ba1\u7406\u753b\u9762\u304b\u3089\u3082\u78ba\u8a8d\u3057\u3066\u307f\u308b\n\n\n\ntest.txt\u30d5\u30a1\u30a4\u30eb\u3092\u524a\u9664\u3059\u308b\n\n$ hdfs dfs -rm /foo/test.txt\n16/03/04 04:20:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\nDeleted /foo/test.txt\n\n\nfoo\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664\u3059\u308b\n\n$ hdfs dfs -rmdir /foo\n$ hdfs dfs -ls /\n\n\n\u6700\u5f8c\u306b\u3001Hadoop\u3092\u505c\u6b62\u3057\u3066\u304a\u304f\n\n$ stop-dfs.sh\nStopping namenodes on [localhost]\nlocalhost: stopping namenode\nlocalhost: stopping datanode\nStopping secondary namenodes [0.0.0.0]\n0.0.0.0: stopping secondarynamenode\n\n\u4ee5\u4e0a\n\u4eca\u56de\u306f\u3001[Apache hadoop\u30d5\u30a1\u30df\u30ea\u30fc\u306eHDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)\u3092\u8a66\u3057\u3066\u307f\u305f\n\u3061\u306a\u307f\u306b\u3001HDFS\u3068\u306f\u3001\"Hadoop Distributed File System\"\u3068\u306e\u3053\u3068\u3060\u305d\u3046\u3067\u3059\u3002\n\n#\u2b1b\ufe0e HDFS\u74b0\u5883\u69cb\u7bc9\n\n- JDK\u74b0\u5883\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3001JAVA_HOME\u74b0\u5883\u5909\u6570\u306b\u30bb\u30c3\u30c8\u3059\u308b\n\n```\n$ sudo apt-get install default-jdk\n$ sudo update-alternatives --list java\n/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n\n$ cd $HOME\n$ vi .profile\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport PATH=$HOME/hadoop-2.7.2/bin:$HOME/hadoop-2.7.2/sbin:$PATH\n$ source .profile\n```\n\n- ssh\u74b0\u5883\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\n\n```\n$ sudo apt-get install ssh\n$ sudo apt-get install rsync\n$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n```\nssh localhost\u3067\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u554f\u3044\u5408\u308f\u305b\u304c\u7121\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\n\n```\n$ ssh localhost\n$ exit\n```\n\n- Hadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\n\n```\n$ wget http://www.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz\n$ tar xfz hadoop-2.7.2.tar.gz\n$ cd hadoop-2.7.2/etc/hadoop/\n```\n- hadoop\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u4fee\u6b63\u3059\u308b\n\n\u307e\u305a\u3001core-site.xml\u3092\u7de8\u96c6\u3059\u308b\n\n```\n$ vi core-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n    <property>\n        <name>fs.default.name</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n```\n\u3064\u3065\u3044\u3066\u3001hdfs-site.xml\u3092\u7de8\u96c6\u3059\u308b\n\n```\n$ vi hdfs-site.xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n```\n\u305d\u3057\u3066\u3001hadoop-env.sh\u3082\u7de8\u96c6\u3057\u3066\u304a\u304f\n\n```\n$ vi hadoop-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n```\n\n\n- NameNode\u3092\u521d\u671f\u5316\u3059\u308b\n\n```\n$ hdfs namenode -format\n16/03/04 03:55:40 INFO namenode.NameNode: STARTUP_MSG: \n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = HDFS/127.0.1.1\nSTARTUP_MSG:   args = [-format]\nSTARTUP_MSG:   version = 2.7.2\n\n... (snip)\n\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at HDFS/127.0.1.1\n************************************************************/\n```\n\n- hadoop\u3092\u8d77\u52d5\u3059\u308b\n\n```\n$ start-dfs.sh \nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-namenode-HDFS.out\nlocalhost: starting datanode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-datanode-HDFS.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /home/tsubo/hadoop-2.7.2/logs/hadoop-tsubo-secondarynamenode-HDFS.out\n```\n\n- hadoop\u30d7\u30ed\u30bb\u30b9\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b\n\n```\n$ jps\n12767 Jps\n12656 SecondaryNameNode\n12462 DataNode\n12302 NameNode\n```\n- hadoop\u52d5\u4f5c\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\n\nWeb\u30d6\u30e9\u30a6\u30b6\u304b\u3089\u3001[http://localhost:50070/](http://localhost:50070/) \u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u3001hadoop\u52d5\u4f5c\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\n\n![Overview.png](https://qiita-image-store.s3.amazonaws.com/0/103595/4b0061d3-ea75-5231-69f1-88fd1ef93263.png)\n\n\n#\u2b1b\ufe0e HDFS\u3092\u4f7f\u3063\u3066\u307f\u308b\n\n- foo\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u307f\u308b\n\n```\n$ hdfs dfs -ls /\n$ hdfs dfs -mkdir /foo\n$ hdfs dfs -ls /\nFound 1 items\ndrwxr-xr-x   - tsubo supergroup          0 2016-03-04 04:11 /foo\n```\n\n- UNIX\u30ed\u30fc\u30ab\u30eb\u4e0a\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u3001HDFS\u4e0a\u306b\u4fdd\u7ba1\u3057\u3066\u307f\u308b\n\n```\n$ cat /home/tsubo/test.txt\ntest data\n$ hdfs dfs -put /home/tsubo/test.txt /foo\n@HDFS:~$ hdfs dfs -ls /foo\nFound 1 items\n-rw-r--r--   1 tsubo supergroup         10 2016-03-04 04:14 /foo/test.txt\n```\n- HDFS\u4e0a\u306etest.txt\u30d5\u30a1\u30a4\u30eb\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u3066\u307f\u308b\n\n```\n$ hdfs dfs -cat /foo/test.txt\ntest data\n```\n\n- Web\u30d6\u30e9\u30a6\u30b6\u306e\u7ba1\u7406\u753b\u9762\u304b\u3089\u3082\u78ba\u8a8d\u3057\u3066\u307f\u308b\n\n![Browse.png](https://qiita-image-store.s3.amazonaws.com/0/103595/7267afdd-6f75-7b0e-3dca-e83648d2dfc3.png)\n\n- test.txt\u30d5\u30a1\u30a4\u30eb\u3092\u524a\u9664\u3059\u308b\n\n```\n$ hdfs dfs -rm /foo/test.txt\n16/03/04 04:20:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\nDeleted /foo/test.txt\n```\n\n- foo\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664\u3059\u308b\n\n```\n$ hdfs dfs -rmdir /foo\n$ hdfs dfs -ls /\n```\n\n- \u6700\u5f8c\u306b\u3001Hadoop\u3092\u505c\u6b62\u3057\u3066\u304a\u304f\n\n```\n$ stop-dfs.sh\nStopping namenodes on [localhost]\nlocalhost: stopping namenode\nlocalhost: stopping datanode\nStopping secondary namenodes [0.0.0.0]\n0.0.0.0: stopping secondarynamenode\n```\n\u4ee5\u4e0a\n", "tags": ["hadoop", "Hdfs"]}