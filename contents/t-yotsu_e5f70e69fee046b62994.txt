{"context": "spark-shell\u304b\u3089\u30d0\u30c3\u30c1\u306b\u5909\u3048\u305f\u969b\u306b\u5c11\u3057\u30cf\u30de\u3063\u305f\u306e\u3067\u3001\u305d\u306e\u6642\u306e\u30e1\u30e2\u3002\n\n\u4e8b\u524d\u6e96\u5099\n\n\u74b0\u5883\nProxy\u74b0\u5883\u4e0b\u306b\u3042\u308bPC\u3067VirtualBOX\u3092\u5229\u7528\u3057\u3066\u4f5c\u6210\u3057\u305f\u4eee\u60f3\u30de\u30b7\u30f3\nHadoop\u306fCDH5.5.1\u3092\u5229\u7528\u3002\u64ec\u4f3c\u5206\u6563\u74b0\u5883\u3002\nSpark\u3082CDH\u306e\u3082\u306e\u3092\u5229\u7528\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_spark_install.html\n\nScala\nCDH\u306eSpark\u304cscala 2.10.4\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3055\u308c\u305f\u3082\u306e\u306e\u3088\u3046\u306a\u306e\u3067\u3001\u540c\u3058\u30d0\u30fc\u30b8\u30e7\u30f3\u306eScala\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3001PATH\u3092\u901a\u3057\u3066\u304a\u304f\u3002\n\u203b\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5408\u308f\u305b\u306a\u3044\u3068\u30b3\u30f3\u30d1\u30a4\u30eb\u6642\u306b\u30a8\u30e9\u30fc\u304c\u767a\u751f\nhttp://www.scala-lang.org/download/2.10.4.html\n\nsbt\n\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3001\u89e3\u51cd\u5f8c\u3001sbt-launch.jar\u3092~/bin\u914d\u4e0b\u306b\u914d\u7f6e\u3059\u308b\u3002\nhttp://www.scala-sbt.org/download.html\n~bin/sbt\u3092\u4f5c\u6210\nSBT_OPTS=\"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\"\njava $JAVA_OPTS $SBT_OPTS -jar `dirname $0`/sbt-launch.jar \"$@\"\n\nJAVA_OPTS\u74b0\u5883\u5909\u6570\u306bProxy\u60c5\u5831\u3092\u8a2d\u5b9a\nJAVA_OPTS= -Dhttp.proxyHost={Proxy\u30b5\u30fc\u30d0\u540d} -Dhttp.proxyPort={Port\u756a\u53f7} -Dhttps.proxyHost={Proxy\u30b5\u30fc\u30d0\u540d} -Dhttps.proxyPort={Port\u756a\u53f7}\n\nsbt-assembly\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306b\u3001project/plugins.sbt\u306b\u4ee5\u4e0b\u3092\u8a18\u8f09\n\nplugins.sbt\nresolvers += \"Bintray sbt plugin releases\" at \"http://dl.bintray.com/sbt/sbt-plugin-releases/\"\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n\n\n\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3002\u30bd\u30fc\u30b9\u306fsrc/main/scala\u914d\u4e0b\u306b\u914d\u7f6e\u3002\nsrc/main/scala/\nsrc/test/scala/\nlib\nproject\ntarget\n\n\nSparkSQL\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u30b3\u30f3\u30d1\u30a4\u30eb\u53ca\u3073\u5b9f\u884c\n\ntest.scala\n\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.DataFrame\n\nobject Test{\n  case class TUser(tId: String, attribute: String)\n  case class NUser(nId: String)\n\n  def exec1(dirPath:String, sc: SparkContext,sqlContext: SQLContext, nDF:DataFrame){\n    try {\n      val filePath = dirPath+\"t.txt\"\n      import sqlContext.implicits._\n      val tDF = sc.textFile(filePath).map { record =>\n        val splitRecord = record.split(\",\")\n        val tId = splitRecord(0)\n        val attribute = splitRecord(1)\n        TUser(tId,attribute)\n      }.toDF\n\n      val tuser = tDF.distinct().count()\n      printf(\"The number of user is %s \\n\",tuser)\n\n      val nuser = nDF.distinct().count()\n      printf(\"The number of nuser is %s \\n\",nuser)\n\n      val tnDF = tDF.join(nDF,tDF(\"tId\") === nDF(\"nId\"),\"inner\").select($\"tId\")\n      val numtnuser = tnDF.distinct().count()\n      printf(\"The number of tnuser is %s \\n\",numtnuser)\n    }\n  }\n\n  def main(args: Array[String]){\n\n    require(args.length >=1, \"Pls specify path\")\n    val dirPath = args(0)\n    val conf = new SparkConf\n    val sc = new SparkContext(conf)\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n    try {\n      val filePath = dirPath+\"n.txt\"\n      import sqlContext.implicits._\n      val nDF = sc.textFile(filePath).map { record =>\n        val splitRecord = record.split(\",\")\n        val nId = splitRecord(0)\n        NUser(nId)\n      }.toDF\n\n    exec1(dirPath,sc,sqlContext,nDF)\n    }\n  }\n}\n\n\n\nbuild.sbt\nname := \"Test\"\nversion := \"0.1\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies ++= Seq(\"org.apache.spark\" % \"spark-core_2.10\" % \"1.5.0\" % \"provided\", \"org.apache.spark\" % \"spark-sql_2.10\" % \"1.5.0\" % \"provided\")\nassemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)\n\n\n\n\u30b3\u30f3\u30d1\u30a4\u30eb\n$ sbt assembly\n\n\n\u30b8\u30e7\u30d6\u5b9f\u884c\n$ spark-submit --class Test--name Test target/scala-2.10/Test-assembly-0.1.jar /user/yotsu/input/\n\nspark-shell\u304b\u3089\u30d0\u30c3\u30c1\u306b\u5909\u3048\u305f\u969b\u306b\u5c11\u3057\u30cf\u30de\u3063\u305f\u306e\u3067\u3001\u305d\u306e\u6642\u306e\u30e1\u30e2\u3002\n\n\n# \u4e8b\u524d\u6e96\u5099\n## \u74b0\u5883\nProxy\u74b0\u5883\u4e0b\u306b\u3042\u308bPC\u3067VirtualBOX\u3092\u5229\u7528\u3057\u3066\u4f5c\u6210\u3057\u305f\u4eee\u60f3\u30de\u30b7\u30f3\nHadoop\u306fCDH5.5.1\u3092\u5229\u7528\u3002\u64ec\u4f3c\u5206\u6563\u74b0\u5883\u3002\nSpark\u3082CDH\u306e\u3082\u306e\u3092\u5229\u7528\u3002\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\u3002\nhttp://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_spark_install.html\n\n## Scala\nCDH\u306eSpark\u304cscala 2.10.4\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3055\u308c\u305f\u3082\u306e\u306e\u3088\u3046\u306a\u306e\u3067\u3001\u540c\u3058\u30d0\u30fc\u30b8\u30e7\u30f3\u306eScala\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3001PATH\u3092\u901a\u3057\u3066\u304a\u304f\u3002\n\u203b\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5408\u308f\u305b\u306a\u3044\u3068\u30b3\u30f3\u30d1\u30a4\u30eb\u6642\u306b\u30a8\u30e9\u30fc\u304c\u767a\u751f\nhttp://www.scala-lang.org/download/2.10.4.html\n\n## sbt\n\u4ee5\u4e0b\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3001\u89e3\u51cd\u5f8c\u3001sbt-launch.jar\u3092~/bin\u914d\u4e0b\u306b\u914d\u7f6e\u3059\u308b\u3002\nhttp://www.scala-sbt.org/download.html\n\n~bin/sbt\u3092\u4f5c\u6210\n\n```sbt\nSBT_OPTS=\"-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M\"\njava $JAVA_OPTS $SBT_OPTS -jar `dirname $0`/sbt-launch.jar \"$@\"\n```\n\nJAVA_OPTS\u74b0\u5883\u5909\u6570\u306bProxy\u60c5\u5831\u3092\u8a2d\u5b9a\n\n```\nJAVA_OPTS= -Dhttp.proxyHost={Proxy\u30b5\u30fc\u30d0\u540d} -Dhttp.proxyPort={Port\u756a\u53f7} -Dhttps.proxyHost={Proxy\u30b5\u30fc\u30d0\u540d} -Dhttps.proxyPort={Port\u756a\u53f7}\n```\n\nsbt-assembly\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306b\u3001project/plugins.sbt\u306b\u4ee5\u4e0b\u3092\u8a18\u8f09\n\n```plugins.sbt\nresolvers += \"Bintray sbt plugin releases\" at \"http://dl.bintray.com/sbt/sbt-plugin-releases/\"\n\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n```\n\n\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3002\u30bd\u30fc\u30b9\u306fsrc/main/scala\u914d\u4e0b\u306b\u914d\u7f6e\u3002\n\n```\nsrc/main/scala/\nsrc/test/scala/\nlib\nproject\ntarget\n````\n\n# SparkSQL\u30b5\u30f3\u30d7\u30eb\u30d7\u30ed\u30b0\u30e9\u30e0\u30b3\u30f3\u30d1\u30a4\u30eb\u53ca\u3073\u5b9f\u884c\n\n```test.scala\n\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.DataFrame\n\nobject Test{\n  case class TUser(tId: String, attribute: String)\n  case class NUser(nId: String)\n\n  def exec1(dirPath:String, sc: SparkContext,sqlContext: SQLContext, nDF:DataFrame){\n    try {\n      val filePath = dirPath+\"t.txt\"\n      import sqlContext.implicits._\n      val tDF = sc.textFile(filePath).map { record =>\n        val splitRecord = record.split(\",\")\n        val tId = splitRecord(0)\n        val attribute = splitRecord(1)\n        TUser(tId,attribute)\n      }.toDF\n\n      val tuser = tDF.distinct().count()\n      printf(\"The number of user is %s \\n\",tuser)\n\n      val nuser = nDF.distinct().count()\n      printf(\"The number of nuser is %s \\n\",nuser)\n\n      val tnDF = tDF.join(nDF,tDF(\"tId\") === nDF(\"nId\"),\"inner\").select($\"tId\")\n      val numtnuser = tnDF.distinct().count()\n      printf(\"The number of tnuser is %s \\n\",numtnuser)\n    }\n  }\n\n  def main(args: Array[String]){\n\n    require(args.length >=1, \"Pls specify path\")\n    val dirPath = args(0)\n    val conf = new SparkConf\n    val sc = new SparkContext(conf)\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n    try {\n      val filePath = dirPath+\"n.txt\"\n      import sqlContext.implicits._\n      val nDF = sc.textFile(filePath).map { record =>\n        val splitRecord = record.split(\",\")\n        val nId = splitRecord(0)\n        NUser(nId)\n      }.toDF\n\n    exec1(dirPath,sc,sqlContext,nDF)\n    }\n  }\n}\n```\n\n```build.sbt\nname := \"Test\"\nversion := \"0.1\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies ++= Seq(\"org.apache.spark\" % \"spark-core_2.10\" % \"1.5.0\" % \"provided\", \"org.apache.spark\" % \"spark-sql_2.10\" % \"1.5.0\" % \"provided\")\nassemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)\n```\n\n## \u30b3\u30f3\u30d1\u30a4\u30eb\n\n```\n$ sbt assembly\n```\n\n##\u30b8\u30e7\u30d6\u5b9f\u884c\n\n```\n$ spark-submit --class Test--name Test target/scala-2.10/Test-assembly-0.1.jar /user/yotsu/input/\n```\n", "tags": ["Spark", "Scala", "CDH5.5.1"]}