{"context": "\n\n\u306f\u3058\u3081\u306b\n\u5148\u65e5\uff082017/2/16\uff09\u306bchainer\u304b\u3089\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaChainerRL\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3002\nhttps://github.com/pfnet/chainerrl\n\u3053\u308c\u307e\u3067\u9811\u5f35\u3063\u3066\u30b3\u30fc\u30c9\u66f8\u3044\u3066\u305f\u8eab\u3068\u3057\u3066\u306f\u60b2\u3057\u3044\u51fa\u6765\u4e8b\u3067\u3042\u308b\u304c\u3001\u6642\u4ee3\u306e\u6d41\u308c\u306b\u306f\u9006\u3089\u3048\u306a\u3044\u3002\u3080\u3057\u308d\u65e9\u3005\u3068\u4f7f\u3044\u3053\u306a\u3059\u3053\u3068\u3067\u6642\u4ee3\u306b\u55b0\u3089\u3044\u7740\u3044\u3066\u3044\u304d\u305f\u3044\u3002\n\u4eca\u56de\u306fQickStart\u306b\u5f93\u3063\u3066sample\u3092\u52d5\u304b\u3059\u3002\n\n\u74b0\u5883\nOS:Ubuntu14.04\nGPU:GTX1070\nCUDA:8.0 RC\ncuDNN:5.1\npython\uff1a2.7.6\nchainer\uff1a1.20.0.1\n\u306a\u3069\ngym\u7b49\u3001\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u4e0a\u8a18GitHub\u306eREAME.md\u306b\u5f93\u3063\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\nsudo pip install chainerrl\n\n\u4e00\u77ac\u3067\u5b8c\u4e86\u3002\n\nQuick Start\u3092\u8a66\u3059\n\u6b21\u306b\u3053\u3053\nhttps://github.com/pfnet/chainerrl/blob/master/examples/quickstart/quickstart.ipynb\n\u306b\u5f93\u3063\u3066\u3001quickstart\u3092\u8a66\u3059\u3002\u4ee5\u4e0b\u3001train.py\u306b\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u3044\u304f\u3002\u307e\u305a\u306fimport\u3059\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u3002\n\ntrain.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport gym\nimport numpy as np\n\n\n\u4eca\u56de\u306f'CartPole-v0'\u3068\u3044\u3046gym\u3067\u3088\u304f\u30c6\u30b9\u30c8\u306b\u4f7f\u308f\u308c\u308b\u30b2\u30fc\u30e0\u3092\u8a66\u3059\u3002\n\ntrain.py\nenv = gym.make('CartPole-v0')\nprint('observation space:', env.observation_space)\nprint('action space:', env.action_space)\n\nobs = env.reset()\nenv.render()\nprint('initial observation:', obs)\n\naction = env.action_space.sample()\nobs, r, done, info = env.step(action)\nprint('next observation:', obs)\nprint('reward:', r)\nprint('done:', done)\nprint('info:', info)\n\n\n\u3053\u308c\u3092\u5b9f\u884c\u3059\u308b\u3068\npython train.py\n[2017-03-03 03:33:00,118] Making new env: CartPole-v0\n('observation space:', Box(4,))\n('action space:', Discrete(2))\n('initial observation:', array([ 0.0428945 , -0.00220352, -0.04834014, -0.04557467]))\n('next observation:', array([ 0.04285043, -0.19660017, -0.04925163,  0.23147321]))\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n\n\u3053\u3093\u306a\u611f\u3058\u3067\u8868\u793a\u3055\u308c\u305f\u3002observation\u306f\uff14\u3064\u306e\u8981\u7d20\u306e\u914d\u5217\u3002action\u306espace\u304cDiscrete(2)\u3068\u306a\u3063\u3066\u308b\u306e\u3067\u3001\u884c\u52d5\u306f\uff12\u7a2e\u985e\u3063\u307d\u3044\u3002\u5831\u916c\u306f\u30b9\u30ab\u30e9\u30fc\u3002\nenv.reset\u3067\u74b0\u5883\u3092\u521d\u671f\u5316\u3057\u3066\u521d\u671f\u72b6\u614b\u306eobservation\u3092\u5f97\u308b\u3002env.step\u306f\u884c\u52d5\u3092\u4e0e\u3048\u3066\u6b21\u306e\u72b6\u614b\u306eobservation\u3001\u5831\u916c\u3001\u306a\u3069\u306a\u3069\u3092\u8fd4\u3059\u3002\u5f37\u5316\u5b66\u7fd2\u306e1Step\u306d\u3002\n\nagent\u306a\u3069\u3092\u8a2d\u5b9a\u3059\u308b\n\u6b21\u306b\u5b66\u7fd2\u3055\u305b\u308b\u3002\u307e\u305a\u901a\u5e38\u306echainer\u3068\u540c\u3058\u304f\u30af\u30e9\u30b9\u3067\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u5b9a\u7fa9\u3059\u308b\u3002\n\ntrain.py\nclass QFunction(chainer.Chain):\n\n    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n        super(QFunctin, self).__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_actions))\n\n    def __call__(self, x, test=False):\n        \"\"\"\n        Args:\n            x (ndarray or chainer.Variable): An observation\n            test (bool): a flag indicating whether it is in test mode\n        \"\"\"\n        h = F.tanh(self.l0(x))\n        h = F.tanh(self.l1(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n\nobs_size = env.observation_space.shape[0]\nn_actions = env.action_space.n\nq_func = QFunction(obs_size, n_actions)\nq_func.to_qpu()\n\n\n\u89aa\u30af\u30e9\u30b9\u306e\u521d\u671f\u5316\u306e\u90e8\u5206\u3060\u3051python2\u7cfb\u3088\u3046\u306b\u66f8\u304d\u63db\u3048\u305f\u3002\n\u30af\u30e9\u30b9\u5185\u3067\u306fcall\u95a2\u6570\u3067chainerrl.action_value.DiscreteActionValue(self.l2(h))\u3092\u8fd4\u3059\u3068\u3053\u308d\u3060\u3051\u304c\u901a\u5e38\u306eDL\u3068\u9055\u3046\u3002observation\u306e\u30b5\u30a4\u30ba\u3084\u884c\u52d5\u306e\u6570\u3092\u53d6\u5f97\u3057\u305f\u5f8c\u3001\u30af\u30e9\u30b9\u3092\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5316\u3002\u6700\u5f8c\u306bGPU\u306b\u9001\u308b\u3002\n\u6b21\u306boptimizer\u306e\u8a2d\u5b9a\u3002\n\ntrain.py\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n\n\n\u306a\u305c\u304bAdam\u3002DQN\u8ad6\u6587\u3067\u306fRMSProp\u3060\u3063\u305f\u3088\u3046\u306a\u30fb\u30fb\u30fb\u3002\u6b21\u306b\u4ed6\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3002\n\ntrain.py\n# Set the discount factor that discounts future rewards.\ngamma = 0.95\n\n# Use epsilon-greedy for exploration\nexplorer = chainerrl.explorers.ConstantEpsilonGreedy(\n    epsilon=0.3, random_action_func=env.action_space.sample)\n\n# DQN uses Experience Replay.\n# Specify a replay buffer and its capacity.\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n\n# Since observations from CartPole-v0 is numpy.float64 while\n# Chainer only accepts numpy.float32 by default, specify\n# a converter as a feature extractor function phi.\nphi = lambda x: x.astype(np.float32, copy=False)\n\n# Now create an agent that will interact with the environment.\nagent = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100, phi=phi)\n\n\ngamma\u306f\u5831\u916c\u306e\u5272\u5f15\u7387\u3002\u30a4\u30d7\u30b7\u30ed\u30f3\u306f\u4eca\u56de0.3\u306b\u56fa\u5b9a\u3002replay memory\u306f10\u306e6\u4e57\u306b\u8a2d\u5b9a\u3002phi\u306f\u30b2\u30fc\u30e0\u304b\u3089\u6765\u308bobservation\u304cfloat32\u3067\u306a\u3044\u5834\u5408\u306b\u5099\u3048\u305f\u95a2\u6570\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u4eca\u56deDoubleDQN\u3002\u3061\u306a\u307f\u306bchainerrl/agents/\u3092\u898b\u308b\u3068\u3001doubleDQN\u306e\u4ed6\u306bA3C\u3084ACER\u306a\u3069\u4e3b\u8981\u306a\u30e2\u30c7\u30eb\u306e\u5e7e\u3064\u304b\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3002\nreplay_start_size\u306freplay memory\u306bobservation\u306e\u7d44\u304c\uff15\uff10\uff10\u500b\u6e9c\u307e\u3063\u3066\u304b\u3089\u5b66\u7fd2\u3092\u59cb\u3081\u308b\u3068\u3044\u3046\u610f\u5473\u304b\uff1ftarget_update_frequency\u306ftarget\u306e\u65b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u30b3\u30d4\u30fc\u3059\u308b\u983b\u5ea6\u3060\u308d\u3046\u3002\n\u3053\u308c\u3067agent\u3068\u74b0\u5883\u306e\u8a2d\u5b9a\u306f\u5b8c\u4e86\u3002\n\n\u5b66\u7fd2\u3055\u305b\u308b\n\u6b21\u306b\u30eb\u30fc\u30d7\u3092\u7d44\u3093\u3067\u5b66\u7fd2\u3055\u305b\u308b\u3002\n\ntrain.py\nn_episodes = 200\nmax_episode_len = 200\nfor i in range(1, n_episodes + 1):\n    obs = env.reset()\n    reward = 0\n    done = False\n    R = 0  # return (sum of rewards)\n    t = 0  # time step\n    while not done and t < max_episode_len:\n        # Uncomment to watch the behaviour\n        # env.render()\n        action = agent.act_and_train(obs, reward)\n        obs, reward, done, _ = env.step(action)\n        R += reward\n        t += 1\n    if i % 10 == 0:\n        print('episode:', i,\n              'R:', R,\n              'statistics:', agent.get_statistics())\n    agent.stop_episode_and_train(obs, reward, done)\nprint('Finished.')\n\n\n200\u30b9\u30c6\u30c3\u30d7\u306e\u30b2\u30fc\u30e0\u3092200\u30a8\u30d4\u30bd\u30fc\u30c9\u884c\u3046\u3002\npython train.py\n[2017-03-03 04:26:10,092] Making new env: CartPole-v0\n('observation space:', Box(4,))\n('action space:', Discrete(2))\n('initial observation:', array([ 0.00686075, -0.00872222,  0.0262407 , -0.00072111]))\n('next observation:', array([ 0.0066863 , -0.20421048,  0.02622628,  0.30012421]))\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n('episode:', 10, 'R:', 101.0, 'statistics:', [(u'average_q', 3.839410603590915), (u'average_loss', 0.03739733160654156)])\n('episode:', 20, 'R:', 126.0, 'statistics:', [(u'average_q', 8.168648753284264), (u'average_loss', 0.07854945771670627)])\n('episode:', 30, 'R:', 143.0, 'statistics:', [(u'average_q', 12.791942632038957), (u'average_loss', 0.08622470318985226)])\n......\n......\n......\n('episode:', 190, 'R:', 200.0, 'statistics:', [(u'average_q', 19.98455506145719), (u'average_loss', 0.0627547477493958)])\n('episode:', 200, 'R:', 105.0, 'statistics:', [(u'average_q', 20.00981249193506), (u'average_loss', 0.04760965162900658)])\nFinished.\n\n\u306a\u3093\u304b\u53ce\u76ca\u5897\u3048\u3066\u306a\u3044\u6c17\u304c\u3059\u308b\u3051\u3069\u30fb\u30fb\u30fb\u3002\u30b3\u30e1\u30f3\u30c8\u6b04\u306e\u958b\u767a\u8005\u69d8\u306e\u5fa1\u6307\u6458\u901a\u308a\u3001\u53ce\u76ca\u306emax\u306f200\u306a\u306e\u3067\u3001190episode\u306a\u3069\u306fmax\u306b\u3044\u3063\u3066\u3044\u308b\u3002\n\n\u30c6\u30b9\u30c8\u3059\u308b\n\u6b21\u306b\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u30e2\u30c7\u30eb\u3092\u30c6\u30b9\u30c8\u3059\u308b\u3002\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u90e8\u5206\u306e\u30b3\u30fc\u30c9\u306b\u7d9a\u3044\u3066\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u8db3\u3059\u3002\n\ntrain.py\nfor i in range(10):\n    obs = env.reset()\n    done = False\n    R = 0\n    t = 0\n    while not done and t < 200:\n        env.render()\n        action = agent.act(obs)\n        obs, r, done, _ = env.step(action)\n        R += r\n        t += 1\n    print('test episode:', i, 'R:', R)\n    agent.stop_episode()\n\n\n\u3053\u308c\u3092\u8d70\u3089\u305b\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\npython train.py\n.....\n.....\n.....\n.....\n('episode:', 190, 'R:', 11.0, 'statistics:', [(u'average_q', 20.311043770645103), (u'average_loss', 0.09018823270227477)])\n('episode:', 200, 'R:', 196.0, 'statistics:', [(u'average_q', 20.352799336256385), (u'average_loss', 0.05998000704222223)])\ntraining Finished.\n('test episode:', 0, 'R:', 200.0)\n('test episode:', 1, 'R:', 200.0)\n('test episode:', 2, 'R:', 200.0)\n('test episode:', 3, 'R:', 200.0)\n('test episode:', 4, 'R:', 200.0)\n('test episode:', 5, 'R:', 200.0)\n('test episode:', 6, 'R:', 200.0)\n('test episode:', 7, 'R:', 200.0)\n('test episode:', 8, 'R:', 200.0)\n('test episode:', 9, 'R:', 200.0)\ntest Finished.\n\n\u3053\u3068\u3054\u3068\u304fmax\u53ce\u76ca\u3092\u305f\u305f\u304d\u51fa\u3057\u3066\u3044\u308b\u3002\u3061\u306a\u307f\u306b\u3001training\u3055\u305b\u305a\u306btest\u3060\u3051\u3055\u305b\u3066\u307f\u305f\u7d50\u679c\u304c\u4ee5\u4e0b\u3002\npython test_1.py\n.....\n.....\n.....\n.....\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n('test episode:', 0, 'R:', 14.0)\n('test episode:', 1, 'R:', 15.0)\n('test episode:', 2, 'R:', 16.0)\n('test episode:', 3, 'R:', 13.0)\n('test episode:', 4, 'R:', 15.0)\n('test episode:', 5, 'R:', 16.0)\n('test episode:', 6, 'R:', 16.0)\n('test episode:', 7, 'R:', 15.0)\n('test episode:', 8, 'R:', 15.0)\n('test episode:', 9, 'R:', 17.0)\ntest Finished.\n\n\u5b66\u7fd2\u6e08\u307f\u3068\u5168\u7136\u9055\u3046\u306d\u3002\u3061\u306a\u307f\u306b\u3001test.py\u3068\u3044\u3046\u540d\u524d\u306e\u30d5\u30a1\u30a4\u30eb\u3060\u3068\u4f55\u6545\u304b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u306e\u3067\u3001\u4f7f\u308f\u306a\u3044\u65b9\u304c\u3044\u3044\u3002\n\n\u30bb\u30fc\u30d6\u3068\u30ed\u30fc\u30c9\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30bb\u30fc\u30d6\u304c\u3067\u304d\u308b\u3002\n# Save an agent to the 'agent' directory\nagent.save('agent')\n\n\u30ed\u30fc\u30c9\u306f\u3053\u308c\u3002\n# Uncomment to load an agent from the 'agent' directory\nagent.load('agent')\n\n\u3057\u304b\u3057\u3001\u3053\u308c\u306fagent\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3060\u3051\u3067\u306f\uff1f\u4f8b\u3048\u3070\u3001\u30d1\u30bd\u30b3\u30f3\u304c\u56fa\u307e\u3063\u305f\u6642\u306e\u305f\u3081\u306b\u5b9a\u671f\u7684\u306b\u5b66\u7fd2\u3092\u30bb\u30fc\u30d6\u3059\u308b\u5834\u5408\u3001replay memory\u3082\u30bb\u30fc\u30d6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3068\u601d\u3046\u304c\u3001\u305d\u3061\u3089\u306f\u30bb\u30fc\u30d6\u3055\u308c\u306a\u3044\u306e\u306d\uff1f\n##\u306f\u3058\u3081\u306b\n\u5148\u65e5\uff082017/2/16\uff09\u306bchainer\u304b\u3089\u6df1\u5c64\u5f37\u5316\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eaChainerRL\u304c\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3002\nhttps://github.com/pfnet/chainerrl\n\u3053\u308c\u307e\u3067\u9811\u5f35\u3063\u3066\u30b3\u30fc\u30c9\u66f8\u3044\u3066\u305f\u8eab\u3068\u3057\u3066\u306f\u60b2\u3057\u3044\u51fa\u6765\u4e8b\u3067\u3042\u308b\u304c\u3001\u6642\u4ee3\u306e\u6d41\u308c\u306b\u306f\u9006\u3089\u3048\u306a\u3044\u3002\u3080\u3057\u308d\u65e9\u3005\u3068\u4f7f\u3044\u3053\u306a\u3059\u3053\u3068\u3067\u6642\u4ee3\u306b\u55b0\u3089\u3044\u7740\u3044\u3066\u3044\u304d\u305f\u3044\u3002\n\n\u4eca\u56de\u306fQickStart\u306b\u5f93\u3063\u3066sample\u3092\u52d5\u304b\u3059\u3002\n##\u74b0\u5883\nOS:Ubuntu14.04\nGPU:GTX1070\nCUDA:8.0 RC\ncuDNN:5.1\npython\uff1a2.7.6\nchainer\uff1a1.20.0.1\n\u306a\u3069\n\ngym\u7b49\u3001\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\n##\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\u4e0a\u8a18GitHub\u306eREAME.md\u306b\u5f93\u3063\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002\n\n```\nsudo pip install chainerrl\n```\n\n\u4e00\u77ac\u3067\u5b8c\u4e86\u3002\n##Quick Start\u3092\u8a66\u3059\n\u6b21\u306b\u3053\u3053\nhttps://github.com/pfnet/chainerrl/blob/master/examples/quickstart/quickstart.ipynb\n\u306b\u5f93\u3063\u3066\u3001quickstart\u3092\u8a66\u3059\u3002\u4ee5\u4e0b\u3001`train.py`\u306b\u5fc5\u8981\u306a\u30b3\u30fc\u30c9\u3092\u5165\u529b\u3057\u3066\u3044\u304f\u3002\u307e\u305a\u306f`import`\u3059\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u3002\n\n```train.py\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport chainerrl\nimport gym\nimport numpy as np\n```\n\n\u4eca\u56de\u306f'CartPole-v0'\u3068\u3044\u3046gym\u3067\u3088\u304f\u30c6\u30b9\u30c8\u306b\u4f7f\u308f\u308c\u308b\u30b2\u30fc\u30e0\u3092\u8a66\u3059\u3002\n\n```train.py\nenv = gym.make('CartPole-v0')\nprint('observation space:', env.observation_space)\nprint('action space:', env.action_space)\n\nobs = env.reset()\nenv.render()\nprint('initial observation:', obs)\n\naction = env.action_space.sample()\nobs, r, done, info = env.step(action)\nprint('next observation:', obs)\nprint('reward:', r)\nprint('done:', done)\nprint('info:', info)\n```\n\n\u3053\u308c\u3092\u5b9f\u884c\u3059\u308b\u3068\n\n```\npython train.py\n[2017-03-03 03:33:00,118] Making new env: CartPole-v0\n('observation space:', Box(4,))\n('action space:', Discrete(2))\n('initial observation:', array([ 0.0428945 , -0.00220352, -0.04834014, -0.04557467]))\n('next observation:', array([ 0.04285043, -0.19660017, -0.04925163,  0.23147321]))\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n```\n\n\u3053\u3093\u306a\u611f\u3058\u3067\u8868\u793a\u3055\u308c\u305f\u3002observation\u306f\uff14\u3064\u306e\u8981\u7d20\u306e\u914d\u5217\u3002action\u306espace\u304cDiscrete(2)\u3068\u306a\u3063\u3066\u308b\u306e\u3067\u3001\u884c\u52d5\u306f\uff12\u7a2e\u985e\u3063\u307d\u3044\u3002\u5831\u916c\u306f\u30b9\u30ab\u30e9\u30fc\u3002\n\n`env.reset`\u3067\u74b0\u5883\u3092\u521d\u671f\u5316\u3057\u3066\u521d\u671f\u72b6\u614b\u306eobservation\u3092\u5f97\u308b\u3002`env.step`\u306f\u884c\u52d5\u3092\u4e0e\u3048\u3066\u6b21\u306e\u72b6\u614b\u306eobservation\u3001\u5831\u916c\u3001\u306a\u3069\u306a\u3069\u3092\u8fd4\u3059\u3002\u5f37\u5316\u5b66\u7fd2\u306e1Step\u306d\u3002\n##agent\u306a\u3069\u3092\u8a2d\u5b9a\u3059\u308b\n\u6b21\u306b\u5b66\u7fd2\u3055\u305b\u308b\u3002\u307e\u305a\u901a\u5e38\u306echainer\u3068\u540c\u3058\u304f\u30af\u30e9\u30b9\u3067\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u5b9a\u7fa9\u3059\u308b\u3002\n\n```train.py\nclass QFunction(chainer.Chain):\n\n    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n        super(QFunctin, self).__init__(\n            l0=L.Linear(obs_size, n_hidden_channels),\n            l1=L.Linear(n_hidden_channels, n_hidden_channels),\n            l2=L.Linear(n_hidden_channels, n_actions))\n\n    def __call__(self, x, test=False):\n        \"\"\"\n        Args:\n            x (ndarray or chainer.Variable): An observation\n            test (bool): a flag indicating whether it is in test mode\n        \"\"\"\n        h = F.tanh(self.l0(x))\n        h = F.tanh(self.l1(h))\n        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n\nobs_size = env.observation_space.shape[0]\nn_actions = env.action_space.n\nq_func = QFunction(obs_size, n_actions)\nq_func.to_qpu()\n```\n\n\u89aa\u30af\u30e9\u30b9\u306e\u521d\u671f\u5316\u306e\u90e8\u5206\u3060\u3051python2\u7cfb\u3088\u3046\u306b\u66f8\u304d\u63db\u3048\u305f\u3002\n\n\u30af\u30e9\u30b9\u5185\u3067\u306f`call`\u95a2\u6570\u3067`chainerrl.action_value.DiscreteActionValue(self.l2(h))`\u3092\u8fd4\u3059\u3068\u3053\u308d\u3060\u3051\u304c\u901a\u5e38\u306eDL\u3068\u9055\u3046\u3002observation\u306e\u30b5\u30a4\u30ba\u3084\u884c\u52d5\u306e\u6570\u3092\u53d6\u5f97\u3057\u305f\u5f8c\u3001\u30af\u30e9\u30b9\u3092\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5316\u3002\u6700\u5f8c\u306bGPU\u306b\u9001\u308b\u3002\n\n\u6b21\u306boptimizer\u306e\u8a2d\u5b9a\u3002\n\n```train.py\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n```\n\n\u306a\u305c\u304bAdam\u3002DQN\u8ad6\u6587\u3067\u306fRMSProp\u3060\u3063\u305f\u3088\u3046\u306a\u30fb\u30fb\u30fb\u3002\u6b21\u306b\u4ed6\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8a2d\u5b9a\u3059\u308b\u3002\n\n```train.py\n# Set the discount factor that discounts future rewards.\ngamma = 0.95\n\n# Use epsilon-greedy for exploration\nexplorer = chainerrl.explorers.ConstantEpsilonGreedy(\n    epsilon=0.3, random_action_func=env.action_space.sample)\n\n# DQN uses Experience Replay.\n# Specify a replay buffer and its capacity.\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n\n# Since observations from CartPole-v0 is numpy.float64 while\n# Chainer only accepts numpy.float32 by default, specify\n# a converter as a feature extractor function phi.\nphi = lambda x: x.astype(np.float32, copy=False)\n\n# Now create an agent that will interact with the environment.\nagent = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100, phi=phi)\n```\n\n`gamma`\u306f\u5831\u916c\u306e\u5272\u5f15\u7387\u3002\u30a4\u30d7\u30b7\u30ed\u30f3\u306f\u4eca\u56de0.3\u306b\u56fa\u5b9a\u3002replay memory\u306f10\u306e6\u4e57\u306b\u8a2d\u5b9a\u3002phi\u306f\u30b2\u30fc\u30e0\u304b\u3089\u6765\u308bobservation\u304c`float32`\u3067\u306a\u3044\u5834\u5408\u306b\u5099\u3048\u305f\u95a2\u6570\u3002\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u4eca\u56deDoubleDQN\u3002\u3061\u306a\u307f\u306b`chainerrl/agents/`\u3092\u898b\u308b\u3068\u3001doubleDQN\u306e\u4ed6\u306bA3C\u3084ACER\u306a\u3069\u4e3b\u8981\u306a\u30e2\u30c7\u30eb\u306e\u5e7e\u3064\u304b\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u3002\n\nreplay_start_size\u306freplay memory\u306bobservation\u306e\u7d44\u304c\uff15\uff10\uff10\u500b\u6e9c\u307e\u3063\u3066\u304b\u3089\u5b66\u7fd2\u3092\u59cb\u3081\u308b\u3068\u3044\u3046\u610f\u5473\u304b\uff1ftarget_update_frequency\u306ftarget\u306e\u65b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u30b3\u30d4\u30fc\u3059\u308b\u983b\u5ea6\u3060\u308d\u3046\u3002\n\n\u3053\u308c\u3067agent\u3068\u74b0\u5883\u306e\u8a2d\u5b9a\u306f\u5b8c\u4e86\u3002\n##\u5b66\u7fd2\u3055\u305b\u308b\n\u6b21\u306b\u30eb\u30fc\u30d7\u3092\u7d44\u3093\u3067\u5b66\u7fd2\u3055\u305b\u308b\u3002\n\n```train.py\nn_episodes = 200\nmax_episode_len = 200\nfor i in range(1, n_episodes + 1):\n    obs = env.reset()\n    reward = 0\n    done = False\n    R = 0  # return (sum of rewards)\n    t = 0  # time step\n    while not done and t < max_episode_len:\n        # Uncomment to watch the behaviour\n        # env.render()\n        action = agent.act_and_train(obs, reward)\n        obs, reward, done, _ = env.step(action)\n        R += reward\n        t += 1\n    if i % 10 == 0:\n        print('episode:', i,\n              'R:', R,\n              'statistics:', agent.get_statistics())\n    agent.stop_episode_and_train(obs, reward, done)\nprint('Finished.')\n```\n\n200\u30b9\u30c6\u30c3\u30d7\u306e\u30b2\u30fc\u30e0\u3092200\u30a8\u30d4\u30bd\u30fc\u30c9\u884c\u3046\u3002\n\n```\npython train.py\n[2017-03-03 04:26:10,092] Making new env: CartPole-v0\n('observation space:', Box(4,))\n('action space:', Discrete(2))\n('initial observation:', array([ 0.00686075, -0.00872222,  0.0262407 , -0.00072111]))\n('next observation:', array([ 0.0066863 , -0.20421048,  0.02622628,  0.30012421]))\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n('episode:', 10, 'R:', 101.0, 'statistics:', [(u'average_q', 3.839410603590915), (u'average_loss', 0.03739733160654156)])\n('episode:', 20, 'R:', 126.0, 'statistics:', [(u'average_q', 8.168648753284264), (u'average_loss', 0.07854945771670627)])\n('episode:', 30, 'R:', 143.0, 'statistics:', [(u'average_q', 12.791942632038957), (u'average_loss', 0.08622470318985226)])\n......\n......\n......\n('episode:', 190, 'R:', 200.0, 'statistics:', [(u'average_q', 19.98455506145719), (u'average_loss', 0.0627547477493958)])\n('episode:', 200, 'R:', 105.0, 'statistics:', [(u'average_q', 20.00981249193506), (u'average_loss', 0.04760965162900658)])\nFinished.\n```\n\n~~\u306a\u3093\u304b\u53ce\u76ca\u5897\u3048\u3066\u306a\u3044\u6c17\u304c\u3059\u308b\u3051\u3069\u30fb\u30fb\u30fb\u3002~~\u30b3\u30e1\u30f3\u30c8\u6b04\u306e\u958b\u767a\u8005\u69d8\u306e\u5fa1\u6307\u6458\u901a\u308a\u3001\u53ce\u76ca\u306emax\u306f200\u306a\u306e\u3067\u3001190episode\u306a\u3069\u306fmax\u306b\u3044\u3063\u3066\u3044\u308b\u3002\n\n##\u30c6\u30b9\u30c8\u3059\u308b\n\u6b21\u306b\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u30e2\u30c7\u30eb\u3092\u30c6\u30b9\u30c8\u3059\u308b\u3002\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u90e8\u5206\u306e\u30b3\u30fc\u30c9\u306b\u7d9a\u3044\u3066\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u8db3\u3059\u3002\n\n```train.py\nfor i in range(10):\n    obs = env.reset()\n    done = False\n    R = 0\n    t = 0\n    while not done and t < 200:\n        env.render()\n        action = agent.act(obs)\n        obs, r, done, _ = env.step(action)\n        R += r\n        t += 1\n    print('test episode:', i, 'R:', R)\n    agent.stop_episode()\n```\n\n\u3053\u308c\u3092\u8d70\u3089\u305b\u308b\u3068\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7d50\u679c\u3068\u306a\u3063\u305f\u3002\n\n```\npython train.py\n.....\n.....\n.....\n.....\n('episode:', 190, 'R:', 11.0, 'statistics:', [(u'average_q', 20.311043770645103), (u'average_loss', 0.09018823270227477)])\n('episode:', 200, 'R:', 196.0, 'statistics:', [(u'average_q', 20.352799336256385), (u'average_loss', 0.05998000704222223)])\ntraining Finished.\n('test episode:', 0, 'R:', 200.0)\n('test episode:', 1, 'R:', 200.0)\n('test episode:', 2, 'R:', 200.0)\n('test episode:', 3, 'R:', 200.0)\n('test episode:', 4, 'R:', 200.0)\n('test episode:', 5, 'R:', 200.0)\n('test episode:', 6, 'R:', 200.0)\n('test episode:', 7, 'R:', 200.0)\n('test episode:', 8, 'R:', 200.0)\n('test episode:', 9, 'R:', 200.0)\ntest Finished.\n```\n\n\u3053\u3068\u3054\u3068\u304fmax\u53ce\u76ca\u3092\u305f\u305f\u304d\u51fa\u3057\u3066\u3044\u308b\u3002\u3061\u306a\u307f\u306b\u3001training\u3055\u305b\u305a\u306btest\u3060\u3051\u3055\u305b\u3066\u307f\u305f\u7d50\u679c\u304c\u4ee5\u4e0b\u3002\n\n```\npython test_1.py\n.....\n.....\n.....\n.....\n('reward:', 1.0)\n('done:', False)\n('info:', {})\n('test episode:', 0, 'R:', 14.0)\n('test episode:', 1, 'R:', 15.0)\n('test episode:', 2, 'R:', 16.0)\n('test episode:', 3, 'R:', 13.0)\n('test episode:', 4, 'R:', 15.0)\n('test episode:', 5, 'R:', 16.0)\n('test episode:', 6, 'R:', 16.0)\n('test episode:', 7, 'R:', 15.0)\n('test episode:', 8, 'R:', 15.0)\n('test episode:', 9, 'R:', 17.0)\ntest Finished.\n```\n\n\u5b66\u7fd2\u6e08\u307f\u3068\u5168\u7136\u9055\u3046\u306d\u3002\u3061\u306a\u307f\u306b\u3001`test.py`\u3068\u3044\u3046\u540d\u524d\u306e\u30d5\u30a1\u30a4\u30eb\u3060\u3068\u4f55\u6545\u304b\u30a8\u30e9\u30fc\u3068\u306a\u308b\u306e\u3067\u3001\u4f7f\u308f\u306a\u3044\u65b9\u304c\u3044\u3044\u3002\n##\u30bb\u30fc\u30d6\u3068\u30ed\u30fc\u30c9\n\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u30bb\u30fc\u30d6\u304c\u3067\u304d\u308b\u3002\n\n```\n# Save an agent to the 'agent' directory\nagent.save('agent')\n```\n\n\u30ed\u30fc\u30c9\u306f\u3053\u308c\u3002\n\n```\n# Uncomment to load an agent from the 'agent' directory\nagent.load('agent')\n```\n\n\u3057\u304b\u3057\u3001\u3053\u308c\u306fagent\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3060\u3051\u3067\u306f\uff1f\u4f8b\u3048\u3070\u3001\u30d1\u30bd\u30b3\u30f3\u304c\u56fa\u307e\u3063\u305f\u6642\u306e\u305f\u3081\u306b\u5b9a\u671f\u7684\u306b\u5b66\u7fd2\u3092\u30bb\u30fc\u30d6\u3059\u308b\u5834\u5408\u3001replay memory\u3082\u30bb\u30fc\u30d6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3068\u601d\u3046\u304c\u3001\u305d\u3061\u3089\u306f\u30bb\u30fc\u30d6\u3055\u308c\u306a\u3044\u306e\u306d\uff1f\n", "tags": ["Chainer", "python2.7", "ReinforcementLearning", "Ubuntu14.04"]}