{"context": " More than 1 year has passed since last update.Cassandra \u3092\u30ad\u30e5\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u306f\u30a2\u30f3\u30c1\u30d1\u30bf\u30fc\u30f3\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002\u5b9f\u969b\u306b\u3069\u3093\u306a\u554f\u984c\u304c\u3042\u308b\u306e\u304b\u3001Datastax \u306e\u30d6\u30ed\u30b0\u306b\u826f\u3055\u3052\u306a\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u8aad\u3093\u3067\u307f\u305f\u3002\u3053\u306e\u8a18\u4e8b\u306f\u3059\u3053\u3057\u53e4\u3044 (2013/04/26) \u3051\u3069 Cassandra-1.2.x \u3092\u5bfe\u8c61\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u73fe\u6642\u70b9\u3067\u306e Cassandra-2.0.x \u3067\u3082\u5927\u4f53\u540c\u3058\u3053\u3068\u304c\u8a00\u3048\u308b\u3068\u601d\u3046\u3002\u9069\u5f53\u306a\u610f\u8a33\u306a\u306e\u3067\u9593\u9055\u3063\u3066\u305f\u3089\u3054\u3081\u3093\u306a\u3055\u3044\u3002\n\nAuthor\nBy Aleksey Yeschenko - April 26, 2013 \nhttp://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets\n\nDeletes in Cassandra\nCassandra uses a log-structured storage engine. Because of this, deletes do not remove the rows and columns immediately and in-place. Instead, Cassandra writes a special marker, called a tombstone, indicating that a row, column, or range of columns was deleted. These tombstones are kept for at least the period of time defined by the gc_grace_seconds per-table setting. Only then a tombstone can be permanently discarded by compaction.\nCassandra \u306f\u30ed\u30b0\u69cb\u9020\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u305f\u3081\u3001deletes \u306f rows \u3084 columns \u3092\u5373\u5ea7\u306b\u524a\u9664\u3057\u307e\u305b\u3093\u3002\u4ee3\u308f\u308a\u306b\u3001\u305d\u306e row \u3084 column \u3042\u308b\u3044\u306f columns \u306e\u7bc4\u56f2\u304c\u524a\u9664\u3055\u308c\u305f\u3053\u3068\u3092\u793a\u3059 tombstone \u3068\u547c\u3070\u308c\u308b\u7279\u5225\u306a\u30de\u30fc\u30ab\u3092\u66f8\u304d\u8fbc\u307f\u307e\u3059\u3002\u3053\u308c\u3089\u306e tombstone \u306f\u3001\u5c11\u306a\u304f\u3066\u3082\u30c6\u30fc\u30d6\u30eb\u3054\u3068\u306e gc_grace_seconds \u8a2d\u5b9a\u5024\u306b\u3088\u3063\u3066\u5b9a\u7fa9\u3055\u308c\u305f\u671f\u9593\u306f\u7dad\u6301\u3055\u308c\u307e\u3059\u3002\u305d\u306e\u5f8c tombstone \u306f compaction \u306b\u3088\u3063\u3066\u5ec3\u68c4\u3055\u308c\u307e\u3059\u3002\nThis scheme allows for very fast deletes (and writes in general), but it\u2019s not free: aside from the obvious RAM/disk overhead of tombstones, you might have to pay a certain price when reading data back if you haven\u2019t modelled your data well.\n\u3053\u306e\u30b9\u30ad\u30fc\u30de\u306f\u975e\u5e38\u306b\u9ad8\u901f\u306a deletes (\u304a\u3088\u3073\u4e00\u822c\u7684\u306a\u66f8\u304d\u8fbc\u307f) \u3092\u8a31\u53ef\u3057\u307e\u3059\u304c\u3001\u81ea\u7531\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002tombstones \u306b\u304a\u3051\u308b RAM/disk \u306e\u30aa\u30fc\u30d0\u30d8\u30c3\u30c9\u306f\u5225\u3068\u3057\u3066\u3082\u3001\u30c7\u30fc\u30bf\u3092\u826f\u304f\u30e2\u30c7\u30eb\u5316\u3057\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fd4\u3059\u5834\u5408\u306b\u3001\u4e00\u5b9a\u306e\u30b3\u30b9\u30c8\u3092\u6255\u3046\u5fc5\u8981\u304c\u751f\u3058\u308b\u3067\u3057\u3087\u3046\u3002\nSpecifically, tombstones will bite you if you do lots of deletes (especially column-level deletes) and later perform slice queries on rows with a lot of tombstones.\n\u5177\u4f53\u7684\u306b\u306f\u3001\u3082\u3057\u591a\u304f\u306e deletes (\u7279\u306b column \u30ec\u30d9\u30eb\u306e deletes) \u3092\u884c\u3046\u3068 tombstones \u306f\u554f\u984c\u304c\u751f\u3058\u308b\u3067\u3057\u3087\u3046\u3002\u305d\u3057\u3066\u3001\u591a\u304f\u306e tombstones \u3092\u4fdd\u6709\u3057\u305f rows \u306b\u5bfe\u3059\u308b\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u51e6\u7406\u304c\u9045\u304f\u306a\u308b\u3067\u3057\u3087\u3046\u3002\n\nSymptoms of a wrong data model\nTo illustrate this scenario, let\u2019s consider the most extreme case \u2013 using Cassandra as a durable queue, a known anti-pattern\n\u3053\u306e\u30b7\u30ca\u30ea\u30aa\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u3001\u6700\u3082\u6975\u7aef\u306a\u30b1\u30fc\u30b9\u3067\u691c\u8a0e\u3057\u307e\u3057\u3087\u3046\u3002Cassandra \u3092\u6c38\u7d9a\u30ad\u30e5\u30fc (\u30a2\u30f3\u30c1\u30d1\u30bf\u30fc\u30f3\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u3044\u308b) \u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002\nCREATE TABLE queues (\n    name text,\n    enqueued_at timeuuid,\n    payload blob,\n    PRIMARY KEY (name, enqueued_at)\n);\n\nHaving enqueued 10000 10-byte messages and then dequeued 9999 of them, one by one, let\u2019s peek at the last remaining message using cqlsh with TRACING ON:\n10-byte \u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092 10,000 \u500b\u30ad\u30e5\u30fc\u306b\u683c\u7d0d\u3057\u3001\u305d\u306e\u3046\u3061 9,999 \u500b\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u3072\u3068\u3064\u305a\u3064\u30ad\u30e5\u30fc\u304b\u3089\u53d6\u308a\u9664\u304d\u307e\u3059\u3002\u6b21\u306b\u3001cqlsh \u3067 TRACING \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6709\u52b9\u306b\u3057\u3066\u6700\u5f8c\u306b\u6b8b\u3063\u305f\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u308a\u51fa\u3057\u307e\u3059\u3002\nSELECT enqueued_at, payload\n  FROM queues\n WHERE name = 'queue-1'\n LIMIT 1;\n\nactivity                                   | source    | elapsed\n-------------------------------------------+-----------+--------\n                        execute_cql3_query | 127.0.0.3 |       0\n                         Parsing statement | 127.0.0.3 |      48\n                        Peparing statement | 127.0.0.3 |     362\n          Message received from /127.0.0.3 | 127.0.0.1 |      42\n             Sending message to /127.0.0.1 | 127.0.0.3 |     718\nExecuting single-partition query on queues | 127.0.0.1 |     145\n              Acquiring sstable references | 127.0.0.1 |     158\n                 Merging memtable contents | 127.0.0.1 |     189\nMerging data from memtables and 0 sstables | 127.0.0.1 |     235\n    Read 1 live and 19998 tombstoned cells | 127.0.0.1 |  251102\n          Enqueuing response to /127.0.0.3 | 127.0.0.1 |  252976\n             Sending message to /127.0.0.3 | 127.0.0.1 |  253052\n          Message received from /127.0.0.1 | 127.0.0.3 |  324314\n       Processing response from /127.0.0.1 | 127.0.0.3 |  324535\n                          Request complete | 127.0.0.3 |  324812\n\n\nNow even though the whole row was still in memory, the request took more than 300 milliseconds (all the numbers are from a 3-node ccm cluster running on a 2012 MacBook Air).\n\u3059\u3079\u3066\u306e row \u306f\u30e1\u30e2\u30ea\u30fc\u306b\u53ce\u307e\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u30ea\u30af\u30a8\u30b9\u30c8\u306f 300 \u30df\u30ea\u79d2\u4ee5\u4e0a\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u691c\u8a3c\u3057\u305f\u74b0\u5883\u306f ccm \u3092\u4f7f\u7528\u3057\u3066 2012 Macbook Air \u306b 3 \u30ce\u30fc\u30c9\u306e Cassandra \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\n\nWhy did the query take so long to complete?\nA slice query will keep reading columns until one of the following condition is met (assuming regular, non-reverse order):\n\nthe specified limit of live columns has been read\na column beyond the finish column has been read (if specified)\nall columns in the row have been read\n\n\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306f\u3001\u4e0b\u8a18\u306e\u6761\u4ef6\u306e\u3046\u3061\u3072\u3068\u3064\u306b\u8a72\u5f53\u3059\u308b\u307e\u3067 columns \u3092\u8aad\u307f\u7d9a\u3051\u308b\u3067\u3057\u3087\u3046\u3002\n\n\u6307\u5b9a\u3055\u308c\u305f limit \u4ef6\u306e\u6709\u52b9\u306a columns \u304c\u8aad\u307e\u308c\u305f\n\u30ec\u30f3\u30b8\u306e\u7d42\u4e86 column \u307e\u3067\u8aad\u307e\u308c\u305f (\u6307\u5b9a\u3057\u3066\u3042\u308b\u5834\u5408)\nrow \u306e\u4e2d\u306e\u3059\u3079\u3066\u306e columns \u304c\u8aad\u307e\u308c\u305f\n\nIn the previous scenario Cassandra had to read 9999 tombstones (and create 9999 DeletedColumn objects) before it could get to the only live entry. And all the collected tombstones 1) were consuming heap and 2) had to be serialised and sent back to the coordinator node along with the single live column.\n\u524d\u306e\u30b7\u30ca\u30ea\u30aa\u3067\u306f\u3001\u552f\u4e00\u306e\u6709\u52b9\u306a\u30a8\u30f3\u30c8\u30ea\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306b 9,999 \u500b\u306e tombstones (\u305d\u3057\u3066 9,999 \u500b\u306e DeletedClumn \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b) \u3092\u8aad\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3057\u305f\u3002\u307e\u305f\u96c6\u3081\u305f tombstones \u306f\u30d2\u30fc\u30d7\u3092\u6d88\u8cbb\u3057\u3001\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u3055\u308c\u3001\u3072\u3068\u3064\u306e\u6709\u52b9\u306a column \u3068\u5171\u306b coordinator \u30ce\u30fc\u30c9\u306b\u9001\u308a\u8fd4\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3057\u305f\u3002\nFor comparison, it took less than 1 millisecond for the same query to complete when no column-level tombstones were involved.\n\u6bd4\u8f03\u306e\u305f\u3081\u306b\u3001tombstones \u304c\u542b\u307e\u308c\u3066\u3044\u306a\u3044 column \u306b\u540c\u3058\u30af\u30a8\u30ea\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068 1 \u30df\u30ea\u79d2\u672a\u6e80\u3067\u7d42\u4e86\u3057\u307e\u3057\u305f\u3002\nThe queue example might be extreme, but you\u2019ll see the same behaviour when performing slice queries on any row with lots of deleted columns. Also, expiring columns, while more subtle, are going to have the same effect on slice queries once they expire and become tombstones.\n\u3053\u306e\u30ad\u30e5\u30fc\u306e\u4f8b\u306f\u6975\u7aef\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001delete \u3055\u308c\u305f columns \u3092\u591a\u304f\u542b\u3093\u3060\u4efb\u610f\u306e row \u306b\u304a\u3044\u3066\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u3092\u884c\u3046\u3068\u540c\u3058\u3088\u3046\u306a\u6319\u52d5\u3092\u898b\u308b\u3067\u3057\u3087\u3046\u3002\u307e\u305f TTL \u3092\u4f7f\u7528\u3059\u308b expiring columns \u306b\u304a\u3044\u3066\u3082\u3001\u671f\u9650\u304c\u5207\u308c\u306b\u3088\u308a tombstones \u306b\u306a\u308b\u305f\u3081\u540c\u3058\u6319\u52d5\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002\n\nPotential workarounds\nIf you are seeing this pattern (have to read past many deleted columns before getting to the live ones), chances are that you got your data model wrong and must fix it.\nFor example, consider partitioning data with heavy churn rate into separate rows and deleting the entire rows when you no longer need them. Alternatively, partition it into separate tables and truncate them when they aren\u2019t needed anymore.\nIn other words, if you use column-level deletes (or expiring columns) heavily and also need to perform slice queries over that data, try grouping columns with close \u2018expiration date\u2019 together and getting rid of them in a single move.\n\nWhen you know where your live columns begin\nNote that it\u2019s possible to improve on this hypothetical queue scenario. Specifically, when knowing what the last entry was, a consumer can specify the start column and thus somewhat mitigate the effect of tombstones by not having to either 1) start scanning at the beginning of the row and 2) collect and keep all the irrelevant tombstones in memory.\n\u3053\u306e\u4eee\u5b9a\u3057\u305f\u30ad\u30e5\u30fc\u30b7\u30ca\u30ea\u30aa\u4e0a\u3067\u6539\u5584\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7279\u306b\u3001\u6700\u5f8c\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u304c\u4f55\u3067\u3042\u3063\u305f\u304b\u3092\u77e5\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001consumer \u306f\u6700\u521d\u306e\u30ab\u30e9\u30e0\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001tombstones \u306e\u5f71\u97ff\u3092\u591a\u5c11\u8efd\u6e1b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\nTo show what I mean, let\u2019s modify the original example by using the previously consumed entry\u2019s key as the start column for the query, i.e.\n\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u4f8b\u3067\u884c\u3063\u305f\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306b\u30b9\u30bf\u30fc\u30c8 column \u3092\u6307\u5b9a\u3057\u3066\u5b9f\u884c\u3057\u307e\u3059\u3002\nSELECT enqueued_at, payload\n  FROM queues\n WHERE name = 'queue-1'\n   AND enqueued_at > 9d1cb818-9d7a-11b6-96ba-60c5470cbf0e\n LIMIT 1;\n\nactivity                                   | source    | elapsed\n-------------------------------------------+-----------+--------\n                        execute_cql3_query | 127.0.0.3 |       0\n                         Parsing statement | 127.0.0.3 |      45\n                        Peparing statement | 127.0.0.3 |     329\n             Sending message to /127.0.0.1 | 127.0.0.3 |     965\n          Message received from /127.0.0.3 | 127.0.0.1 |      34\nExecuting single-partition query on queues | 127.0.0.1 |     339\n              Acquiring sstable references | 127.0.0.1 |     355\n                 Merging memtable contents | 127.0.0.1 |     461\n Partition index lookup over for sstable 3 | 127.0.0.1 |    1122\nMerging data from memtables and 1 sstables | 127.0.0.1 |    2268\n        Read 1 live and 0 tombstoned cells | 127.0.0.1 |    4404\n          Message received from /127.0.0.1 | 127.0.0.3 |    6109\n          Enqueuing response to /127.0.0.3 | 127.0.0.1 |    4492\n             Sending message to /127.0.0.3 | 127.0.0.1 |    4606\n       Processing response from /127.0.0.1 | 127.0.0.3 |    6608\n                          Request complete | 127.0.0.3 |    6901\n\nDespite reading from disk this time, the complete request took 7 milliseconds. Specifying a start column allowed to start scanning the row close to the actual live column and to skip collecting all the tombstones. The difference grows larger with size of the row increasing.\n\u4eca\u56de\u306f\u30c7\u30a3\u30b9\u30af\u304b\u3089\u8aad\u3080\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u3001\u30ea\u30af\u30a8\u30b9\u30c8\u306f 7 \u30df\u30ea\u79d2\u3067\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\u30b9\u30bf\u30fc\u30c8 column \u306e\u6307\u5b9a\u306f\u3001\u5b9f\u969b\u306b\u6709\u52b9\u306a column \u306b\u8fd1\u3044 row \u304b\u3089\u30b9\u30ad\u30e3\u30f3\u3057\u306f\u3058\u3081\u308b\u305f\u3081\u3001\u3059\u3079\u3066\u306e tombstones \u306e\u53ce\u96c6\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b\u3053\u3068\u3092\u53ef\u80fd\u306b\u3057\u307e\u3057\u305f\u3002row \u306e\u30b5\u30a4\u30ba\u3092\u5897\u52a0\u3055\u305b\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u3053\u306e\u9055\u3044\u306f\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u3002\n\nSummary\nLots of deleted columns (also expiring columns) and slice queries don\u2019t play well together. If you observe this pattern in your cluster, you should correct your data model. If you know where your live data begins, hint Cassandra with a start column, to reduce the scan times and the amount of tombstones to collect. Do not use Cassandra to implement a durable queue.\n\u591a\u304f\u306e\u524a\u9664\u3055\u308c\u305f columns (\u3055\u3089\u306b\u671f\u9650\u5207\u308c\u306e columns) \u306f\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306e\u901f\u5ea6\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u3002\u3082\u3057\u3001\u3053\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u73fe\u308c\u308b\u5834\u5408\u306f\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u3092\u4fee\u6b63\u3059\u308b\u3079\u304d\u3067\u3059\u3002\u3082\u3057\u6709\u52b9\u306a\u30c7\u30fc\u30bf\u304c\u3069\u3053\u3067\u306f\u3058\u307e\u308b\u304b\u77e5\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001Cassandra \u306b\u30b9\u30bf\u30fc\u30c8 column \u306e\u30d2\u30f3\u30c8\u3092\u4e0e\u3048\u3001\u53ce\u96c6\u3059\u308b tombstones \u306e\u91cf\u3092\u6e1b\u3089\u3057\u30b9\u30ad\u30e3\u30f3\u6642\u9593\u3092\u7e2e\u5c0f\u3057\u3066\u304f\u3060\u3055\u3044\u3002Cassandra \u3092\u6c38\u7d9a\u30ad\u30e5\u30fc\u306e\u5b9f\u88c5\u3068\u3057\u3066\u4f7f\u7528\u3057\u3066\u306f\u3044\u3051\u307e\u305b\u3093\u3002\n\n\u4e00\u7dd2\u306b\u8aad\u307f\u305f\u3044\n\n\nIs it possible to use a cassandra table as a basic queue\n\nhttp://stackoverflow.com/questions/17945924/is-it-possible-to-use-a-cassandra-table-as-a-basic-queue\n\n\n\nSafety valve on number of tombstones skipped on read path to prevent a full heap\n\nhttps://issues.apache.org/jira/browse/CASSANDRA-5143\n\n\n\nAvoid death-by-tombstone by default\n\nhttps://issues.apache.org/jira/browse/CASSANDRA-6117\n\n\n\nMessage Queue\n\nhttps://github.com/Netflix/astyanax/wiki/Message-Queue\n\n\n\n\nCassandra \u3092\u30ad\u30e5\u30fc\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3053\u3068\u306f\u30a2\u30f3\u30c1\u30d1\u30bf\u30fc\u30f3\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002\u5b9f\u969b\u306b\u3069\u3093\u306a\u554f\u984c\u304c\u3042\u308b\u306e\u304b\u3001Datastax \u306e\u30d6\u30ed\u30b0\u306b\u826f\u3055\u3052\u306a\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u8aad\u3093\u3067\u307f\u305f\u3002\u3053\u306e\u8a18\u4e8b\u306f\u3059\u3053\u3057\u53e4\u3044 (2013/04/26) \u3051\u3069 Cassandra-1.2.x \u3092\u5bfe\u8c61\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u73fe\u6642\u70b9\u3067\u306e Cassandra-2.0.x \u3067\u3082\u5927\u4f53\u540c\u3058\u3053\u3068\u304c\u8a00\u3048\u308b\u3068\u601d\u3046\u3002\u9069\u5f53\u306a\u610f\u8a33\u306a\u306e\u3067\u9593\u9055\u3063\u3066\u305f\u3089\u3054\u3081\u3093\u306a\u3055\u3044\u3002\n\n## Author\n\nBy Aleksey Yeschenko - April 26, 2013 \nhttp://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets\n\n## Deletes in Cassandra\n\nCassandra uses a log-structured storage engine. Because of this, deletes do not remove the rows and columns immediately and in-place. Instead, Cassandra writes a special marker, called a tombstone, indicating that a row, column, or range of columns was deleted. These tombstones are kept for at least the period of time defined by the gc_grace_seconds per-table setting. Only then a tombstone can be permanently discarded by compaction.\n\nCassandra \u306f\u30ed\u30b0\u69cb\u9020\u30b9\u30c8\u30ec\u30fc\u30b8\u30a8\u30f3\u30b8\u30f3\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u305f\u3081\u3001deletes \u306f rows \u3084 columns \u3092\u5373\u5ea7\u306b\u524a\u9664\u3057\u307e\u305b\u3093\u3002\u4ee3\u308f\u308a\u306b\u3001\u305d\u306e row \u3084 column \u3042\u308b\u3044\u306f columns \u306e\u7bc4\u56f2\u304c\u524a\u9664\u3055\u308c\u305f\u3053\u3068\u3092\u793a\u3059 tombstone \u3068\u547c\u3070\u308c\u308b\u7279\u5225\u306a\u30de\u30fc\u30ab\u3092\u66f8\u304d\u8fbc\u307f\u307e\u3059\u3002\u3053\u308c\u3089\u306e tombstone \u306f\u3001\u5c11\u306a\u304f\u3066\u3082\u30c6\u30fc\u30d6\u30eb\u3054\u3068\u306e gc_grace_seconds \u8a2d\u5b9a\u5024\u306b\u3088\u3063\u3066\u5b9a\u7fa9\u3055\u308c\u305f\u671f\u9593\u306f\u7dad\u6301\u3055\u308c\u307e\u3059\u3002\u305d\u306e\u5f8c tombstone \u306f compaction \u306b\u3088\u3063\u3066\u5ec3\u68c4\u3055\u308c\u307e\u3059\u3002\n\nThis scheme allows for very fast deletes (and writes in general), but it\u2019s not free: aside from the obvious RAM/disk overhead of tombstones, you might have to pay a certain price when reading data back if you haven\u2019t modelled your data well.\n\n\u3053\u306e\u30b9\u30ad\u30fc\u30de\u306f\u975e\u5e38\u306b\u9ad8\u901f\u306a deletes (\u304a\u3088\u3073\u4e00\u822c\u7684\u306a\u66f8\u304d\u8fbc\u307f) \u3092\u8a31\u53ef\u3057\u307e\u3059\u304c\u3001\u81ea\u7531\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002tombstones \u306b\u304a\u3051\u308b RAM/disk \u306e\u30aa\u30fc\u30d0\u30d8\u30c3\u30c9\u306f\u5225\u3068\u3057\u3066\u3082\u3001\u30c7\u30fc\u30bf\u3092\u826f\u304f\u30e2\u30c7\u30eb\u5316\u3057\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fd4\u3059\u5834\u5408\u306b\u3001\u4e00\u5b9a\u306e\u30b3\u30b9\u30c8\u3092\u6255\u3046\u5fc5\u8981\u304c\u751f\u3058\u308b\u3067\u3057\u3087\u3046\u3002\n\nSpecifically, tombstones will [bite](https://issues.apache.org/jira/browse/CASSANDRA-5143) you if you do lots of deletes (especially column-level deletes) and later perform slice queries on rows with a lot of tombstones.\n\n\u5177\u4f53\u7684\u306b\u306f\u3001\u3082\u3057\u591a\u304f\u306e deletes (\u7279\u306b column \u30ec\u30d9\u30eb\u306e deletes) \u3092\u884c\u3046\u3068 tombstones \u306f[\u554f\u984c](https://issues.apache.org/jira/browse/CASSANDRA-5143)\u304c\u751f\u3058\u308b\u3067\u3057\u3087\u3046\u3002\u305d\u3057\u3066\u3001\u591a\u304f\u306e tombstones \u3092\u4fdd\u6709\u3057\u305f rows \u306b\u5bfe\u3059\u308b\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u51e6\u7406\u304c\u9045\u304f\u306a\u308b\u3067\u3057\u3087\u3046\u3002\n\n## Symptoms of a wrong data model\n\nTo illustrate this scenario, let\u2019s consider the most extreme case \u2013 using Cassandra as a durable queue, a known anti-pattern\n\n\u3053\u306e\u30b7\u30ca\u30ea\u30aa\u3092\u8aac\u660e\u3059\u308b\u305f\u3081\u306b\u3001\u6700\u3082\u6975\u7aef\u306a\u30b1\u30fc\u30b9\u3067\u691c\u8a0e\u3057\u307e\u3057\u3087\u3046\u3002Cassandra \u3092\u6c38\u7d9a\u30ad\u30e5\u30fc (\u30a2\u30f3\u30c1\u30d1\u30bf\u30fc\u30f3\u3068\u3057\u3066\u77e5\u3089\u308c\u3066\u3044\u308b) \u3068\u3057\u3066\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n```\nCREATE TABLE queues (\n    name text,\n    enqueued_at timeuuid,\n    payload blob,\n    PRIMARY KEY (name, enqueued_at)\n);\n```\n\nHaving enqueued 10000 10-byte messages and then dequeued 9999 of them, one by one, let\u2019s peek at the last remaining message using cqlsh with TRACING ON:\n\n10-byte \u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092 10,000 \u500b\u30ad\u30e5\u30fc\u306b\u683c\u7d0d\u3057\u3001\u305d\u306e\u3046\u3061 9,999 \u500b\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u3072\u3068\u3064\u305a\u3064\u30ad\u30e5\u30fc\u304b\u3089\u53d6\u308a\u9664\u304d\u307e\u3059\u3002\u6b21\u306b\u3001cqlsh \u3067 TRACING \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6709\u52b9\u306b\u3057\u3066\u6700\u5f8c\u306b\u6b8b\u3063\u305f\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u308a\u51fa\u3057\u307e\u3059\u3002\n\n```\nSELECT enqueued_at, payload\n  FROM queues\n WHERE name = 'queue-1'\n LIMIT 1;\n\nactivity                                   | source    | elapsed\n-------------------------------------------+-----------+--------\n                        execute_cql3_query | 127.0.0.3 |       0\n                         Parsing statement | 127.0.0.3 |      48\n                        Peparing statement | 127.0.0.3 |     362\n          Message received from /127.0.0.3 | 127.0.0.1 |      42\n             Sending message to /127.0.0.1 | 127.0.0.3 |     718\nExecuting single-partition query on queues | 127.0.0.1 |     145\n              Acquiring sstable references | 127.0.0.1 |     158\n                 Merging memtable contents | 127.0.0.1 |     189\nMerging data from memtables and 0 sstables | 127.0.0.1 |     235\n    Read 1 live and 19998 tombstoned cells | 127.0.0.1 |  251102\n          Enqueuing response to /127.0.0.3 | 127.0.0.1 |  252976\n             Sending message to /127.0.0.3 | 127.0.0.1 |  253052\n          Message received from /127.0.0.1 | 127.0.0.3 |  324314\n       Processing response from /127.0.0.1 | 127.0.0.3 |  324535\n                          Request complete | 127.0.0.3 |  324812\n\n```\n\nNow even though the whole row was still in memory, the request took more than 300 milliseconds (all the numbers are from a 3-node [ccm](https://github.com/pcmanus/ccm) cluster running on a 2012 MacBook Air).\n\n\u3059\u3079\u3066\u306e row \u306f\u30e1\u30e2\u30ea\u30fc\u306b\u53ce\u307e\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u30ea\u30af\u30a8\u30b9\u30c8\u306f 300 \u30df\u30ea\u79d2\u4ee5\u4e0a\u304b\u304b\u308a\u307e\u3057\u305f\u3002\u691c\u8a3c\u3057\u305f\u74b0\u5883\u306f ccm \u3092\u4f7f\u7528\u3057\u3066 2012 Macbook Air \u306b 3 \u30ce\u30fc\u30c9\u306e Cassandra \u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\n\n## Why did the query take so long to complete?\n\nA slice query will keep reading columns until one of the following condition is met (assuming regular, non-reverse order):\n\n* the specified limit of live columns has been read\n* a column beyond the finish column has been read (if specified)\n* all columns in the row have been read\n\n\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306f\u3001\u4e0b\u8a18\u306e\u6761\u4ef6\u306e\u3046\u3061\u3072\u3068\u3064\u306b\u8a72\u5f53\u3059\u308b\u307e\u3067 columns \u3092\u8aad\u307f\u7d9a\u3051\u308b\u3067\u3057\u3087\u3046\u3002\n\n* \u6307\u5b9a\u3055\u308c\u305f limit \u4ef6\u306e\u6709\u52b9\u306a columns \u304c\u8aad\u307e\u308c\u305f\n* \u30ec\u30f3\u30b8\u306e\u7d42\u4e86 column \u307e\u3067\u8aad\u307e\u308c\u305f (\u6307\u5b9a\u3057\u3066\u3042\u308b\u5834\u5408)\n* row \u306e\u4e2d\u306e\u3059\u3079\u3066\u306e columns \u304c\u8aad\u307e\u308c\u305f\n\nIn the previous scenario Cassandra had to read 9999 tombstones (and create 9999 DeletedColumn objects) before it could get to the only live entry. And all the collected tombstones 1) were consuming heap and 2) had to be serialised and sent back to the coordinator node along with the single live column.\n\n\u524d\u306e\u30b7\u30ca\u30ea\u30aa\u3067\u306f\u3001\u552f\u4e00\u306e\u6709\u52b9\u306a\u30a8\u30f3\u30c8\u30ea\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306b 9,999 \u500b\u306e tombstones (\u305d\u3057\u3066 9,999 \u500b\u306e DeletedClumn \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b) \u3092\u8aad\u3080\u5fc5\u8981\u304c\u3042\u308a\u307e\u3057\u305f\u3002\u307e\u305f\u96c6\u3081\u305f tombstones \u306f\u30d2\u30fc\u30d7\u3092\u6d88\u8cbb\u3057\u3001\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u3055\u308c\u3001\u3072\u3068\u3064\u306e\u6709\u52b9\u306a column \u3068\u5171\u306b coordinator \u30ce\u30fc\u30c9\u306b\u9001\u308a\u8fd4\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3057\u305f\u3002\n\nFor comparison, it took less than 1 millisecond for the same query to complete when no column-level tombstones were involved.\n\n\u6bd4\u8f03\u306e\u305f\u3081\u306b\u3001tombstones \u304c\u542b\u307e\u308c\u3066\u3044\u306a\u3044 column \u306b\u540c\u3058\u30af\u30a8\u30ea\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068 1 \u30df\u30ea\u79d2\u672a\u6e80\u3067\u7d42\u4e86\u3057\u307e\u3057\u305f\u3002\n\nThe queue example might be extreme, but you\u2019ll see the same behaviour when performing slice queries on any row with lots of deleted columns. Also, expiring columns, while more subtle, are going to have the same effect on slice queries once they expire and become tombstones.\n\n\u3053\u306e\u30ad\u30e5\u30fc\u306e\u4f8b\u306f\u6975\u7aef\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001delete \u3055\u308c\u305f columns \u3092\u591a\u304f\u542b\u3093\u3060\u4efb\u610f\u306e row \u306b\u304a\u3044\u3066\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u3092\u884c\u3046\u3068\u540c\u3058\u3088\u3046\u306a\u6319\u52d5\u3092\u898b\u308b\u3067\u3057\u3087\u3046\u3002\u307e\u305f TTL \u3092\u4f7f\u7528\u3059\u308b expiring columns \u306b\u304a\u3044\u3066\u3082\u3001\u671f\u9650\u304c\u5207\u308c\u306b\u3088\u308a tombstones \u306b\u306a\u308b\u305f\u3081\u540c\u3058\u6319\u52d5\u306b\u306a\u308b\u3067\u3057\u3087\u3046\u3002\n\n## Potential workarounds\n\nIf you are seeing this pattern (have to read past many deleted columns before getting to the live ones), chances are that you got your data model wrong and must fix it.\n\nFor example, consider partitioning data with heavy churn rate into separate rows and deleting the entire rows when you no longer need them. Alternatively, partition it into separate tables and truncate them when they aren\u2019t needed anymore.\n\nIn other words, if you use column-level deletes (or expiring columns) heavily and also need to perform slice queries over that data, try grouping columns with close \u2018expiration date\u2019 together and getting rid of them in a single move.\n\n## When you know where your live columns begin\n\nNote that it\u2019s possible to improve on this hypothetical queue scenario. Specifically, when knowing what the last entry was, a consumer can specify the start column and thus somewhat mitigate the effect of tombstones by not having to either 1) start scanning at the beginning of the row and 2) collect and keep all the irrelevant tombstones in memory.\n\n\u3053\u306e\u4eee\u5b9a\u3057\u305f\u30ad\u30e5\u30fc\u30b7\u30ca\u30ea\u30aa\u4e0a\u3067\u6539\u5584\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7279\u306b\u3001\u6700\u5f8c\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u304c\u4f55\u3067\u3042\u3063\u305f\u304b\u3092\u77e5\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001consumer \u306f\u6700\u521d\u306e\u30ab\u30e9\u30e0\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001tombstones \u306e\u5f71\u97ff\u3092\u591a\u5c11\u8efd\u6e1b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\nTo show what I mean, let\u2019s modify the original example by using the previously consumed entry\u2019s key as the start column for the query, i.e.\n\n\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u4f8b\u3067\u884c\u3063\u305f\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306b\u30b9\u30bf\u30fc\u30c8 column \u3092\u6307\u5b9a\u3057\u3066\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```\nSELECT enqueued_at, payload\n  FROM queues\n WHERE name = 'queue-1'\n   AND enqueued_at > 9d1cb818-9d7a-11b6-96ba-60c5470cbf0e\n LIMIT 1;\n\nactivity                                   | source    | elapsed\n-------------------------------------------+-----------+--------\n                        execute_cql3_query | 127.0.0.3 |       0\n                         Parsing statement | 127.0.0.3 |      45\n                        Peparing statement | 127.0.0.3 |     329\n             Sending message to /127.0.0.1 | 127.0.0.3 |     965\n          Message received from /127.0.0.3 | 127.0.0.1 |      34\nExecuting single-partition query on queues | 127.0.0.1 |     339\n              Acquiring sstable references | 127.0.0.1 |     355\n                 Merging memtable contents | 127.0.0.1 |     461\n Partition index lookup over for sstable 3 | 127.0.0.1 |    1122\nMerging data from memtables and 1 sstables | 127.0.0.1 |    2268\n        Read 1 live and 0 tombstoned cells | 127.0.0.1 |    4404\n          Message received from /127.0.0.1 | 127.0.0.3 |    6109\n          Enqueuing response to /127.0.0.3 | 127.0.0.1 |    4492\n             Sending message to /127.0.0.3 | 127.0.0.1 |    4606\n       Processing response from /127.0.0.1 | 127.0.0.3 |    6608\n                          Request complete | 127.0.0.3 |    6901\n```\n\nDespite reading from disk this time, the complete request took 7 milliseconds. Specifying a start column allowed to start scanning the row close to the actual live column and to skip collecting all the tombstones. The difference grows larger with size of the row increasing.\n\n\u4eca\u56de\u306f\u30c7\u30a3\u30b9\u30af\u304b\u3089\u8aad\u3080\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u3001\u30ea\u30af\u30a8\u30b9\u30c8\u306f 7 \u30df\u30ea\u79d2\u3067\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\u30b9\u30bf\u30fc\u30c8 column \u306e\u6307\u5b9a\u306f\u3001\u5b9f\u969b\u306b\u6709\u52b9\u306a column \u306b\u8fd1\u3044 row \u304b\u3089\u30b9\u30ad\u30e3\u30f3\u3057\u306f\u3058\u3081\u308b\u305f\u3081\u3001\u3059\u3079\u3066\u306e tombstones \u306e\u53ce\u96c6\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b\u3053\u3068\u3092\u53ef\u80fd\u306b\u3057\u307e\u3057\u305f\u3002row \u306e\u30b5\u30a4\u30ba\u3092\u5897\u52a0\u3055\u305b\u308b\u3053\u3068\u306b\u3088\u308a\u3001\u3053\u306e\u9055\u3044\u306f\u5927\u304d\u304f\u306a\u308a\u307e\u3059\u3002\n\n## Summary\n\nLots of deleted columns (also expiring columns) and slice queries don\u2019t play well together. If you observe this pattern in your cluster, you should correct your data model. If you know where your live data begins, hint Cassandra with a start column, to reduce the scan times and the amount of tombstones to collect. Do not use Cassandra to implement a durable queue.\n\n\u591a\u304f\u306e\u524a\u9664\u3055\u308c\u305f columns (\u3055\u3089\u306b\u671f\u9650\u5207\u308c\u306e columns) \u306f\u30ec\u30f3\u30b8\u30af\u30a8\u30ea\u30fc\u306e\u901f\u5ea6\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u3002\u3082\u3057\u3001\u3053\u306e\u30d1\u30bf\u30fc\u30f3\u304c\u73fe\u308c\u308b\u5834\u5408\u306f\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u3092\u4fee\u6b63\u3059\u308b\u3079\u304d\u3067\u3059\u3002\u3082\u3057\u6709\u52b9\u306a\u30c7\u30fc\u30bf\u304c\u3069\u3053\u3067\u306f\u3058\u307e\u308b\u304b\u77e5\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u3001Cassandra \u306b\u30b9\u30bf\u30fc\u30c8 column \u306e\u30d2\u30f3\u30c8\u3092\u4e0e\u3048\u3001\u53ce\u96c6\u3059\u308b tombstones \u306e\u91cf\u3092\u6e1b\u3089\u3057\u30b9\u30ad\u30e3\u30f3\u6642\u9593\u3092\u7e2e\u5c0f\u3057\u3066\u304f\u3060\u3055\u3044\u3002Cassandra \u3092\u6c38\u7d9a\u30ad\u30e5\u30fc\u306e\u5b9f\u88c5\u3068\u3057\u3066\u4f7f\u7528\u3057\u3066\u306f\u3044\u3051\u307e\u305b\u3093\u3002\n\n## \u4e00\u7dd2\u306b\u8aad\u307f\u305f\u3044\n\n* Is it possible to use a cassandra table as a basic queue\n * http://stackoverflow.com/questions/17945924/is-it-possible-to-use-a-cassandra-table-as-a-basic-queue\n\n* Safety valve on number of tombstones skipped on read path to prevent a full heap\n * https://issues.apache.org/jira/browse/CASSANDRA-5143\n\n* Avoid death-by-tombstone by default\n * https://issues.apache.org/jira/browse/CASSANDRA-6117\n\n* Message Queue\n * https://github.com/Netflix/astyanax/wiki/Message-Queue\n\n", "tags": ["Cassandra", "memo"]}