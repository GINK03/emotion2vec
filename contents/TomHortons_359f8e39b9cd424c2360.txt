{"context": "\n\n\u306f\u3058\u3081\u306b\n\u904e\u53bb\u306b\u53c2\u52a0\u3057\u305fKaggle\u306e\u60c5\u5831\u3092\u30a2\u30c3\u30d7\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\u3053\u3053\u3067\u306f\uff0cBOSCH\u306e\u30ab\u30fc\u30cd\u30eb\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u305f\u4fbf\u5229\u306a\u30b3\u30fc\u30c9\u3092\u30d4\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u307e\u3059\uff0e\n\u30b3\u30f3\u30da\u6982\u8981\u3084\u512a\u52dd\u8005\u306e\u30b3\u30fc\u30c9\u306b\u95a2\u3057\u3066\u306f\uff0cKaggle\u307e\u3068\u3081:BOSCH(intro+forum discussion)\uff0cKaggle\u307e\u3068\u3081:BOSCH(winner)\u306b\u307e\u3068\u3081\u3066\u304a\u308a\uff0c\u3053\u3061\u3089\u306f\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u4ea4\u3048\u305f\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u7d50\u679c\u3092\u307e\u3068\u3081\u305f\u3082\u306e\u306b\u306a\u308a\u307e\u3059\uff0e\n\n\u672c\u8a18\u4e8b\u306fPython2.7, numpy 1.11, scipy 0.17, scikit-learn 0.18, matplotlib 1.5, seaborn 0.7, pandas 0.17\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\uff0e\njupyter notebook\u4e0a\u3067\u52d5\u4f5c\u78ba\u8a8d\u6e08\u307f\u3067\u3059\uff0e(%matplotlib inline\u306f\u9069\u5f53\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044)\n\u30b5\u30f3\u30d7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u5b9f\u884c\u3057\u305f\u969b\u306b\u30a8\u30e9\u30fc\u7b49\u3042\u3063\u305f\u5834\u5408\u306f\uff0c\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\uff0e\n\n\u76ee\u6b21\n\n\u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u30c7\u30fc\u30bf\u306e\u5468\u671f\u6027\u3092\u8abf\u3079\u308b\n\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\uff12\u5206\u985e\u554f\u984c\u3092XGBoost\u3067\u89e3\u3044\u3066\u307f\u308b\nMagic Features\nEDA of important features\nt-SNE\u3092\u3082\u3061\u3044\u305f\u6b20\u640d\u5024\u306e\u5727\u7e2e\u3068\u53ef\u8996\u5316 (R\u3092\u4f7f\u7528)\n\u88fd\u9020\u30e9\u30a4\u30f3\u306e\u6709\u5411\u30b0\u30e9\u30d5\u8868\u8a18 (R\u3092\u4f7f\u7528)\n\n\n1. \u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u30c7\u30fc\u30bf\u306e\u5468\u671f\u6027\u3092\u8abf\u3079\u308b\n\u4eca\u56de\u306f\u5168\u3066\u306e\u30c7\u30fc\u30bf\u304c\u5fb9\u5e95\u7684\u306b\u533f\u540d\u5316\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u305d\u308c\u306f\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3082\u4f8b\u5916\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\n\u305d\u3082\u305d\u3082date\u30d5\u30a1\u30a4\u30eb\u306e\u6642\u9593\u304c\uff11\u79d2\u306a\u306e\u304b\uff11\u6642\u9593\u306a\u306e\u304b\uff0c\u304d\u308a\u306e\u826f\u3044\u5024\u3067\u306a\u3044\u306e\u304b\u3082\u8b0e\u3067\u3059\uff0e\nData Exploration\u3067\u306f\uff0cbeluga\u304c\u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u3053\u306e\u70b9\u306b\u3064\u3044\u3066\u30a2\u30d7\u30ed\u30fc\u30c1\u3057\u3066\u3044\u307e\u3059\uff0e\ntrain_date.csv\u3092\u3056\u3063\u3068\u898b\u3066\u307f\u308b\u3068\uff0c\u6b21\u306e\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n\n1157 columns\n80 %\u4ee5\u4e0a\u306e\u6b20\u640d\u5024\n\u540c\u3058\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u5909\u6570\u306f\uff0c\u983b\u7e41\u306b\u540c\u3058\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u6301\u3063\u3066\u3044\u308b\n\n\u307e\u305a\u306fTRAIN_DATE\u304b\u3089\u982d10000\u500b\u306e\u5168\u5909\u6570\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u307e\u3059\uff0e\n\ndate_missing.py\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_date_part = pd.read_csv(TRAIN_DATE, nrows=10000)\nprint(train_date_part.shape)\nprint(1.0 * train_date_part.count().sum() / train_date_part.size)\n\n\n\u5b9f\u884c\u7d50\u679c\u304b\u3089\uff0c10000\u500b\u4e2d17.8%\u306e\u307f\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n(10000, 1157)\n0.177920570441\n\n\n1.1. \u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u8abf\u3079\u308b\n\u6b21\u306b\u540c\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u304c\u3069\u308c\u3060\u3051\u8fd1\u3057\u3044\u304b\u3092\u8abf\u3079\u307e\u3059\uff0e\n\nread_station_times.py\n# Let's check the min and max times for each station\ndef get_station_times(dates, withId=False):\n    times = []\n    cols = list(dates.columns)\n    if 'Id' in cols:\n        cols.remove('Id')\n    for feature_name in cols:\n        if withId:\n            df = dates[['Id', feature_name]].copy()\n            df.columns = ['Id', 'time']\n        else:\n            df = dates[[feature_name]].copy()\n            df.columns = ['time']\n        df['station'] = feature_name.split('_')[1][1:]\n        df = df.dropna()\n        times.append(df)\n    return pd.concat(times)\n\nstation_times = get_station_times(train_date_part, withId=True).sort_values(by=['Id', 'station'])\nprint(station_times[:5])\nprint(station_times.shape)\nmin_station_times = station_times.groupby(['Id', 'station']).min()['time']\nmax_station_times = station_times.groupby(['Id', 'station']).max()['time']\nprint(np.mean(1. * (min_station_times == max_station_times)))\n\n\n\u6b20\u640d\u5024\u306f\u53d6\u308a\u9664\u304d\u307e\u3057\u305f\uff0e\u4e0b\u306e\u5b9f\u884c\u7d50\u679c\u3092\u898b\u3066\u304f\u3060\u3055\u3044\uff0e\n\u5168\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u306e98%\u4ee5\u4e0a\u306f\uff0c\u540c\u3058\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u6240\u6301\u3057\u3066\u3044\u307e\u3059\uff0e\n\u305d\u306e\u305f\u3081\u3053\u3053\u3067\u306f\uff0c\u554f\u984c\u3092\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\uff0c\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306f\u5168\u3066\u7b49\u3057\u3044\u3068\u3057\u3066\u3044\u307e\u3059\uff0e\n\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u7a2e\u985e\u306f52\u500b\u306e\u305f\u3081\uff0c\u8abf\u3079\u308b\u5909\u6570\u304c\u5927\u5e45\u306b\u6e1b\u308a\u307e\u3059\uff0e\n(\u3042\u304f\u307e\u3067\u5168date\u30d5\u30a1\u30a4\u30eb\u306e1%\u3092\u898b\u305f\u3060\u3051\u306e\u7d50\u679c\u3067\u3059)\n   Id   time station\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n(2048541, 3)\n0.9821721580467314\n\n\n1.2. \u6642\u9593\u3054\u3068\u306e\u30b5\u30f3\u30d7\u30eb\u6570\u3092\u6642\u7cfb\u5217\u3067\u30d7\u30ed\u30c3\u30c8\ntrain, test\u306e\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\uff0e\n\nread_train_test_station_date.py\n# Read station times for train and test\ndate_cols = train_date_part.drop('Id', axis=1).count().reset_index().sort_values(by=0, ascending=False)\ndate_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\n\ntrain_date = pd.read_csv(TRAIN_DATE, usecols=date_cols)\ntrain_station_times = get_station_times(train_date, withId=False)\ntrain_time_cnt = train_station_times.groupby('time').count()[['station']].reset_index()\ntrain_time_cnt.columns = ['time', 'cnt']\n\ntest_date = pd.read_csv(TEST_DATE, usecols=date_cols)\ntest_station_times = get_station_times(test_date, withId=False)\ntest_time_cnt = test_station_times.groupby('time').count()[['station']].reset_index()\ntest_time_cnt.columns = ['time', 'cnt']\n\n\n\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3068\uff0c\u305d\u306e\u6642\u306b\u53d6\u5f97\u3055\u308c\u3066\u3044\u308b\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6570\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u307e\u3059\uff0e\nfig = plt.figure()\nplt.plot(train_time_cnt['time'].values, train_time_cnt['cnt'].values, 'b.', alpha=0.1, label='train')\nplt.plot(test_time_cnt['time'].values, test_time_cnt['cnt'].values, 'r.', alpha=0.1, label='test')\nplt.title('Original date values')\nplt.ylabel('Number of records')\nplt.xlabel('Time')\nfig.savefig('original_date_values.png', dpi=300)\nplt.show()\n\nprint((train_time_cnt['time'].min(), train_time_cnt['time'].max()))\nprint((test_time_cnt['time'].min(), test_time_cnt['time'].max()))\n\n\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306e\u6700\u5927\u6700\u5c0f\u5024\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n(0.0, 1718.48)\n(0.0, 1718.49)\n\n\u30d7\u30ed\u30c3\u30c8\u306e\u7d50\u679c\u306f\u3053\u3093\u306a\u611f\u3058\uff0e\ntrain, test\u5171\u306b\uff0c\u5168\u304f\u540c\u3058\u30d7\u30ed\u30c3\u30c8\u7d50\u679c\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\n\n\u3053\u3053\u307e\u3067\u3067date\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066\u308f\u304b\u3063\u305f\u3053\u3068\u3092\u307e\u3068\u3081\u308b\u3068\uff0c\n\nTrain, test\u306f\u540c\u3058\u6642\u9593\u9593\u9694\n\u660e\u3089\u304b\u306a\u5468\u671f\u6027\u3089\u3057\u304d\u3082\u306e\u304c\u3042\u308b\n\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306f\uff0c0 - 1718\u304b\u30640.01\u306e\u7cbe\u5ea6\u3067\u53d6\u5f97\u3057\u3066\u3044\u308b\n\u771f\u3093\u4e2d\u306b\u9699\u9593\u304c\u5b58\u5728\u3059\u308b\n\n\n1.3. 0.01\u304c\u5b9f\u6642\u9593\u3067\u4f55\u79d2\u304b\u8abf\u3079\u308b\n0.01\u304c\u5b9f\u6642\u9593\u3067\u4f55\u79d2\u304b\u308f\u304b\u308b\u304b\u8abf\u3079\u3066\u898b\u307e\u3057\u3087\u3046\uff0e\n\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u304b\u3089\u4fee\u6b63\u3092\u6c42\u3081\u305f\u3044\u5834\u5408\uff0c\u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304c\u4f7f\u3048\u307e\u3059\uff0e\ntime_ticks = np.arange(train_time_cnt['time'].min(), train_time_cnt['time'].max() + 0.01, 0.01)\ntime_ticks = pd.DataFrame({'time': time_ticks})\ntime_ticks = pd.merge(time_ticks, train_time_cnt, how='left', on='time')\ntime_ticks = time_ticks.fillna(0)\n# Autocorrelation\nx = time_ticks['cnt'].values\nmax_lag = 8000\nauto_corr_ks = range(1, max_lag)\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nfig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')\nperiod = 25\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')\nperiod = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')\nplt.xlabel('k * 0.01 -  autocorrelation lag')\nplt.ylabel('autocorrelation')\nplt.legend(loc=0)\nfig.savefig('train_time_auto_correlation.png', dpi=300)\n\n\n16.75\u304c\uff11\u5468\u671f\u3068\u898b\u3066\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3059\uff0e\u3053\u3053\u3067\uff0c16.75\u3054\u3068\u306b\u5727\u7e2e\u3057\u3066\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6570\u3092\u6539\u3081\u3066\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u898b\u307e\u3059\uff0e\n\n\u306f\u3063\u304d\u308a\u3068\uff11\u9031\u9593\u306e\u5468\u671f\u6027\u304c\u898b\u3048\u307e\u3057\u305f\uff0e\u4f11\u307f\u3082\u89b3\u6e2c\u3067\u304d\u3066\u3044\u307e\u3059\uff0e\n\u4ee5\u4e0a\u304b\u3089\uff0c0.01==6min\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\uff0e\n\n2. \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\uff12\u5206\u985e\u554f\u984c\u3092XGBoost\u3067\u89e3\u3044\u3066\u307f\u308b\n\u306f\u3058\u3081\u306f\u3042\u307e\u308a\u8907\u96d1\u306b\u8003\u3048\u305a\uff0c\u751f\u30c7\u30fc\u30bf\u304b\u3089\u76f4\u63a5\u5206\u985e\u3057\u3066\u307f\u308b\u3053\u3068\u3082\u91cd\u8981\u3067\u3059\uff0e\n\u305f\u3060\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u306b\u4e57\u308a\u5207\u3089\u306a\u3044\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u6271\u3063\u3066\u3044\u308b\u306e\u3067\uff0c\u4f55\u304b\u3057\u3089\u306e\u5de5\u592b\u304c\u5fc5\u8981\u3067\u3059\uff0e\n\u3053\u3053\u3067\u306f\u5168\u4f53\u306e10%\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066XGBoost\u306e\u5b66\u7fd2\u3092\u5b9f\u884c\u3057\u3066\u898b\u307e\u3059\uff0e\n\n2.1. \u30c7\u30fc\u30bf\u306eread\u3068\u91cd\u8981\u5ea6\u306e\u6e2c\u5b9a\n\nchunk.py\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import matthews_corrcoef, roc_auc_score\nfrom sklearn.cross_validation import cross_val_score, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# I'm limited by RAM here and taking the first N rows is likely to be\n# a bad idea for the date data since it is ordered.\n# Sample the data in a roundabout way:\ndate_chunks = pd.read_csv(TRAIN_DATE, index_col=0, chunksize=100000, dtype=np.float32)\nnum_chunks = pd.read_csv(TRAIN_NUMERIC, index_col=0,\n                         usecols=list(range(969)), chunksize=100000, dtype=np.float32)\nX = pd.concat([pd.concat([dchunk, nchunk], axis=1).sample(frac=0.05)\n               for dchunk, nchunk in zip(date_chunks, num_chunks)])\ny = pd.read_csv(TRAIN_NUMERIC, index_col=0, usecols=[0,969], dtype=np.float32).loc[X.index].values.ravel()\nX = X.values\n\n\n\u3053\u306e\u3088\u3046\u306b\uff0cpandas.read_csv\u3067chunksize\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\uff0c\u4e0a\u624b\u306b\u30e1\u30e2\u30ea\u3078\u4e57\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\nmake_clf.py\nclf = XGBClassifier(base_score=0.005)\nclf.fit(X, y)\n\n\nXGBoost\u3092\u6e96\u5099\u3057\uff0c\n\nimportance.py\n# threshold for a manageable number of features\nplt.hist(clf.feature_importances_[clf.feature_importances_>0])\nimportant_indices = np.where(clf.feature_importances_>0.005)[0]\nprint(important_indices)\n\n\nXGBoost.feature_importances\u304b\u3089\uff0c\u5206\u985e\u3078\u8ca2\u732e\u3057\u305f\u5909\u6570\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u307e\u3059\uff0e\n\u5b9f\u884c\u7d50\u679c\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n[  14   23   41   50  385 1019 1029 1034 1042 1056 1156 1161 1166 1171 1172\n 1183 1203 1221 1294 1327 1350 1363 1403 1404 1482 1501 1507 1512 1535 1549\n 1550 1843 1846 1849 1858 1879 1885 1887 1888 1891 1911 1940 1948 1951 1959\n 1974 1975 1982 1985 1988 1993 1994 1995 1999 2006 2007 2010 2028 2040 2046\n 2075 2093]\n\n\n\n2.2. \u91cd\u8981\u306a\u5909\u6570\u306b\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u7d5e\u308a\u5206\u985e\u554f\u984c\u3092\u89e3\u304f\n\u3053\u3053\u307e\u3067\u306e\u7d50\u679c\u304b\u3089\uff0c\u5168\u4f53\u306e1\u5272\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u91cd\u8981\u306a\u5909\u6570\u3060\u3051\u3092\u62bd\u51fa\u3057\u307e\u3057\u305f\uff0e\n\u3053\u306e\u91cd\u8981\u306a\u5909\u6570\u306b\u307e\u3068\u3092\u3057\u307c\u3063\u3066read\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\nread_important_features.py\n# load entire dataset for these features. \n# note where the feature indices are split so we can load the correct ones straight from read_csv\nn_date_features = 1156\nX = np.concatenate([\n    pd.read_csv(TRAIN_DATE, index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices < n_date_features] + 1])).values,\n    pd.read_csv(TRAIN_NUMERIC, index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices >= n_date_features] + 1 - 1156])).values\n], axis=1)\ny = pd.read_csv(TRAIN_NUMERIC, index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()\n\n\n\u30c7\u30fc\u30bf\u3092imortant_indices\u304b\u3089\u9078\u629e\u3057read\u3057\u307e\u3059\uff0e\n\nCV.py\nclf = XGBClassifier(max_depth=5, base_score=0.005)\ncv = StratifiedKFold(y, n_folds=3)\npreds = np.ones(y.shape[0])\nfor i, (train, test) in enumerate(cv):\n    preds[test] = clf.fit(X[train], y[train]).predict_proba(X[test])[:,1]\n    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(y[test], preds[test])))\nprint(roc_auc_score(y, preds))\n\n\nCross-validation\u3067\u7cbe\u5ea6\u3092\u78ba\u304b\u3081\u3066\u898b\u307e\u3059\uff0e\nfold 0, ROC AUC: 0.718\nfold 1, ROC AUC: 0.704\nfold 2, ROC AUC: 0.698\n0.706413059353\n\n\nthres.py\n# pick the best threshold out-of-fold\nthresholds = np.linspace(0.01, 0.99, 50)\nmcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\nplt.plot(thresholds, mcc)\nbest_threshold = thresholds[mcc.argmax()]\nprint(mcc.max())\n\n\n\u4eca\u56de\u306e\u8a55\u4fa1\u6307\u6a19\u306fMCC\u306e\u305f\u3081\uff0c\u4e88\u6e2c\u7d50\u679c\u306eprobability\u304b\u3089\u4e0d\u826f\u54c1\u3068\u5224\u65ad\u3059\u308b\u95be\u5024\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u65b9\u304c\u826f\u3055\u305d\u3046\u3067\u3059\uff0e\u78ba\u304b\u3081\u3066\u898b\u305f\u7d50\u679c\u304c\u6b21\u306e\u56f3\u3067\u3059\uff0e\n\n0.213120930917\n\n\n2.3. \u63d0\u51fa\u7528\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3059\u308b\n\u3053\u306e\u95be\u5024\u306a\u3089\uff0cMCC=0.213\u307e\u3067\u9054\u6210\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\uff0e\n\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u95be\u5024\u3092\u7528\u3044\u3066\uff0c\u63d0\u51fa\u7528\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3057\u307e\u3059\uff0e\n\nmake_submission.py\n# load test data\nX = np.concatenate([\n    pd.read_csv(\"../input/test_date.csv\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices<1156]+1])).values,\n    pd.read_csv(\"../input/test_numeric.csv\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices>=1156] +1 - 1156])).values\n], axis=1)\n\n# generate predictions at the chosen threshold\npreds = (clf.predict_proba(X)[:,1] > best_threshold).astype(np.int8)\n\n# and submit\nsub = pd.read_csv(\"../input/sample_submission.csv\", index_col=0)\nsub[\"Response\"] = preds\nsub.to_csv(\"submission.csv.gz\", compression=\"gzip\")\n\n\n\n3. Magic Features\n\u306a\u305c\u7cbe\u5ea6\u304c\u51fa\u308b\u304b\uff0c\u8ab0\u3082\u308f\u304b\u3089\u306a\u3044\u3082\u306e\u306e\u7cbe\u5ea6\u304c\u51fa\u308b\u88cf\u6280\u304cmagic feature\u3067\u3059\uff0e\n\u306a\u305c\u3053\u308c\u3092\u601d\u3044\u3064\u3044\u305f\u304b\u306f\u308f\u304b\u308a\u307e\u305b\u3093\u304c\uff0c\u4ed6\u306ekaggle\u30b3\u30f3\u30da\u306b\u306f\u5fdc\u7528\u3067\u304d\u306a\u3055\u305d\u3046\u306a\u306e\u3067\u53c2\u8003\u307e\u3067\u306b\u307f\u3066\u304f\u3060\u3055\u3044\uff0e\n\n3.1. \u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: Faron\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nDATA_DIR = \"../input\"\n\nID_COLUMN = 'Id'\nTARGET_COLUMN = 'Response'\n\nSEED = 0\nCHUNKSIZE = 50000\nNROWS = 250000\n\nTRAIN_NUMERIC = \"{0}/train_numeric.csv\".format(DATA_DIR)\nTRAIN_DATE = \"{0}/train_date.csv\".format(DATA_DIR)\n\nTEST_NUMERIC = \"{0}/test_numeric.csv\".format(DATA_DIR)\nTEST_DATE = \"{0}/test_date.csv\".format(DATA_DIR)\n\nFILENAME = \"etimelhoods\"\n\ntrain = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)\ntest = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)\n\ntrain[\"StartTime\"] = -1\ntest[\"StartTime\"] = -1\n\n\nnrows = 0\nfor tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):\n    feats = np.setdiff1d(tr.columns, [ID_COLUMN])\n\n    stime_tr = tr[feats].min(axis=1).values\n    stime_te = te[feats].min(axis=1).values\n\n    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n\n    nrows += CHUNKSIZE\n    if nrows >= NROWS:\n        break\n\n\nntrain = train.shape[0]\ntrain_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n\ntrain_test['magic1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\ntrain_test['magic2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n\ntrain_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n\ntrain_test['magic3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\ntrain_test['magic4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n\ntrain_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\ntrain = train_test.iloc[:ntrain, :]\n\nfeatures = np.setdiff1d(list(train.columns), [TARGET_COLUMN, ID_COLUMN])\n\ny = train.Response.ravel()\ntrain = np.array(train[features])\n\nprint('train: {0}'.format(train.shape))\nprior = np.sum(y) / (1.*len(y))\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n    'subsample': 0.7,\n    'learning_rate': 0.1,\n    'objective': 'binary:logistic',\n    'max_depth': 4,\n    'num_parallel_tree': 1,\n    'min_child_weight': 2,\n    'eval_metric': 'auc',\n    'base_score': prior\n}\n\n\ndtrain = xgb.DMatrix(train, label=y)\nres = xgb.cv(xgb_params, dtrain, num_boost_round=10, nfold=4, seed=0, stratified=True,\n             early_stopping_rounds=1, verbose_eval=1, show_stdv=True)\n\ncv_mean = res.iloc[-1, 0]\ncv_std = res.iloc[-1, 1]\n\nprint('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n\nId\u306e\u5dee\u5206\u3092\u53d6\u308a\u30ea\u30d0\u30fc\u30b9\uff0c\u305d\u308c\u3092\u3082\u3046\u4e00\u5ea6\u7e70\u308a\u8fd4\u3059\u3068MCC>0.4\u3092\u8d85\u3048\u308bmagic feature\u3068\u306a\u308a\u307e\u3059\uff0e\n\u4e0a\u4f4d\u8005\u306e\u30b3\u30e1\u30f3\u30c8\u3092\u898b\u3066\u3044\u308b\u3068\uff0c\u3053\u308c\u3092\u4f7f\u7528\u305b\u305a\u3068\u3082\u3055\u3089\u306b\u9ad8\u3044\u30b9\u30b3\u30a2\u3092\u51fa\u305b\u308b\u305d\u3046\u3067\u3059\uff0e\n\n3.2. magic features\u3092\u89e3\u6790\u3059\u308b\n\u306a\u305c\u7cbe\u5ea6\u304c\u51fa\u308b\u304b\u8a73\u3057\u304f\u307f\u3066\u3044\u304d\u307e\u3059\uff0e\ndef twoplot(df, col, xaxis=None):\n    ''' scatter plot a feature split into response values as two subgraphs '''\n    if col not in df.columns.values:\n        print('ERROR: %s not a column' % col)\n    ndf = pd.DataFrame(index = df.index)\n    ndf[col] = df[col]\n    ndf[xaxis] = df[xaxis] if xaxis else df.index\n    ndf['Response'] = df['Response']\n\n    g = sns.FacetGrid(ndf, col=\"Response\", hue=\"Response\")\n    g.map(plt.scatter, xaxis, col, alpha=.7, s=1)\n    g.add_legend();\n\n    del ndf\n\n\u4e8c\u3064\u306e\u30d7\u30ed\u30c3\u30c8\u304c\u898b\u3048\u308b\u3088\u3046\u306btwoplot\u3092\u4f5c\u6210\u3057\uff0cmagic1, magic2, magic3, magic4\u306b\u3064\u3044\u3066\u307f\u305f\u7d50\u679c\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\n\n\n\n\nmagic3, magic4\u306b\u306f\u826f\u54c1\u3068\u4e0d\u826f\u54c1\u306b\u9055\u3044\u304c\u898b\u3066\u53d6\u308c\u307e\u3059\uff0e\n\u3053\u306emagic3, magic4\u3092\uff0c\u751f\u30c7\u30fc\u30bf\uff0cdate\uff0ccategory\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u7cbe\u5ea6\u304c\u51fa\u308b\u3068\u306e\u3053\u3068\u3067\u3059\uff0e\n\n4. EDA of important features\n\u6295\u7a3f\u8005\u304c\uff0c1000\u500b\u8fd1\u304f\u3042\u308b\u5909\u6570\u304b\u3089700\u500b\u307b\u3069\u9078\u629e\u3057\u8a08\u7b97\u3057\u305f\u7d50\u679c\uff0cMCC=0.25\u7a0b\u5ea6\u3060\u3063\u305f\u305d\u3046\u3067\u3059\uff0e\n\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u308b\u305f\u3081\uff0c\u4f55\u304b\u3057\u3089\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u308a\u51fa\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u305d\u3053\u3067\uff0c\u3069\u306e\u3088\u3046\u306b\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u308c\u308b\u304b\u691c\u8a0e\u3057\u3066\u898b\u307e\u3057\u305f\uff0c\u3068\u3044\u3046\u5185\u5bb9\u3067\u3059\uff0e\n\n4.1. \u30c7\u30fc\u30bf\u306eread\u3068\u30e9\u30d9\u30eb\u306e\u5206\u5272\n\u307e\u305a\uff0c\u4e00\u90e8\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068XGBoost\u3092\u4f7f\u3063\u3066\uff0c\u6700\u3082\u4e0d\u826f\u54c1\u691c\u77e5\u3078\u8ca2\u732e\u3057\u305d\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u309220\u500b\u9078\u629e\u3057\u307e\u3057\u305f\uff0e\u30b5\u30f3\u30d7\u30eb\u306efeature_names\u304c\u9078\u629e\u3055\u308c\u305f\u5909\u6570\u3067\u3059\uff0e\n\ndata_preparation.py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nfeature_names = ['L3_S38_F3960', 'L3_S33_F3865', 'L3_S38_F3956', 'L3_S33_F3857',\n       'L3_S29_F3321', 'L1_S24_F1846', 'L3_S32_F3850', 'L3_S29_F3354',\n       'L3_S29_F3324', 'L3_S35_F3889', 'L0_S1_F28', 'L1_S24_F1844',\n       'L3_S29_F3376', 'L0_S0_F22', 'L3_S33_F3859', 'L3_S38_F3952', \n       'L3_S30_F3754', 'L2_S26_F3113', 'L3_S30_F3759', 'L0_S5_F114']\n\n\n\u6b21\u306b\u9078\u629e\u3057\u305f\u7279\u5fb4\u91cf\u3092read\u3057\u307e\u3059\uff0e\n\u540c\u6642\u306b\uff0c'Response'\u30672\u3064\u306e\u30c7\u30fc\u30bf\u3078\u5206\u985e\u3057\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u306e\u30c7\u30fc\u30bf\u3068\uff0c\u826f\u54c1\u3060\u3063\u305f\u5834\u5408\u306e\u30c7\u30fc\u30bf\u3078\uff12\u5206\u3057\u307e\u3059\uff0e\n\nread_and_split.py\nnumeric_cols = pd.read_csv(\"../input/train_numeric.csv\", nrows = 1).columns.values\nimp_idxs = [np.argwhere(feature_name == numeric_cols)[0][0] for feature_name in feature_names]\ntrain = pd.read_csv(\"../input/train_numeric.csv\", \n                index_col = 0, header = 0, usecols = [0, len(numeric_cols) - 1] + imp_idxs)\ntrain = train[feature_names + ['Response']]\nX_neg, X_pos = train[train['Response'] == 0].iloc[:, :-1], train[train['Response']==1].iloc[:, :-1]\n\n\n\n4.2. \u5206\u5e03\u306e\u78ba\u8a8d\n\u6b21\u306b\u5404\u5909\u6570\u306eviolinplot\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u898b\u307e\u3059\uff0e\n\u3053\u308c\u306b\u3088\u308a\uff0c\u500b\u3005\u306e\u5909\u6570\u304c\u72ec\u7acb\u3057\u3066\u3044\u3066\u5206\u5e03\u306e\u5f62\u304c\u7570\u306a\u308b\u5909\u6570\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\uff0c\u30ca\u30a4\u30fc\u30d6\u30d9\u30a4\u30ba\u306e\u3088\u3046\u306a\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u304b\u3089\u4e0d\u826f\u54c1\u63a8\u5b9a\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n\u5168\u3066\u306e\u5909\u6570\u3092\u53ef\u8996\u5316\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u51fa\u529b\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u305f\u306e\u3067(4.1)\uff0c\u3088\u3051\u308c\u3070\u305d\u3061\u3089\u3082\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\nBATCH_SIZE = 5\ntrain_batch =[pd.melt(train[train.columns[batch: batch + BATCH_SIZE].append(np.array(['Response']))], \n                      id_vars = 'Response', value_vars = feature_names[batch: batch + BATCH_SIZE])\n              for batch in list(range(0, train.shape[1] - 1, BATCH_SIZE))]\nFIGSIZE = (12,16)\n_, axs = plt.subplots(len(train_batch), figsize = FIGSIZE)\nplt.suptitle('Univariate distributions')\nfor data, ax in zip(train_batch, axs):\n    sns.violinplot(x = 'variable',  y = 'value', hue = 'Response', data = data, ax = ax, split =True)\n\n\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u305f\u7d50\u679c\u304c\u3053\u3061\u3089\uff0e\n\n\u6b20\u640d\u5024\u306e\u5272\u5408\u304c\u5927\u304d\u3059\u304e\u3066\uff0c\u5206\u5e03\u306e\u5f62\u304c\u9055\u3046\u306e\u304b\uff0c\u7247\u65b9\u306e\u30b5\u30f3\u30d7\u30eb\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3060\u3051\u306a\u306e\u304b\uff0c\u3044\u307e\u3044\u3061\u5224\u65ad\u304c\u3067\u304d\u307e\u305b\u3093\uff0e\n\u305d\u3053\u3067\uff0c\u3053\u308c\u308920\u500b\u306e\u5909\u6570\u3092\u6b20\u640d\u5024\u306e\u5272\u5408\u3067\u53ef\u8996\u5316\u3057\u3066\u898b\u307e\u3059\uff0e\nnon_missing = pd.DataFrame(pd.concat([(X_neg.count()/X_neg.shape[0]).to_frame('negative samples'),\n                                      (X_pos.count()/X_pos.shape[0]).to_frame('positive samples'),  \n                                      ], \n                       axis = 1))\nnon_missing_sort = non_missing.sort_values(['negative samples'])\nnon_missing_sort.plot.barh(title = 'Proportion of non-missing values', figsize = FIGSIZE)\nplt.gca().invert_yaxis()\n\n\u7d50\u679c\u304c\u3053\u3061\u3089\uff0e\n\u7d04\u534a\u5206\u306e\u5909\u6570\u306b\u95a2\u3057\u3066\u306f\uff0c50%\u4ee5\u4e0a\u304c\u6b20\u640d\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n\n\n4.3. \u76f8\u95a2\u4fc2\u6570\u304b\u3089\u65b0\u305f\u306a\u7279\u5fb4\u91cf\u3092\u63a2\u3059\nviolinplot\u3067\u306f\u72ec\u7acb\u5909\u6570\u3068\u3057\u3066\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3057\u305f\uff0e\n\u6b21\u306b\u76f8\u95a2\u95a2\u4fc2\u3068\u3044\u3046\u70b9\u304b\u3089\u7279\u5fb4\u91cf\u3092\u63a2\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\u30e9\u30d9\u30eb\u3067\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\uff0c\u76f8\u95a2\u95a2\u4fc2\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u4f5c\u6210\u3057\u307e\u3059\uff0e\nFIGSIZE = (13,4)\n_, (ax1, ax2) = plt.subplots(1,2, figsize = FIGSIZE)\nMIN_PERIODS = 100\n\ntriang_mask = np.zeros((X_pos.shape[1], X_pos.shape[1]))\ntriang_mask[np.triu_indices_from(triang_mask)] = True\n\nax1.set_title('Negative Class')\nsns.heatmap(X_neg.corr(min_periods = MIN_PERIODS), mask = triang_mask, square=True,  ax = ax1)\n\nax2.set_title('Positive Class')\nsns.heatmap(X_pos.corr(min_periods = MIN_PERIODS), mask = triang_mask, square=True,  ax = ax2)\n\n\n\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3057\u305f\u6642\u306b\u76f8\u95a2\u95a2\u4fc2\u304c\u5d29\u308c\u305f\u5909\u6570\u3092\u63a2\u3057\u307e\u3059\uff0e\n\u3053\u3053\u3067\u306f\u5358\u7d14\u306b\u5dee\u5206\u3092\u6c42\u3081\u3066\u307e\u3059\u304c\uff0c\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u672c\u5f53\u306b\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306f\u5c11\u3057\u7591\u554f\u3067\u3059\uff0e\nsns.heatmap(X_pos.corr(min_periods = MIN_PERIODS) -X_neg.corr(min_periods = MIN_PERIODS), \n             mask = triang_mask, square=True)\n\n\n\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3059\u308b\u3068\uff0c\u76f8\u95a2\u95a2\u4fc2\u304c\u5d29\u308c\u308b\u5909\u6570\u306e\u7d44\u307f\u5408\u308f\u305b\u304c\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3057\u305f\uff0e\n\u3053\u308c\u3089\u3092PCA\u3067\u5727\u7e2e\u3059\u308b\u3068\uff0c\u826f\u3044\u5909\u6570\u304c\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n\u7279\u5fb4\u91cf\u3068\u3044\u3046\u610f\u5473\u3067\u306f\uff0c\u6b20\u640d\u5024\u305d\u306e\u3082\u306e\u304c\u65b0\u305f\u306a\u60c5\u5831\u306b\u306a\u308b\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u3059\uff0e\n\u540c\u3058\u624b\u9806\u3067\uff0c\u30e9\u30d9\u30eb\u3054\u3068\u306b\u6b20\u640d\u5024\u3092\u5206\u5272\u3057\uff0c\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\uff0e\nnan_pos, nan_neg = np.isnan(X_pos), np.isnan(X_neg)\n\ntriang_mask = np.zeros((X_pos.shape[1], X_pos.shape[1]))\ntriang_mask[np.triu_indices_from(triang_mask)] = True\n\nFIGSIZE = (13,4)\n_, (ax1, ax2) = plt.subplots(1,2, figsize = FIGSIZE)\nMIN_PERIODS = 100\n\nax1.set_title('Negative Class')\nsns.heatmap(nan_neg.corr(),   square=True, mask = triang_mask, ax = ax1)\n\nax2.set_title('Positive Class')\nsns.heatmap(nan_pos.corr(), square=True, mask = triang_mask,  ax = ax2)\n\n\n\u540c\u3058\u304f\u4fc2\u6570\u306e\u5dee\u5206\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u4e0d\u826f\u54c1\u767a\u751f\u6642\u306b\u6b20\u640d\u5024\u306e\u767a\u751f\u983b\u5ea6\u304c\u5909\u5316\u3059\u308b\u5909\u6570\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\uff0e\nsns.heatmap(nan_neg.corr() - nan_pos.corr(), mask = triang_mask, square=True)\n\n\n\n5. t-SNE\u3092\u7528\u3044\u305f\u6b20\u640d\u5024\u306e\u5727\u7e2e\u3068\u53ef\u8996\u5316 (R\u3092\u4f7f\u7528)\n\u3053\u3061\u3089\u306e\u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3\u3092\u53c2\u8003\u306b\uff0c\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u5024\u3092t-SNE\u3092\u7528\u3044\u3066\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u304b\u3089\uff12\u6b21\u5143\u3078\u843d\u3068\u3057\u8fbc\u307f\u307e\u3059\uff0e\n\u4f7f\u7528\u8a00\u8a9e\u306fR\u3067\u3059\uff0e\n\u5168\u3066\u306e\u5909\u6570\u306e\u6b20\u640d\u5024(NAN to 0, present to 1)\u3092\uff0c\u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u304b\u3089\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\uff0e\u8a08\u7b97\u3057\u305f\u7d50\u679c\u304czip\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30ea\u30f3\u30af\u5148\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\uff0e(cor_train_numeric.csv)\n\u3053\u3061\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u3044\u3066t-SNE\u3059\u308b\u30b3\u30fc\u30c9\uff0c\u305d\u306e\u7d50\u679c\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\nt-SNE\u306f\u512a\u308c\u305f\u6b21\u5143\u5727\u7e2e\u624b\u6cd5\u3067\u3059\u304c\uff0c\u4eca\u56de\u306e\u30b3\u30f3\u30da\u3067\u306f\u3053\u3061\u3089\u306e\u7d50\u679c\u306f\u7279\u306b\u6709\u52b9\u6d3b\u7528\u3055\u308c\u3066\u3044\u307e\u305b\u3093\uff0e\nlibrary(data.table)\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(ggrepel)\ncor_out <- as.matrix(fread(\"cor_train_numeric.csv\", header = TRUE, sep = \",\"))\ngc(verbose = FALSE)\nset.seed(78)\ntsne_model <- Rtsne(data.frame(cor_out),\n                    dims = 2,\n                    #initial_dims = 50,\n                    initial_dims = ncol(cor_out),\n                    perplexity = 322, #floor((ncol(cor_out)-1)/3)\n                    theta = 0.00,\n                    check_duplicates = FALSE,\n                    pca = FALSE,\n                    max_iter = 1350,\n                    verbose = TRUE,\n                    is_distance = FALSE)\ncorMatrix_out <- as.data.frame(tsne_model$Y)\n    cor_kmeans <- kmeans(corMatrix_out, centers = 5, iter.max = 10, nstart = 3)\n    corMatrix_outclust <- as.factor(c(cor_kmeans$cluster[1:968], 6))\ncorMatrix_names <- colnames(cor_out)\nggplot(corMatrix_out, aes(x = V1, y = V2, color = corMatrix_outclust, shape = corMatrix_outclust)) + geom_point(size = 2.5) + geom_rug() + stat_ellipse(type = \"norm\") + ggtitle(\"T-SNE of Features\") + xlab(\"X\") + ylab(\"Y\") + labs(color = \"Cluster\", shape = \"Cluster\") + geom_text_repel(aes(x = V1, y = V2, label = corMatrix_names), size = 2.8)\n\n\nR\u3067\u6b20\u640d\u5024\u306e\u30d0\u30a4\u30ca\u30ea\u5316\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\n\u5c11\u306a\u304f\u3068\u308216GB\u306e\u30e1\u30e2\u30ea\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\uff0e\nlibrary(propagate)\nfor (i in 1:969) {\n  cat(\"\\rStep \", i, \"\\n\", sep = \"\")\n  train_data[!is.na(train_data[, i]), i] <- 1\n  train_data[is.na(train_data[, i]), i] <- 0\n  if (i %% 10) {gc(verbose = FALSE)}\n}\ngc(verbose = FALSE)\n\ncor_table <- bigcor(train_data, fun = \"cor\", size = 194, verbose = TRUE)\n\n\n6. \u88fd\u9020\u30e9\u30a4\u30f3\u306e\u6709\u5411\u30b0\u30e9\u30d5\u8868\u8a18 (R\u3092\u4f7f\u7528)\n\u3053\u3061\u3089\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fR\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\uff0e\nGGplot\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\uff0c\u5927\u5909\u6d17\u7df4\u3055\u308c\u305f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u63cf\u3044\u3066\u3044\u307e\u3059\uff0e\n\u512a\u52dd\u8005\u306b\u95a2\u3059\u308b\u8a18\u4e8b\u3067\u3082\u66f8\u304d\u307e\u3057\u305f\u304c\uff0c\u3053\u306e\u88fd\u9020\u30e9\u30a4\u30f3\u306e\u95a2\u4fc2\u6027\u304b\u3089\u76f8\u95a2\u3092\u898b\u51fa\u305b\u308b\u304b\u3069\u3046\u304b\u304c\u4e00\u3064\u306e\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u3068\u306a\u308a\u307e\u3057\u305f\uff0e\noptions(warn=-1) #surpress warnings\n\n#imports for data wrangling\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(dplyr)\n\n#get the data - nrows set to 10000 to keep runtime manageable.\n#one expansion option would be to select a time frame to visualized\ndtNum <- fread(\"input/train_numeric.csv\", select = c(\"Id\", \"Response\"),nrows = 10000)\ndtDate <- fread(\"input/train_date.csv\", nrows = 10000)\n\n#for each job identify which stations are passed through and for those store the minimum time\nfor (station in paste0(\"S\",0:51))\n{\n  cols = min(which((grepl(station,colnames(dtDate)))))\n  if(!cols==Inf){\n    dtDate[,paste0(station) := dtDate[,cols,with = FALSE]]\n  }\n}\n\n#limit data to only when passed through station X\ndtStations = dtDate[,!grepl(\"L\",colnames(dtDate)),with=F]\n\n#melt data to go from wide to long format\ndtStationsM = melt(dtStations,id.vars=c(\"Id\"))\n\n#join with numeric to have Response\ndtStationsM %>%\n  left_join(dtNum, by = \"Id\") -> dtStationsM\n\n#remove NA entries - these are plentiful as after melting each station-job combination has its own row\ndtStationsM %>%\n  filter(!is.na(value)) -> dtStationsMFiltered\n\n#sort entries by ascending time\ndtStationsMFiltered %>%\n  arrange(value) -> dtStationsMFiltered\n\n#imports for plotting\nrequire(GGally)\nlibrary(network)\nlibrary(sna)\nlibrary(ggplot2)\n\n#plotting format\noptions(repr.plot.width=5, repr.plot.height=15)\n\n#for each row obtain the subsequent statoin\ndtStationsMFiltered %>%\n  group_by(Id) %>%\n  mutate(nextStation = lead(variable)) -> edgelistsComplete\n\n#for each id find the first node to be entered \nedgelistsComplete %>%\n  group_by(Id) %>%\n  filter(!(variable %in% nextStation)) %>%\n  ungroup() %>%\n  select(variable,Response) -> startingPoints\n\n#prior to each starting point insert an edge from a common origin\ncolnames(startingPoints) = c(\"nextStation\",\"Response\")\nstartingPoints$variable = \"S\"\nedgelistsComplete %>%\n  select(variable,nextStation,Response) -> paths\n\n#for each id find the row where there is no next station (last station to be visited)\n#fill this station with Response value\npaths[is.na(nextStation)]$nextStation = paste(\"Result\",paths[is.na(nextStation)]$Response)\n\n#combine data\npaths = rbind(startingPoints,paths)\npaths = select(paths,-Response)\npaths$nextStation = as.character(paths$nextStation)\npaths$variable = as.character(paths$variable)\n\n#rename columns for plotting\ncolnames(paths) <- c(\"Target\",\"Source\")\n\n#flip columns in a costly way because ggnet is a little dumb and I am lazy\npathshelp = select(paths,Source)\npathshelp$Target = paths$Target\npaths=pathshelp\n\n#create network from edgelist\nnet = network(as.data.frame(na.omit(paths)),\n              directed = TRUE)\n\n#create a station-line mapping lookup\nLineStations = NULL\nfor (station in unique(paths$Source)){\n  if(station!=\"S\")\n  {\n    x=paste0(\"_\",station,\"_\")\n    y=head(colnames(dtDate)[which(grepl(x,colnames(dtDate)))],1)\n    y=strsplit(y,\"_\")[[1]][1]\n    LineStations = rbind(LineStations,data.frame(Node=station,Line=y))\n  }\n}\nLineStations = rbind(LineStations,data.frame(Node=c(\"Result 1\",\"Result 0\",\"S\"),Line=c(\"Outcome\",\"Outcome\",\"START\")))\n\n#merge station-line mapping into graph for coloring purposes\nx = data.frame(Node = network.vertex.names(net))\nx = merge(x, LineStations, by = \"Node\", sort = FALSE)$Line\nnet %v% \"line\" = as.character(x)\n\n#setup station coordinates analogue to @JohnM\nnodeCoordinates=data.frame(label=c(\"S\",\"S0\",\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\n                                   \"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\",\n                                   \"S14\",\"S15\",\"S16\",\"S17\",\"S18\",\"S19\",\n                                   \"S20\",\"S21\",\"S22\",\"S23\",\"S24\",\"S25\",\n                                   \"S26\",\"S27\",\"S28\",\"S29\",\"S30\",\"S31\",\n                                   \"S32\",\"S33\",\"S34\",\"S35\",\"S36\",\"S37\",\n                                   \"S38\",\"S39\",\"S40\",\"S41\",\"S43\",\n                                   \"S44\",\"S45\",\"S47\",\"S48\",\"S49\",\n                                   \"S50\",\"S51\",\"Result 0\",\"Result 1\"),\n                           y=c(0,\n                               1,2,3,3,4,4,5,5,6,7,7,7,\n                               1,2,3,3,4,4,5,5,6,7,7,7,\n                               6,6,7,7,7,\n                               8,9,10,10,10,11,11,12,13,14,\n                               8,9,10,11,11,12,13,14,15,15,16,\n                               17,17),\n                           x=c(5,\n                               9,9,10,8,10,8,10,8,7,10,9,8,\n                               5,5,6,4,6,4,6,4,5,6,5,4,\n                               2,0,2,1,0,\n                               7,7,8,7,6,8,6,7,7,7,\n                               3,3,3,4,2,3,3,3,4,2,3,\n                               7,3))\n\nnodeCoordinates$y = -3 * nodeCoordinates$y\n\n#setup initial plot\nnetwork = ggnet2(net)\n\n#grab node list from initial plot and attach coordinates\nnetCoordinates = select(network$data,label)\nnetCoordinates = left_join(netCoordinates,nodeCoordinates,by = \"label\")\nnetCoordinates = as.matrix(select(netCoordinates,x,y))\n\n#setup plot with manual layout\nnetwork = ggnet2(net,\n                 alpha = 0.75, size = \"indegree\",\n                 label = T, size.cut = 4,\n                 color = \"line\",palette = \"Set1\",\n                 mode = netCoordinates,\n                 edge.alpha = 0.5, edge.size = 1,\n                 legend.position = \"bottom\")\n\n\n#output plot on graphics device\nprint(network)\n\n\u5b9f\u884c\u3059\u308b\u3068\u3053\u3093\u306a\u611f\u3058\n\n\u5185\u8a33\u3092\u898b\u3066\u307f\u307e\u3059\uff0e\n#obtain summary statistic of number of edges on each station pair\npathshelp %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(-count) %>%\n    print(n=10)\n\npathshelp %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=10)\n\npathshelp %>%\n    filter(Target==\"Result 1\") %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=20)\n\npathshelp %>%\n    filter(Target==\"Result 0\") %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=20)\n\n\u5b9f\u884c\u7d50\u679c\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n\u3042\u308b\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u5225\u306e\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u307e\u3067\uff0c\u3069\u308c\u3060\u3051\u306e\u983b\u5ea6\u3067\u9077\u79fb\u3057\u3066\u3044\u308b\u304b\uff0c\u898b\u3048\u3066\u304d\u307e\u3059\uff0e\n# tbl_dt [162 \u00d7 2]\n     stationpair count\n           <chr> <int>\n1       S29->S30  9452\n2       S33->S34  9421\n3  S37->Result 0  9211\n4       S30->S33  8977\n5          S->S0  5733\n6         S0->S1  5726\n7       S36->S37  4827\n8       S34->S36  4801\n9       S35->S37  4646\n10      S34->S35  4635\n# ... with 152 more rows\nSource: local data table [162 x 2]\n\n# tbl_dt [162 \u00d7 2]\n   stationpair count\n         <chr> <int>\n1       S->S30     1\n2     S22->S23     1\n3      S6->S10     1\n4     S10->S40     1\n5     S16->S17     1\n6     S28->S30     1\n7     S21->S23     1\n8       S2->S3     1\n9     S24->S25     1\n10      S4->S5     1\n# ... with 152 more rows\nSource: local data table [3 x 2]\n\n# tbl_dt [3 \u00d7 2]\n    stationpair count\n          <chr> <int>\n1 S51->Result 1     2\n2 S38->Result 1     4\n3 S37->Result 1    47\nSource: local data table [6 x 2]\n\n# tbl_dt [6 \u00d7 2]\n    stationpair count\n          <chr> <int>\n1 S50->Result 0     1\n2 S35->Result 0     3\n3 S36->Result 0     3\n4 S38->Result 0   227\n5 S51->Result 0   499\n6 S37->Result 0  9211\n\n\u3053\u3053\u3067\u3053\u308c\u4ee5\u4e0a\u66f8\u304f\u3068\uff0c\u30dc\u30ea\u30e5\u30fc\u30e0\u304c\u591a\u3059\u304e\u308b\u3068\u601d\u3063\u305f\u306e\u3067\uff0c\u5225\u306e\u8a18\u4e8b\u306b\u3066\u3053\u3061\u3089\u306e\u30d1\u30b9\u306b\u3064\u3044\u3066\u6df1\u6398\u308a\u3057\u3066\u3044\u307e\u3059\uff0e\n\u5404\u30d1\u30b9\u306e\u9077\u79fb\u91cf\u306e\u53ef\u8996\u5316\uff0cResponse\u3054\u3068\u306e\u6bd4\u8f03\uff0cPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308bNetworkX\u3092\u4f7f\u7528\u3057\u305f\u5834\u5408\uff0c\u306e\u767a\u5c55\u7684\u5185\u5bb9\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u3066\u3042\u308b\u306e\u3067\uff0c\u3088\u3051\u308c\u3070\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n# \u306f\u3058\u3081\u306b\n\u904e\u53bb\u306b\u53c2\u52a0\u3057\u305fKaggle\u306e\u60c5\u5831\u3092\u30a2\u30c3\u30d7\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\u3053\u3053\u3067\u306f\uff0c[BOSCH](https://www.kaggle.com/c/bosch-production-line-performance)\u306e\u30ab\u30fc\u30cd\u30eb\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u305f\u4fbf\u5229\u306a\u30b3\u30fc\u30c9\u3092\u30d4\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u307e\u3059\uff0e\n\u30b3\u30f3\u30da\u6982\u8981\u3084\u512a\u52dd\u8005\u306e\u30b3\u30fc\u30c9\u306b\u95a2\u3057\u3066\u306f\uff0c[Kaggle\u307e\u3068\u3081:BOSCH(intro+forum discussion)](http://qiita.com/TomHortons/items/e8a7cea90226bd5ed32f)\uff0c[Kaggle\u307e\u3068\u3081:BOSCH(winner)](http://qiita.com/TomHortons/items/51aa356455c0b6e8945a)\u306b\u307e\u3068\u3081\u3066\u304a\u308a\uff0c\u3053\u3061\u3089\u306f\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u4ea4\u3048\u305f\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u7d50\u679c\u3092\u307e\u3068\u3081\u305f\u3082\u306e\u306b\u306a\u308a\u307e\u3059\uff0e\n\n[![logo](https://qiita-image-store.s3.amazonaws.com/0/72093/64b4d5f8-cce0-8805-e414-43736fdad03a.png)](https://www.kaggle.com/c/bosch-production-line-performance)\n\n\u672c\u8a18\u4e8b\u306fPython2.7, numpy 1.11, scipy 0.17, scikit-learn 0.18, matplotlib 1.5, seaborn 0.7, pandas 0.17\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\uff0e\njupyter notebook\u4e0a\u3067\u52d5\u4f5c\u78ba\u8a8d\u6e08\u307f\u3067\u3059\uff0e(%matplotlib inline\u306f\u9069\u5f53\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044)\n\u30b5\u30f3\u30d7\u30eb\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u5b9f\u884c\u3057\u305f\u969b\u306b\u30a8\u30e9\u30fc\u7b49\u3042\u3063\u305f\u5834\u5408\u306f\uff0c\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\uff0e\n\n# \u76ee\u6b21\n1. \u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u30c7\u30fc\u30bf\u306e\u5468\u671f\u6027\u3092\u8abf\u3079\u308b\n2. \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\uff12\u5206\u985e\u554f\u984c\u3092XGBoost\u3067\u89e3\u3044\u3066\u307f\u308b\n3. Magic Features\n4. EDA of important features\n5. t-SNE\u3092\u3082\u3061\u3044\u305f\u6b20\u640d\u5024\u306e\u5727\u7e2e\u3068\u53ef\u8996\u5316 (R\u3092\u4f7f\u7528)\n6. \u88fd\u9020\u30e9\u30a4\u30f3\u306e\u6709\u5411\u30b0\u30e9\u30d5\u8868\u8a18 (R\u3092\u4f7f\u7528)\n\n\n\n# 1. \u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u30c7\u30fc\u30bf\u306e\u5468\u671f\u6027\u3092\u8abf\u3079\u308b\n\u4eca\u56de\u306f\u5168\u3066\u306e\u30c7\u30fc\u30bf\u304c\u5fb9\u5e95\u7684\u306b\u533f\u540d\u5316\u3055\u308c\u3066\u3044\u307e\u3059\uff0e\u305d\u308c\u306f\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3082\u4f8b\u5916\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff0e\n\u305d\u3082\u305d\u3082\bdate\u30d5\u30a1\u30a4\u30eb\u306e\u6642\u9593\u304c\uff11\u79d2\u306a\u306e\u304b\uff11\u6642\u9593\u306a\u306e\u304b\uff0c\u304d\u308a\u306e\u826f\u3044\u5024\u3067\u306a\u3044\u306e\u304b\u3082\u8b0e\u3067\u3059\uff0e\n\b[Data Exploration](https://www.kaggle.com/gaborfodor/bosch-production-line-performance/notebookd19d11e4f2/comments)\u3067\u306f\uff0cbeluga\u304c\u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304b\u3089\u3053\u306e\u70b9\u306b\u3064\u3044\u3066\u30a2\u30d7\u30ed\u30fc\u30c1\u3057\u3066\u3044\u307e\u3059\uff0e\n\ntrain_date.csv\u3092\u3056\u3063\u3068\u898b\u3066\u307f\u308b\u3068\uff0c\u6b21\u306e\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n\n* 1157 columns\n* 80 %\u4ee5\u4e0a\u306e\u6b20\u640d\u5024\n* \u540c\u3058\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u5909\u6570\u306f\uff0c\u983b\u7e41\u306b\u540c\u3058\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u6301\u3063\u3066\u3044\u308b\n\n\n\u307e\u305a\u306fTRAIN_DATE\u304b\u3089\u982d10000\u500b\u306e\u5168\u5909\u6570\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u307e\u3059\uff0e\n\n```date_missing.py\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_date_part = pd.read_csv(TRAIN_DATE, nrows=10000)\nprint(train_date_part.shape)\nprint(1.0 * train_date_part.count().sum() / train_date_part.size)\n```\n\n\u5b9f\u884c\u7d50\u679c\u304b\u3089\uff0c10000\u500b\u4e2d17.8%\u306e\u307f\u304c\u5b9f\u6570\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n\n```\n(10000, 1157)\n0.177920570441\n```\n\n## 1.1. \u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u8abf\u3079\u308b\n\u6b21\u306b\u540c\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u304c\u3069\u308c\u3060\u3051\u8fd1\u3057\u3044\u304b\u3092\u8abf\u3079\u307e\u3059\uff0e\n\n```read_station_times.py\n# Let's check the min and max times for each station\ndef get_station_times(dates, withId=False):\n    times = []\n    cols = list(dates.columns)\n    if 'Id' in cols:\n        cols.remove('Id')\n    for feature_name in cols:\n        if withId:\n            df = dates[['Id', feature_name]].copy()\n            df.columns = ['Id', 'time']\n        else:\n            df = dates[[feature_name]].copy()\n            df.columns = ['time']\n        df['station'] = feature_name.split('_')[1][1:]\n        df = df.dropna()\n        times.append(df)\n    return pd.concat(times)\n\nstation_times = get_station_times(train_date_part, withId=True).sort_values(by=['Id', 'station'])\nprint(station_times[:5])\nprint(station_times.shape)\nmin_station_times = station_times.groupby(['Id', 'station']).min()['time']\nmax_station_times = station_times.groupby(['Id', 'station']).max()['time']\nprint(np.mean(1. * (min_station_times == max_station_times)))\n```\n\n\u6b20\u640d\u5024\u306f\u53d6\u308a\u9664\u304d\u307e\u3057\u305f\uff0e\u4e0b\u306e\u5b9f\u884c\u7d50\u679c\u3092\u898b\u3066\u304f\u3060\u3055\u3044\uff0e\n\u5168\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u306e98%\u4ee5\u4e0a\u306f\uff0c\u540c\u3058\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3092\u6240\u6301\u3057\u3066\u3044\u307e\u3059\uff0e\n\u305d\u306e\u305f\u3081\u3053\u3053\u3067\u306f\uff0c\u554f\u984c\u3092\u7c21\u5358\u306b\u3059\u308b\u305f\u3081\uff0c\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u5185\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306f\u5168\u3066\u7b49\u3057\u3044\u3068\u3057\u3066\u3044\u307e\u3059\uff0e\n\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u7a2e\u985e\u306f52\u500b\u306e\u305f\u3081\uff0c\u8abf\u3079\u308b\u5909\u6570\u304c\u5927\u5e45\u306b\u6e1b\u308a\u307e\u3059\uff0e\n(\u3042\u304f\u307e\u3067\u5168date\u30d5\u30a1\u30a4\u30eb\u306e1%\u3092\u898b\u305f\u3060\u3051\u306e\u7d50\u679c\u3067\u3059)\n\n```\n   Id   time station\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n0   4  82.24       0\n(2048541, 3)\n0.9821721580467314\n```\n\n## 1.2. \u6642\u9593\u3054\u3068\u306e\u30b5\u30f3\u30d7\u30eb\u6570\u3092\u6642\u7cfb\u5217\u3067\u30d7\u30ed\u30c3\u30c8\n\ntrain, test\u306e\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\uff0e\n\n```read_train_test_station_date.py\n# Read station times for train and test\ndate_cols = train_date_part.drop('Id', axis=1).count().reset_index().sort_values(by=0, ascending=False)\ndate_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\n\ntrain_date = pd.read_csv(TRAIN_DATE, usecols=date_cols)\ntrain_station_times = get_station_times(train_date, withId=False)\ntrain_time_cnt = train_station_times.groupby('time').count()[['station']].reset_index()\ntrain_time_cnt.columns = ['time', 'cnt']\n\ntest_date = pd.read_csv(TEST_DATE, usecols=date_cols)\ntest_station_times = get_station_times(test_date, withId=False)\ntest_time_cnt = test_station_times.groupby('time').count()[['station']].reset_index()\ntest_time_cnt.columns = ['time', 'cnt']\n```\n\n\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u3068\uff0c\u305d\u306e\u6642\u306b\u53d6\u5f97\u3055\u308c\u3066\u3044\u308b\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6570\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u307e\u3059\uff0e\n\n```plot_time_cnt\nfig = plt.figure()\nplt.plot(train_time_cnt['time'].values, train_time_cnt['cnt'].values, 'b.', alpha=0.1, label='train')\nplt.plot(test_time_cnt['time'].values, test_time_cnt['cnt'].values, 'r.', alpha=0.1, label='test')\nplt.title('Original date values')\nplt.ylabel('Number of records')\nplt.xlabel('Time')\nfig.savefig('original_date_values.png', dpi=300)\nplt.show()\n\nprint((train_time_cnt['time'].min(), train_time_cnt['time'].max()))\nprint((test_time_cnt['time'].min(), test_time_cnt['time'].max()))\n```\n\n\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306e\u6700\u5927\u6700\u5c0f\u5024\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n\n```\n(0.0, 1718.48)\n(0.0, 1718.49)\n```\n\n\u30d7\u30ed\u30c3\u30c8\u306e\u7d50\u679c\u306f\u3053\u3093\u306a\u611f\u3058\uff0e\ntrain, test\u5171\u306b\uff0c\u5168\u304f\u540c\u3058\u30d7\u30ed\u30c3\u30c8\u7d50\u679c\u3068\u306a\u3063\u3066\u3044\u307e\u3059\uff0e\n\n![__results___5_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/7d488868-1275-c008-fa39-7ccd17e3fd1d.png)\n\n\u3053\u3053\u307e\u3067\u3067date\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066\u308f\u304b\u3063\u305f\u3053\u3068\u3092\u307e\u3068\u3081\u308b\u3068\uff0c\n\n* Train, test\u306f\u540c\u3058\u6642\u9593\u9593\u9694\n* \u660e\u3089\u304b\u306a\u5468\u671f\u6027\u3089\u3057\u304d\u3082\u306e\u304c\u3042\u308b\n* \u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u306f\uff0c0 - 1718\u304b\u30640.01\u306e\u7cbe\u5ea6\u3067\u53d6\u5f97\u3057\u3066\u3044\u308b\n* \u771f\u3093\u4e2d\u306b\u9699\u9593\u304c\u5b58\u5728\u3059\u308b\n\n## 1.3. 0.01\u304c\u5b9f\u6642\u9593\u3067\u4f55\u79d2\u304b\u8abf\u3079\u308b\n\n0.01\u304c\u5b9f\u6642\u9593\u3067\u4f55\u79d2\u304b\u308f\u304b\u308b\u304b\u8abf\u3079\u3066\u898b\u307e\u3057\u3087\u3046\uff0e\n\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u304b\u3089\u4fee\u6b63\u3092\u6c42\u3081\u305f\u3044\u5834\u5408\uff0c\u81ea\u5df1\u76f8\u95a2\u4fc2\u6570\u304c\u4f7f\u3048\u307e\u3059\uff0e\n\n```\ntime_ticks = np.arange(train_time_cnt['time'].min(), train_time_cnt['time'].max() + 0.01, 0.01)\ntime_ticks = pd.DataFrame({'time': time_ticks})\ntime_ticks = pd.merge(time_ticks, train_time_cnt, how='left', on='time')\ntime_ticks = time_ticks.fillna(0)\n# Autocorrelation\nx = time_ticks['cnt'].values\nmax_lag = 8000\nauto_corr_ks = range(1, max_lag)\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nfig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')\nperiod = 25\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')\nperiod = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')\nplt.xlabel('k * 0.01 -  autocorrelation lag')\nplt.ylabel('autocorrelation')\nplt.legend(loc=0)\nfig.savefig('train_time_auto_correlation.png', dpi=300)\n```\n\n![__results___7_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/08774d34-5b2a-59f9-af57-61fe37911ee2.png)\n\n16.75\u304c\uff11\u5468\u671f\u3068\u898b\u3066\u554f\u984c\u306a\u3055\u305d\u3046\u3067\u3059\uff0e\u3053\u3053\u3067\uff0c16.75\u3054\u3068\u306b\u5727\u7e2e\u3057\u3066\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6570\u3092\u6539\u3081\u3066\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u898b\u307e\u3059\uff0e\n\n![__results___9_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/f8f37fab-df91-25b4-fb71-cf2c2c07d3b1.png)\n\n\u306f\u3063\u304d\u308a\u3068\uff11\u9031\u9593\u306e\u5468\u671f\u6027\u304c\u898b\u3048\u307e\u3057\u305f\uff0e\u4f11\u307f\u3082\u89b3\u6e2c\u3067\u304d\u3066\u3044\u307e\u3059\uff0e\n\n__\u4ee5\u4e0a\u304b\u3089\uff0c0.01==6min\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\uff0e__\n\n# 2. \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\uff12\u5206\u985e\u554f\u984c\u3092XGBoost\u3067\u89e3\u3044\u3066\u307f\u308b\n\u306f\u3058\u3081\u306f\u3042\u307e\u308a\u8907\u96d1\u306b\u8003\u3048\u305a\uff0c\u751f\u30c7\u30fc\u30bf\u304b\u3089\u76f4\u63a5\u5206\u985e\u3057\u3066\u307f\u308b\u3053\u3068\u3082\u91cd\u8981\u3067\u3059\uff0e\n\u305f\u3060\u4eca\u56de\u306f\u30e1\u30e2\u30ea\u306b\u4e57\u308a\u5207\u3089\u306a\u3044\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u6271\u3063\u3066\u3044\u308b\u306e\u3067\uff0c\u4f55\u304b\u3057\u3089\u306e\u5de5\u592b\u304c\u5fc5\u8981\u3067\u3059\uff0e\n\u3053\u3053\u3067\u306f\u5168\u4f53\u306e10%\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066XGBoost\u306e\u5b66\u7fd2\u3092\u5b9f\u884c\u3057\u3066\u898b\u307e\u3059\uff0e\n\n## 2.1. \u30c7\u30fc\u30bf\u306eread\u3068\u91cd\u8981\u5ea6\u306e\u6e2c\u5b9a\n\n```chunk.py\nimport numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import matthews_corrcoef, roc_auc_score\nfrom sklearn.cross_validation import cross_val_score, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# I'm limited by RAM here and taking the first N rows is likely to be\n# a bad idea for the date data since it is ordered.\n# Sample the data in a roundabout way:\ndate_chunks = pd.read_csv(TRAIN_DATE, index_col=0, chunksize=100000, dtype=np.float32)\nnum_chunks = pd.read_csv(TRAIN_NUMERIC, index_col=0,\n                         usecols=list(range(969)), chunksize=100000, dtype=np.float32)\nX = pd.concat([pd.concat([dchunk, nchunk], axis=1).sample(frac=0.05)\n               for dchunk, nchunk in zip(date_chunks, num_chunks)])\ny = pd.read_csv(TRAIN_NUMERIC, index_col=0, usecols=[0,969], dtype=np.float32).loc[X.index].values.ravel()\nX = X.values\n```\n\u3053\u306e\u3088\u3046\u306b\uff0cpandas.read_csv\u3067chunksize\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\uff0c\u4e0a\u624b\u306b\u30e1\u30e2\u30ea\u3078\u4e57\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n```make_clf.py\nclf = XGBClassifier(base_score=0.005)\nclf.fit(X, y)\n```\n\nXGBoost\u3092\u6e96\u5099\u3057\uff0c\n\n```importance.py\n# threshold for a manageable number of features\nplt.hist(clf.feature_importances_[clf.feature_importances_>0])\nimportant_indices = np.where(clf.feature_importances_>0.005)[0]\nprint(important_indices)\n```\nXGBoost.feature_importances\u304b\u3089\uff0c\u5206\u985e\u3078\u8ca2\u732e\u3057\u305f\u5909\u6570\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u307e\u3059\uff0e\n\u5b9f\u884c\u7d50\u679c\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n\n```\n[  14   23   41   50  385 1019 1029 1034 1042 1056 1156 1161 1166 1171 1172\n 1183 1203 1221 1294 1327 1350 1363 1403 1404 1482 1501 1507 1512 1535 1549\n 1550 1843 1846 1849 1858 1879 1885 1887 1888 1891 1911 1940 1948 1951 1959\n 1974 1975 1982 1985 1988 1993 1994 1995 1999 2006 2007 2010 2028 2040 2046\n 2075 2093]\n```\n\n![__results___4_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/892d670b-44e4-1cd6-038e-e7407072ba56.png)\n\n## 2.2. \u91cd\u8981\u306a\u5909\u6570\u306b\u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u7d5e\u308a\u5206\u985e\u554f\u984c\u3092\u89e3\u304f\n\n\u3053\u3053\u307e\u3067\u306e\u7d50\u679c\u304b\u3089\uff0c\u5168\u4f53\u306e1\u5272\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u91cd\u8981\u306a\u5909\u6570\u3060\u3051\u3092\u62bd\u51fa\u3057\u307e\u3057\u305f\uff0e\n\u3053\u306e\u91cd\u8981\u306a\u5909\u6570\u306b\u307e\u3068\u3092\u3057\u307c\u3063\u3066read\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\n```read_important_features.py\n# load entire dataset for these features. \n# note where the feature indices are split so we can load the correct ones straight from read_csv\nn_date_features = 1156\nX = np.concatenate([\n    pd.read_csv(TRAIN_DATE, index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices < n_date_features] + 1])).values,\n    pd.read_csv(TRAIN_NUMERIC, index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices >= n_date_features] + 1 - 1156])).values\n], axis=1)\ny = pd.read_csv(TRAIN_NUMERIC, index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()\n```\n\n\u30c7\u30fc\u30bf\u3092imortant_indices\u304b\u3089\u9078\u629e\u3057read\u3057\u307e\u3059\uff0e\n\n```CV.py\nclf = XGBClassifier(max_depth=5, base_score=0.005)\ncv = StratifiedKFold(y, n_folds=3)\npreds = np.ones(y.shape[0])\nfor i, (train, test) in enumerate(cv):\n    preds[test] = clf.fit(X[train], y[train]).predict_proba(X[test])[:,1]\n    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(y[test], preds[test])))\nprint(roc_auc_score(y, preds))\n```\n\nCross-validation\u3067\u7cbe\u5ea6\u3092\u78ba\u304b\u3081\u3066\u898b\u307e\u3059\uff0e\n\n```\nfold 0, ROC AUC: 0.718\nfold 1, ROC AUC: 0.704\nfold 2, ROC AUC: 0.698\n0.706413059353\n```\n\n```thres.py\n# pick the best threshold out-of-fold\nthresholds = np.linspace(0.01, 0.99, 50)\nmcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\nplt.plot(thresholds, mcc)\nbest_threshold = thresholds[mcc.argmax()]\nprint(mcc.max())\n```\n\n\u4eca\u56de\u306e\u8a55\u4fa1\u6307\u6a19\u306fMCC\u306e\u305f\u3081\uff0c\u4e88\u6e2c\u7d50\u679c\u306eprobability\u304b\u3089\u4e0d\u826f\u54c1\u3068\u5224\u65ad\u3059\u308b\u95be\u5024\u3092\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u65b9\u304c\u826f\u3055\u305d\u3046\u3067\u3059\uff0e\u78ba\u304b\u3081\u3066\u898b\u305f\u7d50\u679c\u304c\u6b21\u306e\u56f3\u3067\u3059\uff0e\n\n![__results___7_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/4856fbd4-f2a6-0e4d-5de4-3012e640f567.png)\n\n```\n0.213120930917\n```\n## 2.3. \u63d0\u51fa\u7528\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3059\u308b\n\n\u3053\u306e\u95be\u5024\u306a\u3089\uff0cMCC=0.213\u307e\u3067\u9054\u6210\u3067\u304d\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\uff0e\n\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u95be\u5024\u3092\u7528\u3044\u3066\uff0c\u63d0\u51fa\u7528\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3057\u307e\u3059\uff0e\n\n```make_submission.py\n# load test data\nX = np.concatenate([\n    pd.read_csv(\"../input/test_date.csv\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices<1156]+1])).values,\n    pd.read_csv(\"../input/test_numeric.csv\", index_col=0, dtype=np.float32,\n                usecols=np.concatenate([[0], important_indices[important_indices>=1156] +1 - 1156])).values\n], axis=1)\n\n# generate predictions at the chosen threshold\npreds = (clf.predict_proba(X)[:,1] > best_threshold).astype(np.int8)\n\n# and submit\nsub = pd.read_csv(\"../input/sample_submission.csv\", index_col=0)\nsub[\"Response\"] = preds\nsub.to_csv(\"submission.csv.gz\", compression=\"gzip\")\n```\n\n# 3. Magic Features\n\u306a\u305c\u7cbe\u5ea6\u304c\u51fa\u308b\u304b\uff0c\u8ab0\u3082\u308f\u304b\u3089\u306a\u3044\u3082\u306e\u306e\u7cbe\u5ea6\u304c\u51fa\u308b\u88cf\u6280\u304cmagic feature\u3067\u3059\uff0e\n\u306a\u305c\u3053\u308c\u3092\u601d\u3044\u3064\u3044\u305f\u304b\u306f\u308f\u304b\u308a\u307e\u305b\u3093\u304c\uff0c\u4ed6\u306ekaggle\u30b3\u30f3\u30da\u306b\u306f\u5fdc\u7528\u3067\u304d\u306a\u3055\u305d\u3046\u306a\u306e\u3067\u53c2\u8003\u307e\u3067\u306b\u307f\u3066\u304f\u3060\u3055\u3044\uff0e\n\n## 3.1. \u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\n\n```\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: Faron\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nDATA_DIR = \"../input\"\n\nID_COLUMN = 'Id'\nTARGET_COLUMN = 'Response'\n\nSEED = 0\nCHUNKSIZE = 50000\nNROWS = 250000\n\nTRAIN_NUMERIC = \"{0}/train_numeric.csv\".format(DATA_DIR)\nTRAIN_DATE = \"{0}/train_date.csv\".format(DATA_DIR)\n\nTEST_NUMERIC = \"{0}/test_numeric.csv\".format(DATA_DIR)\nTEST_DATE = \"{0}/test_date.csv\".format(DATA_DIR)\n\nFILENAME = \"etimelhoods\"\n\ntrain = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)\ntest = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)\n\ntrain[\"StartTime\"] = -1\ntest[\"StartTime\"] = -1\n\n\nnrows = 0\nfor tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):\n    feats = np.setdiff1d(tr.columns, [ID_COLUMN])\n\n    stime_tr = tr[feats].min(axis=1).values\n    stime_te = te[feats].min(axis=1).values\n\n    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n\n    nrows += CHUNKSIZE\n    if nrows >= NROWS:\n        break\n\n\nntrain = train.shape[0]\ntrain_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n\ntrain_test['magic1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\ntrain_test['magic2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n\ntrain_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n\ntrain_test['magic3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\ntrain_test['magic4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n\ntrain_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\ntrain = train_test.iloc[:ntrain, :]\n\nfeatures = np.setdiff1d(list(train.columns), [TARGET_COLUMN, ID_COLUMN])\n\ny = train.Response.ravel()\ntrain = np.array(train[features])\n\nprint('train: {0}'.format(train.shape))\nprior = np.sum(y) / (1.*len(y))\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n    'subsample': 0.7,\n    'learning_rate': 0.1,\n    'objective': 'binary:logistic',\n    'max_depth': 4,\n    'num_parallel_tree': 1,\n    'min_child_weight': 2,\n    'eval_metric': 'auc',\n    'base_score': prior\n}\n\n\ndtrain = xgb.DMatrix(train, label=y)\nres = xgb.cv(xgb_params, dtrain, num_boost_round=10, nfold=4, seed=0, stratified=True,\n             early_stopping_rounds=1, verbose_eval=1, show_stdv=True)\n\ncv_mean = res.iloc[-1, 0]\ncv_std = res.iloc[-1, 1]\n\nprint('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n```\n\nId\u306e\u5dee\u5206\u3092\u53d6\u308a\u30ea\u30d0\u30fc\u30b9\uff0c\u305d\u308c\u3092\u3082\u3046\u4e00\u5ea6\u7e70\u308a\u8fd4\u3059\u3068MCC>0.4\u3092\u8d85\u3048\u308bmagic feature\u3068\u306a\u308a\u307e\u3059\uff0e\n\u4e0a\u4f4d\u8005\u306e\u30b3\u30e1\u30f3\u30c8\u3092\u898b\u3066\u3044\u308b\u3068\uff0c\u3053\u308c\u3092\u4f7f\u7528\u305b\u305a\u3068\u3082\u3055\u3089\u306b\u9ad8\u3044\u30b9\u30b3\u30a2\u3092\u51fa\u305b\u308b\u305d\u3046\u3067\u3059\uff0e\n\n## 3.2. magic features\u3092\u89e3\u6790\u3059\u308b\n\n\u306a\u305c\u7cbe\u5ea6\u304c\u51fa\u308b\u304b\u8a73\u3057\u304f\u307f\u3066\u3044\u304d\u307e\u3059\uff0e\n\n```\ndef twoplot(df, col, xaxis=None):\n    ''' scatter plot a feature split into response values as two subgraphs '''\n    if col not in df.columns.values:\n        print('ERROR: %s not a column' % col)\n    ndf = pd.DataFrame(index = df.index)\n    ndf[col] = df[col]\n    ndf[xaxis] = df[xaxis] if xaxis else df.index\n    ndf['Response'] = df['Response']\n    \n    g = sns.FacetGrid(ndf, col=\"Response\", hue=\"Response\")\n    g.map(plt.scatter, xaxis, col, alpha=.7, s=1)\n    g.add_legend();\n    \n    del ndf\n```\n\n\u4e8c\u3064\u306e\u30d7\u30ed\u30c3\u30c8\u304c\u898b\u3048\u308b\u3088\u3046\u306btwoplot\u3092\u4f5c\u6210\u3057\uff0cmagic1, magic2, magic3, magic4\u306b\u3064\u3044\u3066\u307f\u305f\u7d50\u679c\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\n![__results___7_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/74b5e1ac-8579-771b-74ab-81e2f25c6ee0.png)\n![__results___9_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/8b1c636a-2dbe-5dac-fabe-c302588f23dc.png)\n![__results___11_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/0b0e51eb-d690-b6be-15fa-5936d0807fca.png)\n![__results___13_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/35955864-6e73-5a66-cd9d-21c4f3428c9b.png)\n\nmagic3, magic4\u306b\u306f\u826f\u54c1\u3068\u4e0d\u826f\u54c1\u306b\u9055\u3044\u304c\u898b\u3066\u53d6\u308c\u307e\u3059\uff0e\n\u3053\u306emagic3, magic4\u3092\uff0c\u751f\u30c7\u30fc\u30bf\uff0cdate\uff0ccategory\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u7cbe\u5ea6\u304c\u51fa\u308b\u3068\u306e\u3053\u3068\u3067\u3059\uff0e\n\n# 4. EDA of important features\n\u6295\u7a3f\u8005\u304c\uff0c1000\u500b\u8fd1\u304f\u3042\u308b\u5909\u6570\u304b\u3089700\u500b\u307b\u3069\u9078\u629e\u3057\u8a08\u7b97\u3057\u305f\u7d50\u679c\uff0cMCC=0.25\u7a0b\u5ea6\u3060\u3063\u305f\u305d\u3046\u3067\u3059\uff0e\n\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u308b\u305f\u3081\uff0c\u4f55\u304b\u3057\u3089\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u308a\u51fa\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff0e\n\u305d\u3053\u3067\uff0c\u3069\u306e\u3088\u3046\u306b\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u308c\u308b\u304b\u691c\u8a0e\u3057\u3066\u898b\u307e\u3057\u305f\uff0c\u3068\u3044\u3046\u5185\u5bb9\u3067\u3059\uff0e\n\n## 4.1. \u30c7\u30fc\u30bf\u306eread\u3068\u30e9\u30d9\u30eb\u306e\u5206\u5272\n\u307e\u305a\uff0c\u4e00\u90e8\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068XGBoost\u3092\u4f7f\u3063\u3066\uff0c\u6700\u3082\u4e0d\u826f\u54c1\u691c\u77e5\u3078\u8ca2\u732e\u3057\u305d\u3046\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u309220\u500b\u9078\u629e\u3057\u307e\u3057\u305f\uff0e\u30b5\u30f3\u30d7\u30eb\u306efeature_names\u304c\u9078\u629e\u3055\u308c\u305f\u5909\u6570\u3067\u3059\uff0e\n\n```data_preparation.py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nfeature_names = ['L3_S38_F3960', 'L3_S33_F3865', 'L3_S38_F3956', 'L3_S33_F3857',\n       'L3_S29_F3321', 'L1_S24_F1846', 'L3_S32_F3850', 'L3_S29_F3354',\n       'L3_S29_F3324', 'L3_S35_F3889', 'L0_S1_F28', 'L1_S24_F1844',\n       'L3_S29_F3376', 'L0_S0_F22', 'L3_S33_F3859', 'L3_S38_F3952', \n       'L3_S30_F3754', 'L2_S26_F3113', 'L3_S30_F3759', 'L0_S5_F114']\n```\n\n\u6b21\u306b\u9078\u629e\u3057\u305f\u7279\u5fb4\u91cf\u3092read\u3057\u307e\u3059\uff0e\n\u540c\u6642\u306b\uff0c'Response'\u30672\u3064\u306e\u30c7\u30fc\u30bf\u3078\u5206\u985e\u3057\u307e\u3059\uff0e\n\u3064\u307e\u308a\uff0c\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u306e\u30c7\u30fc\u30bf\u3068\uff0c\u826f\u54c1\u3060\u3063\u305f\u5834\u5408\u306e\u30c7\u30fc\u30bf\u3078\uff12\u5206\u3057\u307e\u3059\uff0e\n\n```read_and_split.py\nnumeric_cols = pd.read_csv(\"../input/train_numeric.csv\", nrows = 1).columns.values\nimp_idxs = [np.argwhere(feature_name == numeric_cols)[0][0] for feature_name in feature_names]\ntrain = pd.read_csv(\"../input/train_numeric.csv\", \n                index_col = 0, header = 0, usecols = [0, len(numeric_cols) - 1] + imp_idxs)\ntrain = train[feature_names + ['Response']]\nX_neg, X_pos = train[train['Response'] == 0].iloc[:, :-1], train[train['Response']==1].iloc[:, :-1]\n```\n\n## 4.2. \u5206\u5e03\u306e\u78ba\u8a8d\n\u6b21\u306b\u5404\u5909\u6570\u306eviolinplot\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u898b\u307e\u3059\uff0e\n\u3053\u308c\u306b\u3088\u308a\uff0c\u500b\u3005\u306e\u5909\u6570\u304c\u72ec\u7acb\u3057\u3066\u3044\u3066\u5206\u5e03\u306e\u5f62\u304c\u7570\u306a\u308b\u5909\u6570\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u308c\u3070\uff0c\u30ca\u30a4\u30fc\u30d6\u30d9\u30a4\u30ba\u306e\u3088\u3046\u306a\u6761\u4ef6\u4ed8\u304d\u78ba\u7387\u304b\u3089\u4e0d\u826f\u54c1\u63a8\u5b9a\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n[\u5168\u3066\u306e\u5909\u6570\u3092\u53ef\u8996\u5316\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u51fa\u529b\u3059\u308b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9](http://qiita.com/TomHortons/items/e8a7cea90226bd5ed32f)\u3092\u4f5c\u6210\u3057\u305f\u306e\u3067(4.1)\uff0c\u3088\u3051\u308c\u3070\u305d\u3061\u3089\u3082\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\n\n```\nBATCH_SIZE = 5\ntrain_batch =[pd.melt(train[train.columns[batch: batch + BATCH_SIZE].append(np.array(['Response']))], \n                      id_vars = 'Response', value_vars = feature_names[batch: batch + BATCH_SIZE])\n              for batch in list(range(0, train.shape[1] - 1, BATCH_SIZE))]\nFIGSIZE = (12,16)\n_, axs = plt.subplots(len(train_batch), figsize = FIGSIZE)\nplt.suptitle('Univariate distributions')\nfor data, ax in zip(train_batch, axs):\n    sns.violinplot(x = 'variable',  y = 'value', hue = 'Response', data = data, ax = ax, split =True)\n```\n\n\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u305f\u7d50\u679c\u304c\u3053\u3061\u3089\uff0e\n\n![__results___13_0 (1).png](https://qiita-image-store.s3.amazonaws.com/0/72093/22c727fe-e10c-7397-f84e-d4b6f7aadf2f.png)\n\n\u6b20\u640d\u5024\u306e\u5272\u5408\u304c\u5927\u304d\u3059\u304e\u3066\uff0c\u5206\u5e03\u306e\u5f62\u304c\u9055\u3046\u306e\u304b\uff0c\u7247\u65b9\u306e\u30b5\u30f3\u30d7\u30eb\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3060\u3051\u306a\u306e\u304b\uff0c\u3044\u307e\u3044\u3061\u5224\u65ad\u304c\u3067\u304d\u307e\u305b\u3093\uff0e\n\u305d\u3053\u3067\uff0c\u3053\u308c\u308920\u500b\u306e\u5909\u6570\u3092\u6b20\u640d\u5024\u306e\u5272\u5408\u3067\u53ef\u8996\u5316\u3057\u3066\u898b\u307e\u3059\uff0e\n\n```\nnon_missing = pd.DataFrame(pd.concat([(X_neg.count()/X_neg.shape[0]).to_frame('negative samples'),\n                                      (X_pos.count()/X_pos.shape[0]).to_frame('positive samples'),  \n                                      ], \n                       axis = 1))\nnon_missing_sort = non_missing.sort_values(['negative samples'])\nnon_missing_sort.plot.barh(title = 'Proportion of non-missing values', figsize = FIGSIZE)\nplt.gca().invert_yaxis()\n```\n\n\u7d50\u679c\u304c\u3053\u3061\u3089\uff0e\n\u7d04\u534a\u5206\u306e\u5909\u6570\u306b\u95a2\u3057\u3066\u306f\uff0c50%\u4ee5\u4e0a\u304c\u6b20\u640d\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\uff0e\n\n![__results___15_0.png](https://qiita-image-store.s3.amazonaws.com/0/72093/ba30d010-7fa7-a935-0a28-f7cbad2e254f.png)\n\n## 4.3. \u76f8\u95a2\u4fc2\u6570\u304b\u3089\u65b0\u305f\u306a\u7279\u5fb4\u91cf\u3092\u63a2\u3059\nviolinplot\u3067\u306f\u72ec\u7acb\u5909\u6570\u3068\u3057\u3066\u305d\u308c\u305e\u308c\u306e\u5909\u6570\u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3057\u305f\uff0e\n\u6b21\u306b\u76f8\u95a2\u95a2\u4fc2\u3068\u3044\u3046\u70b9\u304b\u3089\u7279\u5fb4\u91cf\u3092\u63a2\u3057\u3066\u3044\u304d\u307e\u3059\uff0e\n\u30e9\u30d9\u30eb\u3067\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\uff0c[\u76f8\u95a2\u95a2\u4fc2\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7](http://qiita.com/TomHortons/items/a9e1eddc54a8cecaf797)\u3092\u4f5c\u6210\u3057\u307e\u3059\uff0e\n\n\n```\nFIGSIZE = (13,4)\n_, (ax1, ax2) = plt.subplots(1,2, figsize = FIGSIZE)\nMIN_PERIODS = 100\n\ntriang_mask = np.zeros((X_pos.shape[1], X_pos.shape[1]))\ntriang_mask[np.triu_indices_from(triang_mask)] = True\n\nax1.set_title('Negative Class')\nsns.heatmap(X_neg.corr(min_periods = MIN_PERIODS), mask = triang_mask, square=True,  ax = ax1)\n\nax2.set_title('Positive Class')\nsns.heatmap(X_pos.corr(min_periods = MIN_PERIODS), mask = triang_mask, square=True,  ax = ax2)\n```\n![__results___18_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/0e3d1445-1a28-2fd4-8a83-9134ae3eca82.png)\n\n\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3057\u305f\u6642\u306b\u76f8\u95a2\u95a2\u4fc2\u304c\u5d29\u308c\u305f\u5909\u6570\u3092\u63a2\u3057\u307e\u3059\uff0e\n\u3053\u3053\u3067\u306f\u5358\u7d14\u306b\u5dee\u5206\u3092\u6c42\u3081\u3066\u307e\u3059\u304c\uff0c\u3053\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u672c\u5f53\u306b\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u306f\u5c11\u3057\u7591\u554f\u3067\u3059\uff0e\n\n```\nsns.heatmap(X_pos.corr(min_periods = MIN_PERIODS) -X_neg.corr(min_periods = MIN_PERIODS), \n             mask = triang_mask, square=True)\n```\n\n![__results___20_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/acce4279-a38b-9ca3-c2fd-b05cc528cfc1.png)\n\n\u4e0d\u826f\u54c1\u304c\u767a\u751f\u3059\u308b\u3068\uff0c\u76f8\u95a2\u95a2\u4fc2\u304c\u5d29\u308c\u308b\u5909\u6570\u306e\u7d44\u307f\u5408\u308f\u305b\u304c\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3057\u305f\uff0e\n\u3053\u308c\u3089\u3092PCA\u3067\u5727\u7e2e\u3059\u308b\u3068\uff0c\u826f\u3044\u5909\u6570\u304c\u3067\u304d\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n\n\u7279\u5fb4\u91cf\u3068\u3044\u3046\u610f\u5473\u3067\u306f\uff0c\u6b20\u640d\u5024\u305d\u306e\u3082\u306e\u304c\u65b0\u305f\u306a\u60c5\u5831\u306b\u306a\u308b\u53ef\u80fd\u6027\u3082\u3042\u308a\u307e\u3059\uff0e\n\u540c\u3058\u624b\u9806\u3067\uff0c\u30e9\u30d9\u30eb\u3054\u3068\u306b\u6b20\u640d\u5024\u3092\u5206\u5272\u3057\uff0c\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\uff0e\n\n```\nnan_pos, nan_neg = np.isnan(X_pos), np.isnan(X_neg)\n\ntriang_mask = np.zeros((X_pos.shape[1], X_pos.shape[1]))\ntriang_mask[np.triu_indices_from(triang_mask)] = True\n\nFIGSIZE = (13,4)\n_, (ax1, ax2) = plt.subplots(1,2, figsize = FIGSIZE)\nMIN_PERIODS = 100\n\nax1.set_title('Negative Class')\nsns.heatmap(nan_neg.corr(),   square=True, mask = triang_mask, ax = ax1)\n\nax2.set_title('Positive Class')\nsns.heatmap(nan_pos.corr(), square=True, mask = triang_mask,  ax = ax2)\n```\n\n![__results___22_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/41974722-e9a8-595b-4fc5-b39833b1db72.png)\n\n\u540c\u3058\u304f\u4fc2\u6570\u306e\u5dee\u5206\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u4e0d\u826f\u54c1\u767a\u751f\u6642\u306b\u6b20\u640d\u5024\u306e\u767a\u751f\u983b\u5ea6\u304c\u5909\u5316\u3059\u308b\u5909\u6570\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\uff0e\n\n```\nsns.heatmap(nan_neg.corr() - nan_pos.corr(), mask = triang_mask, square=True)\n```\n\n![__results___24_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/238121f9-5684-b75e-5b5a-55738126b11c.png)\n\n# 5. t-SNE\u3092\u7528\u3044\u305f\u6b20\u640d\u5024\u306e\u5727\u7e2e\u3068\u53ef\u8996\u5316 (R\u3092\u4f7f\u7528)\n[\u3053\u3061\u3089\u306e\u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3](https://www.kaggle.com/c/bosch-production-line-performance/forums/t/23067/numeric-features-2-d-missing-patterns-t-sne)\u3092\u53c2\u8003\u306b\uff0c\u6570\u5024\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u5024\u3092[t-SNE](http://qiita.com/TomHortons/items/2064870af3f3f7f2b209)\u3092\u7528\u3044\u3066\u591a\u6b21\u5143\u30c7\u30fc\u30bf\u304b\u3089\uff12\u6b21\u5143\u3078\u843d\u3068\u3057\u8fbc\u307f\u307e\u3059\uff0e\n\u4f7f\u7528\u8a00\u8a9e\u306fR\u3067\u3059\uff0e\n\u5168\u3066\u306e\u5909\u6570\u306e\u6b20\u640d\u5024(NAN to 0, present to 1)\u3092\uff0c\u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u304b\u3089\u76f8\u95a2\u4fc2\u6570\u3092\u6c42\u3081\u307e\u3059\uff0e\u8a08\u7b97\u3057\u305f\u7d50\u679c\u304czip\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u30ea\u30f3\u30af\u5148\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\uff0e(cor_train_numeric.csv)\n\u3053\u3061\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u3044\u3066t-SNE\u3059\u308b\u30b3\u30fc\u30c9\uff0c\u305d\u306e\u7d50\u679c\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\nt-SNE\u306f\u512a\u308c\u305f\u6b21\u5143\u5727\u7e2e\u624b\u6cd5\u3067\u3059\u304c\uff0c\u4eca\u56de\u306e\u30b3\u30f3\u30da\u3067\u306f\u3053\u3061\u3089\u306e\u7d50\u679c\u306f\u7279\u306b\u6709\u52b9\u6d3b\u7528\u3055\u308c\u3066\u3044\u307e\u305b\u3093\uff0e\n\n```\nlibrary(data.table)\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(ggrepel)\ncor_out <- as.matrix(fread(\"cor_train_numeric.csv\", header = TRUE, sep = \",\"))\ngc(verbose = FALSE)\nset.seed(78)\ntsne_model <- Rtsne(data.frame(cor_out),\n                    dims = 2,\n                    #initial_dims = 50,\n                    initial_dims = ncol(cor_out),\n                    perplexity = 322, #floor((ncol(cor_out)-1)/3)\n                    theta = 0.00,\n                    check_duplicates = FALSE,\n                    pca = FALSE,\n                    max_iter = 1350,\n                    verbose = TRUE,\n                    is_distance = FALSE)\ncorMatrix_out <- as.data.frame(tsne_model$Y)\n    cor_kmeans <- kmeans(corMatrix_out, centers = 5, iter.max = 10, nstart = 3)\n    corMatrix_outclust <- as.factor(c(cor_kmeans$cluster[1:968], 6))\ncorMatrix_names <- colnames(cor_out)\nggplot(corMatrix_out, aes(x = V1, y = V2, color = corMatrix_outclust, shape = corMatrix_outclust)) + geom_point(size = 2.5) + geom_rug() + stat_ellipse(type = \"norm\") + ggtitle(\"T-SNE of Features\") + xlab(\"X\") + ylab(\"Y\") + labs(color = \"Cluster\", shape = \"Cluster\") + geom_text_repel(aes(x = V1, y = V2, label = corMatrix_names), size = 2.8)\n```\n\n![TSNE_features_numeric.png](https://qiita-image-store.s3.amazonaws.com/0/72093/8f5f8f9e-8f29-39bc-e997-b6ad778dc4d4.png)\n\n\nR\u3067\u6b20\u640d\u5024\u306e\u30d0\u30a4\u30ca\u30ea\u5316\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u304c\u3053\u3061\u3089\u3067\u3059\uff0e\n\u5c11\u306a\u304f\u3068\u308216GB\u306e\u30e1\u30e2\u30ea\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\uff0e\n\n```\nlibrary(propagate)\nfor (i in 1:969) {\n  cat(\"\\rStep \", i, \"\\n\", sep = \"\")\n  train_data[!is.na(train_data[, i]), i] <- 1\n  train_data[is.na(train_data[, i]), i] <- 0\n  if (i %% 10) {gc(verbose = FALSE)}\n}\ngc(verbose = FALSE)\n\ncor_table <- bigcor(train_data, fun = \"cor\", size = 194, verbose = TRUE)\n```\n\n\n\n# 6. \u88fd\u9020\u30e9\u30a4\u30f3\u306e\u6709\u5411\u30b0\u30e9\u30d5\u8868\u8a18 (R\u3092\u4f7f\u7528)\n\u3053\u3061\u3089\u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u306fR\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\uff0e\n[GGplot](https://briatte.github.io/ggnet/)\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3067\uff0c\u5927\u5909\u6d17\u7df4\u3055\u308c\u305f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u63cf\u3044\u3066\u3044\u307e\u3059\uff0e\n[\u512a\u52dd\u8005\u306b\u95a2\u3059\u308b\u8a18\u4e8b](http://qiita.com/TomHortons/items/51aa356455c0b6e8945a)\u3067\u3082\u66f8\u304d\u307e\u3057\u305f\u304c\uff0c\u3053\u306e\u88fd\u9020\u30e9\u30a4\u30f3\u306e\u95a2\u4fc2\u6027\u304b\u3089\u76f8\u95a2\u3092\u898b\u51fa\u305b\u308b\u304b\u3069\u3046\u304b\u304c\u4e00\u3064\u306e\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u3068\u306a\u308a\u307e\u3057\u305f\uff0e\n\n```\noptions(warn=-1) #surpress warnings\n\n#imports for data wrangling\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(dplyr)\n\n#get the data - nrows set to 10000 to keep runtime manageable.\n#one expansion option would be to select a time frame to visualized\ndtNum <- fread(\"input/train_numeric.csv\", select = c(\"Id\", \"Response\"),nrows = 10000)\ndtDate <- fread(\"input/train_date.csv\", nrows = 10000)\n\n#for each job identify which stations are passed through and for those store the minimum time\nfor (station in paste0(\"S\",0:51))\n{\n  cols = min(which((grepl(station,colnames(dtDate)))))\n  if(!cols==Inf){\n    dtDate[,paste0(station) := dtDate[,cols,with = FALSE]]\n  }\n}\n\n#limit data to only when passed through station X\ndtStations = dtDate[,!grepl(\"L\",colnames(dtDate)),with=F]\n\n#melt data to go from wide to long format\ndtStationsM = melt(dtStations,id.vars=c(\"Id\"))\n\n#join with numeric to have Response\ndtStationsM %>%\n  left_join(dtNum, by = \"Id\") -> dtStationsM\n\n#remove NA entries - these are plentiful as after melting each station-job combination has its own row\ndtStationsM %>%\n  filter(!is.na(value)) -> dtStationsMFiltered\n\n#sort entries by ascending time\ndtStationsMFiltered %>%\n  arrange(value) -> dtStationsMFiltered\n\n#imports for plotting\nrequire(GGally)\nlibrary(network)\nlibrary(sna)\nlibrary(ggplot2)\n\n#plotting format\noptions(repr.plot.width=5, repr.plot.height=15)\n\n#for each row obtain the subsequent statoin\ndtStationsMFiltered %>%\n  group_by(Id) %>%\n  mutate(nextStation = lead(variable)) -> edgelistsComplete\n\n#for each id find the first node to be entered \nedgelistsComplete %>%\n  group_by(Id) %>%\n  filter(!(variable %in% nextStation)) %>%\n  ungroup() %>%\n  select(variable,Response) -> startingPoints\n\n#prior to each starting point insert an edge from a common origin\ncolnames(startingPoints) = c(\"nextStation\",\"Response\")\nstartingPoints$variable = \"S\"\nedgelistsComplete %>%\n  select(variable,nextStation,Response) -> paths\n\n#for each id find the row where there is no next station (last station to be visited)\n#fill this station with Response value\npaths[is.na(nextStation)]$nextStation = paste(\"Result\",paths[is.na(nextStation)]$Response)\n\n#combine data\npaths = rbind(startingPoints,paths)\npaths = select(paths,-Response)\npaths$nextStation = as.character(paths$nextStation)\npaths$variable = as.character(paths$variable)\n\n#rename columns for plotting\ncolnames(paths) <- c(\"Target\",\"Source\")\n\n#flip columns in a costly way because ggnet is a little dumb and I am lazy\npathshelp = select(paths,Source)\npathshelp$Target = paths$Target\npaths=pathshelp\n\n#create network from edgelist\nnet = network(as.data.frame(na.omit(paths)),\n              directed = TRUE)\n\n#create a station-line mapping lookup\nLineStations = NULL\nfor (station in unique(paths$Source)){\n  if(station!=\"S\")\n  {\n    x=paste0(\"_\",station,\"_\")\n    y=head(colnames(dtDate)[which(grepl(x,colnames(dtDate)))],1)\n    y=strsplit(y,\"_\")[[1]][1]\n    LineStations = rbind(LineStations,data.frame(Node=station,Line=y))\n  }\n}\nLineStations = rbind(LineStations,data.frame(Node=c(\"Result 1\",\"Result 0\",\"S\"),Line=c(\"Outcome\",\"Outcome\",\"START\")))\n\n#merge station-line mapping into graph for coloring purposes\nx = data.frame(Node = network.vertex.names(net))\nx = merge(x, LineStations, by = \"Node\", sort = FALSE)$Line\nnet %v% \"line\" = as.character(x)\n\n#setup station coordinates analogue to @JohnM\nnodeCoordinates=data.frame(label=c(\"S\",\"S0\",\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\n                                   \"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\",\n                                   \"S14\",\"S15\",\"S16\",\"S17\",\"S18\",\"S19\",\n                                   \"S20\",\"S21\",\"S22\",\"S23\",\"S24\",\"S25\",\n                                   \"S26\",\"S27\",\"S28\",\"S29\",\"S30\",\"S31\",\n                                   \"S32\",\"S33\",\"S34\",\"S35\",\"S36\",\"S37\",\n                                   \"S38\",\"S39\",\"S40\",\"S41\",\"S43\",\n                                   \"S44\",\"S45\",\"S47\",\"S48\",\"S49\",\n                                   \"S50\",\"S51\",\"Result 0\",\"Result 1\"),\n                           y=c(0,\n                               1,2,3,3,4,4,5,5,6,7,7,7,\n                               1,2,3,3,4,4,5,5,6,7,7,7,\n                               6,6,7,7,7,\n                               8,9,10,10,10,11,11,12,13,14,\n                               8,9,10,11,11,12,13,14,15,15,16,\n                               17,17),\n                           x=c(5,\n                               9,9,10,8,10,8,10,8,7,10,9,8,\n                               5,5,6,4,6,4,6,4,5,6,5,4,\n                               2,0,2,1,0,\n                               7,7,8,7,6,8,6,7,7,7,\n                               3,3,3,4,2,3,3,3,4,2,3,\n                               7,3))\n\nnodeCoordinates$y = -3 * nodeCoordinates$y\n\n#setup initial plot\nnetwork = ggnet2(net)\n\n#grab node list from initial plot and attach coordinates\nnetCoordinates = select(network$data,label)\nnetCoordinates = left_join(netCoordinates,nodeCoordinates,by = \"label\")\nnetCoordinates = as.matrix(select(netCoordinates,x,y))\n\n#setup plot with manual layout\nnetwork = ggnet2(net,\n                 alpha = 0.75, size = \"indegree\",\n                 label = T, size.cut = 4,\n                 color = \"line\",palette = \"Set1\",\n                 mode = netCoordinates,\n                 edge.alpha = 0.5, edge.size = 1,\n                 legend.position = \"bottom\")\n\n\n#output plot on graphics device\nprint(network)\n```\n\n\u5b9f\u884c\u3059\u308b\u3068\u3053\u3093\u306a\u611f\u3058\n\n![__results___1_1.png](https://qiita-image-store.s3.amazonaws.com/0/72093/39464c17-5c81-1aff-447b-03bdd7ac8fcf.png)\n\n\u5185\u8a33\u3092\u898b\u3066\u307f\u307e\u3059\uff0e\n\n```\n#obtain summary statistic of number of edges on each station pair\npathshelp %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(-count) %>%\n    print(n=10)\n\npathshelp %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=10)\n\npathshelp %>%\n    filter(Target==\"Result 1\") %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=20)\n\npathshelp %>%\n    filter(Target==\"Result 0\") %>%\n    mutate(stationpair=paste0(Source,\"->\",Target)) %>%\n    group_by(stationpair) %>%\n    summarize(count=n()) %>%\n    arrange(count) %>%\n    print(n=20)\n```\n\n\u5b9f\u884c\u7d50\u679c\u306f\u6b21\u306e\u901a\u308a\u3067\u3059\uff0e\n\u3042\u308b\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u5225\u306e\u30b9\u30c6\u30fc\u30b7\u30e7\u30f3\u307e\u3067\uff0c\u3069\u308c\u3060\u3051\u306e\u983b\u5ea6\u3067\u9077\u79fb\u3057\u3066\u3044\u308b\u304b\uff0c\u898b\u3048\u3066\u304d\u307e\u3059\uff0e\n\n```\n# tbl_dt [162 \u00d7 2]\n     stationpair count\n           <chr> <int>\n1       S29->S30  9452\n2       S33->S34  9421\n3  S37->Result 0  9211\n4       S30->S33  8977\n5          S->S0  5733\n6         S0->S1  5726\n7       S36->S37  4827\n8       S34->S36  4801\n9       S35->S37  4646\n10      S34->S35  4635\n# ... with 152 more rows\nSource: local data table [162 x 2]\n\n# tbl_dt [162 \u00d7 2]\n   stationpair count\n         <chr> <int>\n1       S->S30     1\n2     S22->S23     1\n3      S6->S10     1\n4     S10->S40     1\n5     S16->S17     1\n6     S28->S30     1\n7     S21->S23     1\n8       S2->S3     1\n9     S24->S25     1\n10      S4->S5     1\n# ... with 152 more rows\nSource: local data table [3 x 2]\n\n# tbl_dt [3 \u00d7 2]\n    stationpair count\n          <chr> <int>\n1 S51->Result 1     2\n2 S38->Result 1     4\n3 S37->Result 1    47\nSource: local data table [6 x 2]\n\n# tbl_dt [6 \u00d7 2]\n    stationpair count\n          <chr> <int>\n1 S50->Result 0     1\n2 S35->Result 0     3\n3 S36->Result 0     3\n4 S38->Result 0   227\n5 S51->Result 0   499\n6 S37->Result 0  9211\n```\n\n\u3053\u3053\u3067\u3053\u308c\u4ee5\u4e0a\u66f8\u304f\u3068\uff0c\u30dc\u30ea\u30e5\u30fc\u30e0\u304c\u591a\u3059\u304e\u308b\u3068\u601d\u3063\u305f\u306e\u3067\uff0c[\u5225\u306e\u8a18\u4e8b](http://qiita.com/TomHortons/items/8bc0117bdeac4bb33e1c)\u306b\u3066\u3053\u3061\u3089\u306e\u30d1\u30b9\u306b\u3064\u3044\u3066\u6df1\u6398\u308a\u3057\u3066\u3044\u307e\u3059\uff0e\n\u5404\u30d1\u30b9\u306e\u9077\u79fb\u91cf\u306e\u53ef\u8996\u5316\uff0cResponse\u3054\u3068\u306e\u6bd4\u8f03\uff0cPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308bNetworkX\u3092\u4f7f\u7528\u3057\u305f\u5834\u5408\uff0c\u306e\u767a\u5c55\u7684\u5185\u5bb9\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u3066\u3042\u308b\u306e\u3067\uff0c\u3088\u3051\u308c\u3070\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\n", "tags": ["Kaggle", "Python", "\u6a5f\u68b0\u5b66\u7fd2", "analytics", "\u30c7\u30fc\u30bf\u5206\u6790"]}