{"context": "Spark\u3067Dataset\u3092\u4f5c\u6210\u3059\u308b\u969b\u3001JSON\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304b\u3089\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3092case class\u3067\u6307\u5b9a\u3059\u308b\u3068\u3001\u6574\u6570\u304c\u6a19\u6e96\u3067Long\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u3044\u3063\u305f\u3053\u3068\u304c\u539f\u56e0\u3067\u4f8b\u5916\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308b\u3002\nimport org.apache.spark.sql.SparkSession\n\nval ss = SparkSession.builder.getOrCreate\nval rdd = sc.parallelize(Seq(\"\"\"{\"i\":1}\"\"\"))\n\nval ds = ss.read.json(rdd)\nds.schema\n// => org.apache.spark.sql.types.StructType = StructType(StructField(i,LongType,true))\n\ncase class C(i: Int)\nds.as[C]\n// => org.apache.spark.sql.AnalysisException: Cannot up cast `i` from bigint to int as it may truncate\n//    The type path of the target object is:\n//    - field (class: \"scala.Int\", name: \"i\")\n//    - root class: \"C\"\n//    You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object;\n//    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:2016)\n//    ...\n\ncase class D(i: Long)\nds.as[D]\n// => org.apache.spark.sql.Dataset[D] = [i: bigint]\n\n\u6b21\u306e\u3088\u3046\u306b\u4e88\u3081\u30b9\u30ad\u30fc\u30de\u3092\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u3068\u3001\u6700\u521d\u304b\u3089\u30e2\u30c7\u30eb\u5b9a\u7fa9\u306b\u5f93\u3063\u3066JSON\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304f\u308c\u308b\u3002\ncase class C(i: Int)\nval schema = ScalaReflection.schemaFor[C].dataType.asInstanceOf[StructType]\n\nval ds = ss.read.schema(schema).json(rdd)\nds.schema\n// => org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,false))\n\nds.as[C]\n// => org.apache.spark.sql.Dataset[C] = [i: int]\n\n\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3054\u3068\u306b\u30b9\u30ad\u30fc\u30de\u3092\u4f5c\u308b\u306e\u304c\u9762\u5012\u306a\u3089\u3001\u6b21\u306e\u3088\u3046\u306a\u30e1\u30bd\u30c3\u30c9\u3092\u4f5c\u3063\u3066\u304a\u304f\u3068\u4fbf\u5229\u3002\ndef readJsonAs[Model <: Product : TypeTag](rdd: RDD[String])(implicit ss: SparkSession): Dataset[Model] = {\n  val schema = ScalaReflection.schemaFor[Model].dataType.asInstanceOf[StructType]\n  ss.read.schema(schema).json(rdd).as[Model]\n}\n\nval ds = readJsonAs[C](rdd)\n\nSpark\u3067Dataset\u3092\u4f5c\u6210\u3059\u308b\u969b\u3001JSON\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304b\u3089\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3092case class\u3067\u6307\u5b9a\u3059\u308b\u3068\u3001\u6574\u6570\u304c\u6a19\u6e96\u3067Long\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u3044\u3063\u305f\u3053\u3068\u304c\u539f\u56e0\u3067\u4f8b\u5916\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u304c\u3042\u308b\u3002\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\nval ss = SparkSession.builder.getOrCreate\nval rdd = sc.parallelize(Seq(\"\"\"{\"i\":1}\"\"\"))\n\nval ds = ss.read.json(rdd)\nds.schema\n// => org.apache.spark.sql.types.StructType = StructType(StructField(i,LongType,true))\n\ncase class C(i: Int)\nds.as[C]\n// => org.apache.spark.sql.AnalysisException: Cannot up cast `i` from bigint to int as it may truncate\n//    The type path of the target object is:\n//    - field (class: \"scala.Int\", name: \"i\")\n//    - root class: \"C\"\n//    You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object;\n//    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:2016)\n//    ...\n\ncase class D(i: Long)\nds.as[D]\n// => org.apache.spark.sql.Dataset[D] = [i: bigint]\n```\n\n\u6b21\u306e\u3088\u3046\u306b\u4e88\u3081\u30b9\u30ad\u30fc\u30de\u3092\u8a2d\u5b9a\u3057\u3066\u304a\u304f\u3068\u3001\u6700\u521d\u304b\u3089\u30e2\u30c7\u30eb\u5b9a\u7fa9\u306b\u5f93\u3063\u3066JSON\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304f\u308c\u308b\u3002\n\n```scala\ncase class C(i: Int)\nval schema = ScalaReflection.schemaFor[C].dataType.asInstanceOf[StructType]\n\nval ds = ss.read.schema(schema).json(rdd)\nds.schema\n// => org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,false))\n\nds.as[C]\n// => org.apache.spark.sql.Dataset[C] = [i: int]\n```\n\n\u30e2\u30c7\u30eb\u5b9a\u7fa9\u3054\u3068\u306b\u30b9\u30ad\u30fc\u30de\u3092\u4f5c\u308b\u306e\u304c\u9762\u5012\u306a\u3089\u3001\u6b21\u306e\u3088\u3046\u306a\u30e1\u30bd\u30c3\u30c9\u3092\u4f5c\u3063\u3066\u304a\u304f\u3068\u4fbf\u5229\u3002\n\n```scala\ndef readJsonAs[Model <: Product : TypeTag](rdd: RDD[String])(implicit ss: SparkSession): Dataset[Model] = {\n  val schema = ScalaReflection.schemaFor[Model].dataType.asInstanceOf[StructType]\n  ss.read.schema(schema).json(rdd).as[Model]\n}\n\nval ds = readJsonAs[C](rdd)\n```\n", "tags": ["Spark"]}