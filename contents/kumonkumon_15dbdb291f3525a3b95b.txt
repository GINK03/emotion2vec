{"context": "(\u76ee\u6b21\u306f\u3053\u3061\u3089)\n\n\u306f\u3058\u3081\u306b\n\u524d\u56de\u306e\u8a18\u4e8b \u3067\u306f\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u3064\u3044\u3066\u6271\u3063\u305f\u3002\u3053\u308c\u306f\u3001 TensorFlow\u306eTutorial\u306eMNIST For ML Beginners \u307b\u307c\u305d\u306e\u307e\u307e\u306a\u306e\u3067\u5168\u304f\u9762\u767d\u304f\u306a\u3044\u306e\u3067\u5c11\u3057\u62e1\u5f35\u3057\u3066\u307f\u308b\u3002\nMNIST For ML Beginners \u3067\u306f\u3001\n\nThis should be about 91%.\nIs that good? Well, not really. In fact, it's pretty bad. This is because we're using a very simple model. With some small changes, we can get to 97%\n\n\u300c\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u300191%\u304f\u3089\u3044\u306e\u7cbe\u5ea6\u3067\u3001\u3044\u3044\u7d50\u679c\u3058\u3083\u306a\u3044\u3088\u3001\u5c11\u3057\u5909\u3048\u308c\u307097%\u304f\u3089\u3044\u3044\u304f\u3088\u3002\u300d\u3068\u3002\n\u305f\u3060\u3001\u3053\u306e\u3001\n\nWith some small changes\n\n\u304c\u306a\u3093\u306a\u306e\u304b\u306f\u66f8\u3044\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u3061\u3087\u3063\u3068\u4f55\u304b\u8a66\u3057\u3066\u307f\u308b\u3002\n\n\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08multilayer perceptron\uff09\n\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3078\u3068\u62e1\u5f35\u3057\u3066\u307f\u308b\u3002\n\u753b\u50cf\u306b\u9650\u3089\u305a\u3001\u4f55\u304b\u306e\u30c7\u30fc\u30bf\u3092\u8b58\u5225\u3059\u308b\u3068\u304d\u306b\u306f\u3001\u300c\u5165\u529b\u30c7\u30fc\u30bf\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u3066\u8b58\u5225\u5668\u306b\u304b\u3051\u308b\u300d\u3068\u3044\u3046\u306e\u304c\u738b\u9053\u3002\n\u524d\u56de\u306e\u8a18\u4e8b \u3067\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3092\u76f4\u63a5\u8b58\u5225\u5668\u306b\u304b\u3051\u3066\u8a8d\u8b58\u3057\u3066\u3044\u305f\u306e\u3067\u3001\u5165\u529b\u30c7\u30fc\u30bf\u305d\u306e\u3082\u306e\u3001\u3059\u306a\u308f\u3061\u3001\u753b\u50cf\u306e\u753b\u7d20\u5024\u305d\u306e\u3082\u306e\u304c\u7279\u5fb4\u3068\u3057\u3066\u6271\u308f\u308c\u3066\u3044\u305f\u3053\u3068\u306b\u306a\u308b\u3002\n\u4eca\u56de\u306f\u3001\u7279\u5fb4\u62bd\u51fa\u90e8\u5206\u3092\u8ffd\u52a0\u3057\u3066\u307f\u308b\u3002\u7279\u5fb4\u62bd\u51fa\u3068\u3044\u3063\u3066\u3082\u3084\u308a\u65b9\u306f\u3044\u308d\u3044\u308d\u3042\u3063\u3066\u3001\n\n\u30a8\u30c3\u30b8\u62bd\u51fa\u30d5\u30a3\u30eb\u30bf\nSIFT\nSURF\netc...\n\n\u306a\u3069\u306a\u3069\u3001\u3001\u3067\u3082\u3001\u3053\u3053\u3067\u306f\u3001\u753b\u50cf\u306b\u3068\u3089\u308f\u308c\u306a\u3044\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u3092\u4e00\u3064\u8ffd\u52a0\u3059\u308b\u3053\u3068\u306b\u3059\u308b\u3002\u300c\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u300d\u3063\u3068\u8a00\u3063\u3066\u3082\u306a\u3093\u306e\u3053\u3068\u3084\u3089\u3060\u3051\u3069\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4e2d\u306b\u3001\u300c\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u300d\u3068\u3057\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u306d\u3058\u8fbc\u3093\u3067\u307f\u308b\u3002\n\u524d\u56de\u306e\u8a18\u4e8b \u3067\u306e\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306f\u3053\u3093\u306a\u611f\u3058\u3002\n\n\u3067\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u306d\u3058\u8fbc\u3080\u3002\n\nreadout layer\u3068input layer\u306e\u9593\u306b\u3001fully-conneced layer\uff08\u5168\u7d50\u5408\u5c64\uff09\u3068\u3044\u3046\u306e\u304c\u306d\u3058\u8fbc\u307e\u308c\u3066\u3044\u308b\u3002\u3067\u3001\u3088\u304f\u898b\u3066\u307f\u308b\u3068\u3001\u5165\u529b\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066ReLU\uff08\u30e9\u30f3\u30d7\u95a2\u6570\uff09\u3068\u3044\u3046\u6d3b\u6027\u5316\u95a2\u6570\u306b\u304b\u3051\u3066\u3001\u305d\u306e\u7d50\u679c\u3092readout layer\u306b\u5165\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\u3053\u308c\u306f\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306b\u30b9\u30c6\u30c3\u30d7\u95a2\u6570\u3092\u4f7f\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3067\u306f\u306a\u3044\u3051\u3069\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08multilayer perceptron\uff09\u3068\u3088\u3070\u308c\u3066\u3044\u308b\u3082\u306e\u3002\u3053\u3053\u307e\u3067\u304f\u308b\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3063\u307d\u304f\u306a\u3063\u305f\u3002\n\n\u30b3\u30fc\u30c9\nmnist_softmax_fc.py\n\nmnist_softmax_fc.py\nfrom helper import *\n\nIMAGE_WIDTH = 28\nIMAGE_HEIGHT = 28\nIMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT\nCATEGORY_NUM = 10\nLEARNING_RATE = 0.1\nFEATURE_DIM = 100\nTRAINING_LOOP = 20000\nBATCH_SIZE = 100\nSUMMARY_DIR = 'log_softmax_fc'\nSUMMARY_INTERVAL = 100\n\nmnist = input_data.read_data_sets('data', one_hot=True)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n\n    with tf.name_scope('fully-connected'):\n        W_fc = weight_variable([IMAGE_SIZE, FEATURE_DIM], name='weight_fc')\n        b_fc = bias_variable([FEATURE_DIM], name='bias_fc')\n        h_fc = tf.nn.relu(tf.matmul(x, W_fc) + b_fc)\n\n    with tf.name_scope('readout'):\n        W = weight_variable([FEATURE_DIM, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.softmax(tf.matmul(h_fc, W) + b)\n\n    with tf.name_scope('optimize'):\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n\n    with tf.Session() as sess:\n        train_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/train', sess.graph)\n        test_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/test', sess.graph)\n\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            batch = mnist.train.next_batch(BATCH_SIZE)\n            sess.run(train_step, {x: batch[0], y_: batch[1]})\n\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([train_accuracy_summary]), {x: batch[0], y_: batch[1]})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: mnist.test.labels})\n                test_writer.add_summary(summary, i)\n\n\n\n\u30b3\u30fc\u30c9\u306e\u8aac\u660e\n\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3068\u7570\u306a\u308b\u90e8\u5206\u3092\u3002\n\n\u5168\u7d50\u5408\u5c64\n\u8ffd\u52a0\u3055\u308c\u305f\u5c64\u3002\u5165\u529b\u30c7\u30fc\u30bf\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066ReLU\u3068\u3044\u3046\u6d3b\u6027\u5316\u95a2\u6570\u3092\u901a\u3063\u3066\u3044\u308b\u3002FEATURE_DIM\u306f\u51fa\u529b\u3092\u4f55\u6b21\u5143\u306b\u3059\u308b\u304b\uff08\u4f55\u6b21\u5143\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b\u304b\uff09\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3002\n    with tf.name_scope('fully-connected'):\n        W_fc = weight_variable([IMAGE_SIZE, FEATURE_DIM], name='weight_fc')\n        b_fc = bias_variable([FEATURE_DIM], name='bias_fc')\n        h_fc = tf.nn.relu(tf.matmul(x, W_fc) + b_fc)\n\n\n\u51fa\u529b\u5c64\n\u5165\u529b\u304c\u5168\u7d50\u5408\u5c64\u306e\u51fa\u529b\u3068\u306a\u308b\u3088\u3046\u306b\u5909\u66f4\u3002\n    with tf.name_scope('readout'):\n        W = weight_variable([FEATURE_DIM, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.softmax(tf.matmul(h_fc, W) + b)\n\n\u4ee5\u4e0a\u3067\u3059\u3002\u5909\u66f4\u91cf\u306f\u975e\u5e38\u306b\u5c11\u306a\u3044\u3002\n\n\u7d50\u679c\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff08\u9752\u7dda\uff09\u3067\u306e\u8b58\u5225\u7387\u306f\u300197.8%\u7a0b\u5ea6\u3002\n\u524d\u56de\u306e\u8a18\u4e8b \u3067\u306e\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306e\u7d50\u679c\u304c\u300192.4%\u7a0b\u5ea6\u3060\u3063\u305f\u306e\u3067\u304b\u306a\u308a\u6539\u5584\u3057\u3066\u3044\u308b\u3002\n\n\n\u3042\u3068\u304c\u304d\n\u4eca\u56de\u306f\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u62e1\u5f35\u3057\u3066\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3057\u3066\u307f\u307e\u3057\u305f\u3002\u6b21\u56de\u306e\u8a18\u4e8b\u3067\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08Convolutional Neural Network\uff1aCNN\uff09\u306b\u8db3\u3092\u8e0f\u307f\u5165\u308c\u3066\u307f\u307e\u3059\u3002\n([\u76ee\u6b21\u306f\u3053\u3061\u3089](http://qiita.com/kumonkumon/items/6fd05963df92e9eec8c0))\n\n#\u306f\u3058\u3081\u306b\n[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/2a659075d55b7b49df5a) \u3067\u306f\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u3064\u3044\u3066\u6271\u3063\u305f\u3002\u3053\u308c\u306f\u3001 TensorFlow\u306eTutorial\u306e[MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html) \u307b\u307c\u305d\u306e\u307e\u307e\u306a\u306e\u3067\u5168\u304f\u9762\u767d\u304f\u306a\u3044\u306e\u3067\u5c11\u3057\u62e1\u5f35\u3057\u3066\u307f\u308b\u3002\n\n[MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html) \u3067\u306f\u3001\n\n> This should be about 91%.\n> \n> Is that good? Well, not really. In fact, it's pretty bad. This is because we're using a very simple model. With some small changes, we can get to 97%\n\n\u300c\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306f\u300191%\u304f\u3089\u3044\u306e\u7cbe\u5ea6\u3067\u3001\u3044\u3044\u7d50\u679c\u3058\u3083\u306a\u3044\u3088\u3001\u5c11\u3057\u5909\u3048\u308c\u307097%\u304f\u3089\u3044\u3044\u304f\u3088\u3002\u300d\u3068\u3002\n\n\u305f\u3060\u3001\u3053\u306e\u3001\n> With some small changes\n\n\u304c\u306a\u3093\u306a\u306e\u304b\u306f\u66f8\u3044\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u3061\u3087\u3063\u3068\u4f55\u304b\u8a66\u3057\u3066\u307f\u308b\u3002\n\n#\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08multilayer perceptron\uff09\n\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3078\u3068\u62e1\u5f35\u3057\u3066\u307f\u308b\u3002\n\n\u753b\u50cf\u306b\u9650\u3089\u305a\u3001\u4f55\u304b\u306e\u30c7\u30fc\u30bf\u3092\u8b58\u5225\u3059\u308b\u3068\u304d\u306b\u306f\u3001\u300c\u5165\u529b\u30c7\u30fc\u30bf\u304b\u3089\u7279\u5fb4\u62bd\u51fa\u3057\u3066\u8b58\u5225\u5668\u306b\u304b\u3051\u308b\u300d\u3068\u3044\u3046\u306e\u304c\u738b\u9053\u3002\n[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/2a659075d55b7b49df5a) \u3067\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u3092\u76f4\u63a5\u8b58\u5225\u5668\u306b\u304b\u3051\u3066\u8a8d\u8b58\u3057\u3066\u3044\u305f\u306e\u3067\u3001\u5165\u529b\u30c7\u30fc\u30bf\u305d\u306e\u3082\u306e\u3001\u3059\u306a\u308f\u3061\u3001\u753b\u50cf\u306e\u753b\u7d20\u5024\u305d\u306e\u3082\u306e\u304c\u7279\u5fb4\u3068\u3057\u3066\u6271\u308f\u308c\u3066\u3044\u305f\u3053\u3068\u306b\u306a\u308b\u3002\n\n\u4eca\u56de\u306f\u3001\u7279\u5fb4\u62bd\u51fa\u90e8\u5206\u3092\u8ffd\u52a0\u3057\u3066\u307f\u308b\u3002\u7279\u5fb4\u62bd\u51fa\u3068\u3044\u3063\u3066\u3082\u3084\u308a\u65b9\u306f\u3044\u308d\u3044\u308d\u3042\u3063\u3066\u3001\n\n* \u30a8\u30c3\u30b8\u62bd\u51fa\u30d5\u30a3\u30eb\u30bf\n* SIFT\n* SURF\n* etc...\n\n\u306a\u3069\u306a\u3069\u3001\u3001\u3067\u3082\u3001\u3053\u3053\u3067\u306f\u3001\u753b\u50cf\u306b\u3068\u3089\u308f\u308c\u306a\u3044\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u3092\u4e00\u3064\u8ffd\u52a0\u3059\u308b\u3053\u3068\u306b\u3059\u308b\u3002\u300c\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u300d\u3063\u3068\u8a00\u3063\u3066\u3082\u306a\u3093\u306e\u3053\u3068\u3084\u3089\u3060\u3051\u3069\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u4e2d\u306b\u3001\u300c\u7279\u5fb4\u62bd\u51fa\u7684\u306a\u5c64\u300d\u3068\u3057\u3066\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u306d\u3058\u8fbc\u3093\u3067\u307f\u308b\u3002\n\n[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/2a659075d55b7b49df5a) \u3067\u306e\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306f\u3053\u3093\u306a\u611f\u3058\u3002\n![multinomial logistic regression](https://qiita-image-store.s3.amazonaws.com/0/127038/fe9bffd6-d7c6-356a-b994-02d9536540ad.png)\n\n\u3067\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u306d\u3058\u8fbc\u3080\u3002\n![png (6).png](https://qiita-image-store.s3.amazonaws.com/0/127038/f6d65e63-b749-c354-52c6-4981e38c0220.png)\n\nreadout layer\u3068input layer\u306e\u9593\u306b\u3001fully-conneced layer\uff08\u5168\u7d50\u5408\u5c64\uff09\u3068\u3044\u3046\u306e\u304c\u306d\u3058\u8fbc\u307e\u308c\u3066\u3044\u308b\u3002\u3067\u3001\u3088\u304f\u898b\u3066\u307f\u308b\u3068\u3001\u5165\u529b\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066ReLU\uff08\u30e9\u30f3\u30d7\u95a2\u6570\uff09\u3068\u3044\u3046\u6d3b\u6027\u5316\u95a2\u6570\u306b\u304b\u3051\u3066\u3001\u305d\u306e\u7d50\u679c\u3092readout layer\u306b\u5165\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\n\u3053\u308c\u306f\u3001\u6d3b\u6027\u5316\u95a2\u6570\u306b\u30b9\u30c6\u30c3\u30d7\u95a2\u6570\u3092\u4f7f\u3063\u3066\u3044\u306a\u3044\u306e\u3067\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u3067\u306f\u306a\u3044\u3051\u3069\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff08multilayer perceptron\uff09\u3068\u3088\u3070\u308c\u3066\u3044\u308b\u3082\u306e\u3002\u3053\u3053\u307e\u3067\u304f\u308b\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3063\u307d\u304f\u306a\u3063\u305f\u3002\n\n##\u30b3\u30fc\u30c9\n[mnist_softmax_fc.py](https://github.com/kumon/DeepLearningExercise/blob/master/src/tensorflow/mnist_softmax_fc.py)\n\n```py:mnist_softmax_fc.py\nfrom helper import *\n\nIMAGE_WIDTH = 28\nIMAGE_HEIGHT = 28\nIMAGE_SIZE = IMAGE_WIDTH * IMAGE_HEIGHT\nCATEGORY_NUM = 10\nLEARNING_RATE = 0.1\nFEATURE_DIM = 100\nTRAINING_LOOP = 20000\nBATCH_SIZE = 100\nSUMMARY_DIR = 'log_softmax_fc'\nSUMMARY_INTERVAL = 100\n\nmnist = input_data.read_data_sets('data', one_hot=True)\n\nwith tf.Graph().as_default():\n    with tf.name_scope('input'):\n        y_ = tf.placeholder(tf.float32, [None, CATEGORY_NUM], name='labels')\n        x = tf.placeholder(tf.float32, [None, IMAGE_SIZE], name='input_images')\n\n    with tf.name_scope('fully-connected'):\n        W_fc = weight_variable([IMAGE_SIZE, FEATURE_DIM], name='weight_fc')\n        b_fc = bias_variable([FEATURE_DIM], name='bias_fc')\n        h_fc = tf.nn.relu(tf.matmul(x, W_fc) + b_fc)\n\n    with tf.name_scope('readout'):\n        W = weight_variable([FEATURE_DIM, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.softmax(tf.matmul(h_fc, W) + b)\n\n    with tf.name_scope('optimize'):\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n        train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n\n    with tf.Session() as sess:\n        train_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/train', sess.graph)\n        test_writer = tf.train.SummaryWriter(SUMMARY_DIR + '/test', sess.graph)\n\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        train_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n        test_accuracy_summary = tf.scalar_summary('accuracy', accuracy)\n\n        sess.run(tf.initialize_all_variables())\n        for i in range(TRAINING_LOOP + 1):\n            batch = mnist.train.next_batch(BATCH_SIZE)\n            sess.run(train_step, {x: batch[0], y_: batch[1]})\n\n            if i % SUMMARY_INTERVAL == 0:\n                print('step %d' % i)\n                summary = sess.run(tf.merge_summary([train_accuracy_summary]), {x: batch[0], y_: batch[1]})\n                train_writer.add_summary(summary, i)\n                summary = sess.run(tf.merge_summary([test_accuracy_summary]), {x: mnist.test.images, y_: mnist.test.labels})\n                test_writer.add_summary(summary, i)\n```\n\n##\u30b3\u30fc\u30c9\u306e\u8aac\u660e\n\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3068\u7570\u306a\u308b\u90e8\u5206\u3092\u3002\n\n###\u5168\u7d50\u5408\u5c64\n\u8ffd\u52a0\u3055\u308c\u305f\u5c64\u3002\u5165\u529b\u30c7\u30fc\u30bf\u3092\u7dda\u5f62\u5909\u63db\u3057\u3066ReLU\u3068\u3044\u3046\u6d3b\u6027\u5316\u95a2\u6570\u3092\u901a\u3063\u3066\u3044\u308b\u3002`FEATURE_DIM`\u306f\u51fa\u529b\u3092\u4f55\u6b21\u5143\u306b\u3059\u308b\u304b\uff08\u4f55\u6b21\u5143\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b\u304b\uff09\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3002\n\n```py\n    with tf.name_scope('fully-connected'):\n        W_fc = weight_variable([IMAGE_SIZE, FEATURE_DIM], name='weight_fc')\n        b_fc = bias_variable([FEATURE_DIM], name='bias_fc')\n        h_fc = tf.nn.relu(tf.matmul(x, W_fc) + b_fc)\n```\n\n###\u51fa\u529b\u5c64\n\u5165\u529b\u304c\u5168\u7d50\u5408\u5c64\u306e\u51fa\u529b\u3068\u306a\u308b\u3088\u3046\u306b\u5909\u66f4\u3002\n\n```py\n    with tf.name_scope('readout'):\n        W = weight_variable([FEATURE_DIM, CATEGORY_NUM], name='weight')\n        b = bias_variable([CATEGORY_NUM], name='bias')\n        y = tf.nn.softmax(tf.matmul(h_fc, W) + b)\n```\n\n\u4ee5\u4e0a\u3067\u3059\u3002\u5909\u66f4\u91cf\u306f\u975e\u5e38\u306b\u5c11\u306a\u3044\u3002\n\n##\u7d50\u679c\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff08\u9752\u7dda\uff09\u3067\u306e\u8b58\u5225\u7387\u306f\u300197.8%\u7a0b\u5ea6\u3002\n[\u524d\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/2a659075d55b7b49df5a) \u3067\u306e\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u306e\u7d50\u679c\u304c\u300192.4%\u7a0b\u5ea6\u3060\u3063\u305f\u306e\u3067\u304b\u306a\u308a\u6539\u5584\u3057\u3066\u3044\u308b\u3002\n\n![result_softmax_fc](https://qiita-image-store.s3.amazonaws.com/0/127038/3797fe19-e0b4-3f7b-880e-bd16cad7e32f.png)\n\n#\u3042\u3068\u304c\u304d\n\u4eca\u56de\u306f\u3001\u591a\u9805\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u62e1\u5f35\u3057\u3066\u3001\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3057\u3066\u307f\u307e\u3057\u305f\u3002[\u6b21\u56de\u306e\u8a18\u4e8b](http://qiita.com/kumonkumon/items/ebeff45afc5743bb99d9)\u3067\u306f\u3001\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08Convolutional Neural Network\uff1aCNN\uff09\u306b\u8db3\u3092\u8e0f\u307f\u5165\u308c\u3066\u307f\u307e\u3059\u3002\n\n", "tags": ["TensorFlow", "DeepLearning", "CNN", "ConvolutionalNeuralNetworks", "\u6df1\u5c64\u5b66\u7fd2"]}