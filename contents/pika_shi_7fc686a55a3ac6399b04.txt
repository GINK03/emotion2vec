{"tags": ["hadoop", "Python"], "context": " More than 1 year has passed since last update.\nHadoop\u59cb\u3081\u305f\u3044\u3051\u3069Java\u3067\u66f8\u304f\u306e\u3081\u3093\u3069\u304f\u3055\u3044\u2026 \u3068\u3044\u3046\u4eba\u306e\u305f\u3081\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\uff0e\nHadoop\u306fJava\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u57fa\u672c\u7684\u306b\u306fMapper/Reducer\u3082Java\u3067\u8a18\u8ff0\u3059\u308b\u304c\uff0cHadoop\u306b\u306fHadoop Streaming\u3068\u3044\u3046\u6a5f\u80fd\u304c\u3042\u308a\uff0cUnix\u306e\u6a19\u6e96\u5165\u51fa\u529b\u3092\u4ecb\u3057\u3066\u30c7\u30fc\u30bf\u306e\u53d7\u3051\u6e21\u3057\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\n\u3053\u308c\u3092\u7528\u3044\u3066Python\u3067Mapper/Reducer\u3092\u66f8\u3044\u3066\u307f\u305f\uff0e\u3082\u3061\u308d\u3093Hadoop Streaming\u3092\u5229\u7528\u3059\u308c\u3070Python\u4ee5\u5916\u306e\u8a00\u8a9e\u3067\u3082\u66f8\u3051\u308b\uff0e\n\u4eca\u56de\u306fUbuntu\u4e0a\u306b\u64ec\u4f3c\u5206\u6563\u74b0\u5883\u3092\u69cb\u7bc9\u3057\u3066\u307f\u305f\uff0e\nUbuntu12.04 + Haadoop2.4.1\n\n\nHadoop\u306e\u74b0\u5883\u69cb\u7bc9\n\nJava\u304c\u306a\u3044\u5834\u5408\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n$ sudo apt-get update\n$ sudo apt-get install openjdk-7-jdk\n\n\nHadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n$ wget http://mirror.nexcess.net/apache/hadoop/common/hadoop-2.4.1/hadoop-2.4.1.tar.gz\n$ tar zxvf hadoop-2.4.1.tar.gz\n$ mv hadoop-2.4.1.tar.gz hadoop\n$ rm hadoop-2.4.1.tar.gz\n$ sudo mv hadoop /usr/local\n$ cd /usr/local/hadoop\n$ export PATH=$PATH:/usr/local/hadoop/bin #.zshrc\u306b\u66f8\u3044\u3066\u304a\u304f\u3068\u3088\u3044\n\n\u4ee5\u4e0b\u306e4\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\n$ vim etc/hadoop/core-site.xml\n\n\ncore-site.xml\n...\n<configuration>\n     <property>\n         <name>fs.default.name</name>\n         <value>hdfs://localhost:9000</value>\n     </property>\n</configuration>\n\n\n$ vim etc/hadoop/hdfs-site.xml\n\n\nhdfs-site.xml\n...\n<configuration>\n     <property>\n         <name>dfs.replication</name>\n         <value>1</value>\n     </property>\n</configuration>\n\n\n$ mv etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml\n$ vim etc/hadoop/mapred-site.xml\n\n\nmapred-site.xml\n...\n<configuration>\n     <property>\n         <name>dfs.replication</name>\n         <value>1</value>\n     </property>\n</configuration>\n\n\n$ vim etc/hadoop/hadoop-env.xml\n\n\nhadoop-env.xml\n...\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport HADOOP_INSTALL=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib\"\n...\n\n\n\n\u9375\u304c\u306a\u3044\u5834\u5408\u306f\u8ffd\u52a0\u3059\u308b\n\n$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n\n\n\u6700\u5f8c\u306bnamenode\u3092\u521d\u671f\u5316\u3057\uff0cHadoop\u3092\u8d77\u52d5\u3059\u308b\n\n$ hdfs namenode -format\n$ sbin/start-dfs.sh\n\n\nPython\u306b\u3088\u308bMapper/Reducer\u306e\u8a18\u8ff0\n\n\u4eca\u56de\u306f\uff0cHadoop\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3042\u308bWordCount\u3092Python\u3067\u8a18\u8ff0\u3059\u308b\n\u307e\u305a\uff0c\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u610f\u3059\u308b\n\n$ mkdir inputs\n$ echo \"a b b c c c\" > inputs/input.txt\n\n\nMapper\n$ vim mapper.py\n\n\nmapper.py\n#!/usr/bin/env python\n\nimport sys\n\nfor l in sys.stdin:\n    for word in l.strip().split(): print '{0}\\t1'.format(word)\n\n\n\nMapper\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u51fa\u529b\u3059\u308b\n\na    1\nb    1\nb    1\nc    1\nc    1\nc    1\n\n\nReducer\n$ vim reducer.py\n\n\nreducer.py\n#!/usr/bin/env python\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nimport sys\n\nwordcount_dict = defaultdict(int)\n\nfor l in sys.stdin:\n    word, count = line.strip().split('\\t')\n    wordcount_dict[word] += int(count)\n\nfor word, count in sorted(wordcount_dict.items(), key=itemgetter(0)):\n    print '{0}\\t{1}'.format(word, count)\n\n\n\nReducer\u306fMapper\u3067\u51fa\u529b\u3055\u308c\u305f\u305d\u308c\u305e\u308c\u306eword\u3092\u6570\u3048\u4e0a\u3052\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u51fa\u529b\u3059\u308b\n\na    1\nb    2\nc    3\n\n\nHadoop Streaming\u306b\u3088\u308b\u5b9f\u884c\n\n\u3044\u3088\u3044\u3088\u4e0a\u8a18\u306eMapper/Reducer\u3092Hadoop\u4e0a\u3067\u5b9f\u884c\u3059\u308b\n\u307e\u305a\uff0eHadoop Streaming\u306e\u305f\u3081\u306ejar\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\n\n$ wget http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.4.1/hadoop-streaming-2.4.1.jar\n\n\nHDFS\u4e0a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\uff0c\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u306e\u305b\u308b\n(\u30ed\u30fc\u30ab\u30eb\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3068HDFS\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3053\u3063\u3061\u3083\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u6ce8\u610f)\n\n$ hdfs dfs -mkdir /user\n$ hdfs dfs -mkdir /user/vagrant\n$ hdfs dfs -put inputs/input.txt /user/vagrant\n\n\n\u5b9f\u884c\u3059\u308b\u3068\u6307\u5b9a\u3057\u305f\u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u7d50\u679c\u304c\u683c\u7d0d\u3055\u308c\u308b\n\n$ hadoop jar hadoop-streaming-2.4.1.jar -mapper mapper.py -reducer reducer.py -input /user/vagrant/input.txt -output outputs\n$ hdfs dfs -cat /user/vagrant/outputs/part-00000\na    1\nb    2\nc    3\n\n>Hadoop\u59cb\u3081\u305f\u3044\u3051\u3069Java\u3067\u66f8\u304f\u306e\u3081\u3093\u3069\u304f\u3055\u3044\u2026 \u3068\u3044\u3046\u4eba\u306e\u305f\u3081\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\uff0e\n>\n>Hadoop\u306fJava\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u57fa\u672c\u7684\u306b\u306fMapper/Reducer\u3082Java\u3067\u8a18\u8ff0\u3059\u308b\u304c\uff0cHadoop\u306b\u306fHadoop Streaming\u3068\u3044\u3046\u6a5f\u80fd\u304c\u3042\u308a\uff0cUnix\u306e\u6a19\u6e96\u5165\u51fa\u529b\u3092\u4ecb\u3057\u3066\u30c7\u30fc\u30bf\u306e\u53d7\u3051\u6e21\u3057\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\uff0e\n>\u3053\u308c\u3092\u7528\u3044\u3066Python\u3067Mapper/Reducer\u3092\u66f8\u3044\u3066\u307f\u305f\uff0e\u3082\u3061\u308d\u3093Hadoop Streaming\u3092\u5229\u7528\u3059\u308c\u3070Python\u4ee5\u5916\u306e\u8a00\u8a9e\u3067\u3082\u66f8\u3051\u308b\uff0e\n>\n>\u4eca\u56de\u306fUbuntu\u4e0a\u306b\u64ec\u4f3c\u5206\u6563\u74b0\u5883\u3092\u69cb\u7bc9\u3057\u3066\u307f\u305f\uff0e\n>\n> Ubuntu12.04 + Haadoop2.4.1\n\n# Hadoop\u306e\u74b0\u5883\u69cb\u7bc9\n\n>Java\u304c\u306a\u3044\u5834\u5408\u306f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n```\n$ sudo apt-get update\n$ sudo apt-get install openjdk-7-jdk\n```\n\n>Hadoop\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n```\n$ wget http://mirror.nexcess.net/apache/hadoop/common/hadoop-2.4.1/hadoop-2.4.1.tar.gz\n$ tar zxvf hadoop-2.4.1.tar.gz\n$ mv hadoop-2.4.1.tar.gz hadoop\n$ rm hadoop-2.4.1.tar.gz\n$ sudo mv hadoop /usr/local\n$ cd /usr/local/hadoop\n$ export PATH=$PATH:/usr/local/hadoop/bin #.zshrc\u306b\u66f8\u3044\u3066\u304a\u304f\u3068\u3088\u3044\n```\n\n\u4ee5\u4e0b\u306e4\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\n\n```\n$ vim etc/hadoop/core-site.xml\n```\n\n```core-site.xml\n...\n<configuration>\n     <property>\n         <name>fs.default.name</name>\n         <value>hdfs://localhost:9000</value>\n     </property>\n</configuration>\n```\n\n```\n$ vim etc/hadoop/hdfs-site.xml\n```\n\n```hdfs-site.xml\n...\n<configuration>\n     <property>\n         <name>dfs.replication</name>\n         <value>1</value>\n     </property>\n</configuration>\n```\n\n```\n$ mv etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml\n$ vim etc/hadoop/mapred-site.xml\n```\n\n```mapred-site.xml\n...\n<configuration>\n     <property>\n         <name>dfs.replication</name>\n         <value>1</value>\n     </property>\n</configuration>\n```\n\n```\n$ vim etc/hadoop/hadoop-env.xml\n```\n\n```hadoop-env.xml\n...\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport HADOOP_INSTALL=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib\"\n...\n```\n>\u9375\u304c\u306a\u3044\u5834\u5408\u306f\u8ffd\u52a0\u3059\u308b\n\n```\n$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n```\n\n>\u6700\u5f8c\u306bnamenode\u3092\u521d\u671f\u5316\u3057\uff0cHadoop\u3092\u8d77\u52d5\u3059\u308b\n\n```\n$ hdfs namenode -format\n$ sbin/start-dfs.sh\n```\n\n# Python\u306b\u3088\u308bMapper/Reducer\u306e\u8a18\u8ff0\n\n> \u4eca\u56de\u306f\uff0cHadoop\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3067\u3042\u308b[WordCount](http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html \"WordCount\")\u3092Python\u3067\u8a18\u8ff0\u3059\u308b\n\n>\u307e\u305a\uff0c\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u7528\u610f\u3059\u308b\n\n```\n$ mkdir inputs\n$ echo \"a b b c c c\" > inputs/input.txt\n```\n\n## Mapper\n\n```\n$ vim mapper.py\n```\n\n```mapper.py\n#!/usr/bin/env python\n\nimport sys\n\nfor l in sys.stdin:\n    for word in l.strip().split(): print '{0}\\t1'.format(word)\n```\n\n>Mapper\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u51fa\u529b\u3059\u308b\n\n```\na    1\nb    1\nb    1\nc    1\nc    1\nc    1\n```\n\n## Reducer\n\n```\n$ vim reducer.py\n```\n\n```reducer.py\n#!/usr/bin/env python\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nimport sys\n\nwordcount_dict = defaultdict(int)\n\nfor l in sys.stdin:\n    word, count = line.strip().split('\\t')\n    wordcount_dict[word] += int(count)\n\nfor word, count in sorted(wordcount_dict.items(), key=itemgetter(0)):\n    print '{0}\\t{1}'.format(word, count)\n```\n\n>Reducer\u306fMapper\u3067\u51fa\u529b\u3055\u308c\u305f\u305d\u308c\u305e\u308c\u306eword\u3092\u6570\u3048\u4e0a\u3052\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u3092\u51fa\u529b\u3059\u308b\n\n```\na    1\nb    2\nc    3\n```\n\n# Hadoop Streaming\u306b\u3088\u308b\u5b9f\u884c\n>\u3044\u3088\u3044\u3088\u4e0a\u8a18\u306eMapper/Reducer\u3092Hadoop\u4e0a\u3067\u5b9f\u884c\u3059\u308b\n\n>\u307e\u305a\uff0eHadoop Streaming\u306e\u305f\u3081\u306ejar\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\n\n```\n$ wget http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.4.1/hadoop-streaming-2.4.1.jar\n```\n\n>HDFS\u4e0a\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\uff0c\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u306e\u305b\u308b\n>(\u30ed\u30fc\u30ab\u30eb\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3068HDFS\u4e0a\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u3053\u3063\u3061\u3083\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u6ce8\u610f)\n\n```\n$ hdfs dfs -mkdir /user\n$ hdfs dfs -mkdir /user/vagrant\n$ hdfs dfs -put inputs/input.txt /user/vagrant\n```\n\n>\u5b9f\u884c\u3059\u308b\u3068\u6307\u5b9a\u3057\u305f\u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u7d50\u679c\u304c\u683c\u7d0d\u3055\u308c\u308b\n\n```\n$ hadoop jar hadoop-streaming-2.4.1.jar -mapper mapper.py -reducer reducer.py -input /user/vagrant/input.txt -output outputs\n$ hdfs dfs -cat /user/vagrant/outputs/part-00000\na    1\nb    2\nc    3\n"}