{"context": " More than 1 year has passed since last update.\u5f62\u614b\u7d20\u89e3\u6790\u30a8\u30f3\u30b8\u30f3\u3068\u3057\u3066 Gomoku \u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\nWordCount.java\npackage org.holidayworking.hadoop.japanese;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\npublic class WordCount extends Configured implements Tool {\n\n    @Override\n    public int run(String[] args) throws Exception {\n        if (args.length != 2) {\n            System.err.println(\"Usage: WordCount <input path> <output path>\");\n            return -1;\n        }\n\n        Job job = new Job(getConf(), \"WordCount\");\n        job.setJarByClass(WordCount.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputKeyClass(IntWritable.class);\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    public static void main(String[] args) throws Exception {\n        System.exit(ToolRunner.run(new WordCount(), args));\n    }\n\n}\n\n\n\nWordCountMapper.java\npackage org.holidayworking.hadoop.japanese;\n\nimport java.io.IOException;\n\nimport net.reduls.gomoku.Morpheme;\nimport net.reduls.gomoku.Tagger;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nclass WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        String line = value.toString();\n        for (Morpheme m : Tagger.parse(line)) {\n            if (m.feature.startsWith(\"\u540d\u8a5e\")) {\n                context.write(new Text(m.surface), new IntWritable(1));\n            }\n        }\n    }\n\n}\n\n\n\nWordCountReducer.java\npackage org.holidayworking.hadoop.japanese;\n\nimport java.io.IOException;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nclass WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int count = 0;\n        for (IntWritable value : values) {\n            count += value.get();\n        }\n        context.write(key, new IntWritable(count));\n    }\n\n}\n\n\n\u5f62\u614b\u7d20\u89e3\u6790\u30a8\u30f3\u30b8\u30f3\u3068\u3057\u3066 [Gomoku](https://github.com/sile/gomoku) \u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\n\n```java:WordCount.java\npackage org.holidayworking.hadoop.japanese;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\npublic class WordCount extends Configured implements Tool {\n\n    @Override\n    public int run(String[] args) throws Exception {\n        if (args.length != 2) {\n            System.err.println(\"Usage: WordCount <input path> <output path>\");\n            return -1;\n        }\n\n        Job job = new Job(getConf(), \"WordCount\");\n        job.setJarByClass(WordCount.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.setMapperClass(WordCountMapper.class);\n        job.setReducerClass(WordCountReducer.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputKeyClass(IntWritable.class);\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    public static void main(String[] args) throws Exception {\n        System.exit(ToolRunner.run(new WordCount(), args));\n    }\n\n}\n```\n\n```java:WordCountMapper.java\npackage org.holidayworking.hadoop.japanese;\n\nimport java.io.IOException;\n\nimport net.reduls.gomoku.Morpheme;\nimport net.reduls.gomoku.Tagger;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nclass WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        String line = value.toString();\n        for (Morpheme m : Tagger.parse(line)) {\n            if (m.feature.startsWith(\"\u540d\u8a5e\")) {\n                context.write(new Text(m.surface), new IntWritable(1));\n            }\n        }\n    }\n\n}\n```\n\n```java:WordCountReducer.java\npackage org.holidayworking.hadoop.japanese;\n\nimport java.io.IOException;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nclass WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int count = 0;\n        for (IntWritable value : values) {\n            count += value.get();\n        }\n        context.write(key, new IntWritable(count));\n    }\n\n}\n```", "tags": ["Java", "hadoop"]}