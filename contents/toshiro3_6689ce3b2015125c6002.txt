{"context": "\n\n\u306f\u3058\u3081\u306b\nCDH3\u3067HDFS\u3092\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\u3059\u308b\u624b\u9806\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\n\u74b0\u5883\n\nCentOS 6.5\nCDH3\nJDK 1.6\n\n\nhadoop\u30af\u30e9\u30b9\u30bf\u306e\u69cb\u6210\n\n\n\n\u5f79\u5272\n\u30db\u30b9\u30c8\u540d\nIP\u30a2\u30c9\u30ec\u30b9\n\n\n\n\nmaster\nhadoop-master\n192.168.121.11\n\n\nslave\nhadoop-slave\n192.168.121.21\n\n\nslave\nhadoop-slave2\n192.168.121.22\n\n\nclient\nhadoop-client\n192.168.121.101\n\n\n\n\u203b CDH3\u3067\u306ehadoop\u306e\u30af\u30e9\u30b9\u30bf\u306e\u69cb\u7bc9\u65b9\u6cd5\u306f\u3001CDH3\u3067hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b \u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\nHDFS\u306e\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\n\nHDFS\u306e\u4f7f\u7528\u7387\u306e\u78ba\u8a8d\n\nhdfs \u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n$ sudo su - hdfs\n$ hadoop dfsadmin -report\nConfigured Capacity: 14184988672 (13.21 GB)\nPresent Capacity: 9819820032 (9.15 GB)\nDFS Remaining: 9819365376 (9.14 GB)\nDFS Used: 454656 (444 KB)\nDFS Used%: 0%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\n\n-------------------------------------------------\nDatanodes available: 2 (2 total, 0 dead)\n\nName: 192.168.121.21:50010\nDecommission Status : Normal\nConfigured Capacity: 7092494336 (6.61 GB)\nDFS Used: 413696 (404 KB)\nNon DFS Used: 2182565888 (2.03 GB)\nDFS Remaining: 4909514752(4.57 GB)\nDFS Used%: 0.01%\nDFS Remaining%: 69.22%\nLast contact: Tue Sep 09 22:22:33 JST 2014\n\n\nName: 192.168.121.22:50010\nDecommission Status : Normal\nConfigured Capacity: 7092494336 (6.61 GB)\nDFS Used: 40960 (40 KB)\nNon DFS Used: 2182602752 (2.03 GB)\nDFS Remaining: 4909850624(4.57 GB)\nDFS Used%: 0%\nDFS Remaining%: 69.23%\nLast contact: Tue Sep 09 22:22:33 JST 2014\n\nBlocks with corrupt replicas \u53ca\u3073 Missing blocks \u304c0\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u30c7\u30fc\u30bf\u306e\u504f\u308a\u3092\u88dc\u6b63\u3059\u308b\n\nhdfs \u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n$ sudo su - hdfs\n$ hadoop balancer -threshold 5\n14/09/09 23:51:15 INFO balancer.Balancer: Using a threshold of 5.0\nTime Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved\n14/09/09 23:51:16 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.121.21:50010\n14/09/09 23:51:16 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.121.22:50010\n14/09/09 23:51:16 INFO balancer.Balancer: 0 over utilized nodes:\n14/09/09 23:51:16 INFO balancer.Balancer: 0 under utilized nodes:\nThe cluster is balanced. Exiting...\nBalancing took 1.09 seconds\n\n\nHDFS\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9(hadoop-master)\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\n$ sudo su - hdfs\n$ hadoop fsck /user/hdfs/test -file -blocks -racks\nFSCK started by hdfs (auth:SIMPLE) from /192.168.121.11 for path /user/hdfs/test at Tue Sep 09 23:43:28 JST 2014\n/user/hdfs/test <dir>\n/user/hdfs/test/LICENSE.txt 13366 bytes, 1 block(s):  OK\n0. blk_2224989497314884607_1496 len=13366 repl=1 [/default-rack/192.168.121.21:50010]\n\n/user/hdfs/test/README.txt 1366 bytes, 1 block(s):  OK\n0. blk_738116991491030545_1490 len=1366 repl=1 [/default-rack/192.168.121.21:50010]\n\n/user/hdfs/test/hostname.txt 22 bytes, 1 block(s):  OK\n0. blk_3353161374104946145_1494 len=22 repl=1 [/default-rack/192.168.121.22:50010]\n\n/user/hdfs/test/hostname.txt.gz 39 bytes, 1 block(s):  OK\n0. blk_7234611454969758918_1495 len=39 repl=1 [/default-rack/192.168.121.22:50010]\n\n\n Total size:   Status: HEALTHY 14793 B\n Total dirs:    1\n Total files:   4\n Total blocks (validated):      4 (avg. block size 3698 B)\n Minimally replicated blocks:   4 (100.0 %)\n Over-replicated blocks:        0 (0.0 %)\n Under-replicated blocks:       0 (0.0 %)\n Mis-replicated blocks:         0 (0.0 %)\n Default replication factor:    1\n Average block replication:     1.0\n Corrupt blocks:                0\n Missing replicas:              0 (0.0 %)\n Number of data-nodes:          2\n Number of racks:               1\nFSCK ended at Tue Sep 09 23:43:28 JST 2014 in 1 milliseconds\n\n\nThe filesystem under path '/user/hdfs/test' is HEALTHY\n\nStatus: HEALTHY \u3092\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3059\u308b\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u5225\u30b5\u30fc\u30d0\u3067\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u3001\u300cfsimage\u300d\u53ca\u3073\u300cedits\u300d\u3092\u5225\u30b5\u30fc\u30d0\u3067\u3082\u4fdd\u5b58\u3059\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\nhadoop\u306e\u30af\u30e9\u30b9\u30bf\u3078\u30ce\u30fc\u30c9(hadoop-master2)\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\u8ffd\u52a0\u5f8c\u306e\u69cb\u6210\n\n\n\n\u5f79\u5272\n\u30db\u30b9\u30c8\u540d\nIP\u30a2\u30c9\u30ec\u30b9\n\n\n\n\nmaster\nhadoop-master\n192.168.121.11\n\n\nmaster\nhadoop-master2\n192.168.121.12\n\n\nslave\nhadoop-slave\n192.168.121.21\n\n\nslave\nhadoop-slave2\n192.168.121.22\n\n\nclient\nhadoop-client\n192.168.121.101\n\n\n\nsecondlynamenode\u306e\u8a2d\u5b9a\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n(\u8a2d\u5b9a\u3092\u30af\u30e9\u30b9\u30bf\u5168\u4f53\u3067\u7d71\u4e00\u3057\u3066\u304a\u304f\u305f\u3081\u3001\u5168\u30ce\u30fc\u30c9\u3078\u8ffd\u52a0\u3057\u307e\u3059\u3002)\n\n/etc/hadoop-0.20/conf.cluster/core-site.xml\n<cofiguration>\n..(\u7701\u7565)..\n  <property>\n    <name>fs.checkpoint.period</name>\n    <value>60</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.size</name>\n    <value>67108864</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.dir</name>\n    <value>/var/lib/hadoop/dfs/snn</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.edits.dir</name>\n    <value>/var/lib/hadoop/dfs/snn</value>\n  </property>\n<configuration>\n\n\nfs.checkpoint.period \u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f 3600 \u3067\u3059\u304c\u3001\u691c\u8a3c\u306e\u305f\u3081 60 \u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\n\u540d\u524d\u89e3\u6c7a\u3092\u884c\u3048\u308b\u3088\u3046\u306b /etc/hosts \u3078\u8a18\u8ff0\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n(\u8a2d\u5b9a\u3092\u30af\u30e9\u30b9\u30bf\u5168\u4f53\u3067\u7d71\u4e00\u3057\u3066\u304a\u304f\u305f\u3081\u3001\u5168\u30ce\u30fc\u30c9\u3078\u8ffd\u52a0\u3057\u307e\u3059\u3002)\n\n/etc/hosts\n192.168.121.12 hadoop-master2\n\n\nsecondlynamenode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n$ sudo /etc/init.d/hadoop-0.20-secondlynamenode start\n\n\u8d77\u52d5\u5f8c\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u8ee2\u9001\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n$ ls -l /var/lib/hadoop/dfs/snn\ntotal 16\n-rw-r--r--. 1 hdfs hdfs    4 Sep 10 07:03 edits\n-rw-r--r--. 1 hdfs hdfs 1979 Sep 10 07:03 fsimage\n-rw-r--r--. 1 hdfs hdfs    8 Sep 10 07:03 fstime\n-rw-r--r--. 1 hdfs hdfs  101 Sep 10 07:03 VERSION\n\n\n\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u30a2\u3059\u308b\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u7a3c\u50cd\u3055\u305b\u308b\u30b5\u30fc\u30d0\u3092hadoop-master2\u3078\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u30a2\u3092\u884c\u3044\u307e\u3059\u3002\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u306bfsimage\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\n(hadoop-master2\u306e\u307f)\n$ ls -l /var/lib/hadoop/dfs/snn/current\ntotal 16\n-rw-r--r--. 1 hdfs hdfs    4 Sep 10 14:32 edits\n-rw-r--r--. 1 hdfs hdfs 2079 Sep 10 14:32 fsimage\n-rw-r--r--. 1 hdfs hdfs    8 Sep 10 14:32 fstime\n-rw-r--r--. 1 hdfs hdfs  101 Sep 10 14:32 VERSION\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\n(hadoop-master2\u306e\u307f)\n$ sudo /etc/init.d/hadoop-0.20-secondarynamenode stop\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\n(hadoop-master\u306e\u307f)\n$ sudo /etc/init.d/hadoop-0.20-namenode stop\n\n\u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2)\n$ sudo /etc/init.d/hadoop-0.20-datanode stop\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u306e\u7a3c\u50cd\u3059\u308b\u30db\u30b9\u30c8\u3092hadoop-master\u304b\u3089hadoop-master2\u3078\u5909\u66f4\u3057\u307e\u3059\u3002\n(\u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u3059\u3079\u3066)\n\n/etc/hadoop-0.20/conf.cluster/core-site.xml\n<configuration>\n  <property>\n    <name>fs.default.name</name>\n    <value>hdfs://hadoop-master2:8020</value>\n  </property>\n..(\u7701\u7565)..\n</configuration>\n\n\nhadoop-master2\u306e\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u5b58\u5728\u3057\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n$ ls -l /var/lib/hadoop/dfs/nn\ntotal 0\n\nhadoop-master2\u3067\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u30a2\u3092\u884c\u3044\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n$ sudo su - hdfs\n$ hadoop namenode -importCheckpoint\n14/09/10 14:53:00 INFO namenode.NameNode: STARTUP_MSG:\n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-master2/192.168.121.12\nSTARTUP_MSG:   args = [-importCheckpoint]\nSTARTUP_MSG:   version = 0.20.2-cdh3u6\nSTARTUP_MSG:   build = file:///data/1/tmp/topdir/BUILD/hadoop-0.20.2-cdh3u6 -r efb405d2aa54039bdf39e0733cd0bb9423a1eb0a; compiled by 'root' on Wed Mar 20 13:11:26 PDT 2013\n************************************************************/\n14/09/10 14:53:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null\n14/09/10 14:53:00 INFO metrics.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext\n14/09/10 14:53:00 INFO util.GSet: VM type       = 64-bit\n14/09/10 14:53:00 INFO util.GSet: 2% max memory = 19.33375 MB\n14/09/10 14:53:00 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n14/09/10 14:53:00 INFO util.GSet: recommended=2097152, actual=2097152\n14/09/10 14:53:00 INFO namenode.FSNamesystem: fsOwner=hdfs (auth:SIMPLE)\n14/09/10 14:53:00 INFO namenode.FSNamesystem: supergroup=supergroup\n14/09/10 14:53:00 INFO namenode.FSNamesystem: isPermissionEnabled=false\n14/09/10 14:53:00 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=1000\n14/09/10 14:53:00 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)\n14/09/10 14:53:01 INFO metrics.FSNamesystemMetrics: Initializing FSNamesystemMetrics using context object:org.apache.hadoop.metrics.spi.NullContext\n14/09/10 14:53:01 INFO common.Storage: Storage directory /var/lib/hadoop/dfs/nn is not formatted.\n14/09/10 14:53:01 INFO common.Storage: Formatting ...\n14/09/10 14:53:01 INFO common.Storage: Number of files = 22\n14/09/10 14:53:01 INFO common.Storage: Number of files under construction = 0\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 loaded in 0 seconds.\n14/09/10 14:53:01 INFO common.Storage: Edits file /var/lib/hadoop/dfs/snn/current/edits of size 4 edits # 0 loaded in 0 seconds.\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 saved in 0 seconds.\n14/09/10 14:53:01 INFO namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 saved in 0 seconds.\n14/09/10 14:53:01 INFO namenode.FSNamesystem: Finished loading FSImage in 1258 msecs\n14/09/10 14:53:01 INFO hdfs.StateChange: STATE* Safe mode ON.\nThe reported blocks 0 needs additional 6 blocks to reach the threshold 0.9990 of total blocks 7. Safe mode will be turned off automatically.\n14/09/10 14:53:01 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\n14/09/10 14:53:01 INFO metrics.RpcMetrics: Initializing RPC Metrics with hostName=NameNode, port=8020\n14/09/10 14:53:01 INFO ipc.Server: Starting Socket Reader #1 for port 8020\n14/09/10 14:53:01 INFO metrics.RpcDetailedMetrics: Initializing RPC Metrics with hostName=NameNode, port=8020\n14/09/10 14:53:01 INFO namenode.NameNode: Namenode up at: hadoop-master2/192.168.121.12:8020\n14/09/10 14:53:02 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n14/09/10 14:53:02 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n14/09/10 14:53:02 INFO http.HttpServer: dfs.webhdfs.enabled = false\n14/09/10 14:53:02 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50070\n14/09/10 14:53:02 INFO http.HttpServer: listener.getLocalPort() returned 50070 webServer.getConnectors()[0].getLocalPort() returned 50070\n14/09/10 14:53:02 INFO http.HttpServer: Jetty bound to port 50070\n14/09/10 14:53:02 INFO mortbay.log: jetty-6.1.26.cloudera.2\n14/09/10 14:53:02 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50070\n14/09/10 14:53:02 INFO namenode.NameNode: Web-server up at: 0.0.0.0:50070\n14/09/10 14:53:02 INFO ipc.Server: IPC Server Responder: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server listener on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 0 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 1 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 2 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 3 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 4 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 5 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 6 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 7 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 8 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 9 on 8020: starting\n14/09/10 14:54:30 INFO namenode.NameNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at hadoop-master2/192.168.121.12\n************************************************************/\n\n(Ctrl + C\u3067\u4e00\u65e6\u505c\u6b62\u3055\u305b\u307e\u3059)\n\n\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u30ea\u30b9\u30c8\u30a2\u3055\u308c\u305f\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n$ ls -l /var/lib/hadoop/dfs/nn\ntotal 12\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 current\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 image\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 previous.checkpoint\n\n$ ls -l /var/lib/hadoop/dfs/nn/current\ntotal 16\n-rw-rw-r--. 1 hdfs hdfs    4 Sep 10 14:53 edits\n-rw-rw-r--. 1 hdfs hdfs 2079 Sep 10 14:53 fsimage\n-rw-rw-r--. 1 hdfs hdfs    8 Sep 10 14:53 fstime\n-rw-rw-r--. 1 hdfs hdfs  101 Sep 10 14:53 VERSION\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n$ sudo /etc/init.d/hadoop-0.20-namenode start\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n$ sudo /etc/init.d/hadoop-0.20-secondarynamenode start\n\n\u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2\u306e\u307f)\n$ sudo /etc/init.d/hadoop-0.20-datanode start\n\nHDFS\u3078\u306e\u8aad\u307f\u66f8\u304d\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\n(hadoop-client)\n$ hadoop fs -ls\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2014-09-10 06:59 /user/hdfs/test\ndrwxr-xr-x   - hdfs supergroup          0 2014-09-09 10:19 /user/hdfs/test2\n\n$ hadoop fs -ls test/\nFound 5 items\n-rw-r--r--   1 hdfs supergroup      13366 2014-09-09 22:31 /user/hdfs/test/LICENSE.txt\n-rw-r--r--   1 hdfs supergroup        101 2014-09-10 06:59 /user/hdfs/test/NOTICE.txt\n-rw-r--r--   1 hdfs supergroup       1366 2014-09-09 07:46 /user/hdfs/test/README.txt\n-rw-r--r--   1 hdfs supergroup         22 2014-09-09 10:29 /user/hdfs/test/hostname.txt\n-rw-r--r--   1 hdfs supergroup         39 2014-09-09 10:30 /user/hdfs/test/hostname.txt.gz\n\n$ dmesg | hadoop fs -put - test/dmesg.txt\n\n$ hadoop fs -ls test/\nFound 6 items\n-rw-r--r--   1 hdfs supergroup      13366 2014-09-09 22:31 /user/hdfs/test/LICENSE.txt\n-rw-r--r--   1 hdfs supergroup        101 2014-09-10 06:59 /user/hdfs/test/NOTICE.txt\n-rw-r--r--   1 hdfs supergroup       1366 2014-09-09 07:46 /user/hdfs/test/README.txt\n-rw-r--r--   1 hdfs supergroup      21881 2014-09-10 15:05 /user/hdfs/test/dmesg.txt\n-rw-r--r--   1 hdfs supergroup         22 2014-09-09 10:29 /user/hdfs/test/hostname.txt\n-rw-r--r--   1 hdfs supergroup         39 2014-09-09 10:30 /user/hdfs/test/hostname.txt.gz\n\n\n\u53c2\u8003\n\nCDH3\u3067hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b\n\n\n## \u306f\u3058\u3081\u306b\n\nCDH3\u3067HDFS\u3092\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\u3059\u308b\u624b\u9806\u3092\u8a18\u8ff0\u3057\u307e\u3059\u3002\n\n## \u74b0\u5883\n\n* CentOS 6.5\n* CDH3\n* JDK 1.6\n\n## hadoop\u30af\u30e9\u30b9\u30bf\u306e\u69cb\u6210\n\n|\u5f79\u5272  |\u30db\u30b9\u30c8\u540d      |IP\u30a2\u30c9\u30ec\u30b9     |\n|:-----|:-------------|:--------------|\n|master|hadoop-master |192.168.121.11 |\n|slave |hadoop-slave  |192.168.121.21 |\n|slave |hadoop-slave2 |192.168.121.22 |\n|client|hadoop-client |192.168.121.101|\n\n\u203b CDH3\u3067\u306ehadoop\u306e\u30af\u30e9\u30b9\u30bf\u306e\u69cb\u7bc9\u65b9\u6cd5\u306f\u3001[CDH3\u3067hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b](http://qiita.com/toshiro3/items/071d7d35969a3c4aa581) \u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\n## HDFS\u306e\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\n\n* HDFS\u306e\u4f7f\u7528\u7387\u306e\u78ba\u8a8d\n\n_hdfs_ \u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - hdfs\n$ hadoop dfsadmin -report\nConfigured Capacity: 14184988672 (13.21 GB)\nPresent Capacity: 9819820032 (9.15 GB)\nDFS Remaining: 9819365376 (9.14 GB)\nDFS Used: 454656 (444 KB)\nDFS Used%: 0%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\n\n-------------------------------------------------\nDatanodes available: 2 (2 total, 0 dead)\n\nName: 192.168.121.21:50010\nDecommission Status : Normal\nConfigured Capacity: 7092494336 (6.61 GB)\nDFS Used: 413696 (404 KB)\nNon DFS Used: 2182565888 (2.03 GB)\nDFS Remaining: 4909514752(4.57 GB)\nDFS Used%: 0.01%\nDFS Remaining%: 69.22%\nLast contact: Tue Sep 09 22:22:33 JST 2014\n\n\nName: 192.168.121.22:50010\nDecommission Status : Normal\nConfigured Capacity: 7092494336 (6.61 GB)\nDFS Used: 40960 (40 KB)\nNon DFS Used: 2182602752 (2.03 GB)\nDFS Remaining: 4909850624(4.57 GB)\nDFS Used%: 0%\nDFS Remaining%: 69.23%\nLast contact: Tue Sep 09 22:22:33 JST 2014\n```\n__Blocks with corrupt replicas__ \u53ca\u3073 __Missing blocks__ \u304c0\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n* \u30c7\u30fc\u30bf\u306e\u504f\u308a\u3092\u88dc\u6b63\u3059\u308b\n\n_hdfs_ \u30e6\u30fc\u30b6\u3067\u64cd\u4f5c\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - hdfs\n$ hadoop balancer -threshold 5\n14/09/09 23:51:15 INFO balancer.Balancer: Using a threshold of 5.0\nTime Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved\n14/09/09 23:51:16 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.121.21:50010\n14/09/09 23:51:16 INFO net.NetworkTopology: Adding a new node: /default-rack/192.168.121.22:50010\n14/09/09 23:51:16 INFO balancer.Balancer: 0 over utilized nodes:\n14/09/09 23:51:16 INFO balancer.Balancer: 0 under utilized nodes:\nThe cluster is balanced. Exiting...\nBalancing took 1.09 seconds\n```\n\n* HDFS\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9(hadoop-master)\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo su - hdfs\n$ hadoop fsck /user/hdfs/test -file -blocks -racks\nFSCK started by hdfs (auth:SIMPLE) from /192.168.121.11 for path /user/hdfs/test at Tue Sep 09 23:43:28 JST 2014\n/user/hdfs/test <dir>\n/user/hdfs/test/LICENSE.txt 13366 bytes, 1 block(s):  OK\n0. blk_2224989497314884607_1496 len=13366 repl=1 [/default-rack/192.168.121.21:50010]\n\n/user/hdfs/test/README.txt 1366 bytes, 1 block(s):  OK\n0. blk_738116991491030545_1490 len=1366 repl=1 [/default-rack/192.168.121.21:50010]\n\n/user/hdfs/test/hostname.txt 22 bytes, 1 block(s):  OK\n0. blk_3353161374104946145_1494 len=22 repl=1 [/default-rack/192.168.121.22:50010]\n\n/user/hdfs/test/hostname.txt.gz 39 bytes, 1 block(s):  OK\n0. blk_7234611454969758918_1495 len=39 repl=1 [/default-rack/192.168.121.22:50010]\n\n\n Total size:   Status: HEALTHY 14793 B\n Total dirs:    1\n Total files:   4\n Total blocks (validated):      4 (avg. block size 3698 B)\n Minimally replicated blocks:   4 (100.0 %)\n Over-replicated blocks:        0 (0.0 %)\n Under-replicated blocks:       0 (0.0 %)\n Mis-replicated blocks:         0 (0.0 %)\n Default replication factor:    1\n Average block replication:     1.0\n Corrupt blocks:                0\n Missing replicas:              0 (0.0 %)\n Number of data-nodes:          2\n Number of racks:               1\nFSCK ended at Tue Sep 09 23:43:28 JST 2014 in 1 milliseconds\n\n\nThe filesystem under path '/user/hdfs/test' is HEALTHY\n```\n__Status: HEALTHY__ \u3092\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n* \u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u3059\u308b\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u5225\u30b5\u30fc\u30d0\u3067\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u3001\u300cfsimage\u300d\u53ca\u3073\u300cedits\u300d\u3092\u5225\u30b5\u30fc\u30d0\u3067\u3082\u4fdd\u5b58\u3059\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\n\nhadoop\u306e\u30af\u30e9\u30b9\u30bf\u3078\u30ce\u30fc\u30c9(hadoop-master2)\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n\u8ffd\u52a0\u5f8c\u306e\u69cb\u6210\n\n|\u5f79\u5272  |\u30db\u30b9\u30c8\u540d      |IP\u30a2\u30c9\u30ec\u30b9     |\n|:-----|:-------------|:--------------|\n|master|hadoop-master |192.168.121.11 |\n|master|hadoop-master2|192.168.121.12 |\n|slave |hadoop-slave  |192.168.121.21 |\n|slave |hadoop-slave2 |192.168.121.22 |\n|client|hadoop-client |192.168.121.101|\n\n\nsecondlynamenode\u306e\u8a2d\u5b9a\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n(\u8a2d\u5b9a\u3092\u30af\u30e9\u30b9\u30bf\u5168\u4f53\u3067\u7d71\u4e00\u3057\u3066\u304a\u304f\u305f\u3081\u3001\u5168\u30ce\u30fc\u30c9\u3078\u8ffd\u52a0\u3057\u307e\u3059\u3002)\n\n```xml:/etc/hadoop-0.20/conf.cluster/core-site.xml\n<cofiguration>\n..(\u7701\u7565)..\n  <property>\n    <name>fs.checkpoint.period</name>\n    <value>60</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.size</name>\n    <value>67108864</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.dir</name>\n    <value>/var/lib/hadoop/dfs/snn</value>\n  </property>\n  <property>\n    <name>fs.checkpoint.edits.dir</name>\n    <value>/var/lib/hadoop/dfs/snn</value>\n  </property>\n<configuration>\n```\n\n_fs.checkpoint.period_ \u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f __3600__ \u3067\u3059\u304c\u3001\u691c\u8a3c\u306e\u305f\u3081 __60__ \u306b\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u540d\u524d\u89e3\u6c7a\u3092\u884c\u3048\u308b\u3088\u3046\u306b /etc/hosts \u3078\u8a18\u8ff0\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n(\u8a2d\u5b9a\u3092\u30af\u30e9\u30b9\u30bf\u5168\u4f53\u3067\u7d71\u4e00\u3057\u3066\u304a\u304f\u305f\u3081\u3001\u5168\u30ce\u30fc\u30c9\u3078\u8ffd\u52a0\u3057\u307e\u3059\u3002)\n\n\n```shell:/etc/hosts\n192.168.121.12 hadoop-master2\n```\n\nsecondlynamenode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-secondlynamenode start\n```\n\n\u8d77\u52d5\u5f8c\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u8ee2\u9001\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n\n```shell\n$ ls -l /var/lib/hadoop/dfs/snn\ntotal 16\n-rw-r--r--. 1 hdfs hdfs    4 Sep 10 07:03 edits\n-rw-r--r--. 1 hdfs hdfs 1979 Sep 10 07:03 fsimage\n-rw-r--r--. 1 hdfs hdfs    8 Sep 10 07:03 fstime\n-rw-r--r--. 1 hdfs hdfs  101 Sep 10 07:03 VERSION\n```\n\n* \u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u30a2\u3059\u308b\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u7a3c\u50cd\u3055\u305b\u308b\u30b5\u30fc\u30d0\u3092hadoop-master2\u3078\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u30a2\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u306bfsimage\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ ls -l /var/lib/hadoop/dfs/snn/current\ntotal 16\n-rw-r--r--. 1 hdfs hdfs    4 Sep 10 14:32 edits\n-rw-r--r--. 1 hdfs hdfs 2079 Sep 10 14:32 fsimage\n-rw-r--r--. 1 hdfs hdfs    8 Sep 10 14:32 fstime\n-rw-r--r--. 1 hdfs hdfs  101 Sep 10 14:32 VERSION\n```\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-secondarynamenode stop\n```\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\n(hadoop-master\u306e\u307f)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-namenode stop\n```\n\n\u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u505c\u6b62\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-datanode stop\n```\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u306e\u7a3c\u50cd\u3059\u308b\u30db\u30b9\u30c8\u3092hadoop-master\u304b\u3089hadoop-master2\u3078\u5909\u66f4\u3057\u307e\u3059\u3002\n(\u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u3059\u3079\u3066)\n\n```xml:/etc/hadoop-0.20/conf.cluster/core-site.xml\n<configuration>\n  <property>\n    <name>fs.default.name</name>\n    <value>hdfs://hadoop-master2:8020</value>\n  </property>\n..(\u7701\u7565)..\n</configuration>\n```\n\nhadoop-master2\u306e\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u5b58\u5728\u3057\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n\n\n```shell\n$ ls -l /var/lib/hadoop/dfs/nn\ntotal 0\n```\nhadoop-master2\u3067\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8\u30a2\u3092\u884c\u3044\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ sudo su - hdfs\n$ hadoop namenode -importCheckpoint\n14/09/10 14:53:00 INFO namenode.NameNode: STARTUP_MSG:\n/************************************************************\nSTARTUP_MSG: Starting NameNode\nSTARTUP_MSG:   host = hadoop-master2/192.168.121.12\nSTARTUP_MSG:   args = [-importCheckpoint]\nSTARTUP_MSG:   version = 0.20.2-cdh3u6\nSTARTUP_MSG:   build = file:///data/1/tmp/topdir/BUILD/hadoop-0.20.2-cdh3u6 -r efb405d2aa54039bdf39e0733cd0bb9423a1eb0a; compiled by 'root' on Wed Mar 20 13:11:26 PDT 2013\n************************************************************/\n14/09/10 14:53:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null\n14/09/10 14:53:00 INFO metrics.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext\n14/09/10 14:53:00 INFO util.GSet: VM type       = 64-bit\n14/09/10 14:53:00 INFO util.GSet: 2% max memory = 19.33375 MB\n14/09/10 14:53:00 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n14/09/10 14:53:00 INFO util.GSet: recommended=2097152, actual=2097152\n14/09/10 14:53:00 INFO namenode.FSNamesystem: fsOwner=hdfs (auth:SIMPLE)\n14/09/10 14:53:00 INFO namenode.FSNamesystem: supergroup=supergroup\n14/09/10 14:53:00 INFO namenode.FSNamesystem: isPermissionEnabled=false\n14/09/10 14:53:00 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=1000\n14/09/10 14:53:00 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)\n14/09/10 14:53:01 INFO metrics.FSNamesystemMetrics: Initializing FSNamesystemMetrics using context object:org.apache.hadoop.metrics.spi.NullContext\n14/09/10 14:53:01 INFO common.Storage: Storage directory /var/lib/hadoop/dfs/nn is not formatted.\n14/09/10 14:53:01 INFO common.Storage: Formatting ...\n14/09/10 14:53:01 INFO common.Storage: Number of files = 22\n14/09/10 14:53:01 INFO common.Storage: Number of files under construction = 0\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 loaded in 0 seconds.\n14/09/10 14:53:01 INFO common.Storage: Edits file /var/lib/hadoop/dfs/snn/current/edits of size 4 edits # 0 loaded in 0 seconds.\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 saved in 0 seconds.\n14/09/10 14:53:01 INFO namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0\n14/09/10 14:53:01 INFO common.Storage: Image file of size 2079 saved in 0 seconds.\n14/09/10 14:53:01 INFO namenode.FSNamesystem: Finished loading FSImage in 1258 msecs\n14/09/10 14:53:01 INFO hdfs.StateChange: STATE* Safe mode ON.\nThe reported blocks 0 needs additional 6 blocks to reach the threshold 0.9990 of total blocks 7. Safe mode will be turned off automatically.\n14/09/10 14:53:01 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\n14/09/10 14:53:01 INFO metrics.RpcMetrics: Initializing RPC Metrics with hostName=NameNode, port=8020\n14/09/10 14:53:01 INFO ipc.Server: Starting Socket Reader #1 for port 8020\n14/09/10 14:53:01 INFO metrics.RpcDetailedMetrics: Initializing RPC Metrics with hostName=NameNode, port=8020\n14/09/10 14:53:01 INFO namenode.NameNode: Namenode up at: hadoop-master2/192.168.121.12:8020\n14/09/10 14:53:02 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n14/09/10 14:53:02 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n14/09/10 14:53:02 INFO http.HttpServer: dfs.webhdfs.enabled = false\n14/09/10 14:53:02 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50070\n14/09/10 14:53:02 INFO http.HttpServer: listener.getLocalPort() returned 50070 webServer.getConnectors()[0].getLocalPort() returned 50070\n14/09/10 14:53:02 INFO http.HttpServer: Jetty bound to port 50070\n14/09/10 14:53:02 INFO mortbay.log: jetty-6.1.26.cloudera.2\n14/09/10 14:53:02 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50070\n14/09/10 14:53:02 INFO namenode.NameNode: Web-server up at: 0.0.0.0:50070\n14/09/10 14:53:02 INFO ipc.Server: IPC Server Responder: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server listener on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 0 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 1 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 2 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 3 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 4 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 5 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 6 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 7 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 8 on 8020: starting\n14/09/10 14:53:02 INFO ipc.Server: IPC Server handler 9 on 8020: starting\n14/09/10 14:54:30 INFO namenode.NameNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at hadoop-master2/192.168.121.12\n************************************************************/\n\n(Ctrl + C\u3067\u4e00\u65e6\u505c\u6b62\u3055\u305b\u307e\u3059)\n```\n\n\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u30ea\u30b9\u30c8\u30a2\u3055\u308c\u305f\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ ls -l /var/lib/hadoop/dfs/nn\ntotal 12\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 current\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 image\ndrwxrwxr-x. 2 hdfs hdfs 4096 Sep 10 14:53 previous.checkpoint\n\n$ ls -l /var/lib/hadoop/dfs/nn/current\ntotal 16\n-rw-rw-r--. 1 hdfs hdfs    4 Sep 10 14:53 edits\n-rw-rw-r--. 1 hdfs hdfs 2079 Sep 10 14:53 fsimage\n-rw-rw-r--. 1 hdfs hdfs    8 Sep 10 14:53 fstime\n-rw-rw-r--. 1 hdfs hdfs  101 Sep 10 14:53 VERSION\n```\n\n\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-namenode start\n```\n\n\u30bb\u30ab\u30f3\u30c0\u30ea\u30cd\u30fc\u30e0\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-master2\u306e\u307f)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-secondarynamenode start\n```\n\n\u30c7\u30fc\u30bf\u30ce\u30fc\u30c9\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n(hadoop-slave,hadoop-slave2\u306e\u307f)\n\n```shell\n$ sudo /etc/init.d/hadoop-0.20-datanode start\n```\n\nHDFS\u3078\u306e\u8aad\u307f\u66f8\u304d\u304c\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\n(hadoop-client)\n\n```shell\n$ hadoop fs -ls\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2014-09-10 06:59 /user/hdfs/test\ndrwxr-xr-x   - hdfs supergroup          0 2014-09-09 10:19 /user/hdfs/test2\n\n$ hadoop fs -ls test/\nFound 5 items\n-rw-r--r--   1 hdfs supergroup      13366 2014-09-09 22:31 /user/hdfs/test/LICENSE.txt\n-rw-r--r--   1 hdfs supergroup        101 2014-09-10 06:59 /user/hdfs/test/NOTICE.txt\n-rw-r--r--   1 hdfs supergroup       1366 2014-09-09 07:46 /user/hdfs/test/README.txt\n-rw-r--r--   1 hdfs supergroup         22 2014-09-09 10:29 /user/hdfs/test/hostname.txt\n-rw-r--r--   1 hdfs supergroup         39 2014-09-09 10:30 /user/hdfs/test/hostname.txt.gz\n\n$ dmesg | hadoop fs -put - test/dmesg.txt\n\n$ hadoop fs -ls test/\nFound 6 items\n-rw-r--r--   1 hdfs supergroup      13366 2014-09-09 22:31 /user/hdfs/test/LICENSE.txt\n-rw-r--r--   1 hdfs supergroup        101 2014-09-10 06:59 /user/hdfs/test/NOTICE.txt\n-rw-r--r--   1 hdfs supergroup       1366 2014-09-09 07:46 /user/hdfs/test/README.txt\n-rw-r--r--   1 hdfs supergroup      21881 2014-09-10 15:05 /user/hdfs/test/dmesg.txt\n-rw-r--r--   1 hdfs supergroup         22 2014-09-09 10:29 /user/hdfs/test/hostname.txt\n-rw-r--r--   1 hdfs supergroup         39 2014-09-09 10:30 /user/hdfs/test/hostname.txt.gz\n```\n\n## \u53c2\u8003\n\n* [CDH3\u3067hadoop\u306e\u30af\u30e9\u30b9\u30bf\u3092\u69cb\u7bc9\u3059\u308b](http://qiita.com/toshiro3/items/071d7d35969a3c4aa581)\n", "tags": ["hadoop", "CDH3u6"]}