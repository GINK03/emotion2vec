{"context": " More than 1 year has passed since last update.Stream\u3092\u4f7f\u3063\u3066\u307f\u308b\uff08CUDA7.0, Windows, GTX980\uff09\u3002\n\nsample.cu\n#include <helper_cuda.h>\n#include <cuda_runtime.h>\n#include <stdio.h>\n#include <time.h>\nconst int N_STREAMS = 10;\nconst int width  = 3840;\nconst int height = 2160;\nconst int size   = width*height;\n\n//---------------------------------------------\n// cuda kernel function\n//---------------------------------------------\n__global__ void\nkernel(int *d_a, int *d_b, int *d_c, int n, int offset = 0)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x + offset;\n    if (i < n) {\n        d_c[i] = d_a[i] + d_b[i];\n    }\n    return;\n}\n\n//---------------------------------------------\n// stream\u3092\u4f7f\u3046\n//---------------------------------------------\nint* cuda_exec_stream()\n{\n    cudaStream_t stream[N_STREAMS];\n    // host\u5074\u30c7\u30fc\u30bf\u6e96\u5099\u3002\n    int* h_a = NULL;\n    int* h_b = NULL;\n    int* h_c = NULL;\n    cudaHostAlloc((void **)&h_a, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_b, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_c, sizeof(int)*size, cudaHostAllocDefault);\n    for (int i=0; i<height; i++) {\n        for (int j=0; j<width; j++) {\n            h_a[i*height + j] = (i + j)*height/(width+1);\n            h_b[i*height + j] = (i + j)*width/(height+1);\n            h_c[i*height + j] = -1;\n        }\n    }\n    // host -> device \u8ee2\u9001\n    int *d_a = NULL;\n    int *d_b = NULL;\n    int *d_c = NULL;\n    cudaMalloc((void **)&d_a, sizeof(int)*size);\n    cudaMalloc((void **)&d_b, sizeof(int)*size);\n    cudaMalloc((void **)&d_c, sizeof(int)*size);\n\n    cudaMemset(d_c, 0, sizeof(int)*size);\n    // stream\u7528\u610f\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaStreamCreate(&stream[i]);\n    }\n    // kernel\u5b9f\u884c\n    int N = size / N_STREAMS; //assert(\u5272\u308a\u5207\u308c\u308b)\n    int th = 1024;\n    int bl = (N + th - 1) / th;\n    dim3 threads(th, 1, 1);\n    dim3 blocks(bl, 1, 1);\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaMemcpyAsync(d_a, h_a, sizeof(int)*size, cudaMemcpyHostToDevice, stream[i]);\n        cudaMemcpyAsync(d_b, h_b, sizeof(int)*size, cudaMemcpyHostToDevice, stream[i]);\n        kernel<<<(N + th - 1) / th, th, 0, stream[i]>>>(d_a, d_b, d_c, N*(i+1), i*N);\n        cudaMemcpyAsync(&h_c[i * N], &d_c[i * N], sizeof(int)*N, cudaMemcpyDeviceToHost, stream[i]);\n    }\n    // \u5f8c\u59cb\u672b\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaStreamSynchronize(stream[i]);\n        cudaStreamDestroy(stream[i]);\n    }\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    cudaFreeHost(h_a);\n    cudaFreeHost(h_b);\n    return h_c;\n}\n\n//---------------------------------------------\n// stream\u3092\u4f7f\u308f\u306a\u3044\n//---------------------------------------------\nint* cuda_exec()\n{\n    // host\u5074\u30c7\u30fc\u30bf\u6e96\u5099\u3002\n    int* h_a = NULL;\n    int* h_b = NULL;\n    int* h_c = NULL;\n    cudaHostAlloc((void **)&h_a, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_b, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_c, sizeof(int)*size, cudaHostAllocDefault);\n    for (int i=0; i<height; i++) {\n        for (int j=0; j<width; j++) {\n            h_a[i*height + j] = (i + j)*height/(width+1);\n            h_b[i*height + j] = (i + j)*width/(height+1);\n            h_c[i*height + j] = -2;\n        }\n    }\n    // host -> device \u8ee2\u9001\n    int *d_a = NULL;\n    int *d_b = NULL;\n    int *d_c = NULL;\n    cudaMalloc((void **)&d_a, sizeof(int)*size);\n    cudaMalloc((void **)&d_b, sizeof(int)*size);\n    cudaMalloc((void **)&d_c, sizeof(int)*size);\n    cudaMemcpy(d_a, h_a, sizeof(int)*size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, sizeof(int)*size, cudaMemcpyHostToDevice);\n    cudaMemset(d_c, 0, sizeof(int)*size);\n    // \u30ab\u30fc\u30cd\u30eb\u547c\u3073\u51fa\u3057\n    int th = 1024;\n    int bl = (size + th - 1) / th;\n    dim3 threads(th, 1, 1);\n    dim3 blocks(bl, 1, 1);\n    kernel<<<blocks, threads>>>(d_a, d_b, d_c, size);\n    // device -> host \u8ee2\u9001\n    cudaMemcpy((void*)h_c, (void*)d_c, sizeof(int)*size, cudaMemcpyDeviceToHost);\n    // \u5f8c\u59cb\u672b\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    cudaFreeHost(h_a);\n    cudaFreeHost(h_b);\n    return h_c;\n}\n\nvoid cuda_main()\n{\n    //CUDA Kernel\u306e\u5b9f\u884c\u7d42\u4e86\u3092\u5f85\u3064\u969b\u306e\u6319\u52d5\n    //cudaDeviceScheduleSpin: \u30dd\u30fc\u30ea\u30f3\u30b0\u3092\u5f37\u5236\u3000to decrease latency when we wait result\n    //cudaDeviceScheduleYield: \u30b9\u30ea\u30fc\u30d7\u3092\u5f37\u5236\u3000to increase general performance\n    //cudaDeviceScheduleAuto: \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u6319\u52d5\n    cudaSetDeviceFlags(cudaDeviceScheduleAuto); \n    // stream\u306a\u3057\n    clock_t t1 = clock();\n    int result1 = cuda_exec();\n    clock_t t2 = clock();\n    fprintf(stderr, \"cuda_exec: %f\\n\", (double)(t2 - t1) / CLOCKS_PER_SEC);\n    // stream\u3042\u308a\n    t1 = clock();\n    int result2 = cuda_exec_stream();\n    t2 = clock();\n    fprintf(stderr, \"cuda_exec_stream: %f\\n\", (double)(t2 - t1) / CLOCKS_PER_SEC);\n    // \u7d50\u679c\u3092\u6bd4\u8f03\n    for (int i=0; i<size; i++) {\n        if (result1[i] != result2[i]) {\n            fprintf(stderr, \"[%d] %d %d\\n\", i, result1[i], result2[i]);\n        }\n    }\n    cudaFreeHost(result1);\n    cudaFreeHost(result2);\n    return;\n}\n\n\nStream\u3092\u4f7f\u3063\u3066\u307f\u308b\uff08CUDA7.0, Windows, GTX980\uff09\u3002\n\n```c:sample.cu\n#include <helper_cuda.h>\n#include <cuda_runtime.h>\n#include <stdio.h>\n#include <time.h>\nconst int N_STREAMS = 10;\nconst int width  = 3840;\nconst int height = 2160;\nconst int size   = width*height;\n\n//---------------------------------------------\n// cuda kernel function\n//---------------------------------------------\n__global__ void\nkernel(int *d_a, int *d_b, int *d_c, int n, int offset = 0)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x + offset;\n    if (i < n) {\n        d_c[i] = d_a[i] + d_b[i];\n    }\n    return;\n}\n\n//---------------------------------------------\n// stream\u3092\u4f7f\u3046\n//---------------------------------------------\nint* cuda_exec_stream()\n{\n    cudaStream_t stream[N_STREAMS];\n    // host\u5074\u30c7\u30fc\u30bf\u6e96\u5099\u3002\n    int* h_a = NULL;\n    int* h_b = NULL;\n    int* h_c = NULL;\n    cudaHostAlloc((void **)&h_a, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_b, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_c, sizeof(int)*size, cudaHostAllocDefault);\n    for (int i=0; i<height; i++) {\n    \tfor (int j=0; j<width; j++) {\n            h_a[i*height + j] = (i + j)*height/(width+1);\n            h_b[i*height + j] = (i + j)*width/(height+1);\n            h_c[i*height + j] = -1;\n        }\n    }\n    // host -> device \u8ee2\u9001\n    int *d_a = NULL;\n    int *d_b = NULL;\n    int *d_c = NULL;\n    cudaMalloc((void **)&d_a, sizeof(int)*size);\n    cudaMalloc((void **)&d_b, sizeof(int)*size);\n    cudaMalloc((void **)&d_c, sizeof(int)*size);\n\n    cudaMemset(d_c, 0, sizeof(int)*size);\n    // stream\u7528\u610f\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaStreamCreate(&stream[i]);\n    }\n    // kernel\u5b9f\u884c\n    int N = size / N_STREAMS; //assert(\u5272\u308a\u5207\u308c\u308b)\n    int th = 1024;\n    int bl = (N + th - 1) / th;\n    dim3 threads(th, 1, 1);\n    dim3 blocks(bl, 1, 1);\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaMemcpyAsync(d_a, h_a, sizeof(int)*size, cudaMemcpyHostToDevice, stream[i]);\n        cudaMemcpyAsync(d_b, h_b, sizeof(int)*size, cudaMemcpyHostToDevice, stream[i]);\n        kernel<<<(N + th - 1) / th, th, 0, stream[i]>>>(d_a, d_b, d_c, N*(i+1), i*N);\n        cudaMemcpyAsync(&h_c[i * N], &d_c[i * N], sizeof(int)*N, cudaMemcpyDeviceToHost, stream[i]);\n    }\n    // \u5f8c\u59cb\u672b\n    for (int i = 0; i < N_STREAMS; ++i) {\n        cudaStreamSynchronize(stream[i]);\n        cudaStreamDestroy(stream[i]);\n    }\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    cudaFreeHost(h_a);\n    cudaFreeHost(h_b);\n    return h_c;\n}\n\n//---------------------------------------------\n// stream\u3092\u4f7f\u308f\u306a\u3044\n//---------------------------------------------\nint* cuda_exec()\n{\n    // host\u5074\u30c7\u30fc\u30bf\u6e96\u5099\u3002\n    int* h_a = NULL;\n    int* h_b = NULL;\n    int* h_c = NULL;\n    cudaHostAlloc((void **)&h_a, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_b, sizeof(int)*size, cudaHostAllocDefault);\n    cudaHostAlloc((void **)&h_c, sizeof(int)*size, cudaHostAllocDefault);\n    for (int i=0; i<height; i++) {\n        for (int j=0; j<width; j++) {\n            h_a[i*height + j] = (i + j)*height/(width+1);\n            h_b[i*height + j] = (i + j)*width/(height+1);\n            h_c[i*height + j] = -2;\n        }\n    }\n    // host -> device \u8ee2\u9001\n    int *d_a = NULL;\n    int *d_b = NULL;\n    int *d_c = NULL;\n    cudaMalloc((void **)&d_a, sizeof(int)*size);\n    cudaMalloc((void **)&d_b, sizeof(int)*size);\n    cudaMalloc((void **)&d_c, sizeof(int)*size);\n    cudaMemcpy(d_a, h_a, sizeof(int)*size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, sizeof(int)*size, cudaMemcpyHostToDevice);\n    cudaMemset(d_c, 0, sizeof(int)*size);\n    // \u30ab\u30fc\u30cd\u30eb\u547c\u3073\u51fa\u3057\n    int th = 1024;\n    int bl = (size + th - 1) / th;\n    dim3 threads(th, 1, 1);\n    dim3 blocks(bl, 1, 1);\n    kernel<<<blocks, threads>>>(d_a, d_b, d_c, size);\n    // device -> host \u8ee2\u9001\n    cudaMemcpy((void*)h_c, (void*)d_c, sizeof(int)*size, cudaMemcpyDeviceToHost);\n    // \u5f8c\u59cb\u672b\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    cudaFreeHost(h_a);\n    cudaFreeHost(h_b);\n    return h_c;\n}\n\nvoid cuda_main()\n{\n    //CUDA Kernel\u306e\u5b9f\u884c\u7d42\u4e86\u3092\u5f85\u3064\u969b\u306e\u6319\u52d5\n    //cudaDeviceScheduleSpin: \u30dd\u30fc\u30ea\u30f3\u30b0\u3092\u5f37\u5236\u3000to decrease latency when we wait result\n    //cudaDeviceScheduleYield: \u30b9\u30ea\u30fc\u30d7\u3092\u5f37\u5236\u3000to increase general performance\n    //cudaDeviceScheduleAuto: \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u6319\u52d5\n    cudaSetDeviceFlags(cudaDeviceScheduleAuto); \n    // stream\u306a\u3057\n    clock_t t1 = clock();\n    int result1 = cuda_exec();\n    clock_t t2 = clock();\n    fprintf(stderr, \"cuda_exec: %f\\n\", (double)(t2 - t1) / CLOCKS_PER_SEC);\n    // stream\u3042\u308a\n    t1 = clock();\n    int result2 = cuda_exec_stream();\n    t2 = clock();\n    fprintf(stderr, \"cuda_exec_stream: %f\\n\", (double)(t2 - t1) / CLOCKS_PER_SEC);\n    // \u7d50\u679c\u3092\u6bd4\u8f03\n    for (int i=0; i<size; i++) {\n        if (result1[i] != result2[i]) {\n            fprintf(stderr, \"[%d] %d %d\\n\", i, result1[i], result2[i]);\n        }\n    }\n    cudaFreeHost(result1);\n    cudaFreeHost(result2);\n    return;\n}\n```\n", "tags": ["CUDA"]}