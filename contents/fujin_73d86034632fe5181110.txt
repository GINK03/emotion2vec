{"context": "\u4eca\u56de\u306f\u3001MNIST \u3092\u4f7f\u3063\u305f\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u6570\u5b57\u306e\u5206\u985e\u3092\u884c\u3044\u307e\u3059\u3002\n\nMNIST\nMNIST \u306f\u30010\u301c9 \u307e\u3067\u306e\u624b\u66f8\u304d\u6587\u5b57\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u3001\u753b\u50cf\u30b5\u30a4\u30ba 28x28 \u306e\u30c7\u30fc\u30bf\u304c\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3068\u3057\u3066 60,000\u4ef6\u3001\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u3057\u3066 10,000\u4ef6 \u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\u53c8\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30c7\u30fc\u30bf\u3082\u540c\u6570\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f7f\u3044\u3001\u5bfe\u8c61\u306e\u753b\u50cf\u306e\u6570\u5b57\u304c\u4f55\u3067\u3042\u308b\u304b\u3092\u6c42\u3081\u307e\u3059\u3002\n\n\u4e8b\u524d\u6e96\u5099\n\u4e88\u3081\u306e MNIST \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u5168\u4f53\n\u5b9f\u88c5\u5185\u5bb9\u306f\u3001TensorFlow\u306e MNIST \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u30d9\u30fc\u30b9\u306b\nDeep MNIST for Experts \u306e\u8a18\u8f09\u5185\u5bb9\u306e\u53d6\u308a\u8fbc\u307f\u3001\u4e00\u90e8\u4fee\u6b63\u3092\u884c\u3063\u305f\u3082\u306e\u306b\u306a\u308a\u307e\u3059\u3002\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u306e\u5168\u4f53\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u5148\u7a0b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u306e mnist \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u76f4\u4e0b\u3078\u914d\u7f6e\u3057\u307e\u3059\u3002\n\u203b tensorflow/tensorflow/examples/tutorials/mnist\ndeep_mnist_softmax.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\n# \u91cd\u307f\u5909\u6570\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n# \u30d0\u30a4\u30a2\u30b9\u5909\u6570\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n# \u7573\u307f\u8fbc\u307f\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n# \u30d7\u30fc\u30ea\u30f3\u30b0\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\ndef main(_):\n  # \u30c7\u30fc\u30bf\u53d6\u5f97\n  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n  # placeholder\u4f5c\u6210\n  x = tf.placeholder(tf.float32, [None, 784])\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  # \u7573\u307f\u8fbc\u307f\uff11\u5c64\u76ee\n  W_conv1 = weight_variable([5, 5, 1, 32])\n  b_conv1 = bias_variable([32])\n  x_image = tf.reshape(x, [-1,28,28,1])\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  h_pool1 = max_pool_2x2(h_conv1)\n\n  # \u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\n  W_conv2 = weight_variable([5, 5, 32, 64])\n  b_conv2 = bias_variable([64])\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n  h_pool2 = max_pool_2x2(h_conv2)\n\n  # \u5168\u7d50\u5408\u5c64\n  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n  b_fc1 = bias_variable([1024])\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n  # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\n  keep_prob = tf.placeholder(tf.float32)\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n  # \u51fa\u529b\u5c64\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\n  W_fc2 = weight_variable([1024, 10])\n  b_fc2 = bias_variable([10])\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n  # \u640d\u5931\u95a2\u6570\uff08\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\uff09\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n\n  # \u52fe\u914d\n  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n  # \u7cbe\u5ea6\n  correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  # \u30bb\u30c3\u30b7\u30e7\u30f3\n  sess = tf.InteractiveSession()\n  sess.run(tf.global_variables_initializer())\n\n  # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n  for i in range(5000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 500 == 0:\n      # \u9014\u4e2d\u7d4c\u904e\uff08500\u4ef6\u3054\u3068\uff09\n      train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n      print(\"step %d, training accuracy %f\" % (i, train_accuracy))\n\n    # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5b9f\u884c\n    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n  # \u8a55\u4fa1\n  print(\"test accuracy %f\" % accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\u4e0a\u8a18\u30b3\u30fc\u30c9\u306e\u51e6\u7406\u306e\u6d41\u308c\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5f62\u72b6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u51e6\u7406\u306e\u6d41\u308c\n\n\n\u5f62\u72b6\n\n\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u8a73\u7d30\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u306e\u8a73\u7d30\u3092\u4ee5\u4e0b\u306b\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n\u91cd\u307f\n\n# \u91cd\u307f\u5909\u6570\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n\u91cd\u307f\u5909\u6570\u3068\u3057\u3066\u3001\u6b63\u898f\u5206\u5e03\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u5024\u3067\u521d\u671f\u5316\u3057\u307e\u3059\u3002\n\n\u30d0\u30a4\u30a2\u30b9\n\n# \u30d0\u30a4\u30a2\u30b9\u5909\u6570\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n\u30d0\u30a4\u30a2\u30b9\u5909\u6570\u3068\u3057\u3066\u3001\u5b9a\u6570(0.1)\u3067\u521d\u671f\u5316\u3057\u307e\u3059\u3002\n\nshape [2, 3] \u306e\u5834\u5408\n[[0.1, 0.1, 0.1],\n[0.1, 0.1, 0.1]]\n\n\n\u7573\u307f\u8fbc\u307f\n\n# \u7573\u307f\u8fbc\u307f\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\u91cd\u307f(\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba) W \u306e\u5f62\u72b6\u3067 \u30b9\u30c8\u30e9\u30a4\u30c9strides \u3068 \u30d1\u30c7\u30a3\u30f3\u30b0padding \u3092\u6307\u5b9a\u3057\u3066\n\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u30d7\u30fc\u30ea\u30f3\u30b0\n\n# \u30d7\u30fc\u30ea\u30f3\u30b0\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba ksize \u306e\u5f62\u72b6\u3067 \u30b9\u30c8\u30e9\u30a4\u30c9strides \u3068 \u30d1\u30c7\u30a3\u30f3\u30b0padding \u3092\u6307\u5b9a\u3057\u3066\n\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3044\u307e\u3059\u3002\n\nksize\uff1a\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba\u6307\u5b9a\u65b9\u6cd5\n 2x2 \u306e\u5834\u5408\u306f [1, 2, 2, 1]\n 3x3 \u306e\u5834\u5408\u306f [1, 3, 3, 1]\n\n\n\u30c7\u30fc\u30bf\u53d6\u5f97\n\n  # \u30c7\u30fc\u30bf\u53d6\u5f97\n  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nMNIST\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\u3092\u884c\u3044\u307e\u3059\u3002\n\nplaceholder\n\n # placeholder\u4f5c\u6210\n x = tf.placeholder(tf.float32, [None, 784])\n y_ = tf.placeholder(tf.float32, [None, 10])\n\n\u5165\u529b\u30c7\u30fc\u30bf\uff1a x \u3068\u3057\u3066 n x 784 \u3092 placeholder \u3068\u3057\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002\n\u30e9\u30d9\u30eb(\u6b63\u89e3)\u30c7\u30fc\u30bf\uff1a y_ \u3068\u3057\u3066 n x 10 \u3092 placeholder \u3068\u3057\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002\nplaceholder \u306f\u3001\u5b9f\u884c\u6642\u306b\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\n784 \u306f 28x28(=784) \u306e\u753b\u50cf\u3092\u4e00\u6b21\u5143\u3068\u3057\u3066\u6271\u3063\u305f\u5834\u5408\u306e\u6570\u5024\u3067\u3059\u3002\n\n\n\u7573\u307f\u8fbc\u307f\u5c64\n\n# \u7573\u307f\u8fbc\u307f\uff11\u5c64\u76ee\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1,28,28,1])\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# \u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n\u3053\u3053\u3067\u306f\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba(5x5)\u3001\u51fa\u529b\u6570 32 \u3067\u7573\u307f\u8fbc\u307f\n\u30d0\u30a4\u30a2\u30b9\u52a0\u7b97\n\u6d3b\u6027\u5316\u95a2\u6570 ReLU \u3092\u9069\u5fdc\n\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba(2x2)\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\n\uff12\u5c64\u76ee\u3078\n\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba(5x5)\u3001\u51fa\u529b\u6570 64 \u3067\u7573\u307f\u8fbc\u307f\n\uff11\u5c64\u76ee\u3068\u540c\u69d8\u306e\u51e6\u7406\u3092\u3057\u3001\u5168\u7d50\u5408\u5c64\u3078\n\n\n\u5168\u7d50\u5408\u5c64\n\n# \u5168\u7d50\u5408\u5c64\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n[7 * 7 * 64, 1024] \u306f\u30017 * 7 \u304c\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30b5\u30a4\u30ba\u3001\n64 \u304c\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u3067\u306e\u51fa\u529b\u6570\u30011024 \u304c\u3001\u5168\u7d50\u5408\u5c64\u306e\u51fa\u529b\u6570\u3068\u306a\u308a\u307e\u3059\u3002\n\u3053\u3053\u3067\u306f\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u4e57\u7b97\u3092\u3059\u308b\u305f\u3081\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u306e\u51fa\u529b\u7d50\u679c\u3092\uff12\u6b21\u5143\u306b\u6574\u5f62\n\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u51fa\u529b(n, 7x7x64) \u3068 \u91cd\u307f(7x7x64, 1024) \u3067\u4e57\u7b97\n\u30d0\u30a4\u30a2\u30b9\u52a0\u7b97\n\u6d3b\u6027\u5316\u95a2\u6570 ReLU \u3092\u9069\u5fdc\n\u6b21\u306e\u5c64\u3078\n\n\n\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\n\n# \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nkeep_prob \u306f\u30c9\u30ed\u30c3\u30d7\u7387\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\u203bplaceholder \u306e\u305f\u3081\u3001\u5b9f\u884c\u6642\u306b\u30c9\u30ed\u30c3\u30d7\u7387\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\n\u51fa\u529b\u5c64\n\n# \u51fa\u529b\u5c64\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n\u51fa\u529b\u3059\u308b\u5206\u985e\u6570\u3067\u3042\u308b 10 \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n\u640d\u5931\u95a2\u6570\u30fb\u52fe\u914d\u30fb\u7cbe\u5ea6\n\n# \u640d\u5931\u95a2\u6570\uff08\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\uff09\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n\n# \u52fe\u914d\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n# \u7cbe\u5ea6\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\u640d\u5931\u95a2\u6570\u3068\u3057\u3066\u3001\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u6307\u5b9a\n\u52fe\u914d\u306f Adam \u3092\u6307\u5b9a\u30021e-4 \u306f\u5b66\u7fd2\u7387\u306b\u306a\u308a\u307e\u3059\u3002\n\u7cbe\u5ea6\u306f\u6b63\u89e3\u306e\u5e73\u5747\uff08\u6b63\u89e3\u6570 / n\uff09\u3068\u306a\u308a\u307e\u3059\u3002\n\n\u30bb\u30c3\u30b7\u30e7\u30f3\n\n# \u30bb\u30c3\u30b7\u30e7\u30f3\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\n\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\nsess.run(tf.global_variables_initializer()) \u306b\u3066 tf.Variable \u3092\u521d\u671f\u5316\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n\n# \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\nfor i in range(5000):\n  batch = mnist.train.next_batch(50)\n\n  if i % 500 == 0:\n    # \u9014\u4e2d\u7d4c\u904e\uff08500\u4ef6\u3054\u3068\uff09\n    train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %f\" % (i, train_accuracy))\n\n  # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5b9f\u884c\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570\u306f\u30015000\u56de\u3092\u8a2d\u5b9a\uff08\u6642\u9593\u304c\u304b\u304b\u308b\u305f\u3081\u5c11\u306a\u3081\u306b\u6307\u5b9a\uff09\n\u4e00\u5ea6\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092 50\u4ef6\u305a\u3064\u8aad\u307f\u8fbc\u307f train_step \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u307e\u305f\u3001\u9014\u4e2d\u7d4c\u904e\u3068\u3057\u3066\u3001500\u56de\u6bce\u306b\u7cbe\u5ea6\u3092 print\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\uff08\u3053\u306e\u9014\u4e2d\u7d4c\u904e\u306e\u7cbe\u5ea6\u51fa\u529b\u306f\u3001\u7b97\u51fa\u30c7\u30fc\u30bf\u3068\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092\u305d\u306e\u307e\u307e\u4f7f\u3063\u3066\u304a\u308a\u3001\n\u3000\u304b\u3064\u3001\u30c7\u30fc\u30bf\u4ef6\u6570\u308250\u4ef6\u3068\u5c11\u306a\u3044\u305f\u3081\u4fe1\u983c\u5ea6\u306f\u4f4e\u304f\u306a\u3063\u3066\u3044\u307e\u3059\uff09\n\n\u88dc\u8db3\n\u30fbmnist.train.next_batch() \u306f\u3001\u30c7\u30fc\u30bf\u3092\u6700\u5f8c\u307e\u3067\u8aad\u307f\u8fbc\u3093\u3060\u6642\u70b9\u3067\u30c7\u30fc\u30bf\u306e\u30b7\u30e3\u30c3\u30d5\u30eb\u3092\u884c\u3044\u3001\n\u3000\u518d\u3073\u5148\u982d\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\u30fbfeed_dict={x: batch[0], y_: batch[1] \u306f\u3001placeholder \u306e\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\u30fbkeep_prob: 0.5 \u306f\u3001\u30c9\u30ed\u30c3\u30d7\u7387 50% \u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u30011.0 \u3092\u6307\u5b9a\u3059\u308b\u3068\u30c9\u30ed\u30c3\u30d7\u3057\u307e\u305b\u3093\u3002\n\u3000\u8a55\u4fa1\u3084\u4e88\u6e2c\u6642\u306b\u306f\u30011.0 \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n\n\u8a55\u4fa1\n\n# \u8a55\u4fa1\nprint(\"test accuracy %f\" % accuracy.eval(feed_dict={\n  x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\n\u3053\u3053\u3067\u306f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf 10,000\u4ef6 \u3092\u4f7f\u3063\u3066\u7cbe\u5ea6\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002\nkeep_prob \u306f\u30011.0 \u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u5b9f\u884c\n\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\npython deep_mnist_softmax.py\n\n\u203b\u524d\u56de\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u3067\u306e\u4f5c\u6210\u3057\u305f\u74b0\u5883\u3067\u5b9f\u884c\u3059\u308b\u5834\u5408\u306f\u3001\u4eee\u60f3\u74b0\u5883\u3092\u8d77\u52d5\u5f8c\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n\u7d50\u679c\n\u5b9f\u884c\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\u7cbe\u5ea6\u306f\u300198.57% \u307e\u3067\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3082\u3046\u5c11\u3057\u7cbe\u5ea6\u304c\u4e0a\u304c\u308b\u306f\u305a\u3067\u3059\u3002\n\n\n\u521d\u56de\u306f\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u5c11\u3057\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u4eca\u56de\u306f MNIST \u3092\u4f7f\u3063\u305f\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u6570\u5b57\u5206\u985e\u3092\u884c\u3044\u307e\u3057\u305f\u3002\n\u4eca\u56de\u306f\u3001MNIST \u3092\u4f7f\u3063\u305f\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u6570\u5b57\u306e\u5206\u985e\u3092\u884c\u3044\u307e\u3059\u3002\n\n## MNIST\nMNIST \u306f\u30010\u301c9 \u307e\u3067\u306e\u624b\u66f8\u304d\u6587\u5b57\u753b\u50cf\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u3001\u753b\u50cf\u30b5\u30a4\u30ba 28x28 \u306e\u30c7\u30fc\u30bf\u304c\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3068\u3057\u3066 60,000\u4ef6\u3001\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u3057\u3066 10,000\u4ef6 \u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\u53c8\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30c7\u30fc\u30bf\u3082\u540c\u6570\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\n![mnist_sample.png](https://qiita-image-store.s3.amazonaws.com/0/134550/7dba725a-92f0-1e2d-f867-9822e4ed3578.png)\n\n\n\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f7f\u3044\u3001\u5bfe\u8c61\u306e\u753b\u50cf\u306e\u6570\u5b57\u304c\u4f55\u3067\u3042\u308b\u304b\u3092\u6c42\u3081\u307e\u3059\u3002\n\n## \u4e8b\u524d\u6e96\u5099\n\u4e88\u3081\u306e [MNIST \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist)\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\n## \u5b9f\u88c5\u30b3\u30fc\u30c9\u5168\u4f53\n\n\u5b9f\u88c5\u5185\u5bb9\u306f\u3001TensorFlow\u306e [MNIST \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist)\u3092\u30d9\u30fc\u30b9\u306b\n[Deep MNIST for Experts](https://www.tensorflow.org/tutorials/mnist/pros/) \u306e\u8a18\u8f09\u5185\u5bb9\u306e\u53d6\u308a\u8fbc\u307f\u3001\u4e00\u90e8\u4fee\u6b63\u3092\u884c\u3063\u305f\u3082\u306e\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u306e\u5168\u4f53\u306f\u4ee5\u4e0b\u3068\u306a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u5148\u7a0b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u306e mnist \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u76f4\u4e0b\u3078\u914d\u7f6e\u3057\u307e\u3059\u3002\n\u203b tensorflow/tensorflow/examples/tutorials/mnist\n\ndeep_mnist_softmax.py\n\n```python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\n# \u91cd\u307f\u5909\u6570\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n# \u30d0\u30a4\u30a2\u30b9\u5909\u6570\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n# \u7573\u307f\u8fbc\u307f\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n# \u30d7\u30fc\u30ea\u30f3\u30b0\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\ndef main(_):\n  # \u30c7\u30fc\u30bf\u53d6\u5f97\n  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n  # placeholder\u4f5c\u6210\n  x = tf.placeholder(tf.float32, [None, 784])\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  # \u7573\u307f\u8fbc\u307f\uff11\u5c64\u76ee\n  W_conv1 = weight_variable([5, 5, 1, 32])\n  b_conv1 = bias_variable([32])\n  x_image = tf.reshape(x, [-1,28,28,1])\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  h_pool1 = max_pool_2x2(h_conv1)\n\n  # \u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\n  W_conv2 = weight_variable([5, 5, 32, 64])\n  b_conv2 = bias_variable([64])\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n  h_pool2 = max_pool_2x2(h_conv2)\n\n  # \u5168\u7d50\u5408\u5c64\n  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n  b_fc1 = bias_variable([1024])\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n  # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\n  keep_prob = tf.placeholder(tf.float32)\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n  # \u51fa\u529b\u5c64\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\n  W_fc2 = weight_variable([1024, 10])\n  b_fc2 = bias_variable([10])\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n  # \u640d\u5931\u95a2\u6570\uff08\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\uff09\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n\n  # \u52fe\u914d\n  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n  # \u7cbe\u5ea6\n  correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  # \u30bb\u30c3\u30b7\u30e7\u30f3\n  sess = tf.InteractiveSession()\n  sess.run(tf.global_variables_initializer())\n\n  # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n  for i in range(5000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 500 == 0:\n      # \u9014\u4e2d\u7d4c\u904e\uff08500\u4ef6\u3054\u3068\uff09\n      train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n      print(\"step %d, training accuracy %f\" % (i, train_accuracy))\n\n    # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5b9f\u884c\n    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n  # \u8a55\u4fa1\n  print(\"test accuracy %f\" % accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\n```\n\n## \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\n\u4e0a\u8a18\u30b3\u30fc\u30c9\u306e\u51e6\u7406\u306e\u6d41\u308c\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5f62\u72b6\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\n#### \u51e6\u7406\u306e\u6d41\u308c\n![nn_line.png](https://qiita-image-store.s3.amazonaws.com/0/134550/da59359b-f43b-0a9c-65d8-947a9ae0262e.png)\n\n#### \u5f62\u72b6\n\n![nn_shape.png](https://qiita-image-store.s3.amazonaws.com/0/134550/9e489a42-b8ef-5e98-0e2a-2d13eea46ec2.png)\n\n## \u5b9f\u88c5\u30b3\u30fc\u30c9\u8a73\u7d30\n\n\u5b9f\u88c5\u30b3\u30fc\u30c9\u306e\u8a73\u7d30\u3092\u4ee5\u4e0b\u306b\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n- \u91cd\u307f\n\n```python\n# \u91cd\u307f\u5909\u6570\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n```\n\u91cd\u307f\u5909\u6570\u3068\u3057\u3066\u3001\u6b63\u898f\u5206\u5e03\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u5024\u3067\u521d\u671f\u5316\u3057\u307e\u3059\u3002\n\n- \u30d0\u30a4\u30a2\u30b9\n\n```python\n# \u30d0\u30a4\u30a2\u30b9\u5909\u6570\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n```\n\u30d0\u30a4\u30a2\u30b9\u5909\u6570\u3068\u3057\u3066\u3001\u5b9a\u6570(0.1)\u3067\u521d\u671f\u5316\u3057\u307e\u3059\u3002\n>shape [2, 3] \u306e\u5834\u5408\n[[0.1, 0.1, 0.1],\n[0.1, 0.1, 0.1]]\n\n- \u7573\u307f\u8fbc\u307f\n\n```python\n# \u7573\u307f\u8fbc\u307f\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n```\n\u91cd\u307f(\u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba) ```W``` \u306e\u5f62\u72b6\u3067 \u30b9\u30c8\u30e9\u30a4\u30c9```strides``` \u3068 \u30d1\u30c7\u30a3\u30f3\u30b0```padding``` \u3092\u6307\u5b9a\u3057\u3066\n\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u307e\u3059\u3002\n\n- \u30d7\u30fc\u30ea\u30f3\u30b0\n\n```python\n# \u30d7\u30fc\u30ea\u30f3\u30b0\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n```\n\n\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba ```ksize``` \u306e\u5f62\u72b6\u3067 \u30b9\u30c8\u30e9\u30a4\u30c9```strides``` \u3068 \u30d1\u30c7\u30a3\u30f3\u30b0```padding``` \u3092\u6307\u5b9a\u3057\u3066\n\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u884c\u3044\u307e\u3059\u3002\n>ksize\uff1a\u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba\u6307\u5b9a\u65b9\u6cd5\n 2x2 \u306e\u5834\u5408\u306f [1, 2, 2, 1]\n 3x3 \u306e\u5834\u5408\u306f [1, 3, 3, 1]\n\n- \u30c7\u30fc\u30bf\u53d6\u5f97\n\n```python\n  # \u30c7\u30fc\u30bf\u53d6\u5f97\n  mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n```\nMNIST\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\u3092\u884c\u3044\u307e\u3059\u3002\n\n- placeholder\n\n```python\n # placeholder\u4f5c\u6210\n x = tf.placeholder(tf.float32, [None, 784])\n y_ = tf.placeholder(tf.float32, [None, 10])\n```\n\u5165\u529b\u30c7\u30fc\u30bf\uff1a ```x``` \u3068\u3057\u3066 n x 784 \u3092 placeholder \u3068\u3057\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002\n\u30e9\u30d9\u30eb(\u6b63\u89e3)\u30c7\u30fc\u30bf\uff1a ```y_``` \u3068\u3057\u3066 n x 10 \u3092 placeholder \u3068\u3057\u3066\u4f5c\u6210\u3057\u307e\u3059\u3002\nplaceholder \u306f\u3001\u5b9f\u884c\u6642\u306b\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\n>784 \u306f 28x28(=784) \u306e\u753b\u50cf\u3092\u4e00\u6b21\u5143\u3068\u3057\u3066\u6271\u3063\u305f\u5834\u5408\u306e\u6570\u5024\u3067\u3059\u3002\n\n- \u7573\u307f\u8fbc\u307f\u5c64\n\n```python\n# \u7573\u307f\u8fbc\u307f\uff11\u5c64\u76ee\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nx_image = tf.reshape(x, [-1,28,28,1])\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# \u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n```\n\u3053\u3053\u3067\u306f\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n1. \u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba(5x5)\u3001\u51fa\u529b\u6570 32 \u3067\u7573\u307f\u8fbc\u307f\n2. \u30d0\u30a4\u30a2\u30b9\u52a0\u7b97\n3. \u6d3b\u6027\u5316\u95a2\u6570 ReLU \u3092\u9069\u5fdc\n4. \u30d7\u30fc\u30ea\u30f3\u30b0\u30b5\u30a4\u30ba(2x2)\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\n5. \uff12\u5c64\u76ee\u3078\n6. \u30d5\u30a3\u30eb\u30bf\u30fc\u30b5\u30a4\u30ba(5x5)\u3001\u51fa\u529b\u6570 64 \u3067\u7573\u307f\u8fbc\u307f\n7. \uff11\u5c64\u76ee\u3068\u540c\u69d8\u306e\u51e6\u7406\u3092\u3057\u3001\u5168\u7d50\u5408\u5c64\u3078\n\n\n- \u5168\u7d50\u5408\u5c64\n\n```python\n# \u5168\u7d50\u5408\u5c64\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n```\n\n```[7 * 7 * 64, 1024]``` \u306f\u3001```7 * 7``` \u304c\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30b5\u30a4\u30ba\u3001\n```64``` \u304c\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u3067\u306e\u51fa\u529b\u6570\u3001```1024``` \u304c\u3001\u5168\u7d50\u5408\u5c64\u306e\u51fa\u529b\u6570\u3068\u306a\u308a\u307e\u3059\u3002\n\n\u3053\u3053\u3067\u306f\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n1. \u4e57\u7b97\u3092\u3059\u308b\u305f\u3081\u3001\u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u306e\u51fa\u529b\u7d50\u679c\u3092\uff12\u6b21\u5143\u306b\u6574\u5f62\n2. \u7573\u307f\u8fbc\u307f\uff12\u5c64\u76ee\u51fa\u529b(n, 7x7x64) \u3068 \u91cd\u307f(7x7x64, 1024) \u3067\u4e57\u7b97\n2. \u30d0\u30a4\u30a2\u30b9\u52a0\u7b97\n3. \u6d3b\u6027\u5316\u95a2\u6570 ReLU \u3092\u9069\u5fdc\n4. \u6b21\u306e\u5c64\u3078\n\n\n- \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\n\n```python\n# \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n```\n\n```keep_prob``` \u306f\u30c9\u30ed\u30c3\u30d7\u7387\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\u203bplaceholder \u306e\u305f\u3081\u3001\u5b9f\u884c\u6642\u306b\u30c9\u30ed\u30c3\u30d7\u7387\u3092\u5165\u529b\u3057\u307e\u3059\u3002\n\n- \u51fa\u529b\u5c64\n\n```python\n# \u51fa\u529b\u5c64\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n```\n\n\u51fa\u529b\u3059\u308b\u5206\u985e\u6570\u3067\u3042\u308b ```10``` \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n- \u640d\u5931\u95a2\u6570\u30fb\u52fe\u914d\u30fb\u7cbe\u5ea6\n\n```python\n# \u640d\u5931\u95a2\u6570\uff08\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\uff09\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n\n# \u52fe\u914d\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n# \u7cbe\u5ea6\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n```\n\n\u640d\u5931\u95a2\u6570\u3068\u3057\u3066\u3001\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u6307\u5b9a\n\u52fe\u914d\u306f Adam \u3092\u6307\u5b9a\u3002```1e-4``` \u306f\u5b66\u7fd2\u7387\u306b\u306a\u308a\u307e\u3059\u3002\n\u7cbe\u5ea6\u306f\u6b63\u89e3\u306e\u5e73\u5747\uff08\u6b63\u89e3\u6570 / n\uff09\u3068\u306a\u308a\u307e\u3059\u3002\n\n\n - \u30bb\u30c3\u30b7\u30e7\u30f3\n\n```python\n# \u30bb\u30c3\u30b7\u30e7\u30f3\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n```\n\n\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\n```sess.run(tf.global_variables_initializer())``` \u306b\u3066 tf.Variable \u3092\u521d\u671f\u5316\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n- \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n\n```python\n# \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\nfor i in range(5000):\n  batch = mnist.train.next_batch(50)\n\n  if i % 500 == 0:\n    # \u9014\u4e2d\u7d4c\u904e\uff08500\u4ef6\u3054\u3068\uff09\n    train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %f\" % (i, train_accuracy))\n\n  # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5b9f\u884c\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n```\n\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570\u306f\u30015000\u56de\u3092\u8a2d\u5b9a\uff08\u6642\u9593\u304c\u304b\u304b\u308b\u305f\u3081\u5c11\u306a\u3081\u306b\u6307\u5b9a\uff09\n\u4e00\u5ea6\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092 50\u4ef6\u305a\u3064\u8aad\u307f\u8fbc\u307f ```train_step``` \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u307e\u305f\u3001\u9014\u4e2d\u7d4c\u904e\u3068\u3057\u3066\u3001500\u56de\u6bce\u306b\u7cbe\u5ea6\u3092 print\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\uff08\u3053\u306e\u9014\u4e2d\u7d4c\u904e\u306e\u7cbe\u5ea6\u51fa\u529b\u306f\u3001\u7b97\u51fa\u30c7\u30fc\u30bf\u3068\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u3092\u305d\u306e\u307e\u307e\u4f7f\u3063\u3066\u304a\u308a\u3001\n\u3000\u304b\u3064\u3001\u30c7\u30fc\u30bf\u4ef6\u6570\u308250\u4ef6\u3068\u5c11\u306a\u3044\u305f\u3081\u4fe1\u983c\u5ea6\u306f\u4f4e\u304f\u306a\u3063\u3066\u3044\u307e\u3059\uff09\n\n>\u88dc\u8db3\n\u30fb```mnist.train.next_batch()``` \u306f\u3001\u30c7\u30fc\u30bf\u3092\u6700\u5f8c\u307e\u3067\u8aad\u307f\u8fbc\u3093\u3060\u6642\u70b9\u3067\u30c7\u30fc\u30bf\u306e\u30b7\u30e3\u30c3\u30d5\u30eb\u3092\u884c\u3044\u3001\n\u3000\u518d\u3073\u5148\u982d\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002\n\u30fb```feed_dict={x: batch[0], y_: batch[1]``` \u306f\u3001placeholder \u306e\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u3044\u307e\u3059\u3002\n\u30fb```keep_prob: 0.5``` \u306f\u3001\u30c9\u30ed\u30c3\u30d7\u7387 50% \u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u30011.0 \u3092\u6307\u5b9a\u3059\u308b\u3068\u30c9\u30ed\u30c3\u30d7\u3057\u307e\u305b\u3093\u3002\n\u3000\u8a55\u4fa1\u3084\u4e88\u6e2c\u6642\u306b\u306f\u30011.0 \u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n\n- \u8a55\u4fa1\n\n```python\n# \u8a55\u4fa1\nprint(\"test accuracy %f\" % accuracy.eval(feed_dict={\n  x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n```\n\n\u3053\u3053\u3067\u306f\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf 10,000\u4ef6 \u3092\u4f7f\u3063\u3066\u7cbe\u5ea6\u3092\u7b97\u51fa\u3057\u307e\u3059\u3002\n```keep_prob``` \u306f\u30011.0 \u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n## \u5b9f\u884c\n\n\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```\npython deep_mnist_softmax.py\n```\n\u203b\u524d\u56de\u306e[\u30a8\u30f3\u30c8\u30ea\u30fc](http://qiita.com/fujin/items/93aa9144d756eb85004d)\u3067\u306e\u4f5c\u6210\u3057\u305f\u74b0\u5883\u3067\u5b9f\u884c\u3059\u308b\u5834\u5408\u306f\u3001\u4eee\u60f3\u74b0\u5883\u3092\u8d77\u52d5\u5f8c\u306b\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n## \u7d50\u679c\n\n\u5b9f\u884c\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\n\u7cbe\u5ea6\u306f\u300198.57% \u307e\u3067\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002\n\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u3082\u3046\u5c11\u3057\u7cbe\u5ea6\u304c\u4e0a\u304c\u308b\u306f\u305a\u3067\u3059\u3002\n\n![sc_2017-02-001.png](https://qiita-image-store.s3.amazonaws.com/0/134550/55eafe53-65bd-e8bd-b73c-1e66b96f2ee0.png)\n\n>\u521d\u56de\u306f\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u5c11\u3057\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u4eca\u56de\u306f MNIST \u3092\u4f7f\u3063\u305f\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u6570\u5b57\u5206\u985e\u3092\u884c\u3044\u307e\u3057\u305f\u3002\n", "tags": ["TensorFlow", "DeepLearning", "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0", "\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af", "Python"]}