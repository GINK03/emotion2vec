{"context": "\n\u3068\u3044\u3046\u3053\u3068\u3067Chainer\u306e\u5f37\u5316\u5b66\u7fd2\u7248ChainerRL\u304c\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u3002\n\u3053\u306e\u3068\u3053\u308d\u624b\u3092\u5e83\u3052\u3059\u304e\u3066\u3044\u3066\u4e2d\u3005\u4e01\u5be7\u306a\u4ed5\u4e8b\u304c\u3067\u304d\u306a\u3044\u4e2d\u3001\n30\u5206 x 2\u65e5\u3067Double DQN\u3067\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3067\u304d\u307e\u3057\u305f\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u4fbf\u5229\u3067\u52a9\u304b\u308b\uff01\n\n\u4f7f\u7528\u74b0\u5883\n\nWindows 10 - 64bit\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000<\u8ae6\u3081\u304c\u60aa\u3044\u5fc3\u306e\u5f37\u3055\u2026\uff01>\nPython 3.6.0 |Anaconda 4.3.0 (64-bit) \u3000\u3000\u3000\u3000<\u30d7\u30e9\u30a4\u30c9\u3088\u308a\u5b9f\u5229\uff01>\n\n\nChainerRL 0.2 \u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000< Dependency\u7de9\u304f\u3057\u3066\u307b\u3057\u3044>\nChainer 1.19 \u3000\u3000        \u3000\u3000\u3000\u3000\u3000\u3000\u3000<1.20\u3067Theano\u306e\u304a\u4e16\u8a71\u3067\u304d\u3066\u306a\u3044!\u4f1a\u793e\u3067\u306fTheano\u52d5\u3044\u305f\u306e\u306b\u3002>\ncached_property\ngym (0.7.3)\ngym environment \u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u306e\u74b0\u5883\u3092OpenAI I/F\u7684\u306b\u3057\u305f - Qiita\n\n\n\n\n\n\u74b0\u5883&\u30b5\u30f3\u30d7\u30eb \u30ea\u30dd\u30b8\u30c8\u30ea\nChachay/Gym_LineFollower: Simple Open AI gym like Environment\nGUI tool kit\u3092Qt5\u5316\u3057\u307e\u3057\u305f\n\nChainerRL\npip\u3067\u5165\u308c\u3088\u3046\u3068\u3059\u308b\u3068 Arcade Learning Environment\u5165\u308c\u3088\u3046\u3068\u3057\u3066\u30b3\u30b1\u308b\u3002\nALE\u3092Windows\u306b\u7a81\u3063\u8fbc\u3080\u3068\u3053\u308d\u306b\u6642\u9593\u3092\u304b\u3051\u3066\u3082\u3044\u3044\u3051\u3069\u3001\nWindows\u3067pong\u3057\u305f\u3044\u3068\u3082\u601d\u308f\u306a\u3044\u306e\u3067\u3001\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\uff01\uff01\n\n1. GitHub\u304b\u3089\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u843d\u3068\u3059\nGitHub - pfnet/chainerrl: ChainerRL is a deep reinforcement learning library built on top of Chainer.\n\n2. \u540c\u68b1\u306eChainerRL\u30d5\u30a9\u30eb\u30c0\u3092\u4f5c\u696d\u30d5\u30a9\u30eb\u30c0\u306b\u79fb\u3059\n\u4ee5\u964d\u306eexampl2.py\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u306d\n\n\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u74b0\u5883\u5b9a\u7fa9\n\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u306e\u74b0\u5883\u3092OpenAI I/F\u7684\u306b\u3057\u305f - Qiita\u306e\u74b0\u5883\u306f\u3001\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u304c\u53d7\u3051\u53d6\u308b\u8eca\u8f2a\u901f\u5ea6\u3092\u9023\u7d9a\u5024\u3067\u74b0\u5883\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u3092\u96e2\u6563\u5024\u306b\u3059\u308b\u305f\u3081\u3001\u7d99\u627f\u3067\u96e2\u6563\u5024\u74b0\u5883\u306b\u3057\u306a\u304a\u3057\u307e\u3059\u3002\n\u540c\u6642\u306b\u4e00\u3064\u76ee\u30bb\u30f3\u30b7\u30f3\u30b0\u30924\u30b9\u30c6\u30c3\u30d7\u5206\u306e\u5c65\u6b74\u89b3\u6e2c\u306b\u3057\u307e\u3059\u3002\nclass SState(object):\n    def __init__(self, STATE_NUM, DIM):\n        self.STATE_NUM = STATE_NUM\n        self.DIM = DIM\n        self.seq = np.zeros((STATE_NUM, DIM), dtype=np.float32)\n\n    def push_s(self, state):\n        self.seq[1:self.STATE_NUM ] = self.seq[0:self.STATE_NUM -1]\n        self.seq[0] = state\n\n    def reset(self):\n        self.seq = np.zeros_like(self.seq)\n\n    def fill_s(self, state):\n        for i in range(0, self.STATE_NUM):\n            self.seq[i] = state\n\n# Define Agent And Environment\nclass LineTracerEnvDiscrete(LineTracerEnv):\n    actions = np.array([[0.5, 0.1], [1.0, 1.0],  [0.1, 0.5]]) \n    OBSDIM = 4\n\n    def __init__(self):\n        self.action_space_d = spaces.Discrete(self.actions.shape[0])\n        self.observation_space_d = spaces.Discrete(self.OBSDIM)\n        self.MyState = SState(self.OBSDIM, 1)\n\n        super().__init__()\n\n    def _step(self, action):\n        tempState, tmpReward, tmpDone, tmpInfo = super()._step(self.actions[action])\n        self.MyState.push_s(tempState)\n        return self.MyState.seq.flatten(), tmpReward, tmpDone, tmpInfo\n\n    def _reset(self):\n        s = super()._reset()\n        self.MyState.reset()\n        return self.MyState.seq.flatten()\n\nenv = LineTracerEnvDiscrete()\n\n\u3042\u3068\u306f\u85e4\u7530\u3055\u3093\uff08@mooopann)\u306e\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8\u304b\u3089\u30b3\u30d4\u30da\u30fb\u30b3\u30d4\u30da\nenv.OBSDIM, env.action_space_d.n\u3068env.action_space_d.sample\u3060\u3051\u82e5\u5e72\u6ce8\u610f\u3002\nq_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n    env.OBSDIM, env.action_space_d.n,\n    n_hidden_layers=2, n_hidden_channels=50)\n\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n\ngamma = 0.95\n\nexplorer = chainerrl.explorers.ConstantEpsilonGreedy(\n    epsilon=0.3, random_action_func=env.action_space_d.sample)\n\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n\nagent = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n\n\n\u5b9f\u884c\n\u52d5\u304f\u2026\uff01 \npython example2.py\n\n\n\u203b \u5468\u56de\u8def\u56de\u308c\u308b\u3088\u3046\u306a\u884c\u52d5\u3092\u5b66\u7fd2\u3067\u304d\u305f\u304b\u306f\u2026\u8a66\u3057\u3066\u306a\u3044\u3063\u3059\n   \u305d\u306e\u3042\u305f\u308a\u306f\u904e\u53bb\u8a18\u4e8b\u3092\u2026 \u21d2 \u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b - Chainer - Qiita\n\n\u611f\u60f3\n\n\u521d\u65e5:\u6b63\u547330\u5206\n\n\n2\u65e5\u76ee\uff1a\u6b63\u547330\u5206\n\nEnv\u306e\u7d99\u627f\u30af\u30e9\u30b9\u4f5c\u6210\n\u52d5\u3044\u305f\n\n\n\u307e\u3068\u3081\n\n\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3065\u304f\u308a\u304c\u6357\u308a\u307e\u304f\u3063\u3066\u7d20\u6674\u3089\u3057\u3044\npip\u306b\u3066Windows\u3067\u8e74\u3089\u308c\u306a\u3044\u3088\u3046\u306a\u6c17\u3065\u304b\u3044\u306f\u3044\u305f\u3060\u304d\u305f\u3044\n\u3084\u3063\u3071\u308a\u2193\u306f\u3042\u308b\n\n\n\n\u30b9\u30af\u30ea\u30d7\u30c8\nChachay/Gym_LineFollower: Simple Open AI gym like Environment\nexample2.py\u53c2\u7167\u306e\u3053\u3068\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"ja\" dir=\"ltr\">\u66f8\u304d\u307e\u3057\u305f <a href=\"https://t.co/m4YhZolDjD\">https://t.co/m4YhZolDjD</a></p>&mdash; mooopan (@mooopan) <a href=\"https://twitter.com/mooopan/status/833624161083760640\">February 20, 2017</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\u3068\u3044\u3046\u3053\u3068\u3067Chainer\u306e\u5f37\u5316\u5b66\u7fd2\u7248ChainerRL\u304c\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3057\u305f\u3002\n\n\u3053\u306e\u3068\u3053\u308d\u624b\u3092\u5e83\u3052\u3059\u304e\u3066\u3044\u3066\u4e2d\u3005\u4e01\u5be7\u306a\u4ed5\u4e8b\u304c\u3067\u304d\u306a\u3044\u4e2d\u3001\n30\u5206 x 2\u65e5\u3067Double DQN\u3067\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3067\u304d\u307e\u3057\u305f\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u4fbf\u5229\u3067\u52a9\u304b\u308b\uff01\n\n# \u4f7f\u7528\u74b0\u5883\n\n* Windows 10 - 64bit\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000<\u8ae6\u3081\u304c\u60aa\u3044\u5fc3\u306e\u5f37\u3055\u2026\uff01>\n* Python 3.6.0 |Anaconda 4.3.0 (64-bit) \u3000\u3000\u3000\u3000<\u30d7\u30e9\u30a4\u30c9\u3088\u308a\u5b9f\u5229\uff01>\n    - ChainerRL 0.2 \u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000\u3000< Dependency\u7de9\u304f\u3057\u3066\u307b\u3057\u3044>\n    - Chainer 1.19 \u3000\u3000        \u3000\u3000\u3000\u3000\u3000\u3000\u3000<1.20\u3067Theano\u306e\u304a\u4e16\u8a71\u3067\u304d\u3066\u306a\u3044!\u4f1a\u793e\u3067\u306fTheano\u52d5\u3044\u305f\u306e\u306b\u3002>\n    - cached_property\n    - gym (0.7.3)\n    - gym environment [\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u306e\u74b0\u5883\u3092OpenAI I/F\u7684\u306b\u3057\u305f \\- Qiita](http://qiita.com/chachay/items/aa7b94f22163905b4c2b)\n\n# \u74b0\u5883&\u30b5\u30f3\u30d7\u30eb \u30ea\u30dd\u30b8\u30c8\u30ea\n[Chachay/Gym\\_LineFollower: Simple Open AI gym like Environment](https://github.com/Chachay/Gym_LineFollower)\n\nGUI tool kit\u3092Qt5\u5316\u3057\u307e\u3057\u305f\n\n# ChainerRL\n\npip\u3067\u5165\u308c\u3088\u3046\u3068\u3059\u308b\u3068 Arcade Learning Environment\u5165\u308c\u3088\u3046\u3068\u3057\u3066\u30b3\u30b1\u308b\u3002\nALE\u3092Windows\u306b\u7a81\u3063\u8fbc\u3080\u3068\u3053\u308d\u306b\u6642\u9593\u3092\u304b\u3051\u3066\u3082\u3044\u3044\u3051\u3069\u3001\nWindows\u3067pong\u3057\u305f\u3044\u3068\u3082\u601d\u308f\u306a\u3044\u306e\u3067\u3001\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\uff01\uff01\n\n## 1. GitHub\u304b\u3089\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u843d\u3068\u3059\n\n[GitHub \\- pfnet/chainerrl: ChainerRL is a deep reinforcement learning library built on top of Chainer\\.](https://github.com/pfnet/chainerrl)\n\n## 2. \u540c\u68b1\u306eChainerRL\u30d5\u30a9\u30eb\u30c0\u3092\u4f5c\u696d\u30d5\u30a9\u30eb\u30c0\u306b\u79fb\u3059\n\n\u4ee5\u964d\u306eexampl2.py\u3068\u540c\u3058\u30d5\u30a9\u30eb\u30c0\u306d\n\n# \u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u74b0\u5883\u5b9a\u7fa9\n[\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u306e\u74b0\u5883\u3092OpenAI I/F\u7684\u306b\u3057\u305f \\- Qiita](http://qiita.com/chachay/items/aa7b94f22163905b4c2b)\u306e\u74b0\u5883\u306f\u3001\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u304c\u53d7\u3051\u53d6\u308b\u8eca\u8f2a\u901f\u5ea6\u3092\u9023\u7d9a\u5024\u3067\u74b0\u5883\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u308c\u3092\u96e2\u6563\u5024\u306b\u3059\u308b\u305f\u3081\u3001\u7d99\u627f\u3067\u96e2\u6563\u5024\u74b0\u5883\u306b\u3057\u306a\u304a\u3057\u307e\u3059\u3002\n\u540c\u6642\u306b\u4e00\u3064\u76ee\u30bb\u30f3\u30b7\u30f3\u30b0\u30924\u30b9\u30c6\u30c3\u30d7\u5206\u306e\u5c65\u6b74\u89b3\u6e2c\u306b\u3057\u307e\u3059\u3002\n\n```python\nclass SState(object):\n    def __init__(self, STATE_NUM, DIM):\n        self.STATE_NUM = STATE_NUM\n        self.DIM = DIM\n        self.seq = np.zeros((STATE_NUM, DIM), dtype=np.float32)\n        \n    def push_s(self, state):\n        self.seq[1:self.STATE_NUM ] = self.seq[0:self.STATE_NUM -1]\n        self.seq[0] = state\n        \n    def reset(self):\n        self.seq = np.zeros_like(self.seq)\n        \n    def fill_s(self, state):\n        for i in range(0, self.STATE_NUM):\n            self.seq[i] = state\n\n# Define Agent And Environment\nclass LineTracerEnvDiscrete(LineTracerEnv):\n    actions = np.array([[0.5, 0.1], [1.0, 1.0],  [0.1, 0.5]]) \n    OBSDIM = 4\n    \n    def __init__(self):\n        self.action_space_d = spaces.Discrete(self.actions.shape[0])\n        self.observation_space_d = spaces.Discrete(self.OBSDIM)\n        self.MyState = SState(self.OBSDIM, 1)\n        \n        super().__init__()\n\n    def _step(self, action):\n        tempState, tmpReward, tmpDone, tmpInfo = super()._step(self.actions[action])\n        self.MyState.push_s(tempState)\n        return self.MyState.seq.flatten(), tmpReward, tmpDone, tmpInfo\n        \n    def _reset(self):\n        s = super()._reset()\n        self.MyState.reset()\n        return self.MyState.seq.flatten()\n\nenv = LineTracerEnvDiscrete()\n```\n\n\u3042\u3068\u306f\u85e4\u7530\u3055\u3093\uff08@mooopann)\u306e[\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8](https://github.com/pfnet/chainerrl/blob/master/examples/quickstart/quickstart.ipynb)\u304b\u3089\u30b3\u30d4\u30da\u30fb\u30b3\u30d4\u30da\n\nenv.OBSDIM, env.action_space_d.n\u3068env.action_space_d.sample\u3060\u3051\u82e5\u5e72\u6ce8\u610f\u3002\n\n```python\nq_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n    env.OBSDIM, env.action_space_d.n,\n    n_hidden_layers=2, n_hidden_channels=50)\n\noptimizer = chainer.optimizers.Adam(eps=1e-2)\noptimizer.setup(q_func)\n\ngamma = 0.95\n\nexplorer = chainerrl.explorers.ConstantEpsilonGreedy(\n    epsilon=0.3, random_action_func=env.action_space_d.sample)\n\nreplay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n\nagent = chainerrl.agents.DoubleDQN(\n    q_func, optimizer, replay_buffer, gamma, explorer,\n    replay_start_size=500, update_frequency=1,\n    target_update_frequency=100)\n```\n\n# \u5b9f\u884c\n\n\u52d5\u304f\u2026\uff01 \n\n```python\npython example2.py\n```\n\n![20170221.png](https://qiita-image-store.s3.amazonaws.com/0/117379/d7b3e6f7-3bab-fb22-491a-e9a7ebf1a1a0.png)\n\n\u203b \u5468\u56de\u8def\u56de\u308c\u308b\u3088\u3046\u306a\u884c\u52d5\u3092\u5b66\u7fd2\u3067\u304d\u305f\u304b\u306f\u2026\u8a66\u3057\u3066\u306a\u3044\u3063\u3059\n   \u305d\u306e\u3042\u305f\u308a\u306f\u904e\u53bb\u8a18\u4e8b\u3092\u2026 \u21d2 [\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30b5\u30fc\u3092Deep Q Learning\u3067\u6559\u80b2\u3059\u308b \\- Chainer \\- Qiita](http://qiita.com/chachay/items/555638e3079fce9d59c9)\n\n# \u611f\u60f3\n\n## \u521d\u65e5:\u6b63\u547330\u5206\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"ja\" dir=\"ltr\">ChainerRL\u4f7f\u304a\u3046\u3068\u3057\u305f\u3051\u3069\u3001\u898b\u672c\u306e\u66f8\u304d\u65b9\u304c<a href=\"https://t.co/cDBp9FgwA5\">https://t.co/cDBp9FgwA5</a>\u3067\u306f\u306a\u3044\u3068\u3053\u308d\u307e\u3067\u7406\u89e3\u3057\u3066\u7d42\u308f\u3063\u3066\u3057\u307e\u3063\u305f\u3002ENV\u74b0\u5883\u3092\u9023\u7d9a\u5024\u3067\u4f5c\u3063\u3068\u3044\u3066\u306a\u3093\u3060\u3051\u3069\u3001\u96e2\u6563\u5024\u306b\u5b9a\u7fa9\u3057\u306a\u304a\u3057\u305f\u74b0\u5883\u3078\u306e\u4f5c\u308a\u76f4\u3057\u304c\u5fc5\u8981&gt; <a href=\"https://t.co/x57EkRzQfj\">https://t.co/x57EkRzQfj</a></p>&mdash; chachay (@chachay) <a href=\"https://twitter.com/chachay/status/833685506202284032\">February 20, 2017</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n## 2\u65e5\u76ee\uff1a\u6b63\u547330\u5206\n* Env\u306e\u7d99\u627f\u30af\u30e9\u30b9\u4f5c\u6210\n* \u52d5\u3044\u305f\n\n## \u307e\u3068\u3081\n\n* \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3065\u304f\u308a\u304c\u6357\u308a\u307e\u304f\u3063\u3066\u7d20\u6674\u3089\u3057\u3044\n* pip\u306b\u3066Windows\u3067\u8e74\u3089\u308c\u306a\u3044\u3088\u3046\u306a\u6c17\u3065\u304b\u3044\u306f\u3044\u305f\u3060\u304d\u305f\u3044\n* \u3084\u3063\u3071\u308a\u2193\u306f\u3042\u308b\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"ja\" dir=\"ltr\">\u9023\u7d9a\u5024\u3092\u666e\u6bb5\u5165\u529b\u5024\u3068\u3057\u3066\u6271\u3063\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u3068\u3057\u3066\u306f\u3001\u9023\u7d9a\u5024\u306b\u5bfe\u3057\u3066\u884c\u52d5\u5019\u88dc(\u96e2\u6563\u5316)\u3092\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3067\u5b9a\u7fa9\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3001\u96e2\u6563\u30fb\u9023\u7d9a\u5024\u9593\u306e\u9593\u3092\u53d6\u308a\u6301\u3063\u3066\u304f\u308c\u308b\u3068\u52a9\u304b\u308b\u3002gym\u306e\u8a2d\u8a08\u30b3\u30f3\u30bb\u30d7\u30c8\u304b\u3089\u5916\u308c\u308b\u3068\u3057\u3066\u3082\u3002</p>&mdash; chachay (@chachay) <a href=\"https://twitter.com/chachay/status/833688196877660160\">February 20, 2017</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n# \u30b9\u30af\u30ea\u30d7\u30c8\n[Chachay/Gym\\_LineFollower: Simple Open AI gym like Environment](https://github.com/Chachay/Gym_LineFollower)\n\nexample2.py\u53c2\u7167\u306e\u3053\u3068\n", "tags": ["DeepLearning", "Chainer", "\u5f37\u5316\u5b66\u7fd2", "\u6a5f\u68b0\u5b66\u7fd2"]}