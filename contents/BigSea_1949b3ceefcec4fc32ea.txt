{"context": "\n\n\u6982\u8981\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306f\u8907\u6570\u306e\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u91cd\u307f\u4ed8\u304d\u52a0\u7b97\u306b\u3088\u308b\u30e2\u30c7\u30eb\u3067\uff0c\u591a\u5cf0\u6027\u306e\u5206\u5e03\u3092\u8868\u3059\u3053\u3068\u304c\u51fa\u6765\u308b\u3082\u306e\u3067\u3059\uff0e\u307e\u305f\uff0c\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u306b\u306fEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7528\u3044\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\uff0e\u3053\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306eE-step\u3067\u6f5c\u5728\u5909\u6570\u306e\u671f\u5f85\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u304c\uff0c\u305d\u306e\u969b\u306b\u30ca\u30a4\u30fc\u30d6\u306b\u8a08\u7b97\u3092\u884c\u3046\u3068overflow, underflow\u306e\u554f\u984c\u304c\u8d77\u3053\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\uff0e\u305d\u306e\u3068\u304d\uff0clogsumexp\u3068\u3044\u3046\u6709\u540d\u306a\u6570\u5024\u8a08\u7b97\u65b9\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3053\u306e\u554f\u984c\u3092\u56de\u907f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u6982\u8981\u3068logsumexp\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u6700\u9069\u5316\u306b\u95a2\u3057\u3066\uff0clogsumexp\u306b\u95a2\u4fc2\u3059\u308b\u90e8\u5206\u3060\u3051\u7c21\u5358\u306b\u793a\u3057\u307e\u3059\uff0e\u3088\u308a\u8a73\u3057\u304f\u306f\u300e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u3068\u6a5f\u68b0\u5b66\u7fd2\u300f\u306a\u3069\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\uff08\u4e00\u5fdc\uff0cSlideShare\u306b\u8f2a\u8b1b\u3057\u305f\u3068\u304d\u306e\u8cc7\u6599\u3092\u4e0a\u3052\u3066\u304a\u304d\u307e\u3059 PRML 9.0-9.2)\n\\begin{aligned}\np(x|\\theta) &= \\sum_k \\pi_k N(x_n|\\mu_k,\\Sigma_k)\\\\\n\\pi_k&: \u6df7\u5408\u6bd4\n\\end{aligned}\n\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u89b3\u6e2c\u5909\u6570$x$\u306e\u5206\u5e03\u306f\u4e0a\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff0e\nEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9069\u7528\u3059\u308b\u305f\u3081\u306b\uff0c\u6f5c\u5728\u5909\u6570$z$\u3092\u5c0e\u5165\u3057\uff0c$x,z$\u306e\u540c\u6642\u5206\u5e03\u3092\u6b21\u306e\u3088\u3046\u306b\u8868\u3057\u307e\u3059\uff0e\n\\begin{aligned}\np(x,z) &= \\prod_k \\pi_k^{z_k} N(x_n|\\mu_k,\\Sigma_k)^{z_k}\\\\\nz&: 1ofK\n\\end{aligned}\n\n\u3053\u308c\u3089\u3092\u7528\u3044\u3066\u6f5c\u5728\u5909\u6570z\u306e\u671f\u5f85\u5024\u3092\u30d9\u30a4\u30ba\u306e\u5b9a\u7406\u3088\u308a\u6c42\u3081\u308b\u3068\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n\\begin{aligned}\n\\gamma(z_{nk})\n&= E[z_{nk}] \\\\ \n&=\\frac{\\pi_k N(x_n|\\mu_k,\\Sigma_k)}{\\sum_{k^\\prime}\\pi_{k^\\prime} N(x_n|\\mu_{k^\\prime},\\Sigma_{k^\\prime})}\n\\end{aligned}\n\n\u6df7\u5408\u30ac\u30a6\u30b9\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u6700\u9069\u5316\u3067\u306eE-step\u3067\u306f\uff0c\u3053\u306e\u6f5c\u5728\u5909\u6570\u306e\u671f\u5f85\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\uff0e\u5206\u6bcd\u3092\u898b\u308b\u3068\u30ac\u30a6\u30b9\u5206\u5e03\u306esum\u6f14\u7b97\u304c\u3042\u308b\u306e\u3067\uff0clog scale\u3067\u6f14\u7b97\u3092\u884c\u3046\u5834\u5408\u3067\u3082\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u6307\u6570\u6f14\u7b97\u3067underflow\u304c\u8d77\u3053\u308a\u3046\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\uff0e\u305d\u3053\u3067\u4f55\u3089\u304b\u306e\u5de5\u592b\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u305d\u306e\u4e00\u3064\u306e\u65b9\u6cd5\u3068\u3057\u3066logsumexp\u304c\u3042\u308a\u307e\u3059\uff0e\n\nlogsumexp\n\\log(\\sum^N_{i=1} \\exp(x_i))\n\n\u3068\u3044\u3046\u8a08\u7b97\u3092\u3057\u305f\u3044\u3068\u304d\u306b\uff0c\u3053\u306e\u8a08\u7b97\u306e\u7d50\u679c\u81ea\u4f53\u306foverflow, underflow\u3057\u306a\u3044\u5834\u5408\u3067\u3082\uff0c\u500b\u5225\u306e$\\exp(x_i)$\u306foverflow, underflow\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff0e\u305d\u3053\u3067\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5f0f\u5909\u5f62\u3092\u884c\u3044\u307e\u3059\uff0e\n\\begin{aligned}\n\\log(\\sum^N_{i=1} \\exp(x_i)) \n&= \\log\\{\\exp(x_{max})\\sum^N_{i=1} \\exp(x_i - x_{max})\\} \\\\\n& = \\log\\{\\sum^N_{i=1} \\exp(x_i - x_{max})\\}  + x_{max}\n\\end{aligned}\n\n\u3053\u306e\u3088\u3046\u306b\u8a08\u7b97\u3092\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u6307\u6570\u6f14\u7b97\u306e\u5f15\u6570\u304c$[x_{min} - x_{max},0]$\u306e\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u306e\u3067\uff0coverflow, underflow\u304c\u8d77\u3053\u308b\u53ef\u80fd\u6027\u304c\u5927\u5e45\u306b\u4e0b\u304c\u308a\u307e\u3059\uff0e\n\u3053\u306elogsumexp\u306fPython\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eascikitlearn\u306b\u3082\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\uff0c\u7c21\u5358\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\uff08Scikit Learn Utilities for Developers\uff09\uff0e\u307e\u305f\uff0cscikit learn\u3067\u306elogsumexp\u306e\u5b9f\u88c5\u306fsklearn/utils/extmath.py\u306b\u3042\u308a\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0eaxis\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u90e8\u5206\u4ee5\u5916\u306f\u4e0a\u306b\u793a\u3057\u305f\u6570\u5f0f\u3068\u5168\u304f\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\uff0e\n\ndef logsumexp(arr, axis=0):\n    \"\"\"Computes the sum of arr assuming arr is in the log domain.\n    Returns log(sum(exp(arr))) while minimizing the possibility of\n    over/underflow.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n    \"\"\"\n    arr = np.rollaxis(arr, axis)\n    # Use the max to normalize, as with the log this is what accumulates\n    # the less errors\n    vmax = arr.max(axis=0)\n    out = np.log(np.sum(np.exp(arr - vmax), axis=0))\n    out += vmax\n    return out\n\n\n\u6df7\u5408\u30ac\u30a6\u30b9\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n\u4ee5\u4e0a\u3092\u8e0f\u307e\u3048\u3066\u6df7\u5408\u30ac\u30a6\u30b9\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3057\u305f\u306e\u3067\u7f6e\u3044\u3066\u304a\u304d\u307e\u3059\uff08https://github.com/seataK/machine-learning/blob/master/GaussianMixture/GaussianMixture.py\uff09\uff0e\nimport numpy as np\nimport random\nimport pylab as plt\nfrom sklearn.utils import extmath\nfrom sklearn.cluster import KMeans\nimport sys\n\n\nna = np.newaxis\n\n\nclass DataFormatter:\n    def __init__(self, X):\n        self.mean = np.mean(X, axis=0)\n        self.std = np.std(X, axis=0)\n\n    def standarize(self, X):\n        return (X - self.mean[na, :]) / self.std[na, :]\n\n\ndef log_gaussian(X, mu, cov):\n    d = X.shape[1]\n    det_sig = np.linalg.det(cov)\n    A = 1.0 / (2*np.pi)**(d/2.0) * 1.0 / det_sig**(0.5)\n    x_mu = X - mu[na, :]\n    inv_cov = np.linalg.inv(cov)\n    ex = - 0.5 * np.sum(x_mu[:, :, na] * inv_cov[na, :, :] *\n                        x_mu[:, na, :], axis=(1, 2))\n    return np.log(A) + ex\n\n\nclass GMM:\n    def __init__(self,\n                 K=2,\n                 max_iter=300,\n                 diag=False):\n        self.K = K\n        self.data_form = None\n        self.pi = None\n        self.mean = None\n        self.cov = None\n        self.max_iter = max_iter\n        self.diag = diag\n\n    def fit(self, _X):\n        self.data_form = DataFormatter(_X)\n        X = self.data_form.standarize(_X)\n        N = X.shape[0]\n        D = X.shape[1]\n        K = self.K\n\n        # init parameters using K-means\n        kmeans = KMeans(n_clusters=self.K)\n\n        kmeans.fit(X)\n\n        self.mean = kmeans.cluster_centers_\n\n        self.cov = np.array([[[1 if i == j else 0\n                             for i in range(D)]\n                             for j in range(D)]\n                             for k in range(K)])\n\n        self.pi = np.ones(K) / K\n\n        # Optimization\n        for _ in range(self.max_iter):\n            # E-step\n\n            gam_nk = self._gam(X)\n\n            # M-step\n            Nk = np.sum(gam_nk, axis=0)\n\n            self.pi = Nk / N\n\n            self.mean = np.sum(gam_nk[:, :, na] * X[:, na, :],\n                               axis=0) / Nk[:, na]\n\n            x_mu_nkd = X[:, na, :] - self.mean[na, :, :]\n\n            self.cov = np.sum(gam_nk[:, :, na, na] *\n                              x_mu_nkd[:, :, :, na] *\n                              x_mu_nkd[:, :, na, :],\n                              axis=0) / Nk[:, na, na]\n\n            if(self.diag):\n                for k in range(K):\n                    var = np.diag(self.cov[k])\n                    self.cov[k] = np.array([[var[i] if i == j else 0\n                                             for i in range(D)]\n                                            for j in range(D)])\n\n    def _gam(self, X):\n        log_gs_nk = np.array([log_gaussian(X, self.mean[i], self.cov[i])\n                              for i in range(self.K)]).T\n\n        log_pi_gs_nk = np.log(self.pi)[na, :] + log_gs_nk\n\n        log_gam_nk = log_pi_gs_nk[:, :] - extmath.logsumexp(log_pi_gs_nk, axis=1)[:, na]\n\n        return np.exp(log_gam_nk)\n\n    def predict(self, _X):\n        X = self.data_form.standarize(_X)\n\n        gam_nk = self._gam(X)\n\n        return np.argmax(gam_nk, axis=1)\n\n\n\u305d\u306e\u4ed6\nlogsumexp\u3067\u306flog,exp\u306e\u6f14\u7b97\u304c\u591a\u304f\u51fa\u3066\u304f\u308b\u306e\u3067\u8a08\u7b97\u901f\u5ea6\u304c\u9045\u304f\u306a\u308b\u3088\u3046\u3067\u3059(logsumexp \u306f\u4eba\u985e\u306e\u9ed2\u6b74\u53f2)\uff0e\u901f\u5ea6\u304c\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u306b\u306a\u308b\u5834\u5408\u306f\u4ed6\u306e\u65b9\u6cd5\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u826f\u3044\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n#\u6982\u8981\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306f\u8907\u6570\u306e\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u91cd\u307f\u4ed8\u304d\u52a0\u7b97\u306b\u3088\u308b\u30e2\u30c7\u30eb\u3067\uff0c\u591a\u5cf0\u6027\u306e\u5206\u5e03\u3092\u8868\u3059\u3053\u3068\u304c\u51fa\u6765\u308b\u3082\u306e\u3067\u3059\uff0e\u307e\u305f\uff0c\u3053\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u306b\u306fEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7528\u3044\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\uff0e\u3053\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306eE-step\u3067\u6f5c\u5728\u5909\u6570\u306e\u671f\u5f85\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\u304c\uff0c\u305d\u306e\u969b\u306b\u30ca\u30a4\u30fc\u30d6\u306b\u8a08\u7b97\u3092\u884c\u3046\u3068overflow, underflow\u306e\u554f\u984c\u304c\u8d77\u3053\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\uff0e\u305d\u306e\u3068\u304d\uff0clogsumexp\u3068\u3044\u3046\u6709\u540d\u306a\u6570\u5024\u8a08\u7b97\u65b9\u6cd5\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u3053\u306e\u554f\u984c\u3092\u56de\u907f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff0e\n\n#\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u6982\u8981\u3068logsumexp\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u6700\u9069\u5316\u306b\u95a2\u3057\u3066\uff0clogsumexp\u306b\u95a2\u4fc2\u3059\u308b\u90e8\u5206\u3060\u3051\u7c21\u5358\u306b\u793a\u3057\u307e\u3059\uff0e\u3088\u308a\u8a73\u3057\u304f\u306f[\u300e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u3068\u6a5f\u68b0\u5b66\u7fd2\u300f](http://www.amazon.co.jp/%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3%E8%AA%8D%E8%AD%98%E3%81%A8%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E4%B8%8B-%E3%83%99%E3%82%A4%E3%82%BA%E7%90%86%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E7%B5%B1%E8%A8%88%E7%9A%84%E4%BA%88%E6%B8%AC-C-M-%E3%83%93%E3%82%B7%E3%83%A7%E3%83%83%E3%83%97/dp/4621061240/ref=pd_sim_14_1?ie=UTF8&refRID=1GS4SDYG8VEYR40YSADB)\u306a\u3069\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\uff0e\uff08\u4e00\u5fdc\uff0cSlideShare\u306b\u8f2a\u8b1b\u3057\u305f\u3068\u304d\u306e\u8cc7\u6599\u3092\u4e0a\u3052\u3066\u304a\u304d\u307e\u3059 [PRML 9.0-9.2](http://www.slideshare.net/taikaitakeda/prmltitech9092))\n\n```math\n\\begin{aligned}\np(x|\\theta) &= \\sum_k \\pi_k N(x_n|\\mu_k,\\Sigma_k)\\\\\n\\pi_k&: \u6df7\u5408\u6bd4\n\\end{aligned}\n```\n\u6df7\u5408\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u89b3\u6e2c\u5909\u6570$x$\u306e\u5206\u5e03\u306f\u4e0a\u306e\u5f0f\u3067\u8868\u3055\u308c\u307e\u3059\uff0e\nEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9069\u7528\u3059\u308b\u305f\u3081\u306b\uff0c\u6f5c\u5728\u5909\u6570$z$\u3092\u5c0e\u5165\u3057\uff0c$x,z$\u306e\u540c\u6642\u5206\u5e03\u3092\u6b21\u306e\u3088\u3046\u306b\u8868\u3057\u307e\u3059\uff0e\n\n```math\n\\begin{aligned}\np(x,z) &= \\prod_k \\pi_k^{z_k} N(x_n|\\mu_k,\\Sigma_k)^{z_k}\\\\\nz&: 1ofK\n\\end{aligned}\n```\n\u3053\u308c\u3089\u3092\u7528\u3044\u3066\u6f5c\u5728\u5909\u6570z\u306e\u671f\u5f85\u5024\u3092\u30d9\u30a4\u30ba\u306e\u5b9a\u7406\u3088\u308a\u6c42\u3081\u308b\u3068\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\uff0e\n\n```math\n\\begin{aligned}\n\\gamma(z_{nk})\n&= E[z_{nk}] \\\\ \n&=\\frac{\\pi_k N(x_n|\\mu_k,\\Sigma_k)}{\\sum_{k^\\prime}\\pi_{k^\\prime} N(x_n|\\mu_{k^\\prime},\\Sigma_{k^\\prime})}\n\\end{aligned}\n```\n\n\u6df7\u5408\u30ac\u30a6\u30b9\u306eEM\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u6700\u9069\u5316\u3067\u306eE-step\u3067\u306f\uff0c\u3053\u306e\u6f5c\u5728\u5909\u6570\u306e\u671f\u5f85\u5024\u3092\u8a08\u7b97\u3057\u307e\u3059\uff0e\u5206\u6bcd\u3092\u898b\u308b\u3068\u30ac\u30a6\u30b9\u5206\u5e03\u306esum\u6f14\u7b97\u304c\u3042\u308b\u306e\u3067\uff0clog scale\u3067\u6f14\u7b97\u3092\u884c\u3046\u5834\u5408\u3067\u3082\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u6307\u6570\u6f14\u7b97\u3067underflow\u304c\u8d77\u3053\u308a\u3046\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\uff0e\u305d\u3053\u3067\u4f55\u3089\u304b\u306e\u5de5\u592b\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\uff0c\u305d\u306e\u4e00\u3064\u306e\u65b9\u6cd5\u3068\u3057\u3066logsumexp\u304c\u3042\u308a\u307e\u3059\uff0e\n\n#logsumexp\n```math\n\\log(\\sum^N_{i=1} \\exp(x_i))\n```\n\u3068\u3044\u3046\u8a08\u7b97\u3092\u3057\u305f\u3044\u3068\u304d\u306b\uff0c\u3053\u306e\u8a08\u7b97\u306e\u7d50\u679c\u81ea\u4f53\u306foverflow, underflow\u3057\u306a\u3044\u5834\u5408\u3067\u3082\uff0c\u500b\u5225\u306e$\\exp(x_i)$\u306foverflow, underflow\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff0e\u305d\u3053\u3067\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5f0f\u5909\u5f62\u3092\u884c\u3044\u307e\u3059\uff0e\n\n```math\n\\begin{aligned}\n\\log(\\sum^N_{i=1} \\exp(x_i)) \n&= \\log\\{\\exp(x_{max})\\sum^N_{i=1} \\exp(x_i - x_{max})\\} \\\\\n& = \\log\\{\\sum^N_{i=1} \\exp(x_i - x_{max})\\}  + x_{max}\n\\end{aligned}\n```\n\u3053\u306e\u3088\u3046\u306b\u8a08\u7b97\u3092\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u6307\u6570\u6f14\u7b97\u306e\u5f15\u6570\u304c$[x_{min} - x_{max},0]$\u306e\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u306e\u3067\uff0coverflow, underflow\u304c\u8d77\u3053\u308b\u53ef\u80fd\u6027\u304c\u5927\u5e45\u306b\u4e0b\u304c\u308a\u307e\u3059\uff0e\n\n\u3053\u306elogsumexp\u306fPython\u306e\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30eascikitlearn\u306b\u3082\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\uff0c\u7c21\u5358\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\uff08[Scikit Learn Utilities for Developers](http://scikit-learn.org/stable/developers/utilities.html)\uff09\uff0e\u307e\u305f\uff0cscikit learn\u3067\u306elogsumexp\u306e\u5b9f\u88c5\u306f[sklearn/utils/extmath.py](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L316)\u306b\u3042\u308a\uff0c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0eaxis\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u90e8\u5206\u4ee5\u5916\u306f\u4e0a\u306b\u793a\u3057\u305f\u6570\u5f0f\u3068\u5168\u304f\u540c\u3058\u306b\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\uff0e\n\n```py\n\ndef logsumexp(arr, axis=0):\n    \"\"\"Computes the sum of arr assuming arr is in the log domain.\n    Returns log(sum(exp(arr))) while minimizing the possibility of\n    over/underflow.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n    \"\"\"\n    arr = np.rollaxis(arr, axis)\n    # Use the max to normalize, as with the log this is what accumulates\n    # the less errors\n    vmax = arr.max(axis=0)\n    out = np.log(np.sum(np.exp(arr - vmax), axis=0))\n    out += vmax\n    return out\n```\n\n#\u6df7\u5408\u30ac\u30a6\u30b9\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n\u4ee5\u4e0a\u3092\u8e0f\u307e\u3048\u3066\u6df7\u5408\u30ac\u30a6\u30b9\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3057\u305f\u306e\u3067\u7f6e\u3044\u3066\u304a\u304d\u307e\u3059\uff08https://github.com/seataK/machine-learning/blob/master/GaussianMixture/GaussianMixture.py\uff09\uff0e\n\n```py\nimport numpy as np\nimport random\nimport pylab as plt\nfrom sklearn.utils import extmath\nfrom sklearn.cluster import KMeans\nimport sys\n\n\nna = np.newaxis\n\n\nclass DataFormatter:\n    def __init__(self, X):\n        self.mean = np.mean(X, axis=0)\n        self.std = np.std(X, axis=0)\n\n    def standarize(self, X):\n        return (X - self.mean[na, :]) / self.std[na, :]\n\n\ndef log_gaussian(X, mu, cov):\n    d = X.shape[1]\n    det_sig = np.linalg.det(cov)\n    A = 1.0 / (2*np.pi)**(d/2.0) * 1.0 / det_sig**(0.5)\n    x_mu = X - mu[na, :]\n    inv_cov = np.linalg.inv(cov)\n    ex = - 0.5 * np.sum(x_mu[:, :, na] * inv_cov[na, :, :] *\n                        x_mu[:, na, :], axis=(1, 2))\n    return np.log(A) + ex\n\n\nclass GMM:\n    def __init__(self,\n                 K=2,\n                 max_iter=300,\n                 diag=False):\n        self.K = K\n        self.data_form = None\n        self.pi = None\n        self.mean = None\n        self.cov = None\n        self.max_iter = max_iter\n        self.diag = diag\n\n    def fit(self, _X):\n        self.data_form = DataFormatter(_X)\n        X = self.data_form.standarize(_X)\n        N = X.shape[0]\n        D = X.shape[1]\n        K = self.K\n\n        # init parameters using K-means\n        kmeans = KMeans(n_clusters=self.K)\n\n        kmeans.fit(X)\n\n        self.mean = kmeans.cluster_centers_\n\n        self.cov = np.array([[[1 if i == j else 0\n                             for i in range(D)]\n                             for j in range(D)]\n                             for k in range(K)])\n\n        self.pi = np.ones(K) / K\n\n        # Optimization\n        for _ in range(self.max_iter):\n            # E-step\n\n            gam_nk = self._gam(X)\n\n            # M-step\n            Nk = np.sum(gam_nk, axis=0)\n\n            self.pi = Nk / N\n\n            self.mean = np.sum(gam_nk[:, :, na] * X[:, na, :],\n                               axis=0) / Nk[:, na]\n\n            x_mu_nkd = X[:, na, :] - self.mean[na, :, :]\n\n            self.cov = np.sum(gam_nk[:, :, na, na] *\n                              x_mu_nkd[:, :, :, na] *\n                              x_mu_nkd[:, :, na, :],\n                              axis=0) / Nk[:, na, na]\n\n            if(self.diag):\n                for k in range(K):\n                    var = np.diag(self.cov[k])\n                    self.cov[k] = np.array([[var[i] if i == j else 0\n                                             for i in range(D)]\n                                            for j in range(D)])\n\n    def _gam(self, X):\n        log_gs_nk = np.array([log_gaussian(X, self.mean[i], self.cov[i])\n                              for i in range(self.K)]).T\n\n        log_pi_gs_nk = np.log(self.pi)[na, :] + log_gs_nk\n\n        log_gam_nk = log_pi_gs_nk[:, :] - extmath.logsumexp(log_pi_gs_nk, axis=1)[:, na]\n\n        return np.exp(log_gam_nk)\n\n    def predict(self, _X):\n        X = self.data_form.standarize(_X)\n\n        gam_nk = self._gam(X)\n\n        return np.argmax(gam_nk, axis=1)\n```\n\n#\u305d\u306e\u4ed6\nlogsumexp\u3067\u306flog,exp\u306e\u6f14\u7b97\u304c\u591a\u304f\u51fa\u3066\u304f\u308b\u306e\u3067\u8a08\u7b97\u901f\u5ea6\u304c\u9045\u304f\u306a\u308b\u3088\u3046\u3067\u3059([logsumexp \u306f\u4eba\u985e\u306e\u9ed2\u6b74\u53f2](http://d.hatena.ne.jp/takeda25/20110906/1315315416))\uff0e\u901f\u5ea6\u304c\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u306b\u306a\u308b\u5834\u5408\u306f\u4ed6\u306e\u65b9\u6cd5\u3092\u4f7f\u3063\u305f\u307b\u3046\u304c\u826f\u3044\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\uff0e\n", "tags": ["\u6a5f\u68b0\u5b66\u7fd2", "MachineLearning", "Python", "numpy", "scikit-learn"]}