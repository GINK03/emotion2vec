{"context": "JAVA\u304c\u4e00\u756a\u66f8\u304d\u3084\u3059\u3044\u306e\u3067java\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u3066\u307f\u305f\u306e\u3067\u3059\u304c\u3001\nC\u306b\u306f\u3042\u308b\u4e0b\u8a18\u306e\u95a2\u6570\u304c\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u4f55\u3084\u3089\u4e00\u5de5\u592b\u8981\u308a\u305d\u3046\u3067\u3059\u3002\n\u4e00\u65e6\u4f5c\u6210\u3057\u305f Word2VecModel\u306b\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u4e8b\u304c\u3067\u304d\u306a\u305d\u3046\u3067\u3001\u6700\u521d\u304b\u3089\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u3092\u6295\u5165\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u304c\u5acc\u3067\u3059\u306d\u3002\n\u3061\u3087\u3063\u3068\u4ed6\u306e\u65b9\u6cd5\u3092\u63a2\u3055\u306a\u304d\u3083\u3067\u3059\u3002\nWord2Vec C example\n\u3053\u3063\u3061\u3082\u3042\u3068\u3067\u3084\u3063\u3066\u307f\u3088\u3046\n\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])\nmodel.doesnt_match(\"breakfast cereal dinner lunch\".split())\nmodel.similarity('woman', 'man')\n\n\n\u624b\u9806\n1.\u305f\u3060\u306ejava project\u3092\u4f5c\u6210\n2.maven project\u306b\u5909\u66f4\n3.pom.xml\u3092\u8a18\u8f09\u3057\u3066MavenInstall\n4.main\u30af\u30e9\u30b9\u3092\u4e00\u500b\u4f5c\u6210\n\n3.pom.xml\u3092\u8a18\u8f09\u3057\u3066MavenInstall\n\nxml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>SimpleSpark</groupId>\n  <artifactId>SimpleSpark</artifactId>\n  <version>0.0.1-SNAPSHOT</version>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.10</artifactId>\n            <version>1.6.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.10</artifactId>\n            <version>1.6.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-mllib_2.10</artifactId>\n            <version>1.6.1</version>\n        </dependency>\n    </dependencies>\n  <build>\n    <sourceDirectory>src</sourceDirectory>\n    <plugins>\n      <plugin>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.1</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n\n\n\n4.main\u30af\u30e9\u30b9\u3092\u4e00\u500b\u4f5c\u6210\n\njava\npackage demo;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.ml.feature.PolynomialExpansion;\nimport org.apache.spark.ml.feature.Word2Vec;\nimport org.apache.spark.ml.feature.Word2VecModel;\nimport org.apache.spark.sql.DataFrame;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.RowFactory;\nimport org.apache.spark.sql.SQLContext;\nimport org.apache.spark.sql.types.ArrayType;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.Metadata;\nimport org.apache.spark.sql.types.StructField;\nimport org.apache.spark.sql.types.StructType;\n\npublic class W2VecApplication {\n    static String data_vec = \"/tmp/word2/\";\n    static String data_model = \"/tmp/model/\";\n    static SparkConf conf;\n    static JavaSparkContext jsc;\n    static SQLContext sqlContext;\n\n    public static void main(String[] args) {\n        conf = new SparkConf().setAppName(\"demo.W2VecApplication\").setMaster(\"local\");\n        jsc = new JavaSparkContext(conf);\n        sqlContext = new org.apache.spark.sql.SQLContext(jsc);\n\n        String input = \"The largest open source project in data processing\u00a5n\"\n                + \"Since its release, Apache Spark has seen rapid adoption by enterprises across a wide range of industries. Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. It has quickly become the largest open source community in big data, with over 1000 contributors from 250+ organizations.\u00a5n\"\n                + \"The team that created Apache Spark founded Databricks in 2013.\u00a5n\"\n                + \"Apache Spark is 100% open source, hosted at the vendor-independent Apache Software Foundation. At Databricks, we are fully committed to maintaining this open development model. Together with the Spark community, Databricks continues to contribute heavily to the Apache Spark project, through both development and community evangelism.\u00a5n\"\n                + \"At Databricks, we\u2019re working hard to make Spark easier to use and run than ever, through our efforts on both the Spark codebase and support materials around it. All of our work on Spark is open source and goes directly to Apache.\u00a5n\"\n                + \"Speed Engineered from the bottom-up for performance, Spark can be 100x faster than Hadoop for large scale data processing by exploiting in memory computing and other optimizations. Spark is also fast when data is stored on disk, and currently holds the world record for large-scale on-disk sorting.\u00a5n\"\n                + \"Ease of Use Spark has easy-to-use APIs for operating on large datasets. This includes a collection of over 100 operators for transforming data and familiar data frame APIs for manipulating semi-structured data.\u00a5n\"\n                + \"A Unified Engine Spark comes packaged with higher-level libraries, including support for SQL queries, streaming data, machine learning and graph processing. These standard libraries increase developer productivity and can be seamlessly combined to create complex workflows.\u00a5n\"\n                + \"Apache Spark is an open source cluster computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\u00a5n\"\n                + \"Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.[1] It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[2]\u00a5n\"\n                + \"The availability of RDDs facilitates the implementation of both iterative algorithms, that visit their dataset multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications (compared to Apache Hadoop, a popular MapReduce implementation) may be reduced by several orders of magnitude.[1][3] Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.[4]\u00a5n\"\n                + \"Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos.[5] For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS),[6] MapR File System (MapR-FS),[7] Cassandra,[8] OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\";\n\n        try {\n            //  \u5b66\u7fd2\u30c7\u30fc\u30bf\u4f5c\u6210\n            startStudy(Arrays.asList(input.split(\"\u00a5n\")));\n            //  \u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u691c\u7d22\n            findSomething(\"Spark\");\n\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }catch(IllegalStateException e2){\n\n        }\n    }\n\n    private static void startStudy(List<String> al) throws IOException{\n        // TODO Auto-generated method stub\n        PolynomialExpansion polyExpansion = new PolynomialExpansion()\n          .setInputCol(\"features\")\n          .setOutputCol(\"polyFeatures\")\n          .setDegree(3);\n\n        //  \u6587\u7ae0\u3092\u3056\u3063\u304f\u308a\u8aad\u307e\u305b\u307e\u3059\n        List<Row> aljr = new ArrayList<Row>();\n        for(int i = 0 ; i < al.size() ; i++){\n            aljr.add((Row)RowFactory.create(Arrays.asList(al.get(i).toString().split(\" \"))));\n        }\n        JavaRDD<Row> jrdd = jsc.parallelize(aljr);\n\n        StructType schema = new StructType(new StructField[]{\n          new StructField(\"text\", new ArrayType(DataTypes.StringType, true), false, Metadata.empty())\n        });\n        DataFrame documentDF = sqlContext.createDataFrame(jrdd, schema);\n\n        // Learn a mapping from words to Vectors.\n        Word2Vec word2Vec = new Word2Vec()\n          .setInputCol(\"text\")\n          .setOutputCol(\"result\")\n          .setVectorSize(3)\n          .setMinCount(0);\n\n        //  Word2 vec model \u306e\u751f\u6210\n        Word2VecModel model = word2Vec.fit(documentDF);\n        DataFrame result = model.transform(documentDF);\n        for (Row r : result.select(\"result\").take(3)) {\n            System.out.println(r);\n        }\n\n        //  \u4e00\u65e6\u30c7\u30fc\u30bf\u4fdd\u5b58\n        if(!new File(data_model).exists()){\n            model.save(data_model);\n        }else{\n            model.write().overwrite().save(data_model);\n        }       \n        if(!new File(data_vec).exists()){\n            word2Vec.save(data_vec);\n        }else{\n            word2Vec.write().overwrite().save(data_vec);\n        }       \n    }\n\n    private static void findSomething(String str) throws IllegalStateException{\n        //  \u5148\u307b\u3069\u4fdd\u5b58\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u518d\u5229\u7528\n        Word2Vec vec = Word2Vec.load(data_vec);\n        Word2VecModel model = Word2VecModel.load(data_model);\n\n        // Find similar word\n        DataFrame similar = model.findSynonyms(str, 30);\n        for (int i = 0 ; i < similar.count() ; i++) {\n            System.out.println(similar.showString(i,false));\n        }\n    }\n}\n\n\n+--------------+-------------------+\n|word          |similarity         |\n+--------------+-------------------+\n|system.       |0.2014637445894577 |\n|called        |0.19980325869996565|\n|analysis,     |0.1993920149609777 |\n|availability  |0.19908706675080787|\n|datasets.     |0.19904390575754743|\n|higher-level  |0.19838541460232392|\n|particular    |0.1955145561157753 |\n|Use           |0.19286837761820919|\n|powerhouses   |0.19223540828654095|\n|bottom-up     |0.19097962682578687|\n|or            |0.18865127679894006|\n|processing.   |0.18847629567307478|\n|nodes.        |0.18771307406284526|\n|Engine        |0.18674049047044267|\n|when          |0.1864894622682742 |\n|may           |0.1849541028612345 |\n|implicit      |0.181728914295441  |\n|linear        |0.179022874456727  |\n|Amazon        |0.1789115889397055 |\n|Apache.       |0.17816780927062123|\n|response      |0.17644830351552274|\n|it            |0.1755878682373225 |\n|purposes,     |0.17553053395023288|\n|collectively  |0.17349352919593652|\n|scale         |0.16585955085424214|\n|the           |0.1641250039271931 |\n|reduction     |0.16363822547590098|\n|record        |0.16354020270709446|\n|optimizations.|0.1595631774706085 |\n+--------------+-------------------+\nonly showing top 29 rows\n\nSpark\u306e\u8aac\u660e\u3092wiki\u3068Spark\u306e\u6982\u8981\u304b\u3089\u53d6\u5f97\u3057\u3066\u304d\u3066\u3044\u308c\u305f\u306e\u3067\u8981\u7d04\u304c\u63b4\u3081\u305f\u308a\u3059\u308b\u306e\u304b\u306a\u3068\u601d\u3063\u3066\u307f\u305f\u306e\u3067\u52dd\u624b\u306b\u4e0a\u306e\u65b9\u304b\u3089\u8a00\u8449\u3092\u3064\u306a\u3052\u3066\u6587\u7ae0\u306b\u306a\u308b\u304b\u3092\u307f\u3066\u898b\u307e\u3057\u305f\u304c....\u3053\u308c\u3060\u3051\u3060\u3068\u4f55\u8a00\u3063\u3066\u308b\u304b\u308f\u304b\u3089\u306a\u3044\u3067\u3059\u3002\u6b74\u4ee3\u306e\u4f1a\u8b70\u306e\u8b70\u4e8b\u9332\u3068\u304b\u8aad\u307e\u305b\u3048\u3066\u307f\u305f\u3044\u3067\u3059\u306d\u3002\nSystem called \"Spark\uff11(\u5165\u529b\u3057\u305f\u6587\u5b57\uff09\". \nAnalysis datasets with higher-level availability, particular use of powerhouses and buttom-up or processing nodes Engine\n\nJAVA\u304c\u4e00\u756a\u66f8\u304d\u3084\u3059\u3044\u306e\u3067java\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u3066\u307f\u305f\u306e\u3067\u3059\u304c\u3001\nC\u306b\u306f\u3042\u308b\u4e0b\u8a18\u306e\u95a2\u6570\u304c\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u4f55\u3084\u3089\u4e00\u5de5\u592b\u8981\u308a\u305d\u3046\u3067\u3059\u3002\n\u4e00\u65e6\u4f5c\u6210\u3057\u305f Word2VecModel\u306b\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u4e8b\u304c\u3067\u304d\u306a\u305d\u3046\u3067\u3001\u6700\u521d\u304b\u3089\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u3092\u6295\u5165\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u306e\u304c\u5acc\u3067\u3059\u306d\u3002\n\u3061\u3087\u3063\u3068\u4ed6\u306e\u65b9\u6cd5\u3092\u63a2\u3055\u306a\u304d\u3083\u3067\u3059\u3002\n\n[Word2Vec C example](https://radimrehurek.com/gensim/models/word2vec.html)\n\n[\u3053\u3063\u3061\u3082\u3042\u3068\u3067\u3084\u3063\u3066\u307f\u3088\u3046](http://edof.hatenablog.com/entry/2015/05/11/161342)\n\n> model.most_similar(positive=['woman', 'king'], negative=['man'])\n> model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n> model.similarity('woman', 'man')\n\n## \u624b\u9806\n1.\u305f\u3060\u306ejava project\u3092\u4f5c\u6210\n2.maven project\u306b\u5909\u66f4\n3.pom.xml\u3092\u8a18\u8f09\u3057\u3066MavenInstall\n4.main\u30af\u30e9\u30b9\u3092\u4e00\u500b\u4f5c\u6210\n\n## 3.pom.xml\u3092\u8a18\u8f09\u3057\u3066MavenInstall\n```pom.xml:xml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>SimpleSpark</groupId>\n  <artifactId>SimpleSpark</artifactId>\n  <version>0.0.1-SNAPSHOT</version>\n\t<dependencies>\n\t    <dependency>\n\t        <groupId>org.apache.spark</groupId>\n\t        <artifactId>spark-core_2.10</artifactId>\n\t        <version>1.6.1</version>\n\t    </dependency>\n\t\t<dependency>\n\t\t    <groupId>org.apache.spark</groupId>\n\t\t    <artifactId>spark-sql_2.10</artifactId>\n\t\t    <version>1.6.1</version>\n\t\t</dependency>\n\t\t<dependency>\n\t\t    <groupId>org.apache.spark</groupId>\n\t\t    <artifactId>spark-mllib_2.10</artifactId>\n\t\t    <version>1.6.1</version>\n\t\t</dependency>\n\t</dependencies>\n  <build>\n    <sourceDirectory>src</sourceDirectory>\n    <plugins>\n      <plugin>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.1</version>\n        <configuration>\n          <source>1.8</source>\n          <target>1.8</target>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n```\n\n## 4.main\u30af\u30e9\u30b9\u3092\u4e00\u500b\u4f5c\u6210\n```W2VecApplication.java:java\npackage demo;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\n\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.ml.feature.PolynomialExpansion;\nimport org.apache.spark.ml.feature.Word2Vec;\nimport org.apache.spark.ml.feature.Word2VecModel;\nimport org.apache.spark.sql.DataFrame;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.RowFactory;\nimport org.apache.spark.sql.SQLContext;\nimport org.apache.spark.sql.types.ArrayType;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.Metadata;\nimport org.apache.spark.sql.types.StructField;\nimport org.apache.spark.sql.types.StructType;\n\npublic class W2VecApplication {\n\tstatic String data_vec = \"/tmp/word2/\";\n\tstatic String data_model = \"/tmp/model/\";\n\tstatic SparkConf conf;\n\tstatic JavaSparkContext jsc;\n\tstatic SQLContext sqlContext;\n\t\n\tpublic static void main(String[] args) {\n\t\tconf = new SparkConf().setAppName(\"demo.W2VecApplication\").setMaster(\"local\");\n\t\tjsc = new JavaSparkContext(conf);\n\t\tsqlContext = new org.apache.spark.sql.SQLContext(jsc);\n\t\t\n\t\tString input = \"The largest open source project in data processing\u00a5n\"\n\t\t\t\t+ \"Since its release, Apache Spark has seen rapid adoption by enterprises across a wide range of industries. Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. It has quickly become the largest open source community in big data, with over 1000 contributors from 250+ organizations.\u00a5n\"\n\t\t\t\t+ \"The team that created Apache Spark founded Databricks in 2013.\u00a5n\"\n\t\t\t\t+ \"Apache Spark is 100% open source, hosted at the vendor-independent Apache Software Foundation. At Databricks, we are fully committed to maintaining this open development model. Together with the Spark community, Databricks continues to contribute heavily to the Apache Spark project, through both development and community evangelism.\u00a5n\"\n\t\t\t\t+ \"At Databricks, we\u2019re working hard to make Spark easier to use and run than ever, through our efforts on both the Spark codebase and support materials around it. All of our work on Spark is open source and goes directly to Apache.\u00a5n\"\n\t\t\t\t+ \"Speed Engineered from the bottom-up for performance, Spark can be 100x faster than Hadoop for large scale data processing by exploiting in memory computing and other optimizations. Spark is also fast when data is stored on disk, and currently holds the world record for large-scale on-disk sorting.\u00a5n\"\n\t\t\t\t+ \"Ease of Use Spark has easy-to-use APIs for operating on large datasets. This includes a collection of over 100 operators for transforming data and familiar data frame APIs for manipulating semi-structured data.\u00a5n\"\n\t\t\t\t+ \"A Unified Engine Spark comes packaged with higher-level libraries, including support for SQL queries, streaming data, machine learning and graph processing. These standard libraries increase developer productivity and can be seamlessly combined to create complex workflows.\u00a5n\"\n\t\t\t\t+ \"Apache Spark is an open source cluster computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\u00a5n\"\n\t\t\t\t+ \"Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.[1] It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[2]\u00a5n\"\n\t\t\t\t+ \"The availability of RDDs facilitates the implementation of both iterative algorithms, that visit their dataset multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications (compared to Apache Hadoop, a popular MapReduce implementation) may be reduced by several orders of magnitude.[1][3] Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.[4]\u00a5n\"\n\t\t\t\t+ \"Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos.[5] For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS),[6] MapR File System (MapR-FS),[7] Cassandra,[8] OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core.\";\n\t\t\n\t\ttry {\n\t\t\t//\t\u5b66\u7fd2\u30c7\u30fc\u30bf\u4f5c\u6210\n\t\t\tstartStudy(Arrays.asList(input.split(\"\u00a5n\")));\n\t\t\t//\t\u5b66\u7fd2\u30c7\u30fc\u30bf\u304b\u3089\u691c\u7d22\n\t\t\tfindSomething(\"Spark\");\n\t\t\t\n\t\t} catch (IOException e) {\n\t\t\t// TODO Auto-generated catch block\n\t\t\te.printStackTrace();\n\t\t}catch(IllegalStateException e2){\n\t\t\t\n\t\t}\n\t}\n\t\n\tprivate static void startStudy(List<String> al) throws IOException{\n\t\t// TODO Auto-generated method stub\n\t\tPolynomialExpansion polyExpansion = new PolynomialExpansion()\n\t\t  .setInputCol(\"features\")\n\t\t  .setOutputCol(\"polyFeatures\")\n\t\t  .setDegree(3);\n\n\t\t//\t\u6587\u7ae0\u3092\u3056\u3063\u304f\u308a\u8aad\u307e\u305b\u307e\u3059\n\t\tList<Row> aljr = new ArrayList<Row>();\n\t\tfor(int i = 0 ; i < al.size() ; i++){\n\t\t\taljr.add((Row)RowFactory.create(Arrays.asList(al.get(i).toString().split(\" \"))));\n\t\t}\n\t\tJavaRDD<Row> jrdd = jsc.parallelize(aljr);\n\t\t\n\t\tStructType schema = new StructType(new StructField[]{\n\t\t  new StructField(\"text\", new ArrayType(DataTypes.StringType, true), false, Metadata.empty())\n\t\t});\n\t\tDataFrame documentDF = sqlContext.createDataFrame(jrdd, schema);\n\n\t\t// Learn a mapping from words to Vectors.\n\t\tWord2Vec word2Vec = new Word2Vec()\n\t\t  .setInputCol(\"text\")\n\t\t  .setOutputCol(\"result\")\n\t\t  .setVectorSize(3)\n\t\t  .setMinCount(0);\n\t\t\n\t\t//\tWord2 vec model \u306e\u751f\u6210\n\t\tWord2VecModel model = word2Vec.fit(documentDF);\n\t\tDataFrame result = model.transform(documentDF);\n\t\tfor (Row r : result.select(\"result\").take(3)) {\n\t\t\tSystem.out.println(r);\n\t\t}\n\t\t\n\t\t//\t\u4e00\u65e6\u30c7\u30fc\u30bf\u4fdd\u5b58\n\t\tif(!new File(data_model).exists()){\n\t\t\tmodel.save(data_model);\n\t\t}else{\n\t\t\tmodel.write().overwrite().save(data_model);\n\t\t}\t\t\n\t\tif(!new File(data_vec).exists()){\n\t\t\tword2Vec.save(data_vec);\n\t\t}else{\n\t\t\tword2Vec.write().overwrite().save(data_vec);\n\t\t}\t\t\n\t}\n\t\n\tprivate static void findSomething(String str) throws IllegalStateException{\n\t\t//\t\u5148\u307b\u3069\u4fdd\u5b58\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u518d\u5229\u7528\n\t\tWord2Vec vec = Word2Vec.load(data_vec);\n\t\tWord2VecModel model = Word2VecModel.load(data_model);\n\t\t\n\t\t// Find similar word\n\t\tDataFrame similar = model.findSynonyms(str, 30);\n\t\tfor (int i = 0 ; i < similar.count() ; i++) {\n\t\t\tSystem.out.println(similar.showString(i,false));\n\t\t}\n\t}\n}\n```\n\n```\u51fa\u529b\n+--------------+-------------------+\n|word          |similarity         |\n+--------------+-------------------+\n|system.       |0.2014637445894577 |\n|called        |0.19980325869996565|\n|analysis,     |0.1993920149609777 |\n|availability  |0.19908706675080787|\n|datasets.     |0.19904390575754743|\n|higher-level  |0.19838541460232392|\n|particular    |0.1955145561157753 |\n|Use           |0.19286837761820919|\n|powerhouses   |0.19223540828654095|\n|bottom-up     |0.19097962682578687|\n|or            |0.18865127679894006|\n|processing.   |0.18847629567307478|\n|nodes.        |0.18771307406284526|\n|Engine        |0.18674049047044267|\n|when          |0.1864894622682742 |\n|may           |0.1849541028612345 |\n|implicit      |0.181728914295441  |\n|linear        |0.179022874456727  |\n|Amazon        |0.1789115889397055 |\n|Apache.       |0.17816780927062123|\n|response      |0.17644830351552274|\n|it            |0.1755878682373225 |\n|purposes,     |0.17553053395023288|\n|collectively  |0.17349352919593652|\n|scale         |0.16585955085424214|\n|the           |0.1641250039271931 |\n|reduction     |0.16363822547590098|\n|record        |0.16354020270709446|\n|optimizations.|0.1595631774706085 |\n+--------------+-------------------+\nonly showing top 29 rows\n```\n\nSpark\u306e\u8aac\u660e\u3092wiki\u3068Spark\u306e\u6982\u8981\u304b\u3089\u53d6\u5f97\u3057\u3066\u304d\u3066\u3044\u308c\u305f\u306e\u3067\u8981\u7d04\u304c\u63b4\u3081\u305f\u308a\u3059\u308b\u306e\u304b\u306a\u3068\u601d\u3063\u3066\u307f\u305f\u306e\u3067\u52dd\u624b\u306b\u4e0a\u306e\u65b9\u304b\u3089\u8a00\u8449\u3092\u3064\u306a\u3052\u3066\u6587\u7ae0\u306b\u306a\u308b\u304b\u3092\u307f\u3066\u898b\u307e\u3057\u305f\u304c....\u3053\u308c\u3060\u3051\u3060\u3068\u4f55\u8a00\u3063\u3066\u308b\u304b\u308f\u304b\u3089\u306a\u3044\u3067\u3059\u3002\u6b74\u4ee3\u306e\u4f1a\u8b70\u306e\u8b70\u4e8b\u9332\u3068\u304b\u8aad\u307e\u305b\u3048\u3066\u307f\u305f\u3044\u3067\u3059\u306d\u3002\n\n```\nSystem called \"Spark\uff11(\u5165\u529b\u3057\u305f\u6587\u5b57\uff09\". \nAnalysis datasets with higher-level availability, particular use of powerhouses and buttom-up or processing nodes Engine\n```\n", "tags": ["Java", "Spark", "\u6a5f\u68b0\u5b66\u7fd2"]}