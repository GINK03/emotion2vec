{"context": "Chainer \u3067\u3001\u5b9a\u756a\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u306e \u304a\u6c7a\u307e\u308a\u30b3\u30fc\u30c9 \u306e \u66f8\u304d\u65b9\uff08\u304a\u4f5c\u7528\uff09\u3092 \u5fd8\u308c\u305f\u3068\u304d\u3001\u6bce\u56de \u8abf\u3079\u308b\u306e\u306f\u3001\u6642\u9593\u30b3\u30b9\u30c8\u304c\u304b\u304b\u308b\u3002\n\n\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u30af\u30e9\u30b9\u5b9a\u7fa9\n\u9806\u65b9\u5411\u6f14\u7b97\uff08 fwd\u95a2\u6570 \uff09\n\u30b3\u30b9\u30c8\u95a2\u6570\u6f14\u7b97\uff08 call\u95a2\u6570 \uff09\n\u6d3b\u6027\u5316\u95a2\u6570 \u9069\u7528\u30e1\u30bd\u30c3\u30c9\n\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\uff08BP\u6cd5\uff09\u306b\u3088\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u7d50\u5408\u8377\u91cd\uff08\u5c64\u9593\u306e\u91cd\u307f\uff09\u8a08\u7b97\n\n\n\u57fa\u672c\u3068\u306a\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u3054\u3068\u306b\u3001\u300eChainer\u306b\u3088\u308b\u5b9f\u8df5\u6df1\u5c64\u5b66\u7fd2\u300f\uff082016\uff09\u306e\u30b3\u30fc\u30c9 \u3092 \u5199\u7d4c\u3057\u3066web\u306b\u4e0a\u3052\u3066\u3001\u5fc5\u8981\u306a\u7b87\u6240 \u3092 \u30b3\u30d4\u30da\u3067\u304d\u308b\u3088\u3046 \u306b \u3057\u307e\u3057\u305f\u3002\n\n\u3010 \u30b3\u30fc\u30c9\u5199\u7d4c\u5143 \u3011\n\n\n\u65b0\u7d0d \u6d69\u5e78\uff08\u8457\uff09\u300eChainer\u306b\u3088\u308b\u5b9f\u8df5\u6df1\u5c64\u5b66\u7fd2\u300fOhmsha (2016) \u7b2c4\u7ae0 pp.44-47\n\n\n\n\u3010 \u4eca\u56de\u3001\u53d6\u308a\u4e0a\u3052\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u3011\n\n\uff08 \u5165\u529b\u5024 \uff09\n\n\uff14\u5909\u6570 : \u30a2\u30e4\u30e1\u306e\u82b1\uff08iris\uff09\u306e\u304c\u304f\u7247\u306e\u9577\u3055\u30fb\u5e45\u3001\u82b1\u5f01\u306e\u9577\u3055\u30fb\u5e45\n\n\uff08 \u51fa\u529b\u5024 \uff09\n\n3\u5909\u6570: \u30a2\u30e4\u30e1\u306e\u7a2e\u985e\uff08Setosa, Versicolor, Virginica\uff09\n\n\uff08 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \uff09\n\n\u5c64\u306e\u6570 : 3\u5c64 \uff08\u5165\u529b\u5c64\u3001\u4e2d\u9593\u5c64\uff11\u3064\u3001\u51fa\u529b\u5c64\uff09\n\u5165\u529b\u5c64\u306e\u30ce\u30fc\u30c9\u6570 : 4\u30ce\u30fc\u30c9 \uff08 \u5165\u529b\u5024\u306e\u6570 \uff09\n\u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9 : 6\u30ce\u30fc\u30c9\n\u51fa\u529b\u5c64\u306e\u30ce\u30fc\u30c9 : 3\u30ce\u30fc\u30c9 \uff08 \u51fa\u529b\u5024 \uff1d \u82b1\u7a2e\u5225\u306e\u5224\u5225\u6570 \uff09\n\u6d3b\u6027\u5316\u95a2\u6570 : \u30b7\u30b0\u30e2\u30a4\u30c8\u95a2\u6570 \uff08 \u591a\u5024\u5224\u5225\u554f\u984c \uff09\u203b Softmax\u95a2\u6570\u3001Cross entropy\u95a2\u6570 \u306f \u4f7f\u7528\u305b\u305a\nOptimizer : SGD \uff08 \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5 \uff09\n\n\n\n\u3010 \u6559\u79d1\u66f8 \u5199\u7d4c\u30b3\u30fc\u30c9 \u3011Python\u30a4\u30f3\u30bf\u30d7\u30ea\u30bf \u3067 \u5b9f\u884c\n\n\n\uff08 \u5b9f\u884c\u74b0\u5883 \uff09\n\n\n\nOS : MacBook \uff08 CPU\u4f7f\u7528 \uff09\nPython version : 3.5.2\nChainer version : 1.14.0\n\n\n\nPython 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python --version\nPython 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$\n\n\n\nPython 3.5.2\n>>> print(chainer.__version__)\n1.14.0\n>>> \n\n\n\nPython 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> print(iris)\n{'DESCR': 'Iris Plants Database\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n', 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n       [ 4.9,  3. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [ 4.6,  3.1,  1.5,  0.2],\n       [ 5. ,  3.6,  1.4,  0.2],\n       [ 5.4,  3.9,  1.7,  0.4],\n       [ 4.6,  3.4,  1.4,  0.3],\n       [ 5. ,  3.4,  1.5,  0.2],\n       [ 4.4,  2.9,  1.4,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 5.4,  3.7,  1.5,  0.2],\n       [ 4.8,  3.4,  1.6,  0.2],\n       [ 4.8,  3. ,  1.4,  0.1],\n       [ 4.3,  3. ,  1.1,  0.1],\n       [ 5.8,  4. ,  1.2,  0.2],\n       [ 5.7,  4.4,  1.5,  0.4],\n       [ 5.4,  3.9,  1.3,  0.4],\n       [ 5.1,  3.5,  1.4,  0.3],\n       [ 5.7,  3.8,  1.7,  0.3],\n       [ 5.1,  3.8,  1.5,  0.3],\n       [ 5.4,  3.4,  1.7,  0.2],\n       [ 5.1,  3.7,  1.5,  0.4],\n       [ 4.6,  3.6,  1. ,  0.2],\n       [ 5.1,  3.3,  1.7,  0.5],\n       [ 4.8,  3.4,  1.9,  0.2],\n       [ 5. ,  3. ,  1.6,  0.2],\n       [ 5. ,  3.4,  1.6,  0.4],\n       [ 5.2,  3.5,  1.5,  0.2],\n       [ 5.2,  3.4,  1.4,  0.2],\n       [ 4.7,  3.2,  1.6,  0.2],\n       [ 4.8,  3.1,  1.6,  0.2],\n       [ 5.4,  3.4,  1.5,  0.4],\n       [ 5.2,  4.1,  1.5,  0.1],\n       [ 5.5,  4.2,  1.4,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 5. ,  3.2,  1.2,  0.2],\n       [ 5.5,  3.5,  1.3,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 4.4,  3. ,  1.3,  0.2],\n       [ 5.1,  3.4,  1.5,  0.2],\n       [ 5. ,  3.5,  1.3,  0.3],\n       [ 4.5,  2.3,  1.3,  0.3],\n       [ 4.4,  3.2,  1.3,  0.2],\n       [ 5. ,  3.5,  1.6,  0.6],\n       [ 5.1,  3.8,  1.9,  0.4],\n       [ 4.8,  3. ,  1.4,  0.3],\n       [ 5.1,  3.8,  1.6,  0.2],\n       [ 4.6,  3.2,  1.4,  0.2],\n       [ 5.3,  3.7,  1.5,  0.2],\n       [ 5. ,  3.3,  1.4,  0.2],\n       [ 7. ,  3.2,  4.7,  1.4],\n       [ 6.4,  3.2,  4.5,  1.5],\n       [ 6.9,  3.1,  4.9,  1.5],\n       [ 5.5,  2.3,  4. ,  1.3],\n       [ 6.5,  2.8,  4.6,  1.5],\n       [ 5.7,  2.8,  4.5,  1.3],\n       [ 6.3,  3.3,  4.7,  1.6],\n       [ 4.9,  2.4,  3.3,  1. ],\n       [ 6.6,  2.9,  4.6,  1.3],\n       [ 5.2,  2.7,  3.9,  1.4],\n       [ 5. ,  2. ,  3.5,  1. ],\n       [ 5.9,  3. ,  4.2,  1.5],\n       [ 6. ,  2.2,  4. ,  1. ],\n       [ 6.1,  2.9,  4.7,  1.4],\n       [ 5.6,  2.9,  3.6,  1.3],\n       [ 6.7,  3.1,  4.4,  1.4],\n       [ 5.6,  3. ,  4.5,  1.5],\n       [ 5.8,  2.7,  4.1,  1. ],\n       [ 6.2,  2.2,  4.5,  1.5],\n       [ 5.6,  2.5,  3.9,  1.1],\n       [ 5.9,  3.2,  4.8,  1.8],\n       [ 6.1,  2.8,  4. ,  1.3],\n       [ 6.3,  2.5,  4.9,  1.5],\n       [ 6.1,  2.8,  4.7,  1.2],\n       [ 6.4,  2.9,  4.3,  1.3],\n       [ 6.6,  3. ,  4.4,  1.4],\n       [ 6.8,  2.8,  4.8,  1.4],\n       [ 6.7,  3. ,  5. ,  1.7],\n       [ 6. ,  2.9,  4.5,  1.5],\n       [ 5.7,  2.6,  3.5,  1. ],\n       [ 5.5,  2.4,  3.8,  1.1],\n       [ 5.5,  2.4,  3.7,  1. ],\n       [ 5.8,  2.7,  3.9,  1.2],\n       [ 6. ,  2.7,  5.1,  1.6],\n       [ 5.4,  3. ,  4.5,  1.5],\n       [ 6. ,  3.4,  4.5,  1.6],\n       [ 6.7,  3.1,  4.7,  1.5],\n       [ 6.3,  2.3,  4.4,  1.3],\n       [ 5.6,  3. ,  4.1,  1.3],\n       [ 5.5,  2.5,  4. ,  1.3],\n       [ 5.5,  2.6,  4.4,  1.2],\n       [ 6.1,  3. ,  4.6,  1.4],\n       [ 5.8,  2.6,  4. ,  1.2],\n       [ 5. ,  2.3,  3.3,  1. ],\n       [ 5.6,  2.7,  4.2,  1.3],\n       [ 5.7,  3. ,  4.2,  1.2],\n       [ 5.7,  2.9,  4.2,  1.3],\n       [ 6.2,  2.9,  4.3,  1.3],\n       [ 5.1,  2.5,  3. ,  1.1],\n       [ 5.7,  2.8,  4.1,  1.3],\n       [ 6.3,  3.3,  6. ,  2.5],\n       [ 5.8,  2.7,  5.1,  1.9],\n       [ 7.1,  3. ,  5.9,  2.1],\n       [ 6.3,  2.9,  5.6,  1.8],\n       [ 6.5,  3. ,  5.8,  2.2],\n       [ 7.6,  3. ,  6.6,  2.1],\n       [ 4.9,  2.5,  4.5,  1.7],\n       [ 7.3,  2.9,  6.3,  1.8],\n       [ 6.7,  2.5,  5.8,  1.8],\n       [ 7.2,  3.6,  6.1,  2.5],\n       [ 6.5,  3.2,  5.1,  2. ],\n       [ 6.4,  2.7,  5.3,  1.9],\n       [ 6.8,  3. ,  5.5,  2.1],\n       [ 5.7,  2.5,  5. ,  2. ],\n       [ 5.8,  2.8,  5.1,  2.4],\n       [ 6.4,  3.2,  5.3,  2.3],\n       [ 6.5,  3. ,  5.5,  1.8],\n       [ 7.7,  3.8,  6.7,  2.2],\n       [ 7.7,  2.6,  6.9,  2.3],\n       [ 6. ,  2.2,  5. ,  1.5],\n       [ 6.9,  3.2,  5.7,  2.3],\n       [ 5.6,  2.8,  4.9,  2. ],\n       [ 7.7,  2.8,  6.7,  2. ],\n       [ 6.3,  2.7,  4.9,  1.8],\n       [ 6.7,  3.3,  5.7,  2.1],\n       [ 7.2,  3.2,  6. ,  1.8],\n       [ 6.2,  2.8,  4.8,  1.8],\n       [ 6.1,  3. ,  4.9,  1.8],\n       [ 6.4,  2.8,  5.6,  2.1],\n       [ 7.2,  3. ,  5.8,  1.6],\n       [ 7.4,  2.8,  6.1,  1.9],\n       [ 7.9,  3.8,  6.4,  2. ],\n       [ 6.4,  2.8,  5.6,  2.2],\n       [ 6.3,  2.8,  5.1,  1.5],\n       [ 6.1,  2.6,  5.6,  1.4],\n       [ 7.7,  3. ,  6.1,  2.3],\n       [ 6.3,  3.4,  5.6,  2.4],\n       [ 6.4,  3.1,  5.5,  1.8],\n       [ 6. ,  3. ,  4.8,  1.8],\n       [ 6.9,  3.1,  5.4,  2.1],\n       [ 6.7,  3.1,  5.6,  2.4],\n       [ 6.9,  3.1,  5.1,  2.3],\n       [ 5.8,  2.7,  5.1,  1.9],\n       [ 6.8,  3.2,  5.9,  2.3],\n       [ 6.7,  3.3,  5.7,  2.5],\n       [ 6.7,  3. ,  5.2,  2.3],\n       [ 6.3,  2.5,  5. ,  1.9],\n       [ 6.5,  3. ,  5.2,  2. ],\n       [ 6.2,  3.4,  5.4,  2.3],\n       [ 5.9,  3. ,  5.1,  1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'target_names': array(['setosa', 'versicolor', 'virginica'], \n      dtype='<U10')}\n>>>\n\n\n\nPython 3.5.2\n>>> import numpy as np\n>>> \n>>> X = iris.data.astype(np.float32)\n>>> Y = iris.target\n>>> N = Y.size\n>>> \n>>> Y2 = np.zeros(3*N).reshape(N,3).astype(np.float32)\n>>> Y2\narray([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]], dtype=float32)\n>>> \n>>> for i in range(N):\n...     Y2[i, Y[i]] = 1.0\n... \n>>> Y2\narray([[ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.]], dtype=float32)\n>>> \n>>> index = np.arange(N)\n>>> index\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149])\n>>> \n>>> xtrain = X[index[index % 2 != 0], :]\n>>> ytrain = Y2[index[index % 2 != 0], :]\n>>> xtest = X[index[index % 2 == 0], :]\n>>> yans = Y[index[index % 2 == 0]]\n>>> yans\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2])\n>>> \n>>> xtest\narray([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],\n       [ 4.69999981,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5999999 ,  1.39999998,  0.2       ],\n       [ 4.5999999 ,  3.4000001 ,  1.39999998,  0.30000001],\n       [ 4.4000001 ,  2.9000001 ,  1.39999998,  0.2       ],\n       [ 5.4000001 ,  3.70000005,  1.5       ,  0.2       ],\n       [ 4.80000019,  3.        ,  1.39999998,  0.1       ],\n       [ 5.80000019,  4.        ,  1.20000005,  0.2       ],\n       [ 5.4000001 ,  3.9000001 ,  1.29999995,  0.40000001],\n       [ 5.69999981,  3.79999995,  1.70000005,  0.30000001],\n       [ 5.4000001 ,  3.4000001 ,  1.70000005,  0.2       ],\n       [ 4.5999999 ,  3.5999999 ,  1.        ,  0.2       ],\n       [ 4.80000019,  3.4000001 ,  1.89999998,  0.2       ],\n       [ 5.        ,  3.4000001 ,  1.60000002,  0.40000001],\n       [ 5.19999981,  3.4000001 ,  1.39999998,  0.2       ],\n       [ 4.80000019,  3.0999999 ,  1.60000002,  0.2       ],\n       [ 5.19999981,  4.0999999 ,  1.5       ,  0.1       ],\n       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n       [ 5.5       ,  3.5       ,  1.29999995,  0.2       ],\n       [ 4.4000001 ,  3.        ,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5       ,  1.29999995,  0.30000001],\n       [ 4.4000001 ,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.0999999 ,  3.79999995,  1.89999998,  0.40000001],\n       [ 5.0999999 ,  3.79999995,  1.60000002,  0.2       ],\n       [ 5.30000019,  3.70000005,  1.5       ,  0.2       ],\n       [ 7.        ,  3.20000005,  4.69999981,  1.39999998],\n       [ 6.9000001 ,  3.0999999 ,  4.9000001 ,  1.5       ],\n       [ 6.5       ,  2.79999995,  4.5999999 ,  1.5       ],\n       [ 6.30000019,  3.29999995,  4.69999981,  1.60000002],\n       [ 6.5999999 ,  2.9000001 ,  4.5999999 ,  1.29999995],\n       [ 5.        ,  2.        ,  3.5       ,  1.        ],\n       [ 6.        ,  2.20000005,  4.        ,  1.        ],\n       [ 5.5999999 ,  2.9000001 ,  3.5999999 ,  1.29999995],\n       [ 5.5999999 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.19999981,  2.20000005,  4.5       ,  1.5       ],\n       [ 5.9000001 ,  3.20000005,  4.80000019,  1.79999995],\n       [ 6.30000019,  2.5       ,  4.9000001 ,  1.5       ],\n       [ 6.4000001 ,  2.9000001 ,  4.30000019,  1.29999995],\n       [ 6.80000019,  2.79999995,  4.80000019,  1.39999998],\n       [ 6.        ,  2.9000001 ,  4.5       ,  1.5       ],\n       [ 5.5       ,  2.4000001 ,  3.79999995,  1.10000002],\n       [ 5.80000019,  2.70000005,  3.9000001 ,  1.20000005],\n       [ 5.4000001 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],\n       [ 5.5999999 ,  3.        ,  4.0999999 ,  1.29999995],\n       [ 5.5       ,  2.5999999 ,  4.4000001 ,  1.20000005],\n       [ 5.80000019,  2.5999999 ,  4.        ,  1.20000005],\n       [ 5.5999999 ,  2.70000005,  4.19999981,  1.29999995],\n       [ 5.69999981,  2.9000001 ,  4.19999981,  1.29999995],\n       [ 5.0999999 ,  2.5       ,  3.        ,  1.10000002],\n       [ 6.30000019,  3.29999995,  6.        ,  2.5       ],\n       [ 7.0999999 ,  3.        ,  5.9000001 ,  2.0999999 ],\n       [ 6.5       ,  3.        ,  5.80000019,  2.20000005],\n       [ 4.9000001 ,  2.5       ,  4.5       ,  1.70000005],\n       [ 6.69999981,  2.5       ,  5.80000019,  1.79999995],\n       [ 6.5       ,  3.20000005,  5.0999999 ,  2.        ],\n       [ 6.80000019,  3.        ,  5.5       ,  2.0999999 ],\n       [ 5.80000019,  2.79999995,  5.0999999 ,  2.4000001 ],\n       [ 6.5       ,  3.        ,  5.5       ,  1.79999995],\n       [ 7.69999981,  2.5999999 ,  6.9000001 ,  2.29999995],\n       [ 6.9000001 ,  3.20000005,  5.69999981,  2.29999995],\n       [ 7.69999981,  2.79999995,  6.69999981,  2.        ],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.0999999 ],\n       [ 6.19999981,  2.79999995,  4.80000019,  1.79999995],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.0999999 ],\n       [ 7.4000001 ,  2.79999995,  6.0999999 ,  1.89999998],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005],\n       [ 6.0999999 ,  2.5999999 ,  5.5999999 ,  1.39999998],\n       [ 6.30000019,  3.4000001 ,  5.5999999 ,  2.4000001 ],\n       [ 6.        ,  3.        ,  4.80000019,  1.79999995],\n       [ 6.69999981,  3.0999999 ,  5.5999999 ,  2.4000001 ],\n       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.5       ],\n       [ 6.30000019,  2.5       ,  5.        ,  1.89999998],\n       [ 6.19999981,  3.4000001 ,  5.4000001 ,  2.29999995]], dtype=float32)\n>>> \n\n\n\nPython 3.5.2\n>>> import chainer\n>>> from chainer import cuda, Function, gradient_check, Variable, optimizers,serializers, utils\n>>> \n>>> from chainer import Link, Chain, ChainList\n>>> import chainer.functions as F\n>>> import chainer.links as L\n>>>\n>>> class IrisChain(Chain):\n...     def __init__(self):\n...             super(IrisChain, self).__init__(\n...                     l1=L.Linear(4,6),\n...                     l2=L.Linear(6,3),\n...             )\n...     def __call__(self, x, y):\n...             return F.mean_squared_error(self.fwd(x), y)\n...     def fwd(self, x):\n...             h1 = F.sigmoid(self.l1(x))\n...             h2 = self.l2(h1)\n...             return h2\n... \n>>> \n>>> \n>>> model = IrisChain()\n>>>\n>>> optimizer = optimizers.SGD()\n>>>\n>>> optimizer.setup(model)\n>>>\n>>> for i in range(1000):\n...     x = Variable(xtrain)\n...     y = Variable(ytrain)\n...     model.zerograds()\n...     loss = model(x, y)\n...     loss.backward()\n...     optimizer.update()\n... \n>>>\n\n\n\nPython 3.5.2\n>>> xt = Variable(xtest, volatile='on')\n>>> xt\n<variable at 0x10e1f5f98>\n>>> \n>>> xt.data\narray([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],\n       [ 4.69999981,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5999999 ,  1.39999998,  0.2       ],\n       [ 4.5999999 ,  3.4000001 ,  1.39999998,  0.30000001],\n       [ 4.4000001 ,  2.9000001 ,  1.39999998,  0.2       ],\n       [ 5.4000001 ,  3.70000005,  1.5       ,  0.2       ],\n       [ 4.80000019,  3.        ,  1.39999998,  0.1       ],\n       [ 5.80000019,  4.        ,  1.20000005,  0.2       ],\n       [ 5.4000001 ,  3.9000001 ,  1.29999995,  0.40000001],\n       [ 5.69999981,  3.79999995,  1.70000005,  0.30000001],\n       [ 5.4000001 ,  3.4000001 ,  1.70000005,  0.2       ],\n       [ 4.5999999 ,  3.5999999 ,  1.        ,  0.2       ],\n       [ 4.80000019,  3.4000001 ,  1.89999998,  0.2       ],\n       [ 5.        ,  3.4000001 ,  1.60000002,  0.40000001],\n       [ 5.19999981,  3.4000001 ,  1.39999998,  0.2       ],\n       [ 4.80000019,  3.0999999 ,  1.60000002,  0.2       ],\n       [ 5.19999981,  4.0999999 ,  1.5       ,  0.1       ],\n       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n       [ 5.5       ,  3.5       ,  1.29999995,  0.2       ],\n       [ 4.4000001 ,  3.        ,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5       ,  1.29999995,  0.30000001],\n       [ 4.4000001 ,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.0999999 ,  3.79999995,  1.89999998,  0.40000001],\n       [ 5.0999999 ,  3.79999995,  1.60000002,  0.2       ],\n       [ 5.30000019,  3.70000005,  1.5       ,  0.2       ],\n       [ 7.        ,  3.20000005,  4.69999981,  1.39999998],\n       [ 6.9000001 ,  3.0999999 ,  4.9000001 ,  1.5       ],\n       [ 6.5       ,  2.79999995,  4.5999999 ,  1.5       ],\n       [ 6.30000019,  3.29999995,  4.69999981,  1.60000002],\n       [ 6.5999999 ,  2.9000001 ,  4.5999999 ,  1.29999995],\n       [ 5.        ,  2.        ,  3.5       ,  1.        ],\n       [ 6.        ,  2.20000005,  4.        ,  1.        ],\n       [ 5.5999999 ,  2.9000001 ,  3.5999999 ,  1.29999995],\n       [ 5.5999999 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.19999981,  2.20000005,  4.5       ,  1.5       ],\n       [ 5.9000001 ,  3.20000005,  4.80000019,  1.79999995],\n       [ 6.30000019,  2.5       ,  4.9000001 ,  1.5       ],\n       [ 6.4000001 ,  2.9000001 ,  4.30000019,  1.29999995],\n       [ 6.80000019,  2.79999995,  4.80000019,  1.39999998],\n       [ 6.        ,  2.9000001 ,  4.5       ,  1.5       ],\n       [ 5.5       ,  2.4000001 ,  3.79999995,  1.10000002],\n       [ 5.80000019,  2.70000005,  3.9000001 ,  1.20000005],\n       [ 5.4000001 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],\n       [ 5.5999999 ,  3.        ,  4.0999999 ,  1.29999995],\n       [ 5.5       ,  2.5999999 ,  4.4000001 ,  1.20000005],\n       [ 5.80000019,  2.5999999 ,  4.        ,  1.20000005],\n       [ 5.5999999 ,  2.70000005,  4.19999981,  1.29999995],\n       [ 5.69999981,  2.9000001 ,  4.19999981,  1.29999995],\n       [ 5.0999999 ,  2.5       ,  3.        ,  1.10000002],\n       [ 6.30000019,  3.29999995,  6.        ,  2.5       ],\n       [ 7.0999999 ,  3.        ,  5.9000001 ,  2.0999999 ],\n       [ 6.5       ,  3.        ,  5.80000019,  2.20000005],\n       [ 4.9000001 ,  2.5       ,  4.5       ,  1.70000005],\n       [ 6.69999981,  2.5       ,  5.80000019,  1.79999995],\n       [ 6.5       ,  3.20000005,  5.0999999 ,  2.        ],\n       [ 6.80000019,  3.        ,  5.5       ,  2.0999999 ],\n       [ 5.80000019,  2.79999995,  5.0999999 ,  2.4000001 ],\n       [ 6.5       ,  3.        ,  5.5       ,  1.79999995],\n       [ 7.69999981,  2.5999999 ,  6.9000001 ,  2.29999995],\n       [ 6.9000001 ,  3.20000005,  5.69999981,  2.29999995],\n       [ 7.69999981,  2.79999995,  6.69999981,  2.        ],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.0999999 ],\n       [ 6.19999981,  2.79999995,  4.80000019,  1.79999995],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.0999999 ],\n       [ 7.4000001 ,  2.79999995,  6.0999999 ,  1.89999998],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005],\n       [ 6.0999999 ,  2.5999999 ,  5.5999999 ,  1.39999998],\n       [ 6.30000019,  3.4000001 ,  5.5999999 ,  2.4000001 ],\n       [ 6.        ,  3.        ,  4.80000019,  1.79999995],\n       [ 6.69999981,  3.0999999 ,  5.5999999 ,  2.4000001 ],\n       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.5       ],\n       [ 6.30000019,  2.5       ,  5.        ,  1.89999998],\n       [ 6.19999981,  3.4000001 ,  5.4000001 ,  2.29999995]], dtype=float32)\n>>> \n\n\n\nPython 3.5.2\n>>> yt = model.fwd(xt)\n>>>\n>>> yt\n<variable at 0x10e1f5588>\n>>>\n>>> yt.data\narray([[ 0.65049964,  0.23795809,  0.24554214],\n       [ 0.62719226,  0.23875675,  0.22648317],\n       [ 0.64241773,  0.23161672,  0.24438597],\n       [ 0.59631473,  0.23288801,  0.23215252],\n       [ 0.58576518,  0.25176534,  0.21117243],\n       [ 0.66428065,  0.23924124,  0.25869802],\n       [ 0.63109839,  0.25416443,  0.22202918],\n       [ 0.73607868,  0.21926098,  0.27093801],\n       [ 0.67306954,  0.22140984,  0.26996952],\n       [ 0.65458411,  0.24881724,  0.27508536],\n       [ 0.63596576,  0.26035985,  0.25529292],\n       [ 0.65964806,  0.19990538,  0.22868469],\n       [ 0.55474687,  0.26271337,  0.23830664],\n       [ 0.59476733,  0.25154948,  0.25202763],\n       [ 0.658508  ,  0.24384874,  0.24651586],\n       [ 0.59510976,  0.2601572 ,  0.23061764],\n       [ 0.65880483,  0.21749733,  0.25574028],\n       [ 0.62710112,  0.25634563,  0.22806311],\n       [ 0.69697362,  0.23948437,  0.25520656],\n       [ 0.5996784 ,  0.24176781,  0.21221915],\n       [ 0.64568824,  0.23134014,  0.24646805],\n       [ 0.60062736,  0.23199877,  0.21577176],\n       [ 0.56366283,  0.25170848,  0.26456407],\n       [ 0.62552553,  0.23490125,  0.25275666],\n       [ 0.65573025,  0.23768446,  0.25596967],\n       [ 0.25162345,  0.36968628,  0.38573527],\n       [ 0.22027135,  0.37532166,  0.38907501],\n       [ 0.22420493,  0.37677348,  0.37243152],\n       [ 0.19910896,  0.37140176,  0.37709099],\n       [ 0.24390751,  0.37491554,  0.36819336],\n       [ 0.281192  ,  0.37256598,  0.2826277 ],\n       [ 0.29205048,  0.37670088,  0.32333279],\n       [ 0.28298745,  0.3564935 ,  0.32707334],\n       [ 0.18488854,  0.37344661,  0.3460111 ],\n       [ 0.2176438 ,  0.3850477 ,  0.35694426],\n       [ 0.16189924,  0.37328535,  0.37198263],\n       [ 0.19034311,  0.38613886,  0.36839598],\n       [ 0.26136968,  0.37009892,  0.35905325],\n       [ 0.22990566,  0.37844852,  0.37835509],\n       [ 0.20544407,  0.37538639,  0.35853291],\n       [ 0.27136594,  0.37146965,  0.31252044],\n       [ 0.27191132,  0.3674165 ,  0.33076164],\n       [ 0.17530948,  0.37267056,  0.33839971],\n       [ 0.22682884,  0.37311682,  0.3818129 ],\n       [ 0.23151192,  0.36670592,  0.33391237],\n       [ 0.20754093,  0.37984082,  0.32642967],\n       [ 0.26151592,  0.37155959,  0.331011  ],\n       [ 0.22269309,  0.37436131,  0.33236569],\n       [ 0.22816277,  0.37066641,  0.33755785],\n       [ 0.33448419,  0.34711313,  0.28980288],\n       [ 0.09845747,  0.37358919,  0.41850656],\n       [ 0.13468292,  0.38254431,  0.42583305],\n       [ 0.11796579,  0.38091168,  0.41271448],\n       [ 0.14768445,  0.37716714,  0.32236713],\n       [ 0.13980955,  0.39015827,  0.40133795],\n       [ 0.15918028,  0.37595895,  0.40041429],\n       [ 0.14390096,  0.38079563,  0.41431043],\n       [ 0.11919124,  0.37685126,  0.38937098],\n       [ 0.14521134,  0.38309085,  0.3964771 ],\n       [ 0.11141011,  0.38486871,  0.45222563],\n       [ 0.13043676,  0.37796527,  0.42679274],\n       [ 0.12379643,  0.3864055 ,  0.44254023],\n       [ 0.13072453,  0.37825826,  0.41605335],\n       [ 0.17543721,  0.37968686,  0.37833184],\n       [ 0.12674943,  0.38342461,  0.40337616],\n       [ 0.14262259,  0.3864415 ,  0.42629564],\n       [ 0.12340668,  0.38253206,  0.40663058],\n       [ 0.1465188 ,  0.39115438,  0.36892551],\n       [ 0.11346267,  0.3727971 ,  0.41301125],\n       [ 0.16633913,  0.3766984 ,  0.37390679],\n       [ 0.12529424,  0.37775251,  0.42312664],\n       [ 0.13806322,  0.38252181,  0.37191916],\n       [ 0.11825769,  0.37486488,  0.42833257],\n       [ 0.16253296,  0.38508436,  0.38451606],\n       [ 0.12144916,  0.37244728,  0.40509725]], dtype=float32)\n>>> \n\n\n\nPython 3.5.2\n>>> ans = yt.data\n>>> \n>>> nrow, ncols = ans.shape\n>>> ans.shape\n(75, 3)\n>>> \n>>> nrow, ncol = ans.shape\n>>>\n\n\n\nPython 3.5.2\n>>> ok = 0\n>>> \n>>> for i in range(nrow):\n...     cls = np.argmax(ans[i, :])\n...     if cls == yans[i]:\n...             ok += 1\n... \n>>> \n>>> print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n65 / 75  =  0.8666666666666667\n>>>\n\n\n\n10,000\u56de epoch \u5b66\u7fd2 : \u6b63\u89e3\u7387 0.96\n\n\nPython 3.5.2\n>>> for i in range(10000):\n...     x = Variable(xtrain)\n...     y = Variable(ytrain)\n...     model.zerograds()\n...     loss = model(x, y)\n...     loss.backward()\n...     optimizer.update()\n... \n>>> \n>>> xt = Variable(xtest, volatile='on')\n>>> yt = model.fwd(xt)\n>>> ans = yt.data\n>>> nrow, ncol = ans.shape\n>>> \n>>> ok = 0\n>>> \n>>> for i in range(nrow):\n...     cls = np.argmax(ans[i, :])\n...     if cls == yans[i]:\n...             ok += 1\n... \n>>> \n>>> print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n72 / 75  =  0.96\n>>>\n\n\n\n\u30e2\u30c7\u30eb\u5b66\u7fd2\uff08\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u306b\u3088\u308b\u91cd\u307f\u5b66\u7fd2\uff09\u5b9f\u884c\u30e1\u30bd\u30c3\u30c9 \u3068 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067\u306e\u5224\u5225\u6b63\u89e3\u7387\u51fa\u529b\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f\n\n\nPython 3.5.2\n>>> def bp_learning_iteration(n):\n...     for i in range(n):\n...             x = Variable(xtrain)\n...             y = Variable(ytrain)\n...             model.zerograds()\n...             loss = model(x, y)\n...             loss.backward()\n...             optimizer.update()\n... \n>>>\n\n\n\nPython 3.5.2\n>>> def print_classification_accuracy():\n...     xt = Variable(xtest, volatile='on')\n...     yt = model.fwd(xt)\n...     ans = yt.data\n...     nrow, ncol = ans.shape\n...     ok = 0\n...     for i in range(nrow):\n...             cls = np.argmax(ans[i, :])\n...             if cls == yans[i]:\n...                     ok += 1\n...     print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n... \n>>>\n\n\n\n50,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.96\n\n\nPython 3.5.2\n>>> bp_learning_iteration(50000)\n>>> print_classification_accuracy()\n72 / 75  =  0.96\n>>>\n\n\n\n100,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.9733\n\n\nPython 3.5.2\n>>> bp_learning_iteration(100000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>>\n\n\n\n200,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387  0.9733\n\n\nPython 3.5.2\n>>> bp_learning_iteration(200000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>> \n\n\n\n400,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.9733\n\n\nPython 3.5.2\n>>> bp_learning_iteration(400000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>> \n\n\n\n\n\u3010 \u53c2\u8003 \u3011\u5224\u5225\u554f\u984c \u306e \u6d3b\u6027\u5316\u95a2\u6570\n\n\nSigmoid\u95a2\u6570\u3001 Softmax\u95a2\u6570\u3001 Cross entropy\u95a2\u6570\n\n\nHatena Blog hshinji\u306e\u30e1\u30e2 \uff082015-05-15\uff09\u300c\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u52c9\u5f37\u306e\u307e\u3068\u3081\u3000\u305d\u306e1\u300d\nHatena Blog hshinji\u306e\u30e1\u30e2 \uff082015-05-20\uff09\u300c\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u52c9\u5f37\u306e\u307e\u3068\u3081\u3000\u305d\u306e2\u300d\n\u9ad8\u6821\u6570\u5b66\u306e\u7f8e\u3057\u3044\u7269\u8a9e \uff082016/05/22\uff09\u300c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u300d\nkenmatsu4\u3055\u3093 Qiita\u8a18\u4e8b\uff08\u6700\u7d42\u66f4\u65b0: 2015\u5e7409\u670826\u65e5\uff09\u300c\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0 \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30afChainer\u3092\u8a66\u3057\u306a\u304c\u3089\u89e3\u8aac\u3057\u3066\u307f\u308b\u3002\u300d\nTomokIshii\u3055\u3093 Qiita\u8a18\u4e8b\uff08\u6700\u7d42\u66f4\u65b0: 2016\u5e7405\u670830\u65e5\uff09\u300c\u843d\u3061\u3053\u307c\u308c\u306a\u3044\u305f\u3081\u306eTensorFlow Tutorial\u30b3\u30fc\u30c9\u300d\nHatena Blog neuralnet\u306e\u65e5\u8a18 \uff082016-05-17\uff09\u300c\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc(Cross Entropy)\u300d\n\n\nOptimizer \u306e \u9078\u629e\n\n\n\u4ffa\u3068\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \uff082016-02-11\uff09 \u300cCNN\u306e\u5b66\u7fd2\u306b\u6700\u9ad8\u306e\u6027\u80fd\u3092\u793a\u3059\u6700\u9069\u5316\u624b\u6cd5\u306f\u3069\u308c\u304b\u300d\nhogefugabar\u3055\u3093 Qiita\u8a18\u4e8b \uff08\u6700\u7d42\u66f4\u65b0\u65e5:2016\u5e7408\u670822\u65e5\uff09 \u300c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8272\u3005\u8a66\u3057\u3066\u307f\u308b\u300d\nJProgramer \uff082015\u5e7411\u670819\u65e5\uff09 \u300cChainer\u306e\u8a73\u3057\u3044\u69cb\u9020\u300d\n\n__Chainer \u3067\u3001\u5b9a\u756a\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u306e \u304a\u6c7a\u307e\u308a\u30b3\u30fc\u30c9 \u306e \u66f8\u304d\u65b9\uff08\u304a\u4f5c\u7528\uff09\u3092 \u5fd8\u308c\u305f\u3068\u304d\u3001\u6bce\u56de \u8abf\u3079\u308b\u306e\u306f\u3001\u6642\u9593\u30b3\u30b9\u30c8\u304c\u304b\u304b\u308b\u3002__\n\n___\n\n* __\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u30af\u30e9\u30b9\u5b9a\u7fa9__\n* __\u9806\u65b9\u5411\u6f14\u7b97\uff08 fwd\u95a2\u6570 \uff09__\n* __\u30b3\u30b9\u30c8\u95a2\u6570\u6f14\u7b97\uff08 call\u95a2\u6570 \uff09__\n* __\u6d3b\u6027\u5316\u95a2\u6570 \u9069\u7528\u30e1\u30bd\u30c3\u30c9__\n* __\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\uff08BP\u6cd5\uff09\u306b\u3088\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u7d50\u5408\u8377\u91cd\uff08\u5c64\u9593\u306e\u91cd\u307f\uff09\u8a08\u7b97__\n\n___\n\n__\u57fa\u672c\u3068\u306a\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210\u3054\u3068\u306b\u3001\u300eChainer\u306b\u3088\u308b\u5b9f\u8df5\u6df1\u5c64\u5b66\u7fd2\u300f\uff082016\uff09\u306e\u30b3\u30fc\u30c9 \u3092 \u5199\u7d4c\u3057\u3066web\u306b\u4e0a\u3052\u3066\u3001\u5fc5\u8981\u306a\u7b87\u6240 \u3092 \u30b3\u30d4\u30da\u3067\u304d\u308b\u3088\u3046 \u306b \u3057\u307e\u3057\u305f\u3002__\n\n\n###__\u3010 \u30b3\u30fc\u30c9\u5199\u7d4c\u5143 \u3011__\n\n* __\u65b0\u7d0d \u6d69\u5e78\uff08\u8457\uff09\u300eChainer\u306b\u3088\u308b\u5b9f\u8df5\u6df1\u5c64\u5b66\u7fd2\u300fOhmsha (2016) \u7b2c4\u7ae0 *pp.44-47*__\n\n___\n\n###__\u3010 \u4eca\u56de\u3001\u53d6\u308a\u4e0a\u3052\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u3011__\n\n__\uff08 \u5165\u529b\u5024 \uff09__\n\n* \uff14\u5909\u6570 : \u30a2\u30e4\u30e1\u306e\u82b1\uff08iris\uff09\u306e\u304c\u304f\u7247\u306e\u9577\u3055\u30fb\u5e45\u3001\u82b1\u5f01\u306e\u9577\u3055\u30fb\u5e45\n\n__\uff08 \u51fa\u529b\u5024 \uff09__\n\n* 3\u5909\u6570: \u30a2\u30e4\u30e1\u306e\u7a2e\u985e\uff08Setosa, Versicolor, Virginica\uff09\n\n__\uff08 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u6210 \uff09__\n\n* \u5c64\u306e\u6570 : 3\u5c64 \uff08\u5165\u529b\u5c64\u3001\u4e2d\u9593\u5c64\uff11\u3064\u3001\u51fa\u529b\u5c64\uff09\n\n* \u5165\u529b\u5c64\u306e\u30ce\u30fc\u30c9\u6570 : 4\u30ce\u30fc\u30c9 \uff08 \u5165\u529b\u5024\u306e\u6570 \uff09\n* \u4e2d\u9593\u5c64\u306e\u30ce\u30fc\u30c9 : 6\u30ce\u30fc\u30c9\n* \u51fa\u529b\u5c64\u306e\u30ce\u30fc\u30c9 : 3\u30ce\u30fc\u30c9 \uff08 \u51fa\u529b\u5024 \uff1d \u82b1\u7a2e\u5225\u306e\u5224\u5225\u6570 \uff09\n* \u6d3b\u6027\u5316\u95a2\u6570 : \u30b7\u30b0\u30e2\u30a4\u30c8\u95a2\u6570 \uff08 \u591a\u5024\u5224\u5225\u554f\u984c \uff09\u203b Softmax\u95a2\u6570\u3001Cross entropy\u95a2\u6570 \u306f \u4f7f\u7528\u305b\u305a\n* Optimizer : SGD \uff08 \u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5 \uff09\n\n___\n\n###__\u3010 \u6559\u79d1\u66f8 \u5199\u7d4c\u30b3\u30fc\u30c9 \u3011Python\u30a4\u30f3\u30bf\u30d7\u30ea\u30bf \u3067 \u5b9f\u884c__\n\n####__\uff08 \u5b9f\u884c\u74b0\u5883 \uff09__\n___\n\n* OS : MacBook \uff08 CPU\u4f7f\u7528 \uff09\n* Python version : 3.5.2\n* Chainer version : 1.14.0\n\n___\n\n\n```{Python:Python 3.5.2} \nHirofumiYashima-no-MacBook:~ hirofumiyashima$ pyenv local 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python --version\nPython 3.5.2\nHirofumiYashima-no-MacBook:~ hirofumiyashima$\n```\n\n```{Python:Python 3.5.2} \n>>> print(chainer.__version__)\n1.14.0\n>>> \n```\n\n```{Python:Python 3.5.2} \nHirofumiYashima-no-MacBook:~ hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> print(iris)\n{'DESCR': 'Iris Plants Database\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n', 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n       [ 4.9,  3. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [ 4.6,  3.1,  1.5,  0.2],\n       [ 5. ,  3.6,  1.4,  0.2],\n       [ 5.4,  3.9,  1.7,  0.4],\n       [ 4.6,  3.4,  1.4,  0.3],\n       [ 5. ,  3.4,  1.5,  0.2],\n       [ 4.4,  2.9,  1.4,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 5.4,  3.7,  1.5,  0.2],\n       [ 4.8,  3.4,  1.6,  0.2],\n       [ 4.8,  3. ,  1.4,  0.1],\n       [ 4.3,  3. ,  1.1,  0.1],\n       [ 5.8,  4. ,  1.2,  0.2],\n       [ 5.7,  4.4,  1.5,  0.4],\n       [ 5.4,  3.9,  1.3,  0.4],\n       [ 5.1,  3.5,  1.4,  0.3],\n       [ 5.7,  3.8,  1.7,  0.3],\n       [ 5.1,  3.8,  1.5,  0.3],\n       [ 5.4,  3.4,  1.7,  0.2],\n       [ 5.1,  3.7,  1.5,  0.4],\n       [ 4.6,  3.6,  1. ,  0.2],\n       [ 5.1,  3.3,  1.7,  0.5],\n       [ 4.8,  3.4,  1.9,  0.2],\n       [ 5. ,  3. ,  1.6,  0.2],\n       [ 5. ,  3.4,  1.6,  0.4],\n       [ 5.2,  3.5,  1.5,  0.2],\n       [ 5.2,  3.4,  1.4,  0.2],\n       [ 4.7,  3.2,  1.6,  0.2],\n       [ 4.8,  3.1,  1.6,  0.2],\n       [ 5.4,  3.4,  1.5,  0.4],\n       [ 5.2,  4.1,  1.5,  0.1],\n       [ 5.5,  4.2,  1.4,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 5. ,  3.2,  1.2,  0.2],\n       [ 5.5,  3.5,  1.3,  0.2],\n       [ 4.9,  3.1,  1.5,  0.1],\n       [ 4.4,  3. ,  1.3,  0.2],\n       [ 5.1,  3.4,  1.5,  0.2],\n       [ 5. ,  3.5,  1.3,  0.3],\n       [ 4.5,  2.3,  1.3,  0.3],\n       [ 4.4,  3.2,  1.3,  0.2],\n       [ 5. ,  3.5,  1.6,  0.6],\n       [ 5.1,  3.8,  1.9,  0.4],\n       [ 4.8,  3. ,  1.4,  0.3],\n       [ 5.1,  3.8,  1.6,  0.2],\n       [ 4.6,  3.2,  1.4,  0.2],\n       [ 5.3,  3.7,  1.5,  0.2],\n       [ 5. ,  3.3,  1.4,  0.2],\n       [ 7. ,  3.2,  4.7,  1.4],\n       [ 6.4,  3.2,  4.5,  1.5],\n       [ 6.9,  3.1,  4.9,  1.5],\n       [ 5.5,  2.3,  4. ,  1.3],\n       [ 6.5,  2.8,  4.6,  1.5],\n       [ 5.7,  2.8,  4.5,  1.3],\n       [ 6.3,  3.3,  4.7,  1.6],\n       [ 4.9,  2.4,  3.3,  1. ],\n       [ 6.6,  2.9,  4.6,  1.3],\n       [ 5.2,  2.7,  3.9,  1.4],\n       [ 5. ,  2. ,  3.5,  1. ],\n       [ 5.9,  3. ,  4.2,  1.5],\n       [ 6. ,  2.2,  4. ,  1. ],\n       [ 6.1,  2.9,  4.7,  1.4],\n       [ 5.6,  2.9,  3.6,  1.3],\n       [ 6.7,  3.1,  4.4,  1.4],\n       [ 5.6,  3. ,  4.5,  1.5],\n       [ 5.8,  2.7,  4.1,  1. ],\n       [ 6.2,  2.2,  4.5,  1.5],\n       [ 5.6,  2.5,  3.9,  1.1],\n       [ 5.9,  3.2,  4.8,  1.8],\n       [ 6.1,  2.8,  4. ,  1.3],\n       [ 6.3,  2.5,  4.9,  1.5],\n       [ 6.1,  2.8,  4.7,  1.2],\n       [ 6.4,  2.9,  4.3,  1.3],\n       [ 6.6,  3. ,  4.4,  1.4],\n       [ 6.8,  2.8,  4.8,  1.4],\n       [ 6.7,  3. ,  5. ,  1.7],\n       [ 6. ,  2.9,  4.5,  1.5],\n       [ 5.7,  2.6,  3.5,  1. ],\n       [ 5.5,  2.4,  3.8,  1.1],\n       [ 5.5,  2.4,  3.7,  1. ],\n       [ 5.8,  2.7,  3.9,  1.2],\n       [ 6. ,  2.7,  5.1,  1.6],\n       [ 5.4,  3. ,  4.5,  1.5],\n       [ 6. ,  3.4,  4.5,  1.6],\n       [ 6.7,  3.1,  4.7,  1.5],\n       [ 6.3,  2.3,  4.4,  1.3],\n       [ 5.6,  3. ,  4.1,  1.3],\n       [ 5.5,  2.5,  4. ,  1.3],\n       [ 5.5,  2.6,  4.4,  1.2],\n       [ 6.1,  3. ,  4.6,  1.4],\n       [ 5.8,  2.6,  4. ,  1.2],\n       [ 5. ,  2.3,  3.3,  1. ],\n       [ 5.6,  2.7,  4.2,  1.3],\n       [ 5.7,  3. ,  4.2,  1.2],\n       [ 5.7,  2.9,  4.2,  1.3],\n       [ 6.2,  2.9,  4.3,  1.3],\n       [ 5.1,  2.5,  3. ,  1.1],\n       [ 5.7,  2.8,  4.1,  1.3],\n       [ 6.3,  3.3,  6. ,  2.5],\n       [ 5.8,  2.7,  5.1,  1.9],\n       [ 7.1,  3. ,  5.9,  2.1],\n       [ 6.3,  2.9,  5.6,  1.8],\n       [ 6.5,  3. ,  5.8,  2.2],\n       [ 7.6,  3. ,  6.6,  2.1],\n       [ 4.9,  2.5,  4.5,  1.7],\n       [ 7.3,  2.9,  6.3,  1.8],\n       [ 6.7,  2.5,  5.8,  1.8],\n       [ 7.2,  3.6,  6.1,  2.5],\n       [ 6.5,  3.2,  5.1,  2. ],\n       [ 6.4,  2.7,  5.3,  1.9],\n       [ 6.8,  3. ,  5.5,  2.1],\n       [ 5.7,  2.5,  5. ,  2. ],\n       [ 5.8,  2.8,  5.1,  2.4],\n       [ 6.4,  3.2,  5.3,  2.3],\n       [ 6.5,  3. ,  5.5,  1.8],\n       [ 7.7,  3.8,  6.7,  2.2],\n       [ 7.7,  2.6,  6.9,  2.3],\n       [ 6. ,  2.2,  5. ,  1.5],\n       [ 6.9,  3.2,  5.7,  2.3],\n       [ 5.6,  2.8,  4.9,  2. ],\n       [ 7.7,  2.8,  6.7,  2. ],\n       [ 6.3,  2.7,  4.9,  1.8],\n       [ 6.7,  3.3,  5.7,  2.1],\n       [ 7.2,  3.2,  6. ,  1.8],\n       [ 6.2,  2.8,  4.8,  1.8],\n       [ 6.1,  3. ,  4.9,  1.8],\n       [ 6.4,  2.8,  5.6,  2.1],\n       [ 7.2,  3. ,  5.8,  1.6],\n       [ 7.4,  2.8,  6.1,  1.9],\n       [ 7.9,  3.8,  6.4,  2. ],\n       [ 6.4,  2.8,  5.6,  2.2],\n       [ 6.3,  2.8,  5.1,  1.5],\n       [ 6.1,  2.6,  5.6,  1.4],\n       [ 7.7,  3. ,  6.1,  2.3],\n       [ 6.3,  3.4,  5.6,  2.4],\n       [ 6.4,  3.1,  5.5,  1.8],\n       [ 6. ,  3. ,  4.8,  1.8],\n       [ 6.9,  3.1,  5.4,  2.1],\n       [ 6.7,  3.1,  5.6,  2.4],\n       [ 6.9,  3.1,  5.1,  2.3],\n       [ 5.8,  2.7,  5.1,  1.9],\n       [ 6.8,  3.2,  5.9,  2.3],\n       [ 6.7,  3.3,  5.7,  2.5],\n       [ 6.7,  3. ,  5.2,  2.3],\n       [ 6.3,  2.5,  5. ,  1.9],\n       [ 6.5,  3. ,  5.2,  2. ],\n       [ 6.2,  3.4,  5.4,  2.3],\n       [ 5.9,  3. ,  5.1,  1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'target_names': array(['setosa', 'versicolor', 'virginica'], \n      dtype='<U10')}\n>>>\n```\n\n\n```{Python:Python 3.5.2} \n>>> import numpy as np\n>>> \n>>> X = iris.data.astype(np.float32)\n>>> Y = iris.target\n>>> N = Y.size\n>>> \n>>> Y2 = np.zeros(3*N).reshape(N,3).astype(np.float32)\n>>> Y2\narray([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]], dtype=float32)\n>>> \n>>> for i in range(N):\n...     Y2[i, Y[i]] = 1.0\n... \n>>> Y2\narray([[ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.],\n       [ 0.,  0.,  1.]], dtype=float32)\n>>> \n>>> index = np.arange(N)\n>>> index\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149])\n>>> \n>>> xtrain = X[index[index % 2 != 0], :]\n>>> ytrain = Y2[index[index % 2 != 0], :]\n>>> xtest = X[index[index % 2 == 0], :]\n>>> yans = Y[index[index % 2 == 0]]\n>>> yans\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2])\n>>> \n>>> xtest\narray([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],\n       [ 4.69999981,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5999999 ,  1.39999998,  0.2       ],\n       [ 4.5999999 ,  3.4000001 ,  1.39999998,  0.30000001],\n       [ 4.4000001 ,  2.9000001 ,  1.39999998,  0.2       ],\n       [ 5.4000001 ,  3.70000005,  1.5       ,  0.2       ],\n       [ 4.80000019,  3.        ,  1.39999998,  0.1       ],\n       [ 5.80000019,  4.        ,  1.20000005,  0.2       ],\n       [ 5.4000001 ,  3.9000001 ,  1.29999995,  0.40000001],\n       [ 5.69999981,  3.79999995,  1.70000005,  0.30000001],\n       [ 5.4000001 ,  3.4000001 ,  1.70000005,  0.2       ],\n       [ 4.5999999 ,  3.5999999 ,  1.        ,  0.2       ],\n       [ 4.80000019,  3.4000001 ,  1.89999998,  0.2       ],\n       [ 5.        ,  3.4000001 ,  1.60000002,  0.40000001],\n       [ 5.19999981,  3.4000001 ,  1.39999998,  0.2       ],\n       [ 4.80000019,  3.0999999 ,  1.60000002,  0.2       ],\n       [ 5.19999981,  4.0999999 ,  1.5       ,  0.1       ],\n       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n       [ 5.5       ,  3.5       ,  1.29999995,  0.2       ],\n       [ 4.4000001 ,  3.        ,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5       ,  1.29999995,  0.30000001],\n       [ 4.4000001 ,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.0999999 ,  3.79999995,  1.89999998,  0.40000001],\n       [ 5.0999999 ,  3.79999995,  1.60000002,  0.2       ],\n       [ 5.30000019,  3.70000005,  1.5       ,  0.2       ],\n       [ 7.        ,  3.20000005,  4.69999981,  1.39999998],\n       [ 6.9000001 ,  3.0999999 ,  4.9000001 ,  1.5       ],\n       [ 6.5       ,  2.79999995,  4.5999999 ,  1.5       ],\n       [ 6.30000019,  3.29999995,  4.69999981,  1.60000002],\n       [ 6.5999999 ,  2.9000001 ,  4.5999999 ,  1.29999995],\n       [ 5.        ,  2.        ,  3.5       ,  1.        ],\n       [ 6.        ,  2.20000005,  4.        ,  1.        ],\n       [ 5.5999999 ,  2.9000001 ,  3.5999999 ,  1.29999995],\n       [ 5.5999999 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.19999981,  2.20000005,  4.5       ,  1.5       ],\n       [ 5.9000001 ,  3.20000005,  4.80000019,  1.79999995],\n       [ 6.30000019,  2.5       ,  4.9000001 ,  1.5       ],\n       [ 6.4000001 ,  2.9000001 ,  4.30000019,  1.29999995],\n       [ 6.80000019,  2.79999995,  4.80000019,  1.39999998],\n       [ 6.        ,  2.9000001 ,  4.5       ,  1.5       ],\n       [ 5.5       ,  2.4000001 ,  3.79999995,  1.10000002],\n       [ 5.80000019,  2.70000005,  3.9000001 ,  1.20000005],\n       [ 5.4000001 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],\n       [ 5.5999999 ,  3.        ,  4.0999999 ,  1.29999995],\n       [ 5.5       ,  2.5999999 ,  4.4000001 ,  1.20000005],\n       [ 5.80000019,  2.5999999 ,  4.        ,  1.20000005],\n       [ 5.5999999 ,  2.70000005,  4.19999981,  1.29999995],\n       [ 5.69999981,  2.9000001 ,  4.19999981,  1.29999995],\n       [ 5.0999999 ,  2.5       ,  3.        ,  1.10000002],\n       [ 6.30000019,  3.29999995,  6.        ,  2.5       ],\n       [ 7.0999999 ,  3.        ,  5.9000001 ,  2.0999999 ],\n       [ 6.5       ,  3.        ,  5.80000019,  2.20000005],\n       [ 4.9000001 ,  2.5       ,  4.5       ,  1.70000005],\n       [ 6.69999981,  2.5       ,  5.80000019,  1.79999995],\n       [ 6.5       ,  3.20000005,  5.0999999 ,  2.        ],\n       [ 6.80000019,  3.        ,  5.5       ,  2.0999999 ],\n       [ 5.80000019,  2.79999995,  5.0999999 ,  2.4000001 ],\n       [ 6.5       ,  3.        ,  5.5       ,  1.79999995],\n       [ 7.69999981,  2.5999999 ,  6.9000001 ,  2.29999995],\n       [ 6.9000001 ,  3.20000005,  5.69999981,  2.29999995],\n       [ 7.69999981,  2.79999995,  6.69999981,  2.        ],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.0999999 ],\n       [ 6.19999981,  2.79999995,  4.80000019,  1.79999995],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.0999999 ],\n       [ 7.4000001 ,  2.79999995,  6.0999999 ,  1.89999998],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005],\n       [ 6.0999999 ,  2.5999999 ,  5.5999999 ,  1.39999998],\n       [ 6.30000019,  3.4000001 ,  5.5999999 ,  2.4000001 ],\n       [ 6.        ,  3.        ,  4.80000019,  1.79999995],\n       [ 6.69999981,  3.0999999 ,  5.5999999 ,  2.4000001 ],\n       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.5       ],\n       [ 6.30000019,  2.5       ,  5.        ,  1.89999998],\n       [ 6.19999981,  3.4000001 ,  5.4000001 ,  2.29999995]], dtype=float32)\n>>> \n```\n\n\n```{Python:Python 3.5.2} \n>>> import chainer\n>>> from chainer import cuda, Function, gradient_check, Variable, optimizers,serializers, utils\n>>> \n>>> from chainer import Link, Chain, ChainList\n>>> import chainer.functions as F\n>>> import chainer.links as L\n>>>\n>>> class IrisChain(Chain):\n...     def __init__(self):\n...             super(IrisChain, self).__init__(\n...                     l1=L.Linear(4,6),\n...                     l2=L.Linear(6,3),\n...             )\n...     def __call__(self, x, y):\n...             return F.mean_squared_error(self.fwd(x), y)\n...     def fwd(self, x):\n...             h1 = F.sigmoid(self.l1(x))\n...             h2 = self.l2(h1)\n...             return h2\n... \n>>> \n>>> \n>>> model = IrisChain()\n>>>\n>>> optimizer = optimizers.SGD()\n>>>\n>>> optimizer.setup(model)\n>>>\n>>> for i in range(1000):\n...     x = Variable(xtrain)\n...     y = Variable(ytrain)\n...     model.zerograds()\n...     loss = model(x, y)\n...     loss.backward()\n...     optimizer.update()\n... \n>>>\n```\n\n```{Python:Python 3.5.2} \n>>> xt = Variable(xtest, volatile='on')\n>>> xt\n<variable at 0x10e1f5f98>\n>>> \n>>> xt.data\narray([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],\n       [ 4.69999981,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5999999 ,  1.39999998,  0.2       ],\n       [ 4.5999999 ,  3.4000001 ,  1.39999998,  0.30000001],\n       [ 4.4000001 ,  2.9000001 ,  1.39999998,  0.2       ],\n       [ 5.4000001 ,  3.70000005,  1.5       ,  0.2       ],\n       [ 4.80000019,  3.        ,  1.39999998,  0.1       ],\n       [ 5.80000019,  4.        ,  1.20000005,  0.2       ],\n       [ 5.4000001 ,  3.9000001 ,  1.29999995,  0.40000001],\n       [ 5.69999981,  3.79999995,  1.70000005,  0.30000001],\n       [ 5.4000001 ,  3.4000001 ,  1.70000005,  0.2       ],\n       [ 4.5999999 ,  3.5999999 ,  1.        ,  0.2       ],\n       [ 4.80000019,  3.4000001 ,  1.89999998,  0.2       ],\n       [ 5.        ,  3.4000001 ,  1.60000002,  0.40000001],\n       [ 5.19999981,  3.4000001 ,  1.39999998,  0.2       ],\n       [ 4.80000019,  3.0999999 ,  1.60000002,  0.2       ],\n       [ 5.19999981,  4.0999999 ,  1.5       ,  0.1       ],\n       [ 4.9000001 ,  3.0999999 ,  1.5       ,  0.1       ],\n       [ 5.5       ,  3.5       ,  1.29999995,  0.2       ],\n       [ 4.4000001 ,  3.        ,  1.29999995,  0.2       ],\n       [ 5.        ,  3.5       ,  1.29999995,  0.30000001],\n       [ 4.4000001 ,  3.20000005,  1.29999995,  0.2       ],\n       [ 5.0999999 ,  3.79999995,  1.89999998,  0.40000001],\n       [ 5.0999999 ,  3.79999995,  1.60000002,  0.2       ],\n       [ 5.30000019,  3.70000005,  1.5       ,  0.2       ],\n       [ 7.        ,  3.20000005,  4.69999981,  1.39999998],\n       [ 6.9000001 ,  3.0999999 ,  4.9000001 ,  1.5       ],\n       [ 6.5       ,  2.79999995,  4.5999999 ,  1.5       ],\n       [ 6.30000019,  3.29999995,  4.69999981,  1.60000002],\n       [ 6.5999999 ,  2.9000001 ,  4.5999999 ,  1.29999995],\n       [ 5.        ,  2.        ,  3.5       ,  1.        ],\n       [ 6.        ,  2.20000005,  4.        ,  1.        ],\n       [ 5.5999999 ,  2.9000001 ,  3.5999999 ,  1.29999995],\n       [ 5.5999999 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.19999981,  2.20000005,  4.5       ,  1.5       ],\n       [ 5.9000001 ,  3.20000005,  4.80000019,  1.79999995],\n       [ 6.30000019,  2.5       ,  4.9000001 ,  1.5       ],\n       [ 6.4000001 ,  2.9000001 ,  4.30000019,  1.29999995],\n       [ 6.80000019,  2.79999995,  4.80000019,  1.39999998],\n       [ 6.        ,  2.9000001 ,  4.5       ,  1.5       ],\n       [ 5.5       ,  2.4000001 ,  3.79999995,  1.10000002],\n       [ 5.80000019,  2.70000005,  3.9000001 ,  1.20000005],\n       [ 5.4000001 ,  3.        ,  4.5       ,  1.5       ],\n       [ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],\n       [ 5.5999999 ,  3.        ,  4.0999999 ,  1.29999995],\n       [ 5.5       ,  2.5999999 ,  4.4000001 ,  1.20000005],\n       [ 5.80000019,  2.5999999 ,  4.        ,  1.20000005],\n       [ 5.5999999 ,  2.70000005,  4.19999981,  1.29999995],\n       [ 5.69999981,  2.9000001 ,  4.19999981,  1.29999995],\n       [ 5.0999999 ,  2.5       ,  3.        ,  1.10000002],\n       [ 6.30000019,  3.29999995,  6.        ,  2.5       ],\n       [ 7.0999999 ,  3.        ,  5.9000001 ,  2.0999999 ],\n       [ 6.5       ,  3.        ,  5.80000019,  2.20000005],\n       [ 4.9000001 ,  2.5       ,  4.5       ,  1.70000005],\n       [ 6.69999981,  2.5       ,  5.80000019,  1.79999995],\n       [ 6.5       ,  3.20000005,  5.0999999 ,  2.        ],\n       [ 6.80000019,  3.        ,  5.5       ,  2.0999999 ],\n       [ 5.80000019,  2.79999995,  5.0999999 ,  2.4000001 ],\n       [ 6.5       ,  3.        ,  5.5       ,  1.79999995],\n       [ 7.69999981,  2.5999999 ,  6.9000001 ,  2.29999995],\n       [ 6.9000001 ,  3.20000005,  5.69999981,  2.29999995],\n       [ 7.69999981,  2.79999995,  6.69999981,  2.        ],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.0999999 ],\n       [ 6.19999981,  2.79999995,  4.80000019,  1.79999995],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.0999999 ],\n       [ 7.4000001 ,  2.79999995,  6.0999999 ,  1.89999998],\n       [ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005],\n       [ 6.0999999 ,  2.5999999 ,  5.5999999 ,  1.39999998],\n       [ 6.30000019,  3.4000001 ,  5.5999999 ,  2.4000001 ],\n       [ 6.        ,  3.        ,  4.80000019,  1.79999995],\n       [ 6.69999981,  3.0999999 ,  5.5999999 ,  2.4000001 ],\n       [ 5.80000019,  2.70000005,  5.0999999 ,  1.89999998],\n       [ 6.69999981,  3.29999995,  5.69999981,  2.5       ],\n       [ 6.30000019,  2.5       ,  5.        ,  1.89999998],\n       [ 6.19999981,  3.4000001 ,  5.4000001 ,  2.29999995]], dtype=float32)\n>>> \n```\n\n```{Python:Python 3.5.2} \n>>> yt = model.fwd(xt)\n>>>\n>>> yt\n<variable at 0x10e1f5588>\n>>>\n>>> yt.data\narray([[ 0.65049964,  0.23795809,  0.24554214],\n       [ 0.62719226,  0.23875675,  0.22648317],\n       [ 0.64241773,  0.23161672,  0.24438597],\n       [ 0.59631473,  0.23288801,  0.23215252],\n       [ 0.58576518,  0.25176534,  0.21117243],\n       [ 0.66428065,  0.23924124,  0.25869802],\n       [ 0.63109839,  0.25416443,  0.22202918],\n       [ 0.73607868,  0.21926098,  0.27093801],\n       [ 0.67306954,  0.22140984,  0.26996952],\n       [ 0.65458411,  0.24881724,  0.27508536],\n       [ 0.63596576,  0.26035985,  0.25529292],\n       [ 0.65964806,  0.19990538,  0.22868469],\n       [ 0.55474687,  0.26271337,  0.23830664],\n       [ 0.59476733,  0.25154948,  0.25202763],\n       [ 0.658508  ,  0.24384874,  0.24651586],\n       [ 0.59510976,  0.2601572 ,  0.23061764],\n       [ 0.65880483,  0.21749733,  0.25574028],\n       [ 0.62710112,  0.25634563,  0.22806311],\n       [ 0.69697362,  0.23948437,  0.25520656],\n       [ 0.5996784 ,  0.24176781,  0.21221915],\n       [ 0.64568824,  0.23134014,  0.24646805],\n       [ 0.60062736,  0.23199877,  0.21577176],\n       [ 0.56366283,  0.25170848,  0.26456407],\n       [ 0.62552553,  0.23490125,  0.25275666],\n       [ 0.65573025,  0.23768446,  0.25596967],\n       [ 0.25162345,  0.36968628,  0.38573527],\n       [ 0.22027135,  0.37532166,  0.38907501],\n       [ 0.22420493,  0.37677348,  0.37243152],\n       [ 0.19910896,  0.37140176,  0.37709099],\n       [ 0.24390751,  0.37491554,  0.36819336],\n       [ 0.281192  ,  0.37256598,  0.2826277 ],\n       [ 0.29205048,  0.37670088,  0.32333279],\n       [ 0.28298745,  0.3564935 ,  0.32707334],\n       [ 0.18488854,  0.37344661,  0.3460111 ],\n       [ 0.2176438 ,  0.3850477 ,  0.35694426],\n       [ 0.16189924,  0.37328535,  0.37198263],\n       [ 0.19034311,  0.38613886,  0.36839598],\n       [ 0.26136968,  0.37009892,  0.35905325],\n       [ 0.22990566,  0.37844852,  0.37835509],\n       [ 0.20544407,  0.37538639,  0.35853291],\n       [ 0.27136594,  0.37146965,  0.31252044],\n       [ 0.27191132,  0.3674165 ,  0.33076164],\n       [ 0.17530948,  0.37267056,  0.33839971],\n       [ 0.22682884,  0.37311682,  0.3818129 ],\n       [ 0.23151192,  0.36670592,  0.33391237],\n       [ 0.20754093,  0.37984082,  0.32642967],\n       [ 0.26151592,  0.37155959,  0.331011  ],\n       [ 0.22269309,  0.37436131,  0.33236569],\n       [ 0.22816277,  0.37066641,  0.33755785],\n       [ 0.33448419,  0.34711313,  0.28980288],\n       [ 0.09845747,  0.37358919,  0.41850656],\n       [ 0.13468292,  0.38254431,  0.42583305],\n       [ 0.11796579,  0.38091168,  0.41271448],\n       [ 0.14768445,  0.37716714,  0.32236713],\n       [ 0.13980955,  0.39015827,  0.40133795],\n       [ 0.15918028,  0.37595895,  0.40041429],\n       [ 0.14390096,  0.38079563,  0.41431043],\n       [ 0.11919124,  0.37685126,  0.38937098],\n       [ 0.14521134,  0.38309085,  0.3964771 ],\n       [ 0.11141011,  0.38486871,  0.45222563],\n       [ 0.13043676,  0.37796527,  0.42679274],\n       [ 0.12379643,  0.3864055 ,  0.44254023],\n       [ 0.13072453,  0.37825826,  0.41605335],\n       [ 0.17543721,  0.37968686,  0.37833184],\n       [ 0.12674943,  0.38342461,  0.40337616],\n       [ 0.14262259,  0.3864415 ,  0.42629564],\n       [ 0.12340668,  0.38253206,  0.40663058],\n       [ 0.1465188 ,  0.39115438,  0.36892551],\n       [ 0.11346267,  0.3727971 ,  0.41301125],\n       [ 0.16633913,  0.3766984 ,  0.37390679],\n       [ 0.12529424,  0.37775251,  0.42312664],\n       [ 0.13806322,  0.38252181,  0.37191916],\n       [ 0.11825769,  0.37486488,  0.42833257],\n       [ 0.16253296,  0.38508436,  0.38451606],\n       [ 0.12144916,  0.37244728,  0.40509725]], dtype=float32)\n>>> \n```\n\n```{Python:Python 3.5.2} \n>>> ans = yt.data\n>>> \n>>> nrow, ncols = ans.shape\n>>> ans.shape\n(75, 3)\n>>> \n>>> nrow, ncol = ans.shape\n>>>\n```\n\n```{Python:Python 3.5.2} \n>>> ok = 0\n>>> \n>>> for i in range(nrow):\n...     cls = np.argmax(ans[i, :])\n...     if cls == yans[i]:\n...             ok += 1\n... \n>>> \n>>> print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n65 / 75  =  0.8666666666666667\n>>>\n```\n\n* 10,000\u56de epoch \u5b66\u7fd2 : \u6b63\u89e3\u7387 0.96\n\n```{Python:Python 3.5.2} \n>>> for i in range(10000):\n...     x = Variable(xtrain)\n...     y = Variable(ytrain)\n...     model.zerograds()\n...     loss = model(x, y)\n...     loss.backward()\n...     optimizer.update()\n... \n>>> \n>>> xt = Variable(xtest, volatile='on')\n>>> yt = model.fwd(xt)\n>>> ans = yt.data\n>>> nrow, ncol = ans.shape\n>>> \n>>> ok = 0\n>>> \n>>> for i in range(nrow):\n...     cls = np.argmax(ans[i, :])\n...     if cls == yans[i]:\n...             ok += 1\n... \n>>> \n>>> print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n72 / 75  =  0.96\n>>>\n```\n\n* \u30e2\u30c7\u30eb\u5b66\u7fd2\uff08\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u306b\u3088\u308b\u91cd\u307f\u5b66\u7fd2\uff09\u5b9f\u884c\u30e1\u30bd\u30c3\u30c9 \u3068 \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067\u306e\u5224\u5225\u6b63\u89e3\u7387\u51fa\u529b\u30e1\u30bd\u30c3\u30c9 \u3092 \u7528\u610f\n\n```{Python:Python 3.5.2}  \n>>> def bp_learning_iteration(n):\n...     for i in range(n):\n...             x = Variable(xtrain)\n...             y = Variable(ytrain)\n...             model.zerograds()\n...             loss = model(x, y)\n...             loss.backward()\n...             optimizer.update()\n... \n>>>\n```\n\n```{Python:Python 3.5.2} \n>>> def print_classification_accuracy():\n...     xt = Variable(xtest, volatile='on')\n...     yt = model.fwd(xt)\n...     ans = yt.data\n...     nrow, ncol = ans.shape\n...     ok = 0\n...     for i in range(nrow):\n...             cls = np.argmax(ans[i, :])\n...             if cls == yans[i]:\n...                     ok += 1\n...     print(ok, \"/\", nrow, \" = \", (ok * 1.0)/nrow)\n... \n>>>\n```\n\n* 50,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.96\n\n```{Python:Python 3.5.2} \n>>> bp_learning_iteration(50000)\n>>> print_classification_accuracy()\n72 / 75  =  0.96\n>>>\n```\n\n* 100,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.9733\n\n```{Python:Python 3.5.2}\n>>> bp_learning_iteration(100000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>>\n```\n\n* 200,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387  0.9733\n\n```{Python:Python 3.5.2}\n>>> bp_learning_iteration(200000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>> \n```\n\n* 400,000 epoch \u5b66\u7fd2 : \u6210\u679c\u7387 0.9733\n\n```{Python:Python 3.5.2}\n>>> bp_learning_iteration(400000)\n>>> print_classification_accuracy()\n73 / 75  =  0.9733333333333334\n>>> \n```\n\n___\n\n###__\u3010 \u53c2\u8003 \u3011\u5224\u5225\u554f\u984c \u306e \u6d3b\u6027\u5316\u95a2\u6570__\n\n####__Sigmoid\u95a2\u6570\u3001 Softmax\u95a2\u6570\u3001 Cross entropy\u95a2\u6570__\n\n* [Hatena Blog hshinji\u306e\u30e1\u30e2 \uff082015-05-15\uff09\u300c\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u52c9\u5f37\u306e\u307e\u3068\u3081\u3000\u305d\u306e1\u300d](http://hshinji.hateblo.jp/entry/2015/05/15/173000)\n* [Hatena Blog hshinji\u306e\u30e1\u30e2 \uff082015-05-20\uff09\u300c\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u52c9\u5f37\u306e\u307e\u3068\u3081\u3000\u305d\u306e2\u300d](http://hshinji.hateblo.jp/entry/2015/05/20/081530)\n* [\u9ad8\u6821\u6570\u5b66\u306e\u7f8e\u3057\u3044\u7269\u8a9e \uff082016/05/22\uff09\u300c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u300d](http://mathtrain.jp/softmax)\n* [kenmatsu4\u3055\u3093 Qiita\u8a18\u4e8b\uff08\u6700\u7d42\u66f4\u65b0: 2015\u5e7409\u670826\u65e5\uff09\u300c\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0 \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30afChainer\u3092\u8a66\u3057\u306a\u304c\u3089\u89e3\u8aac\u3057\u3066\u307f\u308b\u3002\u300d](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)\n* [TomokIshii\u3055\u3093 Qiita\u8a18\u4e8b\uff08\u6700\u7d42\u66f4\u65b0: 2016\u5e7405\u670830\u65e5\uff09\u300c\u843d\u3061\u3053\u307c\u308c\u306a\u3044\u305f\u3081\u306eTensorFlow Tutorial\u30b3\u30fc\u30c9\u300d](http://qiita.com/TomokIshii/items/92a266b805d7eee02b1d)\n* [Hatena Blog neuralnet\u306e\u65e5\u8a18 \uff082016-05-17\uff09\u300c\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc(Cross Entropy)\u300d](http://neuralnet.hatenablog.jp/entry/2016/05/17/010816)\n\n\n####__Optimizer \u306e \u9078\u629e__\n\n* [\u4ffa\u3068\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 \uff082016-02-11\uff09 \u300cCNN\u306e\u5b66\u7fd2\u306b\u6700\u9ad8\u306e\u6027\u80fd\u3092\u793a\u3059\u6700\u9069\u5316\u624b\u6cd5\u306f\u3069\u308c\u304b\u300d](http://www.iandprogram.net/entry/2016/02/11/181322)\n* [hogefugabar\u3055\u3093 Qiita\u8a18\u4e8b \uff08\u6700\u7d42\u66f4\u65b0\u65e5:2016\u5e7408\u670822\u65e5\uff09 \u300c\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8272\u3005\u8a66\u3057\u3066\u307f\u308b\u300d](http://qiita.com/hogefugabar/items/1d4f6c905d0edbc71af2)\n* [JProgramer \uff082015\u5e7411\u670819\u65e5\uff09 \u300cChainer\u306e\u8a73\u3057\u3044\u69cb\u9020\u300d](http://jprogramer.com/ai/3758)\n", "tags": ["Chainer", "DeepLearning", "\u6df1\u5c64\u5b66\u7fd2", "MachineLearning", "Python"]}