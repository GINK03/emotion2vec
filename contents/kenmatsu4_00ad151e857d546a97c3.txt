{"context": " More than 1 year has passed since last update.Spark\u3092iPython Notebook(Jupyter)\u3067\u52d5\u4f5c\u3055\u305b\u3001MLlib\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u30c6\u30b9\u30c8\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\uff08KMeans\uff09\u3001\u5206\u985e:Classification\uff08SVM, \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30, Random Forest\uff09\u3092iris\u30c7\u30fc\u30bf\u3067\u8a66\u3057\u307e\u3057\u305f\u3002\n\u74b0\u5883\n\nOS: Mac OSX Yosemite 10.10.3\nSpark: spark-1.5.0-bin-hadoop2.6\nPython: 2.7.10 |Anaconda 2.2.0 (x86_64)| (default, May 28 2015, 17:04:42) \n\n\u672c\u7a3f\u3067\u306f\u4e0a\u8a18\u306e\u74b0\u5883\u3067\u884c\u3063\u305f\u3082\u306e\u3092\u8a18\u8f09\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u4ed6\u306e\u74b0\u5883\u3067\u306f\u8a2d\u5b9a\u304c\u7570\u306a\u308b\u5834\u5408\u3082\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002\n\n1. Spark\u30d0\u30a4\u30ca\u30ea\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9&\u914d\u7f6e\nhttp://spark.apache.org/downloads.html\n\u304b\u3089\n\u3000\u3000spark-1.5.0-bin-hadoop2.6.tgz\n\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002(2015/9/14 \u73fe\u5728)\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30d0\u30a4\u30ca\u30ea\u3092\u56de\u7b54\u3057\u3066\u9069\u5207\u306a\u5834\u6240\u306b\u914d\u7f6e\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f/usr/local/bin/\u306b\u7f6e\u3044\u3066\u3044\u307e\u3059\u3002\ntar zxvf spark-1.5.0-bin-hadoop2.6.tar \nmv spark-1.5.0-bin-hadoop2.6 /usr/local/bin/\n\n\u74b0\u5883\u5909\u6570SPARK_HOME\u3092.bashrc\u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u4e0b\u8a18\u3092\u8ffd\u8a18\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\uff08\u521d\u56de\u306fsource ~/.bashrc\u3067\u518d\u8aad\u307f\u8fbc\u307f\u3092\u3057\u3066\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304f\u3060\u3055\u3044\u3002\uff09\n\n.bashrc\nexport SPARK_HOME=/usr/local/bin/spark-1.5.0-bin-hadoop2.6\n\n\n\u6e96\u5099\u304c\u3067\u304d\u305f\u3089iPython Notebook\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\nipython notebook\n\n\n1. iPython Notebook\u3067Spark\u306e\u8d77\u52d5\niPython Notebook\u3067\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\nimport os, sys\nfrom datetime import datetime as dt\nprint \"loading PySpark setting...\"\nspark_home = os.environ.get('SPARK_HOME', None)\nif not spark_home:\n    raise ValueError('SPARK_HOME environment variable is not set')\nsys.path.insert(0, os.path.join(spark_home, 'python'))\nsys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\nexecfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n\n\n\u3053\u3093\u306a\u8868\u793a\u304c\u3055\u308c\u308c\u3070\u6210\u529f\u3067\u3059  \nloading PySpark setting...\n/usr/local/bin/spark-1.5.0-bin-hadoop2.6\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n      /_/\n\nUsing Python version 2.7.10 (default, May 28 2015 17:04:42)\nSparkContext available as sc, HiveContext available as sqlContext.\n\n\n2. MLlib(KMeans)\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\n\n2-1. \u30c7\u30fc\u30bf\u306e\u6e96\u5099\n\u304a\u306a\u3058\u307firis\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002Scikit-learn\u3088\u308a\u7c21\u5358\u306b\u5165\u624b\u3067\u304d\u308b\u306e\u3067\u3053\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3068\u308a\u3042\u3048\u305a('sepal length','petal length')\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u8a66\u3057\u307e\u3059\u3002\u4f55\u306f\u3068\u3082\u3042\u308c\u6563\u5e03\u56f3\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u307e\u3059\u3002\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nplt.style.use('ggplot')\n\n# http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\niris = datasets.load_iris()\nplt.figure(figsize=(9,7))\n\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7)\n\nplt.show()\n\nIris-Setosa\u3001Iris-Versicolour\u3001Iris-Virginica\u306e3\u7a2e\u985e\u3092\u8272\u5206\u3051\u3057\u3066\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n2-2 KMeans\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\n\u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u624b\u6cd5\u306e\uff11\u3064KMeans\u3092\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\u3082\u3068\u3082\u30683\u3064\u306e\u7a2e\u5225\u306eiris\u3067\u3059\u306e\u3067k=3\u306b\u3057\u3066\u6b63\u3057\u304f\u5224\u5b9a\u3067\u304d\u308b\u304b\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.clustering\n\nfrom pyspark.mllib.clustering import KMeans\n\nk = 3\nd_start = dt.now()\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndata = sc.parallelize(iris.data[:,[0,2]])\n\n# KMeans\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = KMeans.train(data, k, initializationMode=\"random\", seed=None)\n\n# \u7d50\u679c\u8868\u793a\nprint(\"Final centers: \" + str(model.clusterCenters))\nprint(\"Total Cost: \" + str(model.computeCost(data)))\ndiff = dt.now() - d_start\nprint(\"{}: [end] {}\".format(dt.now().strftime('%H:%M:%S'), diff ))\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7)\n\nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\")\n\nplt.show()\n\n\u306a\u3093\u3068\u306a\u304f\u5404\u7a2e\u985e\u3054\u3068\u306e\u4e2d\u5fc3\u4ed8\u8fd1\u306bKMeans\u3067\u8a08\u7b97\u3055\u308c\u305fCenter\u304c\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u306d  \n(KMeans\u306e\u7d50\u679c\u306f\u521d\u671f\u5024\u306b\u4f9d\u5b58\u3059\u308b\u306e\u3067\u3001\u3082\u3063\u3068\u5909\u306a\u7d50\u679c\u3068\u306a\u308b\u6642\u3082\u3042\u308a\u307e\u3059)\n\nSpark\u3092\u4f7f\u3063\u3066\u3044\u308b\u90e8\u5206\u306f\u307b\u3093\u306e\u5c11\u3057\u3060\u3051\u3067\u3059\u304c\u3001\u304d\u3061\u3093\u3068\u5206\u6563\u51e6\u7406\u8a2d\u5b9a\u3092\u884c\u3048\u3070\u3053\u308c\u3067\u51e6\u7406\u304c\u5206\u6563\u3055\u308c\u307e\u3059\u3002\uff08\u4eca\u56de\u306fStandalone\u3067\u306e\u304a\u8a66\u3057\u306a\u306e\u3067\u5206\u6563\u3055\u308c\u3066\u3044\u307e\u305b\u3093\uff09\n\u30dd\u30a4\u30f3\u30c8\u306f\u3001numpy\u306endarray\u304b\u3089sc.parallelize()\u3067Spark\u306eRDD\u306b\u5909\u63db\u3057\u3066\u305d\u308c\u3092Spark\u306eKMeans\u30af\u30e9\u30b9\u306e\u5b66\u7fd2\u7528\u95a2\u6570train()\u306b\u6e21\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u3067\u3059\u3002\ndata = sc.parallelize(iris.data[:,[0,2]])\nmodel = KMeans.train(data, k, initializationMode=\"random\", seed=None)\n\nk\u500b\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u305d\u308c\u305e\u308c\u306e\u4e2d\u5fc3\u70b9\u3092\u53d6\u5f97\u3059\u308b\u306b\u306fmodel.clusterCenters\u3092\u4f7f\u3044\u307e\u3059\u3002\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u306fk=3\u500b\u306e\u4e2d\u5fc3\u3092\u53d6\u5f97\u3057\u3066\u25b2\u30de\u30fc\u30af\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\")\n\n\u6700\u5f8c\u306b\u5ea7\u6a19\u306e\u5404\u70b9\u304c\u3069\u306e\u3088\u3046\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u305f\u304b\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u3066\u307f\u307e\u3059\u3002\n# ------- Create Color Map ------- #\nxmin = 4.0\nxmax = 8.5\nymin = 0\nymax = 8\nn = 100\nxx = np.linspace(xmin, xmax, n)\nyy = np.linspace(ymin, ymax, n)\nX, Y = np.meshgrid(xx, yy)\n\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\", zorder=100)\n\n\nplt.pcolor(X, Y, Z, alpha=0.3)\n\n\n\n3. MLlib(SVM)\u3067Classification\n\u540c\u69d8\u306b\u4eca\u5ea6\u306f\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u30de\u30b7\u30f3\u3067\u3059\u3002\u3053\u308c\u306f2\u5024\u5206\u985e\u306a\u306e\u3067\u3001iris\u30c7\u30fc\u30bf\u30922\u7a2e\u306b\u7d5e\u308a\u307e\u3059\u3002\nfrom pyspark.mllib.classification import SVMWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\n\n# SVM\u306f2\u5024\u5206\u985e\u306a\u306e\u3067\u30012\u7a2e\u985e\u306b\u7d5e\u308b\nidx = np.r_[ np.where(iris.target == 1)[0],  np.where(iris.target == 2)[0]]\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndat = np.column_stack([iris.target[idx]-1, iris.data[idx,0],iris.data[idx,2]])\ndata = sc.parallelize(dat)\ndef parsePoint(vec):\n    return LabeledPoint(vec[0], vec[1:])\nparsedData = data.map(parsePoint)\n\n# SVM\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = SVMWithSGD.train(parsedData, iterations=5000)\n\n# ------- Predict Data ------- #\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nxmin = 4.0\nxmax = 8.5\nymin = 2\nymax = 8\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rb'):\n    idx = np.where(iris.target == i+1)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n\n\n\n4. MLlib(\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30)\u3067Classification\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3059\u3002\u3053\u3061\u3089\u30822\u5024\u5206\u985e\u3067\u3059\u306d\u3002\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\n\n# \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = LogisticRegressionWithLBFGS.train(parsedData)\n\n# ------- Predict Data ------- #\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rb'):\n    idx = np.where(iris.target == i+1)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n\n\n\n5. MLlib(\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8)\u3067Classification\n\u6700\u5f8c\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u3059\u3002\u3053\u3061\u3089\u306f\u591a\u5024\u5206\u985e\u304c\u53ef\u80fd\u306a\u306e\u3067\u3001\u307e\u305firis\u306e3\u7a2e\u3067\u5206\u985e\u3092\u884c\u3044\u307e\u3059\u3002\nfrom pyspark.mllib.tree import RandomForest, RandomForestModel\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndat = np.column_stack([iris.target[:], iris.data[:,0],iris.data[:,2]])\ndata = sc.parallelize(dat)\nparsedData = data.map(parsePoint)\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\n(trainingData, testData) = parsedData.randomSplit([0.7, 0.3])\n\n\n# \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = RandomForest.trainClassifier(trainingData, numClasses=3,\n                                     categoricalFeaturesInfo={},\n                                     numTrees=5, featureSubsetStrategy=\"auto\",\n                                     impurity='gini', maxDepth=4, maxBins=32)\n\n# Evaluate model on test instances and compute test error\npredictions = model.predict(testData.map(lambda x: x.features))\nlabelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\ntestErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\nprint('Test Error = ' + str(testErr))\nprint('Learned classification forest model:')\nprint(model.toDebugString())\n\n\u5206\u985e\u7d50\u679c\u306e\u6728\u69cb\u9020\u3082toDebugString()\u3067\u8868\u793a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\nout\nTest Error = 0.0588235294118\nLearned classification forest model:\nTreeEnsembleModel classifier with 5 trees\n\n  Tree 0:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.8)\n      If (feature 1 <= 5.0)\n       If (feature 0 <= 6.3)\n        Predict: 2.0\n       Else (feature 0 > 6.3)\n        Predict: 1.0\n      Else (feature 1 > 5.0)\n       Predict: 2.0\n  Tree 1:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.7)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.7)\n      If (feature 0 <= 6.5)\n       Predict: 2.0\n      Else (feature 0 > 6.5)\n       If (feature 1 <= 5.0)\n        Predict: 1.0\n       Else (feature 1 > 5.0)\n        Predict: 2.0\n  Tree 2:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      If (feature 1 <= 4.7)\n       Predict: 1.0\n      Else (feature 1 > 4.7)\n       If (feature 0 <= 5.9)\n        Predict: 1.0\n       Else (feature 0 > 5.9)\n        Predict: 2.0\n     Else (feature 1 > 4.8)\n      Predict: 2.0\n  Tree 3:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      Predict: 1.0\n     Else (feature 1 > 4.8)\n      Predict: 2.0\n  Tree 4:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.7)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.7)\n      If (feature 1 <= 5.0)\n       If (feature 0 <= 6.0)\n        Predict: 2.0\n       Else (feature 0 > 6.0)\n        Predict: 1.0\n      Else (feature 1 > 5.0)\n       Predict: 2.0\n\n\n\u7d50\u679c\u3092\u63cf\u753b\u3057\u307e\u3059\u3002\n# ------- Predict Data ------- #\nZ = np.zeros_like(X)\nfor i in range(n):\n    for j in range(n):\n        Z[i,j] = model.predict([xx[j],yy[i]])\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nxmin = 4.0\nxmax = 8.5\nymin = 0\nymax = 8\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n\n\n\u6b21\u306f\u30ec\u30b3\u30e1\u30f3\u30c9\u3092\u6271\u3044\u307e\u3059\u3002\n\u300c\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011Spark MLlib\u3092Python\u3067\u52d5\u304b\u3057\u3066\u30ec\u30b3\u30e1\u30f3\u30c7\u30fc\u30b7\u30e7\u30f3\u3057\u3066\u307f\u308b\u300d\n\u3000\u3000http://qiita.com/kenmatsu4/items/42fa2f17865f7914688d\n\n\u4fee\u6b63\u4e88\u5b9a\nRandomForest\u306epredict()\u306e\u4e26\u5217\u5316\u3067\u30a8\u30e9\u30fc\u767a\u751f\u4e2d\u3067\u3001\u5bfe\u5fdc\u65b9\u6cd5\u6a21\u7d22\u4e2d\u3067\u3059\u3002\n--> \u3069\u3046\u3084\u3089\u3053\u308c\u304c\u539f\u56e0\u3063\u307d\u3044\u30fb\u30fb\u30fb\u3002\n\u3000 \u300cSpark / RDD \u306e\u30cd\u30b9\u30c8\u3067\u304d\u306a\u3044\uff01\u300d\n\u3000\u3000\u89e3\u6c7a\u3067\u304d\u306a\u3044\u306e\u304b\u306a\u30fb\u30fb\u30fb\u3002\n\n\u53c2\u8003\nHow-to: Use IPython Notebook with Apache Spark\n\u3000http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/\nSpark 1.5.0 Machine Learning Library (MLlib) Guide\n\u3000http://spark.apache.org/docs/latest/mllib-guide.html\nSpark\u3092iPython Notebook(Jupyter)\u3067\u52d5\u4f5c\u3055\u305b\u3001MLlib\u3092\u52d5\u304b\u3057\u3066\u307f\u308b\u30c6\u30b9\u30c8\u3067\u3059\u3002\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\uff08KMeans\uff09\u3001\u5206\u985e:Classification\uff08SVM, \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30, Random Forest\uff09\u3092iris\u30c7\u30fc\u30bf\u3067\u8a66\u3057\u307e\u3057\u305f\u3002\n\n\n**\u74b0\u5883**\n\n* OS: Mac OSX Yosemite 10.10.3\n* Spark: spark-1.5.0-bin-hadoop2.6\n* Python: 2.7.10 |Anaconda 2.2.0 (x86_64)| (default, May 28 2015, 17:04:42) \n\n\u672c\u7a3f\u3067\u306f\u4e0a\u8a18\u306e\u74b0\u5883\u3067\u884c\u3063\u305f\u3082\u306e\u3092\u8a18\u8f09\u3057\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u4ed6\u306e\u74b0\u5883\u3067\u306f\u8a2d\u5b9a\u304c\u7570\u306a\u308b\u5834\u5408\u3082\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002\n\n\n#1. Spark\u30d0\u30a4\u30ca\u30ea\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9&\u914d\u7f6e\n\nhttp://spark.apache.org/downloads.html\n\u304b\u3089\n\u3000\u3000spark-1.5.0-bin-hadoop2.6.tgz\n\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002(2015/9/14 \u73fe\u5728)\n\n\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30d0\u30a4\u30ca\u30ea\u3092\u56de\u7b54\u3057\u3066\u9069\u5207\u306a\u5834\u6240\u306b\u914d\u7f6e\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f`/usr/local/bin/`\u306b\u7f6e\u3044\u3066\u3044\u307e\u3059\u3002\n\n```bash\ntar zxvf spark-1.5.0-bin-hadoop2.6.tar \nmv spark-1.5.0-bin-hadoop2.6 /usr/local/bin/\n```\n\n\u74b0\u5883\u5909\u6570`SPARK_HOME`\u3092.bashrc\u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u4e0b\u8a18\u3092\u8ffd\u8a18\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\uff08\u521d\u56de\u306f`source ~/.bashrc`\u3067\u518d\u8aad\u307f\u8fbc\u307f\u3092\u3057\u3066\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u3093\u3067\u304f\u3060\u3055\u3044\u3002\uff09\n\n```bash:.bashrc\nexport SPARK_HOME=/usr/local/bin/spark-1.5.0-bin-hadoop2.6\n```\n\n\u6e96\u5099\u304c\u3067\u304d\u305f\u3089iPython Notebook\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\n\n```bash\nipython notebook\n```\n\n\n#1. iPython Notebook\u3067Spark\u306e\u8d77\u52d5\n\niPython Notebook\u3067\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\n```py\nimport os, sys\nfrom datetime import datetime as dt\nprint \"loading PySpark setting...\"\nspark_home = os.environ.get('SPARK_HOME', None)\nif not spark_home:\n    raise ValueError('SPARK_HOME environment variable is not set')\nsys.path.insert(0, os.path.join(spark_home, 'python'))\nsys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\nexecfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n\n```\n\n\u3053\u3093\u306a\u8868\u793a\u304c\u3055\u308c\u308c\u3070\u6210\u529f\u3067\u3059 :laughing: \n\n```log\nloading PySpark setting...\n/usr/local/bin/spark-1.5.0-bin-hadoop2.6\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.0\n      /_/\n\nUsing Python version 2.7.10 (default, May 28 2015 17:04:42)\nSparkContext available as sc, HiveContext available as sqlContext.\n```\n\n#2. MLlib(KMeans)\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\n\n##2-1. \u30c7\u30fc\u30bf\u306e\u6e96\u5099\n\n\u304a\u306a\u3058\u307firis\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002Scikit-learn\u3088\u308a\u7c21\u5358\u306b\u5165\u624b\u3067\u304d\u308b\u306e\u3067\u3053\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3068\u308a\u3042\u3048\u305a('sepal length','petal length')\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u8a66\u3057\u307e\u3059\u3002\u4f55\u306f\u3068\u3082\u3042\u308c\u6563\u5e03\u56f3\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u307e\u3059\u3002\n\n```py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nplt.style.use('ggplot')\n\n# http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\niris = datasets.load_iris()\nplt.figure(figsize=(9,7))\n\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7)\n\nplt.show()\n```\n\nIris-Setosa\u3001Iris-Versicolour\u3001Iris-Virginica\u306e3\u7a2e\u985e\u3092\u8272\u5206\u3051\u3057\u3066\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n\n![iris1-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/ef3e04ce-9c44-d7e1-2413-8fdd6a9d22fd.png)\n\n##2-2 KMeans\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\n\u3053\u306e\u30c7\u30fc\u30bf\u304b\u3089\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u624b\u6cd5\u306e\uff11\u3064KMeans\u3092\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\u3082\u3068\u3082\u30683\u3064\u306e\u7a2e\u5225\u306eiris\u3067\u3059\u306e\u3067k=3\u306b\u3057\u3066\u6b63\u3057\u304f\u5224\u5b9a\u3067\u304d\u308b\u304b\u8a66\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n```py \n# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.clustering\n\nfrom pyspark.mllib.clustering import KMeans\n\nk = 3\nd_start = dt.now()\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndata = sc.parallelize(iris.data[:,[0,2]])\n\n# KMeans\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = KMeans.train(data, k, initializationMode=\"random\", seed=None)\n\n# \u7d50\u679c\u8868\u793a\nprint(\"Final centers: \" + str(model.clusterCenters))\nprint(\"Total Cost: \" + str(model.computeCost(data)))\ndiff = dt.now() - d_start\nprint(\"{}: [end] {}\".format(dt.now().strftime('%H:%M:%S'), diff ))\n\n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7)\n\nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\")\n\nplt.show()\n```\n\n\u306a\u3093\u3068\u306a\u304f\u5404\u7a2e\u985e\u3054\u3068\u306e\u4e2d\u5fc3\u4ed8\u8fd1\u306bKMeans\u3067\u8a08\u7b97\u3055\u308c\u305fCenter\u304c\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u306d :wink: \n(KMeans\u306e\u7d50\u679c\u306f\u521d\u671f\u5024\u306b\u4f9d\u5b58\u3059\u308b\u306e\u3067\u3001\u3082\u3063\u3068\u5909\u306a\u7d50\u679c\u3068\u306a\u308b\u6642\u3082\u3042\u308a\u307e\u3059)\n\n![iris2-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/fa505a55-89df-8fd4-cd31-2f29c9729414.png)\n\nSpark\u3092\u4f7f\u3063\u3066\u3044\u308b\u90e8\u5206\u306f\u307b\u3093\u306e\u5c11\u3057\u3060\u3051\u3067\u3059\u304c\u3001\u304d\u3061\u3093\u3068\u5206\u6563\u51e6\u7406\u8a2d\u5b9a\u3092\u884c\u3048\u3070\u3053\u308c\u3067\u51e6\u7406\u304c\u5206\u6563\u3055\u308c\u307e\u3059\u3002\uff08\u4eca\u56de\u306fStandalone\u3067\u306e\u304a\u8a66\u3057\u306a\u306e\u3067\u5206\u6563\u3055\u308c\u3066\u3044\u307e\u305b\u3093\uff09\n\n\u30dd\u30a4\u30f3\u30c8\u306f\u3001numpy\u306endarray\u304b\u3089sc.parallelize()\u3067Spark\u306eRDD\u306b\u5909\u63db\u3057\u3066\u305d\u308c\u3092Spark\u306eKMeans\u30af\u30e9\u30b9\u306e\u5b66\u7fd2\u7528\u95a2\u6570train()\u306b\u6e21\u3057\u3066\u3044\u308b\u3068\u3053\u308d\u3067\u3059\u3002\n\n```py \ndata = sc.parallelize(iris.data[:,[0,2]])\nmodel = KMeans.train(data, k, initializationMode=\"random\", seed=None)\n```\n\nk\u500b\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u305d\u308c\u305e\u308c\u306e\u4e2d\u5fc3\u70b9\u3092\u53d6\u5f97\u3059\u308b\u306b\u306f`model.clusterCenters`\u3092\u4f7f\u3044\u307e\u3059\u3002\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u306fk=3\u500b\u306e\u4e2d\u5fc3\u3092\u53d6\u5f97\u3057\u3066\u25b2\u30de\u30fc\u30af\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\n\n```py \nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\")\n```\n\n\n\n\u6700\u5f8c\u306b\u5ea7\u6a19\u306e\u5404\u70b9\u304c\u3069\u306e\u3088\u3046\u306b\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3055\u308c\u305f\u304b\u3092\u53ef\u8996\u5316\u3057\u3066\u307f\u3066\u307f\u307e\u3059\u3002\n\n```py \n# ------- Create Color Map ------- #\nxmin = 4.0\nxmax = 8.5\nymin = 0\nymax = 8\nn = 100\nxx = np.linspace(xmin, xmax, n)\nyy = np.linspace(ymin, ymax, n)\nX, Y = np.meshgrid(xx, yy)\n\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n        \n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\nfor i in range(k):\n    plt.scatter(model.clusterCenters[i][0], model.clusterCenters[i][1], s=200, c=\"purple\", alpha=.7, marker=\"^\", zorder=100)\n\n\nplt.pcolor(X, Y, Z, alpha=0.3)\n```\n![pcolor-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/0b52c55e-ab17-9acd-0778-645b19a19da4.png)\n\n#3. MLlib(SVM)\u3067Classification\n\n\u540c\u69d8\u306b\u4eca\u5ea6\u306f\u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u30de\u30b7\u30f3\u3067\u3059\u3002\u3053\u308c\u306f2\u5024\u5206\u985e\u306a\u306e\u3067\u3001iris\u30c7\u30fc\u30bf\u30922\u7a2e\u306b\u7d5e\u308a\u307e\u3059\u3002\n\n```py \nfrom pyspark.mllib.classification import SVMWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\n\n# SVM\u306f2\u5024\u5206\u985e\u306a\u306e\u3067\u30012\u7a2e\u985e\u306b\u7d5e\u308b\nidx = np.r_[ np.where(iris.target == 1)[0],  np.where(iris.target == 2)[0]]\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndat = np.column_stack([iris.target[idx]-1, iris.data[idx,0],iris.data[idx,2]])\ndata = sc.parallelize(dat)\ndef parsePoint(vec):\n    return LabeledPoint(vec[0], vec[1:])\nparsedData = data.map(parsePoint)\n\n# SVM\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = SVMWithSGD.train(parsedData, iterations=5000)\n\n# ------- Predict Data ------- #\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n   \n        \n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nxmin = 4.0\nxmax = 8.5\nymin = 2\nymax = 8\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rb'):\n    idx = np.where(iris.target == i+1)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n```\n\n![SVM1-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/2f215ea3-7f10-e8d3-e794-cd779ec7a2b2.png)\n\n\n#4. MLlib(\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30)\u3067Classification\n\n\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u3059\u3002\u3053\u3061\u3089\u30822\u5024\u5206\u985e\u3067\u3059\u306d\u3002\n\n```py \nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\n\n# \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = LogisticRegressionWithLBFGS.train(parsedData)\n\n# ------- Predict Data ------- #\n# 2015.9.15 \u8ffd\u52a0\uff1apredict\u306e\u5206\u6563\u51e6\u7406\u5316\nf_XY = np.column_stack([X.flatten(), Y.flatten()])\nsc_XY = sc.parallelize(f_XY)\nres = sc_XY.map(lambda data: model.predict(data))  # \u5b66\u7fd2\u3057\u305f\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5404\u70b9\u304c\u3069\u3061\u3089\u306b\u5206\u985e\u3055\u308c\u308b\u304b\u4e88\u6e2c\u5b9f\u884c\nZ = np.array(res.collect()).reshape(X.shape)\n\n# 2015.9.15 \u524a\u9664\n#Z = np.zeros_like(X)\n#for i in range(n):\n#    for j in range(n):\n#        Z[i,j] = model.predict([xx[j],yy[i]])\n        \n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rb'):\n    idx = np.where(iris.target == i+1)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n```\n![Logistic1-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/153d50e7-b3d3-218c-f035-16599be4dffb.png)\n\n\n#5. MLlib(\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8)\u3067Classification\n\n\u6700\u5f8c\u306b\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u3059\u3002\u3053\u3061\u3089\u306f\u591a\u5024\u5206\u985e\u304c\u53ef\u80fd\u306a\u306e\u3067\u3001\u307e\u305firis\u306e3\u7a2e\u3067\u5206\u985e\u3092\u884c\u3044\u307e\u3059\u3002\n\n```py \nfrom pyspark.mllib.tree import RandomForest, RandomForestModel\n\n# Spark\u304c\u8aad\u307f\u53d6\u308c\u308b\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u5909\u63db\ndat = np.column_stack([iris.target[:], iris.data[:,0],iris.data[:,2]])\ndata = sc.parallelize(dat)\nparsedData = data.map(parsePoint)\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\n(trainingData, testData) = parsedData.randomSplit([0.7, 0.3])\n\n\n# \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u5b66\u7fd2\u5b9f\u884c\nmodel = RandomForest.trainClassifier(trainingData, numClasses=3,\n                                     categoricalFeaturesInfo={},\n                                     numTrees=5, featureSubsetStrategy=\"auto\",\n                                     impurity='gini', maxDepth=4, maxBins=32)\n\n# Evaluate model on test instances and compute test error\npredictions = model.predict(testData.map(lambda x: x.features))\nlabelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\ntestErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\nprint('Test Error = ' + str(testErr))\nprint('Learned classification forest model:')\nprint(model.toDebugString())\n```\n\n\u5206\u985e\u7d50\u679c\u306e\u6728\u69cb\u9020\u3082`toDebugString()`\u3067\u8868\u793a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n```log:out\nTest Error = 0.0588235294118\nLearned classification forest model:\nTreeEnsembleModel classifier with 5 trees\n\n  Tree 0:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.8)\n      If (feature 1 <= 5.0)\n       If (feature 0 <= 6.3)\n        Predict: 2.0\n       Else (feature 0 > 6.3)\n        Predict: 1.0\n      Else (feature 1 > 5.0)\n       Predict: 2.0\n  Tree 1:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.7)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.7)\n      If (feature 0 <= 6.5)\n       Predict: 2.0\n      Else (feature 0 > 6.5)\n       If (feature 1 <= 5.0)\n        Predict: 1.0\n       Else (feature 1 > 5.0)\n        Predict: 2.0\n  Tree 2:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      If (feature 1 <= 4.7)\n       Predict: 1.0\n      Else (feature 1 > 4.7)\n       If (feature 0 <= 5.9)\n        Predict: 1.0\n       Else (feature 0 > 5.9)\n        Predict: 2.0\n     Else (feature 1 > 4.8)\n      Predict: 2.0\n  Tree 3:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.8)\n      Predict: 1.0\n     Else (feature 1 > 4.8)\n      Predict: 2.0\n  Tree 4:\n    If (feature 1 <= 1.9)\n     Predict: 0.0\n    Else (feature 1 > 1.9)\n     If (feature 1 <= 4.7)\n      If (feature 0 <= 4.9)\n       Predict: 2.0\n      Else (feature 0 > 4.9)\n       Predict: 1.0\n     Else (feature 1 > 4.7)\n      If (feature 1 <= 5.0)\n       If (feature 0 <= 6.0)\n        Predict: 2.0\n       Else (feature 0 > 6.0)\n        Predict: 1.0\n      Else (feature 1 > 5.0)\n       Predict: 2.0\n```\n\n\u7d50\u679c\u3092\u63cf\u753b\u3057\u307e\u3059\u3002\n\n```py \n# ------- Predict Data ------- #\nZ = np.zeros_like(X)\nfor i in range(n):\n    for j in range(n):\n        Z[i,j] = model.predict([xx[j],yy[i]])\n        \n# ---------- Draw Graph ---------- # \nplt.figure(figsize=(9,7))\nxmin = 4.0\nxmax = 8.5\nymin = 0\nymax = 8\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\n\n# \u70b9\u3092\u30d7\u30ed\u30c3\u30c8\nfor i, color in enumerate('rgb'):\n    idx = np.where(iris.target == i)[0]\n    plt.scatter(iris.data[idx,0],iris.data[idx,2], c=color, s=30, alpha=.7, zorder=100)\n\n# \u5857\u308a\u3064\u3076\u3057\u63cf\u753b\nplt.pcolor(X, Y, Z, alpha=0.3)\n```\n\n![randomforest-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/1ad3cd20-94ee-f205-3234-fc2f5d915f49.png)\n\n\n\u6b21\u306f\u30ec\u30b3\u30e1\u30f3\u30c9\u3092\u6271\u3044\u307e\u3059\u3002\n\u300c\u3010\u6a5f\u68b0\u5b66\u7fd2\u3011Spark MLlib\u3092Python\u3067\u52d5\u304b\u3057\u3066\u30ec\u30b3\u30e1\u30f3\u30c7\u30fc\u30b7\u30e7\u30f3\u3057\u3066\u307f\u308b\u300d\n\u3000\u3000http://qiita.com/kenmatsu4/items/42fa2f17865f7914688d\n\n# \u4fee\u6b63\u4e88\u5b9a\nRandomForest\u306epredict()\u306e\u4e26\u5217\u5316\u3067\u30a8\u30e9\u30fc\u767a\u751f\u4e2d\u3067\u3001\u5bfe\u5fdc\u65b9\u6cd5\u6a21\u7d22\u4e2d\u3067\u3059\u3002\n\n--> \u3069\u3046\u3084\u3089\u3053\u308c\u304c\u539f\u56e0\u3063\u307d\u3044\u30fb\u30fb\u30fb\u3002\n\u3000 \u300c[Spark / RDD \u306e\u30cd\u30b9\u30c8\u3067\u304d\u306a\u3044\uff01](http://blog.yoslab.com/entry/2015/02/07/005533)\u300d\n\u3000\u3000\u89e3\u6c7a\u3067\u304d\u306a\u3044\u306e\u304b\u306a\u30fb\u30fb\u30fb\u3002\n\n\n# \u53c2\u8003\nHow-to: Use IPython Notebook with Apache Spark\n\u3000http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/\n\nSpark 1.5.0 Machine Learning Library (MLlib) Guide\n\u3000http://spark.apache.org/docs/latest/mllib-guide.html\n", "tags": ["Spark", "Python", "\u6a5f\u68b0\u5b66\u7fd2", "MLlib", "MachineLearning"]}