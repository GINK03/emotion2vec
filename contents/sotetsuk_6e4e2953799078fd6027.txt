{"context": "\n\n\u76ee\u7684\nSpark\u306e\u3088\u304f\u4f7f\u3046API\u3092\uff08\u4e3b\u306b\u81ea\u5206\u7528\u306b\uff09\u30e1\u30e2\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u3001\u4e45\u3057\u3076\u308a\u306b\u958b\u767a\u3059\u308b\u3068\u304d\u3067\u3082\u30b5\u30af\u30b5\u30af\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u3002\u3068\u308a\u3042\u3048\u305aPython\u7248\u3092\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\uff08Scala\u7248\u3082\u6642\u9593\u304c\u3042\u308c\u3070\u52a0\u7b46\u3059\u308b\u304b\u3082\uff09\n\u3053\u306e\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8\u306f\u3042\u304f\u307e\u3067\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8\u306a\u306e\u3067\uff08\u5f15\u6570\u304c\u7701\u7565\u3057\u3066\u3042\u3063\u305f\u308a\u3057\u307e\u3059\uff09\u3001\u6642\u9593\u304c\u3042\u308b\u65b9\u306f\u304d\u3061\u3093\u3068\u516c\u5f0fAPI\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(Spark Python API Docs)\u3092\u898b\u3066\u4e0b\u3055\u3044\u3002\n\nSpark API \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8\uff08Python\uff09\n\u4ee5\u4e0b\u3067\u306f\u6b21\u3092\u524d\u63d0\u3068\u3059\u308b\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\n\nsc = SparkContext()\nsqlContext = SQLContext(sc)\n\n\nRDD\n\nRDD\u3092\u4f5c\u308b\uff08\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff09\n\nparallelize\nsc.parallelize(collection) \u30ea\u30b9\u30c8\u3084\u30bf\u30d7\u30eb\u304b\u3089RDD\u3092\u4f5c\u308b\n>>> a = [1, 2, 3, 4, 5]\n>>> rdd = sc.parallelize(a)\n\n\ntextFile\nsc.textFile(file) \u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u3002\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3084\u6b63\u898f\u8868\u73fe\u3082\u4f7f\u3048\u308b\u3002\n>>> rdd = sc.textFile(\"./words.txt\")\n\n\nwholeTextFiles\nsc.wholeTextFiles(dierctory) \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u66f8\u304f\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u5168\u4f53\u3092\u305d\u308c\u305e\u308cRDD\u306e\u4e00\u3064\u306e\u8981\u7d20\u306b\u5165\u529b\u3059\u308b\n# $ ls\n# a.json b.json c.json\n>>> rdd = sc.textWholeFiles(\"./\")\n\n\nAction\nAction\u304c\u5b9f\u884c\u3055\u308c\u308b\u3068\u521d\u3081\u3066Transformation\u304c\u9806\u306b\u5b9f\u884c\u3055\u308c\u308b\uff08\u9045\u5ef6\u5b9f\u884c\uff09\n\n\u8981\u7d20\u3092\u8fd4\u3059\u3082\u306e\n\ncollect\ncollect() \u5168\u3066\u306e\u8981\u7d20\u3092\u8fd4\u3059\n>>> print(rdd.collect())\n[1, 2, 3, 4, 5]\n\n\ntake\ntake(n) \u6700\u521dn\u500b\u306e\u8981\u7d20\u3092\u8fd4\u3059\n>>> print(rdd.take(3)\n[1, 2, 3]\n\n\nfirst\nfirst() \u4e00\u756a\u6700\u521d\u306e\u8981\u7d20\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.first()\n1\n\n\ntop\ntop(n) \u5927\u304d\u3044\u3082\u306e\u304b\u3089n\u500b\u8981\u7d20\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.first()\n[3, 2]\n\n\n(\u7d71\u8a08)\u91cf\u3092\u8fd4\u3059\u3082\u306e\n\ncount\ncount() \u8981\u7d20\u6570\u3092\u6570\u3048\u3066\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.count()\n3\n\n\nmean\nmean() \u5e73\u5747\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.mean()\n3.0\n\n\nsum\nsum() \u5408\u8a08\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.sum()\n6\n\n\nvariance\nvariance() \u5206\u6563\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.variance()\n0.6666666666666666\n\n\nstdev\nstdev() \u6a19\u6e96\u504f\u5dee\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.stdev()\n0.816496580927726\n\n\n\u4fdd\u5b58\u3059\u308b\u3082\u306e\n\nsaveAsTextFile\nsaveAsTextFile(file) \u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3059\u308b\n>>> rdd.saveAsTextFile(\"./a.txt\")\n\n\nTransformation\nTransformation\u306fimmutable\u306a\u65b0\u3057\u3044RDD\u3092\u8fd4\u3059\n\nfilter/map/reduce\n\nfilter\nfilter(f) f\u304c\u771f\u3068\u306a\u308b\u8981\u7d20\u3060\u3051\u3092\u542b\u3080RDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.filter(lambda x: x % 2 == 0).collect()\n[2]\n\n\nmap\nmap(f) \u5168\u3066\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u305fRDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.map(lambda x: x * 2).collect()\n[2, 4, 6]\n\n\nflatMap\nflatMap(f) \u5168\u3066\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u305f\u3042\u3068\u3001\u8981\u7d20\u5185\u306e\u30ea\u30b9\u30c8\u3092\u5c55\u958b\u3057\u305fRDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([\"This is a pen\", \"This is an apple\"])\n>>> rdd.flatMap(lambda x: x.split()).collect()\n['This', 'is', 'a', 'pen', 'This', 'is', 'an', 'apple']\n\n\nReduce\nreduce(f) \u4e8c\u3064\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u7d9a\u3051\u3066\u4e00\u3064\u306e\u8fd4\u308a\u5024\u3092\u5f97\u308b\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.reduce(lambda x, y: x + y)\n6\n\n\n\u30da\u30a2RDD\u306b\u5bfe\u3059\u308b\u64cd\u4f5c\n\n\u30da\u30a2RDD\u3092\u3064\u304f\u308b\n\u30da\u30a2RDD\u306fPython\u3067\u3044\u3046Tuple\u3092\u8981\u7d20\u306b\u3082\u3064RDD\u3002key\u3068value\u3092\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\u4f5c\u308a\u65b9\u306fkeyBy\u3092\u4f7f\u3046\u304bmap\u3067\u8981\u7d20\u65702\u306e\u30bf\u30d7\u30eb\u3092\u8981\u7d20\u306b\u8fd4\u3059\u3002\n\nkeyBy(PairRDD)\nkeyBy(f) \u666e\u901a\u306eRDD\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u3001\u305d\u306e\u8fd4\u308a\u5024\u3092key\u306b\u3001\u5143\u306e\u8981\u7d20\u3092\u305d\u306e\u307e\u307evalue\u306b\u3057\u305fRDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([\"Ken 27 180 83\", \"Bob 32 170 65\", \"Meg 29 165 45\"])\n>>> rdd.keyBy(lambda x: x.split()[0]).collect()\n[('Ken', 'Ken 27 180 83'), ('Bob', 'Bob 32 170 65'), ('Meg', 'Meg 29 165 45')]\n\n\nkeys\nkeys \u30da\u30a2RDD\u306ekey\u3060\u3051\u304b\u3089\u306a\u308bRDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.keys().collect()\n['Ken', 'Bob', 'Taka', 'Ken', 'Bob']\n\n\nvalues\nvalues \u30da\u30a2RDD\u306evlaue\u3060\u3051\u304b\u3089\u306a\u308bRDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.values().collect()\n[2, 3, 1, 3, 2]\n\n\nflatMapValues\nflatMapValues(f) PairRDD\u306evalue\u306bflatMap\u3092\u4f5c\u7528\u3055\u305b\u3066key\u3092\u8907\u88fd\u3057\u3066\u6240\u8b02\u7e26\u6301\u3061\u306b\u3059\u308b\n>>> rdd = sc.parallelize([(\"Ken\", \"Yumi,Yukiko\"), (\"Bob\", \"Meg, Tomomi, Akira\"), (\"Taka\", \"Yuki\")])\n>>> rdd.flatMapValues(lambda x: x.split(\",\"))\n[('Ken', 'Yumi'),\n ('Ken', 'Yukiko'),\n ('Bob', 'Meg'),\n ('Bob', ' Tomomi'),\n ('Bob', ' Akira'),\n ('Taka', 'Yuki')]\n\n\nreduceByKey\nreduceByKey(f) \u540c\u3058key\u306e\u8981\u7d20\u3067\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3066value\u306breduce\u3092\u4f5c\u7528\u3055\u305b\u308b\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.reduceByKey(lambda x, y: x + y).collect()\n\n\ncountByKey\ncountByKey() \u540c\u3058key\u306e\u5024\u304c\u3044\u304f\u3064\u3042\u308b\u304b\u6570\u3048\u3066dict\u3067\u8fd4\u3059\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.countByKey()\ndefaultdict(<type 'int'>, {'Ken': 2, 'Bob': 2, 'Taka': 1})\n\n\nsortByKey\nsortByKey \u30da\u30a2RDD\u3092key\u3067\u30bd\u30fc\u30c8\u3057\u307e\u3059\n>>> rdd = sc.parallelize([(\"cba\", 2), (\"abc\", 3), (\"bac\", 1), (\"bbb\", \n>>> rdd.sortByKey().collect()\n[('aaa', 2), ('abc', 3), ('bac', 1), ('bbb', 3), ('cba', 2)]\n\n\nJoin\u64cd\u4f5c\n\nleftOuterJoin\n\u4e8c\u3064\u306eRDD\u3092left outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.leftOuterJoin(rdd2).collect()\n[('Bob', (2, None)), ('Meg', (3, None)), ('Ken', (1, 1))]\n\n\nrightOuterJoin\n\u4e8c\u3064\u306eRDD\u3092right outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.rightOuterJoin(rdd2).collect()\n[('Ken', (1, 1)), ('Kaz', (3, None))]\n\n\nfullOuterJoin\n\u4e8c\u3064\u306eRDD\u3092full outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.fullOuterJoin(rdd2).collect()\n[('Bob', (2, None)), ('Meg', (3, None)), ('Ken', (1, 1)), ('Kaz', (None, 3))]\n\n\n\u30bd\u30fc\u30c8\u64cd\u4f5c\n\nsortBy\nsortBy(f) f\u306e\u8fd4\u3059\u5024\u306b\u3088\u3063\u3066\u30bd\u30fc\u30c8\u3059\u308b\n>>> rdd = sc.parallelize([(\"cba\", 2), (\"abc\", 3), (\"bac\", 1), (\"bbb\", \n>>> rdd.sortBy(lambda (x, y): x).collect() # sortByKey\u3068\u540c\u3058\n\n\n\u96c6\u5408\u64cd\u4f5c\u306a\u3069\n\nintersection\nintersection(rdd) \u4e8c\u3064\u306eRDD\u306eintersection\u3092\u8fd4\u3059\n\nunion\nunion(rdd) \u4e8c\u3064\u306eRDD\u306eunion\u3092\u8fd4\u3059\n\nzip\nzip(rdd) \u5f15\u6570\u306erdd\u306e\u5404\u8981\u7d20\u3092vlaue\u306b\u3057\u305f\u30da\u30a2RDD\u3092\u8fd4\u3059\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.keys().zip(rdd.values())\n[('Ken', 2), ('Bob', 3), ('Taka', 1), ('Ken', 3), ('Bob', 2)]\n\n\ndistinct\n\u540c\u3058\u8981\u7d20\u3092\u542b\u307e\u306a\u3044RDD\u3092\u8fd4\u3057\u307e\u3059\n\n\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u64cd\u4f5c\n\nsample\nsample(bool, frac) \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305fRDD\u3092\u8fd4\u3059\u3002\u7b2c\u4e00\u5f15\u6570\u3067\u540c\u3058\u8981\u7d20\u306e\u91cd\u8907\u3092\u8a31\u3059\u304b\u6c7a\u3081\u308b\u3002\n>>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n>>> rdd.sample(True, 0.5).collect()\n[1, 5, 5]\n>>> rdd.sample(False, 0.5).collect()\n[1, 3, 5]\n\n\ntakeSample\ntakeSmaple(bool, size) \u56fa\u5b9a\u3055\u308c\u305f\u30b5\u30a4\u30ba\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u30ea\u30b9\u30c8\u3067\u8fd4\u3059\u3002\u7b2c\u4e00\u5f15\u6570\u3067\u540c\u3058\u8981\u7d20\u306e\u91cd\u8907\u3092\u8a31\u3059\u304b\u6c7a\u3081\u308b\u3002\n>>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n>>> rdd.takeSample(True, 2)\n[5, 5]\n>>> rdd.takeSample(False, 2)\n[3, 5]\n\n\n\u30c7\u30d0\u30c3\u30b0\n\ntoDebugString\ntoDebugString() \u5b9f\u884c\u8a08\u753b\u3092\u8fd4\u3059\nprint(rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).toDebugString())\n(1) PythonRDD[190] at RDD at PythonRDD.scala:43 []\n |  MapPartitionsRDD[189] at mapPartitions at PythonRDD.scala:374 []\n |  ShuffledRDD[188] at partitionBy at null:-1 []\n +-(1) PairwiseRDD[187] at reduceByKey at <ipython-input-114-71f5cb742e13>:1 []\n    |  PythonRDD[186] at reduceByKey at <ipython-input-114-71f5cb742e13>:1 []\n    |  ParallelCollectionRDD[141] at parallelize at PythonRDD.scala:423 []\n\n\n\u6c38\u7d9a\u5316\n\npersist\npersist() RDD\u3092\u305d\u306e\u307e\u307e\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30e1\u30e2\u30ea\u306b\uff09\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u3002\u30e1\u30e2\u30ea\u3060\u3051\u3001\u30e1\u30e2\u30ea\u304c\u7121\u7406\u306a\u3089\u30c7\u30a3\u30b9\u30af\u3001\u30c7\u30a3\u30b9\u30af\u3060\u3051\u3001\u306a\u3069\u306e\u8a2d\u5b9a\u304c\u51fa\u6765\u308b\uff08StorageLevel\u3067\u6307\u5b9a\uff09\n>>> rdd.persist()\n\n\nunpersist\nunpersist() RDD\u306e\u6c38\u7d9a\u5316\u3092\u89e3\u304f\u3002\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u3092\u5909\u3048\u308b\u6642\u306a\u3069\u306b\u4f7f\u3046\u3002\n>>> from pyspark import StorageLevel\n>>> rdd.persist()\n>>> rdd.unpersist()\n>>> rdd.persist(StorageLevel.DISK_ONLY)\n\n\n\u3088\u304f\u3042\u308b\u4f8b\n\u968f\u6642\u8ffd\u52a0\u3057\u3066\u3044\u304f\u4e88\u5b9a\n\nword count\n>>> rdd.flatMap(lambda x: x.split())\\\n       .map(lambda x: (x, 1))\\\n       .reduceByKey(lambda x, y: x + y)\\\n       .take(5)\n\n\nDataFrame\n\u7279\u306b\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u3092\u6271\u3046\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u304c\u4fbf\u5229\u3002\n\nDataFrame\u3092\u3064\u304f\u308b\uff08\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff09\n\nread.json\nread.json(file) json\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\n# $ cat a.json\n# {\"name\":\"Ken\", \"age\":35}\n# {\"name\":\"Bob\", \"age\":30, \"weight\":80}\n# {\"name\":\"Meg\", \"age\":29, \"weight\":45}\ndf = sqlContext.read.json(\"a.json\")\n\n\nDataFrame\u3092\u8868\u793a\u3059\u308b\nRDD\u3068\u540c\u3058collect, take\u306e\u4ed6\u306bshow \u304c\u3042\u308b\n\nshow\nshow(n) n\u884c\u8868\u793a\u3059\u308b\uff08n\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306720\uff09\n>>> df.show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n| 29| Meg|    45|\n+---+----+------+\n\n\nDataFrame\u306e\u64cd\u4f5c\n\nselect\nselect(column) string\u304bColumn\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3057\u3066select\u3057\u305fDataFrame\u3092\u8fd4\u3059\u3002\u30ab\u30e9\u30e0\u3092\u5217\u6319\u3057\u3066\u8907\u6570\u5217\u53d6\u5f97\u3057\u305f\u308a\u3001\u6f14\u7b97\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002\n>>> df.select(\"age\").show()\n+---+\n|age|\n+---+\n| 35|\n| 30|\n| 29|\n+---+\n\n# \u6b21\u3082\u540c\u3058\n>>> df.select(df.age).show() # Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3059\n>>> df.select(df[\"age\"]).show() # Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3059\n\n>>> df.select(df.name, df.age).show()\n+----+---+\n|name|age|\n+----+---+\n| Ken| 35|\n| Bob| 30|\n| Meg| 29|\n+----+---+\n\n\nDataframe\u306eColumn\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\nselect\u3067\u6e21\u3059Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30a2\u30af\u30bb\u30a8\u30b9\u306e\u4ed5\u65b9\u3068\u3057\u3066\u3001Python\u3067\u306f\u6b21\u306e2\u30d1\u30bf\u30fc\u30f3\u304c\u5bb9\u6613\u3055\u308c\u3066\u3044\u308b: \n>>> df.age\nColumn<age>\n>>> df[\"age\"]\nColumn<age>\n\n\nwhere/filter\nfilter(condition) string\u306e\u6761\u4ef6\u306b\u5408\u3046\u884c\u3060\u3051\u304b\u3089\u306a\u308bDataFrame\u3092\u8fd4\u3059\u3002 where\u306ffilter\u306ealias\u3067\u3042\u308b\u3002\n>>> df.where(df.age >=30).show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n+---+----+------+\n\n\nsort\nsort(column) \u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0\u3067\u30bd\u30fc\u30c8\u3055\u308c\u305fDataFrame\u3092\u8fd4\u3059\n>>> df.sort(df.age)\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 29| Meg|    45|\n| 30| Bob|    80|\n| 35| Ken|  null|\n+---+----+------+\n\n\nlimit\nlimit(n) \u5148\u982dn\u884c\u3060\u3051\u306b\u5236\u9650\u3057\u305fDataFrame\u3092\u8fd4\u3059\n>>> df.limit(1).show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n+---+----+------+\n\n\ndistinct\ndistinct() distinct\u3057\u305f\u7d50\u679c\u306e\u884c\u3060\u3051\u304b\u3089\u306a\u308bDataFrame\u3092\u8fd4\u3059\n>>> df.distinct().count()\n3\n\n\njoin\njoin(dataframe, on, how) how\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306finnner\n\non: \u30ab\u30e9\u30e0\u3001\u3082\u3057\u304f\u306f\u30ab\u30e9\u30e0\u306e\u30ea\u30b9\u30c8\nhow: \"inner\", \"outer\", \"left_outer\", \"right_outer\", \"leftsemi\" \u306e\u3044\u305a\u308c\u304b\n\n\nDataframe\u304b\u3089RDD\u3078\u5909\u63db\nDataFrame\u306fRDD\u4e0a\u306b\u69cb\u7bc9\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5143\u3068\u306a\u308bRDD\u3092\u53d6\u308a\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u308b\n>>> print(df.rdd.collect())\n[Row(age=35, name=u'Ken', weight=None),\n Row(age=30, name=u'Bob', weight=80),\n Row(age=29, name=u'Meg', weight=45)]\n\n\u7279\u5b9a\u306e\u5217\u3060\u3051\u53d6\u308a\u51fa\u3059\u306b\u306fRow\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5bfe\u5fdc\u3059\u308b\u5c5e\u6027\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\ndf.rdd.map(lambda row: (row.age, row.weight)).collect()\n[(35, None), (30, 80), (29, 45)]\n\n\nDataframe\u3092\u4fdd\u5b58\u3059\u308b\n\ntoJson\ntoJson() json\u306e\u5f62\u3067RDD\u306b\u5909\u63db\u3059\u308b\u3002\u3053\u306e\u3042\u3068saveAsTextFile\u3092\u547c\u3079\u3070json\u5f62\u5f0f\u3067\u4fdd\u5b58\u3067\u304d\u308b\u3002\n>>> df.toJSON().saveAsTextFile(\"b.json\")\n>>> df2 = sqlContext.read.json(\"/b.json\")\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n| 29| Meg|    45|\n+---+----+------+\n\n\n\u4eca\u5f8c\nSpark Streaming\u3084Mllib\u95a2\u9023\u3082\u3053\u3061\u3089\u306b\u8ffd\u8a18\u3057\u3066\u3044\u304f\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n# \u76ee\u7684\nSpark\u306e\u3088\u304f\u4f7f\u3046API\u3092\uff08\u4e3b\u306b\u81ea\u5206\u7528\u306b\uff09\u30e1\u30e2\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u3001\u4e45\u3057\u3076\u308a\u306b\u958b\u767a\u3059\u308b\u3068\u304d\u3067\u3082\u30b5\u30af\u30b5\u30af\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u3002\u3068\u308a\u3042\u3048\u305aPython\u7248\u3092\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\uff08Scala\u7248\u3082\u6642\u9593\u304c\u3042\u308c\u3070\u52a0\u7b46\u3059\u308b\u304b\u3082\uff09\n\n**\u3053\u306e\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8\u306f\u3042\u304f\u307e\u3067\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8**\u306a\u306e\u3067\uff08\u5f15\u6570\u304c\u7701\u7565\u3057\u3066\u3042\u3063\u305f\u308a\u3057\u307e\u3059\uff09\u3001\u6642\u9593\u304c\u3042\u308b\u65b9\u306f\u304d\u3061\u3093\u3068[\u516c\u5f0fAPI\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8(Spark Python API Docs)](http://spark.apache.org/docs/latest/api/python/index.html)\u3092\u898b\u3066\u4e0b\u3055\u3044\u3002\n\n# Spark API \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8\uff08Python\uff09\n\n\u4ee5\u4e0b\u3067\u306f\u6b21\u3092\u524d\u63d0\u3068\u3059\u308b\n\n```py\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\n\nsc = SparkContext()\nsqlContext = SQLContext(sc)\n```\n\n## RDD\n\n### RDD\u3092\u4f5c\u308b\uff08\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff09\n\n##### parallelize\n\n```sc.parallelize(collection)``` \u30ea\u30b9\u30c8\u3084\u30bf\u30d7\u30eb\u304b\u3089RDD\u3092\u4f5c\u308b\n\n```py\n>>> a = [1, 2, 3, 4, 5]\n>>> rdd = sc.parallelize(a)\n```\n\n##### textFile\n\n```sc.textFile(file)``` \u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u3002\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3084\u6b63\u898f\u8868\u73fe\u3082\u4f7f\u3048\u308b\u3002\n\n```py\n>>> rdd = sc.textFile(\"./words.txt\")\n```\n\n##### wholeTextFiles\n\n```sc.wholeTextFiles(dierctory)``` \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u66f8\u304f\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u5168\u4f53\u3092\u305d\u308c\u305e\u308cRDD\u306e\u4e00\u3064\u306e\u8981\u7d20\u306b\u5165\u529b\u3059\u308b\n\n```py\n# $ ls\n# a.json b.json c.json\n>>> rdd = sc.textWholeFiles(\"./\")\n```\n\n### Action\n\nAction\u304c\u5b9f\u884c\u3055\u308c\u308b\u3068\u521d\u3081\u3066Transformation\u304c\u9806\u306b\u5b9f\u884c\u3055\u308c\u308b\uff08\u9045\u5ef6\u5b9f\u884c\uff09\n\n#### \u8981\u7d20\u3092\u8fd4\u3059\u3082\u306e\n\n##### collect\n\n```collect()``` \u5168\u3066\u306e\u8981\u7d20\u3092\u8fd4\u3059\n\n```py\n>>> print(rdd.collect())\n[1, 2, 3, 4, 5]\n```\n\n##### take\n\n```take(n)``` \u6700\u521dn\u500b\u306e\u8981\u7d20\u3092\u8fd4\u3059\n\n```py\n>>> print(rdd.take(3)\n[1, 2, 3]\n```\n\n##### first\n\n```first()``` \u4e00\u756a\u6700\u521d\u306e\u8981\u7d20\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.first()\n1\n```\n\n##### top\n\n```top(n)``` \u5927\u304d\u3044\u3082\u306e\u304b\u3089n\u500b\u8981\u7d20\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.first()\n[3, 2]\n```\n\n####(\u7d71\u8a08)\u91cf\u3092\u8fd4\u3059\u3082\u306e\n\n##### count\n\n```count()``` \u8981\u7d20\u6570\u3092\u6570\u3048\u3066\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.count()\n3\n```\n\n##### mean\n\n```mean()``` \u5e73\u5747\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.mean()\n3.0\n```\n\n##### sum\n\n```sum()``` \u5408\u8a08\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.sum()\n6\n```\n\n##### variance\n\n```variance()``` \u5206\u6563\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.variance()\n0.6666666666666666\n```\n\n##### stdev\n\n```stdev()``` \u6a19\u6e96\u504f\u5dee\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.stdev()\n0.816496580927726\n```\n\n#### \u4fdd\u5b58\u3059\u308b\u3082\u306e\n\n##### saveAsTextFile\n\n```saveAsTextFile(file)``` \u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3059\u308b\n\n```py\n>>> rdd.saveAsTextFile(\"./a.txt\")\n```\n\n### Transformation\n\nTransformation\u306fimmutable\u306a\u65b0\u3057\u3044RDD\u3092\u8fd4\u3059\n\n#### filter/map/reduce\n\n##### filter\n\n```filter(f)``` f\u304c\u771f\u3068\u306a\u308b\u8981\u7d20\u3060\u3051\u3092\u542b\u3080RDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.filter(lambda x: x % 2 == 0).collect()\n[2]\n```\n\n##### map\n\n```map(f)``` \u5168\u3066\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u305fRDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.map(lambda x: x * 2).collect()\n[2, 4, 6]\n```\n\n##### flatMap\n\n```flatMap(f)``` \u5168\u3066\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u305f\u3042\u3068\u3001\u8981\u7d20\u5185\u306e\u30ea\u30b9\u30c8\u3092\u5c55\u958b\u3057\u305fRDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([\"This is a pen\", \"This is an apple\"])\n>>> rdd.flatMap(lambda x: x.split()).collect()\n['This', 'is', 'a', 'pen', 'This', 'is', 'an', 'apple']\n```\n\n##### Reduce\n\n```reduce(f)``` \u4e8c\u3064\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u7d9a\u3051\u3066\u4e00\u3064\u306e\u8fd4\u308a\u5024\u3092\u5f97\u308b\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3])\n>>> rdd.reduce(lambda x, y: x + y)\n6\n```\n\n#### \u30da\u30a2RDD\u306b\u5bfe\u3059\u308b\u64cd\u4f5c\n\n##### \u30da\u30a2RDD\u3092\u3064\u304f\u308b\n\n\u30da\u30a2RDD\u306fPython\u3067\u3044\u3046Tuple\u3092\u8981\u7d20\u306b\u3082\u3064RDD\u3002key\u3068value\u3092\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\u4f5c\u308a\u65b9\u306f```keyBy```\u3092\u4f7f\u3046\u304b```map```\u3067\u8981\u7d20\u65702\u306e\u30bf\u30d7\u30eb\u3092\u8981\u7d20\u306b\u8fd4\u3059\u3002\n\n##### keyBy(PairRDD)\n\n```keyBy(f)``` \u666e\u901a\u306eRDD\u306e\u8981\u7d20\u306bf\u3092\u4f5c\u7528\u3055\u305b\u3001\u305d\u306e\u8fd4\u308a\u5024\u3092key\u306b\u3001\u5143\u306e\u8981\u7d20\u3092\u305d\u306e\u307e\u307evalue\u306b\u3057\u305fRDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([\"Ken 27 180 83\", \"Bob 32 170 65\", \"Meg 29 165 45\"])\n>>> rdd.keyBy(lambda x: x.split()[0]).collect()\n[('Ken', 'Ken 27 180 83'), ('Bob', 'Bob 32 170 65'), ('Meg', 'Meg 29 165 45')]\n```\n\n##### keys\n\n```keys``` \u30da\u30a2RDD\u306ekey\u3060\u3051\u304b\u3089\u306a\u308bRDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.keys().collect()\n['Ken', 'Bob', 'Taka', 'Ken', 'Bob']\n```\n\n##### values\n\n```values``` \u30da\u30a2RDD\u306evlaue\u3060\u3051\u304b\u3089\u306a\u308bRDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.values().collect()\n[2, 3, 1, 3, 2]\n```\n\n##### flatMapValues\n\n```flatMapValues(f)``` PairRDD\u306evalue\u306bflatMap\u3092\u4f5c\u7528\u3055\u305b\u3066key\u3092\u8907\u88fd\u3057\u3066\u6240\u8b02\u7e26\u6301\u3061\u306b\u3059\u308b\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", \"Yumi,Yukiko\"), (\"Bob\", \"Meg, Tomomi, Akira\"), (\"Taka\", \"Yuki\")])\n>>> rdd.flatMapValues(lambda x: x.split(\",\"))\n[('Ken', 'Yumi'),\n ('Ken', 'Yukiko'),\n ('Bob', 'Meg'),\n ('Bob', ' Tomomi'),\n ('Bob', ' Akira'),\n ('Taka', 'Yuki')]\n```\n\n##### reduceByKey\n\n```reduceByKey(f)``` \u540c\u3058key\u306e\u8981\u7d20\u3067\u30b0\u30eb\u30fc\u30d7\u5316\u3057\u3066value\u306breduce\u3092\u4f5c\u7528\u3055\u305b\u308b\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.reduceByKey(lambda x, y: x + y).collect()\n```\n\n##### countByKey\n\n```countByKey()``` \u540c\u3058key\u306e\u5024\u304c\u3044\u304f\u3064\u3042\u308b\u304b\u6570\u3048\u3066dict\u3067\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.countByKey()\ndefaultdict(<type 'int'>, {'Ken': 2, 'Bob': 2, 'Taka': 1})\n```\n\n##### sortByKey\n\n```sortByKey``` \u30da\u30a2RDD\u3092key\u3067\u30bd\u30fc\u30c8\u3057\u307e\u3059\n\n```py\n>>> rdd = sc.parallelize([(\"cba\", 2), (\"abc\", 3), (\"bac\", 1), (\"bbb\", \n>>> rdd.sortByKey().collect()\n[('aaa', 2), ('abc', 3), ('bac', 1), ('bbb', 3), ('cba', 2)]\n```\n\n#### Join\u64cd\u4f5c\n\n##### leftOuterJoin\n\u4e8c\u3064\u306eRDD\u3092left outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.leftOuterJoin(rdd2).collect()\n[('Bob', (2, None)), ('Meg', (3, None)), ('Ken', (1, 1))]\n```\n\n##### rightOuterJoin\n\u4e8c\u3064\u306eRDD\u3092right outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.rightOuterJoin(rdd2).collect()\n[('Ken', (1, 1)), ('Kaz', (3, None))]\n```\n\n##### fullOuterJoin\n\u4e8c\u3064\u306eRDD\u3092full outer join\u3057\u3066\u3001value\u306b\u4e8c\u3064\u306e\u8981\u7d20\u306e\u30bf\u30d7\u30eb\u3092\u3082\u3064\u30da\u30a2RDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd1 = sc.parallelize([(\"Ken\", 1), (\"Bob\", 2), (\"Meg\", 3)])\n>>> rdd2 = sc.parallelize([(\"Ken\", 1), (\"Kaz\", 3)])\n>>> rdd1.fullOuterJoin(rdd2).collect()\n[('Bob', (2, None)), ('Meg', (3, None)), ('Ken', (1, 1)), ('Kaz', (None, 3))]\n```\n\n#### \u30bd\u30fc\u30c8\u64cd\u4f5c\n\n##### sortBy\n\n```sortBy(f)``` f\u306e\u8fd4\u3059\u5024\u306b\u3088\u3063\u3066\u30bd\u30fc\u30c8\u3059\u308b\n\n```py\n>>> rdd = sc.parallelize([(\"cba\", 2), (\"abc\", 3), (\"bac\", 1), (\"bbb\", \n>>> rdd.sortBy(lambda (x, y): x).collect() # sortByKey\u3068\u540c\u3058\n```\n\n#### \u96c6\u5408\u64cd\u4f5c\u306a\u3069\n\n##### intersection\n\n```intersection(rdd)``` \u4e8c\u3064\u306eRDD\u306eintersection\u3092\u8fd4\u3059\n\n##### union\n\n```union(rdd)``` \u4e8c\u3064\u306eRDD\u306eunion\u3092\u8fd4\u3059\n\n##### zip\n```zip(rdd)``` \u5f15\u6570\u306erdd\u306e\u5404\u8981\u7d20\u3092vlaue\u306b\u3057\u305f\u30da\u30a2RDD\u3092\u8fd4\u3059\n\n```py\n>>> rdd = sc.parallelize([(\"Ken\", 2), (\"Bob\", 3), (\"Taka\", 1), (\"Ken\", 3), (\"Bob\", 2)])\n>>> rdd.keys().zip(rdd.values())\n[('Ken', 2), ('Bob', 3), ('Taka', 1), ('Ken', 3), ('Bob', 2)]\n```\n\n##### distinct\n\u540c\u3058\u8981\u7d20\u3092\u542b\u307e\u306a\u3044RDD\u3092\u8fd4\u3057\u307e\u3059\n\n#### \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u64cd\u4f5c\n\n##### sample\n\n```sample(bool, frac)``` \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305fRDD\u3092\u8fd4\u3059\u3002\u7b2c\u4e00\u5f15\u6570\u3067\u540c\u3058\u8981\u7d20\u306e\u91cd\u8907\u3092\u8a31\u3059\u304b\u6c7a\u3081\u308b\u3002\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n>>> rdd.sample(True, 0.5).collect()\n[1, 5, 5]\n>>> rdd.sample(False, 0.5).collect()\n[1, 3, 5]\n```\n\n##### takeSample\n\n```takeSmaple(bool, size)``` \u56fa\u5b9a\u3055\u308c\u305f\u30b5\u30a4\u30ba\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u30ea\u30b9\u30c8\u3067\u8fd4\u3059\u3002\u7b2c\u4e00\u5f15\u6570\u3067\u540c\u3058\u8981\u7d20\u306e\u91cd\u8907\u3092\u8a31\u3059\u304b\u6c7a\u3081\u308b\u3002\n\n\n```py\n>>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n>>> rdd.takeSample(True, 2)\n[5, 5]\n>>> rdd.takeSample(False, 2)\n[3, 5]\n```\n\n#### \u30c7\u30d0\u30c3\u30b0\n\n##### toDebugString\n\n```toDebugString()``` \u5b9f\u884c\u8a08\u753b\u3092\u8fd4\u3059\n\n```py\nprint(rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).toDebugString())\n(1) PythonRDD[190] at RDD at PythonRDD.scala:43 []\n |  MapPartitionsRDD[189] at mapPartitions at PythonRDD.scala:374 []\n |  ShuffledRDD[188] at partitionBy at null:-1 []\n +-(1) PairwiseRDD[187] at reduceByKey at <ipython-input-114-71f5cb742e13>:1 []\n    |  PythonRDD[186] at reduceByKey at <ipython-input-114-71f5cb742e13>:1 []\n    |  ParallelCollectionRDD[141] at parallelize at PythonRDD.scala:423 []\n```\n\n#### \u6c38\u7d9a\u5316\n\n##### persist\n\n```persist()``` RDD\u3092\u305d\u306e\u307e\u307e\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u30e1\u30e2\u30ea\u306b\uff09\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u3002\u30e1\u30e2\u30ea\u3060\u3051\u3001\u30e1\u30e2\u30ea\u304c\u7121\u7406\u306a\u3089\u30c7\u30a3\u30b9\u30af\u3001\u30c7\u30a3\u30b9\u30af\u3060\u3051\u3001\u306a\u3069\u306e\u8a2d\u5b9a\u304c\u51fa\u6765\u308b\uff08```StorageLevel```\u3067\u6307\u5b9a\uff09\n\n```py\n>>> rdd.persist()\n```\n\n##### unpersist\n\n```unpersist()``` RDD\u306e\u6c38\u7d9a\u5316\u3092\u89e3\u304f\u3002\u6c38\u7d9a\u5316\u30ec\u30d9\u30eb\u3092\u5909\u3048\u308b\u6642\u306a\u3069\u306b\u4f7f\u3046\u3002\n\n```py\n>>> from pyspark import StorageLevel\n>>> rdd.persist()\n>>> rdd.unpersist()\n>>> rdd.persist(StorageLevel.DISK_ONLY)\n```\n\n#### \u3088\u304f\u3042\u308b\u4f8b\n\u968f\u6642\u8ffd\u52a0\u3057\u3066\u3044\u304f\u4e88\u5b9a\n\n##### word count\n\n```py\n>>> rdd.flatMap(lambda x: x.split())\\\n       .map(lambda x: (x, 1))\\\n       .reduceByKey(lambda x, y: x + y)\\\n       .take(5)\n```\n\n## DataFrame\n\u7279\u306b\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u3092\u6271\u3046\u3068\u304d\u306f\u3053\u3061\u3089\u306e\u65b9\u304c\u4fbf\u5229\u3002\n\n### DataFrame\u3092\u3064\u304f\u308b\uff08\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff09\n\n##### read.json\n\n```read.json(file)``` json\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\n\n```py\n# $ cat a.json\n# {\"name\":\"Ken\", \"age\":35}\n# {\"name\":\"Bob\", \"age\":30, \"weight\":80}\n# {\"name\":\"Meg\", \"age\":29, \"weight\":45}\ndf = sqlContext.read.json(\"a.json\")\n```\n\n### DataFrame\u3092\u8868\u793a\u3059\u308b\n\nRDD\u3068\u540c\u3058```collect```, ```take```\u306e\u4ed6\u306b```show``` \u304c\u3042\u308b\n\n##### show\n\n```show(n)``` n\u884c\u8868\u793a\u3059\u308b\uff08n\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306720\uff09\n\n```py\n>>> df.show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n| 29| Meg|    45|\n+---+----+------+\n```\n\n### DataFrame\u306e\u64cd\u4f5c\n\n##### select\n\n```select(column)``` string\u304bColumn\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3057\u3066select\u3057\u305fDataFrame\u3092\u8fd4\u3059\u3002\u30ab\u30e9\u30e0\u3092\u5217\u6319\u3057\u3066\u8907\u6570\u5217\u53d6\u5f97\u3057\u305f\u308a\u3001\u6f14\u7b97\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3002\n\n\n```py\n>>> df.select(\"age\").show()\n+---+\n|age|\n+---+\n| 35|\n| 30|\n| 29|\n+---+\n\n# \u6b21\u3082\u540c\u3058\n>>> df.select(df.age).show() # Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3059\n>>> df.select(df[\"age\"]).show() # Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u3059\n```\n\n```py\n>>> df.select(df.name, df.age).show()\n+----+---+\n|name|age|\n+----+---+\n| Ken| 35|\n| Bob| 30|\n| Meg| 29|\n+----+---+\n```\n\n###### Dataframe\u306eColumn\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\n\nselect\u3067\u6e21\u3059Column\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30a2\u30af\u30bb\u30a8\u30b9\u306e\u4ed5\u65b9\u3068\u3057\u3066\u3001Python\u3067\u306f\u6b21\u306e2\u30d1\u30bf\u30fc\u30f3\u304c\u5bb9\u6613\u3055\u308c\u3066\u3044\u308b: \n\n```py\n>>> df.age\nColumn<age>\n>>> df[\"age\"]\nColumn<age>\n```\n\n##### where/filter\n\n```filter(condition)``` string\u306e\u6761\u4ef6\u306b\u5408\u3046\u884c\u3060\u3051\u304b\u3089\u306a\u308bDataFrame\u3092\u8fd4\u3059\u3002 ```where```\u306f```filter```\u306ealias\u3067\u3042\u308b\u3002\n\n\n```py\n>>> df.where(df.age >=30).show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n+---+----+------+\n```\n\n##### sort\n\n```sort(column)``` \u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0\u3067\u30bd\u30fc\u30c8\u3055\u308c\u305fDataFrame\u3092\u8fd4\u3059\n\n```py\n>>> df.sort(df.age)\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 29| Meg|    45|\n| 30| Bob|    80|\n| 35| Ken|  null|\n+---+----+------+\n```\n\n##### limit\n\n```limit(n)``` \u5148\u982dn\u884c\u3060\u3051\u306b\u5236\u9650\u3057\u305fDataFrame\u3092\u8fd4\u3059\n\n```py\n>>> df.limit(1).show()\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n+---+----+------+\n```\n\n##### distinct\n\n```distinct()``` distinct\u3057\u305f\u7d50\u679c\u306e\u884c\u3060\u3051\u304b\u3089\u306a\u308bDataFrame\u3092\u8fd4\u3059\n\n```py\n>>> df.distinct().count()\n3\n```\n\n##### join\n\n```join(dataframe, on, how)``` how\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306finnner\n\n- on: \u30ab\u30e9\u30e0\u3001\u3082\u3057\u304f\u306f\u30ab\u30e9\u30e0\u306e\u30ea\u30b9\u30c8\n- how: ```\"inner\"```, ```\"outer\"```, ```\"left_outer\"```, ```\"right_outer\"```, ```\"leftsemi\"``` \u306e\u3044\u305a\u308c\u304b\n\n### Dataframe\u304b\u3089RDD\u3078\u5909\u63db\nDataFrame\u306fRDD\u4e0a\u306b\u69cb\u7bc9\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5143\u3068\u306a\u308bRDD\u3092\u53d6\u308a\u51fa\u3059\u3053\u3068\u304c\u3067\u304d\u308b\n\n```py\n>>> print(df.rdd.collect())\n[Row(age=35, name=u'Ken', weight=None),\n Row(age=30, name=u'Bob', weight=80),\n Row(age=29, name=u'Meg', weight=45)]\n```\n\n\u7279\u5b9a\u306e\u5217\u3060\u3051\u53d6\u308a\u51fa\u3059\u306b\u306f```Row```\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u5bfe\u5fdc\u3059\u308b\u5c5e\u6027\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\n\n```py\ndf.rdd.map(lambda row: (row.age, row.weight)).collect()\n[(35, None), (30, 80), (29, 45)]\n```\n\n\n### Dataframe\u3092\u4fdd\u5b58\u3059\u308b\n\n##### toJson\n\n```toJson()``` json\u306e\u5f62\u3067RDD\u306b\u5909\u63db\u3059\u308b\u3002\u3053\u306e\u3042\u3068```saveAsTextFile```\u3092\u547c\u3079\u3070json\u5f62\u5f0f\u3067\u4fdd\u5b58\u3067\u304d\u308b\u3002\n\n```py\n>>> df.toJSON().saveAsTextFile(\"b.json\")\n>>> df2 = sqlContext.read.json(\"/b.json\")\n+---+----+------+\n|age|name|weight|\n+---+----+------+\n| 35| Ken|  null|\n| 30| Bob|    80|\n| 29| Meg|    45|\n+---+----+------+\n```\n\n# \u4eca\u5f8c\nSpark Streaming\u3084Mllib\u95a2\u9023\u3082\u3053\u3061\u3089\u306b\u8ffd\u8a18\u3057\u3066\u3044\u304f\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\n<!-- ## Spark Streaming -->\n<!-- ## Mllib -->\n", "tags": ["Spark", "Python"]}