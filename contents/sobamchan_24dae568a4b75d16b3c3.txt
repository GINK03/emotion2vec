{"context": "\u666e\u901a\u306bnltk.tokenize.word_tokenize(text)\u3068\u3057\u3066\u3082\u826f\u3044\u306e\u3060\u304c\u3053\u308c\u3060\u3068\u3001\n'I'm a student.' -> ['I', \"'m\", 'a', 'student', '.']\n\u3068\u306a\u3063\u3066\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u90fd\u5408\u304c\u60aa\u3044\u3002\n\u305d\u3093\u306a\u6642\u306fnltk.tokenize.RegexpTokenizer\u3092\u4f7f\u3046\u3068\u81ea\u5206\u3067\u5206\u3051\u65b9\u3092\u6b63\u898f\u8868\u73fe\u3092\u7528\u3044\u3066\u6307\u5b9a\u3067\u304d\u308b\u3002\n\u4e0a\u306e\u5206\u3092\u4f8b\u306b\u3057\u3066\u8003\u3048\u308b\u3068\u3001\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[\\w']+\")\ntokenizer.tokenize(\"I'm a student.\")\n\n\u3068\u3059\u308b\u3068\n[\"I'm\", 'a', 'student']\n\u306b\u306a\u308b\u3002\n\u53e5\u8aad\u70b9\u306a\u3069\u3092\u6b8b\u3057\u305f\u3044\u969b\u306b\u306fand\u6f14\u7b97\u5b50\u3067\u6b63\u898f\u8868\u73fe\u3092\u52a0\u3048\u3066\u3044\u304f\u3068\u826f\u3044\u3002\ntokenizer = RegexpTokenizer(\"[\\w']+|[\\.]\")\n\n\n[\u53c2\u8003\u30b5\u30a4\u30c8]\nimpythonist\ntwitter\u59cb\u3081\u307e\u3057\u305f\n\u666e\u901a\u306bnltk.tokenize.word_tokenize(text)\u3068\u3057\u3066\u3082\u826f\u3044\u306e\u3060\u304c\u3053\u308c\u3060\u3068\u3001\n'I'm a student.' -> ['I', \"'m\", 'a', 'student', '.']\n\u3068\u306a\u3063\u3066\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u90fd\u5408\u304c\u60aa\u3044\u3002\n\u305d\u3093\u306a\u6642\u306fnltk.tokenize.RegexpTokenizer\u3092\u4f7f\u3046\u3068\u81ea\u5206\u3067\u5206\u3051\u65b9\u3092\u6b63\u898f\u8868\u73fe\u3092\u7528\u3044\u3066\u6307\u5b9a\u3067\u304d\u308b\u3002\n\u4e0a\u306e\u5206\u3092\u4f8b\u306b\u3057\u3066\u8003\u3048\u308b\u3068\u3001\n\n```py3\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(\"[\\w']+\")\ntokenizer.tokenize(\"I'm a student.\")\n```\n\n\u3068\u3059\u308b\u3068\n[\"I'm\", 'a', 'student']\n\u306b\u306a\u308b\u3002\n\n\u53e5\u8aad\u70b9\u306a\u3069\u3092\u6b8b\u3057\u305f\u3044\u969b\u306b\u306fand\u6f14\u7b97\u5b50\u3067\u6b63\u898f\u8868\u73fe\u3092\u52a0\u3048\u3066\u3044\u304f\u3068\u826f\u3044\u3002\n\n```py3\ntokenizer = RegexpTokenizer(\"[\\w']+|[\\.]\")\n```\n\n----\n\n[\u53c2\u8003\u30b5\u30a4\u30c8]\n[impythonist](https://impythonist.wordpress.com/2014/02/11/language-translation-with-python/)\n\n[twitter\u59cb\u3081\u307e\u3057\u305f](https://twitter.com/sobamchan)\n", "tags": ["Python", "nltk", "NLP", "tokenizer"]}