{"context": " More than 1 year has passed since last update.\n\n\u76ee\u7684\n\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b(Hive 0.14 + Tez 0.5.2 + Hadoop 2.6.0)\u3067Hive on Tez\u304c\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3092\u78ba\u304b\u3081\u308b\u3002\n\n\u524d\u63d0\nHadoop 2.6.0\u3068\u3001Tez 0.5.2\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3002\n\nHive\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nHive\u306e\u6700\u65b0\u30ea\u30ea\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n$ wget http://www.us.apache.org/dist/hive/hive-0.14.0/apache-hive-0.14.0-src.tar.gz\n$ tar xf apache-hive-0.14.0-src.tar.gz \n$ cd apache-hive-0.14.0-src\n\n\n\u30d3\u30eb\u30c9\n\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u3001Hadoop 2.6.0\u3068Tez 0.5.2\u3092\u4f7f\u3046\u3088\u3046\u8a2d\u5b9a\n\npom.xml\ndiff --git a/apache-hive-0.14.0-src.org/pom.xml b/apache-hive-0.14.0-src/pom.xml\nindex 501d547..d9a9fd1 100644\n--- a/apache-hive-0.14.0-src.org/pom.xml\n+++ b/apache-hive-0.14.0-src/pom.xml\n@@ -115,7 +115,7 @@\n     <groovy.version>2.1.6</groovy.version>\n     <hadoop-20.version>0.20.2</hadoop-20.version>\n     <hadoop-20S.version>1.2.1</hadoop-20S.version>\n-    <hadoop-23.version>2.5.0</hadoop-23.version>\n+    <hadoop-23.version>2.6.0</hadoop-23.version>\n     <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>\n     <hbase.hadoop1.version>0.98.3-hadoop1</hbase.hadoop1.version>\n     <hbase.hadoop2.version>0.98.3-hadoop2</hbase.hadoop2.version>\n@@ -152,7 +152,7 @@\n     <stax.version>1.0.1</stax.version>\n     <slf4j.version>1.7.5</slf4j.version>\n     <ST4.version>4.0.4</ST4.version>\n-    <tez.version>0.5.2-SNAPSHOT</tez.version>\n+    <tez.version>0.5.2</tez.version>\n     <super-csv.version>2.2.0</super-csv.version>\n     <tempus-fugit.version>1.1</tempus-fugit.version>\n     <snappy.version>0.2</snappy.version>\n@@ -209,8 +209,19 @@\n          <enabled>false</enabled>\n        </snapshots>\n      </repository>\n+     <repository>\n+       <id>org.apache.hadoop</id>\n+       <url>https://repository.apache.org/content/repositories/orgapachehadoop-1012</url>\n+     </repository>\n   </repositories>\n\n+  <pluginRepositories>\n+    <pluginRepository>\n+       <id>org.apache.hadoop</id>\n+       <url>https://repository.apache.org/content/repositories/orgapachehadoop-1012</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n   <!-- Hadoop dependency management is done at the bottom under profiles -->\n   <dependencyManagement>\n     <dependencies>\n\n\n\nHadoop 2.6.0\u304c\u6b63\u5f0f\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3089\u3001repository, pluginRepository\u306e\u6307\u5b9a\u306f\u4e0d\u8981\n\n\nMaven\u3067\u30d3\u30eb\u30c9\n$ mvn clean install -DskipTests -Phadoop-2,dist\n\n\n\nHive\u306e\u8a2d\u5b9a\n$ cp -r packaging/target/apache-hive-0.14.0-bin/apache-hive-0.14.0-bin /usr/local/\n# export\u6587\u306f\u3001bashrc\u3042\u305f\u308a\u306b\u66f8\u3044\u3066\u304a\u304f\u3053\u3068\u3092\u63a8\u5968\n$ export HIVE_HOME=/usr/local/apache-hive-0.14.0-bin\n$ export PATH=$PATH:$HIVE_HOME/bin\n$ cd $HIVE_HOME/conf\n$ cp hive-default.xml.template hive-default.xml\n\n\nHive\u3092\u4f7f\u3046\u305f\u3081\u306eHDFS\u306e\u521d\u671f\u8a2d\u5b9a\n\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b\n$ hdfs dfs -mkdir /tmp\n$ hdfs dfs -mkdir -p /user/hive/warehouse\n$ hdfs dfs -chmod g+w /tmp\n$ hdfs dfs -chmod g+w /user/hive/warehouse\n\n\nHive\u306e\u5b9f\u884c\nsinchii\u3055\u3093\u306eblog:\u3068\u308a\u3042\u3048\u305aPig on Tez \u3092\u52d5\u304b\u3057\u3066\u307f\u305f\u306e\u30c7\u30fc\u30bf\u3068\u30af\u30a8\u30ea\u3092Hive\u306b\u79fb\u690d\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n\u30c7\u30fc\u30bf\u30ed\u30fc\u30c9\nhive> CREATE TABLE flight (number STRING, dept INT, dest INT, equip STRING)                  \n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nOK\nTime taken: 0.916 seconds\nhive> CREATE TABLE airport (id INT, name STRING)                                            \n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nOK\nTime taken: 0.093 seconds\nhive> LOAD DATA LOCAL INPATH './flight.txt' OVERWRITE INTO TABLE flight;\nLoading data to table default.flight\nTable default.flight stats: [numFiles=1, numRows=0, totalSize=284, rawDataSize=0]\nOK\nTime taken: 0.903 seconds\nhive> LOAD DATA LOCAL INPATH './airport.txt' OVERWRITE INTO TABLE airport;\nLoading data to table default.airport\nTable default.airport stats: [numFiles=1, numRows=0, totalSize=30, rawDataSize=0]\nOK\nTime taken: 0.595 seconds\n\n\nPig\u7248\u3060\u3068\u30ab\u30e9\u30e0\u540d\u304cfrom, to\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u305f\u304c\u3001Hive\u3060\u3068from\u304c\u4e88\u7d04\u8a9e\u306e\u305f\u3081\u305d\u308c\u305e\u308cdept, dest\u306b\u5909\u66f4\u3057\u3066\u3044\u308b\n\n\n\u30af\u30a8\u30ea\u3092\u66f8\u304f\n\u500b\u4eba\u7684\u306a\u3053\u3068\u3060\u3051\u3069\u3001\u5909\u63db\u306b\u82e6\u52b4\u3057\u305f\n\n\u7279\u306b\u3001SUBSTRING(number, 0, 2)\u30922\u56de\u66f8\u304b\u3055\u308c\u3066\u3057\u307e\u3063\u305f\u306e\u304c\u3064\u3089\u3044 (AS\u53e5\u304c\u3046\u307e\u304f\u52d5\u3044\u3066\u307b\u3057\u304b\u3063\u305f)\n\n# tez\u3067\u5b9f\u884c\u3059\u308b\u8a2d\u5b9a\nhive> set hive.execution.engine=tez;\nhive> SELECT name, SUBSTRING(number, 0, 2), count(*) FROM flight       \n    > JOIN airport ON flight.dept = airport.id WHERE equip != '777-300'\n    > GROUP BY SUBSTRING(number, 0, 2), name;                          \nQuery ID = root_20141113002626_5b2ab149-c279-4ba8-8089-f51aedf76e76\nTotal jobs = 1\nLaunching Job 1 out of 1\n\n\nStatus: Running (Executing on YARN cluster with App id application_1415780973920_0008)\n\n--------------------------------------------------------------------------------\n        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n--------------------------------------------------------------------------------\nMap 1 ..........   SUCCEEDED      1          1        0        0       0       0\nMap 3 ..........   SUCCEEDED      1          1        0        0       0       0\nReducer 2 ......   SUCCEEDED      1          1        0        0       0       0\n--------------------------------------------------------------------------------\nVERTICES: 03/03  [==========================>>] 100%  ELAPSED TIME: 19.70 s    \n--------------------------------------------------------------------------------\nOK\nhnd 7G  2\nnrt AP  1\nnrt GK  1\nhnd JL  7\nnrt JL  1\nhnd NH  5\nTime taken: 20.202 seconds, Fetched: 6 row(s)\n\n\u30af\u30a8\u30ea\u5b9f\u884c\u4e2d\u306f\u4ee5\u4e0b\u306e\u753b\u50cf\u306e\u3088\u3046\u306aProgress bar\u304c\u8868\u793a\u3055\u308c\u3066\u3001\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u66f4\u65b0\u3055\u308c\u308b\n\n\n\u3053\u306e\u6a5f\u80fd\u306f\u3001Hive 0.14\u3067\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f! HIVE-8495\n\n\n\nMapReduce\u7248\n# MapReduce\u3067\u5b9f\u884c\u3059\u308b\u8a2d\u5b9a\nhive> set hive.execution.engine=mr;\n# MapReduce on Tez\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5143\u306b\u623b\u3059\nhive> set mapreduce.framework.name=yarn;   \nhive> SELECT name, SUBSTRING(number, 0, 2), count(*) FROM flight       \n    > JOIN airport ON flight.dept = airport.id WHERE equip != '777-300'\n    > GROUP BY SUBSTRING(number, 0, 2), name;                          \nQuery ID = root_20141113004545_9de60583-4809-4d0f-b139-efbec88a007b\nTotal jobs = 1\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/apache-hive-0.14.0-bin/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n14/11/13 00:45:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nExecution log at: /tmp/root/root_20141113004545_9de60583-4809-4d0f-b139-efbec88a007b.log\n2014-11-13 12:45:51 Starting to launch local task to process map join;  maximum memory = 477102080\n2014-11-13 12:45:53 Dump the side-table for tag: 1 with group count: 5 into file: file:/tmp/root/d0dab1cf-a125-4722-b6c2-dd7214e13c9f/hive_2014-11-13_00-45-43_180_2968392168506954651-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile61--.hashtable\n2014-11-13 12:45:53 Uploaded 1 File to: file:/tmp/root/d0dab1cf-a125-4722-b6c2-dd7214e13c9f/hive_2014-11-13_00-45-43_180_2968392168506954651-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile61--.hashtable (375 bytes)\n2014-11-13 12:45:53 End of local task; Time Taken: 2.277 sec.\nExecution completed successfully\nMapredLocal task succeeded\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nStarting Job = job_1415780973920_0010, Tracking URL = http://localhost:8088/proxy/application_1415780973920_0010/\nKill Command = /usr/local/hadoop-2.6.0/bin/hadoop job  -kill job_1415780973920_0010\nHadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n2014-11-13 00:46:07,539 Stage-2 map = 0%,  reduce = 0%\n2014-11-13 00:46:20,779 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.08 sec\n2014-11-13 00:46:34,223 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.86 sec\nMapReduce Total cumulative CPU time: 7 seconds 860 msec\nEnded Job = job_1415780973920_0010\nMapReduce Jobs Launched: \nStage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.86 sec   HDFS Read: 497 HDFS Write: 54 SUCCESS\nTotal MapReduce CPU Time Spent: 7 seconds 860 msec\nOK\nhnd 7G  2\nnrt AP  1\nnrt GK  1\nhnd JL  7\nnrt JL  1\nhnd NH  5\nTime taken: 53.31 seconds, Fetched: 6 row(s)\n\n\n\u6027\u80fd\u6bd4\u8f03\n\u7279\u306b\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306f\u3057\u3066\u306a\u3044\u72b6\u6cc1\u3067\u3059\u304c\u3001\n\nTez: 20\u79d2\nMapReduce: 53\u79d2\n\n\u3068\u5dee\u304c\u3064\u304d\u307e\u3057\u305f\u3002Hive on Tez\u3001\u3044\u3044\u611f\u3058\u3060\u3068\u601d\u3044\u307e\u3059\u3002\u7279\u306bProgress bar\u304c\u3002\n\n## \u76ee\u7684\n\n\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u7d44\u307f\u5408\u308f\u305b(Hive 0.14 + Tez 0.5.2 + Hadoop 2.6.0)\u3067Hive on Tez\u304c\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3092\u78ba\u304b\u3081\u308b\u3002\n\n## \u524d\u63d0\n\nHadoop 2.6.0\u3068\u3001Tez 0.5.2\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3002\n\n## Hive\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n### Hive\u306e\u6700\u65b0\u30ea\u30ea\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n\n```bash:\n$ wget http://www.us.apache.org/dist/hive/hive-0.14.0/apache-hive-0.14.0-src.tar.gz\n$ tar xf apache-hive-0.14.0-src.tar.gz \n$ cd apache-hive-0.14.0-src\n```\n\n### \u30d3\u30eb\u30c9\n\n\u305b\u3063\u304b\u304f\u306a\u306e\u3067\u3001Hadoop 2.6.0\u3068Tez 0.5.2\u3092\u4f7f\u3046\u3088\u3046\u8a2d\u5b9a\n\n```diff:pom.xml\ndiff --git a/apache-hive-0.14.0-src.org/pom.xml b/apache-hive-0.14.0-src/pom.xml\nindex 501d547..d9a9fd1 100644\n--- a/apache-hive-0.14.0-src.org/pom.xml\n+++ b/apache-hive-0.14.0-src/pom.xml\n@@ -115,7 +115,7 @@\n     <groovy.version>2.1.6</groovy.version>\n     <hadoop-20.version>0.20.2</hadoop-20.version>\n     <hadoop-20S.version>1.2.1</hadoop-20S.version>\n-    <hadoop-23.version>2.5.0</hadoop-23.version>\n+    <hadoop-23.version>2.6.0</hadoop-23.version>\n     <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>\n     <hbase.hadoop1.version>0.98.3-hadoop1</hbase.hadoop1.version>\n     <hbase.hadoop2.version>0.98.3-hadoop2</hbase.hadoop2.version>\n@@ -152,7 +152,7 @@\n     <stax.version>1.0.1</stax.version>\n     <slf4j.version>1.7.5</slf4j.version>\n     <ST4.version>4.0.4</ST4.version>\n-    <tez.version>0.5.2-SNAPSHOT</tez.version>\n+    <tez.version>0.5.2</tez.version>\n     <super-csv.version>2.2.0</super-csv.version>\n     <tempus-fugit.version>1.1</tempus-fugit.version>\n     <snappy.version>0.2</snappy.version>\n@@ -209,8 +209,19 @@\n          <enabled>false</enabled>\n        </snapshots>\n      </repository>\n+     <repository>\n+       <id>org.apache.hadoop</id>\n+       <url>https://repository.apache.org/content/repositories/orgapachehadoop-1012</url>\n+     </repository>\n   </repositories>\n \n+  <pluginRepositories>\n+    <pluginRepository>\n+       <id>org.apache.hadoop</id>\n+       <url>https://repository.apache.org/content/repositories/orgapachehadoop-1012</url>\n+    </pluginRepository>\n+  </pluginRepositories>\n+\n   <!-- Hadoop dependency management is done at the bottom under profiles -->\n   <dependencyManagement>\n     <dependencies>\n```\n\n* Hadoop 2.6.0\u304c\u6b63\u5f0f\u30ea\u30ea\u30fc\u30b9\u3055\u308c\u305f\u3089\u3001repository, pluginRepository\u306e\u6307\u5b9a\u306f\u4e0d\u8981\n\n```bash:Maven\u3067\u30d3\u30eb\u30c9\n$ mvn clean install -DskipTests -Phadoop-2,dist\n```\n\n### Hive\u306e\u8a2d\u5b9a\n\n```bash:\n$ cp -r packaging/target/apache-hive-0.14.0-bin/apache-hive-0.14.0-bin /usr/local/\n# export\u6587\u306f\u3001bashrc\u3042\u305f\u308a\u306b\u66f8\u3044\u3066\u304a\u304f\u3053\u3068\u3092\u63a8\u5968\n$ export HIVE_HOME=/usr/local/apache-hive-0.14.0-bin\n$ export PATH=$PATH:$HIVE_HOME/bin\n$ cd $HIVE_HOME/conf\n$ cp hive-default.xml.template hive-default.xml\n```\n\n### Hive\u3092\u4f7f\u3046\u305f\u3081\u306eHDFS\u306e\u521d\u671f\u8a2d\u5b9a\n\n\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b\n\n```bash:\n$ hdfs dfs -mkdir /tmp\n$ hdfs dfs -mkdir -p /user/hive/warehouse\n$ hdfs dfs -chmod g+w /tmp\n$ hdfs dfs -chmod g+w /user/hive/warehouse\n```\n\n## Hive\u306e\u5b9f\u884c\n\nsinchii\u3055\u3093\u306eblog:[\u3068\u308a\u3042\u3048\u305aPig on Tez \u3092\u52d5\u304b\u3057\u3066\u307f\u305f](http://sinchii.hatenablog.com/entry/2014/09/13/001818)\u306e\u30c7\u30fc\u30bf\u3068\u30af\u30a8\u30ea\u3092Hive\u306b\u79fb\u690d\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002\n\n### \u30c7\u30fc\u30bf\u30ed\u30fc\u30c9\n\n```xml:\nhive> CREATE TABLE flight (number STRING, dept INT, dest INT, equip STRING)                  \n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nOK\nTime taken: 0.916 seconds\nhive> CREATE TABLE airport (id INT, name STRING)                                            \n    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nOK\nTime taken: 0.093 seconds\nhive> LOAD DATA LOCAL INPATH './flight.txt' OVERWRITE INTO TABLE flight;\nLoading data to table default.flight\nTable default.flight stats: [numFiles=1, numRows=0, totalSize=284, rawDataSize=0]\nOK\nTime taken: 0.903 seconds\nhive> LOAD DATA LOCAL INPATH './airport.txt' OVERWRITE INTO TABLE airport;\nLoading data to table default.airport\nTable default.airport stats: [numFiles=1, numRows=0, totalSize=30, rawDataSize=0]\nOK\nTime taken: 0.595 seconds\n```\n\n* Pig\u7248\u3060\u3068\u30ab\u30e9\u30e0\u540d\u304c`from`, `to`\u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u305f\u304c\u3001Hive\u3060\u3068`from`\u304c\u4e88\u7d04\u8a9e\u306e\u305f\u3081\u305d\u308c\u305e\u308c`dept`, `dest`\u306b\u5909\u66f4\u3057\u3066\u3044\u308b\n\n### \u30af\u30a8\u30ea\u3092\u66f8\u304f\n\n\u500b\u4eba\u7684\u306a\u3053\u3068\u3060\u3051\u3069\u3001\u5909\u63db\u306b\u82e6\u52b4\u3057\u305f\n\n* \u7279\u306b\u3001`SUBSTRING(number, 0, 2)`\u30922\u56de\u66f8\u304b\u3055\u308c\u3066\u3057\u307e\u3063\u305f\u306e\u304c\u3064\u3089\u3044 (AS\u53e5\u304c\u3046\u307e\u304f\u52d5\u3044\u3066\u307b\u3057\u304b\u3063\u305f)\n\n```bash:\n# tez\u3067\u5b9f\u884c\u3059\u308b\u8a2d\u5b9a\nhive> set hive.execution.engine=tez;\nhive> SELECT name, SUBSTRING(number, 0, 2), count(*) FROM flight       \n    > JOIN airport ON flight.dept = airport.id WHERE equip != '777-300'\n    > GROUP BY SUBSTRING(number, 0, 2), name;                          \nQuery ID = root_20141113002626_5b2ab149-c279-4ba8-8089-f51aedf76e76\nTotal jobs = 1\nLaunching Job 1 out of 1\n\n\nStatus: Running (Executing on YARN cluster with App id application_1415780973920_0008)\n\n--------------------------------------------------------------------------------\n        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n--------------------------------------------------------------------------------\nMap 1 ..........   SUCCEEDED      1          1        0        0       0       0\nMap 3 ..........   SUCCEEDED      1          1        0        0       0       0\nReducer 2 ......   SUCCEEDED      1          1        0        0       0       0\n--------------------------------------------------------------------------------\nVERTICES: 03/03  [==========================>>] 100%  ELAPSED TIME: 19.70 s    \n--------------------------------------------------------------------------------\nOK\nhnd\t7G\t2\nnrt\tAP\t1\nnrt\tGK\t1\nhnd\tJL\t7\nnrt\tJL\t1\nhnd\tNH\t5\nTime taken: 20.202 seconds, Fetched: 6 row(s)\n```\n\n\u30af\u30a8\u30ea\u5b9f\u884c\u4e2d\u306f\u4ee5\u4e0b\u306e\u753b\u50cf\u306e\u3088\u3046\u306aProgress bar\u304c\u8868\u793a\u3055\u308c\u3066\u3001\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u66f4\u65b0\u3055\u308c\u308b\n\n![Screen Shot 2014-11-13 at 15.09.46.png](https://qiita-image-store.s3.amazonaws.com/0/32443/743c583e-801d-2626-1e2e-c03751a64989.png \"Screen Shot 2014-11-13 at 15.09.46.png\")\n\n* \u3053\u306e\u6a5f\u80fd\u306f\u3001Hive 0.14\u3067\u8ffd\u52a0\u3055\u308c\u307e\u3057\u305f! [HIVE-8495](https://issues.apache.org/jira/browse/HIVE-8495)\n\n\n### MapReduce\u7248\n\n```bash:\n# MapReduce\u3067\u5b9f\u884c\u3059\u308b\u8a2d\u5b9a\nhive> set hive.execution.engine=mr;\n# MapReduce on Tez\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u5143\u306b\u623b\u3059\nhive> set mapreduce.framework.name=yarn;   \nhive> SELECT name, SUBSTRING(number, 0, 2), count(*) FROM flight       \n    > JOIN airport ON flight.dept = airport.id WHERE equip != '777-300'\n    > GROUP BY SUBSTRING(number, 0, 2), name;                          \nQuery ID = root_20141113004545_9de60583-4809-4d0f-b139-efbec88a007b\nTotal jobs = 1\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/apache-hive-0.14.0-bin/lib/hive-jdbc-0.14.0-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n14/11/13 00:45:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nExecution log at: /tmp/root/root_20141113004545_9de60583-4809-4d0f-b139-efbec88a007b.log\n2014-11-13 12:45:51\tStarting to launch local task to process map join;\tmaximum memory = 477102080\n2014-11-13 12:45:53\tDump the side-table for tag: 1 with group count: 5 into file: file:/tmp/root/d0dab1cf-a125-4722-b6c2-dd7214e13c9f/hive_2014-11-13_00-45-43_180_2968392168506954651-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile61--.hashtable\n2014-11-13 12:45:53\tUploaded 1 File to: file:/tmp/root/d0dab1cf-a125-4722-b6c2-dd7214e13c9f/hive_2014-11-13_00-45-43_180_2968392168506954651-1/-local-10004/HashTable-Stage-2/MapJoin-mapfile61--.hashtable (375 bytes)\n2014-11-13 12:45:53\tEnd of local task; Time Taken: 2.277 sec.\nExecution completed successfully\nMapredLocal task succeeded\nLaunching Job 1 out of 1\nNumber of reduce tasks not specified. Estimated from input data size: 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nStarting Job = job_1415780973920_0010, Tracking URL = http://localhost:8088/proxy/application_1415780973920_0010/\nKill Command = /usr/local/hadoop-2.6.0/bin/hadoop job  -kill job_1415780973920_0010\nHadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n2014-11-13 00:46:07,539 Stage-2 map = 0%,  reduce = 0%\n2014-11-13 00:46:20,779 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.08 sec\n2014-11-13 00:46:34,223 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.86 sec\nMapReduce Total cumulative CPU time: 7 seconds 860 msec\nEnded Job = job_1415780973920_0010\nMapReduce Jobs Launched: \nStage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 7.86 sec   HDFS Read: 497 HDFS Write: 54 SUCCESS\nTotal MapReduce CPU Time Spent: 7 seconds 860 msec\nOK\nhnd\t7G\t2\nnrt\tAP\t1\nnrt\tGK\t1\nhnd\tJL\t7\nnrt\tJL\t1\nhnd\tNH\t5\nTime taken: 53.31 seconds, Fetched: 6 row(s)\n```\n\n### \u6027\u80fd\u6bd4\u8f03\n\n\u7279\u306b\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306f\u3057\u3066\u306a\u3044\u72b6\u6cc1\u3067\u3059\u304c\u3001\n\n* Tez: 20\u79d2\n* MapReduce: 53\u79d2\n\n\u3068\u5dee\u304c\u3064\u304d\u307e\u3057\u305f\u3002Hive on Tez\u3001\u3044\u3044\u611f\u3058\u3060\u3068\u601d\u3044\u307e\u3059\u3002\u7279\u306bProgress bar\u304c\u3002\n", "tags": ["hive", "Tez", "hadoop"]}