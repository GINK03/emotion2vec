{"tags": ["Julia", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "MachineLearning"], "context": "\n\nTL;DR\n\nJulia \u306f\u901f\u3044\u3088\u3002\nJulia \u306f\u884c\u5217\u6f14\u7b97\u7c21\u5358\u3060\u3088\u3002\nJulia \u306f Deep Learning \u5411\u304d\u3060\u3088\u3002\nJulia \u306f\u305d\u3082\u305d\u3082\u66f8\u304d\u3084\u3059\u3044\u3088\u306f\u304b\u3069\u308b\u3088\uff01\nJulia \u826f\u3044\u3088 Julia\uff01\n\n\n\n\u521d\u3081\u306b\n\n\u3053\u306e\u8a18\u4e8b\u306f\u3001Julia Advent Calendar 2016 \u306e3\u65e5\u76ee\u306e\u8a18\u4e8b\u3067\u3059\u3002\n\u3068\u540c\u6642\u306b\u3001\u6a5f\u68b0\u5b66\u7fd2 \u540d\u53e4\u5c4b \u7b2c8\u56de\u52c9\u5f37\u4f1a \u306e\u767a\u8868\u8cc7\u6599\u3067\u3059\u3002\n\u307e\u305f\u3053\u306e\u8a18\u4e8b\u306f\u3001\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning\uff08O'reilly, 2016/09\uff09\u3092\u53c2\u8003\u306b\u69cb\u7bc9\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n\u2191 \u3092Julia\u306b\u79fb\u690d\u4e2d \u2192 DLScratch.jl\n\n\n\n\n\n\n\u81ea\u5df1\u7d39\u4ecb\n\n\n\u81ea\u5df1\u7d39\u4ecb\n\n\u540d\u524d\uff1a\u5f8c\u85e4 \u4fca\u4ecb\n\u6240\u5c5e\uff1a\u6709\u9650\u4f1a\u793e \u6765\u6816\u5ddd\u96fb\u7b97\n\u8a00\u8a9e\uff1aPython, Julia, Ruby, Scala\uff08\u52c9\u5f37\u4e2d\uff09, \u2026\ntwitter: @antimon2\n\nFacebook: antimon2\n\nGitHub: antimon2\n\nQiita: @antimon2\n\n\n\n\nJulia\n\n\nJulia \u3068\u306f\n\nThe Julia Language\n2016/09/20 \u306b v0.5.0 \u304c\u30ea\u30ea\u30fc\u30b9\uff082016/12/03 \u73fe\u5728 v0.6.0 \u3092\u958b\u767a\u4e2d\uff09\nPython/Ruby/R \u7b49\u306e\u300c\u3044\u3044\u3068\u3053\u3069\u308a\u300d\u8a00\u8a9e\n\u79d1\u5b66\u6280\u8853\u8a08\u7b97\u306b\u5f37\u3044\uff01\n\u52d5\u4f5c\u304c\u901f\u3044\uff01\uff08LLVM JIT \u30b3\u30f3\u30d1\u30a4\u30eb\uff09\n\n\n\u2192 \u30d9\u30f3\u30c1\u30de\u30fc\u30af\n\n\n\n\n\n\nJulia \u3067 Deep Learning\n\n\u65e2\u5b58\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\uff08\u5916\u90e8\u30d1\u30c3\u30b1\u30fc\u30b8\uff09\uff1a\n\n\n\nMXNet.jl\uff08\u8efd\u91cf\u30fb\u52b9\u7387\u6027\u30fb\u67d4\u8edf\u6027\u304c\u30a6\u30ea\uff09\n\nTensorFlow.jl\uff08TensorFlow \u306e\u30e9\u30c3\u30d1\u30fc\uff09\n\n\n\u884c\u5217\uff08\u30c6\u30f3\u30bd\u30eb\uff09\u8a08\u7b97\u304c\u7c21\u5358\u3060\u304b\u3089\u3001\u81ea\u5206\u3067\u3082\u624b\u8efd\u306b\u5b9f\u88c5\u51fa\u6765\u307e\u3059\uff01\n\n\n\u3053\u306e\u8a18\u4e8b\uff08\u30b9\u30e9\u30a4\u30c9\uff09\u3067\u306f\u5b9f\u88c5\u4f8b\u3092\u7d39\u4ecb\uff01\n\n\n\n\n\nJulia \u30a4\u30f3\u30bf\u30fc\u30d7\u30ea\u30bf1\n\n\u3068\u3001\u57fa\u672c\u7684\u306a\u6f14\u7b972\u3002\njulia> VERSION\nv\"0.5.0\"\n\njulia> 1 + 2\n3\n\njulia> 7 / 5  # \u9664\u7b97\n1.4\n\njulia> 7 \u00f7 5  # \u6574\u6570\u9664\u7b97\n1\n\njulia> 3 ^ 2  # \u3079\u304d\u4e57\n9\n\n\n\n\u30d9\u30af\u30c8\u30eb\nJulia \u3067\u306f1\u6b21\u5143\u914d\u5217\u304c\u30d9\u30af\u30c8\u30eb\u3002\njulia> a = [1, 2, 3, 4, 5]\n5-element Array{Int64,1}:\n 1\n 2\n 3\n 4\n 5\n\njulia> a[1]   # Julia \u306f 1-origin\n1\n\njulia> println(a[2:3])  # \u7bc4\u56f2\u6307\u5b9a\u306f\u4e21\u7aef\u542b\u3080\n[2,3]\n\n\n\n\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b973\n\njulia> x = [1., 2., 3.];\n\njulia> y = [3., 1., 2.];\n\njulia> x + y  # `x .+ y` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff08elementwise operation\uff09\n[4., 3., 5.]\n\njulia> x .* y # \u3053\u308c\u306f `x * y` \u3068\u66f8\u304f\u3068NG\n[3., 2., 6.]\n\njulia> x \u22c5 y  # \u5185\u7a4d\uff08dot\u7a4d\u3001`dot(x, y)` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff09\n11.0\n\njulia> x \u00d7 y  # \u5916\u7a4d\uff08cross\u7a4d\u3001`cross(x, y)` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff09\n[1., 7., -5.]\n\n\n\n\u884c\u5217\nJulia \u3067\u306f2\u6b21\u5143\u914d\u5217\u304c\u884c\u52174\u3002\njulia> A = [1 2; 3 4]   # \u3053\u306e\u8a18\u6cd5\u306f MATLAB/Octave \u7531\u6765\n2\u00d72 Array{Int64,2}:\n 1  2\n 3  4\n\njulia> A'   # `\u25cb'` \u306f\u8ee2\u7f6e\u884c\u5217\u306e\u8a18\u6cd5\uff08\u3053\u308c\u3082 MATLAB/Octave \u7531\u6765\uff09\n2\u00d72 Array{Int64,2}:\n 1  3\n 2  4\n\n\n\n\u884c\u5217\u306e\u6f14\u7b975\n\njulia> A = [1 2; 3 4]; B = [3 0; 0 6];\n\njulia> A + B  # A .+ B \u3067\u3082\u540c\u69d8\n2\u00d72 Array{Int64,2}:\n 4   2\n 3  10\n\njulia> A .* B   # elementwise multiply\n2\u00d72 Array{Int64,2}:\n 3   0\n 0  24\n\njulia> A * B   # matrix multiply\n2\u00d72 Array{Int64,2}:\n 3  12\n 9  24\n\n\n\n\u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\n\u30d9\u30af\u30c8\u30eb\uff081\u6b21\u5143\u914d\u5217\uff09\u306f\u7e26\u30d9\u30af\u30c8\u30eb\u306e\u6271\u3044\u3002\njulia> A = [1 2; 3 4]; x = [1, 2];\n\njulia> A * x  # 2x2\u884c\u5217\u30682\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u7a4d\u21922\u6b21\u5143\u30d9\u30af\u30c8\u30eb\n[5,10]\n\njulia> A .+ x   # broadcast\n2\u00d72 Array{Int64,2}:\n 2  3\n 5  6\n\njulia> A .+ x'  # broadcast2\uff08\u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e\u306f1xN\u884c\u5217\uff08\u884c\u30d9\u30af\u30c8\u30eb\uff09\uff09\n2\u00d72 Array{Int64,2}:\n 2  4\n 4  6\n\n\n\nJulia \u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\n\n\n\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\n\u306e\u7d39\u4ecb\u3092\u517c\u306d\u305f\u95a2\u6570\u5b9a\u7fa967\u4f8b\u3002\nfunction sigmoid(x)\n    1.0 ./ (1.0 .+ exp(-x))\nend\n\n\u203b\u2191x\u304c\u30b9\u30ab\u30e9\u30fc\u3067\u3082\u30d9\u30af\u30c8\u30eb\u3067\u3082\uff08\u884c\u5217\u3067\u3082\uff09\u305d\u306e\u307e\u307e\u52d5\u4f5c\u3059\u308b\u3002\n\n\nGadfly8 \u3092\u4f7f\u3063\u3066\u30d7\u30ed\u30c3\u30c8\n# Pkg.add(\"Gadfly\")\nusing Gadfly\n\nx = linspace(-5.0, 5.0)\ny = sigmoid(x)\nplot(x=x, y=y, Geom.line)\n\n\n\n\n\n3\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\nfunction nn3lp(x)\n    W1 = [0.1 0.2; 0.3 0.4; 0.5 0.6];\n    b1 = [0.1, 0.2, 0.3];\n    W2 = [0.1 0.2 0.3; 0.4 0.5 0.6];\n    b2 = [0.1, 0.2];\n    W3 = [0.1 0.2; 0.3 0.4];\n    b3 = [0.1, 0.2];\n\n    a1 = W1 * x .+ b1\n    z1 = sigmoid(a1)\n    a2 = W2 * z1 .+ b2\n    z2 = sigmoid(a2)\n    a3 = W3 * z2 .+ b3\n    identity(a3)\nend\n\n\njulia> x = [1.0, 0.5];\n\njulia> y = nn3lp(x)\n2-element Array{Float64,1}:\n 0.316827\n 0.696279\n\n\n\n\u7c21\u5358\u3067\u3059\u306d\uff01\n\n\n\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\n\u306e\u7d39\u4ecb\u3092\u517c\u306d\u305f\u5c11\u3057\u8907\u96d1\u306a\u95a2\u6570\u5b9a\u7fa991011\u4f8b\u3002\nfunction softmax{T}(a::AbstractVector{T})\n    c = maximum(a)  # \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\n    exp_a = exp(a .- c)\n    exp_a ./ sum(exp_a)\nend\n\nfunction softmax{T}(a::AbstractMatrix{T})\n    # \u884c\u5217\u306e\u5834\u5408\uff1a\u5404\u5217\u30d9\u30af\u30c8\u30eb\u306b`softmax`\u3092\u9069\u7528\n    mapslices(softmax, a, 1)\nend\n\n\njulia> a = [0.3, 2.9, 4.0];\n\njulia> softmax(a)\n[0.0182113, 0.245192, 0.736597]\n\njulia> A = [0.3 1001.0; 2.9 1000.0; 4.0 999.0];\n\njulia> softmax(A)\n3\u00d72 Array{Float64,2}:\n 0.0182113  0.665241\n 0.245192   0.244728\n 0.736597   0.0900306\n\n\n\nMNIST\n\u624b\u66f8\u304d\u6570\u5b57\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08MNIST\uff09\u3002\n\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b812\u3067\u7c21\u5358\u306b\u5229\u7528\u53ef\u80fd\u3002\njulia> # Pkg.add(\"MNIST\")\n\njulia> using MNIST\n\njulia> x_train, t_train = traindata();\n\njulia> x_test, t_test = testdata();\n\njulia>size(x_train)\n(784,60000)\n\n\n\n\u753b\u50cf\u78ba\u8a8d\n\u753b\u50cf\u306e\u8868\u793a\u306b\u306f\u3001\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b8 Images.jl13 \u3092\u5229\u7528\u3002\njulia> # Pkg.add(\"Images\")\n\njulia> using Images\n\njulia> grayim(reshape(collect(UInt8, x_train[:, 1]), 28,28)')\n\n\n\n\n\n\u8a13\u7df4\u6e08\u30e2\u30c7\u30eb\u8aad\u8fbc\n\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b8 JLD14 \u3067\u5909\u6570\u306e\u8aad\u8fbc\uff08\u304a\u3088\u3073\u66f8\u8fbc\uff09\u304c\u53ef\u80fd1516\u3002\njulia> # Pkg.add(\"JLD\")\n\njulia> using JLD\n\njulia> network = load(\"/path/to/sample_network.jld\")\nDict{String,Any} with 6 entries:\n  \"W2\" => Float32[-0.10694 0.299116 \u2026 0.100016 -0.0222066; 0.0159125 -0.0332223\u2026\n  \"W3\" => Float32[-0.421736 -0.524321 \u2026 -0.544508 1.07228; 0.689445 -0.143625 \u2026\u2026\n  \"b3\" => Float32[-0.0602398,0.00932628,-0.0135995,0.0216713,0.0107372,0.066197\u2026\n  \"W1\" => Float32[-2.90686f-5 -1.88106f-5 \u2026 -0.000386883 1.09472f-6; -3.09976f-\u2026\n  \"b2\" => Float32[-0.0147111,-0.0721513,-0.00155692,0.121997,0.116033,-0.007549\u2026\n  \"b1\" => Float32[-0.0675032,0.0695926,-0.0273047,0.0225609,-0.220015,-0.220388\u2026\n\n\n\nMNIST \u306e\u4e88\u6e2c\nfunction predict(network, x)\n    W1 = network[\"W1\"]\n    b1 = network[\"b1\"]\n    W2 = network[\"W2\"]\n    b2 = network[\"b2\"]\n    W3 = network[\"W3\"]\n    b3 = network[\"b3\"]\n\n    a1 = W1 * x .+ b1\n    z1 = sigmoid(a1)\n    a2 = W2 * z1 .+ b2\n    z2 = sigmoid(a2)\n    a3 = W3 * z2 .+ b3\n    softmax(a3)\nend\n\n\njulia> predict(network, x_train[:, 1])\n10-element Array{Float64,1}:\n 0.0106677  \n 0.000158301\n 0.000430344\n 0.215528   \n 5.69069e-6 \n 0.7676     \n 3.01649e-5 \n 0.00311578 \n 0.00166548 \n 0.000798537\n\njulia> indmax(predict(network, x_train[:, 1])) - 1\n5\n\njulia> t_train[1]\n5.0   # \u5408\u3063\u3066\u307e\u3059\u306d\uff01\n\n\n\n\u6c4e\u5316\u7cbe\u5ea6\uff08\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u7387\uff09\njulia> y = predict(network, x_test)\n10\u00d710000 Array{Float64,2}:\n 8.44125e-5   0.00483633  1.03583e-7   \u2026  0.000624385  0.000428828\n 2.63506e-6   0.00110459  0.988973        0.000767557  2.0043e-6  \n 0.000715494  0.944252    0.00428949      0.000124992  0.00254057 \n 0.00125863   0.0143091   0.00178321      0.000642085  2.01689e-6 \n 1.1728e-6    5.69896e-7  0.000131734     0.00126703   0.000559177\n 4.49908e-5   0.00667604  0.000759407  \u2026  0.907499     0.00031262 \n 1.62693e-8   0.0275334   0.00046891      0.00274028   0.996148   \n 0.997065     1.27084e-6  0.00226997      3.81287e-5   4.34994e-7 \n 9.37447e-6   0.00128642  0.00123787      0.0862065    6.37568e-6 \n 0.000818312  4.78647e-8  8.67147e-5      8.9935e-5    3.77514e-7\n\njulia> p = reshape(mapslices(indmax, y, 1), (length(t_test),)) .- 1;\n\njulia> mean(p .== t_test)\n0.9352  # 93.52%\uff01\n\n\n\n\u7c21\u5358\u3067\u3059\u306d\uff01\n\n\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5b66\u7fd2\n\n\nOne-hot Vector\n\u30e9\u30d9\u30eb\u304c\u96e2\u6563\u5024\u306e\u5834\u5408\u306b\u3001\u305d\u306e\u5024\u3092 One-hot Vector \u306b\u5909\u63db\u300217\nfunction onehot{T}(::Type{T}, t::AbstractVector, l::AbstractVector)\n    r = zeros(T, length(l), length(t))\n    for i = 1:length(t)\n        r[findfirst(l, t[i]), i] = 1\n    end\n    r\nend\n@inline onehot(t, l) = onehot(Int, t, l)\n\n\njulia> t_train[1:3]\n[5.0, 0.0, 4.0]\n\njulia> onehot(Float32, t_train[1:3], 0:9)\n10\u00d73 Array{Float32,2}:\n 0.0  1.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  1.0\n 1.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\n\n\n\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee18\n\nfunction crossentropyerror(y::Vector, t::Vector)\n    \u03b4 = 1e-7  # \u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\n    # -sum(t .* log(y .+ \u03b4))\n    -(t \u22c5 log(y .+ \u03b4))\nend\nfunction crossentropyerror(y::Matrix, t::Matrix)\n    batch_size = size(y, 2)\n    \u03b4 = 1e-7\n    # -sum(t .* log(y .+ \u03b4)) / batch_size\n    -vecdot(t, log(y .+ \u03b4)) / batch_size\nend\n\n\n\nt \u306f One-hot Vector\uff08\u307e\u305f\u306f\u305d\u308c\u3092\u4e26\u3079\u305f\u884c\u5217\uff09\uff08\u4ee5\u4e0b\u540c\u69d8\uff09\n\n\n\n\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\n\u3092\u610f\u8b58\u3057\u305f\u5404\u5c64\u306e\u8a2d\u8a0819\u3002\nabstract AbstractLayer\n\n\n\n\u5168\u7d50\u5408\u5c64\uff08Affine Layer\uff092021\n\ntype AffineLayer{T} <: AbstractLayer\n    W::AbstractMatrix{T}\n    b::AbstractVector{T}\n    x::AbstractArray{T}\n    dW::AbstractMatrix{T}\n    db::AbstractVector{T}\n    function (::Type{AffineLayer}){T}(W::AbstractMatrix{T}, b::AbstractVector{T})\n        lyr = new{T}()\n        lyr.W = W\n        lyr.b = b\n        lyr\n    end\nend\n\n\n\n\u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u306e\u5b9a\u7fa9 22\n\nfunction forward{T}(lyr::AffineLayer{T}, x::AbstractArray{T})\n    lyr.x = x\n    lyr.W * x .+ lyr.b\nend\n\nfunction backward{T}(lyr::AffineLayer{T}, dout::AbstractArray{T})\n    dx = lyr.W' * dout\n    lyr.dW = dout * lyr.x'\n    lyr.db = _sumvec(dout)\n    dx\nend\n@inline _sumvec{T}(dout::AbstractVector{T}) = dout\n@inline _sumvec{T}(dout::AbstractMatrix{T}) = vec(mapslices(sum, dout, 2))\n@inline _sumvec{T,N}(dout::AbstractArray{T,N}) = vec(mapslices(sum, dout, 2:N))\n\n\n\nReLU\u5c64\ntype ReluLayer <: AbstractLayer\n    mask::AbstractArray{Bool}\n    ReluLayer() = new()\nend\n\nfunction forward{T}(lyr::ReluLayer, x::AbstractArray{T})\n    mask = lyr.mask = (x .<= 0)\n    out = copy(x)\n    out[mask] = zero(T)\n    out\nend\n\nfunction backward{T}(lyr::ReluLayer, dout::AbstractArray{T})\n    dout[lyr.mask] = zero(T)\n    dout\nend\n\n\n\nSoftmaxWithLossLayer\ntype SoftmaxWithLossLayer{T} <: AbstractLayer\n    loss::T\n    y::AbstractArray{T}\n    t::AbstractArray{T}\n    (::Type{SoftmaxWithLossLayer{T}}){T}() = new{T}()\nend\n\n\n\n\n\n\u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u306e\u5b9a\u7fa9\nfunction forward{T}(lyr::SoftmaxWithLossLayer{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    lyr.t = t\n    y = lyr.y = softmax(x)\n    lyr.loss = crossentropyerror(y, t)\nend\n\nfunction backward{T}(lyr::SoftmaxWithLossLayer{T}, dout::T=1)\n    dout .* _swlvec(lyr.y, lyr.t)\nend\n@inline _swlvec{T}(y::AbstractArray{T}, t::AbstractVector{T}) = y .- t\n@inline _swlvec{T}(y::AbstractArray{T}, t::AbstractMatrix{T}) = (y .- t) / size(t)[2]\n\n\n\n2\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\uff082LP\uff09\n\u7c21\u5358\u306e\u305f\u30812\u5c64\u3067\u4f8b\u793a\u300223\ntype TwoLayerNet{T}\n    a1lyr::AffineLayer{T}\n    relu1lyr::ReluLayer\n    a2lyr::AffineLayer{T}\n    softmaxlyr::SoftmaxWithLossLayer{T}\nend\n\n\n\n2LP \u5916\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf24\n\nfunction (::Type{TwoLayerNet{T}}){T}(input_size::Int, hidden_size::Int, output_size::Int,\n        weight_init_std::Float64=0.01)\n    W1 = weight_init_std .* randn(T, hidden_size, input_size)\n    b1 = zeros(T, hidden_size)\n    W2 = weight_init_std .* randn(T, output_size, hidden_size)\n    b2 = zeros(T, output_size)\n    a1lyr = AffineLayer(W1, b1)\n    relu1lyr = ReluLayer()\n    a2lyr = AffineLayer(W2, b2)\n    softmaxlyr = SoftmaxWithLossLayer{T}()\n    TwoLayerNet(a1lyr, relu1lyr, a2lyr, softmaxlyr)\nend\n\n\n\n\u63a8\u6e2c25\n\nfunction predict{T}(net::TwoLayerNet{T}, x::AbstractArray{T})\n    a1 = forward(net.a1lyr, x)\n    z1 = forward(net.relu1lyr, a1)\n    a2 = forward(net.a2lyr, z1)\n    # softmax(a2)\n    a2\nend\n\n\n\n\u640d\u5931\u95a2\u6570\uff08\u76ee\u7684\u95a2\u6570\uff09\nfunction loss{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    y = predict(net, x)\n    forward(net.softmaxlyr, y, t)\nend\n\n\n\n\u6b63\u89e3\u7387\nfunction accuracy{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    y = vec(mapslices(indmax, predict(net, x), 1))\n    mean(y .== t)\nend\n\n\n\n\u52fe\u914d\u8a08\u7b97\uff08\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\uff092627\n\nimmutable TwoLayerNetGrads{T}\n    W1::AbstractMatrix{T}\n    b1::AbstractVector{T}\n    W2::AbstractMatrix{T}\n    b2::AbstractVector{T}\nend\n\nfunction Base.gradient{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    # forward\n    loss(net, x, t)\n    # backward\n    dout = one(T)\n    dz2 = backward(net.softmaxlyr, dout)\n    da2 = backward(net.a2lyr, dz2)\n    dz1 = backward(net.relu1lyr, da2)\n    da1 = backward(net.a1lyr, dz1)\n    TwoLayerNetGrads(net.a1lyr.dW, net.a1lyr.db, net.a2lyr.dW, net.a2lyr.db)\nend\n\n\n\n\u52fe\u914d\u9069\u752828\n\nfunction applygradient!{T}(net::TwoLayerNet{T}, grads::TwoLayerNetGrads{T}, learning_rate::T)\n    net.a1lyr.W -= learning_rate .* grads.W1\n    net.a1lyr.b -= learning_rate .* grads.b1\n    net.a2lyr.W -= learning_rate .* grads.W2\n    net.a2lyr.b -= learning_rate .* grads.b2\nend\n\n\n\n\u5b66\u7fd2\uff08SGD\uff09\n# using MNIST\n\n_x_train, _t_train = traindata();\n_x_test, _t_test = testdata();\nx_train = collect(Float32, _x_train) ./ 255   # \u578b\u5909\u63db+\u6b63\u898f\u5316\nt_train = onehot(Float32, _t_train, 0:9)      # One-hot Vector \u5316\nx_test = collect(Float32, _x_test) ./ 255     # \u578b\u5909\u63db+\u6b63\u898f\u5316\nt_test = onehot(Float32, _t_test, 0:9)        # One-hot Vector \u5316\n\niters_num = 10000;\ntrain_size = size(x_train, 2); # => 60000\nbatch_size = 100;\nlearning_rate = Float32(0.1);\n\n\ntrain_loss_list = Float32[];\ntrain_acc_list = Float32[];\ntest_acc_list = Float32[];\n\niter_per_epoch = max(train_size \u00f7 batch_size, 1)  # => 600\n\nnetwork = TwoLayerNet{Float32}(784, 50, 10);\n\n\nfor i = 1:iters_num\n    batch_mask = rand(1:train_size, batch_size)\n    x_batch = x_train[:, batch_mask]\n    t_batch = t_train[:, batch_mask]\n\n    # \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6c42\u3081\u308b\n    grads = gradient(network, x_batch, t_batch)\n\n    # \u66f4\u65b0\n    applygradient!(network, grads, learning_rate)\n\n    _loss = loss(network, x_batch, t_batch)\n    push!(train_loss_list, _loss)\n\n    if i % iter_per_epoch == 1\n        train_acc = accuracy(network, x_train, t_train)\n        test_acc = accuracy(network, x_test, t_test)\n        push!(train_acc_list, train_acc)\n        push!(test_acc_list, test_acc)\n        println(\"$(i-1): train_acc=$(train_acc) / test_acc=$(test_acc)\")\n    end\nend\n\n\n\n\n\n\u7cbe\u5ea6\uff08\u6b63\u89e3\u7387\uff09\u78ba\u8a8d\nfinal_train_acc = accuracy(network, x_train, t_train)\nfinal_test_acc = accuracy(network, x_test, t_test)\npush!(train_acc_list, final_train_acc)\npush!(test_acc_list, final_test_acc)\nprintln(\"final: train_acc=$(final_train_acc) / test_acc=$(final_test_acc)\")\n# => final: train_acc=0.97685 / test_acc=0.9686\n\n\n\n\n\n\u5b66\u7fd2\u66f2\u7dda\n\n\n\n\u7c21\u5358\u3067\u3059\u306d\u3063\uff01\n\n\n\u767a\u5c55\n\n\u6700\u9069\u5316\u624b\u6cd5\uff08SGD \u306e\u4ee3\u308f\u308a\u306b\uff09\n\n\nMomentum\nAdaGrad\nAdam\n\n\nBatch Normalization\n\u6b63\u5247\u5316\n\n\nWeight Decay\nDropout\n\n\nCNN\n\n\n\n\u53c2\u8003\n\nJulia Documentation (v0.5)\n\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning\n\n\n\n\n\n\u3053\u306e\u8a18\u4e8b\u3067\u306f Julia v0.5.0 \u4ee5\u964d\u3092\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002v0.4.x \u4ee5\u524d\u3067\u306f\u4e00\u90e8\u52d5\u4f5c\u3057\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u00a0\u21a9\n\n\n/\u306f\u3001\u6574\u6570\u306b\u9069\u7528\u3059\u308b\u3068\u6d6e\u52d5\u5c0f\u6570\u306b\u306a\u308b\uff08Python3\u3068\u540c\u69d8\u306e\u52d5\u4f5c\uff09\u3002\u6574\u6570\u9664\u7b97\u306f\uff08//\u3067\u306f\u306a\u304f\uff09\u00f7\u3002\u307e\u305f\u3079\u304d\u4e57\u306f\uff08**\u3067\u306f\u306a\u304f\uff09^\u3002\u00a0\u21a9\n\n\n\u3053\u3053\u306b\u51fa\u3066\u304f\u308b \u22c5 \u306f\u3001\uff08\u65e5\u672c\u8a9e\u306e\u4e2d\u9ed2\u300c\u30fb\u300d\u3067\u306f\u306a\u304f\uff09U+22C5 'DOT OPERATOR'\u3002Julia \u30b3\u30f3\u30bd\u30fc\u30eb\u3084\u3001Julia \u7528\u30d7\u30e9\u30b0\u30a4\u30f3\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30a8\u30c7\u30a3\u30bf\u306a\u3089\u3001\\cdot \u3068\u5165\u529b\u3057\u3066Tab\u30ad\u30fc\u3092\u62bc\u3059\u3068\u5909\u63db\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u00d7\u306f\u65e5\u672c\u3067\u3082\u639b\u3051\u7b97\u8a18\u53f7\u3068\u3057\u3066\u304a\u306a\u3058\u307f\uff08\u305f\u3044\u3066\u3044\u306eIME\u3067\u300c\u304b\u3051\u308b\u300d\u3067\u5909\u63db\u53ef\u80fd\uff09\u3067\u3059\u304c\u3053\u308c\u3082 \\times+Tab \u3067\u5165\u529b\u3067\u304d\u307e\u3059\u3002\u00a0\u21a9\n\n\nNumPy \u306e\u884c\u5217\u306e\u66f8\u5f0f\uff08np.array([[1, 2], [3, 4]])\uff09\u3088\u308a\u5727\u5012\u7684\u306b\u8a18\u8ff0\u91cf\u5c11\u306a\u3044\uff01\u00a0\u21a9\n\n\n* \u304c\u901a\u5e38\u306e\u884c\u5217\u7a4d\uff01 NumPy\u306e A.dot(B) \u3068\u6bd4\u3079\u3066\u8a18\u8ff0\u91cf\u3082\u5c11\u306a\u3044\u3057\u76f4\u89b3\u7684\uff01\u00a0\u21a9\n\n\n\u30ef\u30f3\u30e9\u30a4\u30f3\u3067 sigmoid(x) = 1.0 ./ (1.0 .+ exp(-x)) \u3068\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3002\u00a0\u21a9\n\n\nRuby \u306a\u3069\u3068\u540c\u69d8\u3067\u3001\u6700\u5f8c\u306b\u5b9f\u884c\u3057\u305f\u6587\uff08\u5f0f\uff09\u306e\u5024\u304c\u623b\u308a\u5024\uff08return \u7701\u7565\u53ef\u80fd\uff09\u3002\u660e\u793a\u7684\u306b return \uff5e \u3092\u66f8\u304f\u3053\u3068\u3082\u53ef\u80fd\u3002\u00a0\u21a9\n\n\nhttp://gadflyjl.org/ \u3002Julia \u5b9a\u756a\u306e\u30d7\u30ed\u30c3\u30c8\u30e9\u30a4\u30d6\u30e9\u30ea\u3002\u00a0\u21a9\n\n\n\u95a2\u6570\u5f15\u6570\u306b\u3064\u3044\u3066\u3044\u308b ::Xxx \u306f\u578b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3002\u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\u306b\u578b\u304c\u5408\u308f\u306a\u3051\u308c\u3070\u30a8\u30e9\u30fc\u306b\u306a\u308b\uff08\u52d5\u7684\u578b\u30c1\u30a7\u30c3\u30af\uff09\u3002\u00a0\u21a9\n\n\n\u95a2\u6570\u540d\u3068\u5f15\u6570\u30ea\u30b9\u30c8\u306e\u9593\u306e {T} \u306f\u3001\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3002\u5f15\u6570\u306e\u578b AbstractVector\u3068\u304bAbstractMatrix \u304c\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u308b\u304c\u305d\u308c\u304c\u672a\u5b9a\u306e\u6642\u306e\u8868\u8a18\u3002\u3053\u3053\u306b {T<:AbstractFloat} \u3068\u66f8\u304f\u3053\u3068\u3067\u300cT \u306f AbstractFloat \u306e sub type\u300d\u3068\u3044\u3046\u6307\u5b9a\uff08\u578b\u5883\u754c\u306e\u6307\u5b9a\uff09\u3082\u53ef\u80fd\u3002\u00a0\u21a9\n\n\n\u540c\u3058\u95a2\u6570\u540d\u3067\u7570\u306a\u308b\u5f15\u6570\uff08\u578b\u304a\u3088\u3073\u500b\u6570\uff09\u306e\u95a2\u6570\u3092\u591a\u91cd\u5b9a\u7fa9\u3067\u304d\u308b\u3002\u5b9f\u884c\u6642\uff08\u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\uff09\u306b\u9069\u5207\u306a\u5f15\u6570\u3092\u6301\u3064\u95a2\u6570\u304c\u5b9f\u884c\u3055\u308c\u308b\uff08\u591a\u91cd\u30c7\u30a3\u30b9\u30d1\u30c3\u30c1\u3068\u8a00\u3044\u307e\u3059\uff09\u3002\u307e\u305f\u3053\u306e\u4ed5\u7d44\u307f\u306e\u304a\u304b\u3052\u3067if\u6587\u304c\u6e1b\u3089\u305b\u308b\uff01\u00a0\u21a9\n\n\nhttp://github.com/johnmyleswhite/MNIST.jl\u00a0\u21a9\n\n\nhttp://github.com/timholy/Images.jl\u00a0\u21a9\n\n\nhttp://github.com/JuliaIO/JLD.jl\u00a0\u21a9\n\n\n\u3053\u3053\u3067\u30ed\u30fc\u30c9\u3057\u3066\u3044\u308b\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306f\u3001\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning \u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf sample_weight.pkl \u3092\u8aad\u307f\u8fbc\u3093\u3067\u5909\u5f62\u3057\u305f\u4e0a\u3067 .jld \u5f62\u5f0f\u3067\u4fdd\u5b58\u3057\u305f\u3082\u306e\u3067\u3059\u3002\u00a0\u21a9\n\n\nDict \u306f Julia \u306e\u8f9e\u66f8\u578b\u3002Dict{String,Any} \u306f\u3001\u300c\u30ad\u30fc\u304c\u6587\u5b57\u5217\uff08String\uff09\u3001\u5024\u306f\u4f55\u3067\u3082\u3042\u308a\uff08Any\uff09\u306e\u8f9e\u66f8\u300d\u3068\u3044\u3046\u610f\u5473\u3002\u00a0\u21a9\n\n\n\u7b2c1\u5f15\u6570\u306e ::T \u3068\u3044\u3046\u66f8\u5f0f\u306f\u3001\u6700\u521d\u306e\u5f15\u6570\u304c T \u578b\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3059\u308b\uff08\u305d\u306e\u5024\u306f\u554f\u308f\u306a\u3044\uff08\u4e0d\u4f7f\u7528\uff09\uff09\u3002\u307e\u305f ::Type{T} \u3060\u3068\u3001\u305d\u3053\u306b\u306f \u578b\u305d\u306e\u3082\u306e\uff08Int\u3068\u304bFloat64\u3068\u304b\uff09 \u304c\u6765\u308b\u3053\u3068\u3092\u8868\u3059\u3002\u00a0\u21a9\n\n\n\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3057\u3066\u3042\u308b -sum(t .* log(y .+ \u03b4)) \u306f\u5b9a\u7fa9\u3092\u305d\u306e\u307e\u307e\u66f8\u304d\u4e0b\u3057\u305f\u3082\u306e\u3002\u22c5 \u307e\u305f\u306f vecdot() \u306f\u305d\u308c\u3092\u66f8\u304d\u76f4\u3057\u305f\u3082\u306e\u3067\u3001\uff08\u30d9\u30af\u30c8\u30eb\uff081\u6b21\u5143\u914d\u5217\uff09\u306b\u3057\u305f\u4e0a\u3067\uff09\u5185\u7a4d\u3092\u53d6\u3063\u305f\u65b9\u304c\u7701\u30e1\u30e2\u30ea\u304b\u3064\u9ad8\u901f\uff01\u00a0\u21a9\n\n\n\u30ad\u30fc\u30ef\u30fc\u30c9 abstract \u3067\u59cb\u307e\u308b\u306e\u306f\u62bd\u8c61\u578b\u3002\u30e1\u30f3\u30d0\u30fc\u3092\u6301\u3064\u3053\u3068\u306f\u3067\u304d\u306a\u3044\uff08\u578b\u306e\u968e\u5c64\u69cb\u9020\uff08\u30c4\u30ea\u30fc\u69cb\u9020\uff09\u69cb\u7bc9\u306e\u305f\u3081\u306e\u3082\u306e\uff09\u00a0\u21a9\n\n\n\u30ad\u30fc\u30ef\u30fc\u30c9 type \u306f\u3001\u578b\uff08\u5177\u8c61\u578b\uff09\u306e\u5b9a\u7fa9\u3002 type A <: B \u3068\u3044\u3046\u66f8\u5f0f\u306f\u300cB \u3068\u3044\u3046\u578b\u3092\u7d99\u627f\u3057\u305f A \u3068\u3044\u3046\u578b\u3092\u5b9a\u7fa9\u300d\u3068\u3044\u3046\u610f\u5473\u306b\u306a\u308b\u3002{T} \u306f\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u3002\u00a0\u21a9\n\n\ntype \uff5e end \u5185\u306b\u8a18\u8ff0\u3057\u305f\u3001\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\u306f\u3001\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3068\u306a\u308b\u3002\u4e3b\u306b\u300c\u5ba3\u8a00\u3057\u305f\u30e1\u30f3\u30d0\u306e\u3046\u3061\u306e\u4e00\u90e8\u306e\u307f\u3092\u53d7\u3051\u53d6\u3063\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u300d\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002new() \u306f\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u5185\u306e\u307f\u3067\u5229\u7528\u3067\u304d\u308b\u7279\u5225\u306a\u95a2\u6570\uff08\u30e1\u30e2\u30ea\u78ba\u4fdd\uff09\u3002\u00a0\u21a9\n\n\n@inline \u306f\u3001\u300c\u4eca\u304b\u3089\u66f8\u304f\u95a2\u6570\u5b9a\u7fa9\u3092\uff08\u53ef\u80fd\u306a\u3089\u3070\uff09\u30a4\u30f3\u30e9\u30a4\u30f3\u5c55\u958b\u3057\u3066\u306d\u300d\u3068\u3044\u3046\u6307\u5b9a\u3002\u306a\u304a @\uff5e \u306f Julia \u306e\u30de\u30af\u30ed\u547c\u3073\u51fa\u3057\u306e\u66f8\u5f0f\uff08\u30a4\u30f3\u30e9\u30a4\u30f3\u6307\u5b9a\u306f\u30de\u30af\u30ed\u3068\u3057\u3066\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\uff09\uff08Java \u306e Annotation \u3067\u3082\u306a\u3044\u3057 Python \u306e Decorator \u3068\u3082\u5225\u7269\uff09\u3002\u00a0\u21a9\n\n\n\u4eca\u307e\u3067\u306e type \u5b9a\u7fa9\u3067\u306f\u4e2d\u306b\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf21\uff08\uff1d\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\uff09\u3092\u6307\u5b9a\u3057\u3066\u304d\u305f\u3002\u3053\u3053\u3067\u306f\u305d\u306e\u8a18\u8ff0\u304c\u306a\u3044\u304c\u3053\u306e\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u304c\u6697\u9ed9\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002\u5f15\u6570\u306f\u30e1\u30f3\u30d0\u3092\u5ba3\u8a00\u9806\u306b\u3059\u3079\u3066\uff08\u3053\u306e\u5834\u5408 TwoLayerNet{T}(a1lyr::AffineLayer{T},relu1lyr::ReluLayer,a2lyr::AffineLayer{T},softmaxlyr::SoftmaxWithLossLayer{T})\uff09\u3002\u00a0\u21a9\n\n\ntype \uff5e end \u306e\u5b9a\u7fa9\u5916\u3067\u5b9a\u7fa9\u3057\u305f\u3001\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\u306f\u3001\u5916\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\uff08\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u308b\u95a2\u6570\uff09\u3068\u306a\u308b\u3002\u5f15\u6570\u306b\u5bfe\u3057\u3066\u8907\u96d1\u306a\u51e6\u7406\u3092\u3057\u3066\u304b\u3089\u3001\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u542b\u3080\uff09\u306b\u5f15\u304d\u6e21\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3059\u308b\uff08\u3068\u3044\u3046\u7528\u9014\u304c\u4e00\u822c\u7684\uff09\u3002\u00a0\u21a9\n\n\n\u3053\u3053\u3067\u306f softmax() \u304b\u3051\u306a\u3044\u3002\u00a0\u21a9\n\n\nimmutable \u30ad\u30fc\u30ef\u30fc\u30c9\u3067\u59cb\u307e\u308b\u578b\u5b9a\u7fa9\u306f\u300c\u4e0d\u5909\u578b\u300d\u306e\u5b9a\u7fa9\u3002type \u3067\u5b9a\u7fa9\u3057\u305f\u578b\u3068\u6bd4\u8f03\u3057\u3066\u3001\u30e1\u30f3\u30d0\u30fc\u306e\u5909\u66f4\u3092\u8a31\u53ef\u3057\u306a\u3044\u3002\u00a0\u21a9\n\n\nJulia \u6a19\u6e96\u306e gradient \u3068\u3044\u3046\u95a2\u6570\u304c\u65e2\u306b\u5b58\u5728\u3059\u308b\u306e\u3067\u3001\u95a2\u6570\u591a\u91cd\u5b9a\u7fa9\u6642\u306b Base.gradient \u3068\u3059\u308b\u304b\u3001\u4e8b\u524d\u306b import Base.gradient \u3092\u5b9f\u884c\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u6012\u3089\u308c\u308b\u3002\u00a0\u21a9\n\n\nJulia \u3067\u306f\u3001\u5f15\u6570\u306e\u5185\u5bb9\uff08\u30e1\u30f3\u30d0\u30fc\u7b49\uff09\u3092\u5909\u66f4\u3059\u308b\u95a2\u6570\u540d\u306b ! \u3092\u3064\u3051\u308b\u6163\u7fd2\u304c\u3042\u308b\uff08\u304a\u305d\u3089\u304fRuby\u7531\u6765\uff09\u3002\u00a0\u21a9\n\n\n\n## TL;DR\n\n+ Julia \u306f\u901f\u3044\u3088\u3002\n+ Julia \u306f\u884c\u5217\u6f14\u7b97\u7c21\u5358\u3060\u3088\u3002\n+ Julia \u306f Deep Learning \u5411\u304d\u3060\u3088\u3002\n+ Julia \u306f\u305d\u3082\u305d\u3082\u66f8\u304d\u3084\u3059\u3044\u3088\u306f\u304b\u3069\u308b\u3088\uff01\n+ Julia \u826f\u3044\u3088 Julia\uff01\n\n---\n\n## \u521d\u3081\u306b\n\n+ \u3053\u306e\u8a18\u4e8b\u306f\u3001[Julia Advent Calendar 2016](http://qiita.com/advent-calendar/2016/julialang) \u306e3\u65e5\u76ee\u306e\u8a18\u4e8b\u3067\u3059\u3002\n+ \u3068\u540c\u6642\u306b\u3001[\u6a5f\u68b0\u5b66\u7fd2 \u540d\u53e4\u5c4b \u7b2c8\u56de\u52c9\u5f37\u4f1a](http://machine-learning.connpass.com/event/43540/) \u306e\u767a\u8868\u8cc7\u6599\u3067\u3059\u3002\n+ \u307e\u305f\u3053\u306e\u8a18\u4e8b\u306f\u3001[\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)\uff08O'reilly, 2016/09\uff09\u3092\u53c2\u8003\u306b\u69cb\u7bc9\u3057\u3066\u3044\u307e\u3059\u3002\n    + \u2191 \u3092Julia\u306b\u79fb\u690d\u4e2d \u2192 [DLScratch.jl](https://github.com/antimon2/DLScratch.jl)\n\n---\n\n# \u81ea\u5df1\u7d39\u4ecb\n\n---\n\n## \u81ea\u5df1\u7d39\u4ecb\n\n+ \u540d\u524d\uff1a\u5f8c\u85e4 \u4fca\u4ecb\n+ \u6240\u5c5e\uff1a\u6709\u9650\u4f1a\u793e \u6765\u6816\u5ddd\u96fb\u7b97\n+ \u8a00\u8a9e\uff1aPython, **Julia**, Ruby, Scala\uff08\u52c9\u5f37\u4e2d\uff09, \u2026\n+ twitter: [@antimon2](https://twitter.com/antimon2)\n+ Facebook: [antimon2](https://www.facebook.com/antimon2)\n+ GitHub: [antimon2](https://github.com/antimon2/)\n+ Qiita: @antimon2\n\n---\n\n# Julia\n\n---\n\n## Julia \u3068\u306f\n\n+ [The Julia Language](http://julialang.org/)\n+ 2016/09/20 \u306b [v0.5.0 \u304c\u30ea\u30ea\u30fc\u30b9](https://github.com/JuliaLang/julia/releases/tag/v0.5.0)\uff082016/12/03 \u73fe\u5728 [v0.6.0](https://github.com/JuliaLang/julia) \u3092\u958b\u767a\u4e2d\uff09\n+ Python/Ruby/R \u7b49\u306e\u300c\u3044\u3044\u3068\u3053\u3069\u308a\u300d\u8a00\u8a9e\n+ \u79d1\u5b66\u6280\u8853\u8a08\u7b97\u306b\u5f37\u3044\uff01\n+ \u52d5\u4f5c\u304c\u901f\u3044\uff01\uff08LLVM JIT \u30b3\u30f3\u30d1\u30a4\u30eb\uff09\n    + \u2192 [\u30d9\u30f3\u30c1\u30de\u30fc\u30af](http://julialang.org/#high-performance-jit-compiler)\n\n---\n\n## Julia \u3067 Deep Learning\n\n+ \u65e2\u5b58\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\uff08\u5916\u90e8\u30d1\u30c3\u30b1\u30fc\u30b8\uff09\uff1a\n    + [MXNet.jl](https://github.com/dmlc/MXNet.jl)\uff08\u8efd\u91cf\u30fb\u52b9\u7387\u6027\u30fb\u67d4\u8edf\u6027\u304c\u30a6\u30ea\uff09\n    + [TensorFlow.jl](https://github.com/malmaud/TensorFlow.jl)\uff08[TensorFlow](https://www.tensorflow.org/) \u306e\u30e9\u30c3\u30d1\u30fc\uff09\n+ \u884c\u5217\uff08\u30c6\u30f3\u30bd\u30eb\uff09\u8a08\u7b97\u304c\u7c21\u5358\u3060\u304b\u3089\u3001\u81ea\u5206\u3067\u3082\u624b\u8efd\u306b\u5b9f\u88c5\u51fa\u6765\u307e\u3059\uff01\n    + \u3053\u306e\u8a18\u4e8b\uff08\u30b9\u30e9\u30a4\u30c9\uff09\u3067\u306f\u5b9f\u88c5\u4f8b\u3092\u7d39\u4ecb\uff01\n\n---\n\n## Julia \u30a4\u30f3\u30bf\u30fc\u30d7\u30ea\u30bf[^juliaveraion]\n\n\u3068\u3001\u57fa\u672c\u7684\u306a\u6f14\u7b97[^operation]\u3002\n\n```jlcon\njulia> VERSION\nv\"0.5.0\"\n\njulia> 1 + 2\n3\n\njulia> 7 / 5  # \u9664\u7b97\n1.4\n\njulia> 7 \u00f7 5  # \u6574\u6570\u9664\u7b97\n1\n\njulia> 3 ^ 2  # \u3079\u304d\u4e57\n9\n```\n\n[^juliaveraion]: \u3053\u306e\u8a18\u4e8b\u3067\u306f Julia v0.5.0 \u4ee5\u964d\u3092\u5bfe\u8c61\u3068\u3057\u307e\u3059\u3002v0.4.x \u4ee5\u524d\u3067\u306f\u4e00\u90e8\u52d5\u4f5c\u3057\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n[^operation]: `/`\u306f\u3001\u6574\u6570\u306b\u9069\u7528\u3059\u308b\u3068\u6d6e\u52d5\u5c0f\u6570\u306b\u306a\u308b\uff08Python3\u3068\u540c\u69d8\u306e\u52d5\u4f5c\uff09\u3002\u6574\u6570\u9664\u7b97\u306f\uff08`//`\u3067\u306f\u306a\u304f\uff09`\u00f7`\u3002\u307e\u305f\u3079\u304d\u4e57\u306f\uff08`**`\u3067\u306f\u306a\u304f\uff09`^`\u3002\n\n---\n\n## \u30d9\u30af\u30c8\u30eb\n\nJulia \u3067\u306f1\u6b21\u5143\u914d\u5217\u304c\u30d9\u30af\u30c8\u30eb\u3002\n\n```jlcon\njulia> a = [1, 2, 3, 4, 5]\n5-element Array{Int64,1}:\n 1\n 2\n 3\n 4\n 5\n\njulia> a[1]   # Julia \u306f 1-origin\n1\n\njulia> println(a[2:3])  # \u7bc4\u56f2\u6307\u5b9a\u306f\u4e21\u7aef\u542b\u3080\n[2,3]\n```\n\n---\n\n### \u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97[^vec_op]\n\n```jlcon\njulia> x = [1., 2., 3.];\n\njulia> y = [3., 1., 2.];\n\njulia> x + y  # `x .+ y` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff08elementwise operation\uff09\n[4., 3., 5.]\n\njulia> x .* y # \u3053\u308c\u306f `x * y` \u3068\u66f8\u304f\u3068NG\n[3., 2., 6.]\n\njulia> x \u22c5 y  # \u5185\u7a4d\uff08dot\u7a4d\u3001`dot(x, y)` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff09\n11.0\n\njulia> x \u00d7 y  # \u5916\u7a4d\uff08cross\u7a4d\u3001`cross(x, y)` \u3068\u66f8\u3044\u3066\u3082\u540c\u3058\uff09\n[1., 7., -5.]\n```\n\n[^vec_op]: \u3053\u3053\u306b\u51fa\u3066\u304f\u308b `\u22c5` \u306f\u3001\uff08\u65e5\u672c\u8a9e\u306e\u4e2d\u9ed2\u300c\u30fb\u300d\u3067\u306f\u306a\u304f\uff09U+22C5 'DOT OPERATOR'\u3002Julia \u30b3\u30f3\u30bd\u30fc\u30eb\u3084\u3001Julia \u7528\u30d7\u30e9\u30b0\u30a4\u30f3\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u30a8\u30c7\u30a3\u30bf\u306a\u3089\u3001`\\cdot` \u3068\u5165\u529b\u3057\u3066<kbd>Tab</kbd>\u30ad\u30fc\u3092\u62bc\u3059\u3068\u5909\u63db\u3067\u304d\u307e\u3059\u3002\u307e\u305f`\u00d7`\u306f\u65e5\u672c\u3067\u3082\u639b\u3051\u7b97\u8a18\u53f7\u3068\u3057\u3066\u304a\u306a\u3058\u307f\uff08\u305f\u3044\u3066\u3044\u306eIME\u3067\u300c\u304b\u3051\u308b\u300d\u3067\u5909\u63db\u53ef\u80fd\uff09\u3067\u3059\u304c\u3053\u308c\u3082 `\\times`+<kbd>Tab</kbd> \u3067\u5165\u529b\u3067\u304d\u307e\u3059\u3002\n\n---\n\n## \u884c\u5217\n\nJulia \u3067\u306f2\u6b21\u5143\u914d\u5217\u304c\u884c\u5217[^matrix]\u3002\n\n```jlcon\njulia> A = [1 2; 3 4]   # \u3053\u306e\u8a18\u6cd5\u306f MATLAB/Octave \u7531\u6765\n2\u00d72 Array{Int64,2}:\n 1  2\n 3  4\n\njulia> A'   # `\u25cb'` \u306f\u8ee2\u7f6e\u884c\u5217\u306e\u8a18\u6cd5\uff08\u3053\u308c\u3082 MATLAB/Octave \u7531\u6765\uff09\n2\u00d72 Array{Int64,2}:\n 1  3\n 2  4\n```\n\n[^matrix]: NumPy \u306e\u884c\u5217\u306e\u66f8\u5f0f\uff08`np.array([[1, 2], [3, 4]])`\uff09\u3088\u308a\u5727\u5012\u7684\u306b\u8a18\u8ff0\u91cf\u5c11\u306a\u3044\uff01\n\n---\n\n### \u884c\u5217\u306e\u6f14\u7b97[^matmul]\n\n```jlcon\njulia> A = [1 2; 3 4]; B = [3 0; 0 6];\n\njulia> A + B  # A .+ B \u3067\u3082\u540c\u69d8\n2\u00d72 Array{Int64,2}:\n 4   2\n 3  10\n\njulia> A .* B   # elementwise multiply\n2\u00d72 Array{Int64,2}:\n 3   0\n 0  24\n\njulia> A * B   # matrix multiply\n2\u00d72 Array{Int64,2}:\n 3  12\n 9  24\n```\n\n[^matmul]: `*` \u304c\u901a\u5e38\u306e\u884c\u5217\u7a4d\uff01 NumPy\u306e `A.dot(B)` \u3068\u6bd4\u3079\u3066\u8a18\u8ff0\u91cf\u3082\u5c11\u306a\u3044\u3057\u76f4\u89b3\u7684\uff01\n\n---\n\n### \u884c\u5217\u3068\u30d9\u30af\u30c8\u30eb\u306e\u6f14\u7b97\n\n\u30d9\u30af\u30c8\u30eb\uff081\u6b21\u5143\u914d\u5217\uff09\u306f\u7e26\u30d9\u30af\u30c8\u30eb\u306e\u6271\u3044\u3002\n\n```jlcon\njulia> A = [1 2; 3 4]; x = [1, 2];\n\njulia> A * x  # 2x2\u884c\u5217\u30682\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u7a4d\u21922\u6b21\u5143\u30d9\u30af\u30c8\u30eb\n[5,10]\n\njulia> A .+ x   # broadcast\n2\u00d72 Array{Int64,2}:\n 2  3\n 5  6\n\njulia> A .+ x'  # broadcast2\uff08\u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e\u306f1xN\u884c\u5217\uff08\u884c\u30d9\u30af\u30c8\u30eb\uff09\uff09\n2\u00d72 Array{Int64,2}:\n 2  4\n 4  6\n```\n\n---\n\n# Julia \u3067<br>\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\n\n---\n\n## \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\n\n\u306e\u7d39\u4ecb\u3092\u517c\u306d\u305f\u95a2\u6570\u5b9a\u7fa9[^fndef][^return]\u4f8b\u3002\n\n```jl\nfunction sigmoid(x)\n    1.0 ./ (1.0 .+ exp(-x))\nend\n```\n\n\u203b\u2191`x`\u304c\u30b9\u30ab\u30e9\u30fc\u3067\u3082\u30d9\u30af\u30c8\u30eb\u3067\u3082\uff08\u884c\u5217\u3067\u3082\uff09\u305d\u306e\u307e\u307e\u52d5\u4f5c\u3059\u308b\u3002\n\n[^fndef]: \u30ef\u30f3\u30e9\u30a4\u30f3\u3067 `sigmoid(x) = 1.0 ./ (1.0 .+ exp(-x))` \u3068\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3002\n[^return]: Ruby \u306a\u3069\u3068\u540c\u69d8\u3067\u3001\u6700\u5f8c\u306b\u5b9f\u884c\u3057\u305f\u6587\uff08\u5f0f\uff09\u306e\u5024\u304c\u623b\u308a\u5024\uff08`return` \u7701\u7565\u53ef\u80fd\uff09\u3002\u660e\u793a\u7684\u306b `return \uff5e` \u3092\u66f8\u304f\u3053\u3068\u3082\u53ef\u80fd\u3002\n\n---\n\n### Gadfly[^gadfly] \u3092\u4f7f\u3063\u3066\u30d7\u30ed\u30c3\u30c8\n\n```jl\n# Pkg.add(\"Gadfly\")\nusing Gadfly\n\nx = linspace(-5.0, 5.0)\ny = sigmoid(x)\nplot(x=x, y=y, Geom.line)\n```\n\n[^gadfly]: http://gadflyjl.org/ \u3002Julia \u5b9a\u756a\u306e\u30d7\u30ed\u30c3\u30c8\u30e9\u30a4\u30d6\u30e9\u30ea\u3002\n\n---\n\n![Chapter3_sigmoidplot_800x480.png](https://qiita-image-store.s3.amazonaws.com/0/30400/5b4c7977-a0de-225e-9c3c-3465a722ee11.png)\n\n---\n\n## 3\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\n\n```jl\nfunction nn3lp(x)\n    W1 = [0.1 0.2; 0.3 0.4; 0.5 0.6];\n    b1 = [0.1, 0.2, 0.3];\n    W2 = [0.1 0.2 0.3; 0.4 0.5 0.6];\n    b2 = [0.1, 0.2];\n    W3 = [0.1 0.2; 0.3 0.4];\n    b3 = [0.1, 0.2];\n\n    a1 = W1 * x .+ b1\n    z1 = sigmoid(a1)\n    a2 = W2 * z1 .+ b2\n    z2 = sigmoid(a2)\n    a3 = W3 * z2 .+ b3\n    identity(a3)\nend\n```\n\n---\n\n```jlcon\njulia> x = [1.0, 0.5];\n\njulia> y = nn3lp(x)\n2-element Array{Float64,1}:\n 0.316827\n 0.696279\n```\n\n---\n\n### \u7c21\u5358\u3067\u3059\u306d\uff01\n\n---\n\n## \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\n\n\u306e\u7d39\u4ecb\u3092\u517c\u306d\u305f\u5c11\u3057\u8907\u96d1\u306a\u95a2\u6570\u5b9a\u7fa9[^type_annotation][^typeparam][^multidispatch]\u4f8b\u3002\n\n```jl\nfunction softmax{T}(a::AbstractVector{T})\n    c = maximum(a)  # \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\n    exp_a = exp(a .- c)\n    exp_a ./ sum(exp_a)\nend\n\nfunction softmax{T}(a::AbstractMatrix{T})\n    # \u884c\u5217\u306e\u5834\u5408\uff1a\u5404\u5217\u30d9\u30af\u30c8\u30eb\u306b`softmax`\u3092\u9069\u7528\n    mapslices(softmax, a, 1)\nend\n```\n\n[^type_annotation]: \u95a2\u6570\u5f15\u6570\u306b\u3064\u3044\u3066\u3044\u308b `::Xxx` \u306f\u578b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3002\u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\u306b\u578b\u304c\u5408\u308f\u306a\u3051\u308c\u3070\u30a8\u30e9\u30fc\u306b\u306a\u308b\uff08\u52d5\u7684\u578b\u30c1\u30a7\u30c3\u30af\uff09\u3002\n[^typeparam]: \u95a2\u6570\u540d\u3068\u5f15\u6570\u30ea\u30b9\u30c8\u306e\u9593\u306e `{T}` \u306f\u3001\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3002\u5f15\u6570\u306e\u578b `AbstractVector`\u3068\u304b`AbstractMatrix` \u304c\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u53d7\u3051\u53d6\u308b\u304c\u305d\u308c\u304c\u672a\u5b9a\u306e\u6642\u306e\u8868\u8a18\u3002\u3053\u3053\u306b `{T<:AbstractFloat}` \u3068\u66f8\u304f\u3053\u3068\u3067\u300c`T` \u306f `AbstractFloat` \u306e sub type\u300d\u3068\u3044\u3046\u6307\u5b9a\uff08\u578b\u5883\u754c\u306e\u6307\u5b9a\uff09\u3082\u53ef\u80fd\u3002\n[^multidispatch]: \u540c\u3058\u95a2\u6570\u540d\u3067\u7570\u306a\u308b\u5f15\u6570\uff08\u578b\u304a\u3088\u3073\u500b\u6570\uff09\u306e\u95a2\u6570\u3092\u591a\u91cd\u5b9a\u7fa9\u3067\u304d\u308b\u3002\u5b9f\u884c\u6642\uff08\u95a2\u6570\u547c\u3073\u51fa\u3057\u6642\uff09\u306b\u9069\u5207\u306a\u5f15\u6570\u3092\u6301\u3064\u95a2\u6570\u304c\u5b9f\u884c\u3055\u308c\u308b\uff08**\u591a\u91cd\u30c7\u30a3\u30b9\u30d1\u30c3\u30c1**\u3068\u8a00\u3044\u307e\u3059\uff09\u3002\u307e\u305f\u3053\u306e\u4ed5\u7d44\u307f\u306e\u304a\u304b\u3052\u3067`if`\u6587\u304c\u6e1b\u3089\u305b\u308b\uff01\n\n---\n\n```jlcon\njulia> a = [0.3, 2.9, 4.0];\n\njulia> softmax(a)\n[0.0182113, 0.245192, 0.736597]\n\njulia> A = [0.3 1001.0; 2.9 1000.0; 4.0 999.0];\n\njulia> softmax(A)\n3\u00d72 Array{Float64,2}:\n 0.0182113  0.665241\n 0.245192   0.244728\n 0.736597   0.0900306\n```\n\n---\n\n## MNIST\n\n\u624b\u66f8\u304d\u6570\u5b57\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08[MNIST](http://yann.lecun.com/exdb/mnist/)\uff09\u3002\n\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b8[^mnistjl]\u3067\u7c21\u5358\u306b\u5229\u7528\u53ef\u80fd\u3002\n\n```jlcon\njulia> # Pkg.add(\"MNIST\")\n\njulia> using MNIST\n\njulia> x_train, t_train = traindata();\n\njulia> x_test, t_test = testdata();\n\njulia>size(x_train)\n(784,60000)\n```\n\n[^mnistjl]: http://github.com/johnmyleswhite/MNIST.jl\n\n---\n\n### \u753b\u50cf\u78ba\u8a8d\n\n\u753b\u50cf\u306e\u8868\u793a\u306b\u306f\u3001\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b8 Images.jl[^imagesjl] \u3092\u5229\u7528\u3002\n\n```jlcon\njulia> # Pkg.add(\"Images\")\n\njulia> using Images\n\njulia> grayim(reshape(collect(UInt8, x_train[:, 1]), 28,28)')\n```\n\n[^imagesjl]: http://github.com/timholy/Images.jl\n\n---\n\n![Chapter3_grayim_400x240.png](https://qiita-image-store.s3.amazonaws.com/0/30400/69052ce7-c2fc-73ba-f883-6ba63230a416.png)\n\n---\n\n## \u8a13\u7df4\u6e08\u30e2\u30c7\u30eb\u8aad\u8fbc\n\n\u8ffd\u52a0\u30d1\u30c3\u30b1\u30fc\u30b8 JLD[^jld] \u3067\u5909\u6570\u306e\u8aad\u8fbc\uff08\u304a\u3088\u3073\u66f8\u8fbc\uff09\u304c\u53ef\u80fd[^aboutsavedmodel][^aboutdict]\u3002\n\n```jlcon\njulia> # Pkg.add(\"JLD\")\n\njulia> using JLD\n\njulia> network = load(\"/path/to/sample_network.jld\")\nDict{String,Any} with 6 entries:\n  \"W2\" => Float32[-0.10694 0.299116 \u2026 0.100016 -0.0222066; 0.0159125 -0.0332223\u2026\n  \"W3\" => Float32[-0.421736 -0.524321 \u2026 -0.544508 1.07228; 0.689445 -0.143625 \u2026\u2026\n  \"b3\" => Float32[-0.0602398,0.00932628,-0.0135995,0.0216713,0.0107372,0.066197\u2026\n  \"W1\" => Float32[-2.90686f-5 -1.88106f-5 \u2026 -0.000386883 1.09472f-6; -3.09976f-\u2026\n  \"b2\" => Float32[-0.0147111,-0.0721513,-0.00155692,0.121997,0.116033,-0.007549\u2026\n  \"b1\" => Float32[-0.0675032,0.0695926,-0.0273047,0.0225609,-0.220015,-0.220388\u2026\n```\n\n[^jld]: http://github.com/JuliaIO/JLD.jl\n[^aboutsavedmodel]: \u3053\u3053\u3067\u30ed\u30fc\u30c9\u3057\u3066\u3044\u308b\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306f\u3001[\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning](https://www.oreilly.co.jp/books/9784873117584/) \u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf sample_weight.pkl \u3092\u8aad\u307f\u8fbc\u3093\u3067\u5909\u5f62\u3057\u305f\u4e0a\u3067 .jld \u5f62\u5f0f\u3067\u4fdd\u5b58\u3057\u305f\u3082\u306e\u3067\u3059\u3002\n[^aboutdict]: `Dict` \u306f Julia \u306e\u8f9e\u66f8\u578b\u3002`Dict{String,Any}` \u306f\u3001\u300c\u30ad\u30fc\u304c\u6587\u5b57\u5217\uff08`String`\uff09\u3001\u5024\u306f\u4f55\u3067\u3082\u3042\u308a\uff08`Any`\uff09\u306e\u8f9e\u66f8\u300d\u3068\u3044\u3046\u610f\u5473\u3002\n\n---\n\n## MNIST \u306e\u4e88\u6e2c\n\n```jl\nfunction predict(network, x)\n    W1 = network[\"W1\"]\n    b1 = network[\"b1\"]\n    W2 = network[\"W2\"]\n    b2 = network[\"b2\"]\n    W3 = network[\"W3\"]\n    b3 = network[\"b3\"]\n\n    a1 = W1 * x .+ b1\n    z1 = sigmoid(a1)\n    a2 = W2 * z1 .+ b2\n    z2 = sigmoid(a2)\n    a3 = W3 * z2 .+ b3\n    softmax(a3)\nend\n```\n\n---\n\n```jlcon\njulia> predict(network, x_train[:, 1])\n10-element Array{Float64,1}:\n 0.0106677  \n 0.000158301\n 0.000430344\n 0.215528   \n 5.69069e-6 \n 0.7676     \n 3.01649e-5 \n 0.00311578 \n 0.00166548 \n 0.000798537\n\njulia> indmax(predict(network, x_train[:, 1])) - 1\n5\n\njulia> t_train[1]\n5.0   # \u5408\u3063\u3066\u307e\u3059\u306d\uff01\n```\n\n---\n\n### \u6c4e\u5316\u7cbe\u5ea6\uff08\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6b63\u89e3\u7387\uff09\n\n```jlcon\njulia> y = predict(network, x_test)\n10\u00d710000 Array{Float64,2}:\n 8.44125e-5   0.00483633  1.03583e-7   \u2026  0.000624385  0.000428828\n 2.63506e-6   0.00110459  0.988973        0.000767557  2.0043e-6  \n 0.000715494  0.944252    0.00428949      0.000124992  0.00254057 \n 0.00125863   0.0143091   0.00178321      0.000642085  2.01689e-6 \n 1.1728e-6    5.69896e-7  0.000131734     0.00126703   0.000559177\n 4.49908e-5   0.00667604  0.000759407  \u2026  0.907499     0.00031262 \n 1.62693e-8   0.0275334   0.00046891      0.00274028   0.996148   \n 0.997065     1.27084e-6  0.00226997      3.81287e-5   4.34994e-7 \n 9.37447e-6   0.00128642  0.00123787      0.0862065    6.37568e-6 \n 0.000818312  4.78647e-8  8.67147e-5      8.9935e-5    3.77514e-7\n\njulia> p = reshape(mapslices(indmax, y, 1), (length(t_test),)) .- 1;\n\njulia> mean(p .== t_test)\n0.9352  # 93.52%\uff01\n```\n\n---\n\n### \u7c21\u5358\u3067\u3059\u306d\uff01\n\n---\n\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5b66\u7fd2\n\n---\n\n## One-hot Vector\n\n\u30e9\u30d9\u30eb\u304c\u96e2\u6563\u5024\u306e\u5834\u5408\u306b\u3001\u305d\u306e\u5024\u3092 One-hot Vector \u306b\u5909\u63db\u3002[^colcoltype]\n\n```jl\nfunction onehot{T}(::Type{T}, t::AbstractVector, l::AbstractVector)\n    r = zeros(T, length(l), length(t))\n    for i = 1:length(t)\n        r[findfirst(l, t[i]), i] = 1\n    end\n    r\nend\n@inline onehot(t, l) = onehot(Int, t, l)\n```\n\n[^colcoltype]: \u7b2c1\u5f15\u6570\u306e `::T` \u3068\u3044\u3046\u66f8\u5f0f\u306f\u3001\u6700\u521d\u306e\u5f15\u6570\u304c `T` \u578b\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3059\u308b\uff08\u305d\u306e\u5024\u306f\u554f\u308f\u306a\u3044\uff08\u4e0d\u4f7f\u7528\uff09\uff09\u3002\u307e\u305f `::Type{T}` \u3060\u3068\u3001\u305d\u3053\u306b\u306f \u578b\u305d\u306e\u3082\u306e\uff08`Int`\u3068\u304b`Float64`\u3068\u304b\uff09 \u304c\u6765\u308b\u3053\u3068\u3092\u8868\u3059\u3002\n\n---\n\n```jlcon\njulia> t_train[1:3]\n[5.0, 0.0, 4.0]\n\njulia> onehot(Float32, t_train[1:3], 0:9)\n10\u00d73 Array{Float32,2}:\n 0.0  1.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  1.0\n 1.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n```\n\n---\n\n## \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee[^vecdot]\n\n```jl\nfunction crossentropyerror(y::Vector, t::Vector)\n    \u03b4 = 1e-7  # \u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\n    # -sum(t .* log(y .+ \u03b4))\n    -(t \u22c5 log(y .+ \u03b4))\nend\nfunction crossentropyerror(y::Matrix, t::Matrix)\n    batch_size = size(y, 2)\n    \u03b4 = 1e-7\n    # -sum(t .* log(y .+ \u03b4)) / batch_size\n    -vecdot(t, log(y .+ \u03b4)) / batch_size\nend\n```\n\n+ `t` \u306f One-hot Vector\uff08\u307e\u305f\u306f\u305d\u308c\u3092\u4e26\u3079\u305f\u884c\u5217\uff09\uff08\u4ee5\u4e0b\u540c\u69d8\uff09\n\n[^vecdot]: \u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3057\u3066\u3042\u308b `-sum(t .* log(y .+ \u03b4))` \u306f\u5b9a\u7fa9\u3092\u305d\u306e\u307e\u307e\u66f8\u304d\u4e0b\u3057\u305f\u3082\u306e\u3002`\u22c5` \u307e\u305f\u306f `vecdot()` \u306f\u305d\u308c\u3092\u66f8\u304d\u76f4\u3057\u305f\u3082\u306e\u3067\u3001\uff08\u30d9\u30af\u30c8\u30eb\uff081\u6b21\u5143\u914d\u5217\uff09\u306b\u3057\u305f\u4e0a\u3067\uff09\u5185\u7a4d\u3092\u53d6\u3063\u305f\u65b9\u304c\u7701\u30e1\u30e2\u30ea\u304b\u3064\u9ad8\u901f\uff01\n\n---\n\n## \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\n\n\u3092\u610f\u8b58\u3057\u305f\u5404\u5c64\u306e\u8a2d\u8a08[^abstracttype]\u3002\n\n```jl\nabstract AbstractLayer\n```\n\n[^abstracttype]: \u30ad\u30fc\u30ef\u30fc\u30c9 `abstract` \u3067\u59cb\u307e\u308b\u306e\u306f\u62bd\u8c61\u578b\u3002\u30e1\u30f3\u30d0\u30fc\u3092\u6301\u3064\u3053\u3068\u306f\u3067\u304d\u306a\u3044\uff08\u578b\u306e\u968e\u5c64\u69cb\u9020\uff08\u30c4\u30ea\u30fc\u69cb\u9020\uff09\u69cb\u7bc9\u306e\u305f\u3081\u306e\u3082\u306e\uff09\n\n---\n\n### \u5168\u7d50\u5408\u5c64\uff08Affine Layer\uff09[^typedef][^innerconstructor]\n\n```jl\ntype AffineLayer{T} <: AbstractLayer\n    W::AbstractMatrix{T}\n    b::AbstractVector{T}\n    x::AbstractArray{T}\n    dW::AbstractMatrix{T}\n    db::AbstractVector{T}\n    function (::Type{AffineLayer}){T}(W::AbstractMatrix{T}, b::AbstractVector{T})\n        lyr = new{T}()\n        lyr.W = W\n        lyr.b = b\n        lyr\n    end\nend\n```\n\n[^typedef]: \u30ad\u30fc\u30ef\u30fc\u30c9 `type` \u306f\u3001\u578b\uff08\u5177\u8c61\u578b\uff09\u306e\u5b9a\u7fa9\u3002 `type A <: B` \u3068\u3044\u3046\u66f8\u5f0f\u306f\u300c`B` \u3068\u3044\u3046\u578b\u3092\u7d99\u627f\u3057\u305f `A` \u3068\u3044\u3046\u578b\u3092\u5b9a\u7fa9\u300d\u3068\u3044\u3046\u610f\u5473\u306b\u306a\u308b\u3002`{T}` \u306f\u578b\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u3002\n[^innerconstructor]: `type \uff5e end` \u5185\u306b\u8a18\u8ff0\u3057\u305f\u3001\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\u306f\u3001\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3068\u306a\u308b\u3002\u4e3b\u306b\u300c\u5ba3\u8a00\u3057\u305f\u30e1\u30f3\u30d0\u306e\u3046\u3061\u306e\u4e00\u90e8\u306e\u307f\u3092\u53d7\u3051\u53d6\u3063\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u300d\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002`new()` \u306f\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u5185\u306e\u307f\u3067\u5229\u7528\u3067\u304d\u308b\u7279\u5225\u306a\u95a2\u6570\uff08\u30e1\u30e2\u30ea\u78ba\u4fdd\uff09\u3002\n\n---\n\n#### \u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u306e\u5b9a\u7fa9 [^atinline]\n\n```jl\nfunction forward{T}(lyr::AffineLayer{T}, x::AbstractArray{T})\n    lyr.x = x\n    lyr.W * x .+ lyr.b\nend\n\nfunction backward{T}(lyr::AffineLayer{T}, dout::AbstractArray{T})\n    dx = lyr.W' * dout\n    lyr.dW = dout * lyr.x'\n    lyr.db = _sumvec(dout)\n    dx\nend\n@inline _sumvec{T}(dout::AbstractVector{T}) = dout\n@inline _sumvec{T}(dout::AbstractMatrix{T}) = vec(mapslices(sum, dout, 2))\n@inline _sumvec{T,N}(dout::AbstractArray{T,N}) = vec(mapslices(sum, dout, 2:N))\n```\n\n[^atinline]: `@inline` \u306f\u3001\u300c\u4eca\u304b\u3089\u66f8\u304f\u95a2\u6570\u5b9a\u7fa9\u3092\uff08\u53ef\u80fd\u306a\u3089\u3070\uff09\u30a4\u30f3\u30e9\u30a4\u30f3\u5c55\u958b\u3057\u3066\u306d\u300d\u3068\u3044\u3046\u6307\u5b9a\u3002\u306a\u304a `@\uff5e` \u306f Julia \u306e\u30de\u30af\u30ed\u547c\u3073\u51fa\u3057\u306e\u66f8\u5f0f\uff08\u30a4\u30f3\u30e9\u30a4\u30f3\u6307\u5b9a\u306f\u30de\u30af\u30ed\u3068\u3057\u3066\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\uff09\uff08Java \u306e Annotation \u3067\u3082\u306a\u3044\u3057 Python \u306e Decorator \u3068\u3082\u5225\u7269\uff09\u3002\n\n---\n\n### ReLU\u5c64\n\n```jl\ntype ReluLayer <: AbstractLayer\n    mask::AbstractArray{Bool}\n    ReluLayer() = new()\nend\n\nfunction forward{T}(lyr::ReluLayer, x::AbstractArray{T})\n    mask = lyr.mask = (x .<= 0)\n    out = copy(x)\n    out[mask] = zero(T)\n    out\nend\n\nfunction backward{T}(lyr::ReluLayer, dout::AbstractArray{T})\n    dout[lyr.mask] = zero(T)\n    dout\nend\n```\n\n---\n\n### SoftmaxWithLossLayer\n\n```jl\ntype SoftmaxWithLossLayer{T} <: AbstractLayer\n    loss::T\n    y::AbstractArray{T}\n    t::AbstractArray{T}\n    (::Type{SoftmaxWithLossLayer{T}}){T}() = new{T}()\nend\n\n\n```\n\n---\n\n#### \u9806\u4f1d\u64ad\u30fb\u9006\u4f1d\u64ad\u306e\u5b9a\u7fa9\n\n```jl\nfunction forward{T}(lyr::SoftmaxWithLossLayer{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    lyr.t = t\n    y = lyr.y = softmax(x)\n    lyr.loss = crossentropyerror(y, t)\nend\n\nfunction backward{T}(lyr::SoftmaxWithLossLayer{T}, dout::T=1)\n    dout .* _swlvec(lyr.y, lyr.t)\nend\n@inline _swlvec{T}(y::AbstractArray{T}, t::AbstractVector{T}) = y .- t\n@inline _swlvec{T}(y::AbstractArray{T}, t::AbstractMatrix{T}) = (y .- t) / size(t)[2]\n```\n\n---\n\n## 2\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\uff082LP\uff09\n\n\u7c21\u5358\u306e\u305f\u30812\u5c64\u3067\u4f8b\u793a\u3002[^defaultconstructor]\n\n```jl\ntype TwoLayerNet{T}\n    a1lyr::AffineLayer{T}\n    relu1lyr::ReluLayer\n    a2lyr::AffineLayer{T}\n    softmaxlyr::SoftmaxWithLossLayer{T}\nend\n```\n\n[^defaultconstructor]: \u4eca\u307e\u3067\u306e `type` \u5b9a\u7fa9\u3067\u306f\u4e2d\u306b\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf[^innerconstructor]\uff08\uff1d\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\uff09\u3092\u6307\u5b9a\u3057\u3066\u304d\u305f\u3002\u3053\u3053\u3067\u306f\u305d\u306e\u8a18\u8ff0\u304c\u306a\u3044\u304c\u3053\u306e\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u304c\u6697\u9ed9\u306b\u5b9a\u7fa9\u3055\u308c\u308b\u3002\u5f15\u6570\u306f\u30e1\u30f3\u30d0\u3092\u5ba3\u8a00\u9806\u306b\u3059\u3079\u3066\uff08\u3053\u306e\u5834\u5408 `TwoLayerNet{T}(a1lyr::AffineLayer{T},relu1lyr::ReluLayer,a2lyr::AffineLayer{T},softmaxlyr::SoftmaxWithLossLayer{T})`\uff09\u3002\n\n---\n\n### 2LP \u5916\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf[^outercontructor]\n\n```jl\nfunction (::Type{TwoLayerNet{T}}){T}(input_size::Int, hidden_size::Int, output_size::Int,\n        weight_init_std::Float64=0.01)\n    W1 = weight_init_std .* randn(T, hidden_size, input_size)\n    b1 = zeros(T, hidden_size)\n    W2 = weight_init_std .* randn(T, output_size, hidden_size)\n    b2 = zeros(T, output_size)\n    a1lyr = AffineLayer(W1, b1)\n    relu1lyr = ReluLayer()\n    a2lyr = AffineLayer(W2, b2)\n    softmaxlyr = SoftmaxWithLossLayer{T}()\n    TwoLayerNet(a1lyr, relu1lyr, a2lyr, softmaxlyr)\nend\n```\n\n[^outercontructor]: `type \uff5e end` \u306e\u5b9a\u7fa9\u5916\u3067\u5b9a\u7fa9\u3057\u305f\u3001\u578b\u540d\u3068\u540c\u540d\u306e\u95a2\u6570\u306f\u3001\u5916\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\uff08\u3068\u3057\u3066\u5229\u7528\u3067\u304d\u308b\u95a2\u6570\uff09\u3068\u306a\u308b\u3002\u5f15\u6570\u306b\u5bfe\u3057\u3066\u8907\u96d1\u306a\u51e6\u7406\u3092\u3057\u3066\u304b\u3089\u3001\u5185\u90e8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u542b\u3080\uff09\u306b\u5f15\u304d\u6e21\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3059\u308b\uff08\u3068\u3044\u3046\u7528\u9014\u304c\u4e00\u822c\u7684\uff09\u3002\n\n---\n\n### \u63a8\u6e2c[^predict2lp]\n\n```jl\nfunction predict{T}(net::TwoLayerNet{T}, x::AbstractArray{T})\n    a1 = forward(net.a1lyr, x)\n    z1 = forward(net.relu1lyr, a1)\n    a2 = forward(net.a2lyr, z1)\n    # softmax(a2)\n    a2\nend\n```\n\n[^predict2lp]: \u3053\u3053\u3067\u306f `softmax()` \u304b\u3051\u306a\u3044\u3002\n\n---\n\n### \u640d\u5931\u95a2\u6570\uff08\u76ee\u7684\u95a2\u6570\uff09\n\n```jl\nfunction loss{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    y = predict(net, x)\n    forward(net.softmaxlyr, y, t)\nend\n```\n\n---\n\n### \u6b63\u89e3\u7387\n\n```jl\nfunction accuracy{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    y = vec(mapslices(indmax, predict(net, x), 1))\n    mean(y .== t)\nend\n```\n\n---\n\n### \u52fe\u914d\u8a08\u7b97\uff08\u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\uff09[^immutable][^base_gradient]\n\n```jl\nimmutable TwoLayerNetGrads{T}\n    W1::AbstractMatrix{T}\n    b1::AbstractVector{T}\n    W2::AbstractMatrix{T}\n    b2::AbstractVector{T}\nend\n\nfunction Base.gradient{T}(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T})\n    # forward\n    loss(net, x, t)\n    # backward\n    dout = one(T)\n    dz2 = backward(net.softmaxlyr, dout)\n    da2 = backward(net.a2lyr, dz2)\n    dz1 = backward(net.relu1lyr, da2)\n    da1 = backward(net.a1lyr, dz1)\n    TwoLayerNetGrads(net.a1lyr.dW, net.a1lyr.db, net.a2lyr.dW, net.a2lyr.db)\nend\n```\n\n[^immutable]: `immutable` \u30ad\u30fc\u30ef\u30fc\u30c9\u3067\u59cb\u307e\u308b\u578b\u5b9a\u7fa9\u306f\u300c\u4e0d\u5909\u578b\u300d\u306e\u5b9a\u7fa9\u3002`type` \u3067\u5b9a\u7fa9\u3057\u305f\u578b\u3068\u6bd4\u8f03\u3057\u3066\u3001\u30e1\u30f3\u30d0\u30fc\u306e\u5909\u66f4\u3092\u8a31\u53ef\u3057\u306a\u3044\u3002\n[^base_gradient]: Julia \u6a19\u6e96\u306e `gradient` \u3068\u3044\u3046\u95a2\u6570\u304c\u65e2\u306b\u5b58\u5728\u3059\u308b\u306e\u3067\u3001\u95a2\u6570\u591a\u91cd\u5b9a\u7fa9\u6642\u306b `Base.gradient` \u3068\u3059\u308b\u304b\u3001\u4e8b\u524d\u306b `import Base.gradient` \u3092\u5b9f\u884c\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u6012\u3089\u308c\u308b\u3002\n\n---\n\n#### \u52fe\u914d\u9069\u7528[^bangfunc]\n\n```jl\nfunction applygradient!{T}(net::TwoLayerNet{T}, grads::TwoLayerNetGrads{T}, learning_rate::T)\n    net.a1lyr.W -= learning_rate .* grads.W1\n    net.a1lyr.b -= learning_rate .* grads.b1\n    net.a2lyr.W -= learning_rate .* grads.W2\n    net.a2lyr.b -= learning_rate .* grads.b2\nend\n```\n\n[^bangfunc]: Julia \u3067\u306f\u3001\u5f15\u6570\u306e\u5185\u5bb9\uff08\u30e1\u30f3\u30d0\u30fc\u7b49\uff09\u3092\u5909\u66f4\u3059\u308b\u95a2\u6570\u540d\u306b `!` \u3092\u3064\u3051\u308b\u6163\u7fd2\u304c\u3042\u308b\uff08\u304a\u305d\u3089\u304fRuby\u7531\u6765\uff09\u3002\n\n---\n\n## \u5b66\u7fd2\uff08SGD\uff09\n\n```jl\n# using MNIST\n\n_x_train, _t_train = traindata();\n_x_test, _t_test = testdata();\nx_train = collect(Float32, _x_train) ./ 255   # \u578b\u5909\u63db+\u6b63\u898f\u5316\nt_train = onehot(Float32, _t_train, 0:9)      # One-hot Vector \u5316\nx_test = collect(Float32, _x_test) ./ 255     # \u578b\u5909\u63db+\u6b63\u898f\u5316\nt_test = onehot(Float32, _t_test, 0:9)        # One-hot Vector \u5316\n\niters_num = 10000;\ntrain_size = size(x_train, 2); # => 60000\nbatch_size = 100;\nlearning_rate = Float32(0.1);\n```\n\n---\n\n```jl\ntrain_loss_list = Float32[];\ntrain_acc_list = Float32[];\ntest_acc_list = Float32[];\n\niter_per_epoch = max(train_size \u00f7 batch_size, 1)  # => 600\n\nnetwork = TwoLayerNet{Float32}(784, 50, 10);\n```\n\n---\n\n```jl\nfor i = 1:iters_num\n    batch_mask = rand(1:train_size, batch_size)\n    x_batch = x_train[:, batch_mask]\n    t_batch = t_train[:, batch_mask]\n    \n    # \u8aa4\u5dee\u9006\u4f1d\u64ad\u6cd5\u306b\u3088\u3063\u3066\u52fe\u914d\u3092\u6c42\u3081\u308b\n    grads = gradient(network, x_batch, t_batch)\n    \n    # \u66f4\u65b0\n    applygradient!(network, grads, learning_rate)\n    \n    _loss = loss(network, x_batch, t_batch)\n    push!(train_loss_list, _loss)\n\n    if i % iter_per_epoch == 1\n        train_acc = accuracy(network, x_train, t_train)\n        test_acc = accuracy(network, x_test, t_test)\n        push!(train_acc_list, train_acc)\n        push!(test_acc_list, test_acc)\n        println(\"$(i-1): train_acc=$(train_acc) / test_acc=$(test_acc)\")\n    end\nend\n```\n\n---\n\n![Chapter5_leaning_800x480.png](https://qiita-image-store.s3.amazonaws.com/0/30400/78cd62be-f36d-fcc4-3de5-b3a689208904.png)\n\n---\n\n### \u7cbe\u5ea6\uff08\u6b63\u89e3\u7387\uff09\u78ba\u8a8d\n\n```jl\nfinal_train_acc = accuracy(network, x_train, t_train)\nfinal_test_acc = accuracy(network, x_test, t_test)\npush!(train_acc_list, final_train_acc)\npush!(test_acc_list, final_test_acc)\nprintln(\"final: train_acc=$(final_train_acc) / test_acc=$(final_test_acc)\")\n# => final: train_acc=0.97685 / test_acc=0.9686\n```\n\n---\n\n![Chapter5_accuracyplot_800x480.png](https://qiita-image-store.s3.amazonaws.com/0/30400/3b1e45f6-f206-9fec-48b7-a49226a2bdb9.png)\n\n---\n\n### \u5b66\u7fd2\u66f2\u7dda\n\n![Chapter5_trainlossplot_800x480.png](https://qiita-image-store.s3.amazonaws.com/0/30400/8ae36aae-3b4e-838c-5239-5de59ad98708.png)\n\n---\n\n### \u7c21\u5358\u3067\u3059\u306d\u3063\uff01\n\n---\n\n## \u767a\u5c55\n\n+ \u6700\u9069\u5316\u624b\u6cd5\uff08SGD \u306e\u4ee3\u308f\u308a\u306b\uff09\n    + Momentum\n    + AdaGrad\n    + Adam\n+ Batch Normalization\n+ \u6b63\u5247\u5316\n    + Weight Decay\n    + Dropout\n+ CNN\n\n---\n\n# \u53c2\u8003\n\n+ [Julia Documentation (v0.5)](http://docs.julialang.org/en/release-0.5/ \"Julia Documentation &mdash; Julia Language 0.5.1-pre documentation\")\n+ [\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning](https://www.oreilly.co.jp/books/9784873117584/ \"O'Reilly Japan - \u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep Learning\")\n"}