{"context": " More than 1 year has passed since last update.\n\n\u74b0\u5883\n\nMac OS X Version 10.9.5\nScala 2.10.4\nSpark 1.1.0\nSpark SQL 1.1.0\nsbt 0.13.1\n\n\n\u6e96\u5099\n/root/to/project/path\n   |-- build.sbt\n   |-- src\n   |    |-- main\n   |    |    |-- scala\n   |    |    |    |-- Stoppable.scala\n   |    |-- test\n   |    |    |-- resources\n   |    |    |    |-- people.txt\n   |    |    |-- scala\n   |    |    |    |-- SparkSQLFromFileSpec.scala\n\n\nbuild.sbt\nname := \"Spark SQL From File examples\"\n\nversion := \"0.0.1-SNAPSHOT\"\n\nscalaVersion := \"2.10.4\"\n\nscalacOptions ++= Seq(\"-Xlint\", \"-deprecation\", \"-unchecked\", \"-feature\", \"-Xelide-below\", \"ALL\")\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.1.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"1.1.0\",\n  \"org.specs2\" %% \"specs2\" % \"2.4.1\"\n)\n\n\n\nsrc/test/resources/people.txt\nMichael, 29\nAndy, 30\nJustin, 19\n\n\n\n\u5b9f\u88c5\n\nsrc/test/scala/SparkSQLFromFileSpec.scala\nimport commons._\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.sql._\n\nimport org.specs2._\n\n// you can use custom classes that implement the Product interface.\ncase class Person(name: String, age: Int)\n\nclass SparkSQLFromFileSpec extends Specification with Stoppable { def is = s2\"\"\"\n\n  Spark SQL From File\n\n  FromFile\n    read from file                          $readFromFile\n  \"\"\"\n\n  var retReadFromFile: Array[String] = _ \n  using(new SparkContext(\"local[1]\", \"SparkSQLFromFileSpec\", System.getenv(\"SPARK_HOME\"))) { sc =>\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.createSchemaRDD\n\n    // Create an RDD of Person objects and register it as a table.\n    val people = sc.textFile(\"src/test/resources/people.txt\").map(_.split(\",\")).map(p => Person(p(0), p(1).trim.toInt))\n    people.registerTempTable(\"people\")\n\n    // SQL statements can be run by using the sql methods provided by sqlContext.\n    val teenagers = sqlContext.sql(\"SELECT name, age FROM people WHERE age >= 13 AND age <= 19\")\n\n    // The results of SQL queries are SchemaRDDs and support all the normal RDD operations.\n    // The columns of a row in the result can be accessed by ordinal.\n    retReadFromFile = teenagers.map(t => \"%s:%d\".format(t(0), t(1))).collect  \n  }\n  def readFromFile = retReadFromFile must_== Array(\"Justin:19\")\n}\n\n\n\nsrc/main/scala/Stoppable.scala\npackage commons                                                                                                                                                                   \n\nimport scala.language.reflectiveCalls\n\ntrait Stoppable {\n  type T = { def stop(): Unit }\n  def using[A <: T, B](resource: A)(f: A => B) = try {\n    f(resource)\n  } finally {\n    resource.stop()\n  }\n}\n\n\n\n\u5b9f\u884c\n$ sbt '~test-only SparkSQLFromFileSpec'\n\n\n\u53c2\u8003\nhttp://spark.apache.org/docs/1.1.0/sql-programming-guide.html#inferring-the-schema-using-reflection\n\n## \u74b0\u5883\n* Mac OS X Version 10.9.5\n* Scala 2.10.4\n* Spark 1.1.0\n* Spark SQL 1.1.0\n* sbt 0.13.1\n\n## \u6e96\u5099\n\n```tree\n/root/to/project/path\n   |-- build.sbt\n   |-- src\n   |    |-- main\n   |    |    |-- scala\n   |    |    |    |-- Stoppable.scala\n   |    |-- test\n   |    |    |-- resources\n   |    |    |    |-- people.txt\n   |    |    |-- scala\n   |    |    |    |-- SparkSQLFromFileSpec.scala\n```\n\n```build.sbt\nname := \"Spark SQL From File examples\"\n\nversion := \"0.0.1-SNAPSHOT\"\n\nscalaVersion := \"2.10.4\"\n\nscalacOptions ++= Seq(\"-Xlint\", \"-deprecation\", \"-unchecked\", \"-feature\", \"-Xelide-below\", \"ALL\")\n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % \"1.1.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"1.1.0\",\n  \"org.specs2\" %% \"specs2\" % \"2.4.1\"\n)\n```\n\n```src/test/resources/people.txt\nMichael, 29\nAndy, 30\nJustin, 19\n```\n\n## \u5b9f\u88c5\n```src/test/scala/SparkSQLFromFileSpec.scala\nimport commons._\n                                                                                                                                                                                  \nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.sql._\n\nimport org.specs2._\n\n// you can use custom classes that implement the Product interface.\ncase class Person(name: String, age: Int)\n\nclass SparkSQLFromFileSpec extends Specification with Stoppable { def is = s2\"\"\"\n\n  Spark SQL From File\n\n  FromFile\n    read from file                          $readFromFile\n  \"\"\"\n\n  var retReadFromFile: Array[String] = _ \n  using(new SparkContext(\"local[1]\", \"SparkSQLFromFileSpec\", System.getenv(\"SPARK_HOME\"))) { sc =>\n    val sqlContext = new SQLContext(sc)\n    import sqlContext.createSchemaRDD\n\n    // Create an RDD of Person objects and register it as a table.\n    val people = sc.textFile(\"src/test/resources/people.txt\").map(_.split(\",\")).map(p => Person(p(0), p(1).trim.toInt))\n    people.registerTempTable(\"people\")\n\n    // SQL statements can be run by using the sql methods provided by sqlContext.\n    val teenagers = sqlContext.sql(\"SELECT name, age FROM people WHERE age >= 13 AND age <= 19\")\n\n    // The results of SQL queries are SchemaRDDs and support all the normal RDD operations.\n    // The columns of a row in the result can be accessed by ordinal.\n    retReadFromFile = teenagers.map(t => \"%s:%d\".format(t(0), t(1))).collect  \n  }\n  def readFromFile = retReadFromFile must_== Array(\"Justin:19\")\n}\n```\n\n```src/main/scala/Stoppable.scala\npackage commons                                                                                                                                                                   \n\nimport scala.language.reflectiveCalls\n\ntrait Stoppable {\n  type T = { def stop(): Unit }\n  def using[A <: T, B](resource: A)(f: A => B) = try {\n    f(resource)\n  } finally {\n    resource.stop()\n  }\n}\n```\n\n## \u5b9f\u884c\n\n```shell\n$ sbt '~test-only SparkSQLFromFileSpec'\n```\n\n## \u53c2\u8003\n\nhttp://spark.apache.org/docs/1.1.0/sql-programming-guide.html#inferring-the-schema-using-reflection\n", "tags": ["Scala2.10.4", "Spark1.1.10", "sparksql1.1.0"]}