{"tags": ["scikit-learn", "numpy"], "context": "scikit-learn\u306f\u3088\u304f\u3067\u304d\u305f\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u77e5\u3089\u306a\u304f\u3066\u3082\u4f7f\u3046\u3053\u3068\u306f\u3067\u304d\u308b\u306e\u3060\u304c\u3001\u5b9f\u969b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u304c\u3042\u308b\u3002\u305f\u3068\u3048\u3070\u3001\u81ea\u5206\u304c\u60f3\u5b9a\u3057\u3066\u3044\u308b\u6570\u5f0f\u3068scikit-learn\u304c\u5b9f\u969b\u306b\u3084\u3063\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u9055\u3063\u3066\u3044\u308b\u3068\u56f0\u308b\u3002\u672c\u8a18\u4e8b\u3067\u306f\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u5b9f\u88c5\u3092\u8ffd\u3044\u304b\u3051\u308b\u65b9\u6cd5\u3092\u8ff0\u3079\u308b\u3002\n\n\u3084\u3063\u3066\u307f\u305f\n\u300cNMF\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8abf\u3079\u305f\u3044\u300d\u3068\u3044\u3046\u304a\u984c\u3092\u8003\u3048\u308b\u3002\n\u307e\u305a\u306f\u3001NMF\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u30b0\u30b0\u3063\u305f\u7d50\u679c\u3092\u4ee5\u4e0b\u306b\u793a\u3059\u3002\nimport numpy as np\nX = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n\nfrom sklearn.decomposition import NMF\nmodel = NMF(n_components=2, init='random', random_state=0)\nmodel.fit(X)\n\n\u300cfit\u306f\u5b9f\u969b\u306e\u3068\u3053\u308d\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u306e\uff1f\u300d\u3092\u8abf\u3079\u308b\u306e\u304c\u672c\u8a18\u4e8b\u306e\u76ee\u7684\u3060\u3002\niPython\u3092\u958b\u3044\u3066\u3001??\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046\u3002\n$ ipython\n(\u7565)\nIn [9]: model.fit??\nSignature: model.fit(X, y=None, **params)\nSource:\n    def fit(self, X, y=None, **params):\n        \"\"\"Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        Attributes\n        ----------\n        components_ : array-like, shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations for the transform.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.fit_transform(X, **params)\n        return self\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      method\n\n\u306a\u308b\u307b\u3069\u3001fit_transform\u306b\u6e21\u3057\u3066\u3044\u308b\u306e\u306d\u3002\nIn [12]: model.fit_transform??\nSignature: model.fit_transform(X, y=None, W=None, H=None)\nSource:\n    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        W : array-like, shape (n_samples, n_components)\n            If init='custom', it is used as initial guess for the solution.\n\n        H : array-like, shape (n_components, n_features)\n            If init='custom', it is used as initial guess for the solution.\n\n        Attributes\n        ----------\n        components_ : array-like, shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations for the transform.\n\n        Returns\n        -------\n        W: array, shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = check_array(X, accept_sparse=('csr', 'csc'))\n\n        W, H, n_iter_ = non_negative_factorization(\n            X=X, W=W, H=H, n_components=self.n_components,\n            init=self.init, update_H=True, solver=self.solver,\n            tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,\n            l1_ratio=self.l1_ratio, regularization='both',\n            random_state=self.random_state, verbose=self.verbose,\n            shuffle=self.shuffle,\n            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,\n            beta=self.beta, eta=self.eta)\n\n        if self.solver == 'pg':\n            self.comp_sparseness_ = _sparseness(H.ravel())\n            self.data_sparseness_ = _sparseness(W.ravel())\n\n        self.reconstruction_err_ = _safe_compute_error(X, W, H)\n\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter_\n\n        return W\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      method\n\n\u5b9f\u8cea\u306e\u5b66\u7fd2\u306f\u3001non_negative_factorization\u304c\u3084\u3063\u3066\u3044\u308b\u3089\u3057\u3044\u3002\n\u3055\u3089\u306b\u6398\u308a\u9032\u3081\u308b\u3002\u3068\u3053\u308d\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002\nIn [13]: model.non_negative_factorization??\nObject `model.non_negative_factorization` not found.\n\n\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u3060\u308d\u3046\u304b\uff1f\n\u6b21\u306e1\u884c\u306b\u6ce8\u76ee\u3057\u3088\u3046\u3002\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\n\nsklearn/decomposition/nmf.py\u306b\u3042\u308a\u307e\u3059\u3088\u3001\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\u3057\u305f\u304c\u3063\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3084\u308c\u3070\u3001\u3044\u307e\u304f\u3044\u304f\u3002\nIn [14]: import sklearn.decomposition\nIn [15]: sklearn.decomposition.nmf.non_negative_factorization??\nSignature: sklearn.decomposition.nmf.non_negative_factorization(X, W=None, H=None, n_components=None, init='random', update_H=True, sol\nSource:\ndef non_negative_factorization(X, W=None, H=None, n_components=None,\n                               init='random', update_H=True, solver='cd',\n                               tol=1e-4, max_iter=200, alpha=0., l1_ratio=0.,\n                               regularization=None, random_state=None,\n                               verbose=0, shuffle=False, nls_max_iter=2000,\n                               sparseness=None, beta=1, eta=0.1):\n    \"\"\"Compute Non-negative Matrix Factorization (NMF)\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is::\n\n        0.5 * ||X - WH||_Fro^2\n        + alpha * l1_ratio * ||vec(W)||_1\n        + alpha * l1_ratio * ||vec(H)||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\n    Where::\n\n        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)\n        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like, shape (n_samples, n_components)\n        If init='custom', it is used as initial guess for the solution.\n\n    H : array-like, shape (n_components, n_features)\n        If init='custom', it is used as initial guess for the solution.\n        If update_H=False, it is used as a constant, to solve for W only.\n\n    n_components : integer\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init :  None | 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom'\n        Method used to initialize the procedure.\n        Default: 'nndsvd' if n_components < n_features, otherwise random.\n        Valid options:\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H\n\n    update_H : boolean, default: True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : 'pg' | 'cd'\n        Numerical solver to use:\n        'pg' is a (deprecated) Projected Gradient solver.\n        'cd' is a Coordinate Descent solver.\n\n    tol : float, default: 1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : integer, default: 200\n        Maximum number of iterations before timing out.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    regularization : 'both' | 'components' | 'transformation' | None\n        Select whether the regularization affects the components (H), the\n        transformation (W), both or none of them.\n\n    random_state : integer seed, RandomState instance, or None (default)\n        Random number generator seed control.\n\n    verbose : integer, default: 0\n        The verbosity level.\n\n    shuffle : boolean, default: False\n        If true, randomize the order of coordinates in the CD solver.\n\n    nls_max_iter : integer, default: 2000\n        Number of iterations in NLS subproblem.\n        Used only in the deprecated 'pg' solver.\n\n    sparseness : 'data' | 'components' | None, default: None\n        Where to enforce sparsity in the model.\n        Used only in the deprecated 'pg' solver.\n\n    beta : double, default: 1\n        Degree of sparseness, if sparseness is not None. Larger values mean\n        more sparseness. Used only in the deprecated 'pg' solver.\n\n    eta : double, default: 0.1\n        Degree of correctness to maintain, if sparsity is not None. Smaller\n        values mean larger error. Used only in the deprecated 'pg' solver.\n\n    Returns\n    -------\n    W : array-like, shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : array-like, shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    C.-J. Lin. Projected gradient methods for non-negative matrix\n    factorization. Neural Computation, 19(2007), 2756-2779.\n    http://www.csie.ntu.edu.tw/~cjlin/nmf/\n\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n    \"\"\"\n\n    X = check_array(X, accept_sparse=('csr', 'csc'))\n    check_non_negative(X, \"NMF (input X)\")\n    _check_string_param(sparseness, solver)\n\n    n_samples, n_features = X.shape\n    if n_components is None:\n        n_components = n_features\n\n    if not isinstance(n_components, INTEGER_TYPES) or n_components <= 0:\n        raise ValueError(\"Number of components must be a positive integer;\"\n                         \" got (n_components=%r)\" % n_components)\n    if not isinstance(max_iter, INTEGER_TYPES) or max_iter < 0:\n        raise ValueError(\"Maximum number of iterations must be a positive integer;\"\n                         \" got (max_iter=%r)\" % max_iter)\n    if not isinstance(tol, numbers.Number) or tol < 0:\n        raise ValueError(\"Tolerance for stopping criteria must be \"\n                         \"positive; got (tol=%r)\" % tol)\n\n    # check W and H, or initialize them\n    if init == 'custom' and update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        _check_init(W, (n_samples, n_components), \"NMF (input W)\")\n    elif not update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        W = np.zeros((n_samples, n_components))\n    else:\n        W, H = _initialize_nmf(X, n_components, init=init,\n                               random_state=random_state)\n\n    if solver == 'pg':\n        warnings.warn(\"'pg' solver will be removed in release 0.19.\"\n                      \" Use 'cd' solver instead.\", DeprecationWarning)\n        if update_H:  # fit_transform\n            W, H, n_iter = _fit_projected_gradient(X, W, H, tol,\n                                                   max_iter,\n                                                   nls_max_iter,\n                                                   alpha, l1_ratio,\n                                                   sparseness,\n                                                   beta, eta)\n        else:  # transform\n            W, H, n_iter = _update_projected_gradient_w(X, W, H,\n                                                        tol, nls_max_iter,\n                                                        alpha, l1_ratio,\n                                                        sparseness, beta,\n                                                        eta)\n    elif solver == 'cd':\n        W, H, n_iter = _fit_coordinate_descent(X, W, H, tol,\n                                               max_iter,\n                                               alpha, l1_ratio,\n                                               regularization,\n                                               update_H=update_H,\n                                               verbose=verbose,\n                                               shuffle=shuffle,\n                                               random_state=random_state)\n    else:\n        raise ValueError(\"Invalid solver parameter '%s'.\" % solver)\n\n    if n_iter == max_iter:\n        warnings.warn(\"Maximum number of iteration %d reached. Increase it to\"\n                      \" improve convergence.\" % max_iter, ConvergenceWarning)\n\n    return W, H, n_iter\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n\n\u3053\u306e\u8abf\u5b50\u3067\u6398\u308a\u9032\u3081\u3088\u3046\nIn [19]: sklearn.decomposition.nmf._update_projected_gradient_w??\nSignature: sklearn.decomposition.nmf._update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio, sparse\nSource:\ndef _update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio,\n                                 sparseness, beta, eta):\n    \"\"\"Helper function for _fit_projected_gradient\"\"\"\n    n_samples, n_features = X.shape\n    n_components_ = H.shape[0]\n\n    if sparseness is None:\n        Wt, gradW, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,\n                                           alpha=alpha, l1_ratio=l1_ratio)\n    elif sparseness == 'data':\n        Wt, gradW, iterW = _nls_subproblem(\n            safe_vstack([X.T, np.zeros((1, n_samples))]),\n            safe_vstack([H.T, np.sqrt(beta) * np.ones((1,\n                         n_components_))]),\n            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)\n    elif sparseness == 'components':\n        Wt, gradW, iterW = _nls_subproblem(\n            safe_vstack([X.T,\n                         np.zeros((n_components_, n_samples))]),\n            safe_vstack([H.T,\n                         np.sqrt(eta) * np.eye(n_components_)]),\n            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)\n\n    return Wt.T, gradW.T, iterW\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n\n_nls_subproblem\u304c\u6c17\u306b\u306a\u308b\nIn [20]: sklearn.decomposition.nmf._nls_subproblem??\nSignature: sklearn.decomposition.nmf._nls_subproblem(V, W, H, tol, max_iter, alpha=0.0, l1_ratio=0.0, sigma=0.01, beta\nSource:\ndef _nls_subproblem(V, W, H, tol, max_iter, alpha=0., l1_ratio=0.,\n                    sigma=0.01, beta=0.1):\n    \"\"\"Non-negative least square solver\n\n    Solves a non-negative least squares subproblem using the projected\n    gradient descent algorithm.\n\n    Parameters\n    ----------\n    V : array-like, shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like, shape (n_samples, n_components)\n        Constant matrix.\n\n    H : array-like, shape (n_components, n_features)\n        Initial guess for the solution.\n\n    tol : float\n        Tolerance of the stopping condition.\n\n    max_iter : int\n        Maximum number of iterations before timing out.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms. Set it to zero to\n        have no regularization.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an L2 penalty.\n        For l1_ratio = 1 it is an L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    sigma : float\n        Constant used in the sufficient decrease condition checked by the line\n        search.  Smaller values lead to a looser sufficient decrease condition,\n        thus reducing the time taken by the line search, but potentially\n        increasing the number of iterations of the projected gradient\n        procedure. 0.01 is a commonly used value in the optimization\n        literature.\n\n    beta : float\n        Factor by which the step size is decreased (resp. increased) until\n        (resp. as long as) the sufficient decrease condition is satisfied.\n        Larger values allow to find a better step size but lead to longer line\n        search. 0.1 is a commonly used value in the optimization literature.\n\n    Returns\n    -------\n    H : array-like, shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    grad : array-like, shape (n_components, n_features)\n        The gradient.\n\n    n_iter : int\n        The number of iterations done by the algorithm.\n\n    References\n    ----------\n    C.-J. Lin. Projected gradient methods for non-negative matrix\n    factorization. Neural Computation, 19(2007), 2756-2779.\n    http://www.csie.ntu.edu.tw/~cjlin/nmf/\n    \"\"\"\n    WtV = safe_sparse_dot(W.T, V)\n    WtW = fast_dot(W.T, W)\n\n    # values justified in the paper (alpha is renamed gamma)\n    gamma = 1\n    for n_iter in range(1, max_iter + 1):\n        grad = np.dot(WtW, H) - WtV\n        if alpha > 0 and l1_ratio == 1.:\n            grad += alpha\n        elif alpha > 0:\n            grad += alpha * (l1_ratio + (1 - l1_ratio) * H)\n\n        # The following multiplication with a boolean array is more than twice\n        # as fast as indexing into grad.\n        if norm(grad * np.logical_or(grad < 0, H > 0)) < tol:\n            break\n\n        Hp = H\n\n        for inner_iter in range(20):\n            # Gradient step.\n            Hn = H - gamma * grad\n            # Projection step.\n            Hn *= Hn > 0\n            d = Hn - H\n            gradd = np.dot(grad.ravel(), d.ravel())\n            dQd = np.dot(np.dot(WtW, d).ravel(), d.ravel())\n            suff_decr = (1 - sigma) * gradd + 0.5 * dQd < 0\n            if inner_iter == 0:\n                decr_gamma = not suff_decr\n\n            if decr_gamma:\n                if suff_decr:\n                    H = Hn\n                    break\n                else:\n                    gamma *= beta\n            elif not suff_decr or (Hp == Hn).all():\n                H = Hp\n                break\n            else:\n                gamma /= beta\n                Hp = Hn\n\n    if n_iter == max_iter:\n        warnings.warn(\"Iteration limit reached in nls subproblem.\")\n\n    return H, grad, n_iter\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n\n\u6700\u7d42\u7684\u306bnumpy\u306e\u30b3\u30fc\u30c9\u3001\n\u3064\u307e\u308a\u5b9f\u969b\u306e\u8a08\u7b97\u3092\u3084\u3063\u3066\u3044\u308b\u90e8\u5206\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u304c\u3067\u304d\u305f\u3002\n\n\u307e\u3068\u3081\niPython\u3067\u3072\u305f\u3059\u3089\u8fbf\u3063\u3066\u3044\u304f\u3068\u3001numpy\u306e\u30b3\u30fc\u30c9\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3002\nscikit-learn\u306f\u3088\u304f\u3067\u304d\u305f\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u77e5\u3089\u306a\u304f\u3066\u3082\u4f7f\u3046\u3053\u3068\u306f\u3067\u304d\u308b\u306e\u3060\u304c\u3001\u5b9f\u969b\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u304c\u3042\u308b\u3002\u305f\u3068\u3048\u3070\u3001\u81ea\u5206\u304c\u60f3\u5b9a\u3057\u3066\u3044\u308b\u6570\u5f0f\u3068scikit-learn\u304c\u5b9f\u969b\u306b\u3084\u3063\u3066\u3044\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u9055\u3063\u3066\u3044\u308b\u3068\u56f0\u308b\u3002\u672c\u8a18\u4e8b\u3067\u306f\u3001\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u5b9f\u88c5\u3092\u8ffd\u3044\u304b\u3051\u308b\u65b9\u6cd5\u3092\u8ff0\u3079\u308b\u3002\n\n## \u3084\u3063\u3066\u307f\u305f\n\u300cNMF\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8abf\u3079\u305f\u3044\u300d\u3068\u3044\u3046\u304a\u984c\u3092\u8003\u3048\u308b\u3002\n\u307e\u305a\u306f\u3001NMF\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u30b0\u30b0\u3063\u305f\u7d50\u679c\u3092\u4ee5\u4e0b\u306b\u793a\u3059\u3002\n\n```python\nimport numpy as np\nX = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n\nfrom sklearn.decomposition import NMF\nmodel = NMF(n_components=2, init='random', random_state=0)\nmodel.fit(X)\n```\n\n\u300cfit\u306f\u5b9f\u969b\u306e\u3068\u3053\u308d\u4f55\u3092\u3084\u3063\u3066\u3044\u308b\u306e\uff1f\u300d\u3092\u8abf\u3079\u308b\u306e\u304c\u672c\u8a18\u4e8b\u306e\u76ee\u7684\u3060\u3002\niPython\u3092\u958b\u3044\u3066\u3001`??`\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046\u3002\n\n```py\n$ ipython\n(\u7565)\nIn [9]: model.fit??\nSignature: model.fit(X, y=None, **params)\nSource:\n    def fit(self, X, y=None, **params):\n        \"\"\"Learn a NMF model for the data X.\n\n        Parameters\n        ----------\n        X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        Attributes\n        ----------\n        components_ : array-like, shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations for the transform.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.fit_transform(X, **params)\n        return self\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      method\n```\n\n\u306a\u308b\u307b\u3069\u3001`fit_transform`\u306b\u6e21\u3057\u3066\u3044\u308b\u306e\u306d\u3002\n\n```py\nIn [12]: model.fit_transform??\nSignature: model.fit_transform(X, y=None, W=None, H=None)\nSource:\n    def fit_transform(self, X, y=None, W=None, H=None):\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n\n        This is more efficient than calling fit followed by transform.\n\n        Parameters\n        ----------\n        X: {array-like, sparse matrix}, shape (n_samples, n_features)\n            Data matrix to be decomposed\n\n        W : array-like, shape (n_samples, n_components)\n            If init='custom', it is used as initial guess for the solution.\n\n        H : array-like, shape (n_components, n_features)\n            If init='custom', it is used as initial guess for the solution.\n\n        Attributes\n        ----------\n        components_ : array-like, shape (n_components, n_features)\n            Factorization matrix, sometimes called 'dictionary'.\n\n        n_iter_ : int\n            Actual number of iterations for the transform.\n\n        Returns\n        -------\n        W: array, shape (n_samples, n_components)\n            Transformed data.\n        \"\"\"\n        X = check_array(X, accept_sparse=('csr', 'csc'))\n\n        W, H, n_iter_ = non_negative_factorization(\n            X=X, W=W, H=H, n_components=self.n_components,\n            init=self.init, update_H=True, solver=self.solver,\n            tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,\n            l1_ratio=self.l1_ratio, regularization='both',\n            random_state=self.random_state, verbose=self.verbose,\n            shuffle=self.shuffle,\n            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,\n            beta=self.beta, eta=self.eta)\n\n        if self.solver == 'pg':\n            self.comp_sparseness_ = _sparseness(H.ravel())\n            self.data_sparseness_ = _sparseness(W.ravel())\n\n        self.reconstruction_err_ = _safe_compute_error(X, W, H)\n\n        self.n_components_ = H.shape[0]\n        self.components_ = H\n        self.n_iter_ = n_iter_\n\n        return W\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      method\n```\n\n\u5b9f\u8cea\u306e\u5b66\u7fd2\u306f\u3001`non_negative_factorization`\u304c\u3084\u3063\u3066\u3044\u308b\u3089\u3057\u3044\u3002\n\u3055\u3089\u306b\u6398\u308a\u9032\u3081\u308b\u3002\u3068\u3053\u308d\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\u3002\n\n```\nIn [13]: model.non_negative_factorization??\nObject `model.non_negative_factorization` not found.\n```\n\n\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u306e\u3060\u308d\u3046\u304b\uff1f\n\u6b21\u306e1\u884c\u306b\u6ce8\u76ee\u3057\u3088\u3046\u3002\n\n```\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\n```\n\n`sklearn/decomposition/nmf.py`\u306b\u3042\u308a\u307e\u3059\u3088\u3001\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\u3057\u305f\u304c\u3063\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3084\u308c\u3070\u3001\u3044\u307e\u304f\u3044\u304f\u3002\n\n```py\nIn [14]: import sklearn.decomposition\nIn [15]: sklearn.decomposition.nmf.non_negative_factorization??\nSignature: sklearn.decomposition.nmf.non_negative_factorization(X, W=None, H=None, n_components=None, init='random', update_H=True, sol\nSource:\ndef non_negative_factorization(X, W=None, H=None, n_components=None,\n                               init='random', update_H=True, solver='cd',\n                               tol=1e-4, max_iter=200, alpha=0., l1_ratio=0.,\n                               regularization=None, random_state=None,\n                               verbose=0, shuffle=False, nls_max_iter=2000,\n                               sparseness=None, beta=1, eta=0.1):\n    \"\"\"Compute Non-negative Matrix Factorization (NMF)\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is::\n\n        0.5 * ||X - WH||_Fro^2\n        + alpha * l1_ratio * ||vec(W)||_1\n        + alpha * l1_ratio * ||vec(H)||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\n    Where::\n\n        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)\n        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like, shape (n_samples, n_components)\n        If init='custom', it is used as initial guess for the solution.\n\n    H : array-like, shape (n_components, n_features)\n        If init='custom', it is used as initial guess for the solution.\n        If update_H=False, it is used as a constant, to solve for W only.\n\n    n_components : integer\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init :  None | 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom'\n        Method used to initialize the procedure.\n        Default: 'nndsvd' if n_components < n_features, otherwise random.\n        Valid options:\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H\n\n    update_H : boolean, default: True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : 'pg' | 'cd'\n        Numerical solver to use:\n        'pg' is a (deprecated) Projected Gradient solver.\n        'cd' is a Coordinate Descent solver.\n\n    tol : float, default: 1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : integer, default: 200\n        Maximum number of iterations before timing out.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    regularization : 'both' | 'components' | 'transformation' | None\n        Select whether the regularization affects the components (H), the\n        transformation (W), both or none of them.\n\n    random_state : integer seed, RandomState instance, or None (default)\n        Random number generator seed control.\n\n    verbose : integer, default: 0\n        The verbosity level.\n\n    shuffle : boolean, default: False\n        If true, randomize the order of coordinates in the CD solver.\n\n    nls_max_iter : integer, default: 2000\n        Number of iterations in NLS subproblem.\n        Used only in the deprecated 'pg' solver.\n\n    sparseness : 'data' | 'components' | None, default: None\n        Where to enforce sparsity in the model.\n        Used only in the deprecated 'pg' solver.\n\n    beta : double, default: 1\n        Degree of sparseness, if sparseness is not None. Larger values mean\n        more sparseness. Used only in the deprecated 'pg' solver.\n\n    eta : double, default: 0.1\n        Degree of correctness to maintain, if sparsity is not None. Smaller\n        values mean larger error. Used only in the deprecated 'pg' solver.\n\n    Returns\n    -------\n    W : array-like, shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : array-like, shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    C.-J. Lin. Projected gradient methods for non-negative matrix\n    factorization. Neural Computation, 19(2007), 2756-2779.\n    http://www.csie.ntu.edu.tw/~cjlin/nmf/\n\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n    \"\"\"\n\n    X = check_array(X, accept_sparse=('csr', 'csc'))\n    check_non_negative(X, \"NMF (input X)\")\n    _check_string_param(sparseness, solver)\n\n    n_samples, n_features = X.shape\n    if n_components is None:\n        n_components = n_features\n\n    if not isinstance(n_components, INTEGER_TYPES) or n_components <= 0:\n        raise ValueError(\"Number of components must be a positive integer;\"\n                         \" got (n_components=%r)\" % n_components)\n    if not isinstance(max_iter, INTEGER_TYPES) or max_iter < 0:\n        raise ValueError(\"Maximum number of iterations must be a positive integer;\"\n                         \" got (max_iter=%r)\" % max_iter)\n    if not isinstance(tol, numbers.Number) or tol < 0:\n        raise ValueError(\"Tolerance for stopping criteria must be \"\n                         \"positive; got (tol=%r)\" % tol)\n\n    # check W and H, or initialize them\n    if init == 'custom' and update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        _check_init(W, (n_samples, n_components), \"NMF (input W)\")\n    elif not update_H:\n        _check_init(H, (n_components, n_features), \"NMF (input H)\")\n        W = np.zeros((n_samples, n_components))\n    else:\n        W, H = _initialize_nmf(X, n_components, init=init,\n                               random_state=random_state)\n\n    if solver == 'pg':\n        warnings.warn(\"'pg' solver will be removed in release 0.19.\"\n                      \" Use 'cd' solver instead.\", DeprecationWarning)\n        if update_H:  # fit_transform\n            W, H, n_iter = _fit_projected_gradient(X, W, H, tol,\n                                                   max_iter,\n                                                   nls_max_iter,\n                                                   alpha, l1_ratio,\n                                                   sparseness,\n                                                   beta, eta)\n        else:  # transform\n            W, H, n_iter = _update_projected_gradient_w(X, W, H,\n                                                        tol, nls_max_iter,\n                                                        alpha, l1_ratio,\n                                                        sparseness, beta,\n                                                        eta)\n    elif solver == 'cd':\n        W, H, n_iter = _fit_coordinate_descent(X, W, H, tol,\n                                               max_iter,\n                                               alpha, l1_ratio,\n                                               regularization,\n                                               update_H=update_H,\n                                               verbose=verbose,\n                                               shuffle=shuffle,\n                                               random_state=random_state)\n    else:\n        raise ValueError(\"Invalid solver parameter '%s'.\" % solver)\n\n    if n_iter == max_iter:\n        warnings.warn(\"Maximum number of iteration %d reached. Increase it to\"\n                      \" improve convergence.\" % max_iter, ConvergenceWarning)\n\n    return W, H, n_iter\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n```\n\n\u3053\u306e\u8abf\u5b50\u3067\u6398\u308a\u9032\u3081\u3088\u3046\n\n```py3\nIn [19]: sklearn.decomposition.nmf._update_projected_gradient_w??\nSignature: sklearn.decomposition.nmf._update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio, sparse\nSource:\ndef _update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio,\n                                 sparseness, beta, eta):\n    \"\"\"Helper function for _fit_projected_gradient\"\"\"\n    n_samples, n_features = X.shape\n    n_components_ = H.shape[0]\n\n    if sparseness is None:\n        Wt, gradW, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,\n                                           alpha=alpha, l1_ratio=l1_ratio)\n    elif sparseness == 'data':\n        Wt, gradW, iterW = _nls_subproblem(\n            safe_vstack([X.T, np.zeros((1, n_samples))]),\n            safe_vstack([H.T, np.sqrt(beta) * np.ones((1,\n                         n_components_))]),\n            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)\n    elif sparseness == 'components':\n        Wt, gradW, iterW = _nls_subproblem(\n            safe_vstack([X.T,\n                         np.zeros((n_components_, n_samples))]),\n            safe_vstack([H.T,\n                         np.sqrt(eta) * np.eye(n_components_)]),\n            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)\n\n    return Wt.T, gradW.T, iterW\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n```\n\n`_nls_subproblem`\u304c\u6c17\u306b\u306a\u308b\n\n```py3\nIn [20]: sklearn.decomposition.nmf._nls_subproblem??\nSignature: sklearn.decomposition.nmf._nls_subproblem(V, W, H, tol, max_iter, alpha=0.0, l1_ratio=0.0, sigma=0.01, beta\nSource:\ndef _nls_subproblem(V, W, H, tol, max_iter, alpha=0., l1_ratio=0.,\n                    sigma=0.01, beta=0.1):\n    \"\"\"Non-negative least square solver\n\n    Solves a non-negative least squares subproblem using the projected\n    gradient descent algorithm.\n\n    Parameters\n    ----------\n    V : array-like, shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like, shape (n_samples, n_components)\n        Constant matrix.\n\n    H : array-like, shape (n_components, n_features)\n        Initial guess for the solution.\n\n    tol : float\n        Tolerance of the stopping condition.\n\n    max_iter : int\n        Maximum number of iterations before timing out.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms. Set it to zero to\n        have no regularization.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an L2 penalty.\n        For l1_ratio = 1 it is an L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    sigma : float\n        Constant used in the sufficient decrease condition checked by the line\n        search.  Smaller values lead to a looser sufficient decrease condition,\n        thus reducing the time taken by the line search, but potentially\n        increasing the number of iterations of the projected gradient\n        procedure. 0.01 is a commonly used value in the optimization\n        literature.\n\n    beta : float\n        Factor by which the step size is decreased (resp. increased) until\n        (resp. as long as) the sufficient decrease condition is satisfied.\n        Larger values allow to find a better step size but lead to longer line\n        search. 0.1 is a commonly used value in the optimization literature.\n\n    Returns\n    -------\n    H : array-like, shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    grad : array-like, shape (n_components, n_features)\n        The gradient.\n\n    n_iter : int\n        The number of iterations done by the algorithm.\n\n    References\n    ----------\n    C.-J. Lin. Projected gradient methods for non-negative matrix\n    factorization. Neural Computation, 19(2007), 2756-2779.\n    http://www.csie.ntu.edu.tw/~cjlin/nmf/\n    \"\"\"\n    WtV = safe_sparse_dot(W.T, V)\n    WtW = fast_dot(W.T, W)\n\n    # values justified in the paper (alpha is renamed gamma)\n    gamma = 1\n    for n_iter in range(1, max_iter + 1):\n        grad = np.dot(WtW, H) - WtV\n        if alpha > 0 and l1_ratio == 1.:\n            grad += alpha\n        elif alpha > 0:\n            grad += alpha * (l1_ratio + (1 - l1_ratio) * H)\n\n        # The following multiplication with a boolean array is more than twice\n        # as fast as indexing into grad.\n        if norm(grad * np.logical_or(grad < 0, H > 0)) < tol:\n            break\n\n        Hp = H\n\n        for inner_iter in range(20):\n            # Gradient step.\n            Hn = H - gamma * grad\n            # Projection step.\n            Hn *= Hn > 0\n            d = Hn - H\n            gradd = np.dot(grad.ravel(), d.ravel())\n            dQd = np.dot(np.dot(WtW, d).ravel(), d.ravel())\n            suff_decr = (1 - sigma) * gradd + 0.5 * dQd < 0\n            if inner_iter == 0:\n                decr_gamma = not suff_decr\n\n            if decr_gamma:\n                if suff_decr:\n                    H = Hn\n                    break\n                else:\n                    gamma *= beta\n            elif not suff_decr or (Hp == Hn).all():\n                H = Hp\n                break\n            else:\n                gamma /= beta\n                Hp = Hn\n\n    if n_iter == max_iter:\n        warnings.warn(\"Iteration limit reached in nls subproblem.\")\n\n    return H, grad, n_iter\n\nFile:      ~/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages/sklearn/decomposition/nmf.py\nType:      function\n```\n\n\u6700\u7d42\u7684\u306bnumpy\u306e\u30b3\u30fc\u30c9\u3001\n\u3064\u307e\u308a\u5b9f\u969b\u306e\u8a08\u7b97\u3092\u3084\u3063\u3066\u3044\u308b\u90e8\u5206\u306b\u305f\u3069\u308a\u7740\u304f\u3053\u3068\u304c\u3067\u304d\u305f\u3002\n\n## \u307e\u3068\u3081\niPython\u3067\u3072\u305f\u3059\u3089\u8fbf\u3063\u3066\u3044\u304f\u3068\u3001numpy\u306e\u30b3\u30fc\u30c9\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3002\n"}