{"context": " More than 1 year has passed since last update.Hi, this is Chogo again. Today is cool day and good day for programing inside warm home :)\nSo today topic is Scraping again. before that, I'd like to explain my goal of this series. My goal is building a system for tag suggesting with machine learning of Bayesian method. Learning articles and tags I already put on then checking articles for suggesting tags. \nI have to many things to learn so I don't know how many articles for the goal, I will do one by one. \nOk so now today's topic is still scraping. article #1 I explained how to scrape articles from Hatenablog. However this script was only for Hatenablog. I have to extend this script for other web sites. \nFirst i'd like to show you modified script. \n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\n    response = urllib2.urlopen(url)\n    html = response.read()\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\nThis script can scrape articles from Hatana Blog and Qiita. Below are tags of Hatena blog and Qiita. \nHatena Blog:\n    <div class=entry-contents>\n    CONTENTS to SCRAPE!\n    </div>\n\nQiita:\n    <div class=\"col-sm-9 itemsShowBody_articleColumn\"><section class=\"markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative js-task-list-enabled\" id=\"item-xxx\" itemprop=\"articleBody\">\n    CONTENTS to SCRAPE!\n    </div>\n\nSo with BeautifulSoup, I wrote up like this. \nFeeding the elements for the soup... \n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n\nthen, have the soup!\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]}\n\nGood. now I can get the elements for soup for each web site, I can extend the scrape article on other sites! \n\u3069\u3082\u3001\u6885\u6751\u3067\u3052\u3059\u3002\u4eca\u65e5\u306f\u5bd2\u3044\u3067\u3059\u3002\u3053\u3093\u306a\u65e5\u306f\u5916\u3067\u904a\u3070\u305a\u306b\u3001\u6696\u304b\u3044\u90e8\u5c4b\u3067\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3084\u3067\u3002\n\u3055\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u7d9a\u304d\u306a\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u524d\u306b\u4eca\u56de\u306e\u308f\u305f\u304f\u3057\u306e\u30b4\u30fc\u30eb\u3092\u8aac\u660e\u3092\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u30b4\u30fc\u30eb\u306f\u6a5f\u68b0\u5b66\u7fd2\u3092\u4f7f\u3063\u305f\u30bf\u30b0\u306e\u63a8\u5b9a\u30b7\u30b9\u30c6\u30e0\u306b\u306a\u308a\u307e\u3059\u3002\u79c1\u304c\u500b\u4eba\u7684\u306b\u30bf\u30b0\u4ed8\u3051\u3092\u3057\u305f\u30d6\u30c3\u30af\u30de\u30fc\u30af\u8a18\u4e8b\u3092\u5b66\u7fd2\u3057\u305f\u3046\u3048\u3067\u3001\u8a18\u4e8b\u306b\u3042\u3063\u305f\u30bf\u30b0\u3092\u30d9\u30a4\u30b8\u30a2\u30f3\u65b9\u5f0f\u3067\u63a8\u5b9a\u3057\u3088\u3046\u3068\u3044\u3063\u305f\u4ee3\u7269\u306b\u306a\u308a\u307e\u3059\u3002\n\u3067\u3001\u3084\u3063\u3066\u304f\u3046\u3061\u306b\u3044\u308d\u3044\u308d\u899a\u3048\u306a\u304d\u3083\u3044\u3051\u306a\u3044\u3053\u3068\u304c\u591a\u3044\u3068\u3044\u3046\u3053\u3068\u304c\u5224\u660e\u3057\u3066\u304d\u3066\u3044\u308b\u306e\u3067\u3053\u306e\u30b7\u30ea\u30fc\u30ba\u304c\u3069\u3053\u307e\u3067\u7d9a\u304f\u304b\u306f\u672a\u5b9a\u3067\u3059\u3002\u7d42\u308f\u308b\u304b\u306a\u3041\u3002\n\u3055\u3066\u672c\u984c\u3002\u524d\u56de\u306b\u7d9a\u3044\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306b\u306a\u308a\u307e\u3059\u3002\n\u524d\u56de\u306f\u306f\u3066\u306a\u30d6\u30ed\u30b0\u306e\u8a18\u4e8b\u3092\u90e8\u5206\u3092\u629c\u304d\u51fa\u3059\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u306a\u308a\u307e\u3057\u305f\u304c\u3001\u3082\u3061\u308d\u3093\u3001\u307b\u304b\u306e\u30b5\u30a4\u30c8\u306e\u8a18\u4e8b\u304b\u3089\u3082\u629c\u304d\u51fa\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u3053\u3067\u6c4e\u7528\u6027\u3092\u6301\u3064\u3088\u3046\u306b\u6539\u9020\u3059\u308b\u5fc5\u8981\u304c\u3054\u3056\u3044\u307e\u3059\u3002\n\u3067\u3001\u65e9\u901f\u6539\u9020\u5f8c\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3054\u89a7\u3044\u305f\u3060\u304d\u307e\u3059\u3002\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\n    response = urllib2.urlopen(url)\n    html = response.read()\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\n\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306f\u306f\u3066\u306a\u30d6\u30ed\u30b0\u3068Qiita\u306e\u8a18\u4e8b\u304b\u3089\u30a8\u30f3\u30c8\u30ea\u30fc\u90e8\u5206\u3092\u629c\u304d\u51fa\u3057\u3066\u307e\u3059\u3002\u305d\u308c\u305e\u308c\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30bf\u30b0\u306b\u56f2\u307e\u308c\u3066\u3044\u307e\u3059\u3002\nHatena Blog:\n    <div class=entry-contents>\n    CONTENTS to SCRAPE!\n    </div>\n\nQiita:\n    <div class=\"col-sm-9 itemsShowBody_articleColumn\"><section class=\"markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative js-task-list-enabled\" id=\"item-xxx\" itemprop=\"articleBody\">\n    CONTENTS to SCRAPE!\n    </div>\n\n\u3058\u3083\u3042\u3001BeautifulSoup\u306e\u30bf\u30b0\u5224\u5b9a\u306b\u5fc5\u8981\u306b\u306a\u308b\u3068\u3053\u308d\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6307\u5b9a\u3057\u307e\u3059\u3002\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n\n\u3058\u3083\u3042\u3001\u30b9\u30fc\u30d7\u3092\u53ec\u3057\u4e0a\u304c\u308c\uff01\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]}\n\n\u3068\u3044\u3063\u305f\u611f\u3058\u306b\u306a\u308a\u307e\u3059\u3002\u8a18\u4e8b\u3092\u5f15\u304d\u629c\u304d\u305f\u3044\u30b5\u30a4\u30c8\u306e\u30bf\u30b0\u306e\u60c5\u5831\u3092\u8ffd\u52a0\u3057\u3066\u3044\u3051\u3070\u307b\u304b\u306e\u30b5\u30a4\u30c8\u306b\u3082\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\u672c\u65e5\u306f\u3053\u3053\u307e\u3067\u3067\u3059\u304c\u3001\u3053\u306e\u30b7\u30ea\u30fc\u30ba\u306f\u307e\u3060\u307e\u3060\u7d9a\u304d\u307e\u3059\u3002\nHi, this is Chogo again. Today is cool day and good day for programing inside warm home :)\n\nSo today topic is Scraping again. before that, I'd like to explain my goal of this series. My goal is building a system for tag suggesting with machine learning of Bayesian method. Learning articles and tags I already put on then checking articles for suggesting tags. \nI have to many things to learn so I don't know how many articles for the goal, I will do one by one. \n\nOk so now today's topic is still scraping. article #1 I explained how to scrape articles from Hatenablog. However this script was only for Hatenablog. I have to extend this script for other web sites. \n\nFirst i'd like to show you modified script. \n\n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n        c = 0\n        for domain in scraper:\n            print url, domain[0]\n            if re.search( domain[0], url):\n                break\n            c += 1\n\n        response = urllib2.urlopen(url)\n        html = response.read()\n        \n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n        text = \"\"\n        for con in tag.contents:\n            p = re.compile(r'<.*?>')\n            text += p.sub('', con.encode('utf8'))\n\nThis script can scrape articles from Hatana Blog and Qiita. Below are tags of Hatena blog and Qiita. \n\nHatena Blog:\n\n        <div class=entry-contents>\n        CONTENTS to SCRAPE!\n        </div>\n\nQiita:\n\n        <div class=\"col-sm-9 itemsShowBody_articleColumn\"><section class=\"markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative js-task-list-enabled\" id=\"item-xxx\" itemprop=\"articleBody\">\n        CONTENTS to SCRAPE!\n        </div>\n\nSo with BeautifulSoup, I wrote up like this. \nFeeding the elements for the soup... \n\n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n\nthen, have the soup!\n\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]}\n\nGood. now I can get the elements for soup for each web site, I can extend the scrape article on other sites! \n\n\n\u3069\u3082\u3001\u6885\u6751\u3067\u3052\u3059\u3002\u4eca\u65e5\u306f\u5bd2\u3044\u3067\u3059\u3002\u3053\u3093\u306a\u65e5\u306f\u5916\u3067\u904a\u3070\u305a\u306b\u3001\u6696\u304b\u3044\u90e8\u5c4b\u3067\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3084\u3067\u3002\n\n\u3055\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306e\u7d9a\u304d\u306a\u306e\u3067\u3059\u304c\u3001\u305d\u306e\u524d\u306b\u4eca\u56de\u306e\u308f\u305f\u304f\u3057\u306e\u30b4\u30fc\u30eb\u3092\u8aac\u660e\u3092\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u30b4\u30fc\u30eb\u306f\u6a5f\u68b0\u5b66\u7fd2\u3092\u4f7f\u3063\u305f\u30bf\u30b0\u306e\u63a8\u5b9a\u30b7\u30b9\u30c6\u30e0\u306b\u306a\u308a\u307e\u3059\u3002\u79c1\u304c\u500b\u4eba\u7684\u306b\u30bf\u30b0\u4ed8\u3051\u3092\u3057\u305f\u30d6\u30c3\u30af\u30de\u30fc\u30af\u8a18\u4e8b\u3092\u5b66\u7fd2\u3057\u305f\u3046\u3048\u3067\u3001\u8a18\u4e8b\u306b\u3042\u3063\u305f\u30bf\u30b0\u3092\u30d9\u30a4\u30b8\u30a2\u30f3\u65b9\u5f0f\u3067\u63a8\u5b9a\u3057\u3088\u3046\u3068\u3044\u3063\u305f\u4ee3\u7269\u306b\u306a\u308a\u307e\u3059\u3002\n\u3067\u3001\u3084\u3063\u3066\u304f\u3046\u3061\u306b\u3044\u308d\u3044\u308d\u899a\u3048\u306a\u304d\u3083\u3044\u3051\u306a\u3044\u3053\u3068\u304c\u591a\u3044\u3068\u3044\u3046\u3053\u3068\u304c\u5224\u660e\u3057\u3066\u304d\u3066\u3044\u308b\u306e\u3067\u3053\u306e\u30b7\u30ea\u30fc\u30ba\u304c\u3069\u3053\u307e\u3067\u7d9a\u304f\u304b\u306f\u672a\u5b9a\u3067\u3059\u3002\u7d42\u308f\u308b\u304b\u306a\u3041\u3002\n\n\u3055\u3066\u672c\u984c\u3002\u524d\u56de\u306b\u7d9a\u3044\u3066\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u306b\u306a\u308a\u307e\u3059\u3002\n\u524d\u56de\u306f\u306f\u3066\u306a\u30d6\u30ed\u30b0\u306e\u8a18\u4e8b\u3092\u90e8\u5206\u3092\u629c\u304d\u51fa\u3059\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u306a\u308a\u307e\u3057\u305f\u304c\u3001\u3082\u3061\u308d\u3093\u3001\u307b\u304b\u306e\u30b5\u30a4\u30c8\u306e\u8a18\u4e8b\u304b\u3089\u3082\u629c\u304d\u51fa\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u3053\u3067\u6c4e\u7528\u6027\u3092\u6301\u3064\u3088\u3046\u306b\u6539\u9020\u3059\u308b\u5fc5\u8981\u304c\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u3067\u3001\u65e9\u901f\u6539\u9020\u5f8c\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u3054\u89a7\u3044\u305f\u3060\u304d\u307e\u3059\u3002\n \n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n        c = 0\n        for domain in scraper:\n            print url, domain[0]\n            if re.search( domain[0], url):\n                break\n            c += 1\n\n        response = urllib2.urlopen(url)\n        html = response.read()\n        \n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n        text = \"\"\n        for con in tag.contents:\n            p = re.compile(r'<.*?>')\n            text += p.sub('', con.encode('utf8'))\n\n\n\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306f\u306f\u3066\u306a\u30d6\u30ed\u30b0\u3068Qiita\u306e\u8a18\u4e8b\u304b\u3089\u30a8\u30f3\u30c8\u30ea\u30fc\u90e8\u5206\u3092\u629c\u304d\u51fa\u3057\u3066\u307e\u3059\u3002\u305d\u308c\u305e\u308c\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30bf\u30b0\u306b\u56f2\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\nHatena Blog:\n\n        <div class=entry-contents>\n        CONTENTS to SCRAPE!\n        </div>\n\nQiita:\n\n        <div class=\"col-sm-9 itemsShowBody_articleColumn\"><section class=\"markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative js-task-list-enabled\" id=\"item-xxx\" itemprop=\"articleBody\">\n        CONTENTS to SCRAPE!\n        </div>\n\n\u3058\u3083\u3042\u3001BeautifulSoup\u306e\u30bf\u30b0\u5224\u5b9a\u306b\u5fc5\u8981\u306b\u306a\u308b\u3068\u3053\u308d\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n\n\u3058\u3083\u3042\u3001\u30b9\u30fc\u30d7\u3092\u53ec\u3057\u4e0a\u304c\u308c\uff01\n\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]}\n\n\n\u3068\u3044\u3063\u305f\u611f\u3058\u306b\u306a\u308a\u307e\u3059\u3002\u8a18\u4e8b\u3092\u5f15\u304d\u629c\u304d\u305f\u3044\u30b5\u30a4\u30c8\u306e\u30bf\u30b0\u306e\u60c5\u5831\u3092\u8ffd\u52a0\u3057\u3066\u3044\u3051\u3070\u307b\u304b\u306e\u30b5\u30a4\u30c8\u306b\u3082\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u672c\u65e5\u306f\u3053\u3053\u307e\u3067\u3067\u3059\u304c\u3001\u3053\u306e\u30b7\u30ea\u30fc\u30ba\u306f\u307e\u3060\u307e\u3060\u7d9a\u304d\u307e\u3059\u3002\n", "tags": ["MachineLearning", "Python", "scraping"]}