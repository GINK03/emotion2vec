{"context": "\n\n\u306f\u3058\u3081\u306b\nChainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001MNIST\u306e\u5b9f\u88c5\u4f8b\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nIntroduction to Chainer / Chainer\n'hello world'\u76f8\u5f53\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u304c\u3001\u3082\u3063\u3068\u5358\u7d14\u306a\u4f8b\u3067\u52d5\u304d\u3092\u4f53\u611f\u3059\u308b\u305f\u3081\u3001\u8ad6\u7406\u6f14\u7b97\u306e\u5b9f\u88c5\u3092\u7d39\u4ecb\u3057\u3066\u304f\u308c\u3066\u3044\u308b\u8a18\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\nchainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b66\u3093\u3067\u307f\u308b\u3088(chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c82) / \u4eba\u5de5\u8a00\u8a9e\u51e6\u7406\u5165\u9580\n\u305f\u3060\u3001\u3053\u306e\u4f8b\u3067\u306f\u4ee5\u524d\u306eChainer\u3067\u306e\u5b9f\u88c5\u65b9\u6cd5\u306e\u3088\u3046\u306a\u306e\u3067\u3001\u52c9\u5f37\u3092\u517c\u306d\u3066Chainer\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u8fd1\u3044\u5f62\u3067\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\n1\u3064\u306eLink\u3067\nAnd/Or\u306f\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u3082\u306e\u306e\u3001Link1\u3064\u3067\u306f\u8868\u73fe\u3067\u304d\u306a\u3044Xor\u3067\u5931\u6557\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3082\u306e\u3067\u3059\u3002\n2\u3064\u306e\u5165\u529b\u3092\u30012\u3064\u306e\u51fa\u529b\u3067\u8868\u73fe\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b9a\u7fa9\u3002\u51fa\u529b\u306f\u300cFalse\u304b\u300d\u300cTrue\u304b\u300d\u3092\u308c\u305e\u308c\u3092\u5b9f\u6570\u3067\u8868\u3059\u5206\u985e\u5668\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n## Network definition\nclass NN2x2_1Link(Chain):\n    def __init__(self):\n        super(NN2x2_1Link, self).__init__(\n            l = F.Linear(2, 2),\n        )\n    def __call__(self, x):\n        h = self.l(x)\n        return h\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u3001\u771f\u7406\u5024\u8868\u306e4\u3064\u306e\u5b9a\u7fa9\u3092\u305d\u306e\u307e\u307e\u4f7f\u3046\u3068\u3057\u3066\u3001\u3053\u308c\u3067\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b66\u7fd2\u3059\u308b\u306b\u306f\u5c11\u306a\u3059\u304e\u307e\u3059\u3002\n\u305d\u3053\u3067\u3001\u5358\u7d14\u306b\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3057 augment_size \u56de\u7e70\u308a\u8fd4\u3057\u4e0e\u3048\u308b\u3053\u3068\u30921\u56de\u306eepoch\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n## Runs learning loop\ndef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n    augment_size = 100\n    for epoch in range(epoch_size):\n        print('epoch %d' % epoch)\n        for a in range(augment_size):\n            for i in range(len(inputs)):\n                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n                t = Variable(outputs[i].astype(np.int32), volatile=False)\n                optimizer.update(model, x, t)\n        summarize(model, optimizer, inputs, outputs)\n\n\n\u5b66\u7fd2\u7d50\u679c\u306f\u2026\nAnd/Or\u306f\u6b63\u3057\u304f\u5b66\u3093\u3067\u304f\u308c\u307e\u3057\u305f\u3002\n<<AND: After Learning>>\n  :\nepoch 4\nmodel says:\n  0 & 0 = 0 (zero:1.355372 one:-1.355372)\n  0 & 1 = 0 (zero:0.276090 one:-0.623873)\n  1 & 0 = 0 (zero:0.292820 one:-1.014451)\n  1 & 1 = 1 (zero:-0.786463 one:-0.282952)\n\n<<OR: After Learning>>\n  :\nepoch 4\nmodel says:\n  0 & 0 = 0 (zero:0.210631 one:-0.210631)\n  0 & 1 = 1 (zero:-0.965156 one:0.735971)\n  1 & 0 = 1 (zero:-0.337314 one:1.656959)\n  1 & 1 = 1 (zero:-1.513100 one:2.603561)\n\n\u3057\u304b\u3057 \u203b\u60f3\u5b9a\u901a\u308a\u306b\u203b\u3001Xor\u306f\u5b66\u3079\u3066\u3044\u307e\u305b\u3093\u3002\n<<XOR: Before learning>>\n  :\nepoch 19\nmodel says:\n  0 & 0 = 1 (zero:-0.005195 one:0.005195)\n  0 & 1 = 1 (zero:-0.365441 one:-0.365370)\n  1 & 0 = 0 (zero:0.418680 one:0.408669)\n  1 & 1 = 0 (zero:0.058434 one:0.038104)\n\n\u305d\u3053\u3067\u3001Link\u3092\uff12\u3064\u306b\u3059\u308b\u3068\u5168\u3066\u3046\u307e\u304f\u3044\u304f\u306f\u305a\u3067\u3059\u3002\n\n2\u3064\u306eLink\u3067\nAnd/Or\u306f\u5272\u611b\u3057\u307e\u3059\u304c\u3001Xor\u3082\u7121\u4e8b\u306b\u5b66\u3079\u307e\u3057\u305f\u3002\n\u3057\u304b\u3057\u30012\u3064\u306eLink\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u53ce\u675f\u3059\u308b\u307e\u3067\u306b\u76f8\u5f53\u6642\u9593\u304c\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002\nAnd/Or\u30674\u500d\u306bepoch\u3092\u5897\u3084\u3057\u3001Xor\u306f200\u56de\u3067\u5b89\u5b9a\u7684\u306b\u5b66\u7fd2\u304c\u7d42\u4e86\u3059\u308b\u69d8\u5b50\u3067\u3057\u305f\u3002\n<<XOR: Before learning>>\n  :\nepoch 199\nmodel says:\n  0 & 0 = 0 (zero:2.194428 one:-2.195220)\n  0 & 1 = 1 (zero:-2.352996 one:2.251660)\n  1 & 0 = 1 (zero:-2.347063 one:2.247520)\n  1 & 1 = 0 (zero:1.743878 one:-2.734182)\n\n\n\u307e\u3068\u3081\n\u3053\u3053\u307e\u3067\u3084\u308c\u3070\u3001\u6d41\u77f3\u306b\u8151\u843d\u3061\u3057\u307e\u3059(\u82e6\u7b11)\u3002\n2\u3064Link\u3092\u4f7f\u3046\u4f8b\u3067\u306f\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3063\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3089\u3057\u3055\u304c\u3061\u3087\u3063\u3068\u51fa\u3066\u3044\u307e\u3059\u3002\nChainer\u3092\u4f7f\u3046\u3053\u3068\u3067\u3068\u3066\u3082\u7c21\u5358\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3067\u304d\u307e\u3059\u304c\u3001\u305d\u308c\u3060\u3051\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5e38\u8b58\u304c\u524d\u63d0\u3060\u306a\u3068\u611f\u3058\u307e\u3057\u305f\u3002\u99b4\u67d3\u307f\u306e\u306a\u3044\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\u304c\u5b66\u7fd2\u306e\u305f\u3081\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u30c8\u30e9\u30a4\u3059\u308b\u3068\u3001API\u306e\u540d\u524d\u3084\u6982\u5ff5\u3067\u5206\u304b\u3089\u306a\u3044\u3068\u3053\u308d\u304c\u51fa\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u305d\u306e\u3068\u304d\u6df1\u5c64\u5b66\u7fd2 (\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba)\u306a\u3069\u53c2\u7167\u3057\u306a\u304c\u3089\u9032\u3081\u308b\u3068\u3001\u5b66\u3076\u52b9\u7387\u304c\u826f\u3055\u305d\u3046\u3067\u3059\u3002\n\n\u30b3\u30fc\u30c9\n\n1\u3064\u306eLink\nGIST\n# And/Or/Xor classifier network example\n#\n# This is re-written version of:\n#   http://hi-king.hatenablog.com/entry/2015/06/27/194630\n# By following chainer introduction:\n#   http://docs.chainer.org/en/stable/tutorial/basic.html\n\n## Chainer cliche\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n# Neural Network\n\n## Network definition\nclass NN2x2x1dim(Chain):\n    def __init__(self):\n        super(NN2x2x1dim, self).__init__(\n            l = F.Linear(2, 2),\n        )\n    def __call__(self, x):\n        h = self.l(x)\n        return h\n\n# Sub routine\n\n## Utility: Summarize current results\ndef summarize(model, optimizer, inputs, outputs):\n    sum_loss, sum_accuracy = 0, 0\n    print 'model says:'\n    for i in range(len(inputs)):\n        x  = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n        t  = Variable(outputs[i].astype(np.int32))\n        y = model.predictor(x)\n        loss = model(x, t)\n        sum_loss += loss.data\n        sum_accuracy += model.accuracy.data\n        print('  %d & %d = %d (zero:%f one:%f)' % (x.data[0,0], x.data[0,1], np.argmax(y.data), y.data[0,0], y.data[0,1]))\n    #mean_loss = sum_loss / len(inputs)\n    #mean_accuracy = sum_accuracy / len(inputs)\n    #print sum_loss, sum_accuracy, mean_loss, mean_accuracy\n\n## Runs learning loop\ndef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n    augment_size = 100\n    for epoch in range(epoch_size):\n        print('epoch %d' % epoch)\n        for a in range(augment_size):\n            for i in range(len(inputs)):\n                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n                t = Variable(outputs[i].astype(np.int32), volatile=False)\n                optimizer.update(model, x, t)\n        summarize(model, optimizer, inputs, outputs)\n\n# Main\n\n## Test data\ninputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\nand_outputs = np.array([[0], [0], [0], [1]], dtype=np.int32)\nor_outputs = np.array([[0], [1], [1], [1]], dtype=np.int32)\nxor_outputs = np.array([[0], [1], [1], [0]], dtype=np.int32)\n\n## AND Test --> will learn successfully\n## Model & Optimizer instance\nand_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\n# quicker) optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\noptimizer.setup(and_model)\nprint '<<AND: Before learning>>'\nsummarize(and_model, optimizer, inputs, and_outputs)\nprint '\\n<<AND: After Learning>>'\nlearning_looper(and_model, optimizer, inputs, and_outputs, epoch_size = 5)\n\n## OR Test --> will learn successfully\n## Model & Optimizer instance\nor_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\noptimizer.setup(or_model)\nprint '---------\\n\\n<<OR: Before learning>>'\nsummarize(or_model, optimizer, inputs, or_outputs)\nprint '\\n<<OR: After Learning>>'\nlearning_looper(or_model, optimizer, inputs, or_outputs, epoch_size = 5)\n\n## XOR Test --> will FAIL, single link is not enough for XOR\n## Model & Optimizer instance\nxor_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\noptimizer.setup(xor_model)\nprint '---------\\n\\n<<XOR: Before learning>>'\nsummarize(xor_model, optimizer, inputs, xor_outputs)\nprint '\\n<<XOR: After Learning>>'\nlearning_looper(xor_model, optimizer, inputs, xor_outputs, epoch_size = 20)\n\n\n2\u3064\u306eLink\nGIST\n# Chainer training: And/Or/Xor classifier network example with 2 links.\n#\n# This is re-written version of:\n#   http://hi-king.hatenablog.com/entry/2015/06/27/194630\n# By following chainer introduction:\n#   http://docs.chainer.org/en/stable/tutorial/basic.html\n\n## Chainer cliche\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n# Neural Network\n\n## Network definition\nclass NN2x2_2links(Chain):\n    def __init__(self):\n        super(NN2x2_2links, self).__init__(\n            l1 = F.Linear(2, 2),\n            l2 = F.Linear(2, 2),\n        )\n    def __call__(self, x):\n        h = self.l2(F.sigmoid(self.l1(x)))\n        return h\n\n# Sub routine\n\n## Utility: Summarize current results\ndef summarize(model, optimizer, inputs, outputs):\n    sum_loss, sum_accuracy = 0, 0\n    print 'model says:'\n    for i in range(len(inputs)):\n        x  = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n        t  = Variable(outputs[i].astype(np.int32))\n        y = model.predictor(x)\n        loss = model(x, t)\n        sum_loss += loss.data\n        sum_accuracy += model.accuracy.data\n        print('  %d & %d = %d (zero:%f one:%f)' % (x.data[0,0], x.data[0,1], np.argmax(y.data), y.data[0,0], y.data[0,1]))\n    #mean_loss = sum_loss / len(inputs)\n    #mean_accuracy = sum_accuracy / len(inputs)\n    #print sum_loss, sum_accuracy, mean_loss, mean_accuracy\n\n## Runs learning loop\ndef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n    augment_size = 100\n    for epoch in range(epoch_size):\n        print('epoch %d' % epoch)\n        for a in range(augment_size):\n            for i in range(len(inputs)):\n                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n                t = Variable(outputs[i].astype(np.int32), volatile=False)\n                optimizer.update(model, x, t)\n        summarize(model, optimizer, inputs, outputs)\n\n# Main\n\n## Test data\ninputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\nand_outputs = np.array([[0], [0], [0], [1]], dtype=np.int32)\nor_outputs = np.array([[0], [1], [1], [1]], dtype=np.int32)\nxor_outputs = np.array([[0], [1], [1], [0]], dtype=np.int32)\n\n## AND Test --> will learn successfully\nand_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\n# do it quicker) optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\noptimizer.setup(and_model)\nprint '<<AND: Before learning>>'\nsummarize(and_model, optimizer, inputs, and_outputs)\nprint '\\n<<AND: After Learning>>'\nlearning_looper(and_model, optimizer, inputs, and_outputs, epoch_size = 20)\n\n## OR Test --> will learn successfully\nor_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\noptimizer.setup(or_model)\nprint '---------\\n\\n<<OR: Before learning>>'\nsummarize(or_model, optimizer, inputs, or_outputs)\nprint '\\n<<OR: After Learning>>'\nlearning_looper(or_model, optimizer, inputs, or_outputs, epoch_size = 20)\n\n## XOR Test --> will learn successfully\nxor_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\noptimizer.setup(xor_model)\nprint '---------\\n\\n<<XOR: Before learning>>'\nsummarize(xor_model, optimizer, inputs, xor_outputs)\nprint '\\n<<XOR: After Learning>>'\nlearning_looper(xor_model, optimizer, inputs, xor_outputs, epoch_size = 200)\n\n## \u306f\u3058\u3081\u306b\nChainer\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001MNIST\u306e\u5b9f\u88c5\u4f8b\u304c\u7d39\u4ecb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n[Introduction to Chainer / Chainer](http://docs.chainer.org/en/stable/tutorial/basic.html)\n\n'hello world'\u76f8\u5f53\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u304c\u3001\u3082\u3063\u3068\u5358\u7d14\u306a\u4f8b\u3067\u52d5\u304d\u3092\u4f53\u611f\u3059\u308b\u305f\u3081\u3001\u8ad6\u7406\u6f14\u7b97\u306e\u5b9f\u88c5\u3092\u7d39\u4ecb\u3057\u3066\u304f\u308c\u3066\u3044\u308b\u8a18\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\n\n[chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5b66\u3093\u3067\u307f\u308b\u3088(chainer\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c82) / \u4eba\u5de5\u8a00\u8a9e\u51e6\u7406\u5165\u9580](http://hi-king.hatenablog.com/entry/2015/06/27/194630)\n\n\u305f\u3060\u3001\u3053\u306e\u4f8b\u3067\u306f\u4ee5\u524d\u306eChainer\u3067\u306e\u5b9f\u88c5\u65b9\u6cd5\u306e\u3088\u3046\u306a\u306e\u3067\u3001\u52c9\u5f37\u3092\u517c\u306d\u3066Chainer\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u8fd1\u3044\u5f62\u3067\u66f8\u3044\u3066\u307f\u307e\u3057\u305f\u3002\n\n## 1\u3064\u306eLink\u3067\nAnd/Or\u306f\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u3082\u306e\u306e\u3001Link1\u3064\u3067\u306f\u8868\u73fe\u3067\u304d\u306a\u3044Xor\u3067\u5931\u6557\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3082\u306e\u3067\u3059\u3002\n\n2\u3064\u306e\u5165\u529b\u3092\u30012\u3064\u306e\u51fa\u529b\u3067\u8868\u73fe\u3057\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b9a\u7fa9\u3002\u51fa\u529b\u306f\u300cFalse\u304b\u300d\u300cTrue\u304b\u300d\u3092\u308c\u305e\u308c\u3092\u5b9f\u6570\u3067\u8868\u3059\u5206\u985e\u5668\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\n\t## Network definition\n\tclass NN2x2_1Link(Chain):\n\t    def __init__(self):\n\t        super(NN2x2_1Link, self).__init__(\n\t            l = F.Linear(2, 2),\n\t        )\n\t    def __call__(self, x):\n\t        h = self.l(x)\n\t        return h\n\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306f\u3001\u771f\u7406\u5024\u8868\u306e4\u3064\u306e\u5b9a\u7fa9\u3092\u305d\u306e\u307e\u307e\u4f7f\u3046\u3068\u3057\u3066\u3001\u3053\u308c\u3067\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b66\u7fd2\u3059\u308b\u306b\u306f\u5c11\u306a\u3059\u304e\u307e\u3059\u3002\n\u305d\u3053\u3067\u3001\u5358\u7d14\u306b\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3057 augment_size \u56de\u7e70\u308a\u8fd4\u3057\u4e0e\u3048\u308b\u3053\u3068\u30921\u56de\u306eepoch\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\n\t## Runs learning loop\n\tdef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n\t    augment_size = 100\n\t    for epoch in range(epoch_size):\n\t        print('epoch %d' % epoch)\n\t        for a in range(augment_size):\n\t            for i in range(len(inputs)):\n\t                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n\t                t = Variable(outputs[i].astype(np.int32), volatile=False)\n\t                optimizer.update(model, x, t)\n\t        summarize(model, optimizer, inputs, outputs)\n\n### \u5b66\u7fd2\u7d50\u679c\u306f\u2026\nAnd/Or\u306f\u6b63\u3057\u304f\u5b66\u3093\u3067\u304f\u308c\u307e\u3057\u305f\u3002\n\n\t<<AND: After Learning>>\n      :\n\tepoch 4\n\tmodel says:\n\t  0 & 0 = 0 (zero:1.355372 one:-1.355372)\n\t  0 & 1 = 0 (zero:0.276090 one:-0.623873)\n\t  1 & 0 = 0 (zero:0.292820 one:-1.014451)\n\t  1 & 1 = 1 (zero:-0.786463 one:-0.282952)\n\t\n\t<<OR: After Learning>>\n      :\n\tepoch 4\n\tmodel says:\n\t  0 & 0 = 0 (zero:0.210631 one:-0.210631)\n\t  0 & 1 = 1 (zero:-0.965156 one:0.735971)\n\t  1 & 0 = 1 (zero:-0.337314 one:1.656959)\n\t  1 & 1 = 1 (zero:-1.513100 one:2.603561)\n\n\u3057\u304b\u3057 \u203b\u60f3\u5b9a\u901a\u308a\u306b\u203b\u3001Xor\u306f\u5b66\u3079\u3066\u3044\u307e\u305b\u3093\u3002\n\n\t<<XOR: Before learning>>\n      :\n\tepoch 19\n\tmodel says:\n\t  0 & 0 = 1 (zero:-0.005195 one:0.005195)\n\t  0 & 1 = 1 (zero:-0.365441 one:-0.365370)\n\t  1 & 0 = 0 (zero:0.418680 one:0.408669)\n\t  1 & 1 = 0 (zero:0.058434 one:0.038104)\n\n\u305d\u3053\u3067\u3001Link\u3092\uff12\u3064\u306b\u3059\u308b\u3068\u5168\u3066\u3046\u307e\u304f\u3044\u304f\u306f\u305a\u3067\u3059\u3002\n\n## 2\u3064\u306eLink\u3067\nAnd/Or\u306f\u5272\u611b\u3057\u307e\u3059\u304c\u3001Xor\u3082\u7121\u4e8b\u306b\u5b66\u3079\u307e\u3057\u305f\u3002\n\n\u3057\u304b\u3057\u30012\u3064\u306eLink\u306b\u3059\u308b\u3053\u3068\u3067\u3001\u53ce\u675f\u3059\u308b\u307e\u3067\u306b\u76f8\u5f53\u6642\u9593\u304c\u304b\u304b\u3063\u3066\u3044\u307e\u3059\u3002\nAnd/Or\u30674\u500d\u306bepoch\u3092\u5897\u3084\u3057\u3001Xor\u306f200\u56de\u3067\u5b89\u5b9a\u7684\u306b\u5b66\u7fd2\u304c\u7d42\u4e86\u3059\u308b\u69d8\u5b50\u3067\u3057\u305f\u3002\n\n\t<<XOR: Before learning>>\n      :\n\tepoch 199\n\tmodel says:\n\t  0 & 0 = 0 (zero:2.194428 one:-2.195220)\n\t  0 & 1 = 1 (zero:-2.352996 one:2.251660)\n\t  1 & 0 = 1 (zero:-2.347063 one:2.247520)\n\t  1 & 1 = 0 (zero:1.743878 one:-2.734182)\n\n## \u307e\u3068\u3081\n\u3053\u3053\u307e\u3067\u3084\u308c\u3070\u3001\u6d41\u77f3\u306b\u8151\u843d\u3061\u3057\u307e\u3059(\u82e6\u7b11)\u3002\n2\u3064Link\u3092\u4f7f\u3046\u4f8b\u3067\u306f\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3063\u3066\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3089\u3057\u3055\u304c\u3061\u3087\u3063\u3068\u51fa\u3066\u3044\u307e\u3059\u3002\nChainer\u3092\u4f7f\u3046\u3053\u3068\u3067\u3068\u3066\u3082\u7c21\u5358\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3067\u304d\u307e\u3059\u304c\u3001\u305d\u308c\u3060\u3051\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5e38\u8b58\u304c\u524d\u63d0\u3060\u306a\u3068\u611f\u3058\u307e\u3057\u305f\u3002\u99b4\u67d3\u307f\u306e\u306a\u3044\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\u304c\u5b66\u7fd2\u306e\u305f\u3081\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u30c8\u30e9\u30a4\u3059\u308b\u3068\u3001API\u306e\u540d\u524d\u3084\u6982\u5ff5\u3067\u5206\u304b\u3089\u306a\u3044\u3068\u3053\u308d\u304c\u51fa\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u305d\u306e\u3068\u304d[\u6df1\u5c64\u5b66\u7fd2 (\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30d5\u30a7\u30c3\u30b7\u30e7\u30ca\u30eb\u30b7\u30ea\u30fc\u30ba)](https://www.amazon.co.jp/dp/B018K6C99A/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)\u306a\u3069\u53c2\u7167\u3057\u306a\u304c\u3089\u9032\u3081\u308b\u3068\u3001\u5b66\u3076\u52b9\u7387\u304c\u826f\u3055\u305d\u3046\u3067\u3059\u3002\n\n## \u30b3\u30fc\u30c9\n### 1\u3064\u306eLink\n\n[GIST](https://gist.github.com/daisukelab/56157ee39224a02958c82d06b35f3bd1)\n\n```python\n# And/Or/Xor classifier network example\n#\n# This is re-written version of:\n#   http://hi-king.hatenablog.com/entry/2015/06/27/194630\n# By following chainer introduction:\n#   http://docs.chainer.org/en/stable/tutorial/basic.html\n\n## Chainer cliche\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n# Neural Network\n\n## Network definition\nclass NN2x2x1dim(Chain):\n    def __init__(self):\n        super(NN2x2x1dim, self).__init__(\n            l = F.Linear(2, 2),\n        )\n    def __call__(self, x):\n        h = self.l(x)\n        return h\n\n# Sub routine\n\n## Utility: Summarize current results\ndef summarize(model, optimizer, inputs, outputs):\n    sum_loss, sum_accuracy = 0, 0\n    print 'model says:'\n    for i in range(len(inputs)):\n        x  = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n        t  = Variable(outputs[i].astype(np.int32))\n        y = model.predictor(x)\n        loss = model(x, t)\n        sum_loss += loss.data\n        sum_accuracy += model.accuracy.data\n        print('  %d & %d = %d (zero:%f one:%f)' % (x.data[0,0], x.data[0,1], np.argmax(y.data), y.data[0,0], y.data[0,1]))\n    #mean_loss = sum_loss / len(inputs)\n    #mean_accuracy = sum_accuracy / len(inputs)\n    #print sum_loss, sum_accuracy, mean_loss, mean_accuracy\n\n## Runs learning loop\ndef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n    augment_size = 100\n    for epoch in range(epoch_size):\n        print('epoch %d' % epoch)\n        for a in range(augment_size):\n            for i in range(len(inputs)):\n                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n                t = Variable(outputs[i].astype(np.int32), volatile=False)\n                optimizer.update(model, x, t)\n        summarize(model, optimizer, inputs, outputs)\n\n# Main\n\n## Test data\ninputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\nand_outputs = np.array([[0], [0], [0], [1]], dtype=np.int32)\nor_outputs = np.array([[0], [1], [1], [1]], dtype=np.int32)\nxor_outputs = np.array([[0], [1], [1], [0]], dtype=np.int32)\n\n## AND Test --> will learn successfully\n## Model & Optimizer instance\nand_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\n# quicker) optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\noptimizer.setup(and_model)\nprint '<<AND: Before learning>>'\nsummarize(and_model, optimizer, inputs, and_outputs)\nprint '\\n<<AND: After Learning>>'\nlearning_looper(and_model, optimizer, inputs, and_outputs, epoch_size = 5)\n\n## OR Test --> will learn successfully\n## Model & Optimizer instance\nor_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\noptimizer.setup(or_model)\nprint '---------\\n\\n<<OR: Before learning>>'\nsummarize(or_model, optimizer, inputs, or_outputs)\nprint '\\n<<OR: After Learning>>'\nlearning_looper(or_model, optimizer, inputs, or_outputs, epoch_size = 5)\n\n## XOR Test --> will FAIL, single link is not enough for XOR\n## Model & Optimizer instance\nxor_model = L.Classifier(NN2x2x1dim())\noptimizer = optimizers.SGD()\noptimizer.setup(xor_model)\nprint '---------\\n\\n<<XOR: Before learning>>'\nsummarize(xor_model, optimizer, inputs, xor_outputs)\nprint '\\n<<XOR: After Learning>>'\nlearning_looper(xor_model, optimizer, inputs, xor_outputs, epoch_size = 20)\n```\n\n### 2\u3064\u306eLink\n\n[GIST](https://gist.github.com/daisukelab/13191dceb8881c41f1d93305bc622090)\n\n```python\n# Chainer training: And/Or/Xor classifier network example with 2 links.\n#\n# This is re-written version of:\n#   http://hi-king.hatenablog.com/entry/2015/06/27/194630\n# By following chainer introduction:\n#   http://docs.chainer.org/en/stable/tutorial/basic.html\n\n## Chainer cliche\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n# Neural Network\n\n## Network definition\nclass NN2x2_2links(Chain):\n    def __init__(self):\n        super(NN2x2_2links, self).__init__(\n            l1 = F.Linear(2, 2),\n            l2 = F.Linear(2, 2),\n        )\n    def __call__(self, x):\n        h = self.l2(F.sigmoid(self.l1(x)))\n        return h\n\n# Sub routine\n\n## Utility: Summarize current results\ndef summarize(model, optimizer, inputs, outputs):\n    sum_loss, sum_accuracy = 0, 0\n    print 'model says:'\n    for i in range(len(inputs)):\n        x  = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n        t  = Variable(outputs[i].astype(np.int32))\n        y = model.predictor(x)\n        loss = model(x, t)\n        sum_loss += loss.data\n        sum_accuracy += model.accuracy.data\n        print('  %d & %d = %d (zero:%f one:%f)' % (x.data[0,0], x.data[0,1], np.argmax(y.data), y.data[0,0], y.data[0,1]))\n    #mean_loss = sum_loss / len(inputs)\n    #mean_accuracy = sum_accuracy / len(inputs)\n    #print sum_loss, sum_accuracy, mean_loss, mean_accuracy\n\n## Runs learning loop\ndef learning_looper(model, optimizer, inputs, outputs, epoch_size):\n    augment_size = 100\n    for epoch in range(epoch_size):\n        print('epoch %d' % epoch)\n        for a in range(augment_size):\n            for i in range(len(inputs)):\n                x = Variable(inputs[i].reshape(1,2).astype(np.float32), volatile=False)\n                t = Variable(outputs[i].astype(np.int32), volatile=False)\n                optimizer.update(model, x, t)\n        summarize(model, optimizer, inputs, outputs)\n\n# Main\n\n## Test data\ninputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\nand_outputs = np.array([[0], [0], [0], [1]], dtype=np.int32)\nor_outputs = np.array([[0], [1], [1], [1]], dtype=np.int32)\nxor_outputs = np.array([[0], [1], [1], [0]], dtype=np.int32)\n\n## AND Test --> will learn successfully\nand_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\n# do it quicker) optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\noptimizer.setup(and_model)\nprint '<<AND: Before learning>>'\nsummarize(and_model, optimizer, inputs, and_outputs)\nprint '\\n<<AND: After Learning>>'\nlearning_looper(and_model, optimizer, inputs, and_outputs, epoch_size = 20)\n\n## OR Test --> will learn successfully\nor_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\noptimizer.setup(or_model)\nprint '---------\\n\\n<<OR: Before learning>>'\nsummarize(or_model, optimizer, inputs, or_outputs)\nprint '\\n<<OR: After Learning>>'\nlearning_looper(or_model, optimizer, inputs, or_outputs, epoch_size = 20)\n\n## XOR Test --> will learn successfully\nxor_model = L.Classifier(NN2x2_2links())\noptimizer = optimizers.SGD()\noptimizer.setup(xor_model)\nprint '---------\\n\\n<<XOR: Before learning>>'\nsummarize(xor_model, optimizer, inputs, xor_outputs)\nprint '\\n<<XOR: After Learning>>'\nlearning_looper(xor_model, optimizer, inputs, xor_outputs, epoch_size = 200)\n```\n", "tags": ["Chainer", "NeuralNetwork"]}