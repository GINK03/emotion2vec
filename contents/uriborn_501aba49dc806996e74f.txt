{"context": " More than 1 year has passed since last update.\n\n\u6982\u8981\n\u6b21\u4e16\u4ee3\u5206\u6563\u51e6\u7406\u30a8\u30f3\u30b8\u30f3\u3068\u3057\u3066\u6d41\u884c\u3063\u3066\u307e\u3059\u306d\u3002Apache Spark\u3002\nhttps://spark.apache.org/\nhttps://github.com/apache/spark\n\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u304c\u4e0b\u304c\u308a\u304b\u304b\u3063\u3066\u308b\u3068\u3053\u308d\u306b\u3001\u3061\u3087\u3046\u3069\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u89e3\u6790\u3057\u3066\u301c\u307f\u305f\u3044\u306a\u3053\u3068\u3092\u3084\u308d\u3046\u3068\u3057\u3066\u3044\u305f\u306e\u3067\u3001\u5b9f\u969b\u306e\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u89e3\u6790\u3057\u3001\u30a2\u30af\u30bb\u30b9\u6570\u3092\u96c6\u8a08\u3057\u3066\u3001CSV\u30d5\u30a1\u30a4\u30eb\u306b\u51fa\u529b\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u96c6\u8a08\u3068CSV\u51fa\u529b\n\u4eca\u56de\u306f\u5bfe\u8c61\u306e\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u304b\u3089\u300c/hoge\u300d\u306eURI\u306b\u5bfe\u3057\u3066\u65e5\u6bce\u306b\u3069\u308c\u304f\u3089\u3044\u306e\u30a2\u30af\u30bb\u30b9\u304c\u3042\u3063\u305f\u304b\u3092\u96c6\u8a08\u3057\u3001\u305d\u306e\u7d50\u679c\u3092CSV\u51fa\u529b\u3057\u3066\u307e\u3059\u3002\n\u3053\u3093\u306a\u611f\u3058\u306e\u51fa\u529b\u7d50\u679c\u306b\u3057\u305f\u3044\u3067\u3059\u3002\n  /hoge,1,3,5,60,100,20000,294,...(\u5bfe\u8c61\u65e5\u4ed8\u5206\u3067\u304d\u308b)\n\n\u305d\u3057\u3066\u3001\u5b9f\u969b\u306e\u30b3\u30fc\u30c9\u306f\u3053\u3093\u306a\u611f\u3058\u3002\nimport java.io.File\nimport java.text.SimpleDateFormat\nimport scala.util.parsing.combinator._\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.{MappedRDD, RDD}\n\nobject SparkAnalysis {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"Analysis\")\n    val context = new SparkContext(conf)\n    val (logDir, csvFilePath) = (args(0), args(1))\n\n    val records = logFiles(logDir).map { logFile =>\n      val logData = context.textFile(logFile)\n      logData.map(line => LogParser.parse(line).getOrElse(Log())).filter(!_.time.isEmpty)\n    }.reduce(_ ++ _)\n\n    val filteredRecords = records.filter(_.url == \"/hoge\")\n    val partitions = filteredRecords.mapPartitions { records =>\n      records.map { record => (record.time, 1) }\n    }\n    val counts = partitions.repartition(1).reduceByKey((a, b) => a + b).sortBy(_._1).map(_._2).glom.map(a => \"/hoge,\" + a.mkString(\",\"))\n\n    writeCSV(csvFilePath, counts)\n  }\n\n  private def logFiles(logDir: String): Seq[String] =\n    new File(logDir).listFiles.map(_.getPath))\n\n  private def writeCSV(csvFilePath: String, countData: RDD[String]): Unit = {\n    val tempFilePath = \"/tmp/spark_temp\"\n    FileUtil.fullyDelete(new File(tempFilePath))\n    FileUtil.fullyDelete(new File(csvFilePath))\n\n    countData.saveAsTextFile(tempFilePath)\n    merge(tempFilePath, csvFilePath)\n  }\n\n  private def merge(srcPath: String, dstPath: String): Unit = {\n    val hadoopConfig = new Configuration\n    val hdfs = FileSystem.get(hadoopConfig)\n    FileUtil.copyMerge(hdfs, new Path(srcPath), hdfs, new Path(dstPath), false, hadoopConfig, null)\n  }\n\n}\n\ncase class Log(time: String = \"\", url: String = \"\")\n\nobject LogParser extends RegexParsers {\n\n  def parse(log: String): ParseResult[Log] = parseAll(line, log)\n\n  private def line: Parser[Log] =\n    time ~ logLevel ~ method ~ url ~ routes ~ controller ~ returned ~ status ~ in ~ procTime ~ where ~ requestId ~ and ~ remoteAddress ~ and ~ userAgent ^^ {\n      case time ~ logLevel ~ method ~ url ~ routes ~ controller ~ returned ~ status ~ in ~ procTime ~ where ~ requestId ~ and1 ~ remoteAddress ~ and2 ~ userAgent => Log(time, url)\n    }\n\n  private def time: Parser[String] = \"[\" ~> \"\"\"\\S+ [^ \\]]+\"\"\".r <~ \"]\" ^^ { dayFloor }\n  private def logLevel: Parser[String] = \"[\" ~> \"\"\"\\S+\"\"\".r <~ \"]\"\n  private def method: Parser[String] = \"\"\"[A-Z]+\"\"\".r\n  private def url: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def routes: Parser[String] = \"routes\".r\n  private def controller: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def returned: Parser[String] = \"returned\".r\n  private def status: Parser[Int] = \"\"\"\\S+\"\"\".r ^^ { status => status.split(\"=\")(1).toInt }\n  private def in: Parser[String] = \"in\".r\n  private def procTime: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def where: Parser[String] = \"where\".r\n  private def requestId: Parser[Long] = \"[\" ~> \"\"\"\\S+=\\d+\"\"\".r <~ \"]\" ^^ { requestId => requestId.split(\"=\")(1).toLong }\n  private def and: Parser[String] = \"and\".r\n  private def remoteAddress: Parser[String] = \"\"\"\\S+\"\"\".r ^^ { remoteAddress => remoteAddress.split(\"=\")(1) }\n  private def userAgent: Parser[String] = \"\"\"[^\"]+\"\"\".r\n\n  private def dayFloor(timestamp: String): String = {\n    val dateTime = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss,SSS\").parse(timestamp)\n\n    new SimpleDateFormat(\"yyyy-MM-dd\").format(dateTime)\n  }\n\n}\n\n\n\u30cf\u30de\u3063\u305f\u70b9\n\u300cval counts\u300d\u306e\u3068\u3053\u308d\u3067\u3001\u300crepartition(1)\u300d\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u306f\u3042\u3048\u3066 Spark \u306e\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u30921\u306b\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u3053\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3069\u3046\u306a\u308b\u304b\u3002\u3002\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u51fa\u529b\u7d50\u679c\n/hoge,1\n/hoge,3\n/hoge,5\n/hoge,60\n/hoge,100\n/hoge,20000\n/hoge,294\n...\n\n\u7e26\u306b\u4e26\u3093\u3067\u3057\u307e\u3044\u307e\u3059\u3002\u3053\u3046\u306f\u306a\u3063\u3066\u307b\u3057\u304f\u306a\u3044\u3002\u3002\n\u3053\u308c\u306e\u539f\u56e0\u3067\u3059\u304c\u3001Spark \u306f RDD \u3068\u3044\u3046\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5185\u90e8\u306b\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3092\u6301\u3063\u3066\u304a\u308a\u3001\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u6bce\u306b\u51e6\u7406\u3092\u5b9f\u884c\u3057\u3066\u3044\u304f\u306e\u3067\u3001\norg.apache.spark.rdd.RDD#saveAsTextFile \u3082\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u6bce\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002\n\u5b9f\u969b\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u898b\u3066\u307f\u308b\u3068\u3053\u3093\u306a\u611f\u3058\u3067\u3057\u305f\u3002\n$ ls /tmp/spark_temp\n_SUCCESS    part-00004  part-00009  part-00014  part-00019  part-00024  part-00029  part-00034  part-00039  part-00044  part-00049  part-00054\npart-00000  part-00005  part-00010  part-00015  part-00020  part-00025  part-00030  part-00035  part-00040  part-00045  part-00050  part-00055\npart-00001  part-00006  part-00011  part-00016  part-00021  part-00026  part-00031  part-00036  part-00041  part-00046  part-00051  part-00056\npart-00002  part-00007  part-00012  part-00017  part-00022  part-00027  part-00032  part-00037  part-00042  part-00047  part-00052  part-00057\npart-00003  part-00008  part-00013  part-00018  part-00023  part-00028  part-00033  part-00038  part-00043  part-00048  part-00053\n\n\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u305f\u3061\u3092\u6700\u7d42\u7684\u306b Hadoop \u306e FileUtil#copyMerge \u3067\u4e00\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u304c\u3063\u3061\u3083\u3093\u3053\u3057\u3066\u308b\u304b\u3089\u7e26\u306b\u4e26\u3093\u3067\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3002\n\u306a\u306e\u3067\u3001\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u306e\u30b5\u30a4\u30ba\u30921\u3067\u56fa\u5b9a\u3057\u3066\u3057\u307e\u3063\u3066\u30011\u3064\u306e\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u5185\u3067\n\u96c6\u8a08\u3068CSV\u51fa\u529b\u3092\u884c\u3046\u5f62\u306b\u3057\u307e\u3057\u305f\u3002\n(\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u304c1\u3064\u306a\u306e\u3067\u30ed\u30b0\u306e\u30b5\u30a4\u30ba\u304c\u30d0\u30ab\u3067\u304b\u3044\u3082\u306e\u3060\u3063\u305f\u5834\u5408\u306e\u51e6\u7406\u901f\u5ea6\u3068\u304b\u6c17\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u30ed\u30b0\u30b5\u30a4\u30ba\u3082\u5927\u3057\u305f\u3053\u3068\u306a\u304b\u3063\u305f\u306e\u3067\u59a5\u5354\u3002\u3002)\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u56fa\u5b9a\u3057\u306a\u304f\u3066\u3082\u3053\u3093\u306a\u3044\u3044\u65b9\u6cd5\u3042\u308b\u3088\u30fc\u3068\u304b\u3042\u3063\u305f\u3089\u6559\u3048\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\uff01\uff01\n\n\u307e\u3068\u3081\n\nApache Spark \u306e\u5185\u5074\u3092\u5c11\u3057\u77e5\u308c\u305f\u3002(\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3068\u304b\u30ed\u30b0\u898b\u305f\u308a\u3068\u304b\uff09\nscala \u306e RegexParser \u304c\u7406\u89e3\u3057\u306b\u304f\u3059\u304e\u30ef\u30ed\u30bf\u3002\n\u3084\u3063\u3071\u308a\u65b0\u3057\u3044\u3053\u3068\u306f\u697d\u3057\u3044\uff01\uff01\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u5411\u4e0a\u306b\u306a\u3063\u305f\u3002\nSparkStreaming \u306e\u65b9\u3082\u6642\u9593\u304c\u3042\u3063\u305f\u3089\u8a66\u3057\u3066\u307f\u308b\u3002\n\n\n\u8ffd\u8a18(2015/03/02)\nGithub \u306b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u4e0a\u3052\u3066\u307f\u307e\u3057\u305f\u3002\n\u4e0a\u8a18\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u304b\u3089\u66f4\u306b\u4e0b\u8a18\u306e\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u3066\u3044\u307e\u3059\u3002\n\nURL\u3092\u30ab\u30f3\u30de\u533a\u5207\u308a\u3067\u8907\u6570\u6307\u5b9a\u3067\u304d\u308b\u3088\u3046\u306b\u4fee\u6b63\u3002\nApache Spark \u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u5b9f\u884c\u3057\u3066CSV\u30d5\u30a1\u30a4\u30eb\u3092\u51fa\u529b\u3059\u308b\u307e\u3067\u306e\u5b9f\u884c\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8ffd\u52a0\u3002\n\nhttps://github.com/uriborn/apache-spark-csv-sample\n\n\u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\nhttp://qiita.com/aoiroaoino/items/1463362db165d5b08eba\nhttp://architects.dzone.com/articles/spark-write-csv-file\nhttp://spark.apache.org/docs/1.2.0/programming-guide.html\nhttp://www.ne.jp/asahi/hishidama/home/tech/scala/spark/partition.html\n\n# \u6982\u8981\n\u6b21\u4e16\u4ee3\u5206\u6563\u51e6\u7406\u30a8\u30f3\u30b8\u30f3\u3068\u3057\u3066\u6d41\u884c\u3063\u3066\u307e\u3059\u306d\u3002Apache Spark\u3002\nhttps://spark.apache.org/\nhttps://github.com/apache/spark\n\n\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u304c\u4e0b\u304c\u308a\u304b\u304b\u3063\u3066\u308b\u3068\u3053\u308d\u306b\u3001\u3061\u3087\u3046\u3069\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u89e3\u6790\u3057\u3066\u301c\u307f\u305f\u3044\u306a\u3053\u3068\u3092\u3084\u308d\u3046\u3068\u3057\u3066\u3044\u305f\u306e\u3067\u3001\u5b9f\u969b\u306e\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u3092\u89e3\u6790\u3057\u3001\u30a2\u30af\u30bb\u30b9\u6570\u3092\u96c6\u8a08\u3057\u3066\u3001CSV\u30d5\u30a1\u30a4\u30eb\u306b\u51fa\u529b\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n# \u96c6\u8a08\u3068CSV\u51fa\u529b\n\u4eca\u56de\u306f\u5bfe\u8c61\u306e\u30a2\u30af\u30bb\u30b9\u30ed\u30b0\u304b\u3089\u300c/hoge\u300d\u306eURI\u306b\u5bfe\u3057\u3066\u65e5\u6bce\u306b\u3069\u308c\u304f\u3089\u3044\u306e\u30a2\u30af\u30bb\u30b9\u304c\u3042\u3063\u305f\u304b\u3092\u96c6\u8a08\u3057\u3001\u305d\u306e\u7d50\u679c\u3092CSV\u51fa\u529b\u3057\u3066\u307e\u3059\u3002\n\n\u3053\u3093\u306a\u611f\u3058\u306e\u51fa\u529b\u7d50\u679c\u306b\u3057\u305f\u3044\u3067\u3059\u3002\n\n```\n  /hoge,1,3,5,60,100,20000,294,...(\u5bfe\u8c61\u65e5\u4ed8\u5206\u3067\u304d\u308b)\n```\n\n\u305d\u3057\u3066\u3001\u5b9f\u969b\u306e\u30b3\u30fc\u30c9\u306f\u3053\u3093\u306a\u611f\u3058\u3002\n\n```scala\nimport java.io.File\nimport java.text.SimpleDateFormat\nimport scala.util.parsing.combinator._\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.{MappedRDD, RDD}\n\nobject SparkAnalysis {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"Analysis\")\n    val context = new SparkContext(conf)\n    val (logDir, csvFilePath) = (args(0), args(1))\n\n    val records = logFiles(logDir).map { logFile =>\n      val logData = context.textFile(logFile)\n      logData.map(line => LogParser.parse(line).getOrElse(Log())).filter(!_.time.isEmpty)\n    }.reduce(_ ++ _)\n    \n    val filteredRecords = records.filter(_.url == \"/hoge\")\n    val partitions = filteredRecords.mapPartitions { records =>\n      records.map { record => (record.time, 1) }\n    }\n    val counts = partitions.repartition(1).reduceByKey((a, b) => a + b).sortBy(_._1).map(_._2).glom.map(a => \"/hoge,\" + a.mkString(\",\"))\n\n    writeCSV(csvFilePath, counts)\n  }\n  \n  private def logFiles(logDir: String): Seq[String] =\n    new File(logDir).listFiles.map(_.getPath))\n  \n  private def writeCSV(csvFilePath: String, countData: RDD[String]): Unit = {\n    val tempFilePath = \"/tmp/spark_temp\"\n    FileUtil.fullyDelete(new File(tempFilePath))\n    FileUtil.fullyDelete(new File(csvFilePath))\n\n    countData.saveAsTextFile(tempFilePath)\n    merge(tempFilePath, csvFilePath)\n  }\n  \n  private def merge(srcPath: String, dstPath: String): Unit = {\n    val hadoopConfig = new Configuration\n    val hdfs = FileSystem.get(hadoopConfig)\n    FileUtil.copyMerge(hdfs, new Path(srcPath), hdfs, new Path(dstPath), false, hadoopConfig, null)\n  }\n  \n}\n\ncase class Log(time: String = \"\", url: String = \"\")\n\nobject LogParser extends RegexParsers {\n\n  def parse(log: String): ParseResult[Log] = parseAll(line, log)\n\n  private def line: Parser[Log] =\n    time ~ logLevel ~ method ~ url ~ routes ~ controller ~ returned ~ status ~ in ~ procTime ~ where ~ requestId ~ and ~ remoteAddress ~ and ~ userAgent ^^ {\n      case time ~ logLevel ~ method ~ url ~ routes ~ controller ~ returned ~ status ~ in ~ procTime ~ where ~ requestId ~ and1 ~ remoteAddress ~ and2 ~ userAgent => Log(time, url)\n    }\n\n  private def time: Parser[String] = \"[\" ~> \"\"\"\\S+ [^ \\]]+\"\"\".r <~ \"]\" ^^ { dayFloor }\n  private def logLevel: Parser[String] = \"[\" ~> \"\"\"\\S+\"\"\".r <~ \"]\"\n  private def method: Parser[String] = \"\"\"[A-Z]+\"\"\".r\n  private def url: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def routes: Parser[String] = \"routes\".r\n  private def controller: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def returned: Parser[String] = \"returned\".r\n  private def status: Parser[Int] = \"\"\"\\S+\"\"\".r ^^ { status => status.split(\"=\")(1).toInt }\n  private def in: Parser[String] = \"in\".r\n  private def procTime: Parser[String] = \"\"\"\\S+\"\"\".r\n  private def where: Parser[String] = \"where\".r\n  private def requestId: Parser[Long] = \"[\" ~> \"\"\"\\S+=\\d+\"\"\".r <~ \"]\" ^^ { requestId => requestId.split(\"=\")(1).toLong }\n  private def and: Parser[String] = \"and\".r\n  private def remoteAddress: Parser[String] = \"\"\"\\S+\"\"\".r ^^ { remoteAddress => remoteAddress.split(\"=\")(1) }\n  private def userAgent: Parser[String] = \"\"\"[^\"]+\"\"\".r\n\n  private def dayFloor(timestamp: String): String = {\n    val dateTime = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss,SSS\").parse(timestamp)\n\n    new SimpleDateFormat(\"yyyy-MM-dd\").format(dateTime)\n  }\n\n}\n```\n\n# \u30cf\u30de\u3063\u305f\u70b9\n\u300cval counts\u300d\u306e\u3068\u3053\u308d\u3067\u3001\u300crepartition(1)\u300d\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u306f\u3042\u3048\u3066 Spark \u306e\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u30921\u306b\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n\u3053\u3053\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3069\u3046\u306a\u308b\u304b\u3002\u3002\n\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306e\u51fa\u529b\u7d50\u679c\n\n```\n/hoge,1\n/hoge,3\n/hoge,5\n/hoge,60\n/hoge,100\n/hoge,20000\n/hoge,294\n...\n```\n\n\u7e26\u306b\u4e26\u3093\u3067\u3057\u307e\u3044\u307e\u3059\u3002\u3053\u3046\u306f\u306a\u3063\u3066\u307b\u3057\u304f\u306a\u3044\u3002\u3002\n\u3053\u308c\u306e\u539f\u56e0\u3067\u3059\u304c\u3001Spark \u306f RDD \u3068\u3044\u3046\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5185\u90e8\u306b\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3092\u6301\u3063\u3066\u304a\u308a\u3001\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u6bce\u306b\u51e6\u7406\u3092\u5b9f\u884c\u3057\u3066\u3044\u304f\u306e\u3067\u3001\norg.apache.spark.rdd.RDD#saveAsTextFile \u3082\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u6bce\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002\n\n\u5b9f\u969b\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u898b\u3066\u307f\u308b\u3068\u3053\u3093\u306a\u611f\u3058\u3067\u3057\u305f\u3002\n\n```\n$ ls /tmp/spark_temp\n_SUCCESS    part-00004  part-00009  part-00014  part-00019  part-00024  part-00029  part-00034  part-00039  part-00044  part-00049  part-00054\npart-00000  part-00005  part-00010  part-00015  part-00020  part-00025  part-00030  part-00035  part-00040  part-00045  part-00050  part-00055\npart-00001  part-00006  part-00011  part-00016  part-00021  part-00026  part-00031  part-00036  part-00041  part-00046  part-00051  part-00056\npart-00002  part-00007  part-00012  part-00017  part-00022  part-00027  part-00032  part-00037  part-00042  part-00047  part-00052  part-00057\npart-00003  part-00008  part-00013  part-00018  part-00023  part-00028  part-00033  part-00038  part-00043  part-00048  part-00053\n```\n\n\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u305f\u3061\u3092\u6700\u7d42\u7684\u306b Hadoop \u306e FileUtil#copyMerge \u3067\u4e00\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u304c\u3063\u3061\u3083\u3093\u3053\u3057\u3066\u308b\u304b\u3089\u7e26\u306b\u4e26\u3093\u3067\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3002\n\n\u306a\u306e\u3067\u3001\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u306e\u30b5\u30a4\u30ba\u30921\u3067\u56fa\u5b9a\u3057\u3066\u3057\u307e\u3063\u3066\u30011\u3064\u306e\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u5185\u3067\n\u96c6\u8a08\u3068CSV\u51fa\u529b\u3092\u884c\u3046\u5f62\u306b\u3057\u307e\u3057\u305f\u3002\n(\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u304c1\u3064\u306a\u306e\u3067\u30ed\u30b0\u306e\u30b5\u30a4\u30ba\u304c\u30d0\u30ab\u3067\u304b\u3044\u3082\u306e\u3060\u3063\u305f\u5834\u5408\u306e\u51e6\u7406\u901f\u5ea6\u3068\u304b\u6c17\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u30ed\u30b0\u30b5\u30a4\u30ba\u3082\u5927\u3057\u305f\u3053\u3068\u306a\u304b\u3063\u305f\u306e\u3067\u59a5\u5354\u3002\u3002)\n\n\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u56fa\u5b9a\u3057\u306a\u304f\u3066\u3082\u3053\u3093\u306a\u3044\u3044\u65b9\u6cd5\u3042\u308b\u3088\u30fc\u3068\u304b\u3042\u3063\u305f\u3089\u6559\u3048\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\uff01\uff01\n\n# \u307e\u3068\u3081\n- Apache Spark \u306e\u5185\u5074\u3092\u5c11\u3057\u77e5\u308c\u305f\u3002(\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3068\u304b\u30ed\u30b0\u898b\u305f\u308a\u3068\u304b\uff09\n- scala \u306e RegexParser \u304c\u7406\u89e3\u3057\u306b\u304f\u3059\u304e\u30ef\u30ed\u30bf\u3002\n- \u3084\u3063\u3071\u308a\u65b0\u3057\u3044\u3053\u3068\u306f\u697d\u3057\u3044\uff01\uff01\u30e2\u30c1\u30d9\u30fc\u30b7\u30e7\u30f3\u306e\u5411\u4e0a\u306b\u306a\u3063\u305f\u3002\n- SparkStreaming \u306e\u65b9\u3082\u6642\u9593\u304c\u3042\u3063\u305f\u3089\u8a66\u3057\u3066\u307f\u308b\u3002\n\n# \u8ffd\u8a18(2015/03/02)\nGithub \u306b\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u4e0a\u3052\u3066\u307f\u307e\u3057\u305f\u3002\n\u4e0a\u8a18\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u304b\u3089\u66f4\u306b\u4e0b\u8a18\u306e\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u3066\u3044\u307e\u3059\u3002\n\n- URL\u3092\u30ab\u30f3\u30de\u533a\u5207\u308a\u3067\u8907\u6570\u6307\u5b9a\u3067\u304d\u308b\u3088\u3046\u306b\u4fee\u6b63\u3002\n- Apache Spark \u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u5b9f\u884c\u3057\u3066CSV\u30d5\u30a1\u30a4\u30eb\u3092\u51fa\u529b\u3059\u308b\u307e\u3067\u306e\u5b9f\u884c\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u8ffd\u52a0\u3002\n\nhttps://github.com/uriborn/apache-spark-csv-sample\n\n# \u53c2\u8003\u306b\u3057\u305f\u30b5\u30a4\u30c8\nhttp://qiita.com/aoiroaoino/items/1463362db165d5b08eba\nhttp://architects.dzone.com/articles/spark-write-csv-file\nhttp://spark.apache.org/docs/1.2.0/programming-guide.html\nhttp://www.ne.jp/asahi/hishidama/home/tech/scala/spark/partition.html\n\n", "tags": ["Scala", "Spark"]}