{"context": " More than 1 year has passed since last update.\n\n<ENGLISH>\nHello - I hope you have a good day. Happy weekend should be happy cording day  \nOk, today I will not proceed the scripting and I'd like to modify previous script. The script is below from #2:\nscraper = [ \n        [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n        [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n        ]\nc = 0\nfor domain in scraper:\n    print url, domain[0]\n    if re.search( domain[0], url):\n        break\n    c += 1\n\nresponse = urllib2.urlopen(url)\nhtml = response.read()\n\nsoup = BeautifulSoup( html, \"lxml\" )\nsoup.originalEnoding\ntag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\ntext = \"\"\nfor con in tag.contents:\n    p = re.compile(r'<.*?>')\n    text += p.sub('', con.encode('utf8'))\n\nYes, it works, but want to use (1) BeautifulSoup instead of regular expression and (2)Hash list instead of counting inside for. \n(1) BeautifulSoup\nsoup = BeautifulSoup( html, \"lxml\" )\nsoup.originalEnoding\ntag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\ntext = \"\"\nfor con in tag.contents:\n    p = re.compile(r'<.*?>')\n    text += p.sub('', con.encode('utf8'))\n\nRegular Expression is strong tool, but I have to learn BeautifulSoup more. Beautiful Soup is using unique type for it's string, and we can check how to use it in user's guide. \nI modified it as below. \n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    soup2 = BeautifulSoup(tag.encode('utf8'), \"lxml\")\n    print \"\".join([string.encode('utf8') for string in soup2.strings])\n\nLooks smarter?  \nyou got another soup for getting strings. Which do you like? \n(2) Hash List for splitting. \nWatch out! \nscraper = [ \n        [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n        [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n        ]\nc = 0\nfor domain in scraper:\n    print url, domain[0]\n    if re.search( domain[0], url):\n        break\n    c += 1\n\nTo get splitter strings for each web site, used c as count up integer. That's not cool. So I modified as below. \n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    numHash = {}\n    for i in range(len(scraper)):\n        numHash[scraper[i][0]] = i \n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            c = numHash[domain[0]]\n            break\n\nyes, it becomes longer, but I think it's much better than previous, isn't it? \nGreat, next I hope I can proceed to next step... It will be getting elements for learning. \n\n<\u65e5\u672c\u8a9e>\n\u306f\u3044\u3001\u3069\u30fc\u3082\u3002\u9031\u672b\u3067\u3059\u306d\u3002\u3088\u3044\u9031\u672b\u3092\u8fce\u3048\u308b\u305f\u3081\u306b\u3082\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3044\u305d\u3057\u307f\u307e\u3057\u3087\u3046\u3002\n\u4eca\u65e5\u306f\u6b21\u306e\u9032\u3081\u308b\u524d\u306b#2\u3067\u3084\u3063\u305f\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u4fee\u6b63\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u3061\u3089\u3067\u3059\u306d\u3002\nscraper = [ \n        [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n        [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n        ]\nc = 0\nfor domain in scraper:\n    print url, domain[0]\n    if re.search( domain[0], url):\n        break\n    c += 1\n\nresponse = urllib2.urlopen(url)\nhtml = response.read()\n\nsoup = BeautifulSoup( html, \"lxml\" )\nsoup.originalEnoding\ntag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\ntext = \"\"\nfor con in tag.contents:\n    p = re.compile(r'<.*?>')\n    text += p.sub('', con.encode('utf8'))\n\n\u3053\u308c\u3067\u3082\u52d5\u304f\u3093\u3067\u3059\u304c\u30fb\u30fb\u30fb\u5909\u66f4\u70b9\u3068\u3057\u3066\u306f\uff08\uff11\uff09\u30bf\u30b0\u9664\u53bb\u3092\u6b63\u898f\u8868\u73fe\u3067\u306f\u306a\u304fBeautifulSoup\u3092\u4f7f\u3046\u3000\u306e\u3068\u3001\uff08\uff12\uff09\u533a\u5207\u308a\u6587\u5b57\u306e\u9078\u629e\u306b\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7\u3067\u306f\u306a\u304f\u3001\u30cf\u30c3\u30b7\u30e5\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u3000\u3068\u3044\u3063\u305f\u70b9\u306b\u306a\u308a\u307e\u3059\u3002\n(1) BeautifulSoup\u3092\u4f7f\u3046\nsoup = BeautifulSoup( html, \"lxml\" )\nsoup.originalEnoding\ntag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\ntext = \"\"\nfor con in tag.contents:\n    p = re.compile(r'<.*?>')\n    text += p.sub('', con.encode('utf8'))\n\n\u6b63\u898f\u8868\u73fe\u306f\u3068\u3066\u3082\u4fbf\u5229\u306a\u306e\u3067\u3059\u304c\u3001\u305b\u3063\u304b\u304f\u306eBeautifulSoup\u3092\u3082\u3063\u3068\u6709\u52b9\u306b\u3067\u304d\u306a\u3044\u304b\u3068\u304a\u3082\u3063\u305f\u6b21\u7b2c\u3067\u3059\u3002BS\u3067\u3001\u4e2d\u306e\u6587\u5b57\u5217\u306e\u629c\u51fa\u306e\u305f\u3081\u306e\u30c4\u30fc\u30eb\u306f\u305d\u308d\u3044\u307e\u304f\u3063\u3066\u3044\u308b\u3093\u3067\u3059\u304c\u3001\u72ec\u7279\u306e\u6587\u5b57\u5217\u5f62\u5f0f\u304c\u3042\u308b\u306e\u3067\u3001\u6700\u521d\u306f\u3068\u3063\u3064\u304d\u306b\u304f\u304b\u3063\u305f\u3002\u3067\u3082\u3057\u3063\u304b\u308a\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5316\u3082\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u3042\u3068\u306f\u4f7f\u3044\u307e\u304f\u3063\u3066\u6163\u308c\u308b\u3057\u304b\u306a\u3044\u3067\u3059\u304b\u306d\u3002\n\u3067\u3001\u5909\u66f4\u5f8c\u304c\u3053\u308c\u3060\u3063\uff01\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    soup2 = BeautifulSoup(tag.encode('utf8'), \"lxml\")\n    print \"\".join([string.encode('utf8') for string in soup2.strings])\n\n\u304b\u3063\u3053\u3088\u304f\u306a\u3063\u305f\u6c17\u304c\u3057\u307e\u305b\u3093\u304b\uff1f\u30b9\u30fc\u30d7\u3092\u3082\u3046\u4e00\u56de\u304a\u4ee3\u308f\u308a\u3057\u3066\u3001\u30bf\u30b0\u306e\u4e2d\u306e\u6587\u5b57\u5217\u3092\u5f15\u304d\u51fa\u3059\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\n(2) \u533a\u5207\u308a\u6587\u5b57\u306b\u30cf\u30c3\u30b7\u30e5\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\n\u3053\u3053\u306e\u3053\u3068\u3002\nscraper = [ \n        [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n        [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n        ]\nc = 0\nfor domain in scraper:\n    print url, domain[0]\n    if re.search( domain[0], url):\n        break\n    c += 1\n\nC\u5909\u6570\u3092\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7\u3057\u3066\u3001\u533a\u5207\u308a\u6587\u5b57\u306e\u756a\u53f7\u3092\u8abf\u6574\u3057\u3066\u3044\u308b\u3088\u3046\u306a\u5f62\u3002\u3046\u30fc\u3093\u306a\u3093\u304b\u3076\u3061\u3083\u3044\u304f\uff1f\u3067\u3001\u7d20\u6575\u306b\u5927\u5909\u8eab\u3002 \n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    numHash = {}\n    for i in range(len(scraper)):\n        numHash[scraper[i][0]] = i \n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            c = numHash[domain[0]]\n            break\n\n\u4e88\u60f3\u306b\u53cd\u3057\u3066\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u9577\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3067\u3082\u3053\u3063\u3061\u306e\u307b\u3046\u304c\u3068\u3063\u3066\u3082\u597d\u304d\u3067\u3059\u3002\u3082\u3046\u3061\u3087\u3063\u3068\u304d\u308c\u3044\u306b\u3067\u304d\u308b\u304b\u306a\u3041\u3002\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001\u4eca\u56de\u306f\u81ea\u5df1\u6e80\u8db3\u7684\u306a\u4fee\u6b63\u3092\u307b\u3069\u3053\u3057\u307e\u3057\u305f\u3002\u6b21\u56de\u306f\u6b21\u306b\u9032\u3081\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\u5b66\u7fd2\u3082\u3068\u3068\u306a\u308b\u30ea\u30f3\u30af\u3068\u30bf\u30b0\u30ea\u30b9\u30c8\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3067\u3059\u3002\u3044\u3064\u6a5f\u68b0\u5b66\u7fd2\u306b\u305f\u3069\u308a\u3064\u3051\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u3002\u3002\u3002\u305d\u308d\u305d\u308d\u8a50\u6b3a\u3068\u3044\u308f\u308c\u305d\u3046\u3002\n### \\<ENGLISH>\n\nHello - I hope you have a good day. Happy weekend should be happy cording day :smile: \n\nOk, today I will not proceed the scripting and I'd like to modify previous script. The script is below from #2:\n\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\n    response = urllib2.urlopen(url)\n    html = response.read()\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\nYes, it works, but want to use (1) BeautifulSoup instead of regular expression and (2)Hash list instead of counting inside for. \n\n(1) BeautifulSoup\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\nRegular Expression is strong tool, but I have to learn BeautifulSoup more. Beautiful Soup is using unique type for it's string, and we can check how to use it in [user's guide](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). \nI modified it as below. \n\n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n        soup2 = BeautifulSoup(tag.encode('utf8'), \"lxml\")\n        print \"\".join([string.encode('utf8') for string in soup2.strings])\n\nLooks smarter? :satisfied: \nyou got another soup for getting strings. Which do you like? \n\n(2) Hash List for splitting. \nWatch out! \n\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\nTo get splitter strings for each web site, used c as count up integer. That's not cool. So I modified as below. \n\n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n        numHash = {}\n        for i in range(len(scraper)):\n            numHash[scraper[i][0]] = i \n        for domain in scraper:\n            print url, domain[0]\n            if re.search( domain[0], url):\n                c = numHash[domain[0]]\n                break\n\nyes, it becomes longer, but I think it's much better than previous, isn't it? \n\nGreat, next I hope I can proceed to next step... It will be getting elements for learning. \n\n### \\<\u65e5\u672c\u8a9e>\n\n\u306f\u3044\u3001\u3069\u30fc\u3082\u3002\u9031\u672b\u3067\u3059\u306d\u3002\u3088\u3044\u9031\u672b\u3092\u8fce\u3048\u308b\u305f\u3081\u306b\u3082\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3044\u305d\u3057\u307f\u307e\u3057\u3087\u3046\u3002\n\u4eca\u65e5\u306f\u6b21\u306e\u9032\u3081\u308b\u524d\u306b#2\u3067\u3084\u3063\u305f\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u4fee\u6b63\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u3053\u3061\u3089\u3067\u3059\u306d\u3002\n\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\n    response = urllib2.urlopen(url)\n    html = response.read()\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\n\u3053\u308c\u3067\u3082\u52d5\u304f\u3093\u3067\u3059\u304c\u30fb\u30fb\u30fb\u5909\u66f4\u70b9\u3068\u3057\u3066\u306f\uff08\uff11\uff09\u30bf\u30b0\u9664\u53bb\u3092\u6b63\u898f\u8868\u73fe\u3067\u306f\u306a\u304fBeautifulSoup\u3092\u4f7f\u3046\u3000\u306e\u3068\u3001\uff08\uff12\uff09\u533a\u5207\u308a\u6587\u5b57\u306e\u9078\u629e\u306b\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7\u3067\u306f\u306a\u304f\u3001\u30cf\u30c3\u30b7\u30e5\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u3000\u3068\u3044\u3063\u305f\u70b9\u306b\u306a\u308a\u307e\u3059\u3002\n\n(1) BeautifulSoup\u3092\u4f7f\u3046\n\n    soup = BeautifulSoup( html, \"lxml\" )\n    soup.originalEnoding\n    tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n    text = \"\"\n    for con in tag.contents:\n        p = re.compile(r'<.*?>')\n        text += p.sub('', con.encode('utf8'))\n\n\u6b63\u898f\u8868\u73fe\u306f\u3068\u3066\u3082\u4fbf\u5229\u306a\u306e\u3067\u3059\u304c\u3001\u305b\u3063\u304b\u304f\u306eBeautifulSoup\u3092\u3082\u3063\u3068\u6709\u52b9\u306b\u3067\u304d\u306a\u3044\u304b\u3068\u304a\u3082\u3063\u305f\u6b21\u7b2c\u3067\u3059\u3002BS\u3067\u3001\u4e2d\u306e\u6587\u5b57\u5217\u306e\u629c\u51fa\u306e\u305f\u3081\u306e\u30c4\u30fc\u30eb\u306f\u305d\u308d\u3044\u307e\u304f\u3063\u3066\u3044\u308b\u3093\u3067\u3059\u304c\u3001\u72ec\u7279\u306e\u6587\u5b57\u5217\u5f62\u5f0f\u304c\u3042\u308b\u306e\u3067\u3001\u6700\u521d\u306f\u3068\u3063\u3064\u304d\u306b\u304f\u304b\u3063\u305f\u3002\u3067\u3082\u3057\u3063\u304b\u308a[\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5316](http://kondou.com/BS4/)\u3082\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u3042\u3068\u306f\u4f7f\u3044\u307e\u304f\u3063\u3066\u6163\u308c\u308b\u3057\u304b\u306a\u3044\u3067\u3059\u304b\u306d\u3002\n\n\u3067\u3001\u5909\u66f4\u5f8c\u304c\u3053\u308c\u3060\u3063\uff01\n\n        soup = BeautifulSoup( html, \"lxml\" )\n        soup.originalEnoding\n        tag = soup.find( scraper[c][1], {scraper[c][2] : scraper[c][3]})\n        soup2 = BeautifulSoup(tag.encode('utf8'), \"lxml\")\n        print \"\".join([string.encode('utf8') for string in soup2.strings])\n\n\u304b\u3063\u3053\u3088\u304f\u306a\u3063\u305f\u6c17\u304c\u3057\u307e\u305b\u3093\u304b\uff1f\u30b9\u30fc\u30d7\u3092\u3082\u3046\u4e00\u56de\u304a\u4ee3\u308f\u308a\u3057\u3066\u3001\u30bf\u30b0\u306e\u4e2d\u306e\u6587\u5b57\u5217\u3092\u5f15\u304d\u51fa\u3059\u3088\u3046\u306b\u5909\u66f4\u3057\u307e\u3057\u305f\u3002\n\n(2) \u533a\u5207\u308a\u6587\u5b57\u306b\u30cf\u30c3\u30b7\u30e5\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\n\u3053\u3053\u306e\u3053\u3068\u3002\n\n    scraper = [ \n            [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n            [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n            ]\n    c = 0\n    for domain in scraper:\n        print url, domain[0]\n        if re.search( domain[0], url):\n            break\n        c += 1\n\nC\u5909\u6570\u3092\u30ab\u30a6\u30f3\u30c8\u30a2\u30c3\u30d7\u3057\u3066\u3001\u533a\u5207\u308a\u6587\u5b57\u306e\u756a\u53f7\u3092\u8abf\u6574\u3057\u3066\u3044\u308b\u3088\u3046\u306a\u5f62\u3002\u3046\u30fc\u3093\u306a\u3093\u304b\u3076\u3061\u3083\u3044\u304f\uff1f\u3067\u3001\u7d20\u6575\u306b\u5927\u5909\u8eab\u3002 \n\n        scraper = [ \n                [\"hatenablog.com\",\"div\",\"class\",\"entry-content\"],\n                [\"qiita.com\",\"section\",\"itemprop\", \"articleBody\"]\n                ]\n        numHash = {}\n        for i in range(len(scraper)):\n            numHash[scraper[i][0]] = i \n        for domain in scraper:\n            print url, domain[0]\n            if re.search( domain[0], url):\n                c = numHash[domain[0]]\n                break\n\n\u4e88\u60f3\u306b\u53cd\u3057\u3066\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u9577\u304f\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3067\u3082\u3053\u3063\u3061\u306e\u307b\u3046\u304c\u3068\u3063\u3066\u3082\u597d\u304d\u3067\u3059\u3002\u3082\u3046\u3061\u3087\u3063\u3068\u304d\u308c\u3044\u306b\u3067\u304d\u308b\u304b\u306a\u3041\u3002\n\n\u3068\u3044\u3046\u308f\u3051\u3067\u3001\u4eca\u56de\u306f\u81ea\u5df1\u6e80\u8db3\u7684\u306a\u4fee\u6b63\u3092\u307b\u3069\u3053\u3057\u307e\u3057\u305f\u3002\u6b21\u56de\u306f\u6b21\u306b\u9032\u3081\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\u5b66\u7fd2\u3082\u3068\u3068\u306a\u308b\u30ea\u30f3\u30af\u3068\u30bf\u30b0\u30ea\u30b9\u30c8\u306e\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3067\u3059\u3002\u3044\u3064\u6a5f\u68b0\u5b66\u7fd2\u306b\u305f\u3069\u308a\u3064\u3051\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u3002\u3002\u3002\u305d\u308d\u305d\u308d\u8a50\u6b3a\u3068\u3044\u308f\u308c\u305d\u3046\u3002\n\n", "tags": ["MachineLearning", "Python", "scraping"]}