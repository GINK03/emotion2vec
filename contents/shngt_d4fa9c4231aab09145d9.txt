{"context": "\n\nBeutifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n\u3088\u304f\u3042\u308bURL\u304c\u9023\u756a\u306b\u306a\u3063\u3066\u3044\u308b\u30da\u30fc\u30b8\u3067\u3001\u5f8c\u304b\u3089\u307e\u3068\u3081\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u306bURL\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u305f\u306e\u3067\u30e1\u30e2\n\n\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n$ apt-get install lxml-python\n$ pip install beautifulsoup4\n\n\n\u30bd\u30fc\u30b9\n\nscraper.py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\ntry:\n    # Python 3\n    from urllib import request\nexcept ImportError:\n    # Python 2\n    import urllib2 as request\n\nfrom bs4 import BeautifulSoup\nimport codecs\nimport time\n\ndef getSoup(url):\n    response = request.urlopen(url)\n    body = response.read()\n    # Parse HTML\n    return BeautifulSoup(body, 'lxml')\n\nwait_sec = 3\ndomain = 'http://hoge.com'\nresult_file = 'list.txt'\ni = 1\nwhile(True):\n    url = '{domain}/{index:0>2}/'.format(domain = domain, index = i)\n    try:\n        soup = getSoup(url)\n    except IOError:\n        break\n\n    div = soup.find('div', attrs = {'id': 'div_id'})\n    all_a = div.find_all('a', attrs = {'class': 'a_class'})\n    src_list = []\n    for a in all_a:\n        src_list.append(a.img['src'])\n    with codecs.open(result_file, 'a', 'utf-8') as f:\n        f.write('\\n'.join(src_list))\n    print(i)\n    i += 1\n\n    time.sleep(wait_sec)\n\n\n\n\u53c2\u8003\u30da\u30fc\u30b8\nPython: BeautifulSoup4 \u3092\u4f7f\u3063\u3066 Web \u30b5\u30a4\u30c8\u3092\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3059\u308b\nPython\u3068Beautiful Soup\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n# BeutifulSoup4\u3067WEB\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n\n\u3088\u304f\u3042\u308bURL\u304c\u9023\u756a\u306b\u306a\u3063\u3066\u3044\u308b\u30da\u30fc\u30b8\u3067\u3001\u5f8c\u304b\u3089\u307e\u3068\u3081\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u306bURL\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u305f\u306e\u3067\u30e1\u30e2\n\n## \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\n```\n$ apt-get install lxml-python\n$ pip install beautifulsoup4\n```\n\n## \u30bd\u30fc\u30b9\n\n```python:scraper.py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\ntry:\n    # Python 3\n    from urllib import request\nexcept ImportError:\n    # Python 2\n    import urllib2 as request\n\nfrom bs4 import BeautifulSoup\nimport codecs\nimport time\n\ndef getSoup(url):\n    response = request.urlopen(url)\n    body = response.read()\n    # Parse HTML\n    return BeautifulSoup(body, 'lxml')\n\nwait_sec = 3\ndomain = 'http://hoge.com'\nresult_file = 'list.txt'\ni = 1\nwhile(True):\n    url = '{domain}/{index:0>2}/'.format(domain = domain, index = i)\n    try:\n        soup = getSoup(url)\n    except IOError:\n        break\n\n    div = soup.find('div', attrs = {'id': 'div_id'})\n    all_a = div.find_all('a', attrs = {'class': 'a_class'})\n    src_list = []\n    for a in all_a:\n        src_list.append(a.img['src'])\n    with codecs.open(result_file, 'a', 'utf-8') as f:\n        f.write('\\n'.join(src_list))\n    print(i)\n    i += 1\n\n    time.sleep(wait_sec)\n```\n\n## \u53c2\u8003\u30da\u30fc\u30b8\n\n[Python: BeautifulSoup4 \u3092\u4f7f\u3063\u3066 Web \u30b5\u30a4\u30c8\u3092\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u3059\u308b](http://momijiame.tumblr.com/post/114227737756/python-beautifulsoup4-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6-web-%E3%82%B5%E3%82%A4%E3%83%88%E3%82%92%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%94%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8B)\n\n[Python\u3068Beautiful Soup\u3067\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0](http://qiita.com/itkr/items/513318a9b5b92bd56185)\n", "tags": ["Python", "scraping", "BeautifulSoup"]}