{"context": " More than 1 year has passed since last update.\n\nspark\nspark\u3092download\u3057\u3066\u3001build\u3059\u308b\u3002\nhttp://spark.apache.org/downloads.html\ncd spark-1.5.0\nmake_distribution.sh\n\n\nspark-cassandra-connector\nspark-cassandra-connector\u3092git clone\u3057\u3066 build\u203b\u3053\u3053\u3067\u3001\u30d3\u30eb\u30c9\u5931\u6557\u3057\u3066\u3060\u3081\nhttps://github.com/datastax/spark-cassandra-connector/blob/master/doc/12_building_and_artifacts.md\ngit clone https://github.com/datastax/spark-cassandra-connector\ncd spark-cassandra-connector\ngit checkout b1.5\nsbt package\nsbt assembly\n\n\u3053\u308c\u3067\u3001spark-cassandra-connector/target/scala-2.10\u4ee5\u4e0b\u306bassembly jar\u304c\u751f\u6210\u3055\u308c\u308b\u3002\nspark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u5834\u5408\u306f\u3001maven cetral\u304b\u3089jar\u3092\u500b\u5225\u306bdownload\u3057\u3066\u4f7f\u3046\u3002\ndownload\u3057\u305f\u306e\u306f\u3001\n\nguava-0.18\nspark-cassandra-connector_2.10-1.5.0-M2.jar\ncassandra-driver-core-2.2.0-rc2.jar\n\nguava-0.18\u306f\u306a\u305c\u304b\u306a\u3044\u3068\u6012\u3089\u308c\u308b\u306e\u3067\u3001\u8ffd\u52a0\u3057\u305f\u3002\n\nspark-shell\u3092\u8d77\u52d5\n\nspark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u6210\u529f\u3057\u305f\u5834\u5408\nbin/spark-shell --conf spark.cassandra.connection.host=127.0.0.1\n--jars spark-cassandra-connector-assembly-1.5.0-M2-SNAPSHOT.jar \n\n\n\nspark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3057\u305f\u5834\u5408\nbin/spark-shell --jars spark-cassandra-connector_2.10-1.5.0-M2.jar,cassandra-driver-core-2.2.0-rc2.jar,guava-18.0.jar --conf spark.cassandra.connection.host=127.0.0.1\n\non mac and linux \n\nmac\u3067local\u306ecassandra\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u306f 127.0.0.1, cassandra.yaml\u306eseeds\u306b\u306f 127.0.0.1\u3092\u8a18\u8ff0\nlinux\u3067\u5206\u6563\u30e2\u30fc\u30c9\u3067cassandra\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u306f host=ipaddress of host, \u3053\u306eip\u306fcassandra.yaml\u306eseeds\u306b\u66f8\u3044\u3066\u3042\u308bip\n\n\ncassandra keyspace, table\u306e\u4f5c\u6210\u52d5\u4f5c\u78ba\u8a8d\nspark-shell\u3067\u4ee5\u4e0b\u3092\u5b9f\u884c\nimport com.datastax.spark.connector._ //Imports basic rdd functions\nimport com.datastax.spark.connector.cql._ //(Optional) Imports java driver helper functions\n\nval c = CassandraConnector(sc.getConf)\nc.withSessionDo ( session => session.execute(\"CREATE KEYSPACE test_from_spark WITH replication={'class':'SimpleStrategy', 'replication_factor':1}\"))\nc.withSessionDo ( session => session.execute(\"CREATE TABLE test_from_spark.fun (k int PRIMARY KEY, v int)\"))\n\n\ncqlsh\u3067keyspace, table\u304c\u3067\u304d\u3066\u3044\u308b\u304b\u306e\u78ba\u8a8d\ncqlsh:mykeyspace> describe keyspaces;\n\nsystem_auth  mykeyspace          test           test_from_spark\nsystem       system_distributed  system_traces\n\n\n\ndata insert\u306e\u52d5\u4f5c\u78ba\u8a8d\nscala> c.withSessionDo ( session => session.execute(\"insert into test_from_spark.fun (k,v) values(1, 10)\"))\nres6: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n\n\ncqlsh\u3067table\u306b\u5165\u3063\u3066\u3044\u308b\u304b\u306e\u78ba\u8a8d\ncqlsh:test_from_spark> select * from fun;\n\n k | v\n---+----\n 1 | 10\n 2 | 20\n\n(2 rows)\n\n\n\ncassandraTable\u3067table\u3092RDD\u3068\u3057\u3066\u53d6\u5f97\u52d5\u4f5c\u78ba\u8a8d\nscala> val d = sc.cassandraTable(\"test_from_spark\", \"fun\");\nd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:15\n\nscala> d.count\nres4: Long = 2                                                                  \n                                                                          scala> d\nres7: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:15\n\nscala> d.collect\nres9: Array[com.datastax.spark.connector.CassandraRow] = Array(CassandraRow{k: 1, v: 10}, CassandraRow{k: 2, v: 20})\n\nscala> d.collect.foreach(println)\nCassandraRow{k: 1, v: 10}                                                       \nCassandraRow{k: 2, v: 20}\n\n\n\n\u5b9f\u65bd\u4e2d\u306e\u30a8\u30e9\u30fc\u60c5\u5831\n\ncom.google.util.concurrent.**\u304cclasspath\u306b\u3042\u308a\u307e\u305b\u3093\u30a8\u30e9\u30fc => guave.0.18.jar\u3092\u81ea\u524d\u3067jars\u306b\u8ffd\u52a0\u3059\u308b\u3068\u6cbb\u3063\u305f\u3002\ncassandra connect\u306e\u52d5\u4f5c\u78ba\u8a8d\u4e2d\u306b\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3000\u203bcassandra\u306e\u8a2d\u5b9a\u3092\u3044\u3058\u3063\u305f\u3089\u6cbb\u3063\u305f\uff1f\n\n\nscala\nscala> c.withSessionDo ( session => session.execute(\"CREATE KEYSPACE test WITH replication={'class':'SimpleStrategy', 'replication_factor':1}\"))\n15/11/14 11:21:10 WARN NettyUtil: Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.\njava.io.IOException: Failed to open native connection to Cassandra at {192.168.11.2}:9042\n    at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:164)\n    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:150)\n    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:150)\n    at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n    at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n    at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n    at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:41)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)\n    at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)\n    at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47)\n    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:49)\n    at $iwC$$iwC$$iwC.<init>(<console>:51)\n    at $iwC$$iwC.<init>(<console>:53)\n    at $iwC.<init>(<console>:55)\n    at <init>(<console>:57)\n    at .<init>(<console>:61)\n    at .<clinit>(<console>)\n    at .<init>(<console>:7)\n    at .<clinit>(<console>)\n    at $print(<console>)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n    at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n    at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n    at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)\n    at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)\n    at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)\n    at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)\n    at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)\n    at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)\n    at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)\n    at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n    at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n    at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n    at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)\n    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)\n    at org.apache.spark.repl.Main$.main(Main.scala:31)\n    at org.apache.spark.repl.Main.main(Main.scala)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:680)\n    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /192.168.11.2:9042 (com.datastax.driver.core.TransportException: [/192.168.11.2:9042] Cannot connect))\n    at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:229)\n    at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:84)\n    at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1264)\n    at com.datastax.driver.core.Cluster.getMetadata(Cluster.java:338)\n    at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:157)\n    ... 56 more\n\n\nscala> \n\n\n\n# spark\n\nspark\u3092download\u3057\u3066\u3001build\u3059\u308b\u3002\n\nhttp://spark.apache.org/downloads.html\n\n```\ncd spark-1.5.0\nmake_distribution.sh\n```\n# spark-cassandra-connector\n\n~~spark-cassandra-connector\u3092git clone\u3057\u3066 build\u203b\u3053\u3053\u3067\u3001\u30d3\u30eb\u30c9\u5931\u6557\u3057\u3066\u3060\u3081~~\n\nhttps://github.com/datastax/spark-cassandra-connector/blob/master/doc/12_building_and_artifacts.md\n\n```\ngit clone https://github.com/datastax/spark-cassandra-connector\ncd spark-cassandra-connector\ngit checkout b1.5\nsbt package\nsbt assembly\n```\n\n\u3053\u308c\u3067\u3001spark-cassandra-connector/target/scala-2.10\u4ee5\u4e0b\u306bassembly jar\u304c\u751f\u6210\u3055\u308c\u308b\u3002\n\n\nspark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u5834\u5408\u306f\u3001maven cetral\u304b\u3089jar\u3092\u500b\u5225\u306bdownload\u3057\u3066\u4f7f\u3046\u3002\n\ndownload\u3057\u305f\u306e\u306f\u3001\n\n- guava-0.18\n- spark-cassandra-connector_2.10-1.5.0-M2.jar\n- cassandra-driver-core-2.2.0-rc2.jar\n\nguava-0.18\u306f\u306a\u305c\u304b\u306a\u3044\u3068\u6012\u3089\u308c\u308b\u306e\u3067\u3001\u8ffd\u52a0\u3057\u305f\u3002\n\n# spark-shell\u3092\u8d77\u52d5\n## spark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u6210\u529f\u3057\u305f\u5834\u5408\n```\nbin/spark-shell --conf spark.cassandra.connection.host=127.0.0.1\n--jars spark-cassandra-connector-assembly-1.5.0-M2-SNAPSHOT.jar \n\n```\n\n\n## spark-cassandra-connector\u306e\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3057\u305f\u5834\u5408\n```\nbin/spark-shell --jars spark-cassandra-connector_2.10-1.5.0-M2.jar,cassandra-driver-core-2.2.0-rc2.jar,guava-18.0.jar --conf spark.cassandra.connection.host=127.0.0.1\n```\non mac and linux \n\n- mac\u3067local\u306ecassandra\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u306f 127.0.0.1, cassandra.yaml\u306eseeds\u306b\u306f 127.0.0.1\u3092\u8a18\u8ff0\n- linux\u3067\u5206\u6563\u30e2\u30fc\u30c9\u3067cassandra\u3067\u306e\u52d5\u4f5c\u78ba\u8a8d\u306f host=ipaddress of host, \u3053\u306eip\u306fcassandra.yaml\u306eseeds\u306b\u66f8\u3044\u3066\u3042\u308bip\n\n\n# cassandra keyspace, table\u306e\u4f5c\u6210\u52d5\u4f5c\u78ba\u8a8d\n\nspark-shell\u3067\u4ee5\u4e0b\u3092\u5b9f\u884c\n\n```\nimport com.datastax.spark.connector._ //Imports basic rdd functions\nimport com.datastax.spark.connector.cql._ //(Optional) Imports java driver helper functions\n\nval c = CassandraConnector(sc.getConf)\nc.withSessionDo ( session => session.execute(\"CREATE KEYSPACE test_from_spark WITH replication={'class':'SimpleStrategy', 'replication_factor':1}\"))\nc.withSessionDo ( session => session.execute(\"CREATE TABLE test_from_spark.fun (k int PRIMARY KEY, v int)\"))\n```\n\n## cqlsh\u3067keyspace, table\u304c\u3067\u304d\u3066\u3044\u308b\u304b\u306e\u78ba\u8a8d\n\n```\ncqlsh:mykeyspace> describe keyspaces;\n\nsystem_auth  mykeyspace          test           test_from_spark\nsystem       system_distributed  system_traces\n\n```\n\n# data insert\u306e\u52d5\u4f5c\u78ba\u8a8d\n\n```\nscala> c.withSessionDo ( session => session.execute(\"insert into test_from_spark.fun (k,v) values(1, 10)\"))\nres6: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n```\n\n\n## cqlsh\u3067table\u306b\u5165\u3063\u3066\u3044\u308b\u304b\u306e\u78ba\u8a8d\n\n```\ncqlsh:test_from_spark> select * from fun;\n\n k | v\n---+----\n 1 | 10\n 2 | 20\n\n(2 rows)\n\n```\n\n# cassandraTable\u3067table\u3092RDD\u3068\u3057\u3066\u53d6\u5f97\u52d5\u4f5c\u78ba\u8a8d\n\n```\nscala> val d = sc.cassandraTable(\"test_from_spark\", \"fun\");\nd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:15\n\nscala> d.count\nres4: Long = 2                                                                  \n                                                                          scala> d\nres7: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:15\n\nscala> d.collect\nres9: Array[com.datastax.spark.connector.CassandraRow] = Array(CassandraRow{k: 1, v: 10}, CassandraRow{k: 2, v: 20})\n\nscala> d.collect.foreach(println)\nCassandraRow{k: 1, v: 10}                                                       \nCassandraRow{k: 2, v: 20}\n\n```\n\n\n\n# \u5b9f\u65bd\u4e2d\u306e\u30a8\u30e9\u30fc\u60c5\u5831\n\n- com.google.util.concurrent.**\u304cclasspath\u306b\u3042\u308a\u307e\u305b\u3093\u30a8\u30e9\u30fc => guave.0.18.jar\u3092\u81ea\u524d\u3067jars\u306b\u8ffd\u52a0\u3059\u308b\u3068\u6cbb\u3063\u305f\u3002\n\n- cassandra connect\u306e\u52d5\u4f5c\u78ba\u8a8d\u4e2d\u306b\u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u3000\u203bcassandra\u306e\u8a2d\u5b9a\u3092\u3044\u3058\u3063\u305f\u3089\u6cbb\u3063\u305f\uff1f\n\n```lang:scala\nscala> c.withSessionDo ( session => session.execute(\"CREATE KEYSPACE test WITH replication={'class':'SimpleStrategy', 'replication_factor':1}\"))\n15/11/14 11:21:10 WARN NettyUtil: Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.\njava.io.IOException: Failed to open native connection to Cassandra at {192.168.11.2}:9042\n\tat com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:164)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:150)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:150)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:41)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:49)\n\tat $iwC$$iwC$$iwC.<init>(<console>:51)\n\tat $iwC$$iwC.<init>(<console>:53)\n\tat $iwC.<init>(<console>:55)\n\tat <init>(<console>:57)\n\tat .<init>(<console>:61)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)\n\tat org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)\n\tat org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)\n\tat org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)\n\tat org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)\n\tat org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n\tat scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n\tat org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)\n\tat org.apache.spark.repl.Main$.main(Main.scala:31)\n\tat org.apache.spark.repl.Main.main(Main.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:680)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /192.168.11.2:9042 (com.datastax.driver.core.TransportException: [/192.168.11.2:9042] Cannot connect))\n\tat com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:229)\n\tat com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:84)\n\tat com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1264)\n\tat com.datastax.driver.core.Cluster.getMetadata(Cluster.java:338)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:157)\n\t... 56 more\n\n\nscala> \n\n```\n", "tags": ["Spark", "Cassandra"]}