{"context": "Jupyter\u304b\u3089pyspark\u3092\u4f7f\u3046\u3068\u304d\u306b\u3001 spark-csv\u306a\u3069 spark packages\u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\nspark context\u306e\u521d\u671f\u5316\u3092\u884c\u306a\u3046\u524d\u306b\u3001\u4e0b\u8a18\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066pyspark\u306e\u5b9f\u884c\u30b3\u30de\u30f3\u30c9\u306e\u5f15\u6570\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\nimport os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell'\nimport pyspark\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.11\u306e\u5834\u5408\u3092\u60f3\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.10\u306e\u5834\u5408\u306f\u5f15\u6570\u3067\u6e21\u3057\u3066\u3044\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\u3042\u3068\u306f\u3001\u3044\u3064\u3082\u901a\u308aspark csv\u3092\u4f7f\u3048\u307e\u3059\u3002\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nconf = SparkConf()\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.read.format('com.databricks.spark.csv').load('hdfs://path/to/example.csv')\n\nprint(len(df.collect()))\n\nJupyter\u304b\u3089pyspark\u3092\u4f7f\u3046\u3068\u304d\u306b\u3001 [spark-csv](https://github.com/databrics/spark-csv)\u306a\u3069 [spark packages](http://spark-packages.org/)\u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\nspark context\u306e\u521d\u671f\u5316\u3092\u884c\u306a\u3046\u524d\u306b\u3001\u4e0b\u8a18\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u3066`pyspark`\u306e\u5b9f\u884c\u30b3\u30de\u30f3\u30c9\u306e\u5f15\u6570\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002\n\n```py\nimport os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell'\nimport pyspark\n```\n\n\u3053\u306e\u4f8b\u3067\u306f\u3001scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.11\u306e\u5834\u5408\u3092\u60f3\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002scala\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c2.10\u306e\u5834\u5408\u306f\u5f15\u6570\u3067\u6e21\u3057\u3066\u3044\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u3042\u3068\u306f\u3001\u3044\u3064\u3082\u901a\u308aspark csv\u3092\u4f7f\u3048\u307e\u3059\u3002\n\n```py\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nconf = SparkConf()\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.read.format('com.databricks.spark.csv').load('hdfs://path/to/example.csv')\n\nprint(len(df.collect()))\n```\n", "tags": ["Pyspark", "Spark", "Jupyter"]}