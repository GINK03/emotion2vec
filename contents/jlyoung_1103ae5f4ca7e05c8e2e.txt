{"context": "I was once asked for a tutorial that described how to use pySpark to read data from a Hive table and write to a JDBC datasource like PostgreSQL or SQL Server. Not being able to find a suitable tutorial, I decided to write one.\nThis demo creates a python script which uses pySpark to read data from a Hive table into a DataFrame, perform operations on the DataFrame, and write the results out to a JDBC DataSource (PostgreSQL database). I am using Spark 1.6.2.\n\nGetting some CSV data to populate into Hive\nI will use crime data from the City of Chicago in this tutorial. The data is available in CSV format from the following link:\nhttps://catalog.data.gov/dataset/crimes-2001-to-present-398a4\n[root@jyoung-hdp25-node-1 tmp]# mkdir -p /tmp/crime\n[root@jyoung-hdp25-node-1 tmp]# cd /tmp/crime\n[root@jyoung-hdp25-node-1 crime]# curl -o crime -L https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n[root@jyoung-hdp25-node-1 crime]# cd /tmp/\n[root@jyoung-hdp25-node-1 tmp]# chmod -R 777 crime/\n\n\nCopy the CSV data to HDFS\nI'm using a kerberized Hortonworks Data Platform (HDP) cluster, so I'll use kinit to authenticate.\nroot@jyoung-hdp25-node-1 tmp]# su - hdfs\n[hdfs@jyoung-hdp25-node-1 ~]$ kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-cluster1@EXAMPLE.COM\n[hdfs@jyoung-hdp25-node-1 ~]$ klist\nTicket cache: FILE:/tmp/krb5cc_1006\nDefault principal: hdfs-cluster1@EXAMPLE.COM\n\nValid starting       Expires              Service principal\n10/02/2016 04:15:38  10/03/2016 04:15:38  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n\nI create some HDFS directories to store my data and make them world readable and writeable just to make things easier.\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -mkdir -p /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chmod -R 777 /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -copyFromLocal /tmp/crime/crime /tmp/crime/\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chown -R ambari-qa:hdfs /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chown -R ambari-qa:hdfs /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chmod -R 777 /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -ls /tmp/crime/\nFound 1 items\n-rwxrwxrwx   3 ambari-qa hdfs 1452392942 2016-10-02 04:27 /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ exit\n\n\nCreate an HQL file of DDL commands for creating and populating the Hive table\nUsing kinit again to authenticate.\n[root@jyoung-hdp25-node-1 tmp]# su - hive\n[hive@jyoung-hdp25-node-1 ~]$ kinit -kt /etc/security/keytabs/hive.service.keytab hive/jyoung-hdp25-node-1.openstacklocal@EXAMPLE.COM\n[hive@jyoung-hdp25-node-1 ~]$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: hive/jyoung-hdp25-node-1.openstacklocal@EXAMPLE.COM\n\nValid starting       Expires              Service principal\n10/02/2016 04:34:16  10/03/2016 04:34:16  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n\nI use a heredoc to populate load_crime_table.txt with my Hive CREATE TABLE DDL.\nOnce you execute the first line, you can copy and paste the remaining lines up to the EOF and they'll be written to the file.\n[hive@jyoung-hdp25-node-1 ~]$ cat << EOF > /tmp/load_crime_table.txt\nCREATE EXTERNAL TABLE IF NOT EXISTS crime(\n    ID STRING,\n    Case_Number STRING,\n    Case_Date STRING,\n    Block STRING,\n    IUCR INT,\n    Primary_Type STRING,\n    Description STRING,\n    Location_Description STRING,\n    Arrest BOOLEAN,\n    Domestic BOOLEAN,\n    Beat STRING,\n    District STRING,\n    Ward STRING,\n    Community_Area STRING,\n    FBI_Code STRING,\n    X_Coordinate INT,\n    Y_Coordinate INT,\n    Case_Year INT,\n    Updated_On STRING,\n    Latitude DOUBLE,\n    Longitude DOUBLE,\n    Location STRING)\nCOMMENT 'This is crime data for the city of Chicago.'\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nSTORED AS TEXTFILE\nLOCATION '/tmp/crime'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nEOF\n\n\nPopulate the Hive table from the HQL file\nI use beeline to execute my load_crime_table.txt which creates an external Hive table.  Because I'm in a kerberized environment, I include my Hive Service Principal in beeline's JDBC connection URL.\n[hive@jyoung-hdp25-node-1 ~]$ /usr/bin/beeline -u \"jdbc:hive2://jyoung-hdp25-node-2.openstacklocal:10000/default;principal=hive/_HOST@EXAMPLE.COM\" -f \"/tmp/load_crime_table.txt\"\n[hive@jyoung-hdp25-node-1 ~]$ exit\n\n\nCreate an SQL file of DDL commands for creating the RDBMS table (PostgreSQL)\nI'll use another heredoc to create a file of SQL commands containing the DDL statements for creating my relational database table.\n[root@jyoung-hdp25-node-1 ~]# cat << EOF /tmp/create_pettytheft.sql\nCREATE table pettytheft(\n  id BIGINT ,\n  case_number VARCHAR(255) NOT NULL ,\n  primary_type VARCHAR(255) NOT NULL,\n  description VARCHAR(255) NOT NULL,\n  location_description VARCHAR(255) NOT NULL,\n  beat VARCHAR(255) NOT NULL,\n  district VARCHAR(255) NOT NULL,\n  ward VARCHAR(255) NOT NULL,\n  community_area VARCHAR(255) NOT NULL,\n  PRIMARY KEY(id)\n);\n\nGRANT ALL ON pettytheft TO ambari;\nALTER TABLE pettytheft OWNER TO ambari;\nEOF\n[root@jyoung-hdp25-node-1 ~]# chmod 777 /tmp/create_pettytheft.sql\n\n\nCreate the Database table using the SQL file\nI use PostgreSQL's psql utility to create the database from the SQL file created above.\n[root@jyoung-hdp25-node-1 ~]# su - postgres\n-bash-4.2$ psql -d postgres -f /tmp/create_pettytheft.sql\n-bash-4.2$ exit\n\n\nCreate a python script to read from Hive and write to the JDBC DataSource (PostgreSQL table)\nI will create the python script as /tmp/pyspark_hive_jdbc_demo.py.  Let's go through the logic section-by-section.\nImport the necessary modules for pyspark.\nfrom pyspark import SparkContext\nfrom pyspark.sql import HiveContext\n\nCreate a Spark Context object, which tells Spark how to access a cluster.\nUse the Spark Context to create a Hive Context object, which allows you to execute SQL queries as well as Hive commands.\nsc = SparkContext(\"local\", \"pySpark Hive JDBC Demo App\")\n# Create a Hive Context\nhive_context = HiveContext(sc)\n\nRead from the Hive table \"crime\" on the \"default\" Hive database. The result is a DataFrame.\nprint \"Reading Hive table...\"\ncrime = hive_context.table(\"default.crime\")\n\nRegister the DataFrame crime as a temporary table crime_temp.\nprint \"Registering DataFrame as a table...\"\ncrime.registerTempTable(\"crime_temp\")\n\nSQL can be run over DataFrames that have been registered as a table.\nExecuting an SQL query over this temporary table to get a list of thefts of property worth less than $500USD.\nThe results will be another DataFrame.\nprint \"Executing SQL SELECT query on DataFrame registered as a temporary table...\"\npettythefts = hive_context.sql('SELECT * FROM crime_temp WHERE Primary_Type = \"THEFT\" AND Description = \"$500 AND UNDER\"')\n\nCreate a new DataFrame containing only the columns we wish to write to the JDBC connected datasource using select([list of columns]).\nThis will be a subset of the columns available in the source Hive table.\nprint \"Creating a DataFrame of only the columns of our resultset to be persisted to JDBC DataSource...\"\npettythefts_table_df = pettythefts.select(\"id\", \"case_number\", \"primary_type\", \"description\", \"location_description\", \"beat\", \"district\", \"ward\", \"community_area\")\n\nPrepare the connection properties for the JDBC datasource.\nI'm using a Postgres database so the connection URL below is valid for Postgres.\nMy database table name (as defined in /tmp/create_pettytheft.sql) is public.pettytheft.\nmode = 'overwrite'\nurl = 'jdbc:postgresql://<database server IP address>:5432/postgres?searchpath=public'\nproperties = {\"user\": \"<username>\", \"password\": \"<password>\", \"driver\": \"org.postgresql.Driver\"}\ntable = 'public.pettytheft'\n\nWrite the contents of the DataFrame to the JDBC datasource (Postgres) using the connection URL defined above.\nprint \"Writing DataFrame to JDBC Datasource...\"\npettythefts_table_df.write.jdbc(url=url, table=table, mode=mode, properties=properties)\n\nprint \"Exiting...\"\n\nSave the python script as /tmp/pyspark_hive_jdbc_demo.py\n\nRun the pySpark application locally\nBe sure to include postgresql-jdbc.jar in both your distributed cache using the --jars option and on your class path using the --driver-class-path option.\n[root@jyoung-hdp25-node-1 ~]# su - yarn\n[yarn@jyoung-hdp25-node-1 ~]$ /bin/spark-submit --jars /usr/share/java/postgresql-jdbc.jar --driver-class-path /usr/share/java/postgresql-jdbc.jar --master local[1] /tmp/pyspark_hive_jdbc_demo.py\n[yarn@jyoung-hdp25-node-1 ~]$ exit\n\n\nVerify results in the RDBMS\nI execute a query against my pettytheft table in Postgres to verify that pySpark wrote out the data to the database as expected.\n[root@jyoung-hdp25-node-1 ~]# su - postgres\n-bash-4.2$ psql -d postgres -U <username> -c \"select * from public.pettytheft limit 5;\"\nPassword for user ambari: \n   id    | case_number | primary_type |  description   | location_description | beat | district | ward | community_area \n---------+-------------+--------------+----------------+----------------------+------+----------+------+----------------\n 8503954 | HV180410    | THEFT        | $500 AND UNDER | STREET               | 1114 | 011      | 28   | 26\n 8503999 | HV180357    | THEFT        | $500 AND UNDER | STREET               | 2412 | 024      | 50   | 2\n 8504003 | HV180508    | THEFT        | $500 AND UNDER | STREET               | 0725 | 007      | 17   | 67\n 8504108 | HV180682    | THEFT        | $500 AND UNDER | SIDEWALK             | 0123 | 001      | 2    | 32\n 8504109 | HV180672    | THEFT        | $500 AND UNDER | STREET               | 1911 | 019      | 47   | 5\n(5 rows)\n\n\nWrap-up\nIn the exercise above we:\n\ncreated a Hive table\npopulated the Hive table with CSV data\ncreated a relational database table\ncreated a pySpark script that:\n\n\nread the Hive table into a DataFrame\nregistered the DataFrame as a temporary table\nexecuted an SQL query over the temporary table\nexecuted a select() on the resulting DataFrame\nwrote the contents out to the relational database\n\n\nverified the data was written to our database\n\nIf you'd like to quickly get up-and-running with Spark development, I encourage you to download our HDP Sandbox.  It comes with everything you need to start developing Spark applications today!\nI was once asked for a tutorial that described how to use [pySpark](http://spark.apache.org/docs/1.6.2/programming-guide.html#tab_python_0) to read data from a Hive table and write to a JDBC datasource like PostgreSQL or SQL Server. Not being able to find a suitable tutorial, I decided to write one.\n\nThis demo creates a python script which uses pySpark to read data from a Hive table into a [DataFrame](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame), perform operations on the DataFrame, and write the results out to a JDBC DataSource (PostgreSQL database). I am using Spark 1.6.2.\n\n# Getting some CSV data to populate into Hive\n\nI will use crime data from the City of Chicago in this tutorial. The data is available in CSV format from the following link:\nhttps://catalog.data.gov/dataset/crimes-2001-to-present-398a4\n\n```\n[root@jyoung-hdp25-node-1 tmp]# mkdir -p /tmp/crime\n[root@jyoung-hdp25-node-1 tmp]# cd /tmp/crime\n[root@jyoung-hdp25-node-1 crime]# curl -o crime -L https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n[root@jyoung-hdp25-node-1 crime]# cd /tmp/\n[root@jyoung-hdp25-node-1 tmp]# chmod -R 777 crime/\n```\n\n# Copy the CSV data to HDFS\n\nI'm using a kerberized [Hortonworks Data Platform (HDP)](http://hortonworks.com/products/data-center/hdp/) cluster, so I'll use [kinit](https://web.mit.edu/kerberos/krb5-latest/doc/user/user_commands/kinit.html) to authenticate.\n\n```\nroot@jyoung-hdp25-node-1 tmp]# su - hdfs\n[hdfs@jyoung-hdp25-node-1 ~]$ kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-cluster1@EXAMPLE.COM\n[hdfs@jyoung-hdp25-node-1 ~]$ klist\nTicket cache: FILE:/tmp/krb5cc_1006\nDefault principal: hdfs-cluster1@EXAMPLE.COM\n\nValid starting       Expires              Service principal\n10/02/2016 04:15:38  10/03/2016 04:15:38  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n```\nI create some HDFS directories to store my data and make them world readable and writeable just to make things easier.\n\n```\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -mkdir -p /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chmod -R 777 /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -copyFromLocal /tmp/crime/crime /tmp/crime/\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chown -R ambari-qa:hdfs /tmp/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chown -R ambari-qa:hdfs /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -chmod -R 777 /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ hadoop fs -ls /tmp/crime/\nFound 1 items\n-rwxrwxrwx   3 ambari-qa hdfs 1452392942 2016-10-02 04:27 /tmp/crime/crime\n[hdfs@jyoung-hdp25-node-1 ~]$ exit\n```\n\n# Create an HQL file of DDL commands for creating and populating the Hive table\n\nUsing kinit again to authenticate.\n\n```\n[root@jyoung-hdp25-node-1 tmp]# su - hive\n[hive@jyoung-hdp25-node-1 ~]$ kinit -kt /etc/security/keytabs/hive.service.keytab hive/jyoung-hdp25-node-1.openstacklocal@EXAMPLE.COM\n[hive@jyoung-hdp25-node-1 ~]$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: hive/jyoung-hdp25-node-1.openstacklocal@EXAMPLE.COM\n\nValid starting       Expires              Service principal\n10/02/2016 04:34:16  10/03/2016 04:34:16  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n```\nI use a [heredoc](https://ja.wikipedia.org/wiki/\u30d2\u30a2\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8) to populate `load_crime_table.txt` with my Hive CREATE TABLE DDL.\nOnce you execute the first line, you can copy and paste the remaining lines up to the `EOF` and they'll be written to the file.\n\n```\n[hive@jyoung-hdp25-node-1 ~]$ cat << EOF > /tmp/load_crime_table.txt\nCREATE EXTERNAL TABLE IF NOT EXISTS crime(\n    ID STRING,\n    Case_Number STRING,\n    Case_Date STRING,\n    Block STRING,\n    IUCR INT,\n    Primary_Type STRING,\n    Description STRING,\n    Location_Description STRING,\n    Arrest BOOLEAN,\n    Domestic BOOLEAN,\n    Beat STRING,\n    District STRING,\n    Ward STRING,\n    Community_Area STRING,\n    FBI_Code STRING,\n    X_Coordinate INT,\n    Y_Coordinate INT,\n    Case_Year INT,\n    Updated_On STRING,\n    Latitude DOUBLE,\n    Longitude DOUBLE,\n    Location STRING)\nCOMMENT 'This is crime data for the city of Chicago.'\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nSTORED AS TEXTFILE\nLOCATION '/tmp/crime'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nEOF\n```\n\n# Populate the Hive table from the HQL file\n\nI use [beeline](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline\u2013CommandLineShell) to execute my `load_crime_table.txt` which creates an [external](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ManagedandExternalTables) Hive table.  Because I'm in a kerberized environment, I include my Hive [Service Principal](https://ssimo.org/blog/id_016.html) in beeline's JDBC connection URL.\n\n```\n[hive@jyoung-hdp25-node-1 ~]$ /usr/bin/beeline -u \"jdbc:hive2://jyoung-hdp25-node-2.openstacklocal:10000/default;principal=hive/_HOST@EXAMPLE.COM\" -f \"/tmp/load_crime_table.txt\"\n[hive@jyoung-hdp25-node-1 ~]$ exit\n```\n\n# Create an SQL file of DDL commands for creating the RDBMS table (PostgreSQL)\n\nI'll use another heredoc to create a file of SQL commands containing the DDL statements for creating my relational database table.\n\n```\n[root@jyoung-hdp25-node-1 ~]# cat << EOF /tmp/create_pettytheft.sql\nCREATE table pettytheft(\n  id BIGINT ,\n  case_number VARCHAR(255) NOT NULL ,\n  primary_type VARCHAR(255) NOT NULL,\n  description VARCHAR(255) NOT NULL,\n  location_description VARCHAR(255) NOT NULL,\n  beat VARCHAR(255) NOT NULL,\n  district VARCHAR(255) NOT NULL,\n  ward VARCHAR(255) NOT NULL,\n  community_area VARCHAR(255) NOT NULL,\n  PRIMARY KEY(id)\n);\n\nGRANT ALL ON pettytheft TO ambari;\nALTER TABLE pettytheft OWNER TO ambari;\nEOF\n[root@jyoung-hdp25-node-1 ~]# chmod 777 /tmp/create_pettytheft.sql\n```\n\n# Create the Database table using the SQL file\n\nI use PostgreSQL's [psql](https://www.postgresql.org/docs/9.2/static/app-psql.html) utility to create the database from the SQL file created above.\n\n```\n[root@jyoung-hdp25-node-1 ~]# su - postgres\n-bash-4.2$ psql -d postgres -f /tmp/create_pettytheft.sql\n-bash-4.2$ exit\n```\n\n# Create a python script to read from Hive and write to the JDBC DataSource (PostgreSQL table)\n\nI will create the python script as `/tmp/pyspark_hive_jdbc_demo.py`.  Let's go through the logic section-by-section.\n\nImport the necessary modules for pyspark.\n\n```\nfrom pyspark import SparkContext\nfrom pyspark.sql import HiveContext\n```\n\nCreate a Spark Context object, which tells Spark how to access a cluster.\nUse the Spark Context to create a Hive Context object, which allows you to execute SQL queries as well as Hive commands.\n\n```\nsc = SparkContext(\"local\", \"pySpark Hive JDBC Demo App\")\n# Create a Hive Context\nhive_context = HiveContext(sc)\n```\n\nRead from the Hive table \"crime\" on the \"default\" Hive database. The result is a DataFrame.\n\n```\nprint \"Reading Hive table...\"\ncrime = hive_context.table(\"default.crime\")\n```\n\nRegister the DataFrame `crime` as a temporary table `crime_temp`.\n\n```\nprint \"Registering DataFrame as a table...\"\ncrime.registerTempTable(\"crime_temp\")\n```\n\nSQL can be run over DataFrames that have been registered as a table.\nExecuting an SQL query over this temporary table to get a list of thefts of property worth less than $500USD.\nThe results will be another DataFrame.\n\n```\nprint \"Executing SQL SELECT query on DataFrame registered as a temporary table...\"\npettythefts = hive_context.sql('SELECT * FROM crime_temp WHERE Primary_Type = \"THEFT\" AND Description = \"$500 AND UNDER\"')\n```\n\nCreate a new DataFrame containing only the columns we wish to write to the JDBC connected datasource using `select([list of columns])`.\nThis will be a subset of the columns available in the source Hive table.\n\n```\nprint \"Creating a DataFrame of only the columns of our resultset to be persisted to JDBC DataSource...\"\npettythefts_table_df = pettythefts.select(\"id\", \"case_number\", \"primary_type\", \"description\", \"location_description\", \"beat\", \"district\", \"ward\", \"community_area\")\n```\n\nPrepare the connection properties for the JDBC datasource.\nI'm using a Postgres database so the connection URL below is valid for Postgres.\nMy database table name (as defined in `/tmp/create_pettytheft.sql`) is `public.pettytheft`.\n\n```\nmode = 'overwrite'\nurl = 'jdbc:postgresql://<database server IP address>:5432/postgres?searchpath=public'\nproperties = {\"user\": \"<username>\", \"password\": \"<password>\", \"driver\": \"org.postgresql.Driver\"}\ntable = 'public.pettytheft'\n```\n\nWrite the contents of the DataFrame to the JDBC datasource (Postgres) using the connection URL defined above.\n \n```\nprint \"Writing DataFrame to JDBC Datasource...\"\npettythefts_table_df.write.jdbc(url=url, table=table, mode=mode, properties=properties)\n\nprint \"Exiting...\"\n```\n\nSave the python script as `/tmp/pyspark_hive_jdbc_demo.py`\n\n# Run the pySpark application locally\n\nBe sure to include postgresql-jdbc.jar in both your distributed cache using the `--jars` option and on your class path using the `--driver-class-path` option.\n\n```\n[root@jyoung-hdp25-node-1 ~]# su - yarn\n[yarn@jyoung-hdp25-node-1 ~]$ /bin/spark-submit --jars /usr/share/java/postgresql-jdbc.jar --driver-class-path /usr/share/java/postgresql-jdbc.jar --master local[1] /tmp/pyspark_hive_jdbc_demo.py\n[yarn@jyoung-hdp25-node-1 ~]$ exit\n```\n\n# Verify results in the RDBMS\n\nI execute a query against my pettytheft table in Postgres to verify that pySpark wrote out the data to the database as expected.\n\n```\n[root@jyoung-hdp25-node-1 ~]# su - postgres\n-bash-4.2$ psql -d postgres -U <username> -c \"select * from public.pettytheft limit 5;\"\nPassword for user ambari: \n   id    | case_number | primary_type |  description   | location_description | beat | district | ward | community_area \n---------+-------------+--------------+----------------+----------------------+------+----------+------+----------------\n 8503954 | HV180410    | THEFT        | $500 AND UNDER | STREET               | 1114 | 011      | 28   | 26\n 8503999 | HV180357    | THEFT        | $500 AND UNDER | STREET               | 2412 | 024      | 50   | 2\n 8504003 | HV180508    | THEFT        | $500 AND UNDER | STREET               | 0725 | 007      | 17   | 67\n 8504108 | HV180682    | THEFT        | $500 AND UNDER | SIDEWALK             | 0123 | 001      | 2    | 32\n 8504109 | HV180672    | THEFT        | $500 AND UNDER | STREET               | 1911 | 019      | 47   | 5\n(5 rows)\n```\n\n# Wrap-up\n\nIn the exercise above we:\n\n - created a Hive table\n - populated the Hive table with CSV data\n - created a relational database table\n - created a pySpark script that:\n   - read the Hive table into a DataFrame\n   - registered the DataFrame as a temporary table\n   - executed an SQL query over the temporary table\n   - executed a select() on the resulting DataFrame\n   - wrote the contents out to the relational database\n - verified the data was written to our database\n\n\nIf you'd like to quickly get up-and-running with Spark development, I encourage you to download our [HDP Sandbox](http://hortonworks.com/downloads/#sandbox).  It comes with everything you need to start developing Spark applications today!\n", "tags": ["Spark", "hive", "Pyspark", "sparksql", "HDP"]}