{"context": " More than 1 year has passed since last update.\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3067\u6587\u66f8\u5206\u985e\u3092\u3059\u308b\u305f\u3081\u306e\u30e9\u30d9\u30eb\u4ed8\u3051\u4f5c\u696d\u306b\u75b2\u308c\u305f\u65b9\u304c\u3044\u3089\u3063\u3057\u3083\u3063\u305f\u306e\u3067\u3001\u5c11\u306a\u3044\u30e9\u30d9\u30eb\u3067\u5206\u985e\u304c\u53ef\u80fd\u306a\u3088\u3046\u306b\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u3092\u884c\u3046\u3082\u306e\u3092\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u53c2\u8003\u8cc7\u6599\n\n\n\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u6700\u5148\u7aef\u30ac\u30a4\u30c96 (CVIM\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u30b7\u30ea\u30fc\u30ba)\n\nDeep Learning Tutorials\nLearning with local and global consistency, Zhou+, 2004\n\n\n\u3084\u3063\u305f\u3053\u3068\n\nDBN\u3067\u7279\u5fb4\u91cf\u62bd\u51fa\nLabelSpreading\u3067\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306b\u3088\u308b\u30e9\u30d9\u30eb\u4ed8\u3051\n\n\n\u624b\u9806\n\nDBN\u3067\u7279\u5fb4\u62bd\u51fa\n\n\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u9069\u5f53\u306b\u5c64\u306e\u6df1\u3055\u3001\u5b66\u7fd2\u4fc2\u6570\u3001\u5404\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u6c7a\u5b9a(Top\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u5727\u7e2e\u5f8c\u306e\u6b21\u5143\u6570)\nDBN\u3067pre-training\u3092\u884c\u3044weight\u3068\u30d0\u30a4\u30a2\u30b9\u3092\u5b66\u7fd2\nTop\u5c64\u306e\u51fa\u529b\u3092\u6b21\u5143\u5727\u7e2e\u5f8c\u306e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u5206\u985e\u5668\u306b\u6295\u5165\u3059\u308b\n\n\nLabelSpreading\u3067\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\n\n\u5404\u30ab\u30c6\u30b4\u30ea\u306e\u6559\u5e2b\u30e9\u30d9\u30eb\u306e\u4e0a\u9650\u6570\u3092\u6c7a\u3081\u308b\n\u5404\u30ab\u30c6\u30b4\u30ea\u306e\u30c7\u30fc\u30bf\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3057\u3001\u4e0a\u9650\u6570\u307e\u3067\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\n\u6559\u5e2b\u30e9\u30d9\u30eb\u306e\u4e0a\u9650\u6570\u306b\u9054\u3057\u305f\u5834\u5408\u3001\u30ab\u30c6\u30b4\u30ea\u5185\u306e\u6b8b\u308a\u306e\u30c7\u30fc\u30bf\u306f\u30e9\u30d9\u30eb\u306a\u3057\u30c7\u30fc\u30bf\u3068\u3057\u3066\u6271\u3046\n\u5b66\u7fd2\u30d1\u30e9\u30e1\u30bf\u306e\u8a2d\u5b9a(\u4eca\u56de\u306fkNN\u3092\u4f7f\u7528\u3057\u305f\u306e\u3067\u3001\u30e9\u30d9\u30eb\u4f1d\u642c\u3055\u305b\u308b\u3054\u8fd1\u6240\u3055\u3093\u306e\u6570)\n\n\n\u5229\u70b9\n\nDBN\u3067\u7279\u5fb4\u62bd\u51fa\u3059\u308b\u5229\u70b9\n\nSVD\u3068\u9055\u3044\u3001\u975e\u7dda\u5f62\u306a\u7279\u5fb4\u304c\u62bd\u51fa\u53ef\u80fd\n\n\nLabelSpreading\u306e\u5229\u70b9\n\n\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u5168\u822c\u306e\u5229\u70b9\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u306e\u3060\u304c\u3001\u4eba\u9593\u306e\u52b4\u529b\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u3066\u6bd4\u8f03\u7684\u7cbe\u5ea6\u306e\u826f\u3044\u30e9\u30d9\u30eb\u4ed8\u3051\u304c\u53ef\u80fd\n\n\n\u53c2\u8003\u30b3\u30fc\u30c9\n\u304b\u306a\u308a\u6c5a\u3044\u30b3\u30fc\u30c9\u3067\u7533\u3057\u8a33\u6709\u308a\u307e\u305b\u3093\u304c\u3001\u4ee5\u4e0b\u8cbc\u308a\u4ed8\u3051\u307e\u3059\u3002\nDBN\u306e\u30b3\u30fc\u30c9\u306fDeep Learning Tutorial\u306e\u4e38\u30b3\u30d4\u3067\u3059\u3002Deep Learning Tutorial\u306e\u30b5\u30a4\u30c8\u306b\u306f\u6570\u5f0f\u4ed8\u304d\u3067\u89e3\u8aac\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u8a73\u3057\u3044\u89e3\u8aac\u306f\u305d\u3061\u3089\u306b\u8b72\u308a\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\nDBN\u306e\u30b3\u30fc\u30c9\nBoW\u5f62\u5f0f\u306eCSV\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u7279\u5fb4\u62bd\u51fa\u3057\u305f\u3042\u3068\u306b\u3001CSV\u3092\u5410\u304d\u51fa\u3059\u611f\u3058\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u4f7f\u7528\u3057\u3066\u3044\u306a\u3044\u30e1\u30bd\u30c3\u30c9\u3092\u3061\u3087\u3044\u3061\u3087\u3044\u542b\u3093\u3067\u3044\u307e\u3059\u306e\u3067\u3001\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002\n\nDBN.py\n# coding:utf-8\n\nfrom __future__ import unicode_literals\nimport time\n\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\n\nclass DBN:\n    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n                 hidden_layers_sizes=[500, 500], n_outs=10):\n        self.sigmoid_layers = []\n        self.rbm_layers = []\n        self.params = []\n        self.n_layers = len(hidden_layers_sizes)\n\n        assert self.n_layers > 0\n\n        if not theano_rng:\n            theano_rng = RandomStreams(numpy_rng.randint(2**30))\n\n        # allocate symbolic variables for the data\n        self.x = T.matrix('x')\n        self.y = T.ivector('y')\n\n        for i in xrange(self.n_layers):\n            if i==0:\n                input_size = n_ins\n                layer_input = self.x\n            else:\n                input_size = hidden_layers_sizes[i - 1]\n                layer_input = self.sigmoid_layers[-1].output\n\n            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n                                         input=layer_input,\n                                         n_in=input_size,\n                                         n_out=hidden_layers_sizes[i],\n                                         activation=T.nnet.sigmoid)\n            self.sigmoid_layers.append( sigmoid_layer )\n            self.params.extend(sigmoid_layer.params)\n\n            rbm_layer = RBM(numpy_rng=numpy_rng,\n                            theano_rng=theano_rng,\n                            input=layer_input,\n                            n_visible=input_size,\n                            n_hidden=hidden_layers_sizes[i],\n                            W=sigmoid_layer.W,\n                            hbias=sigmoid_layer.b)\n            self.rbm_layers.append(rbm_layer)\n\n            self.logLayer = LogisticRegression(\n                input=self.sigmoid_layers[-1].output,\n                n_in=hidden_layers_sizes[-1],\n                n_out=n_outs)\n            self.params.extend(self.logLayer.params)\n            self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n            self.errors = self.logLayer.errors(self.y)\n\n    def pretrainig_functions(self, train_set_x, batch_size, k):\n        index = T.lscalar('index')\n        learning_rate = T.scalar('lr')  # learning rate to use\n        # number of batches\n        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n        # begining of a batch, given `index`\n        batch_begin = index * batch_size\n        # ending of a batch given `index`\n        batch_end = batch_begin + batch_size\n\n        pretrain_fns = []\n        for rbm in self.rbm_layers:\n\n            cost, updates = rbm.get_cost_updates(learning_rate,\n                                                 persistent=None, k=k)\n\n            # compile the theano function\n            fn = theano.function(\n                inputs=[index, theano.Param(learning_rate, default=0.1)],\n                outputs=cost,\n                updates=updates,\n                givens={\n                    self.x: train_set_x[batch_begin:batch_end]\n                }\n            )\n            # append `fn` to the list of functions\n            pretrain_fns.append(fn)\n\n        return pretrain_fns\n\n\n    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n        (train_set_x, train_set_y) = datasets[0]\n        (valid_set_x, valid_set_y) = datasets[1]\n        (test_set_x, test_set_y) = datasets[2]\n\n        # compute number of minibatches for training, validation and testing\n        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n        n_valid_batches /= batch_size\n        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n        n_test_batches /= batch_size\n\n        index = T.lscalar('index')  # index to a [mini]batch\n\n        # compute the gradients with respect to the model parameters\n        gparams = T.grad(self.finetune_cost, self.params)\n\n        # compute list of fine-tuning updates\n        updates = []\n        for param, gparam in zip(self.params, gparams):\n            updates.append((param, param - gparam * learning_rate))\n\n        train_fn = theano.function(\n            inputs=[index],\n            outputs=self.finetune_cost,\n            updates=updates,\n            givens={\n                self.x: train_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: train_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        test_score_i = theano.function(\n            [index],\n            self.errors,\n            givens={\n                self.x: test_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: test_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        valid_score_i = theano.function(\n            [index],\n            self.errors,\n            givens={\n                self.x: valid_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: valid_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        # Create a function that scans the entire validation set\n        def valid_score():\n            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n\n        # Create a function that scans the entire test set\n        def test_score():\n            return [test_score_i(i) for i in xrange(n_test_batches)]\n\n        return train_fn, valid_score, test_score\n\n\nclass HiddenLayer:\n    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n                 activation=T.tanh):\n        self.input = input\n\n        if W is None:\n            W_values = np.asarray(\n                rng.uniform(\n                    low=-np.sqrt(6. / (n_in+n_out)),\n                    high=np.sqrt(6. / (n_in+n_out)),\n                    size=(n_in, n_out)\n                ),\n                dtype=theano.config.floatX\n            )\n            if activation == theano.tensor.nnet.sigmoid:\n                W_values *=4\n            W = theano.shared(value=W_values, name='W', borrow=True)\n\n        if b is None:\n            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name='b', borrow=True)\n        self.W = W\n        self.b = b\n\n        lin_output = T.dot(input, self.W) + self.b\n        self.output = (\n              lin_output if activation is None\n              else activation(lin_output)\n        )\n        self.params = [self.W, self.b]\n\nclass LogisticRegression:\n    def __init__(self, input, n_in, n_out):\n        self.W = theano.shared(\n            value=np.zeros(\n                (n_in, n_out),\n                dtype=theano.config.floatX\n            ),\n            name='W',\n            borrow=True\n        )\n        # initialize the baises b as a vector of n_out 0s\n        self.b = theano.shared(\n            value=np.zeros(\n                (n_out,),\n                dtype=theano.config.floatX\n            ),\n            name='b',\n            borrow=True\n        )\n        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n        self.params = [self.W, self.b]\n\n    def negative_log_likelihood(self, y):\n        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n\n    def errors(self, y):\n        # check if y has same dimension of y_pred\n        if y.ndim != self.y_pred.ndim:\n            raise TypeError(\n                'y should have the same shape as self.y_pred',\n                ('y', y.type, 'y_pred', self.y_pred.type)\n            )\n        # check if y is of the correct datatype\n        if y.dtype.startswith('int'):\n            return T.mean(T.neq(self.y_pred, y))\n        else:\n            raise NotImplementedError()\n\n\nclass RBM(object):\n    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n    def __init__(\n        self,\n        input=None,\n        n_visible=784,\n        n_hidden=500,\n        W=None,\n        hbias=None,\n        vbias=None,\n        numpy_rng=None,\n        theano_rng=None\n    ):\n\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n\n        if numpy_rng is None:\n            # create a number generator\n            numpy_rng = np.random.RandomState(1234)\n\n        if theano_rng is None:\n            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n\n        if W is None:\n            initial_W = np.asarray(\n                numpy_rng.uniform(\n                    low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n                    high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n                    size=(n_visible, n_hidden)\n                ),\n                dtype=theano.config.floatX\n            )\n            # theano shared variables for weights and biases\n            W = theano.shared(value=initial_W, name='W', borrow=True)\n\n        if hbias is None:\n            # create shared variable for hidden units bias\n            hbias = theano.shared(\n                value=np.zeros(\n                    n_hidden,\n                    dtype=theano.config.floatX\n                ),\n                name='hbias',\n                borrow=True\n            )\n\n        if vbias is None:\n            # create shared variable for visible units bias\n            vbias = theano.shared(\n                value=np.zeros(\n                    n_visible,\n                    dtype=theano.config.floatX\n                ),\n                name='vbias',\n                borrow=True\n            )\n\n        # initialize input layer for standalone RBM or layer0 of DBN\n        self.input = input\n        if not input:\n            self.input = T.matrix('input')\n\n        self.W = W\n        self.hbias = hbias\n        self.vbias = vbias\n        self.theano_rng = theano_rng\n        self.params = [self.W, self.hbias, self.vbias]\n\n    def propup(self, vis):\n        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n\n\n    def sample_h_given_v(self, v0_sample):\n        ''' This function infers state of hidden units given visible units '''\n        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n                                             n=1, p=h1_mean,\n                                             dtype=theano.config.floatX)\n        return [pre_sigmoid_h1, h1_mean, h1_sample]\n\n\n    def propdown(self, hid):\n        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n\n\n    def sample_v_given_h(self, h0_sample):\n        ''' This function infers state of visible units given hidden units '''\n        # compute the activation of the visible given the hidden sample\n        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n                                             n=1, p=v1_mean,\n                                             dtype=theano.config.floatX)\n        return [pre_sigmoid_v1, v1_mean, v1_sample]\n\n\n    def gibbs_hvh(self, h0_sample):\n        ''' This function implements one step of Gibbs sampling,\n            starting from the hidden state'''\n        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n        return [pre_sigmoid_v1, v1_mean, v1_sample,\n                pre_sigmoid_h1, h1_mean, h1_sample]\n\n\n    def gibbs_vhv(self, v0_sample):\n        ''' This function implements one step of Gibbs sampling,\n            starting from the visible state'''\n        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n        return [pre_sigmoid_h1, h1_mean, h1_sample,\n                pre_sigmoid_v1, v1_mean, v1_sample]\n\n\n    def free_energy(self, v_sample):\n        ''' Function to compute the free energy '''\n        wx_b = T.dot(v_sample, self.W) + self.hbias\n        vbias_term = T.dot(v_sample, self.vbias)\n        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n        return -hidden_term - vbias_term\n\n\n    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n        # compute positive phase\n        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n\n        if persistent is None:\n            chain_start = ph_sample\n        else:\n            chain_start = persistent\n        (\n            [\n                pre_sigmoid_nvs,\n                nv_means,\n                nv_samples,\n                pre_sigmoid_nhs,\n                nh_means,\n                nh_samples\n            ],\n            updates\n        ) = theano.scan(\n            self.gibbs_hvh,\n            outputs_info=[None, None, None, None, None, chain_start],\n            n_steps=k\n        )\n\n        chain_end = nv_samples[-1]\n        cost = T.mean(self.free_energy(self.input)) - T.mean(\n            self.free_energy(chain_end))\n        # We must not compute the gradient through the gibbs sampling\n        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n\n\n        for gparam, param in zip(gparams, self.params):\n            # make sure that the learning rate is of the right dtype\n            updates[param] = param - gparam * T.cast(\n                lr,\n                dtype=theano.config.floatX\n            )\n        if persistent:\n            # Note that this works only if persistent is a shared variable\n            updates[persistent] = nh_samples[-1]\n            # pseudo-likelihood is a better proxy for PCD\n            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n        else:\n            # reconstruction cross-entropy is a better proxy for CD\n            monitoring_cost = self.get_reconstruction_cost(updates,\n                                                           pre_sigmoid_nvs[-1])\n\n        return monitoring_cost, updates\n\n    def get_pseudo_likelihood_cost(self, updates):\n        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n\n        # index of bit i in expression p(x_i | x_{\\i})\n        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n\n        # binarize the input image by rounding to nearest integer\n        xi = T.round(self.input)\n\n        # calculate free energy for the given bit configuration\n        fe_xi = self.free_energy(xi)\n\n        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n        # the result to xi_flip, instead of working in place on xi.\n        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n\n        # calculate free energy with bit flipped\n        fe_xi_flip = self.free_energy(xi_flip)\n\n        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n                                                            fe_xi)))\n\n        # increment bit_i_idx % number as part of updates\n        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n\n        return cost\n\n\n    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n        cross_entropy = T.mean(\n             T.sum(\n                 self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n                 (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n                 axis=1\n             )\n        )\n\n        return cross_entropy\n\ndef output(input_data, w, b):\n    x = np.dot(input_data,w)+np.kron( np.ones((input_data.shape[0],1)),b)\n    return 1/(1+np.exp(-x))\n\nif __name__=='__main__':\n    numpy_rng = np.random.RandomState(123)\n    print '... building the model'\n\n    ifname = 'bow_data.csv'\n    data = np.loadtxt(ifname, delimiter=',')\n    train_set_x = theano.shared(np.asarray(data, np.float64))\n\n    dbn = DBN(numpy_rng=numpy_rng, n_ins=data.shape[1],\n              hidden_layers_sizes=[2000, 1000, 100],\n              n_outs=10)\n    #########################\n    # PRETRAINING THE MODEL #\n    #########################\n    print '... getting the pretraining functions'\n    batch_size=10\n    k = 5\n    pretraining_fns = dbn.pretrainig_functions(train_set_x=train_set_x,\n                                                batch_size=batch_size,\n                                                k=k)\n\n    print '... pre-training the model'\n    pretraining_epochs = 100\n    n_train_batches = 10\n    pretrain_lr = 0.1\n    ## Pre-train layer-wise\n    for i in xrange(dbn.n_layers):\n        # go through pretraining epochs\n        for epoch in xrange(pretraining_epochs):\n            # go through the training set\n            c = []\n            for batch_index in xrange(n_train_batches):\n                c.append(pretraining_fns[i](index=batch_index,\n                                            lr=pretrain_lr))\n            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n            print np.mean(c)\n\n    layer_output =[]\n    for i in xrange(dbn.n_layers):\n        w = dbn.rbm_layers[i].W.get_value()\n        hbias = dbn.rbm_layers[i].hbias.get_value()\n        if i==0:\n            layer_output.append( train_set_x.get_value() )\n            layer_output.append( output(layer_output[-1],w, hbias) )\n        else:\n            layer_output.append( output(layer_output[-1],w, hbias) )\n    print layer_output[-1]\n    np.savetxt('DBN_features.csv',layer_output[-1], delimiter=',')\n\n\n\nLabelSpreading\u306e\u30b3\u30fc\u30c9\noriginal_data.csv\u306e\u4e2d\u8eab\u306f(0\u00a5t1 0 1 0 0 0 0\u00a5txxxx)\u30671\u884c\u304c\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u4f55\u3092\u8868\u3057\u3066\u3044\u308b\u304b\u3068\u3044\u3046\u3068\u3001\u5de6\u304b\u3089\u3001\n\u8907\u6570\u30af\u30e9\u30b9\u306b\u307e\u305f\u304c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u304b\u3069\u3046\u304b\u306eflag\u3001\u5404\u30af\u30e9\u30b9\u306e\u30d5\u30e9\u30b0\u3001\u30c6\u30ad\u30b9\u30c8\n\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306b\u3001\u306a\u308b\u3079\u304f1\u3064\u3057\u304b\u30e9\u30d9\u30eb\u304c\u4ed8\u3044\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u3092\u6559\u5e2b\u30c7\u30fc\u30bf\u306b\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e00\u5fdciris\u30c7\u30fc\u30bf\u3067\u78ba\u8a8d\u51fa\u6765\u308b\u3088\u3046\u306b\u95a2\u6570\u3092\u3064\u3051\u3066\u304a\u304d\u307e\u3057\u305f\u306e\u3067\u3001\u624b\u5143\u306b\u9069\u5f53\u306a\u30c7\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u3067LabelSpreading\u3092\u8a66\u3057\u305f\u3044\u5834\u5408\u306f\u304a\u4f7f\u3044\u304f\u3060\u3055\u3044\u3002\n\nLabelSpreading.py\n# coding: utf-8\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import LabelSpreading\nimport numpy as np\nfrom numpy.random import seed\nseed(555)\nfrom collections import defaultdict\n\ndef iris():\n    iris = datasets.load_iris()\n    random_unlabeled_points = np.where(np.random.random_integers(0, 1, size=len(iris.target)))\n    labels = np.copy(iris.target)\n    labels[random_unlabeled_points] = -1\n\n    label_prop_model = LabelSpreading()\n    label_prop_model.fit(iris.data, labels) # unlabeled as -1\n    pred_prop = label_prop_model.predict_proba(iris.data)\n    pred_label = label_prop_model.predict(iris.data)\n\n    for pp, pl, label, trgt in zip(pred_prop,pred_label,labels,iris.target):\n        print pp, pl, label, trgt\n\ndef main(X, labels_info, min_number=20, label_num=6, n_neighbors=7, alpha=0.3, typ='knn', threshold=0.5):\n    target = get_target(labels_info)\n    random_unlabeled_points = np.where(np.random.random_integers(0, 1, size=len(target)))[0]\n    cnt_dict = defaultdict(int)\n    for i, t in enumerate(target):\n        if len(t)==1 and (i in random_unlabeled_points):\n            target[i] = -1\n            cnt_dict[-1] += 1\n        elif len(t)>=2:\n            target[i] = -1\n            cnt_dict[-1] += 1\n        elif cnt_dict[target[i][0]]<min_number:\n            target[i] = target[i][0]\n            cnt_dict[target[i]] += 1\n        elif cnt_dict[target[i][0]]>=min_number:\n            target[i] = -1\n            cnt_dict[target[i]] += 1\n    print cnt_dict\n\n    if typ=='knn':\n        label_prop_model = LabelSpreading(kernel=typ, n_neighbors=n_neighbors)\n    else:\n        label_prop_model = LabelSpreading(kernel=typ, alpha=alpha)\n    label_prop_model.fit(X, target) # unlabeled as -1\n    pred_prop = label_prop_model.predict_proba(X)\n    pred_label = label_prop_model.predict(X)\n\n    res_dict = defaultdict(dict)  # TP, FP, FN, TN\u3092\u683c\u7d0d\u3059\u308b\n    for label in ('TP', 'FP', 'FN', 'TN'):\n        res_dict[label] = defaultdict(int)\n    label_dict = defaultdict(int)\n\n    for pp, pl, labels, trgt in zip(pred_prop,pred_label,get_target(labels_info),target):\n        # label\u306f\u6b63\u89e3\u30e9\u30d9\u30eb\n        print pp, np.where(pp>=threshold)[0]+1, labels, trgt\n        # \u4e88\u6e2c\u3067\u51fa\u3066\u304d\u305f\u30e9\u30d9\u30eb\n        # softmax\u5316\u3057\u3066\u304a\u304f\n        predicted_labels = np.where(pp/np.sum(pp)>=threshold)[0]+1\n        # predicted_labels = [int(pl)]\n        # \u6b63\u89e3\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u30e9\u30d9\u30eb\n        F_labels = set([l+1 for l in xrange(label_num)]).difference(label)\n        # \u4e88\u6e2c\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u30e9\u30d9\u30eb\n        predicted_F_labels = \\\n                    set([l+1 for l in xrange(label_num)]).difference(predicted_labels)\n\n        # TP\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'TP labels:'\n        print set(labels).intersection(predicted_labels)\n        for tp_l in set(labels).intersection(predicted_labels):\n            res_dict['TP'][tp_l] += 1\n        # FP\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'FP labels:'\n        print set(predicted_labels).difference(labels)\n        for fp_l in set(predicted_labels).difference(labels):\n            res_dict['FP'][fp_l] += 1\n        # FN\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'FN labels'\n        print set(labels).difference(predicted_labels)\n        for fn_l in set(labels).difference(predicted_labels):\n            res_dict['FN'][fn_l] += 1\n        # TN\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'TN labels'\n        print set(F_labels).intersection(predicted_F_labels)\n        for tn_l in set(F_labels).intersection(predicted_F_labels):\n            res_dict['TN'][tn_l] += 1\n        # \u5404\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u6570\u3092\u30ab\u30a6\u30f3\u30c8\n        for l in labels:\n            label_dict[l] += 1\n\n    for i_label in xrange(label_num):\n        print \"label=\",i_label+1\n        print 'TP:', res_dict['TP'][i_label+1], 'FP:',res_dict['FP'][i_label+1], 'FN:', res_dict['FN'][i_label+1], 'TN:',res_dict['TN'][i_label+1]\n        print float(res_dict['TP'][i_label+1])/label_dict[i_label+1], float(res_dict['FP'][i_label+1])/label_dict[i_label+1], float(res_dict['FN'][i_label+1])/label_dict[i_label+1], float(res_dict['TN'][i_label+1])/label_dict[i_label+1]\n        accuracy = float(res_dict['TP'][i_label+1]+res_dict['TN'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FP'][i_label+1]+res_dict['FN'][i_label+1]+res_dict['TN'][i_label+1])\n        precision = float(res_dict['TP'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FP'][i_label+1])\n        recall = float(res_dict['TP'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FN'][i_label+1])\n        f_measure = (2*recall*precision)/(recall+precision)\n        print 'Accuracy:', accuracy, 'Precision:', precision, 'Recall:', recall, 'F-measure:', f_measure\n\n# \u6b63\u89e3\u30c7\u30fc\u30bf\u30e9\u30d9\u30eb\u30921\u304b\u3089n\u307e\u3067\u306e\u756a\u53f7\u306b\u4fee\u6b63\ndef get_target(labels_info):\n    result = []\n    raw_target = labels_info[:,1:]\n    for line in raw_target:\n        result.append( np.where(line==1)[0]+1 )\n    return result\n\ndef get_labels():\n    pass\n\ndef get_labels_info(label_fname):\n    label_flag = []\n    label_flag_apd = label_flag.append\n    labels_info = []\n    labels_info_apd = labels_info.append\n    with open(label_fname, 'r') as f:\n        for line in f:\n            data = line.strip().split('\\t')\n            label_flag_apd(int(data[0]))\n            labels_info_apd(\n                            np.array(data[1].strip().split(' '), dtype=np.int32 )\n                            )\n    return np.hstack( (np.array(label_flag).reshape((len(label_flag), 1)), np.array(labels_info)) )\n\nif __name__=='__main__':\n    ifname = 'DBN_features.csv'\n    label_fname = 'original_data.csv'\n\n    X =np.loadtxt(ifname, delimiter=',')\n    labels_info = get_labels_info(label_fname)\n\n    ## typ\u306f{knn,rbf}\u304b\u3089\u9078\u629e\n    main(X, labels_info, 50, label_num=6, n_neighbors=7, alpha=0.2, typ='knn', threshold=0.5)\n\n\n\u304a\u624b\u6570\u3067\u3059\u304c\u9593\u9055\u3044\u304c\u3042\u308a\u307e\u3057\u305f\u3089\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3067\u6587\u66f8\u5206\u985e\u3092\u3059\u308b\u305f\u3081\u306e\u30e9\u30d9\u30eb\u4ed8\u3051\u4f5c\u696d\u306b\u75b2\u308c\u305f\u65b9\u304c\u3044\u3089\u3063\u3057\u3083\u3063\u305f\u306e\u3067\u3001\u5c11\u306a\u3044\u30e9\u30d9\u30eb\u3067\u5206\u985e\u304c\u53ef\u80fd\u306a\u3088\u3046\u306b\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u3067\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u3092\u884c\u3046\u3082\u306e\u3092\u4f5c\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\n# \u53c2\u8003\u8cc7\u6599\n* [\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u6700\u5148\u7aef\u30ac\u30a4\u30c96 (CVIM\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u30b7\u30ea\u30fc\u30ba)](http://www.amazon.co.jp/gp/product/4915851540/ref=as_li_tf_tl?ie=UTF8&camp=247&creative=1211&creativeASIN=4915851540&linkCode=as2&tag=shimashimao06-22)<img src=\"http://ir-jp.amazon-adsystem.com/e/ir?t=shimashimao06-22&l=as2&o=9&a=4915851540\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\" />\n* [Deep Learning Tutorials](http://deeplearning.net/tutorial/)\n* [Learning with local and global consistency, Zhou+, 2004](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219)\n\n# \u3084\u3063\u305f\u3053\u3068\n* DBN\u3067\u7279\u5fb4\u91cf\u62bd\u51fa\n* LabelSpreading\u3067\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306b\u3088\u308b\u30e9\u30d9\u30eb\u4ed8\u3051\n\n# \u624b\u9806\n## DBN\u3067\u7279\u5fb4\u62bd\u51fa\n* \u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u9069\u5f53\u306b\u5c64\u306e\u6df1\u3055\u3001\u5b66\u7fd2\u4fc2\u6570\u3001\u5404\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u6c7a\u5b9a(Top\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306f\u5727\u7e2e\u5f8c\u306e\u6b21\u5143\u6570)\n* DBN\u3067pre-training\u3092\u884c\u3044weight\u3068\u30d0\u30a4\u30a2\u30b9\u3092\u5b66\u7fd2\n* Top\u5c64\u306e\u51fa\u529b\u3092\u6b21\u5143\u5727\u7e2e\u5f8c\u306e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u5206\u985e\u5668\u306b\u6295\u5165\u3059\u308b\n\n## LabelSpreading\u3067\u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\n* \u5404\u30ab\u30c6\u30b4\u30ea\u306e\u6559\u5e2b\u30e9\u30d9\u30eb\u306e\u4e0a\u9650\u6570\u3092\u6c7a\u3081\u308b\n* \u5404\u30ab\u30c6\u30b4\u30ea\u306e\u30c7\u30fc\u30bf\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3057\u3001\u4e0a\u9650\u6570\u307e\u3067\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\n* \u6559\u5e2b\u30e9\u30d9\u30eb\u306e\u4e0a\u9650\u6570\u306b\u9054\u3057\u305f\u5834\u5408\u3001\u30ab\u30c6\u30b4\u30ea\u5185\u306e\u6b8b\u308a\u306e\u30c7\u30fc\u30bf\u306f\u30e9\u30d9\u30eb\u306a\u3057\u30c7\u30fc\u30bf\u3068\u3057\u3066\u6271\u3046\n* \u5b66\u7fd2\u30d1\u30e9\u30e1\u30bf\u306e\u8a2d\u5b9a(\u4eca\u56de\u306fkNN\u3092\u4f7f\u7528\u3057\u305f\u306e\u3067\u3001\u30e9\u30d9\u30eb\u4f1d\u642c\u3055\u305b\u308b\u3054\u8fd1\u6240\u3055\u3093\u306e\u6570)\n\n# \u5229\u70b9\n## DBN\u3067\u7279\u5fb4\u62bd\u51fa\u3059\u308b\u5229\u70b9\n* SVD\u3068\u9055\u3044\u3001\u975e\u7dda\u5f62\u306a\u7279\u5fb4\u304c\u62bd\u51fa\u53ef\u80fd\n\n## LabelSpreading\u306e\u5229\u70b9\n* \u534a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u5168\u822c\u306e\u5229\u70b9\u306b\u306a\u3063\u3066\u3057\u307e\u3046\u306e\u3060\u304c\u3001\u4eba\u9593\u306e\u52b4\u529b\u3092\u6700\u5c0f\u9650\u306b\u6291\u3048\u3066\u6bd4\u8f03\u7684\u7cbe\u5ea6\u306e\u826f\u3044\u30e9\u30d9\u30eb\u4ed8\u3051\u304c\u53ef\u80fd\n\n# \u53c2\u8003\u30b3\u30fc\u30c9\n\u304b\u306a\u308a\u6c5a\u3044\u30b3\u30fc\u30c9\u3067\u7533\u3057\u8a33\u6709\u308a\u307e\u305b\u3093\u304c\u3001\u4ee5\u4e0b\u8cbc\u308a\u4ed8\u3051\u307e\u3059\u3002\nDBN\u306e\u30b3\u30fc\u30c9\u306fDeep Learning Tutorial\u306e\u4e38\u30b3\u30d4\u3067\u3059\u3002Deep Learning Tutorial\u306e\u30b5\u30a4\u30c8\u306b\u306f\u6570\u5f0f\u4ed8\u304d\u3067\u89e3\u8aac\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u8a73\u3057\u3044\u89e3\u8aac\u306f\u305d\u3061\u3089\u306b\u8b72\u308a\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n### DBN\u306e\u30b3\u30fc\u30c9\nBoW\u5f62\u5f0f\u306eCSV\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u7279\u5fb4\u62bd\u51fa\u3057\u305f\u3042\u3068\u306b\u3001CSV\u3092\u5410\u304d\u51fa\u3059\u611f\u3058\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\u4f7f\u7528\u3057\u3066\u3044\u306a\u3044\u30e1\u30bd\u30c3\u30c9\u3092\u3061\u3087\u3044\u3061\u3087\u3044\u542b\u3093\u3067\u3044\u307e\u3059\u306e\u3067\u3001\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002\n\n```DBN.py:\n# coding:utf-8\n\nfrom __future__ import unicode_literals\nimport time\n\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\n\nclass DBN:\n    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n                 hidden_layers_sizes=[500, 500], n_outs=10):\n        self.sigmoid_layers = []\n        self.rbm_layers = []\n        self.params = []\n        self.n_layers = len(hidden_layers_sizes)\n\n        assert self.n_layers > 0\n\n        if not theano_rng:\n            theano_rng = RandomStreams(numpy_rng.randint(2**30))\n\n        # allocate symbolic variables for the data\n        self.x = T.matrix('x')\n        self.y = T.ivector('y')\n\n        for i in xrange(self.n_layers):\n            if i==0:\n                input_size = n_ins\n                layer_input = self.x\n            else:\n                input_size = hidden_layers_sizes[i - 1]\n                layer_input = self.sigmoid_layers[-1].output\n\n            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n                                         input=layer_input,\n                                         n_in=input_size,\n                                         n_out=hidden_layers_sizes[i],\n                                         activation=T.nnet.sigmoid)\n            self.sigmoid_layers.append( sigmoid_layer )\n            self.params.extend(sigmoid_layer.params)\n\n            rbm_layer = RBM(numpy_rng=numpy_rng,\n                            theano_rng=theano_rng,\n                            input=layer_input,\n                            n_visible=input_size,\n                            n_hidden=hidden_layers_sizes[i],\n                            W=sigmoid_layer.W,\n                            hbias=sigmoid_layer.b)\n            self.rbm_layers.append(rbm_layer)\n\n            self.logLayer = LogisticRegression(\n                input=self.sigmoid_layers[-1].output,\n                n_in=hidden_layers_sizes[-1],\n                n_out=n_outs)\n            self.params.extend(self.logLayer.params)\n            self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n            self.errors = self.logLayer.errors(self.y)\n\n    def pretrainig_functions(self, train_set_x, batch_size, k):\n        index = T.lscalar('index')\n        learning_rate = T.scalar('lr')  # learning rate to use\n        # number of batches\n        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n        # begining of a batch, given `index`\n        batch_begin = index * batch_size\n        # ending of a batch given `index`\n        batch_end = batch_begin + batch_size\n\n        pretrain_fns = []\n        for rbm in self.rbm_layers:\n\n            cost, updates = rbm.get_cost_updates(learning_rate,\n                                                 persistent=None, k=k)\n\n            # compile the theano function\n            fn = theano.function(\n                inputs=[index, theano.Param(learning_rate, default=0.1)],\n                outputs=cost,\n                updates=updates,\n                givens={\n                    self.x: train_set_x[batch_begin:batch_end]\n                }\n            )\n            # append `fn` to the list of functions\n            pretrain_fns.append(fn)\n\n        return pretrain_fns\n\n\n    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n        (train_set_x, train_set_y) = datasets[0]\n        (valid_set_x, valid_set_y) = datasets[1]\n        (test_set_x, test_set_y) = datasets[2]\n\n        # compute number of minibatches for training, validation and testing\n        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n        n_valid_batches /= batch_size\n        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n        n_test_batches /= batch_size\n\n        index = T.lscalar('index')  # index to a [mini]batch\n\n        # compute the gradients with respect to the model parameters\n        gparams = T.grad(self.finetune_cost, self.params)\n\n        # compute list of fine-tuning updates\n        updates = []\n        for param, gparam in zip(self.params, gparams):\n            updates.append((param, param - gparam * learning_rate))\n\n        train_fn = theano.function(\n            inputs=[index],\n            outputs=self.finetune_cost,\n            updates=updates,\n            givens={\n                self.x: train_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: train_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        test_score_i = theano.function(\n            [index],\n            self.errors,\n            givens={\n                self.x: test_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: test_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        valid_score_i = theano.function(\n            [index],\n            self.errors,\n            givens={\n                self.x: valid_set_x[\n                    index * batch_size: (index + 1) * batch_size\n                ],\n                self.y: valid_set_y[\n                    index * batch_size: (index + 1) * batch_size\n                ]\n            }\n        )\n\n        # Create a function that scans the entire validation set\n        def valid_score():\n            return [valid_score_i(i) for i in xrange(n_valid_batches)]\n\n        # Create a function that scans the entire test set\n        def test_score():\n            return [test_score_i(i) for i in xrange(n_test_batches)]\n\n        return train_fn, valid_score, test_score\n\n\nclass HiddenLayer:\n    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n                 activation=T.tanh):\n        self.input = input\n\n        if W is None:\n            W_values = np.asarray(\n                rng.uniform(\n                    low=-np.sqrt(6. / (n_in+n_out)),\n                    high=np.sqrt(6. / (n_in+n_out)),\n                    size=(n_in, n_out)\n                ),\n                dtype=theano.config.floatX\n            )\n            if activation == theano.tensor.nnet.sigmoid:\n                W_values *=4\n            W = theano.shared(value=W_values, name='W', borrow=True)\n\n        if b is None:\n            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name='b', borrow=True)\n        self.W = W\n        self.b = b\n\n        lin_output = T.dot(input, self.W) + self.b\n        self.output = (\n              lin_output if activation is None\n              else activation(lin_output)\n        )\n        self.params = [self.W, self.b]\n\nclass LogisticRegression:\n    def __init__(self, input, n_in, n_out):\n        self.W = theano.shared(\n            value=np.zeros(\n                (n_in, n_out),\n                dtype=theano.config.floatX\n            ),\n            name='W',\n            borrow=True\n        )\n        # initialize the baises b as a vector of n_out 0s\n        self.b = theano.shared(\n            value=np.zeros(\n                (n_out,),\n                dtype=theano.config.floatX\n            ),\n            name='b',\n            borrow=True\n        )\n        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n        self.params = [self.W, self.b]\n\n    def negative_log_likelihood(self, y):\n        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n\n    def errors(self, y):\n        # check if y has same dimension of y_pred\n        if y.ndim != self.y_pred.ndim:\n            raise TypeError(\n                'y should have the same shape as self.y_pred',\n                ('y', y.type, 'y_pred', self.y_pred.type)\n            )\n        # check if y is of the correct datatype\n        if y.dtype.startswith('int'):\n            return T.mean(T.neq(self.y_pred, y))\n        else:\n            raise NotImplementedError()\n\n\nclass RBM(object):\n    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n    def __init__(\n        self,\n        input=None,\n        n_visible=784,\n        n_hidden=500,\n        W=None,\n        hbias=None,\n        vbias=None,\n        numpy_rng=None,\n        theano_rng=None\n    ):\n\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n\n        if numpy_rng is None:\n            # create a number generator\n            numpy_rng = np.random.RandomState(1234)\n\n        if theano_rng is None:\n            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n\n        if W is None:\n            initial_W = np.asarray(\n                numpy_rng.uniform(\n                    low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n                    high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n                    size=(n_visible, n_hidden)\n                ),\n                dtype=theano.config.floatX\n            )\n            # theano shared variables for weights and biases\n            W = theano.shared(value=initial_W, name='W', borrow=True)\n\n        if hbias is None:\n            # create shared variable for hidden units bias\n            hbias = theano.shared(\n                value=np.zeros(\n                    n_hidden,\n                    dtype=theano.config.floatX\n                ),\n                name='hbias',\n                borrow=True\n            )\n\n        if vbias is None:\n            # create shared variable for visible units bias\n            vbias = theano.shared(\n                value=np.zeros(\n                    n_visible,\n                    dtype=theano.config.floatX\n                ),\n                name='vbias',\n                borrow=True\n            )\n\n        # initialize input layer for standalone RBM or layer0 of DBN\n        self.input = input\n        if not input:\n            self.input = T.matrix('input')\n\n        self.W = W\n        self.hbias = hbias\n        self.vbias = vbias\n        self.theano_rng = theano_rng\n        self.params = [self.W, self.hbias, self.vbias]\n\n    def propup(self, vis):\n        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n\n\n    def sample_h_given_v(self, v0_sample):\n        ''' This function infers state of hidden units given visible units '''\n        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n                                             n=1, p=h1_mean,\n                                             dtype=theano.config.floatX)\n        return [pre_sigmoid_h1, h1_mean, h1_sample]\n\n\n    def propdown(self, hid):\n        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n\n\n    def sample_v_given_h(self, h0_sample):\n        ''' This function infers state of visible units given hidden units '''\n        # compute the activation of the visible given the hidden sample\n        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n                                             n=1, p=v1_mean,\n                                             dtype=theano.config.floatX)\n        return [pre_sigmoid_v1, v1_mean, v1_sample]\n\n\n    def gibbs_hvh(self, h0_sample):\n        ''' This function implements one step of Gibbs sampling,\n            starting from the hidden state'''\n        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n        return [pre_sigmoid_v1, v1_mean, v1_sample,\n                pre_sigmoid_h1, h1_mean, h1_sample]\n\n\n    def gibbs_vhv(self, v0_sample):\n        ''' This function implements one step of Gibbs sampling,\n            starting from the visible state'''\n        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n        return [pre_sigmoid_h1, h1_mean, h1_sample,\n                pre_sigmoid_v1, v1_mean, v1_sample]\n\n\n    def free_energy(self, v_sample):\n        ''' Function to compute the free energy '''\n        wx_b = T.dot(v_sample, self.W) + self.hbias\n        vbias_term = T.dot(v_sample, self.vbias)\n        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n        return -hidden_term - vbias_term\n\n\n    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n        # compute positive phase\n        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n\n        if persistent is None:\n            chain_start = ph_sample\n        else:\n            chain_start = persistent\n        (\n            [\n                pre_sigmoid_nvs,\n                nv_means,\n                nv_samples,\n                pre_sigmoid_nhs,\n                nh_means,\n                nh_samples\n            ],\n            updates\n        ) = theano.scan(\n            self.gibbs_hvh,\n            outputs_info=[None, None, None, None, None, chain_start],\n            n_steps=k\n        )\n\n        chain_end = nv_samples[-1]\n        cost = T.mean(self.free_energy(self.input)) - T.mean(\n            self.free_energy(chain_end))\n        # We must not compute the gradient through the gibbs sampling\n        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n\n\n        for gparam, param in zip(gparams, self.params):\n            # make sure that the learning rate is of the right dtype\n            updates[param] = param - gparam * T.cast(\n                lr,\n                dtype=theano.config.floatX\n            )\n        if persistent:\n            # Note that this works only if persistent is a shared variable\n            updates[persistent] = nh_samples[-1]\n            # pseudo-likelihood is a better proxy for PCD\n            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n        else:\n            # reconstruction cross-entropy is a better proxy for CD\n            monitoring_cost = self.get_reconstruction_cost(updates,\n                                                           pre_sigmoid_nvs[-1])\n\n        return monitoring_cost, updates\n\n    def get_pseudo_likelihood_cost(self, updates):\n        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n\n        # index of bit i in expression p(x_i | x_{\\i})\n        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n\n        # binarize the input image by rounding to nearest integer\n        xi = T.round(self.input)\n\n        # calculate free energy for the given bit configuration\n        fe_xi = self.free_energy(xi)\n\n        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n        # the result to xi_flip, instead of working in place on xi.\n        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n\n        # calculate free energy with bit flipped\n        fe_xi_flip = self.free_energy(xi_flip)\n\n        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n                                                            fe_xi)))\n\n        # increment bit_i_idx % number as part of updates\n        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n\n        return cost\n\n\n    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n        cross_entropy = T.mean(\n             T.sum(\n                 self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n                 (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n                 axis=1\n             )\n        )\n\n        return cross_entropy\n\ndef output(input_data, w, b):\n    x = np.dot(input_data,w)+np.kron( np.ones((input_data.shape[0],1)),b)\n    return 1/(1+np.exp(-x))\n\nif __name__=='__main__':\n    numpy_rng = np.random.RandomState(123)\n    print '... building the model'\n\n    ifname = 'bow_data.csv'\n    data = np.loadtxt(ifname, delimiter=',')\n    train_set_x = theano.shared(np.asarray(data, np.float64))\n\n    dbn = DBN(numpy_rng=numpy_rng, n_ins=data.shape[1],\n              hidden_layers_sizes=[2000, 1000, 100],\n              n_outs=10)\n    #########################\n    # PRETRAINING THE MODEL #\n    #########################\n    print '... getting the pretraining functions'\n    batch_size=10\n    k = 5\n    pretraining_fns = dbn.pretrainig_functions(train_set_x=train_set_x,\n                                                batch_size=batch_size,\n                                                k=k)\n\n    print '... pre-training the model'\n    pretraining_epochs = 100\n    n_train_batches = 10\n    pretrain_lr = 0.1\n    ## Pre-train layer-wise\n    for i in xrange(dbn.n_layers):\n        # go through pretraining epochs\n        for epoch in xrange(pretraining_epochs):\n            # go through the training set\n            c = []\n            for batch_index in xrange(n_train_batches):\n                c.append(pretraining_fns[i](index=batch_index,\n                                            lr=pretrain_lr))\n            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n            print np.mean(c)\n\n    layer_output =[]\n    for i in xrange(dbn.n_layers):\n        w = dbn.rbm_layers[i].W.get_value()\n        hbias = dbn.rbm_layers[i].hbias.get_value()\n        if i==0:\n            layer_output.append( train_set_x.get_value() )\n            layer_output.append( output(layer_output[-1],w, hbias) )\n        else:\n            layer_output.append( output(layer_output[-1],w, hbias) )\n    print layer_output[-1]\n    np.savetxt('DBN_features.csv',layer_output[-1], delimiter=',')\n```\n\n### LabelSpreading\u306e\u30b3\u30fc\u30c9\noriginal_data.csv\u306e\u4e2d\u8eab\u306f(0\u00a5t1 0 1 0 0 0 0\u00a5txxxx)\u30671\u884c\u304c\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\u4f55\u3092\u8868\u3057\u3066\u3044\u308b\u304b\u3068\u3044\u3046\u3068\u3001\u5de6\u304b\u3089\u3001\n\u8907\u6570\u30af\u30e9\u30b9\u306b\u307e\u305f\u304c\u3063\u3066\u3044\u308b\u30c7\u30fc\u30bf\u304b\u3069\u3046\u304b\u306eflag\u3001\u5404\u30af\u30e9\u30b9\u306e\u30d5\u30e9\u30b0\u3001\u30c6\u30ad\u30b9\u30c8\n\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u306f\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306b\u3001\u306a\u308b\u3079\u304f1\u3064\u3057\u304b\u30e9\u30d9\u30eb\u304c\u4ed8\u3044\u3066\u3044\u306a\u3044\u30c7\u30fc\u30bf\u3092\u6559\u5e2b\u30c7\u30fc\u30bf\u306b\u3059\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e00\u5fdciris\u30c7\u30fc\u30bf\u3067\u78ba\u8a8d\u51fa\u6765\u308b\u3088\u3046\u306b\u95a2\u6570\u3092\u3064\u3051\u3066\u304a\u304d\u307e\u3057\u305f\u306e\u3067\u3001\u624b\u5143\u306b\u9069\u5f53\u306a\u30c7\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u3067LabelSpreading\u3092\u8a66\u3057\u305f\u3044\u5834\u5408\u306f\u304a\u4f7f\u3044\u304f\u3060\u3055\u3044\u3002\n\n```LabelSpreading.py:\n# coding: utf-8\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import LabelSpreading\nimport numpy as np\nfrom numpy.random import seed\nseed(555)\nfrom collections import defaultdict\n\ndef iris():\n    iris = datasets.load_iris()\n    random_unlabeled_points = np.where(np.random.random_integers(0, 1, size=len(iris.target)))\n    labels = np.copy(iris.target)\n    labels[random_unlabeled_points] = -1\n\n    label_prop_model = LabelSpreading()\n    label_prop_model.fit(iris.data, labels) # unlabeled as -1\n    pred_prop = label_prop_model.predict_proba(iris.data)\n    pred_label = label_prop_model.predict(iris.data)\n\n    for pp, pl, label, trgt in zip(pred_prop,pred_label,labels,iris.target):\n        print pp, pl, label, trgt\n\ndef main(X, labels_info, min_number=20, label_num=6, n_neighbors=7, alpha=0.3, typ='knn', threshold=0.5):\n    target = get_target(labels_info)\n    random_unlabeled_points = np.where(np.random.random_integers(0, 1, size=len(target)))[0]\n    cnt_dict = defaultdict(int)\n    for i, t in enumerate(target):\n        if len(t)==1 and (i in random_unlabeled_points):\n            target[i] = -1\n            cnt_dict[-1] += 1\n        elif len(t)>=2:\n            target[i] = -1\n            cnt_dict[-1] += 1\n        elif cnt_dict[target[i][0]]<min_number:\n            target[i] = target[i][0]\n            cnt_dict[target[i]] += 1\n        elif cnt_dict[target[i][0]]>=min_number:\n            target[i] = -1\n            cnt_dict[target[i]] += 1\n    print cnt_dict\n\n    if typ=='knn':\n        label_prop_model = LabelSpreading(kernel=typ, n_neighbors=n_neighbors)\n    else:\n        label_prop_model = LabelSpreading(kernel=typ, alpha=alpha)\n    label_prop_model.fit(X, target) # unlabeled as -1\n    pred_prop = label_prop_model.predict_proba(X)\n    pred_label = label_prop_model.predict(X)\n\n    res_dict = defaultdict(dict)  # TP, FP, FN, TN\u3092\u683c\u7d0d\u3059\u308b\n    for label in ('TP', 'FP', 'FN', 'TN'):\n        res_dict[label] = defaultdict(int)\n    label_dict = defaultdict(int)\n\n    for pp, pl, labels, trgt in zip(pred_prop,pred_label,get_target(labels_info),target):\n        # label\u306f\u6b63\u89e3\u30e9\u30d9\u30eb\n        print pp, np.where(pp>=threshold)[0]+1, labels, trgt\n        # \u4e88\u6e2c\u3067\u51fa\u3066\u304d\u305f\u30e9\u30d9\u30eb\n        # softmax\u5316\u3057\u3066\u304a\u304f\n        predicted_labels = np.where(pp/np.sum(pp)>=threshold)[0]+1\n        # predicted_labels = [int(pl)]\n        # \u6b63\u89e3\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u30e9\u30d9\u30eb\n        F_labels = set([l+1 for l in xrange(label_num)]).difference(label)\n        # \u4e88\u6e2c\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u30e9\u30d9\u30eb\n        predicted_F_labels = \\\n                    set([l+1 for l in xrange(label_num)]).difference(predicted_labels)\n\n        # TP\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'TP labels:'\n        print set(labels).intersection(predicted_labels)\n        for tp_l in set(labels).intersection(predicted_labels):\n            res_dict['TP'][tp_l] += 1\n        # FP\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'FP labels:'\n        print set(predicted_labels).difference(labels)\n        for fp_l in set(predicted_labels).difference(labels):\n            res_dict['FP'][fp_l] += 1\n        # FN\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'FN labels'\n        print set(labels).difference(predicted_labels)\n        for fn_l in set(labels).difference(predicted_labels):\n            res_dict['FN'][fn_l] += 1\n        # TN\u3092\u8f9e\u66f8\u306b\u683c\u7d0d\n        print 'TN labels'\n        print set(F_labels).intersection(predicted_F_labels)\n        for tn_l in set(F_labels).intersection(predicted_F_labels):\n            res_dict['TN'][tn_l] += 1\n        # \u5404\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u6570\u3092\u30ab\u30a6\u30f3\u30c8\n        for l in labels:\n            label_dict[l] += 1\n\n    for i_label in xrange(label_num):\n        print \"label=\",i_label+1\n        print 'TP:', res_dict['TP'][i_label+1], 'FP:',res_dict['FP'][i_label+1], 'FN:', res_dict['FN'][i_label+1], 'TN:',res_dict['TN'][i_label+1]\n        print float(res_dict['TP'][i_label+1])/label_dict[i_label+1], float(res_dict['FP'][i_label+1])/label_dict[i_label+1], float(res_dict['FN'][i_label+1])/label_dict[i_label+1], float(res_dict['TN'][i_label+1])/label_dict[i_label+1]\n        accuracy = float(res_dict['TP'][i_label+1]+res_dict['TN'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FP'][i_label+1]+res_dict['FN'][i_label+1]+res_dict['TN'][i_label+1])\n        precision = float(res_dict['TP'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FP'][i_label+1])\n        recall = float(res_dict['TP'][i_label+1])/(res_dict['TP'][i_label+1]+res_dict['FN'][i_label+1])\n        f_measure = (2*recall*precision)/(recall+precision)\n        print 'Accuracy:', accuracy, 'Precision:', precision, 'Recall:', recall, 'F-measure:', f_measure\n\n# \u6b63\u89e3\u30c7\u30fc\u30bf\u30e9\u30d9\u30eb\u30921\u304b\u3089n\u307e\u3067\u306e\u756a\u53f7\u306b\u4fee\u6b63\ndef get_target(labels_info):\n    result = []\n    raw_target = labels_info[:,1:]\n    for line in raw_target:\n        result.append( np.where(line==1)[0]+1 )\n    return result\n\ndef get_labels():\n    pass\n\ndef get_labels_info(label_fname):\n    label_flag = []\n    label_flag_apd = label_flag.append\n    labels_info = []\n    labels_info_apd = labels_info.append\n    with open(label_fname, 'r') as f:\n        for line in f:\n            data = line.strip().split('\\t')\n            label_flag_apd(int(data[0]))\n            labels_info_apd(\n                            np.array(data[1].strip().split(' '), dtype=np.int32 )\n                            )\n    return np.hstack( (np.array(label_flag).reshape((len(label_flag), 1)), np.array(labels_info)) )\n\nif __name__=='__main__':\n    ifname = 'DBN_features.csv'\n    label_fname = 'original_data.csv'\n\n    X =np.loadtxt(ifname, delimiter=',')\n    labels_info = get_labels_info(label_fname)\n\n    ## typ\u306f{knn,rbf}\u304b\u3089\u9078\u629e\n    main(X, labels_info, 50, label_num=6, n_neighbors=7, alpha=0.2, typ='knn', threshold=0.5)\n```\n\n\u304a\u624b\u6570\u3067\u3059\u304c\u9593\u9055\u3044\u304c\u3042\u308a\u307e\u3057\u305f\u3089\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n", "tags": ["Python", "DeepLearning", "\u6a5f\u68b0\u5b66\u7fd2", "Theano", "scikit-learn"]}