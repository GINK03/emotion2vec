{"context": " More than 1 year has passed since last update.\n\n\u52d5\u6a5f\nURL\u3092\u30ab\u30c6\u30b4\u30ea\u5206\u3051\u3057\u305f\u3044\u3068\u8003\u3048b-Bit MinHash\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\u5177\u4f53\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3053\u3068\u3092\u3059\u308b\u305f\u3081\u3067\u3059\u3002\ncookie\u304b\u3089\u62bd\u51fa\u3057\u305f\u30b5\u30a4\u30c8\u95b2\u89a7\u60c5\u5831\u3092\u7d20\u6027\u3068\u3057\u3066\u30e6\u30fc\u30b6\u306eCV\u78ba\u7387\u3092\u6c42\u3081\u308b\u5834\u5408\u3001\u7d20\u6027\u30d9\u30af\u30c8\u30eb\u304c\u304b\u306a\u308a\u30b9\u30d1\u30fc\u30b9\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002\u305d\u3053\u3067\u3001URL\u5358\u4f4d\u3067\u30ab\u30c6\u30b4\u30ea\u306b\u5206\u3051\u3001\u3042\u308b\u7a0b\u5ea6\u7d20\u6027\u30d9\u30af\u30c8\u30eb\u306e\u5bc6\u5ea6\u3092\u4e0a\u3052\u305f\u3044\u3068\u8003\u3048\u307e\u3057\u305f\u3002\u305d\u306e\u305f\u3081\u306b\u6a19\u984c\u306b\u3064\u3044\u3066\u5b9f\u9a13\u3092\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u3053\u306e\u30d6\u30ed\u30b0\u306b\u66f8\u3044\u305f\u3053\u3068\n\nb-Bit MinHash\u306e\u6982\u8981\n\u53c2\u8003\u30b3\u30fc\u30c9\n\n\nb-Bit MinHash\u306e\u6982\u8981\n\n\u53c2\u8003\u6587\u732e\n\nb-Bit Minwise Hashing\nb-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions\n\nyono\u7814\u4fee\u5ba4/Python\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u203b\u8a18\u4e8b\u306f\u53e4\u3044\u3067\u3059\u304cpython\u3067\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u3064\u3044\u3066\u5168\u4f53\u7684\u306b\u307e\u3068\u3081\u3066\u3042\u308a\u307e\u3059\n\nBeautifulSoup \u3067 HTML \u6587\u66f8\u304b\u3089\u30bf\u30b0\u3092\u53d6\u308a\u9664\u304f\uff08Python\uff09 \u203b\u30af\u30ed\u30fc\u30e9\u95a2\u4fc2\u306f\u3053\u3061\u3089\u306b\u5927\u5909\u304a\u4e16\u8a71\u306b\u306a\u308a\u307e\u3057\u305f\n\nb-Bit MinHash\u306b\u3088\u308b\u9ad8\u901f\u304b\u3064\u7701\u30b9\u30da\u30fc\u30b9\u306a\u985e\u4f3c\u5ea6\u5224\u5b9a \u203bSmartNews\u3055\u3093\u306e\u958b\u767a\u30d6\u30ed\u30b0\u3067\u3059\u3002\u6a19\u984c\u306e\u6982\u8981\u3092\u77e5\u308b\u306b\u306f\u6700\u9ad8\u306e\u8cc7\u6599\u3060\u3068\u601d\u3044\u307e\u3059\n\u96c6\u5408\u77e5\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\n\n\nb-Bit MinHash\u306e\u624b\u9806\n\u624b\u6cd5\u306e\u6982\u8981\u306b\u3064\u3044\u3066\u306f\u3001\u4e0a\u306eSmartNews\u3055\u3093\u306e\u958b\u767a\u30d6\u30ed\u30b0\u3068\u305d\u306e\u6587\u4e2d\u306b\u51fa\u3066\u304f\u308bPFI\u306e\u5ca1\u91ce\u539f\u3055\u3093\u306e\u8cc7\u6599\u3092\u53c2\u7167\u3057\u3066\u3044\u305f\u3060\u304d\u305f\u304f\u601d\u3044\u307e\u3059\u3002\u79c1\u304c\u8aac\u660e\u3092\u66f8\u304f\u3088\u308a\u306f\u308b\u304b\u306b\u308f\u304b\u308a\u3084\u3059\u304f\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u3057\uff57\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u3053\u3053\u3067\u306f\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u969b\u306e\u624b\u9806\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u66f8\u304d\u307e\u3059\u3002\n\n\u307e\u305a\u306f\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n\n\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\uff08\u30ab\u30c6\u30b4\u30ea\u3092\u4ed8\u3057\u305f\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u5bfe\u8c61\u306eURL\u306e\u30ea\u30b9\u30c8\u3092\u6e96\u5099\uff09\nURL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u5358\u8a9e\u96c6\u5408\u3001\u6700\u5c0fHash\u5024\u5185\u306ebi-Bit\u5206\u306ek\u500b\u306e\u30ea\u30b9\u30c8\uff08\u4ee5\u4e0b\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\uff09\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u308b\uff08k\u306fhash\u95a2\u6570\uff08seed\uff09\u306e\u7a2e\u985e\uff09\nURL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u5b58\n\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u304b\u3089\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u66f4\u65b0\u3059\u308b\n\n\n\u6b21\u306blearner\u306e\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n\nlearner\u306eURL\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\nURL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u5358\u8a9e\u96c6\u5408\u3001\u6700\u5c0fHash\u5024\u5185\u306ebi-Bit\u5206\u306ek\u500b\u306e\u30ea\u30b9\u30c8\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u308b\nURL\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u5b58\n\n\n\u5206\u985e\u306e\u5b9f\u65bd\n\nlearner\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u5404URL\u304b\u3089\u62bd\u51fa\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u547c\u3073\u51fa\u3057\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3001DB\u306e\u66f4\u65b0\n\u4f5c\u6210\u3057\u305f\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3068\u6559\u5e2bDB\u306e\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3067MinHash\u306e\u5b9f\u65bd\n\u30ab\u30c6\u30b4\u30ea\u3092\u53d6\u5f97\u3057DB\u66f4\u65b0\n\n\n\u53c2\u8003\u30b3\u30fc\u30c9\n\u4ee5\u4e0b\u3001\u4f5c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u3092\u8a18\u8f09\u3057\u307e\u3057\u305f\u3002\n\u3044\u308d\u3044\u308d\u5207\u308a\u8cbc\u308a\u3057\u3066\u4f5c\u3063\u305f\u306e\u3067\u3001\u547d\u540d\u898f\u5247\u304c\u3050\u3061\u3083\u3050\u3061\u3083\u3067\u3059\u3002\u3002\n\u4f8b\u306b\u3088\u3063\u3066\u30a8\u30f3\u30b8\u30cb\u30a2\u3067\u306a\u3044\u4eba\u9593\u304c\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u76f8\u5f53\u304a\u898b\u82e6\u3057\u3044\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u5e83\u3044\u5fc3\u3067\u8aad\u3093\u3067\u3044\u305f\u3060\u3051\u308b\u3068\u975e\u5e38\u306b\u52a9\u304b\u308a\u307e\u3059\u3002\u3002\n\n\u6559\u5e2bURL\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u3001\u5358\u8a9e\u96c6\u5408\u306a\u3069\u3092DB\u306b\u683c\u7d0d\n# coding: utf-8\n\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup, NavigableString, \\\n                             Declaration, Comment, BeautifulStoneSoup\nimport unicodedata\nimport chardet\nfrom urlparse import urljoin\nimport sqlite3 as sqlite\nimport pickle\nimport SeparateWords as spw\n\n# \u7121\u8996\u3059\u3079\u304d\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\nignorewords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', '-', '/', '#', \"'\", '_', ',', '(', ')', '[', ']', '\u301c', '!', '|', '\u266a', '...', '>', '<', ':', '!!', '&', '/', '+', '*'])\n# \u30d6\u30ed\u30c3\u30af\u3068\u3057\u3066\u8a8d\u8b58\u3059\u308b\u30bf\u30b0\u306e\u30ea\u30b9\u30c8\nblock_tags = frozenset(['p', 'div', 'table', 'dl', 'ul', 'ol', 'form', 'address',\n                   'blockquote', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'fieldset',\n                   'hr', 'pre' 'article', 'aside', 'dialog', 'figure',\n                   'footer', 'header', 'legend', 'nav', 'section'])\n\nclass crawler:\n    # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u540d\u524d\u3067\u30af\u30ed\u30fc\u30e9\u3092\u521d\u671f\u5316\n    def __init__(self, dbname):\n        self.dbname = dbname\n        self.con=sqlite.connect(dbname)\n        self.con.text_factory = str # utf-8\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306bstr\u3092\u6307\u5b9a\n\n    def __del__(self):\n        self.con.close()\n\n    def dbcommit(self):\n        self.con.commit()\n\n    # \u500b\u3005\u306e\u30da\u30fc\u30b8\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3059\u308b\n    def addtoindex(self,url,category,wordlist):\n        if self.isindexed(url): return\n        print 'Indexing' + url\n\n        # url\u6bce\u306b\u5358\u8a9e\u30ea\u30b9\u30c8\u3092DB\u306b\u683c\u7d0d\n        self.con.execute( \"insert into wordvector values(?,?,?,?)\" , \\\n                            (url, category, pickle.dumps(wordlist), '') )\n        self.dbcommit()\n\n    def getNavigableStrings(self,soup):\n        # soup\u304ccontents\u3092\u542b\u3080\u578b\u304b\u3092\u5224\u5b9a\n        if isinstance(soup, NavigableString): # NavigableString\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306fcontents\u3068string\u3092\u9664\u304f\u5168\u3066\u306e\u30e1\u30f3\u30d0\u5909\u6570\u3092\u6301\u3064\n            if type(soup) not in (Comment, Declaration):\n                yield soup\n        # contents\u3092\u542b\u3093\u3067\u3044\u3066program code\u3067\u306a\u3051\u308c\u3070\u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\n        elif soup.name not in ('script', 'style'):\n            is_block = soup.name in block_tags # \u30d6\u30ed\u30c3\u30af\u30bf\u30b0\u306e\u5b58\u5728\u3092\u78ba\u8a8d\n            if is_block:\n                yield u'\\n'\n            for c in soup.contents:\n                for g in self.getNavigableStrings(c):\n                    yield replace_str(g)\n            if is_block:\n                yield u'\\n'\n\n    # URL\u304c\u65e2\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3055\u308c\u3066\u3044\u305f\u3089ture\u3092\u8fd4\u3059\n    def isindexed(self,url):\n        u=self.con.execute \\\n            (\"select words from wordvector where url='%s'\" % url).fetchone()\n        if u!=None:\n            return True\n        return False    \n\n    # \u30da\u30fc\u30b8\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u3001\u4e0e\u3048\u3089\u308c\u305f\u6df1\u3055\u3067\u5e45\u512a\u5148\u306e\u691c\u7d22\u3092\u884c\u3044\n    # \u30da\u30fc\u30b8\u3092\u30a4\u30f3\u30c7\u30af\u30b7\u30f3\u30b0\u3059\u308b\n    def crawl(self,pages,category,depth=2):\n        for _ in xrange(depth):\n            newpages=set()\n            for page in pages:\n                try:\n                    response=urllib2.urlopen(page).read()\n                except:\n                    print \"Could not open %s\" % page\n                    continue\n                en = chardet.detect(response)['encoding']\n                soup=BeautifulSoup(response.decode(en), # \u6587\u5b57\u30b3\u30fc\u30c9\u3092\u30b5\u30a4\u30c8\u306b\u5408\u308f\u305b\u3066\u5909\u63db\n                        convertEntities = BeautifulStoneSoup.HTML_ENTITIES) # HTML\u7279\u6b8a\u6587\u5b57\u306e\u6587\u5b57\u5909\u63db\n                text = u''.join( self.getNavigableStrings(soup) )\n                text = normalizeText(text) # \u6587\u5b57\u5217\u306e\u6b63\u898f\u5316\n                words = spw.separateWords(text) # \u5358\u8a9e\u96c6\u5408\u306e\u53d6\u5f97\n                self.addtoindex(page,category,words) # url, category, \u5358\u8a9e\u96c6\u5408\u3092DB\u306b\u4fdd\u5b58\n\n                # \u6df1\u30552\u4ee5\u4e0a\u306e\u63a2\u7d22\u3092\u884c\u3046\u5834\u5408\u306e\u305f\u3081\u306ba\u30bf\u30b0\u306eURL\u3092\u53d6\u5f97\n                links=soup('a')\n                for link in links:\n                    if ('href' in dict(link.attrs)):\n                        url=urljoin(page,link['href'])\n                        if url.find(\"'\")!=-1: continue\n                        url=url.split('#')[0] # \u30d5\u30e9\u30b0\u30e1\u30f3\u30c8\u8b58\u5225\u5b50\u3092\u53d6\u308a\u9664\u304f\n                        if url[0:4]=='http' and not self.isindexed(url):\n                            newpages.add(url)\n                pages=newpages\n\n    # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u308b\n    def createindextables(self):\n        name = 'wordvector'\n        sql=\"SELECT name FROM sqlite_master WHERE type='table' AND name='MYTABLE';\" \\\n                .replace('MYTABLE', name)\n        res = self.con.execute(sql).fetchone()\n        if res is None:\n            self.con.execute('create table wordvector(url, category, words, hash_vector)')\n            self.con.execute('create index urlidx on wordvector(url)')\n            self.dbcommit()\n        else:\n            self.con.execute('drop table wordvector')\n            self.con.execute('create table wordvector(url, category, words, hash_vector)')\n            self.con.execute('create index urlidx on wordvector(url)')\n            self.dbcommit()            \n\ndef nonEmptyLines(text):\n    \"\"\" \u4e0d\u8981\u306a\u7a7a\u767d\u3092\u53d6\u308a\u9664\u304d\uff0c\u7a7a\u884c\u4ee5\u5916\u3092\u8fd4\u3059 \"\"\"\n    for line in text.splitlines():\n        line = u' '.join(line.split())\n        if line:\n            yield line\n\ndef normalizeText(text):\n    \"\"\" \u6b63\u898f\u5316\u306e\u5f8c\u3067\u4e0d\u8981\u306a\u7a7a\u767d\u30fb\u6539\u884c\u3092\u53d6\u308a\u9664\u304f \"\"\"\n    Text = unicodedata. normalize ('NFKC', text) # \u65e5\u672c\u8a9e\u534a\u89d2\u3092\u5168\u89d2\u306b\u5909\u63db+\u82f1\u6570\u5b57,\u8a18\u53f7\u306f\u534a\u89d2\n    return u'\\n'.join(nonEmptyLines(text))\n\ndef replace_str(line):\n    return line.replace('&amp;','&')\n\nif __name__=='__main__':    \n    fname = 'teacherURL.txt'\n    crawler=crawler('teacher.db')\n    crawler.createindextables()\n    with open(fname, 'r') as f:\n        for line in f:\n            s = line.strip().split('\\t')\n            print s[0]\n            category = s[0]\n            pagelist = [s[1]]\n            crawler.crawl(pagelist, category, depth=1)\n\n\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u5358\u8a9e\u3092\u62bd\u51fa\u3059\u308b\u90e8\u5206\uff08spw.separateWords(text)\uff09\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30fc\u30c9\u306b\u3057\u307e\u3057\u305f\u3002\n\n\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u5358\u8a9e\u3092\u62bd\u51fa\n# coding: utf-8\nimport MeCab\n\n# \u7121\u8996\u3059\u3079\u304d\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\nignorewords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', '\u306e'])\nigonoresymbols = set(['-', '/', '#', \"'\", '_', ',', '(', ')', '[', ']',\n                     '~', '!', '|', '\u266a', '...', '>', '<', ':', '!',\n                      '&', '/', '+', '*', '\u3010', '\u3011', '\uff08', '\uff09', '\uff01', '\uff1a', '\u30fc',\n                      '\uff3f', '\uff1f','%', '\u300c', '\u300d','\uff5e','.', '{', '}','\"',\n                      '\uff1c', '\uff1e', '\uff0f'])\n\ndef separateWords(text):\n    tagger = MeCab.Tagger('-Ochasen')\n    node = tagger.parseToNode(text.encode('utf-8'))\n    word_set = set([])\n    while node:\n        feat = node.feature.split(\",\")\n        if feat[0] == \"\u540d\u8a5e\" \\\n                and (feat[1] == \"\u4e00\u822c\"\n                or feat[1] == \"\u56fa\u6709\u540d\u8a5e\" or feat[1] == \"\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79\"):\n            #word = node.surface.decode(en)\n            word = node.surface\n            if word not in ignorewords:\n                word_set.add( word )\n        node = node.next\n    return word_set\n\n\u7d9a\u3044\u3066\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u304b\u3089\u6559\u5e2b\u7528\u306e\u6700\u5c0fhash\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001DB\u3092\u66f4\u65b0\u3059\u308b\u90e8\u5206\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n\u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u30fb\u4fdd\u5b58\n# coding: utf-8\n\nimport sqlite3 as sqlite\nimport pickle\nimport hashing\n\nclass CreateDB(object):\n    def __init__(self,dbname):\n        self.db = sqlite.connect(dbname)\n        self.db.text_factory = str # utf-8\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306bstr\u3092\u6307\u5b9a\n        self.hashing = hashing.Hashing()\n\n    def __del__(self):\n        self.db.close()\n\n    def db_commit(self):\n        self.db.commit()\n\n    def create_hashtable(self, url, wordlist):\n        hash_v = self.hashing.get_min_vector(wordlist)\n        self.update_vector(url, pickle.dumps(hash_v))\n        self.db_commit()\n\n    def update_vector(self,url,vector):\n        self.db.execute(\"update wordvector set hash_vector=? where url=?\" , \\\n                         (vector, url))\n\nif __name__=='__main__':\n    dbname = 'teachre.db'\n\n    c_db = CreateDB(dbname)\n\n    con = sqlite.connect(dbname) # word\u30d9\u30af\u30c8\u30eb\u304c\u683c\u7d0d\u3055\u308c\u305fDB\u3092\u547c\u3076\n\n    itr = 1\n    while True:\n        sql = \"select * from wordvector where rowid=='%d'\"        \n        res = con.execute(sql % itr).fetchone()\n        print res\n\n        if res is None:break\n\n        url = res[0]; category = res[1]; wordlist = res[2]\n        c_db.create_hashtable(url, wordlist)\n\n        itr += 1\n    con.close()\n\n\u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n\u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\n# coding: utf-8\n\nimport numpy as np\nimport mmh3\nfrom multiprocessing import *\n\nclass Hashing(object):\n    ''' \n      \u5404url\u3067\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u306b\u5bfe\u3057\u3066\u3001hashing\u3092\u304b\u3051\u3066\u3001\u6700\u5c0f\u5024\u3092\u53d6\u5f97\n      url\u6bce\u306bk\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\u3059\u308b\n      \u3053\u306ek\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u985e\u4f3c\u5ea6\u3092\u5224\u5b9a\u3059\u308b\n    '''\n    def __init__(self,bbit=1,k=128):\n        self.bbit = bbit\n        self.hash_cnt = k\n\n    def get_min_vector(self,feat_names):\n        ''' \u3053\u306e\u95a2\u6570\u306furl\u5206\u56de\u3059\u3053\u3068\u306b\u306a\u308b '''\n        hash_vec = []\n        # \u5358\u8a9e\u96c6\u5408\u306ehash\u306e\u6700\u5c0f\u5024\u306e\u307f\u3092k\u56de\u53d6\u5f97\n        for seed in xrange(self.hash_cnt): # self.hash_cnt\u5206\u306eseed\u3092\u767a\u751f\u3055\u305b\u308b\n            pool = Pool(processes=8)\n            hash_val = pool.map( get_hash_value, ((feat_name, seed) for feat_name in feat_names) ) # k\u500b\u306eseed\u3092\u4f7f\u3063\u3066k\u500b\u306ehash\u95a2\u6570\u3092\u767a\u751f\n            pool.close()\n            pool.join()\n            min_val = int( str( min(hash_val) )[-self.bbit] ) \n            hash_vec.append(min_val) # \u6700\u5c0f\u5024\u306e1\u6841\u76ee\u306e\u307f\u53d6\u5f97\u3059\u308b\n        return hash_vec\n\ndef get_hash_value((feat_name,seed)):\n    return mmh3.hash(feat_name,seed)\n\n\u3053\u3053\u307e\u3067\u3067\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\u304c\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\n\u540c\u69d8\u306b\u5b66\u7fd2\u8005\u7528\u306e\u5358\u8a9e\u96c6\u5408\u3092\u53d6\u5f97\u3057\u3001DB\u306b\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u4ee5\u4e0b\u3067\u306f\u53d6\u5f97\u3057\u305f\u5b66\u7fd2\u8005\u306e\u5358\u8a9e\u96c6\u5408\u3068\u4f5c\u6210\u3057\u305f\u6559\u5e2bhash\u30ea\u30b9\u30c8\u3092\u4f7f\u3063\u3066MinHash\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u305d\u3057\u3066\u3001\u5b66\u7fd2\u8005\u306eURL\u306b\u30ab\u30c6\u30b4\u30ea\u3092\u4ed8\u4e0e\u3057\u3001DB\u3092\u66f4\u65b0\u3057\u3066\u7d42\u4e86\u3067\u3059\u3002\n\u4ee5\u4e0b\u3001\u30b3\u30fc\u30c9\u8a18\u8f09\u3057\u307e\u3059\u3002\n\nb-BIt MinHash\u306e\u5b9f\u65bd\n# coding: utf-8\n\nimport numpy as np\nimport sqlite3 as sqlite\nimport hashing\nimport pickle\nfrom multiprocessing import *\n\nclass MinHash(object):\n    def __init__(self,train_dbname,teacher_dbname):\n        self.train_db = sqlite.connect(train_dbname)\n        self.teacher_db = sqlite.connect(teacher_dbname)\n        self.hashing = hashing.Hashing() # default values are bbit=1, k=128\n\n    def __del__(self):\n        self.train_db.close()\n        self.teacher_db.close()\n\n    def db_commit(self):\n        self.train_db.commit()\n\n    def get_category(self):\n        learner_urls = self.get_urls(self.train_db)\n        teacher_urls = self.get_urls(self.teacher_db)\n        for lrnr_url in learner_urls:\n            print \"computing hash vector:\", lrnr_url[0]\n            learner_hash =  self.calculate_hashvecotr( lrnr_url[0] )\n\n            print \"calculating similarity:\", lrnr_url[0]\n            learner_words =  self.get_wordslist( self.train_db,lrnr_url[0] )\n\n            pool = Pool(processes=8)\n            sim = pool.map(  similarity_value,\n                        ( (learner_words, self.get_wordslist( self.teacher_db,tchr_url[0] ),\n                             learner_hash, self.get_hash_vector(tchr_url[0]))\n                                                for tchr_url in teacher_urls )  )  # \u985e\u4f3c\u5ea6\u306e\u7b97\u51fa\n            pool.close()\n            pool.join()\n\n            sim = np.array(sim)\n            print \"sim: \",sim\n            idx = np.where(sim==max(sim))[0][0]\n            sim_url = teacher_urls[idx][0]\n\n            category = self.get_similer_category( sim_url )\n            print \"similar category of this URL is: \", category\n            self.update_category(category, sim_url)\n\n    def calculate_hashvecotr(self, url):\n        \"\"\" hash\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u308a\u3064\u3064\u3001\u5143\u306eDB\u3092update \"\"\"\n        hash_vector = self.hashing.get_min_vector( \n                            self.get_wordslist(self.train_db, url) )\n        self.train_db.execute( \"update wordvector set hash_vector=? where url=?\" , \n                                    (url, pickle.dumps(hash_vector), ) ) # pickle\u3067\u4fdd\u5b58\u3059\u308b\n        self.db_commit()\n        return hash_vector\n\n    def update_category(self,category,url):\n        self.train_db.execute( \"update wordvector set category=? where url=?\" ,(category, url, ) )\n        self.db_commit()\n\n    def get_wordslist(self,db,url):\n        wordlist = db.execute( \"select words from wordvector where url=?\" , (url,) ).fetchone()[0]\n        return pickle.loads(wordlist)\n\n    def get_urls(self,db):\n        return db.execute(\"select url from wordvector\").fetchall()\n\n    def get_similer_category(self,url):\n        return self.teacher_db.execute( \"select category from wordvector where url=?\" ,(url, ) ).fetchone()[0]\n\n    def get_hash_vector(self,url):\n        hash_v = self.teacher_db.execute( \"select hash_vector from wordvector where url=?\" , (url, ) ).fetchone()[0]\n        return pickle.loads(hash_v)\n\ndef similarity_value( (lrnr_words, teacher_words, lrnr_hash, teacher_hash) ):\n    try:\n        feat_dim = 1 << len(lrnr_hash)\n        C1, C2 = calculate_distribusion(lrnr_words, teacher_words, feat_dim)\n\n        jaccard = float( sum( np.array(lrnr_hash)==np.array(teacher_hash) ) ) \\\n                                                    /len(lrnr_hash) # \u985e\u4f3c\u5ea6\u306e\u7b97\u51fa\n        return (jaccard-C1)/(1-C2)\n\n    except Exception, e:\n        import sys\n        import traceback\n        sys.stderr.write(traceback.format_exc())\n\ndef calculate_distribusion(lrnr_words, teacher_words, feat_dim):\n    all_words = lrnr_words | teacher_words\n    D = len(all_words); f1 = len(lrnr_words); f2 = len(teacher_words) # \u7c21\u6613\u7684\u306b2\u3064\u306e\u5358\u8a9e\u96c6\u5408\u3092\u7d50\u5408\u3057\u305f\u3082\u306e\u3092\u5168\u5358\u8a9e\u96c6\u5408\u3068\u307f\u306a\u3057\u3066\u3044\u308b\n    r1 = float(f1)/D; r2 = float(f2)/D; sum_r = r1 + r2\n\n    A1 = calc_A(r1, feat_dim); A2 = calc_A(r2,feat_dim)\n\n    C1 = A1*r2/sum_r + A2*r1/sum_r\n    C2 = A1*r1/sum_r + A2*r2/sum_r\n\n    return C1, C2\n\ndef calc_A(r, feat_dim):\n    A_num = r*(1-r)**(feat_dim-1)\n    A_denom = 1-(1-r)**feat_dim\n    return A_num/A_denom\n\nif __name__=='__main__':\n    traindbname = 'learner.db'\n    teacherdbname = 'teacher.db'\n\n    minhash = MinHash(traindbname,teacherdbname)\n    minhash.get_category()\n\n\u8ad6\u6587\u8aad\u3093\u3067\u3044\u305f\u3060\u3051\u308c\u3070\u308f\u304b\u308b\u306e\u3067\u3059\u304c\u3001D\uff08\u5358\u8a9e\u96c6\u5408\u306e\u5927\u304d\u3055\uff09\u3068k\uff08hash\u95a2\u6570\u306e\u7a2e\u985e\uff09\u304c\u5341\u5206\u5927\u304d\u3044\u3053\u3068\u304c\u524d\u63d0\u6761\u4ef6\u3067\u3059\u3002\n\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u8fd1\u4f3c\u304c\u6210\u308a\u7acb\u3061\u307e\u305b\u3093\u306e\u3067\u3001\u304a\u304b\u3057\u306a\u5024\u304c\u51fa\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3054\u6ce8\u610f\u4e0b\u3055\u3044\u3002\n\u4e0a\u306e\u30b3\u30fc\u30c9\u3067\u306fD\u30922\u3064\u306e\u5358\u8a9e\u96c6\u5408\u306e\u7d50\u5408\u3068\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u9a13\u306e\u305f\u3081\u7c21\u6613\u7684\u306b\u304a\u3053\u306a\u3044\u307e\u3057\u305f\u3002\u5b9f\u969b\u4f7f\u7528\u3055\u308c\u308b\u6642\u306f\u5168\u5358\u8a9e\u96c6\u5408\u3092D\u3068\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n\u5b9f\u9a13\u7d50\u679c\n\u9069\u5f53\u306b\u30d5\u30a1\u30a4\u30ca\u30f3\u30b9\u3001\u7d4c\u6e08\u3001\u30b9\u30dd\u30fc\u30c4\uff08\u91ce\u7403\u3001\u30b4\u30eb\u30d5\uff09\u306e\u30ab\u30c6\u30b4\u30ea\u3067\u5b9f\u9a13\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u30b9\u30dd\u30fc\u30c4\u306f\u91ce\u7403\u3068\u30b4\u30eb\u30d5\u3067\u306f\u4f7f\u7528\u3055\u308c\u308b\u5358\u8a9e\u304c\u304b\u306a\u308a\u7570\u306a\u308b\u305f\u3081\u3001\u30b9\u30dd\u30fc\u30c4\u3068\u3044\u3046\u304f\u304f\u308a\u3067\u306f\u4e0a\u624b\u304f\u30ab\u30c6\u30b4\u30ea\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\u5b9f\u52d9\u3067\u4f7f\u7528\u3055\u308c\u308b\u5834\u5408\u306f\u30ab\u30c6\u30b4\u30ea\u81ea\u4f53\u306b\u3082\u5de5\u592b\u304c\u5fc5\u8981\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\u3061\u306a\u307f\u306b\u3001\u30ab\u30c6\u30b4\u30ea\u4f5c\u6210\u306e\u969b\u306b\u79c1\u304c\u53c2\u8003\u306b\u3057\u305f\u306e\u304copen directory project(ODP)\u3067\u3059\u3002\n\u8a08\u7b97\u6642\u9593\u306b\u95a2\u3057\u3066\u8a00\u3048\u3070\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u304c\u6700\u3082\u6642\u9593\u3092\u98df\u3044\u307e\u3059\u3002\u4e00\u5fdc\u3001\u4e26\u5217\u51e6\u7406\u3092\u884c\u3046\u3088\u3046\u306b\u306f\u3057\u3066\u3044\u308b\u3093\u3067\u3059\u304c\u3001\u305d\u308c\u3067\u3082\u76f8\u5f53\u5f85\u3061\u6642\u9593\u304c\u767a\u751f\u3059\u308b\u5370\u8c61\u3067\u3059\u3002\n\uff08\u5e73\u5747\u6642\u9593\u3092\u8a08\u6e2c\u3059\u308b\u4f59\u88d5\u304c\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u5370\u8c61\u3060\u3051\u8a18\u8f09\u3057\u307e\u3059\u3002\u624b\u629c\u304d\u3067\u30b9\u30df\u30de\u30bb\u30f3\u3002\u3002\u3002\uff09\n\u305d\u308c\u3068\u3001\u4e0a\u306e\u30b3\u30fc\u30c9\u4e2d\u3067\u306f32bit hash\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u304c\u300164bit hash\u306e\u4f7f\u7528\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u300264bit\u306e\u65b9\u304c\u30b3\u30ea\u30b8\u30e7\u30f3\u3059\u308b\u78ba\u7387\u304c\u4f4e\u3044\u306e\u3067\u3002\n\n\u307e\u3068\u3081\uff08\u3068\u3044\u3046\u304b\u611f\u60f3\uff09\nMinHash\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u8fd1\u4f3c\u624b\u6cd5\u304c\u307e\u305a\u9762\u767d\u3044\u3067\u3059\u3002\nmixi\u3055\u3093\u306e\u30d6\u30ed\u30b0\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u305f\u8efd\u91cf\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u30c4\u30fc\u30ebbayon\u3067\u3082\u540c\u69d8\u306e\u8fd1\u4f3c\u624b\u6cd5\u304c\u4f7f\u308f\u308c\u3066\u3044\u3066\u3001\u5927\u91cf\u30c7\u30fc\u30bf\u3092\u6271\u3046\u306e\u3067\u3042\u308c\u3070\u3001\u3053\u3046\u3044\u3063\u305f\u8fd1\u4f3c\u624b\u6cd5\u3092\u77e5\u3063\u3066\u3044\u306a\u3044\u3068\uff08\u4f5c\u308c\u306a\u3044\u3068\uff09\u52b9\u7387\u306e\u826f\u3044\u8a08\u7b97\u306f\u96e3\u3057\u3044\u306a\u3001\u3068\u611f\u3058\u305f\u3057\u3060\u3044\u3067\u3059\u3002\u3002\n\u30c4\u30fc\u30eb\u3068\u3057\u3066\u306eb-Bit MinHash\u306f\u3001\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u308b\u5272\u306b\u306f\u7d50\u69cb\u7cbe\u5ea6\u826f\u304f\u5224\u5b9a\u3057\u3066\u304f\u308c\u307e\u3059\u3002\n\u307e\u305f\u8ad6\u6587\u4e2d\u306b\u3082\u8a18\u8f09\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u5b9f\u52d9\u3067\u3082\u5fdc\u7528\u3067\u304d\u308b\u5834\u9762\u304c\u591a\u3044\u6c17\u304c\u3057\u3066\u3044\u307e\u3059\u3002\n\u304a\u624b\u6570\u3067\u3059\u304c\u9593\u9055\u3044\u304c\u3042\u308c\u3070\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u52a9\u304b\u308a\u307e\u3059\u3002\n# \u52d5\u6a5f\nURL\u3092\u30ab\u30c6\u30b4\u30ea\u5206\u3051\u3057\u305f\u3044\u3068\u8003\u3048b-Bit MinHash\u3092\u4f7f\u3063\u3066\u307f\u307e\u3057\u305f\u3002\n\u5177\u4f53\u7684\u306b\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3053\u3068\u3092\u3059\u308b\u305f\u3081\u3067\u3059\u3002\ncookie\u304b\u3089\u62bd\u51fa\u3057\u305f\u30b5\u30a4\u30c8\u95b2\u89a7\u60c5\u5831\u3092\u7d20\u6027\u3068\u3057\u3066\u30e6\u30fc\u30b6\u306eCV\u78ba\u7387\u3092\u6c42\u3081\u308b\u5834\u5408\u3001\u7d20\u6027\u30d9\u30af\u30c8\u30eb\u304c\u304b\u306a\u308a\u30b9\u30d1\u30fc\u30b9\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u3002\u305d\u3053\u3067\u3001URL\u5358\u4f4d\u3067\u30ab\u30c6\u30b4\u30ea\u306b\u5206\u3051\u3001\u3042\u308b\u7a0b\u5ea6\u7d20\u6027\u30d9\u30af\u30c8\u30eb\u306e\u5bc6\u5ea6\u3092\u4e0a\u3052\u305f\u3044\u3068\u8003\u3048\u307e\u3057\u305f\u3002\u305d\u306e\u305f\u3081\u306b\u6a19\u984c\u306b\u3064\u3044\u3066\u5b9f\u9a13\u3092\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u3053\u306e\u30d6\u30ed\u30b0\u306b\u66f8\u3044\u305f\u3053\u3068\n* b-Bit MinHash\u306e\u6982\u8981\n* \u53c2\u8003\u30b3\u30fc\u30c9\n\n# b-Bit MinHash\u306e\u6982\u8981\n## \u53c2\u8003\u6587\u732e\n* [b-Bit Minwise Hashing](http://research.microsoft.com/pubs/120078/wfc0398-lips.pdf)\n* [b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions](http://arxiv.org/abs/1205.2958)\n* [yono\u7814\u4fee\u5ba4/Python\u3067\u81ea\u7136\u8a00\u8a9e\u51e6\u7406](http://www.nal.ie.u-ryukyu.ac.jp/wiki/index.php?yono%2FPython%A4%C7%BC%AB%C1%B3%B8%C0%B8%EC%BD%E8%CD%FD) \u203b\u8a18\u4e8b\u306f\u53e4\u3044\u3067\u3059\u304cpython\u3067\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u3064\u3044\u3066\u5168\u4f53\u7684\u306b\u307e\u3068\u3081\u3066\u3042\u308a\u307e\u3059\n* [BeautifulSoup \u3067 HTML \u6587\u66f8\u304b\u3089\u30bf\u30b0\u3092\u53d6\u308a\u9664\u304f\uff08Python\uff09](http://d.hatena.ne.jp/s-yata/20100619/1276961636) \u203b\u30af\u30ed\u30fc\u30e9\u95a2\u4fc2\u306f\u3053\u3061\u3089\u306b\u5927\u5909\u304a\u4e16\u8a71\u306b\u306a\u308a\u307e\u3057\u305f\n* [b-Bit MinHash\u306b\u3088\u308b\u9ad8\u901f\u304b\u3064\u7701\u30b9\u30da\u30fc\u30b9\u306a\u985e\u4f3c\u5ea6\u5224\u5b9a](http://developer.smartnews.be/blog/2013/08/05/efficient-similarity-estimation-using-b-bit-minhash/) \u203bSmartNews\u3055\u3093\u306e\u958b\u767a\u30d6\u30ed\u30b0\u3067\u3059\u3002\u6a19\u984c\u306e\u6982\u8981\u3092\u77e5\u308b\u306b\u306f\u6700\u9ad8\u306e\u8cc7\u6599\u3060\u3068\u601d\u3044\u307e\u3059\n* [\u96c6\u5408\u77e5\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0](http://www.amazon.co.jp/%E9%9B%86%E5%90%88%E7%9F%A5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-Toby-Segaran/dp/4873113644/?_encoding=UTF8&camp=247&creative=1211&linkCode=ur2&tag=shimashimao06-22)\n\n## b-Bit MinHash\u306e\u624b\u9806\n\u624b\u6cd5\u306e\u6982\u8981\u306b\u3064\u3044\u3066\u306f\u3001\u4e0a\u306e[SmartNews\u3055\u3093\u306e\u958b\u767a\u30d6\u30ed\u30b0](http://developer.smartnews.be/blog/2013/08/05/efficient-similarity-estimation-using-b-bit-minhash/)\u3068\u305d\u306e\u6587\u4e2d\u306b\u51fa\u3066\u304f\u308bPFI\u306e[\u5ca1\u91ce\u539f\u3055\u3093\u306e\u8cc7\u6599](http://research.preferred.jp/2011/02/minhash/)\u3092\u53c2\u7167\u3057\u3066\u3044\u305f\u3060\u304d\u305f\u304f\u601d\u3044\u307e\u3059\u3002\u79c1\u304c\u8aac\u660e\u3092\u66f8\u304f\u3088\u308a\u306f\u308b\u304b\u306b\u308f\u304b\u308a\u3084\u3059\u304f\u66f8\u3044\u3066\u3042\u308a\u307e\u3059\u3057\uff57\n\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001\u3053\u3053\u3067\u306f\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u969b\u306e\u624b\u9806\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u66f8\u304d\u307e\u3059\u3002\n#### \u307e\u305a\u306f\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n* \u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\uff08\u30ab\u30c6\u30b4\u30ea\u3092\u4ed8\u3057\u305f\u30af\u30ed\u30fc\u30ea\u30f3\u30b0\u5bfe\u8c61\u306eURL\u306e\u30ea\u30b9\u30c8\u3092\u6e96\u5099\uff09\n* URL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u5358\u8a9e\u96c6\u5408\u3001\u6700\u5c0fHash\u5024\u5185\u306ebi-Bit\u5206\u306ek\u500b\u306e\u30ea\u30b9\u30c8\uff08\u4ee5\u4e0b\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\uff09\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u308b\uff08k\u306fhash\u95a2\u6570\uff08seed\uff09\u306e\u7a2e\u985e\uff09\n* URL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u5b58\n* \u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u304b\u3089\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u66f4\u65b0\u3059\u308b\n\n#### \u6b21\u306blearner\u306e\u30c7\u30fc\u30bf\u306e\u6e96\u5099\n* learner\u306eURL\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\n* URL\u3001\u30ab\u30c6\u30b4\u30ea\u3001\u5358\u8a9e\u96c6\u5408\u3001\u6700\u5c0fHash\u5024\u5185\u306ebi-Bit\u5206\u306ek\u500b\u306e\u30ea\u30b9\u30c8\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u4f5c\u308b\n* URL\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u5b58\n\n#### \u5206\u985e\u306e\u5b9f\u65bd\n* learner\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u5404URL\u304b\u3089\u62bd\u51fa\u3057\u305f\u5358\u8a9e\u96c6\u5408\u3092\u547c\u3073\u51fa\u3057\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3001DB\u306e\u66f4\u65b0\n* \u4f5c\u6210\u3057\u305f\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3068\u6559\u5e2bDB\u306e\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u3067MinHash\u306e\u5b9f\u65bd\n* \u30ab\u30c6\u30b4\u30ea\u3092\u53d6\u5f97\u3057DB\u66f4\u65b0\n\n# \u53c2\u8003\u30b3\u30fc\u30c9\n\u4ee5\u4e0b\u3001\u4f5c\u6210\u3057\u305f\u30b3\u30fc\u30c9\u3092\u8a18\u8f09\u3057\u307e\u3057\u305f\u3002\n\u3044\u308d\u3044\u308d\u5207\u308a\u8cbc\u308a\u3057\u3066\u4f5c\u3063\u305f\u306e\u3067\u3001\u547d\u540d\u898f\u5247\u304c\u3050\u3061\u3083\u3050\u3061\u3083\u3067\u3059\u3002\u3002\n\u4f8b\u306b\u3088\u3063\u3066\u30a8\u30f3\u30b8\u30cb\u30a2\u3067\u306a\u3044\u4eba\u9593\u304c\u66f8\u3044\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u76f8\u5f53\u304a\u898b\u82e6\u3057\u3044\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u5e83\u3044\u5fc3\u3067\u8aad\u3093\u3067\u3044\u305f\u3060\u3051\u308b\u3068\u975e\u5e38\u306b\u52a9\u304b\u308a\u307e\u3059\u3002\u3002\n\n## \u6559\u5e2bURL\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3057\u3001\u5358\u8a9e\u96c6\u5408\u306a\u3069\u3092DB\u306b\u683c\u7d0d\n\n```py:\n# coding: utf-8\n\nimport urllib2\nfrom BeautifulSoup import BeautifulSoup, NavigableString, \\\n                             Declaration, Comment, BeautifulStoneSoup\nimport unicodedata\nimport chardet\nfrom urlparse import urljoin\nimport sqlite3 as sqlite\nimport pickle\nimport SeparateWords as spw\n\n# \u7121\u8996\u3059\u3079\u304d\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\nignorewords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', '-', '/', '#', \"'\", '_', ',', '(', ')', '[', ']', '\u301c', '!', '|', '\u266a', '...', '>', '<', ':', '!!', '&', '/', '+', '*'])\n# \u30d6\u30ed\u30c3\u30af\u3068\u3057\u3066\u8a8d\u8b58\u3059\u308b\u30bf\u30b0\u306e\u30ea\u30b9\u30c8\nblock_tags = frozenset(['p', 'div', 'table', 'dl', 'ul', 'ol', 'form', 'address',\n                   'blockquote', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'fieldset',\n                   'hr', 'pre' 'article', 'aside', 'dialog', 'figure',\n                   'footer', 'header', 'legend', 'nav', 'section'])\n\nclass crawler:\n    # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u540d\u524d\u3067\u30af\u30ed\u30fc\u30e9\u3092\u521d\u671f\u5316\n    def __init__(self, dbname):\n        self.dbname = dbname\n        self.con=sqlite.connect(dbname)\n        self.con.text_factory = str # utf-8\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306bstr\u3092\u6307\u5b9a\n    \n    def __del__(self):\n        self.con.close()\n\n    def dbcommit(self):\n        self.con.commit()\n\n    # \u500b\u3005\u306e\u30da\u30fc\u30b8\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3059\u308b\n    def addtoindex(self,url,category,wordlist):\n        if self.isindexed(url): return\n        print 'Indexing' + url\n                                \n        # url\u6bce\u306b\u5358\u8a9e\u30ea\u30b9\u30c8\u3092DB\u306b\u683c\u7d0d\n        self.con.execute( \"insert into wordvector values(?,?,?,?)\" , \\\n                            (url, category, pickle.dumps(wordlist), '') )\n        self.dbcommit()\n\n    def getNavigableStrings(self,soup):\n        # soup\u304ccontents\u3092\u542b\u3080\u578b\u304b\u3092\u5224\u5b9a\n        if isinstance(soup, NavigableString): # NavigableString\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306fcontents\u3068string\u3092\u9664\u304f\u5168\u3066\u306e\u30e1\u30f3\u30d0\u5909\u6570\u3092\u6301\u3064\n            if type(soup) not in (Comment, Declaration):\n                yield soup\n        # contents\u3092\u542b\u3093\u3067\u3044\u3066program code\u3067\u306a\u3051\u308c\u3070\u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\n        elif soup.name not in ('script', 'style'):\n            is_block = soup.name in block_tags # \u30d6\u30ed\u30c3\u30af\u30bf\u30b0\u306e\u5b58\u5728\u3092\u78ba\u8a8d\n            if is_block:\n                yield u'\\n'\n            for c in soup.contents:\n                for g in self.getNavigableStrings(c):\n                    yield replace_str(g)\n            if is_block:\n                yield u'\\n'\n    \n    # URL\u304c\u65e2\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3055\u308c\u3066\u3044\u305f\u3089ture\u3092\u8fd4\u3059\n    def isindexed(self,url):\n        u=self.con.execute \\\n            (\"select words from wordvector where url='%s'\" % url).fetchone()\n        if u!=None:\n            return True\n        return False    \n    \n    # \u30da\u30fc\u30b8\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u3001\u4e0e\u3048\u3089\u308c\u305f\u6df1\u3055\u3067\u5e45\u512a\u5148\u306e\u691c\u7d22\u3092\u884c\u3044\n    # \u30da\u30fc\u30b8\u3092\u30a4\u30f3\u30c7\u30af\u30b7\u30f3\u30b0\u3059\u308b\n    def crawl(self,pages,category,depth=2):\n        for _ in xrange(depth):\n            newpages=set()\n            for page in pages:\n                try:\n                    response=urllib2.urlopen(page).read()\n                except:\n                    print \"Could not open %s\" % page\n                    continue\n                en = chardet.detect(response)['encoding']\n                soup=BeautifulSoup(response.decode(en), # \u6587\u5b57\u30b3\u30fc\u30c9\u3092\u30b5\u30a4\u30c8\u306b\u5408\u308f\u305b\u3066\u5909\u63db\n                        convertEntities = BeautifulStoneSoup.HTML_ENTITIES) # HTML\u7279\u6b8a\u6587\u5b57\u306e\u6587\u5b57\u5909\u63db\n                text = u''.join( self.getNavigableStrings(soup) )\n                text = normalizeText(text) # \u6587\u5b57\u5217\u306e\u6b63\u898f\u5316\n                words = spw.separateWords(text) # \u5358\u8a9e\u96c6\u5408\u306e\u53d6\u5f97\n                self.addtoindex(page,category,words) # url, category, \u5358\u8a9e\u96c6\u5408\u3092DB\u306b\u4fdd\u5b58\n                \n                # \u6df1\u30552\u4ee5\u4e0a\u306e\u63a2\u7d22\u3092\u884c\u3046\u5834\u5408\u306e\u305f\u3081\u306ba\u30bf\u30b0\u306eURL\u3092\u53d6\u5f97\n                links=soup('a')\n                for link in links:\n                    if ('href' in dict(link.attrs)):\n                        url=urljoin(page,link['href'])\n                        if url.find(\"'\")!=-1: continue\n                        url=url.split('#')[0] # \u30d5\u30e9\u30b0\u30e1\u30f3\u30c8\u8b58\u5225\u5b50\u3092\u53d6\u308a\u9664\u304f\n                        if url[0:4]=='http' and not self.isindexed(url):\n                            newpages.add(url)\n                pages=newpages\n\n    # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u30c6\u30fc\u30d6\u30eb\u3092\u4f5c\u308b\n    def createindextables(self):\n        name = 'wordvector'\n        sql=\"SELECT name FROM sqlite_master WHERE type='table' AND name='MYTABLE';\" \\\n                .replace('MYTABLE', name)\n        res = self.con.execute(sql).fetchone()\n        if res is None:\n            self.con.execute('create table wordvector(url, category, words, hash_vector)')\n            self.con.execute('create index urlidx on wordvector(url)')\n            self.dbcommit()\n        else:\n            self.con.execute('drop table wordvector')\n            self.con.execute('create table wordvector(url, category, words, hash_vector)')\n            self.con.execute('create index urlidx on wordvector(url)')\n            self.dbcommit()            \n\ndef nonEmptyLines(text):\n    \"\"\" \u4e0d\u8981\u306a\u7a7a\u767d\u3092\u53d6\u308a\u9664\u304d\uff0c\u7a7a\u884c\u4ee5\u5916\u3092\u8fd4\u3059 \"\"\"\n    for line in text.splitlines():\n        line = u' '.join(line.split())\n        if line:\n            yield line\n\ndef normalizeText(text):\n    \"\"\" \u6b63\u898f\u5316\u306e\u5f8c\u3067\u4e0d\u8981\u306a\u7a7a\u767d\u30fb\u6539\u884c\u3092\u53d6\u308a\u9664\u304f \"\"\"\n    Text = unicodedata. normalize ('NFKC', text) # \u65e5\u672c\u8a9e\u534a\u89d2\u3092\u5168\u89d2\u306b\u5909\u63db+\u82f1\u6570\u5b57,\u8a18\u53f7\u306f\u534a\u89d2\n    return u'\\n'.join(nonEmptyLines(text))\n\ndef replace_str(line):\n    return line.replace('&amp;','&')\n\nif __name__=='__main__':    \n    fname = 'teacherURL.txt'\n    crawler=crawler('teacher.db')\n    crawler.createindextables()\n    with open(fname, 'r') as f:\n        for line in f:\n            s = line.strip().split('\\t')\n            print s[0]\n            category = s[0]\n            pagelist = [s[1]]\n            crawler.crawl(pagelist, category, depth=1)\n```\n\n\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u5358\u8a9e\u3092\u62bd\u51fa\u3059\u308b\u90e8\u5206\uff08spw.separateWords(text)\uff09\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30b3\u30fc\u30c9\u306b\u3057\u307e\u3057\u305f\u3002\n\n#### \u30c6\u30ad\u30b9\u30c8\u304b\u3089\u5358\u8a9e\u3092\u62bd\u51fa \n```py:\n# coding: utf-8\nimport MeCab\n\n# \u7121\u8996\u3059\u3079\u304d\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\nignorewords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', '\u306e'])\nigonoresymbols = set(['-', '/', '#', \"'\", '_', ',', '(', ')', '[', ']',\n                     '~', '!', '|', '\u266a', '...', '>', '<', ':', '!',\n                      '&', '/', '+', '*', '\u3010', '\u3011', '\uff08', '\uff09', '\uff01', '\uff1a', '\u30fc',\n                      '\uff3f', '\uff1f','%', '\u300c', '\u300d','\uff5e','.', '{', '}','\"',\n                      '\uff1c', '\uff1e', '\uff0f'])\n\ndef separateWords(text):\n    tagger = MeCab.Tagger('-Ochasen')\n    node = tagger.parseToNode(text.encode('utf-8'))\n    word_set = set([])\n    while node:\n        feat = node.feature.split(\",\")\n        if feat[0] == \"\u540d\u8a5e\" \\\n                and (feat[1] == \"\u4e00\u822c\"\n                or feat[1] == \"\u56fa\u6709\u540d\u8a5e\" or feat[1] == \"\u5f62\u5bb9\u52d5\u8a5e\u8a9e\u5e79\"):\n            #word = node.surface.decode(en)\n            word = node.surface\n            if word not in ignorewords:\n                word_set.add( word )\n        node = node.next\n    return word_set\n```\n\n\u7d9a\u3044\u3066\u3001\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u304b\u3089\u6559\u5e2b\u7528\u306e\u6700\u5c0fhash\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3001DB\u3092\u66f4\u65b0\u3059\u308b\u90e8\u5206\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n## \u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u30fb\u4fdd\u5b58\n```py:\n# coding: utf-8\n\nimport sqlite3 as sqlite\nimport pickle\nimport hashing\n\nclass CreateDB(object):\n    def __init__(self,dbname):\n        self.db = sqlite.connect(dbname)\n        self.db.text_factory = str # utf-8\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306bstr\u3092\u6307\u5b9a\n        self.hashing = hashing.Hashing()\n    \n    def __del__(self):\n        self.db.close()\n        \n    def db_commit(self):\n        self.db.commit()\n        \n    def create_hashtable(self, url, wordlist):\n        hash_v = self.hashing.get_min_vector(wordlist)\n        self.update_vector(url, pickle.dumps(hash_v))\n        self.db_commit()\n\n    def update_vector(self,url,vector):\n        self.db.execute(\"update wordvector set hash_vector=? where url=?\" , \\\n                         (vector, url))\n\nif __name__=='__main__':\n    dbname = 'teachre.db'\n        \n    c_db = CreateDB(dbname)\n    \n    con = sqlite.connect(dbname) # word\u30d9\u30af\u30c8\u30eb\u304c\u683c\u7d0d\u3055\u308c\u305fDB\u3092\u547c\u3076\n    \n    itr = 1\n    while True:\n        sql = \"select * from wordvector where rowid=='%d'\"        \n        res = con.execute(sql % itr).fetchone()\n        print res\n\n        if res is None:break\n        \n        url = res[0]; category = res[1]; wordlist = res[2]\n        c_db.create_hashtable(url, wordlist)\n        \n        itr += 1\n    con.close()\n```\n\n\u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\n#### \u6700\u5c0fhash\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\n```py:\n# coding: utf-8\n\nimport numpy as np\nimport mmh3\nfrom multiprocessing import *\n\nclass Hashing(object):\n    ''' \n      \u5404url\u3067\u53d6\u5f97\u3057\u305f\u5358\u8a9e\u96c6\u5408\u306b\u5bfe\u3057\u3066\u3001hashing\u3092\u304b\u3051\u3066\u3001\u6700\u5c0f\u5024\u3092\u53d6\u5f97\n      url\u6bce\u306bk\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3092\u53d6\u5f97\u3059\u308b\n      \u3053\u306ek\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u985e\u4f3c\u5ea6\u3092\u5224\u5b9a\u3059\u308b\n    '''\n    def __init__(self,bbit=1,k=128):\n        self.bbit = bbit\n        self.hash_cnt = k\n    \n    def get_min_vector(self,feat_names):\n        ''' \u3053\u306e\u95a2\u6570\u306furl\u5206\u56de\u3059\u3053\u3068\u306b\u306a\u308b '''\n        hash_vec = []\n        # \u5358\u8a9e\u96c6\u5408\u306ehash\u306e\u6700\u5c0f\u5024\u306e\u307f\u3092k\u56de\u53d6\u5f97\n        for seed in xrange(self.hash_cnt): # self.hash_cnt\u5206\u306eseed\u3092\u767a\u751f\u3055\u305b\u308b\n            pool = Pool(processes=8)\n            hash_val = pool.map( get_hash_value, ((feat_name, seed) for feat_name in feat_names) ) # k\u500b\u306eseed\u3092\u4f7f\u3063\u3066k\u500b\u306ehash\u95a2\u6570\u3092\u767a\u751f\n            pool.close()\n            pool.join()\n            min_val = int( str( min(hash_val) )[-self.bbit] ) \n            hash_vec.append(min_val) # \u6700\u5c0f\u5024\u306e1\u6841\u76ee\u306e\u307f\u53d6\u5f97\u3059\u308b\n        return hash_vec\n        \ndef get_hash_value((feat_name,seed)):\n    return mmh3.hash(feat_name,seed)\n```\n\n\u3053\u3053\u307e\u3067\u3067\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u6e96\u5099\u304c\u5b8c\u4e86\u3057\u307e\u3057\u305f\u3002\n\u540c\u69d8\u306b\u5b66\u7fd2\u8005\u7528\u306e\u5358\u8a9e\u96c6\u5408\u3092\u53d6\u5f97\u3057\u3001DB\u306b\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u4ee5\u4e0b\u3067\u306f\u53d6\u5f97\u3057\u305f\u5b66\u7fd2\u8005\u306e\u5358\u8a9e\u96c6\u5408\u3068\u4f5c\u6210\u3057\u305f\u6559\u5e2bhash\u30ea\u30b9\u30c8\u3092\u4f7f\u3063\u3066MinHash\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\n\u305d\u3057\u3066\u3001\u5b66\u7fd2\u8005\u306eURL\u306b\u30ab\u30c6\u30b4\u30ea\u3092\u4ed8\u4e0e\u3057\u3001DB\u3092\u66f4\u65b0\u3057\u3066\u7d42\u4e86\u3067\u3059\u3002\n\u4ee5\u4e0b\u3001\u30b3\u30fc\u30c9\u8a18\u8f09\u3057\u307e\u3059\u3002\n\n## b-BIt MinHash\u306e\u5b9f\u65bd\n```py:\n# coding: utf-8\n\nimport numpy as np\nimport sqlite3 as sqlite\nimport hashing\nimport pickle\nfrom multiprocessing import *\n\nclass MinHash(object):\n    def __init__(self,train_dbname,teacher_dbname):\n        self.train_db = sqlite.connect(train_dbname)\n        self.teacher_db = sqlite.connect(teacher_dbname)\n        self.hashing = hashing.Hashing() # default values are bbit=1, k=128\n        \n    def __del__(self):\n        self.train_db.close()\n        self.teacher_db.close()\n        \n    def db_commit(self):\n        self.train_db.commit()\n        \n    def get_category(self):\n        learner_urls = self.get_urls(self.train_db)\n        teacher_urls = self.get_urls(self.teacher_db)\n        for lrnr_url in learner_urls:\n            print \"computing hash vector:\", lrnr_url[0]\n            learner_hash =  self.calculate_hashvecotr( lrnr_url[0] )\n            \n            print \"calculating similarity:\", lrnr_url[0]\n            learner_words =  self.get_wordslist( self.train_db,lrnr_url[0] )\n            \n            pool = Pool(processes=8)\n            sim = pool.map(  similarity_value,\n                        ( (learner_words, self.get_wordslist( self.teacher_db,tchr_url[0] ),\n                             learner_hash, self.get_hash_vector(tchr_url[0]))\n                                                for tchr_url in teacher_urls )  )  # \u985e\u4f3c\u5ea6\u306e\u7b97\u51fa\n            pool.close()\n            pool.join()\n            \n            sim = np.array(sim)\n            print \"sim: \",sim\n            idx = np.where(sim==max(sim))[0][0]\n            sim_url = teacher_urls[idx][0]\n            \n            category = self.get_similer_category( sim_url )\n            print \"similar category of this URL is: \", category\n            self.update_category(category, sim_url)\n\n    def calculate_hashvecotr(self, url):\n        \"\"\" hash\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u308a\u3064\u3064\u3001\u5143\u306eDB\u3092update \"\"\"\n        hash_vector = self.hashing.get_min_vector( \n                            self.get_wordslist(self.train_db, url) )\n        self.train_db.execute( \"update wordvector set hash_vector=? where url=?\" , \n                                    (url, pickle.dumps(hash_vector), ) ) # pickle\u3067\u4fdd\u5b58\u3059\u308b\n        self.db_commit()\n        return hash_vector\n    \n    def update_category(self,category,url):\n        self.train_db.execute( \"update wordvector set category=? where url=?\" ,(category, url, ) )\n        self.db_commit()\n    \n    def get_wordslist(self,db,url):\n        wordlist = db.execute( \"select words from wordvector where url=?\" , (url,) ).fetchone()[0]\n        return pickle.loads(wordlist)\n        \n    def get_urls(self,db):\n        return db.execute(\"select url from wordvector\").fetchall()\n    \n    def get_similer_category(self,url):\n        return self.teacher_db.execute( \"select category from wordvector where url=?\" ,(url, ) ).fetchone()[0]\n\n    def get_hash_vector(self,url):\n        hash_v = self.teacher_db.execute( \"select hash_vector from wordvector where url=?\" , (url, ) ).fetchone()[0]\n        return pickle.loads(hash_v)\n\ndef similarity_value( (lrnr_words, teacher_words, lrnr_hash, teacher_hash) ):\n    try:\n        feat_dim = 1 << len(lrnr_hash)\n        C1, C2 = calculate_distribusion(lrnr_words, teacher_words, feat_dim)\n        \n        jaccard = float( sum( np.array(lrnr_hash)==np.array(teacher_hash) ) ) \\\n                                                    /len(lrnr_hash) # \u985e\u4f3c\u5ea6\u306e\u7b97\u51fa\n        return (jaccard-C1)/(1-C2)\n    \n    except Exception, e:\n        import sys\n        import traceback\n        sys.stderr.write(traceback.format_exc())\n\ndef calculate_distribusion(lrnr_words, teacher_words, feat_dim):\n    all_words = lrnr_words | teacher_words\n    D = len(all_words); f1 = len(lrnr_words); f2 = len(teacher_words) # \u7c21\u6613\u7684\u306b2\u3064\u306e\u5358\u8a9e\u96c6\u5408\u3092\u7d50\u5408\u3057\u305f\u3082\u306e\u3092\u5168\u5358\u8a9e\u96c6\u5408\u3068\u307f\u306a\u3057\u3066\u3044\u308b\n    r1 = float(f1)/D; r2 = float(f2)/D; sum_r = r1 + r2\n        \n    A1 = calc_A(r1, feat_dim); A2 = calc_A(r2,feat_dim)\n\n    C1 = A1*r2/sum_r + A2*r1/sum_r\n    C2 = A1*r1/sum_r + A2*r2/sum_r\n    \n    return C1, C2\n    \ndef calc_A(r, feat_dim):\n    A_num = r*(1-r)**(feat_dim-1)\n    A_denom = 1-(1-r)**feat_dim\n    return A_num/A_denom\n    \nif __name__=='__main__':\n    traindbname = 'learner.db'\n    teacherdbname = 'teacher.db'\n     \n    minhash = MinHash(traindbname,teacherdbname)\n    minhash.get_category()\n```\n\n\u8ad6\u6587\u8aad\u3093\u3067\u3044\u305f\u3060\u3051\u308c\u3070\u308f\u304b\u308b\u306e\u3067\u3059\u304c\u3001D\uff08\u5358\u8a9e\u96c6\u5408\u306e\u5927\u304d\u3055\uff09\u3068k\uff08hash\u95a2\u6570\u306e\u7a2e\u985e\uff09\u304c\u5341\u5206\u5927\u304d\u3044\u3053\u3068\u304c\u524d\u63d0\u6761\u4ef6\u3067\u3059\u3002\n\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u8fd1\u4f3c\u304c\u6210\u308a\u7acb\u3061\u307e\u305b\u3093\u306e\u3067\u3001\u304a\u304b\u3057\u306a\u5024\u304c\u51fa\u308b\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3054\u6ce8\u610f\u4e0b\u3055\u3044\u3002\n\u4e0a\u306e\u30b3\u30fc\u30c9\u3067\u306fD\u30922\u3064\u306e\u5358\u8a9e\u96c6\u5408\u306e\u7d50\u5408\u3068\u3057\u3066\u3044\u307e\u3059\u304c\u3001\u5b9f\u9a13\u306e\u305f\u3081\u7c21\u6613\u7684\u306b\u304a\u3053\u306a\u3044\u307e\u3057\u305f\u3002\u5b9f\u969b\u4f7f\u7528\u3055\u308c\u308b\u6642\u306f\u5168\u5358\u8a9e\u96c6\u5408\u3092D\u3068\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n# \u5b9f\u9a13\u7d50\u679c\n\u9069\u5f53\u306b\u30d5\u30a1\u30a4\u30ca\u30f3\u30b9\u3001\u7d4c\u6e08\u3001\u30b9\u30dd\u30fc\u30c4\uff08\u91ce\u7403\u3001\u30b4\u30eb\u30d5\uff09\u306e\u30ab\u30c6\u30b4\u30ea\u3067\u5b9f\u9a13\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\u30b9\u30dd\u30fc\u30c4\u306f\u91ce\u7403\u3068\u30b4\u30eb\u30d5\u3067\u306f\u4f7f\u7528\u3055\u308c\u308b\u5358\u8a9e\u304c\u304b\u306a\u308a\u7570\u306a\u308b\u305f\u3081\u3001\u30b9\u30dd\u30fc\u30c4\u3068\u3044\u3046\u304f\u304f\u308a\u3067\u306f\u4e0a\u624b\u304f\u30ab\u30c6\u30b4\u30ea\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\u5b9f\u52d9\u3067\u4f7f\u7528\u3055\u308c\u308b\u5834\u5408\u306f\u30ab\u30c6\u30b4\u30ea\u81ea\u4f53\u306b\u3082\u5de5\u592b\u304c\u5fc5\u8981\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\u3061\u306a\u307f\u306b\u3001\u30ab\u30c6\u30b4\u30ea\u4f5c\u6210\u306e\u969b\u306b\u79c1\u304c\u53c2\u8003\u306b\u3057\u305f\u306e\u304c[open directory project(ODP)](http://www.dmoz.org/)\u3067\u3059\u3002\n\n\u8a08\u7b97\u6642\u9593\u306b\u95a2\u3057\u3066\u8a00\u3048\u3070\u3001\u6700\u5c0fhash\u5024\u30ea\u30b9\u30c8\u306e\u4f5c\u6210\u304c\u6700\u3082\u6642\u9593\u3092\u98df\u3044\u307e\u3059\u3002\u4e00\u5fdc\u3001\u4e26\u5217\u51e6\u7406\u3092\u884c\u3046\u3088\u3046\u306b\u306f\u3057\u3066\u3044\u308b\u3093\u3067\u3059\u304c\u3001\u305d\u308c\u3067\u3082\u76f8\u5f53\u5f85\u3061\u6642\u9593\u304c\u767a\u751f\u3059\u308b\u5370\u8c61\u3067\u3059\u3002\n\uff08\u5e73\u5747\u6642\u9593\u3092\u8a08\u6e2c\u3059\u308b\u4f59\u88d5\u304c\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u5370\u8c61\u3060\u3051\u8a18\u8f09\u3057\u307e\u3059\u3002\u624b\u629c\u304d\u3067\u30b9\u30df\u30de\u30bb\u30f3\u3002\u3002\u3002\uff09\n\n\u305d\u308c\u3068\u3001\u4e0a\u306e\u30b3\u30fc\u30c9\u4e2d\u3067\u306f32bit hash\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u304c\u300164bit hash\u306e\u4f7f\u7528\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u300264bit\u306e\u65b9\u304c\u30b3\u30ea\u30b8\u30e7\u30f3\u3059\u308b\u78ba\u7387\u304c\u4f4e\u3044\u306e\u3067\u3002\n\n# \u307e\u3068\u3081\uff08\u3068\u3044\u3046\u304b\u611f\u60f3\uff09\nMinHash\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u8fd1\u4f3c\u624b\u6cd5\u304c\u307e\u305a\u9762\u767d\u3044\u3067\u3059\u3002\nmixi\u3055\u3093\u306e\u30d6\u30ed\u30b0\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u305f[\u8efd\u91cf\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u30c4\u30fc\u30ebbayon](http://alpha.mixi.co.jp/2009/10714/)\u3067\u3082\u540c\u69d8\u306e\u8fd1\u4f3c\u624b\u6cd5\u304c\u4f7f\u308f\u308c\u3066\u3044\u3066\u3001\u5927\u91cf\u30c7\u30fc\u30bf\u3092\u6271\u3046\u306e\u3067\u3042\u308c\u3070\u3001\u3053\u3046\u3044\u3063\u305f\u8fd1\u4f3c\u624b\u6cd5\u3092\u77e5\u3063\u3066\u3044\u306a\u3044\u3068\uff08\u4f5c\u308c\u306a\u3044\u3068\uff09\u52b9\u7387\u306e\u826f\u3044\u8a08\u7b97\u306f\u96e3\u3057\u3044\u306a\u3001\u3068\u611f\u3058\u305f\u3057\u3060\u3044\u3067\u3059\u3002\u3002\n\n\u30c4\u30fc\u30eb\u3068\u3057\u3066\u306eb-Bit MinHash\u306f\u3001\u7c21\u5358\u306b\u4f5c\u6210\u3067\u304d\u308b\u5272\u306b\u306f\u7d50\u69cb\u7cbe\u5ea6\u826f\u304f\u5224\u5b9a\u3057\u3066\u304f\u308c\u307e\u3059\u3002\n\u307e\u305f\u8ad6\u6587\u4e2d\u306b\u3082\u8a18\u8f09\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u5b9f\u52d9\u3067\u3082\u5fdc\u7528\u3067\u304d\u308b\u5834\u9762\u304c\u591a\u3044\u6c17\u304c\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u304a\u624b\u6570\u3067\u3059\u304c\u9593\u9055\u3044\u304c\u3042\u308c\u3070\u3054\u6307\u6458\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u52a9\u304b\u308a\u307e\u3059\u3002", "tags": ["Python", "NLP", "classification"]}