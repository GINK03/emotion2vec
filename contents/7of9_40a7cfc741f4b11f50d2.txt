{"context": "\n\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n\n\nv0.1 http://qiita.com/7of9/items/b364d897b95476a30754\nhttp://qiita.com/7of9/items/ce72fea33fc0a2dea479\n\u306b\u304a\u3044\u3066sine curve\u306e\u5b66\u7fd2\u304c\u9032\u307e\u306a\u3044\u3002\n\u306a\u305c\u306a\u306e\u304b\u3002\ntf.train.shuffle_batch() + eval()\u306e\u52b9\u679c\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u306b\u3057\u305f\u3002\n\n\u5931\u6557\u7de8\n\n\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\nshuffle\u3057\u305f\u6642\u306e\u52b9\u679c\u304c\u5206\u304b\u308b\u3088\u3046\u306b\u3001\u5165\u529b\u3059\u308bcsv\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b\u3053\u3068\u306b\u3057\u305f\u3002\u3000\nhttp://qiita.com/7of9/items/bab75cf1e83757dce1d0\n\n1\u5217\u76ee: 1\u304b\u3089100\n2\u5217\u76ee: 501\u304b\u3089600\n\n\u3053\u308c\u3067\u3001\uff11\u5217\u76ee\u3068\uff12\u5217\u76ee\u306e\u5bfe\u5fdc\u304c\u5206\u304b\u308b\u3002\npython prep_seq_data.py > input2.csv\n\u3092\u5b9f\u884c\u3059\u308b\u3002\nJupyter\u3067\u78ba\u8a8d\u3002\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('input2.csv', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,1,1)\n\n#ax1.plot(input1, output1, color='black', linestyle='solid', label='original')\nax1.scatter(input1, output1)\n\nax1.set_title('loss')\nax1.set_xlabel('step')\nax1.set_ylabel('loss')\nax1.grid(True)\nax1.legend()\n\nfig.show()\n\n\n\ntf.train.shuffle_batch() + eval()\u306e\u52b9\u679c\n\ntest_batch.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfilename_queue = tf.train.string_input_producer([\"input2.csv\"])\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(3): #[30000]\n        print(inputs_batch.eval(), output_batch.eval())\n\n#      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inputs_batch.eval(), output_ph:output_batch.eval()})\n#      if (i+1) % 100 == 0:\n#        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n\n\n\n\u7d50\u679c\n$ python test_batch.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nWARNING:tensorflow:sum_of_squares (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-10-01.\nInstructions for updating:\nUse mean_squared_error.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7715\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.24GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n(array([[ 27.],\n       [ 13.],\n       [ 21.],\n       [ 40.]], dtype=float32), array([[ 541.],\n       [ 531.],\n       [ 520.],\n       [ 506.]], dtype=float32))\n(array([[ 14.],\n       [ 44.],\n       [ 34.],\n       [  4.]], dtype=float32), array([[ 537.],\n       [ 519.],\n       [ 515.],\n       [ 535.]], dtype=float32))\n(array([[ 38.],\n       [ 11.],\n       [ 10.],\n       [ 23.]], dtype=float32), array([[ 542.],\n       [ 503.],\n       [ 546.],\n       [ 530.]], dtype=float32))\n\n\n\u4e00\u5217\u76ee {27, 13, 21, 40}\u306b\u5bfe\u3057\u3066\u3001\u4e8c\u5217\u76ee{541, 531, 520, 506}\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\u3064\u307e\u308a\u3001inputs_batch.eval()\u3092\u3057\u305f\u6642\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068output_batch().eval()\u3057\u305f\u6642\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u4e00\u81f4\u3057\u3066\u3044\u306a\u3044\u3002\n\u3053\u308c\u3067\u306fsine curve\u304c\u5b66\u7fd2\u3067\u304d\u308b\u308f\u3051\u304c\u306a\u3044\u3002\n.eval()\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u7fd2\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002\n\neval\u306e\u4ee3\u308f\u308a\n\u53c2\u8003 http://stackoverflow.com/questions/34010987/does-tensorflow-rerun-for-each-eval-call\n.eval()\u3092\u4f7f\u308f\u305a\u3001sess.run()\u3092\u4f7f\u3046\u3002\n#       print(inputs_batch.eval(), output_batch.eval())\n      inp, out = sess.run([inputs_batch, output_batch])\n      print (inp, out)\n\n\ntest_batch2.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfilename_queue = tf.train.string_input_producer([\"input2.csv\"])\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(3): #[30000]\n#       print(inputs_batch.eval(), output_batch.eval())\n\n      inp, out = sess.run([inputs_batch, output_batch])\n      print (inp, out)\n\n#      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inputs_batch.eval(), output_ph:output_batch.eval()})\n#      if (i+1) % 100 == 0:\n#        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n\n\n\n\u7d50\u679c\u3000\n$ python test_batch2.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nWARNING:tensorflow:sum_of_squares (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-10-01.\nInstructions for updating:\nUse mean_squared_error.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7715\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.18GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n(array([[ 36.],\n       [ 29.],\n       [ 11.],\n       [ 37.]], dtype=float32), array([[ 536.],\n       [ 529.],\n       [ 511.],\n       [ 537.]], dtype=float32))\n(array([[ 43.],\n       [ 31.],\n       [ 24.],\n       [ 34.]], dtype=float32), array([[ 543.],\n       [ 531.],\n       [ 524.],\n       [ 534.]], dtype=float32))\n(array([[  1.],\n       [ 21.],\n       [ 41.],\n       [ 45.]], dtype=float32), array([[ 501.],\n       [ 521.],\n       [ 541.],\n       [ 545.]], dtype=float32))\n\n\n\u4e00\u5217\u76ee {36, 29, 11, 37}\u306b\u5bfe\u3057\u3066\u3001\u4e8c\u5217\u76ee{536, 529, 511, 537}\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\u6210\u529f\u3002\n```txt:\u52d5\u4f5c\u74b0\u5883\nGeForce GTX 1070 (8GB)\nASRock Z170M Pro4S [Intel Z170chipset]\nUbuntu 14.04 LTS desktop amd64\nTensorFlow v0.11\ncuDNN v5.1 for Linux\nCUDA v8.0\nPython 2.7.6\nIPython 5.1.0 -- An enhanced Interactive Python.\n```\n\nv0.1 http://qiita.com/7of9/items/b364d897b95476a30754\n\nhttp://qiita.com/7of9/items/ce72fea33fc0a2dea479\n\u306b\u304a\u3044\u3066sine curve\u306e\u5b66\u7fd2\u304c\u9032\u307e\u306a\u3044\u3002\n\n\u306a\u305c\u306a\u306e\u304b\u3002\n\ntf.train.shuffle_batch() + eval()\u306e\u52b9\u679c\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u306b\u3057\u305f\u3002\n\n## \u5931\u6557\u7de8\n\n### \u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\n\nshuffle\u3057\u305f\u6642\u306e\u52b9\u679c\u304c\u5206\u304b\u308b\u3088\u3046\u306b\u3001\u5165\u529b\u3059\u308bcsv\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b\u3053\u3068\u306b\u3057\u305f\u3002\u3000\n\nhttp://qiita.com/7of9/items/bab75cf1e83757dce1d0\n\n- 1\u5217\u76ee: 1\u304b\u3089100\n- 2\u5217\u76ee: 501\u304b\u3089600\n\n\u3053\u308c\u3067\u3001\uff11\u5217\u76ee\u3068\uff12\u5217\u76ee\u306e\u5bfe\u5fdc\u304c\u5206\u304b\u308b\u3002\n\npython prep_seq_data.py > input2.csv\n\u3092\u5b9f\u884c\u3059\u308b\u3002\n\nJupyter\u3067\u78ba\u8a8d\u3002\n\n```py\n%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata1 = np.loadtxt('input2.csv', delimiter=',')\n\ninput1 = data1[:,0]\noutput1 = data1[:,1]\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,1,1)\n\n#ax1.plot(input1, output1, color='black', linestyle='solid', label='original')\nax1.scatter(input1, output1)\n\nax1.set_title('loss')\nax1.set_xlabel('step')\nax1.set_ylabel('loss')\nax1.grid(True)\nax1.legend()\n\nfig.show()\n```\n\n![qiita.png](https://qiita-image-store.s3.amazonaws.com/0/32870/4128fb8b-faac-98ae-63cf-2507cbf2fee4.png)\n\n\n### tf.train.shuffle_batch() + eval()\u306e\u52b9\u679c\n\n```test_batch.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfilename_queue = tf.train.string_input_producer([\"input2.csv\"])\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(3): #[30000]\n    \tprint(inputs_batch.eval(), output_batch.eval())\n\n#      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inputs_batch.eval(), output_ph:output_batch.eval()})\n#      if (i+1) % 100 == 0:\n#        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n```\n\n```txt:\u7d50\u679c\n$ python test_batch.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nWARNING:tensorflow:sum_of_squares (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-10-01.\nInstructions for updating:\nUse mean_squared_error.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7715\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.24GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n(array([[ 27.],\n       [ 13.],\n       [ 21.],\n       [ 40.]], dtype=float32), array([[ 541.],\n       [ 531.],\n       [ 520.],\n       [ 506.]], dtype=float32))\n(array([[ 14.],\n       [ 44.],\n       [ 34.],\n       [  4.]], dtype=float32), array([[ 537.],\n       [ 519.],\n       [ 515.],\n       [ 535.]], dtype=float32))\n(array([[ 38.],\n       [ 11.],\n       [ 10.],\n       [ 23.]], dtype=float32), array([[ 542.],\n       [ 503.],\n       [ 546.],\n       [ 530.]], dtype=float32))\n```\n\n\u4e00\u5217\u76ee {27, 13, 21, 40}\u306b\u5bfe\u3057\u3066\u3001\u4e8c\u5217\u76ee{541, 531, 520, 506}\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\n**<font color=red>\u3064\u307e\u308a\u3001inputs_batch.eval()\u3092\u3057\u305f\u6642\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068output_batch().eval()\u3057\u305f\u6642\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u4e00\u81f4\u3057\u3066\u3044\u306a\u3044\u3002</font>**\n\n\u3053\u308c\u3067\u306fsine curve\u304c\u5b66\u7fd2\u3067\u304d\u308b\u308f\u3051\u304c\u306a\u3044\u3002\n\n.eval()\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u7fd2\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002\n\n\n## eval\u306e\u4ee3\u308f\u308a\n\n\u53c2\u8003 http://stackoverflow.com/questions/34010987/does-tensorflow-rerun-for-each-eval-call\n\n\n.eval()\u3092\u4f7f\u308f\u305a\u3001sess.run()\u3092\u4f7f\u3046\u3002\n\n```\n#       print(inputs_batch.eval(), output_batch.eval())\n      inp, out = sess.run([inputs_batch, output_batch])\n      print (inp, out)\n```\n\n\n```test_batch2.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfilename_queue = tf.train.string_input_producer([\"input2.csv\"])\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\ninput1, output = tf.decode_csv(value, record_defaults=[[0.], [0.]])\ninputs = tf.pack([input1])\noutput = tf.pack([output])\n\nbatch_size=4 # [4]\ninputs_batch, output_batch = tf.train.shuffle_batch([inputs, output], batch_size, capacity=40, min_after_dequeue=batch_size)\n\ninput_ph = tf.placeholder(\"float\", [None,1])\noutput_ph = tf.placeholder(\"float\",[None,1])\n\n## NN \u306e\u30b0\u30e9\u30d5\u751f\u6210\nhiddens = slim.stack(input_ph, slim.fully_connected, [1,7,7,7], \n  activation_fn=tf.nn.sigmoid, scope=\"hidden\")\nprediction = slim.fully_connected(hiddens, 1, activation_fn=tf.nn.sigmoid, scope=\"output\")\nloss = tf.contrib.losses.mean_squared_error(prediction, output_ph)\n\n#train_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.01))\ntrain_op = slim.learning.create_train_op(loss, tf.train.AdamOptimizer(0.001))\n\n#def feed_dict(inputs, output):\n#    return {input_ph: inputs.eval(), output_ph: output.eval()}\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n  try:\n    sess.run(init_op)\n    for i in range(3): #[30000]\n#    \tprint(inputs_batch.eval(), output_batch.eval())\n  \n      inp, out = sess.run([inputs_batch, output_batch])\n      print (inp, out)\n\n#      _, t_loss = sess.run([train_op, loss], feed_dict={input_ph:inputs_batch.eval(), output_ph:output_batch.eval()})\n#      if (i+1) % 100 == 0:\n#        print(\"%d,%f\" % (i+1, t_loss))\n#        print(\"%d,%f,#step, loss\" % (i+1, t_loss))\n  finally:\n    coord.request_stop()\n\n  coord.join(threads)\n```\n\n```txt:\u7d50\u679c\u3000\n$ python test_batch2.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nWARNING:tensorflow:sum_of_squares (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-10-01.\nInstructions for updating:\nUse mean_squared_error.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1070\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7715\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.18GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n(array([[ 36.],\n       [ 29.],\n       [ 11.],\n       [ 37.]], dtype=float32), array([[ 536.],\n       [ 529.],\n       [ 511.],\n       [ 537.]], dtype=float32))\n(array([[ 43.],\n       [ 31.],\n       [ 24.],\n       [ 34.]], dtype=float32), array([[ 543.],\n       [ 531.],\n       [ 524.],\n       [ 534.]], dtype=float32))\n(array([[  1.],\n       [ 21.],\n       [ 41.],\n       [ 45.]], dtype=float32), array([[ 501.],\n       [ 521.],\n       [ 541.],\n       [ 545.]], dtype=float32))\n```\n\n\u4e00\u5217\u76ee {36, 29, 11, 37}\u306b\u5bfe\u3057\u3066\u3001\u4e8c\u5217\u76ee{536, 529, 511, 537}\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\n\u6210\u529f\u3002\n\n\n\n", "tags": ["borgWarp", "TensorFlow"]}