{"context": "Google\u306f\u30b5\u30fc\u30c1\u30a8\u30f3\u30b8\u30f3\u306e\u60c5\u5831\u53ce\u96c6\u306bGooglebot\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u3042\u308b\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3092\u8d77\u70b9\u306b\u3001\u305d\u306e\u30b5\u30a4\u30c8\u306e\u30ea\u30f3\u30af\u3092\u81ea\u52d5\u3067\u8fbf\u308a\u3001\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002\npython\u306e Scrapy \u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u3070\u3001\u540c\u3058\u3088\u3046\u306a\u3053\u3068\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002\nScrapy \u3092\u7528\u3044\u3066\u30b5\u30a4\u30c8\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u3066\u307f\u307e\u3059\u3002\n\n\u6e96\u5099\nScrapy\u3092pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n`$ pip install scrapy\n\n\u4f7f\u3044\u65b9\nScrapy\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5358\u4f4d\u3067\u7ba1\u7406\u3057\u307e\u3059\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3057\u305f\u5f8c\u3001\u305d\u3053\u3067\u81ea\u52d5\u751f\u6210\u3055\u308c\u305f\u4e0b\u8a18\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n1. items.py : \u62bd\u51fa\u30c7\u30fc\u30bf\u3092\u5b9a\u7fa9\u3059\u308b\n2. spiders/\u4ee5\u4e0b\u306e\u30b9\u30d1\u30a4\u30c0\u30fc(\u30af\u30ed\u30fc\u30e9\u30fc)\u30d5\u30a1\u30a4\u30eb\uff1a\u5de1\u56de\u3001\u30c7\u30fc\u30bf\u62bd\u51fa\u6761\u4ef6\n3. pipelines.py\u3000\uff1a\u3000\u62bd\u51fa\u30c7\u30fc\u30bf\u306e\u51fa\u529b\u5148\u3002\u4eca\u56de\u306fmongoDB\n4. settings.py\u3000\uff1a\u3000\u30c7\u30fc\u30bf\u5de1\u56de\u306e\u6761\u4ef6 (\u983b\u5ea6\u3084\u3001\u968e\u5c64\u306a\u3069)\n\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\n\u307e\u305a\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n$ scrapy startproject tutorial\n\u3059\u308b\u3068\u3001\u3053\u306e\u3088\u3046\u306a\u30d5\u30a9\u30eb\u30c0\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\n\ntutorial/\ntutorial/\n    scrapy.cfg            # deploy configuration file\n\n    tutorial/             # project's Python module, you'll import your code from here\n        __init__.py\n\n        items.py          # project items file\n\n        pipelines.py      # project pipelines file\n\n        settings.py       # project settings file\n\n        spiders/          # a directory where you'll later put your spiders\n            __init__.py\n            ...\n\n\n\n\u62bd\u51fa\u30c7\u30fc\u30bf\u306e\u5b9a\u7fa9\n\u4f55\u3092\u5f97\u308b\u304b\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u8a00\u3046\u3001\u30d5\u30a3\u30fc\u30eb\u30c9\u306e\u5b9a\u7fa9\u3067\u3059\u3002\n\nitems.py\nimport scrapy\n\nclass WebItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    date = scrapy.Field()\n\n\n\n\u30b9\u30d1\u30a4\u30c0\u30fc\u306e\u4f5c\u6210\n\u3053\u3053\u304c\u30a6\u30a7\u30d6\u3092\u5de1\u56de\u3057\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\u3059\u308b\u3001\u82b1\u5f62\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\u5de1\u56de\u958b\u59cb\u306e\u30a2\u30c9\u30ec\u30b9\u3068\u3001\u5de1\u56de\u6761\u4ef6\u3001\u305d\u3057\u3066\u30c7\u30fc\u30bf\u62bd\u51fa\u6761\u4ef6\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n\u30b9\u30d1\u30a4\u30c0\u30fc\u306e\u751f\u6210\n\u30b9\u30d1\u30a4\u30c0\u30fc\u3092\u4f5c\u308a\u307e\u3059\u3002\u69cb\u6587\u306f $ scrapy genspider [options] <name> <domain(\u5de1\u56de\u3059\u308b\u30b5\u30a4\u30c8\u30c9\u30e1\u30a4\u30f3)>\u3067\u3059\u3002\n\ncommandline\n$ scrapy genspider webspider exsample.com\n  Created spider 'webspider' using template 'basic' in module:\n  tutorial.spiders.webspider\n\n\n\u751f\u6210\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u306f\n\ntutorial/spiders/webspider.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass WebspiderSpider(scrapy.Spider):\n    name = \"webspider\"   # \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5185\u3067\u306e\u540d\u524d\u3002\u52d5\u304b\u3059\u3068\u304d\u306e\u30b9\u30d1\u30a4\u30c0\u30fc\u6307\u5b9a\u3067\u4f7f\u308f\u308c\u308b\n    allowed_domains = [\"exsample.com\"] # \u5de1\u56deOK\u306e\u30c9\u30e1\u30a4\u30f3\u6307\u5b9a\n    start_urls = (\n        'http://www.exsample.com/', # \u3053\u3053\u3092\u8d77\u70b9\u306b\u3059\u308b\u3002\u30ea\u30b9\u30c8\u3067\u8907\u6570\u6307\u5b9a\u3067\u304d\u308b\u3002\n    )\n\n    def parse(self, response):  # \u3053\u3053\u304c\u62bd\u51fa\u6761\u4ef6\n        pass\n\n\n\u3053\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\n\u3053\u308c\u3092\u3001\u81ea\u5206\u597d\u307f\u306b\u5909\u3048\u307e\u3059\u3002\n\ntutorial/spiders/webspider.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom tutorial.items import WebItem\nimport re\nimport datetime\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nclass WebspiderSpider(CrawlSpider):  #\u30af\u30e9\u30b9\u540d\u306b\u305f\u3044\u3057\u305f\u610f\u5473\u306f\u306a\u3044\n    name = 'WebspiderSpider'  # \u3053\u308c\u306f\u91cd\u8981\u3002\u3053\u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u3066\u30b9\u30d1\u30a4\u30c0\u30fc(\u30af\u30ed\u30fc\u30e9\u30fc)\u3092\u52d5\u304b\u3059\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com']\n\n    xpath = {\n        'title' : \"//title/text()\",\n    }\n\n    list_allow = [r'(\u6b63\u898f\u8868\u73fe)'] #\u3053\u306e\u6761\u4ef6\u306b\u5408\u3046\u30ea\u30f3\u30af\u306f\u5de1\u56de\n    list_deny = [\n                r'/exsample/hogehoge/hoge/', # \u3053\u3061\u3089\u306f\u5de1\u56de\u3057\u306a\u3044\u30ea\u30f3\u30af\u306e\u6307\u5b9a\u4f8b\u3002\u30ea\u30b9\u30c8\u8868\u8a18\u3082\u53ef\u80fd\n            ]\n    list_allow_parse = [r'(\u6b63\u898f\u8868\u73fe)']  #\u30c7\u30fc\u30bf\u62bd\u51fa\u3059\u308b\u30ea\u30f3\u30af\u6307\u5b9a\n    list_deny_parse = [                #\u30c7\u30fc\u30bf\u62bd\u51fa\u3057\u306a\u3044\u30ea\u30f3\u30af\u6307\u5b9a\n                r'(\u6b63\u898f\u8868\u73fe)',\n                r'(\u6b63\u898f\u8868\u73fe)',\n                ]\n\n    rules = (\n        # \u5de1\u56de\u30eb\u30fc\u30eb\u3002\n        Rule(LinkExtractor(\n            allow=list_allow,\n            deny=list_deny,\n            ),\n            follow=True # \u305d\u306e\u30ea\u30f3\u30af\u3078\u5165\u3063\u3066\u3044\u304f\n        ),\n        # \u30c7\u30fc\u30bf\u62bd\u51fa\u30eb\u30fc\u30eb\n        Rule(LinkExtractor(\n            allow=list_allow_parse,\n            deny=list_deny_parse,\n            unique=True # \u304a\u306a\u3058\u30ea\u30f3\u30af\u5148\u3067\u306f\u30c7\u30fc\u30bf\u62bd\u51fa\u3057\u306a\u3044\n            ),\n            callback='parse_items' # \u6761\u4ef6\u306b\u5408\u3048\u3070\u3001\u3053\u3053\u3067\u6307\u5b9a\u3057\u305f\u30c7\u30fc\u30bf\u62bd\u51fa\u5b9f\u884c\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u3002\n        ),\n    )\n\n   #\u30c7\u30fc\u30bf\u62bd\u51fa\u95a2\u6570\u5b9a\u7fa9\n   def parse_items(self, response): # response \u306b\u3001\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u306e\u60c5\u5831\u304c\u5165\u3063\u3066\u3044\u308b\n        item = WebItem()  # items.py\u3067\u6307\u5b9a\u3057\u305f\u30af\u30e9\u30b9\n        item['title'] = response.xpath(self.xpath['title']).extract()[0]\n        item['link'] = response.url\n        item['date'] = datetime.datetime.utcnow() + datetime.timedelta(hours=9) # \u73fe\u5728\u6642\u9593\u3002\u65e5\u672c\u6642\u9593\u306b\u3057\u3066\u7a81\u3063\u8fbc\u3080\u3002\n\n        yield item\n\n\n\u66f8\u3044\u3066\u3044\u308b\u5185\u5bb9\u306f\u30b3\u30e1\u30f3\u30c8\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\npipeline.py\u306e\u7de8\u96c6\n\u4e0a\u8a18\u3067\u4f5c\u6210\u3057\u305f\u30b9\u30d1\u30a4\u30c0\u30fc\u304b\u3089 yield item \u3092\u3001mongoDB \u306b\u7a81\u3063\u8fbc\u307f\u307e\u3059\u3002\n\npipelines.py\nfrom pymongo import MongoClient  # mongoDB \u3068\u306e\u63a5\u7d9a\nimport datetime\n\nclass TutorialPipeline(object):\n\n    def __init__(self, mongo_uri, mongo_db, mongolab_user, mongolab_pass):\n        # \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306b\u6e21\u3055\u308c\u305f\u5f15\u6570\u3067\u3001\u5909\u6570\u521d\u671f\u5316\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n        self.mongolab_user = mongolab_user\n        self.mongolab_pass = mongolab_pass\n\n    @classmethod  # \u5f15\u6570\u306b\u30af\u30e9\u30b9\u304c\u3042\u308b\u306e\u3067\u3001\u30af\u30e9\u30b9\u5909\u6570\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'), # settings.py \u3066\u5b9a\u7fa9\u3057\u305f\u5909\u6570\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items'),\n            mongolab_user=crawler.settings.get('MONGOLAB_USER'),\n            mongolab_pass=crawler.settings.get('MONGOLAB_PASS')\n        ) # def __init__ \u306e\u5f15\u6570\u306b\u306a\u308b\n\n    def open_spider(self, spider): # \u30b9\u30d1\u30a4\u30c0\u30fc\u958b\u59cb\u6642\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\n        self.client = MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n        self.db.authenticate(self.mongolab_user, self.mongolab_pass)\n\n    def close_spider(self, spider): # \u30b9\u30d1\u30a4\u30c0\u30fc\u7d42\u4e86\u6642\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u9589\u3058\u308b\n        self.client.close()\n\n    def process_item(self, item, spider):\n        self.db[self.collection_name].update(\n            {u'link': item['link']},\n            {\"$set\": dict(item)},\n            upsert = True\n        ) # link\u3092\u691c\u7d22\u3057\u3066\u3001\u306a\u3051\u308c\u3070\u65b0\u898f\u4f5c\u6210\u3001\u3042\u308c\u3070\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b\n\n        return item\n\n\n\u3044\u308d\u3044\u308d\u66f8\u3044\u3066\u3044\u307e\u3059\u304c\u3001\u8981\u306f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u958b\u3051\u3066\u3001\u30c7\u30fc\u30bf\u7a81\u3063\u8fbc\u3093\u3067\u3001\u7d42\u308f\u3063\u305f\u3089\u9589\u3058\u3066\u3044\u308b\u3060\u3051\u3067\u3059\u3002\n\nsettings.py\n\u307e\u305a\u306f\u3001pipelines.py \u3067\u547c\u3073\u51fa\u3057\u3066\u3044\u308b\u5404\u7a2e\u5909\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\nsettings.py\nMONGO_URI = 'hogehoge.mongolab.com:(port\u756a\u53f7)'\nMONGO_DATABASE = 'database_name'\nMONGOLAB_USER = 'user_name'\nMONGOLAB_PASS = 'password'\n\n\n\u3053\u308c\u306fmongolab\u306e\u4f8b\u3067\u3059\u3002\nsettings.py\u3067\u306f\u4ed6\u306b\u3001\u6319\u52d5\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\nsettings.py\nREDIRECT_MAX_TIMES = 6\nRETRY_ENABLED = False\nDOWNLOAD_DELAY=10\nCOOKIES_ENABLED=False\n\n\n\u3053\u3053\u3067\u306f\u3001\u30ea\u30c0\u30a4\u30ec\u30af\u30c8\u306e\u6700\u5927\u56de\u6570\u30926\u56de\u306b\u3001\u30ea\u30c8\u30e9\u30a4\u3092\u5b9f\u884c\u3057\u306a\u3044\u3088\u3046\u306b\u3001\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306f10\u79d2\u3054\u3068\u306b\u3001\u30af\u30c3\u30ad\u30fc\u306f\u4fdd\u5b58\u3057\u306a\u3044\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u6761\u4ef6\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\nDOWNLOAD_DELAY\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3001\u30d0\u30b7\u30d0\u30b7\u5168\u529b\u3067\u30a2\u30af\u30bb\u30b9\u3057\u307e\u304f\u308b\u306e\u3067\u3001\u5de1\u56de\u5148\u306e\u30b5\u30a4\u30c8\u306b\u5927\u304d\u306a\u8ca0\u8377\u304c\u304b\u304b\u308a\u307e\u3059\u3002\u3084\u3081\u307e\u3057\u3087\u3046\u3002\n\n\u5b9f\u884c\n\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\ncommandline\n$ scrapy crawl WebspiderSpider\n\n\n\u3069\u3093\u3069\u3093\u30ea\u30f3\u30af\u3092\u8fbf\u3063\u3066\u3044\u304d\u3001\u6761\u4ef6\u306b\u3042\u3046\u30ea\u30f3\u30af\u304b\u3089\u30c7\u30fc\u30bf\u304c\u62bd\u51fa\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002\nGoogle\u306f\u30b5\u30fc\u30c1\u30a8\u30f3\u30b8\u30f3\u306e\u60c5\u5831\u53ce\u96c6\u306b[Googlebot](https://ja.wikipedia.org/wiki/%E3%82%B0%E3%83%BC%E3%82%B0%E3%83%AB%E3%83%9C%E3%83%83%E3%83%88)\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002\u3042\u308b\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3092\u8d77\u70b9\u306b\u3001\u305d\u306e\u30b5\u30a4\u30c8\u306e\u30ea\u30f3\u30af\u3092\u81ea\u52d5\u3067\u8fbf\u308a\u3001\u60c5\u5831\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002\n\npython\u306e [Scrapy](http://scrapy.org/) \u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u3070\u3001\u540c\u3058\u3088\u3046\u306a\u3053\u3068\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002\nScrapy \u3092\u7528\u3044\u3066\u30b5\u30a4\u30c8\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u3066\u307f\u307e\u3059\u3002\n\n#\u6e96\u5099\nScrapy\u3092pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002\n`$ pip install scrapy\n\n#\u4f7f\u3044\u65b9\nScrapy\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5358\u4f4d\u3067\u7ba1\u7406\u3057\u307e\u3059\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\u3057\u305f\u5f8c\u3001\u305d\u3053\u3067\u81ea\u52d5\u751f\u6210\u3055\u308c\u305f\u4e0b\u8a18\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n1. items.py : \u62bd\u51fa\u30c7\u30fc\u30bf\u3092\u5b9a\u7fa9\u3059\u308b\n2. spiders/\u4ee5\u4e0b\u306e\u30b9\u30d1\u30a4\u30c0\u30fc(\u30af\u30ed\u30fc\u30e9\u30fc)\u30d5\u30a1\u30a4\u30eb\uff1a\u5de1\u56de\u3001\u30c7\u30fc\u30bf\u62bd\u51fa\u6761\u4ef6\n3. pipelines.py\u3000\uff1a\u3000\u62bd\u51fa\u30c7\u30fc\u30bf\u306e\u51fa\u529b\u5148\u3002\u4eca\u56de\u306fmongoDB\n4. settings.py\u3000\uff1a\u3000\u30c7\u30fc\u30bf\u5de1\u56de\u306e\u6761\u4ef6 (\u983b\u5ea6\u3084\u3001\u968e\u5c64\u306a\u3069)\n##\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210\n\u307e\u305a\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n`$ scrapy startproject tutorial`\n\u3059\u308b\u3068\u3001\u3053\u306e\u3088\u3046\u306a\u30d5\u30a9\u30eb\u30c0\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\n\n```Python:tutorial/\ntutorial/\n    scrapy.cfg            # deploy configuration file\n\n    tutorial/             # project's Python module, you'll import your code from here\n        __init__.py\n\n        items.py          # project items file\n\n        pipelines.py      # project pipelines file\n\n        settings.py       # project settings file\n\n        spiders/          # a directory where you'll later put your spiders\n            __init__.py\n            ...\n```\n\n##\u62bd\u51fa\u30c7\u30fc\u30bf\u306e\u5b9a\u7fa9\n\u4f55\u3092\u5f97\u308b\u304b\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u8a00\u3046\u3001\u30d5\u30a3\u30fc\u30eb\u30c9\u306e\u5b9a\u7fa9\u3067\u3059\u3002\n\n```items.py\nimport scrapy\n\nclass WebItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    date = scrapy.Field()\n```\n\n##\u30b9\u30d1\u30a4\u30c0\u30fc\u306e\u4f5c\u6210\n\u3053\u3053\u304c\u30a6\u30a7\u30d6\u3092\u5de1\u56de\u3057\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\u3059\u308b\u3001\u82b1\u5f62\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\u5de1\u56de\u958b\u59cb\u306e\u30a2\u30c9\u30ec\u30b9\u3068\u3001\u5de1\u56de\u6761\u4ef6\u3001\u305d\u3057\u3066\u30c7\u30fc\u30bf\u62bd\u51fa\u6761\u4ef6\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n######\u30b9\u30d1\u30a4\u30c0\u30fc\u306e\u751f\u6210\n\u30b9\u30d1\u30a4\u30c0\u30fc\u3092\u4f5c\u308a\u307e\u3059\u3002\u69cb\u6587\u306f `$ scrapy genspider [options] <name> <domain(\u5de1\u56de\u3059\u308b\u30b5\u30a4\u30c8\u30c9\u30e1\u30a4\u30f3)>`\u3067\u3059\u3002\n\n```py:commandline\n$ scrapy genspider webspider exsample.com\n  Created spider 'webspider' using template 'basic' in module:\n  tutorial.spiders.webspider\n```\n\n\u751f\u6210\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u306f\n\n```py:tutorial/spiders/webspider.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass WebspiderSpider(scrapy.Spider):\n    name = \"webspider\"   # \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u5185\u3067\u306e\u540d\u524d\u3002\u52d5\u304b\u3059\u3068\u304d\u306e\u30b9\u30d1\u30a4\u30c0\u30fc\u6307\u5b9a\u3067\u4f7f\u308f\u308c\u308b\n    allowed_domains = [\"exsample.com\"] # \u5de1\u56deOK\u306e\u30c9\u30e1\u30a4\u30f3\u6307\u5b9a\n    start_urls = (\n        'http://www.exsample.com/', # \u3053\u3053\u3092\u8d77\u70b9\u306b\u3059\u308b\u3002\u30ea\u30b9\u30c8\u3067\u8907\u6570\u6307\u5b9a\u3067\u304d\u308b\u3002\n    )\n\n    def parse(self, response):  # \u3053\u3053\u304c\u62bd\u51fa\u6761\u4ef6\n        pass\n```\n\n\u3053\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\n\u3053\u308c\u3092\u3001\u81ea\u5206\u597d\u307f\u306b\u5909\u3048\u307e\u3059\u3002\n\n```py:tutorial/spiders/webspider.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom tutorial.items import WebItem\nimport re\nimport datetime\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nclass WebspiderSpider(CrawlSpider):  #\u30af\u30e9\u30b9\u540d\u306b\u305f\u3044\u3057\u305f\u610f\u5473\u306f\u306a\u3044\n    name = 'WebspiderSpider'  # \u3053\u308c\u306f\u91cd\u8981\u3002\u3053\u306e\u540d\u524d\u3092\u6307\u5b9a\u3057\u3066\u30b9\u30d1\u30a4\u30c0\u30fc(\u30af\u30ed\u30fc\u30e9\u30fc)\u3092\u52d5\u304b\u3059\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com']\n\n    xpath = {\n        'title' : \"//title/text()\",\n    }\n\n    list_allow = [r'(\u6b63\u898f\u8868\u73fe)'] #\u3053\u306e\u6761\u4ef6\u306b\u5408\u3046\u30ea\u30f3\u30af\u306f\u5de1\u56de\n    list_deny = [\n                r'/exsample/hogehoge/hoge/', # \u3053\u3061\u3089\u306f\u5de1\u56de\u3057\u306a\u3044\u30ea\u30f3\u30af\u306e\u6307\u5b9a\u4f8b\u3002\u30ea\u30b9\u30c8\u8868\u8a18\u3082\u53ef\u80fd\n            ]\n    list_allow_parse = [r'(\u6b63\u898f\u8868\u73fe)']  #\u30c7\u30fc\u30bf\u62bd\u51fa\u3059\u308b\u30ea\u30f3\u30af\u6307\u5b9a\n    list_deny_parse = [                #\u30c7\u30fc\u30bf\u62bd\u51fa\u3057\u306a\u3044\u30ea\u30f3\u30af\u6307\u5b9a\n                r'(\u6b63\u898f\u8868\u73fe)',\n                r'(\u6b63\u898f\u8868\u73fe)',\n                ]\n\n    rules = (\n        # \u5de1\u56de\u30eb\u30fc\u30eb\u3002\n        Rule(LinkExtractor(\n            allow=list_allow,\n            deny=list_deny,\n            ),\n            follow=True # \u305d\u306e\u30ea\u30f3\u30af\u3078\u5165\u3063\u3066\u3044\u304f\n        ),\n        # \u30c7\u30fc\u30bf\u62bd\u51fa\u30eb\u30fc\u30eb\n        Rule(LinkExtractor(\n            allow=list_allow_parse,\n            deny=list_deny_parse,\n            unique=True # \u304a\u306a\u3058\u30ea\u30f3\u30af\u5148\u3067\u306f\u30c7\u30fc\u30bf\u62bd\u51fa\u3057\u306a\u3044\n            ),\n            callback='parse_items' # \u6761\u4ef6\u306b\u5408\u3048\u3070\u3001\u3053\u3053\u3067\u6307\u5b9a\u3057\u305f\u30c7\u30fc\u30bf\u62bd\u51fa\u5b9f\u884c\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u3002\n        ),\n    )\n\n   #\u30c7\u30fc\u30bf\u62bd\u51fa\u95a2\u6570\u5b9a\u7fa9\n   def parse_items(self, response): # response \u306b\u3001\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u306e\u60c5\u5831\u304c\u5165\u3063\u3066\u3044\u308b\n        item = WebItem()  # items.py\u3067\u6307\u5b9a\u3057\u305f\u30af\u30e9\u30b9\n        item['title'] = response.xpath(self.xpath['title']).extract()[0]\n        item['link'] = response.url\n        item['date'] = datetime.datetime.utcnow() + datetime.timedelta(hours=9) # \u73fe\u5728\u6642\u9593\u3002\u65e5\u672c\u6642\u9593\u306b\u3057\u3066\u7a81\u3063\u8fbc\u3080\u3002\n\n        yield item\n```\n\n\u66f8\u3044\u3066\u3044\u308b\u5185\u5bb9\u306f\u30b3\u30e1\u30f3\u30c8\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n##pipeline.py\u306e\u7de8\u96c6\n\u4e0a\u8a18\u3067\u4f5c\u6210\u3057\u305f\u30b9\u30d1\u30a4\u30c0\u30fc\u304b\u3089 `yield item` \u3092\u3001mongoDB \u306b\u7a81\u3063\u8fbc\u307f\u307e\u3059\u3002\n\n```py:pipelines.py\nfrom pymongo import MongoClient  # mongoDB \u3068\u306e\u63a5\u7d9a\nimport datetime\n\nclass TutorialPipeline(object):\n\n    def __init__(self, mongo_uri, mongo_db, mongolab_user, mongolab_pass):\n        # \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306b\u6e21\u3055\u308c\u305f\u5f15\u6570\u3067\u3001\u5909\u6570\u521d\u671f\u5316\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n        self.mongolab_user = mongolab_user\n        self.mongolab_pass = mongolab_pass\n\n    @classmethod  # \u5f15\u6570\u306b\u30af\u30e9\u30b9\u304c\u3042\u308b\u306e\u3067\u3001\u30af\u30e9\u30b9\u5909\u6570\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'), # settings.py \u3066\u5b9a\u7fa9\u3057\u305f\u5909\u6570\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items'),\n            mongolab_user=crawler.settings.get('MONGOLAB_USER'),\n            mongolab_pass=crawler.settings.get('MONGOLAB_PASS')\n        ) # def __init__ \u306e\u5f15\u6570\u306b\u306a\u308b\n\n    def open_spider(self, spider): # \u30b9\u30d1\u30a4\u30c0\u30fc\u958b\u59cb\u6642\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\n        self.client = MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n        self.db.authenticate(self.mongolab_user, self.mongolab_pass)\n\n    def close_spider(self, spider): # \u30b9\u30d1\u30a4\u30c0\u30fc\u7d42\u4e86\u6642\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u9589\u3058\u308b\n        self.client.close()\n\n    def process_item(self, item, spider):\n        self.db[self.collection_name].update(\n            {u'link': item['link']},\n            {\"$set\": dict(item)},\n            upsert = True\n        ) # link\u3092\u691c\u7d22\u3057\u3066\u3001\u306a\u3051\u308c\u3070\u65b0\u898f\u4f5c\u6210\u3001\u3042\u308c\u3070\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b\n\n        return item\n```\n\n\u3044\u308d\u3044\u308d\u66f8\u3044\u3066\u3044\u307e\u3059\u304c\u3001\u8981\u306f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u958b\u3051\u3066\u3001\u30c7\u30fc\u30bf\u7a81\u3063\u8fbc\u3093\u3067\u3001\u7d42\u308f\u3063\u305f\u3089\u9589\u3058\u3066\u3044\u308b\u3060\u3051\u3067\u3059\u3002\n\n##settings.py\n\u307e\u305a\u306f\u3001pipelines.py \u3067\u547c\u3073\u51fa\u3057\u3066\u3044\u308b\u5404\u7a2e\u5909\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n```py:settings.py\nMONGO_URI = 'hogehoge.mongolab.com:(port\u756a\u53f7)'\nMONGO_DATABASE = 'database_name'\nMONGOLAB_USER = 'user_name'\nMONGOLAB_PASS = 'password'\n```\n\n\u3053\u308c\u306fmongolab\u306e\u4f8b\u3067\u3059\u3002\nsettings.py\u3067\u306f\u4ed6\u306b\u3001\u6319\u52d5\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n```py:settings.py\nREDIRECT_MAX_TIMES = 6\nRETRY_ENABLED = False\nDOWNLOAD_DELAY=10\nCOOKIES_ENABLED=False\n```\n\n\u3053\u3053\u3067\u306f\u3001\u30ea\u30c0\u30a4\u30ec\u30af\u30c8\u306e\u6700\u5927\u56de\u6570\u30926\u56de\u306b\u3001\u30ea\u30c8\u30e9\u30a4\u3092\u5b9f\u884c\u3057\u306a\u3044\u3088\u3046\u306b\u3001\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306f10\u79d2\u3054\u3068\u306b\u3001\u30af\u30c3\u30ad\u30fc\u306f\u4fdd\u5b58\u3057\u306a\u3044\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u6761\u4ef6\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n`DOWNLOAD_DELAY`\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3001\u30d0\u30b7\u30d0\u30b7\u5168\u529b\u3067\u30a2\u30af\u30bb\u30b9\u3057\u307e\u304f\u308b\u306e\u3067\u3001\u5de1\u56de\u5148\u306e\u30b5\u30a4\u30c8\u306b\u5927\u304d\u306a\u8ca0\u8377\u304c\u304b\u304b\u308a\u307e\u3059\u3002\u3084\u3081\u307e\u3057\u3087\u3046\u3002\n\n##\u5b9f\u884c\n\u5b9f\u884c\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n```py:commandline\n$ scrapy crawl WebspiderSpider\n```\n\n\u3069\u3093\u3069\u3093\u30ea\u30f3\u30af\u3092\u8fbf\u3063\u3066\u3044\u304d\u3001\u6761\u4ef6\u306b\u3042\u3046\u30ea\u30f3\u30af\u304b\u3089\u30c7\u30fc\u30bf\u304c\u62bd\u51fa\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002\n", "tags": ["Python", "Scrapy", "crawler", "MongoDB", "scraping"]}