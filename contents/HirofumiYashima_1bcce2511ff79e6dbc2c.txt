{"context": "\n\n\u3010 \u53c2\u8003\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u3011\n\n\nHatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18 \uff082015-10-25\uff09 \u300cWord Embedding using GloVe\u300d\n\n\n\nGloVe \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u53d6\u5f97\n\n\nGloVe: Global Vectors for Word Representation Jeffrey Pennington, Richard Socher, Christopher D. Manning\n\n\u4e0a\u8a18 \u306e GloVe \u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u304b\u3089\u3001GloVe v.1.2 \u3092 \u30c0\u30d6\u30eb\u30af\u30ea\u30c3\u30af\u3057\u3066\u3001GloVe-1.2.zip \u3092 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 \u3067\u304d\u308b\u3002\n\nRelease history\nGloVe v.1.2: Minor bug fixes in code (memory, off-by-one, errors). Eval code now also available in Python and Octave. UTF-8 encoding of largest data file fixed. Prepared by Russell Stewart and Christopher Manning. Oct 2015.\nGloVe v.1.0: Original release. Prepared by Jeffrey Pennington. Aug 2014.\n\n\u4eca\u56de\u306f\u3001\u4ee5\u4e0b\u306eGitHub \u30ea\u30dd\u30b8\u30c8\u30ea \u304b\u3089 git clone \u3057\u3066\u3082\u3067\u304d\u308b\n\n\uff08GitHub\uff09 stanfordnlp/GloVe\n\n\nDownload pre-trained word vectors\nIf you want word vectors trained on massive web datasets, you need only download one of these text files! Pre-trained word vectors are made available under the Public Domain Dedication and License.\n\nCommon Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\nCommon Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\nWikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download): glove.6B.zip\nTwitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n\nTrain word vectors on a new corpus\nIf the web datasets above don't match the semantics of your end use case, you can train word vectors on your own corpus.\n\n$ git clone http://github.com/stanfordnlp/glove\n$ cd glove && make\n$ ./demo.sh\n\n\nThe demo.sh scipt downloads a small corpus, consisting of the first 100M characters of Wikipedia. It collects unigram counts, constructs and shuffles cooccurrence data, and trains a simple version of the GloVe model. It also runs a word analogy evaluation script in python to verify word vector quality. More details about training on your own corpus can be found by reading demo.sh or the src/README.md\n\n\ngit clone \u5b9f\u884c\n\n\nTerminal\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ git clone http://github.com/stanfordnlp/glove\nCloning into 'glove'...\nremote: Counting objects: 265, done.\nremote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\nReceiving objects: 100% (265/265), 130.67 KiB | 116.00 KiB/s, done.\nResolving deltas: 100% (139/139), done.\nChecking connectivity... done.\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ ls | grep glove\nglove\nglove-python\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ \nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ cd glove && make\nmkdir -p build\ngcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE     Makefile    README.md   build       demo.sh     eval        src\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\ndemo.sh \u5168\u6587\n\n\n\u59cb\u3081\u306b\u3001wget \u30b3\u30de\u30f3\u30c9 \u3092 \u5b9f\u884c\u3057\u3066\u3001corpus\u30d5\u30a1\u30a4\u30eb \u3068\u3057\u3066 Text8\u30d5\u30a1\u30a4\u30eb\u3092 \u53d6\u5f97\u3057\u3066\u3044\u308b\n\n\nwget http://mattmahoney.net/dc/text8.zip\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ cat demo.sh\n#!/bin/bash\nset -e\n\n# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.\n# One optional argument can specify the language used for eval script: matlab, octave or [default] python\n\nmake\nif [ ! -e text8 ]; then\n  if hash wget 2>/dev/null; then\n    wget http://mattmahoney.net/dc/text8.zip\n  else\n    curl -O http://mattmahoney.net/dc/text8.zip\n  fi\n  unzip text8.zip\n  rm text8.zip\nfi\n\nCORPUS=text8\nVOCAB_FILE=vocab.txt\nCOOCCURRENCE_FILE=cooccurrence.bin\nCOOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin\nBUILDDIR=build\nSAVE_FILE=vectors\nVERBOSE=2\nMEMORY=4.0\nVOCAB_MIN_COUNT=5\nVECTOR_SIZE=50\nMAX_ITER=15\nWINDOW_SIZE=15\nBINARY=2\nNUM_THREADS=8\nX_MAX=10\n\necho \"$ $BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\"\n$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\necho \"$ $BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\"\n$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\necho \"$ $BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\"\n$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\necho \"$ $BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\"\n$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\nif [ \"$CORPUS\" = 'text8' ]; then\n   if [ \"$1\" = 'matlab' ]; then\n       matlab -nodisplay -nodesktop -nojvm -nosplash < ./eval/matlab/read_and_evaluate.m 1>&2 \n   elif [ \"$1\" = 'octave' ]; then\n       octave < ./eval/octave/read_and_evaluate_octave.m 1>&2\n   else\n       echo \"$ python eval/python/evaluate.py\"\n       python eval/python/evaluate.py\n   fi\nfi\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\ndemo.sh \u3092 \u5b9f\u884c\u3057\u3066\u307f\u308b\n\n\ntext8.zip \u3092 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3001\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306e \u751f\u6210\u4f5c\u696d \u304c \u59cb\u307e\u308b\u6a21\u69d8\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ./demo.sh\nmkdir -p build\ngcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n--2016-09-25 17:51:02--  http://mattmahoney.net/dc/text8.zip\nResolving mattmahoney.net... 98.139.135.129\nConnecting to mattmahoney.net|98.139.135.129|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 31344016 (30M) [application/zip]\nSaving to: \u2018text8.zip\u2019\n\ntext8.zip                                   100%[=========================================================================================>]  29.89M   664KB/s    in 60s     \n\n2016-09-25 17:52:02 (511 KB/s) - \u2018text8.zip\u2019 saved [31344016/31344016]\n\nArchive:  text8.zip\n  inflating: text8                   \n$ build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\nBUILDING VOCABULARY\nProcessed 17005207 tokens.\nCounted 253854 unique words.\nTruncating vocabulary at min count 5.\nUsing vocabulary of size 71290.\n\n$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\nCOUNTING COOCCURRENCES\nwindow size: 15\ncontext: symmetric\nmax product: 13752509\noverflow length: 38028356\nReading vocab from file \"vocab.txt\"...loaded 71290 words.\nBuilding lookup table...table contains 94990279 elements.\nProcessed 17005206 tokens.\nWriting cooccurrences to disk.........2 files in total.\nMerging cooccurrence files: processed 60666466 lines.\n\n$ build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\nSHUFFLING COOCCURRENCES\narray size: 255013683\nShuffling by chunks: processed 60666466 lines.\nWrote 1 temporary file(s).\nMerging temp files: processed 60666466 lines.\n\n$ build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\nTRAINING MODEL\nRead 60666466 lines.\nInitializing parameters...done.\nvector size: 50\nvocab size: 71290\nx_max: 10.000000\nalpha: 0.750000\n09/25/16 - 05:54.33PM, iter: 001, cost: 0.068953\n09/25/16 - 05:55.01PM, iter: 002, cost: 0.051698\n09/25/16 - 05:55.33PM, iter: 003, cost: 0.046169\n09/25/16 - 05:56.06PM, iter: 004, cost: 0.043052\n09/25/16 - 05:56.37PM, iter: 005, cost: 0.041215\n09/25/16 - 05:57.12PM, iter: 006, cost: 0.040007\n09/25/16 - 05:57.47PM, iter: 007, cost: 0.039140\n09/25/16 - 05:58.23PM, iter: 008, cost: 0.038484\n09/25/16 - 05:59.01PM, iter: 009, cost: 0.037966\n09/25/16 - 05:59.38PM, iter: 010, cost: 0.037545\n09/25/16 - 06:00.18PM, iter: 011, cost: 0.037194\n09/25/16 - 06:00.52PM, iter: 012, cost: 0.036895\n09/25/16 - 06:01.23PM, iter: 013, cost: 0.036641\n09/25/16 - 06:01.56PM, iter: 014, cost: 0.036418\n09/25/16 - 06:02.29PM, iter: 015, cost: 0.036224\n$ python eval/python/evaluate.py\ncapital-common-countries.txt:\nACCURACY TOP1: 50.99% (258/506)\ncapital-world.txt:\nACCURACY TOP1: 25.36% (904/3564)\ncurrency.txt:\nACCURACY TOP1: 4.19% (25/596)\ncity-in-state.txt:\nACCURACY TOP1: 24.68% (575/2330)\nfamily.txt:\nACCURACY TOP1: 40.00% (168/420)\ngram1-adjective-to-adverb.txt:\nACCURACY TOP1: 5.04% (50/992)\ngram2-opposite.txt:\nACCURACY TOP1: 3.04% (23/756)\ngram3-comparative.txt:\nACCURACY TOP1: 26.43% (352/1332)\ngram4-superlative.txt:\nACCURACY TOP1: 8.37% (83/992)\ngram5-present-participle.txt:\nACCURACY TOP1: 11.55% (122/1056)\ngram6-nationality-adjective.txt:\nACCURACY TOP1: 54.17% (824/1521)\ngram7-past-tense.txt:\nACCURACY TOP1: 12.95% (202/1560)\ngram8-plural.txt:\nACCURACY TOP1: 25.23% (336/1332)\ngram9-plural-verbs.txt:\nACCURACY TOP1: 7.13% (62/870)\nQuestions seen/total: 91.21% (17827/19544)\nSemantic accuracy: 26.02%  (1930/7416)\nSyntactic accuracy: 19.73%  (2054/10411)\nTotal accuracy: 22.35%  (3984/17827)\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\n\ndemo.sh \u3067 \u5b9f\u884c\u3055\u308c\u305f\u3053\u3068\n\n\n\u5b9f\u884c\u624b\u9806\n\n-1. build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\n-2. build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\n-3. build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n-4. build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n-5. python eval/python/evaluate.py\n\nGloVe GitHUb\u30ec\u30dd\u30b8\u30c8\u30ea src \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u306e README \u3067 \u89e3\u8aac\u3055\u308c\u3066\u3044\u308b\u901a\u308a\n\n\nhttps://github.com/stanfordnlp/GloVe/tree/master/src\n\n\nThis four main tools in this package are:\n1) vocab_count\nConstructs unigram counts from a corpus, and optionally thresholds the resulting vocabulary based on total vocabulary size or minimum frequency count. \nThis file should already consist of whitespace-separated tokens.\nUse something like the Stanford Tokenizer first on raw text.\n2) cooccur\nConstructs word-word cooccurrence statistics from a corpus. \nThe user should supply a vocabulary file, as produced by vocab_count, and may specify a variety of parameters, as described by running ./build/cooccur.\n3) shuffle\nShuffles the binary file of cooccurrence statistics produced by cooccur. \nFor large files, the file is automatically split into chunks, each of which is shuffled and stored on disk before being merged and shuffled togther. \nThe user may specify a number of parameters, as described by running ./build/shuffle.\n4) glove\nTrain the GloVe model on the specified cooccurrence data, which typically will be the output of the shuffle tool.\nThe user should supply a vocabulary file, as given by vocab_count, and may specify a number of other parameters, which are described by running ./build/glove.\n\n\n\u53c2\u8003\u30a6\u30a7\u30d6\u30da\u30fc\u30b8\u300c\u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18\u300d\u3067\u3082\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u89e3\u8aac\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\nHatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18 \uff082015-10-25\uff09 \u300cWord Embedding using GloVe\u300d\n\n\n\u30ef\u30fc\u30c9\u3068\u30d9\u30af\u30c8\u30eb\u306e\u7d44\u307f\u5408\u308f\u305b\u304c\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u66f8\u304d\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u3002.sh\u3092\u78ba\u8a8d\u3059\u308b\u3068\n\n$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\n$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\n$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\n$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\n\n\n\u3053\u306e\u9806\u756a\u3067\u52d5\u4f5c\u3055\u305b\u308b\u3068\u3001\u624b\u52d5\u3067\u30d5\u30a1\u30a4\u30eb\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305d\u3046\u3067\u3059\u3002\n\u5909\u6570\u306f\u9069\u5b9c\u7f6e\u304d\u63db\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002CORPUS\u306f\u901a\u5e38\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306a\u306e\u3067\u3001\u6587\u5b57\u304c\u5165\u3063\u3066\u3044\u308c\u3070\u554f\u984c\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\uff08\u65e5\u672c\u8a9e\u3060\u3068\u5f62\u614b\u7d20\u89e3\u6790\u306a\u3069\u3092\u3057\u3066split\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u3002\uff09\n\n\n\ndemo.sh \u306e\u51fa\u529b\u30d5\u30a1\u30a4\u30eb \u3092 \u78ba\u8a8d\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE         README.md       cooccurrence.bin    demo.sh         src         vectors.bin     vocab.txt\nMakefile        build           cooccurrence.shuf.bin   eval            text8           vectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\ntext8\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ head text8\n anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to kropotkin zeno repudiated the omnipotence of the state its intervention and regimentation and proclaimed the sovereignty of the moral law of the individual the anabaptists of one six th century europe are sometimes considered to be religious forerunners of modern anarchism bertrand russell in his history of western philosophy writes that the anabaptists repudiated all law since they held that the good man will be guided at every moment by the holy spirit from this premise they arrive at communism the diggers or true levellers were an early communistic movement during the time of the english civil war and are considered by some as forerunners of modern anarchism in the modern era the first to use the term to mean something other than chaos was louis armand baron de lahontan in his nouveaux voyages dans l am rique septentrionale one seven zero three where he described\n\n\uff08 \u4e2d\u7565 \uff09\n\n\n\nvocab.txt\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l vocab.txt\n   71290 vocab.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ head vocab.txt \nthe 1061396\nof 593677\nand 416629\none 411764\nin 372201\na 325873\nto 316376\nzero 264975\nnine 250430\ntwo 192644\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n\n\n\nvectors.txt\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l vectors.txt\n   71291 vectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\uff08 1\u884c\u76ee \uff09\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '1,1p' vectors.txt \nthe -1.379710 -1.462139 -0.282897 -0.863171 0.464334 0.318731 -0.893265 2.000415 0.353473 -1.233146 -0.223402 1.059758 -1.125059 0.767546 -0.983119 -0.456772 0.137045 -1.352054 0.125428 0.130168 -0.256981 -1.010887 -0.014780 0.616861 -0.240992 1.074628 -0.297288 -0.222200 0.127056 -0.914219 -0.629716 0.307570 -0.331981 0.799037 0.790429 0.203322 -1.355381 -1.428889 0.473966 -1.303657 -0.567251 0.961569 -0.013164 -1.152087 -1.055379 -1.303419 -0.918822 -1.171620 0.146521 0.503275\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n\n\n\uff08 2\u884c\u76ee \uff09\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '2,2p' vectors.txt \nof -0.473577 -1.222222 -1.115821 -0.453052 0.219024 1.068070 -1.407289 2.534627 0.428462 -0.850468 0.128521 1.567760 -0.564698 0.859289 -0.633664 -0.552737 -0.425765 -1.330973 0.368091 0.927799 -0.793961 -0.898895 0.318299 0.341107 -0.882182 0.119855 0.285407 -1.103808 -0.656047 -0.928127 -0.675922 0.384731 0.087517 0.508578 -0.068728 -0.412215 -1.295717 -1.643162 0.978488 -0.403695 -0.324177 0.359110 0.283563 -1.574671 -0.516422 -0.965030 0.147162 -0.980163 0.984243 0.313542\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n\n\n\uff08 3\u884c\u76ee \uff09\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '3,3p' vectors.txt \nand -0.371746 -1.289212 -0.320571 -0.952901 0.559102 0.234208 -0.779906 1.527497 0.919236 -0.822447 -0.255847 1.571216 -0.830962 0.497985 -0.050695 0.297993 -1.027054 -1.488593 0.847045 0.021846 -0.474635 -1.270355 0.763695 0.648214 -1.056612 1.253085 0.127453 -0.174881 -0.144203 -0.497339 -0.785462 0.009430 0.461268 0.278945 -0.054788 0.098556 -0.965274 -1.312005 -0.370406 -0.697468 -0.909752 0.665602 0.172027 -1.104685 0.426531 -0.503397 -0.177057 -1.257854 1.528592 0.784191\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\nvectors.txt \u3092 word2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u308b\u5f62\u5f0f \u306e \u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b\u5909\u63db\u3057\u3066\u3001Python gensim \u306b\u98df\u308f\u305b\u3066\u307f\u308b\n\n\nkanjirz50\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/23\uff09 \u300cGloVe\u304c\u51fa\u529b\u3057\u305f\u30e2\u30c7\u30eb(txt)\u3092gensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u6574\u5f62\u3059\u308b\u3002\u300d\n\n\nword2vec\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n1\u884c\u76ee\uff1a\u5358\u8a9e\u7a2e\u985e\u6570 \u6b21\u5143\u6570\n2\u884c\u76ee\u4ee5\u964d\uff1a\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 ...\n\u4e0a\u8a18\u306e\u3088\u3046\u306a\u30b7\u30f3\u30d7\u30eb\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u306a\u3063\u3066\u3044\u308b\u3002\n\uff1e GloVe\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 ...\n\u4e0a\u8a18\u306e\u3088\u3046\u306b\u3001word2vec\u306e1\u884c\u76ee\u304c\u6b20\u3051\u3066\u3044\u308b\u3002\n\u3064\u307e\u308a\u5358\u8a9e\u7a2e\u985e\u6570\u3068\u6b21\u5143\u6570\u30921\u884c\u76ee\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u5f62\u5f0f\u304c\u5909\u63db\u3067\u304d\u308b\u3002\n\n\u4e0a\u8a18\u3001kanjirx50\u3055\u3093 \u306e \u5909\u63db\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u305d\u306e\u307e\u307e\u62dd\u501f\uff08\u4ee5\u4e0b\uff09\u3057\u3066\u3001vectors.txt \u306b 1\u884c\u76ee \u3092 \u8ffd\u8a18\u3059\u308b__ \n\nglove2word2vec_format.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nvectors.txt \u306e\u683c\u7d0d\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u306b glove2word2vec_format.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u3092 \u914d\u7f6e\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ emacs glove2word2vec_format.py\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE             build               demo.sh             src             vectors.txt\nMakefile            cooccurrence.bin        eval                text8               vocab.txt\nREADME.md           cooccurrence.shuf.bin       glove2word2vec_format.py    vectors.bin\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n\n\n\n\u5b9f\u884c\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ python glove2word2vec_format.py vectors.txt > word2vec_formatted_vectors.txt\n\n\n\n1\u884c\u76ee \u304c \u8ffd\u8a18\u3055\u308c\u305f\n\n\nTerminal\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l word2vec_formatted_vectors.txt \n   71292 word2vec_formatted_vectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '1,1p' word2vec_formatted_vectors.txt \n71291 50\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '2,2p' word2vec_formatted_vectors.txt \nthe -1.379710 -1.462139 -0.282897 -0.863171 0.464334 0.318731 -0.893265 2.000415 0.353473 -1.233146 -0.223402 1.059758 -1.125059 0.767546 -0.983119 -0.456772 0.137045 -1.352054 0.125428 0.130168 -0.256981 -1.010887 -0.014780 0.616861 -0.240992 1.074628 -0.297288 -0.222200 0.127056 -0.914219 -0.629716 0.307570 -0.331981 0.799037 0.790429 0.203322 -1.355381 -1.428889 0.473966 -1.303657 -0.567251 0.961569 -0.013164 -1.152087 -1.055379 -1.303419 -0.918822 -1.171620 0.146521 0.503275\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '3,3p' word2vec_formatted_vectors.txt \nof -0.473577 -1.222222 -1.115821 -0.453052 0.219024 1.068070 -1.407289 2.534627 0.428462 -0.850468 0.128521 1.567760 -0.564698 0.859289 -0.633664 -0.552737 -0.425765 -1.330973 0.368091 0.927799 -0.793961 -0.898895 0.318299 0.341107 -0.882182 0.119855 0.285407 -1.103808 -0.656047 -0.928127 -0.675922 0.384731 0.087517 0.508578 -0.068728 -0.412215 -1.295717 -1.643162 0.978488 -0.403695 -0.324177 0.359110 0.283563 -1.574671 -0.516422 -0.965030 0.147162 -0.980163 0.984243 0.313542\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n\n\n\nPython \u30a4\u30f3\u30bf\u30d7\u30ea\u30bf \u3092 \u8d77\u52d5\u3057\u3066\u3001gensim.models.Word2Vec.load_word2vec_format\u30e1\u30bd\u30c3\u30c9 \u3067 vectors.txt \u3092 \u30ed\u30fc\u30c9\u3059\u308b\n\n\nPython 3\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> from gensim.models import Word2Vec\n>>> \n>>> model = Word2Vec.load_word2vec_format('./word2vec_formatted_vectors.txt', binary=False) \n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>>\n\n\n\n\nmodel \u306b \u5199\u50cf\u3055\u308c\u305f \u5358\u8a9e \u3092 \u78ba\u8a8d\n\n\nPython 3\n>>> vocab_list = list(model.vocab.keys())\n>>> print(len(vocab_list))\n71291\n>>> \n>>> from pprint import pprint\n>>> pprint(vocab_list[0:51])\n['electoral',\n 'dinosauria',\n 'voted',\n 'hogline',\n 'hf',\n 'entorhinal',\n 'kg',\n 'akihito',\n 'adds',\n 'microcosm',\n 'mammuthus',\n 'izabella',\n 'multicellular',\n 'canticle',\n 'shah',\n 'kyiv',\n 'fermionic',\n 'informers',\n 'chennai',\n 'encountering',\n 'albinus',\n 'beans',\n 'transmitters',\n 'expand',\n 'thermostat',\n 'semester',\n 'philospher',\n 'meyer',\n 'dimer',\n 'ogof',\n 'enable',\n 'scurvy',\n 'cinema',\n 'raporto',\n 'shipyards',\n 'maeshowe',\n 'theurgy',\n 'collet',\n 'kosovar',\n 'analytic',\n 'cabot',\n 'canalized',\n 'opacity',\n 'rutledge',\n 'clustering',\n 'hopes',\n 'felipe',\n 'giovani',\n 'socket',\n 'crooner',\n 'schiaparelli']\n>>>\n\n\n\nword-similarity \u30bf\u30b9\u30af \u3092 \u5b9f\u884c\n\n\nPython 3\n>>> print(model.similarity(\"cinema\", \"hopes\"))\n0.13977033022\n>>>\n\n\n\nword-analogy \u30bf\u30b9\u30af \u3092 \u5b9f\u884c\n\n\nPython 3\n>>> model.most_similar(positive=[\"cinema\"], negative=[\"hopes\"])\n[('opera', 0.5363454222679138), ('hindustani', 0.5091257095336914), ('fauvism', 0.5073018074035645), ('iptv', 0.49624091386795044), ('caldera', 0.4948546290397644), ('chinese', 0.4925414025783539), ('federico', 0.49221259355545044), ('pilsner', 0.49128100275993347), ('ballad', 0.4854658842086792), ('funniest', 0.4830397367477417)]\n>>> \n\n\n\n\u5b9f\u884c\u6210\u529f\n\n\n\n\u3010 \u53c2\u8003 \u3011\n\n\nyuku_t\u3055\u3093 Qiita\u8a18\u4e8b \uff082015/12/01\uff09 \u300c\u5358\u7d14\u306a\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u8868\u73fe: word2vec, GloVe\u300d\nDeepAge \u4eba\u5de5\u77e5\u80fd\u306e\u4eca\u3068\u4e00\u6b69\u5148\u3092\u767a\u4fe1\u3059\u308b\u30e1\u30c7\u30a3\u30a2 \uff082016-09-2\uff09 \u300cWord2Vec\uff1a\u767a\u660e\u3057\u305f\u672c\u4eba\u3082\u9a5a\u304f\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u9a5a\u7570\u7684\u306a\u529b\u300d\n\n\nWord2Vec\u306e\u6d3e\u751f\u7cfb\u3084\u985e\u4f3c\u30c4\u30fc\u30eb\nGloVe\nGlobal Vectors for Word Representation\u306a\u7565\u3089\u3057\u3044\u3002\nWord2Vec\u3088\u308a\u3082\u6027\u80fd\u304c\u9ad8\u3044\u3002 \u5b66\u7fd2\u304c\u901f\u304f\u3001\u5927\u304d\u306a\u30b3\u30fc\u30d1\u30b9\u3067\u3082\u5bfe\u5fdc\u51fa\u6765\u3001\u5c0f\u3055\u306a\u30b3\u30fc\u30d1\u30b9\u30fb\u5c0f\u3055\u306a\u30d9\u30af\u30c8\u30eb\u3067\u3082\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u51fa\u308b\u3002\n\n####__\u3010 \u53c2\u8003\u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u3011__\n\n* [Hatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18 \uff082015-10-25\uff09 \u300cWord Embedding using GloVe\u300d](http://nonbiri-tereka.hatenablog.com/entry/2015/10/25/223430)\n\n___\n\n\n###__GloVe \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u53d6\u5f97__\n\n* [GloVe: Global Vectors for Word Representation _Jeffrey Pennington, Richard Socher, Christopher D. Manning_](http://nlp.stanford.edu/projects/glove/)\n\n\u4e0a\u8a18 \u306e _GloVe_ \u30a6\u30a7\u30d6\u30da\u30fc\u30b8 \u304b\u3089\u3001GloVe v.1.2 \u3092 \u30c0\u30d6\u30eb\u30af\u30ea\u30c3\u30af\u3057\u3066\u3001GloVe-1.2.zip \u3092 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 \u3067\u304d\u308b\u3002\n\n> __Release history__\n>\n> GloVe v.1.2: Minor bug fixes in code (memory, off-by-one, errors). Eval code now also available in Python and Octave. UTF-8 encoding of largest data file fixed. Prepared by Russell Stewart and Christopher Manning. Oct 2015.\n> GloVe v.1.0: Original release. Prepared by Jeffrey Pennington. Aug 2014.\n\n__\u4eca\u56de\u306f\u3001\u4ee5\u4e0b\u306eGitHub \u30ea\u30dd\u30b8\u30c8\u30ea \u304b\u3089 git clone \u3057\u3066\u3082\u3067\u304d\u308b__\n\n* [\uff08GitHub\uff09 stanfordnlp/GloVe]\n(http://nlp.stanford.edu/projects/glove/)\n\n> __Download pre-trained word vectors__\n>\n>If you want word vectors trained on massive web datasets, you need only download one of these text files! Pre-trained word vectors are made available under the Public Domain Dedication and License.\n>\n> * Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n>* Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n>* Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download): glove.6B.zip\n>* Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n>\n> __Train word vectors on a new corpus__\n>\n>If the web datasets above don't match the semantics of your end use case, you can train word vectors on your own corpus.\n>\n>>```\n>>$ git clone http://github.com/stanfordnlp/glove\n>>$ cd glove && make\n>>$ ./demo.sh\n>>```\n>\n> The demo.sh scipt downloads a small corpus, consisting of the first 100M characters of Wikipedia. It collects unigram counts, constructs and shuffles cooccurrence data, and trains a simple version of the GloVe model. It also runs a word analogy evaluation script in python to verify word vector quality. More details about training on your own corpus can be found by reading demo.sh or the src/README.md\n\n* git clone \u5b9f\u884c\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ git clone http://github.com/stanfordnlp/glove\nCloning into 'glove'...\nremote: Counting objects: 265, done.\nremote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\nReceiving objects: 100% (265/265), 130.67 KiB | 116.00 KiB/s, done.\nResolving deltas: 100% (139/139), done.\nChecking connectivity... done.\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ ls | grep glove\nglove\nglove-python\nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ \nHirofumiYashima-no-MacBook:pdf hirofumiyashima$ cd glove && make\nmkdir -p build\ngcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE\t\tMakefile\tREADME.md\tbuild\t\tdemo.sh\t\teval\t\tsrc\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n####__demo.sh \u5168\u6587__\n\n* __\u59cb\u3081\u306b\u3001wget \u30b3\u30de\u30f3\u30c9 \u3092 \u5b9f\u884c\u3057\u3066\u3001corpus\u30d5\u30a1\u30a4\u30eb \u3068\u3057\u3066 Text8\u30d5\u30a1\u30a4\u30eb\u3092 \u53d6\u5f97\u3057\u3066\u3044\u308b__\n\n> wget http://mattmahoney.net/dc/text8.zip\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ cat demo.sh\n#!/bin/bash\nset -e\n\n# Makes programs, downloads sample data, trains a GloVe model, and then evaluates it.\n# One optional argument can specify the language used for eval script: matlab, octave or [default] python\n\nmake\nif [ ! -e text8 ]; then\n  if hash wget 2>/dev/null; then\n    wget http://mattmahoney.net/dc/text8.zip\n  else\n    curl -O http://mattmahoney.net/dc/text8.zip\n  fi\n  unzip text8.zip\n  rm text8.zip\nfi\n\nCORPUS=text8\nVOCAB_FILE=vocab.txt\nCOOCCURRENCE_FILE=cooccurrence.bin\nCOOCCURRENCE_SHUF_FILE=cooccurrence.shuf.bin\nBUILDDIR=build\nSAVE_FILE=vectors\nVERBOSE=2\nMEMORY=4.0\nVOCAB_MIN_COUNT=5\nVECTOR_SIZE=50\nMAX_ITER=15\nWINDOW_SIZE=15\nBINARY=2\nNUM_THREADS=8\nX_MAX=10\n\necho \"$ $BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\"\n$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\necho \"$ $BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\"\n$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\necho \"$ $BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\"\n$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\necho \"$ $BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\"\n$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\nif [ \"$CORPUS\" = 'text8' ]; then\n   if [ \"$1\" = 'matlab' ]; then\n       matlab -nodisplay -nodesktop -nojvm -nosplash < ./eval/matlab/read_and_evaluate.m 1>&2 \n   elif [ \"$1\" = 'octave' ]; then\n       octave < ./eval/octave/read_and_evaluate_octave.m 1>&2\n   else\n       echo \"$ python eval/python/evaluate.py\"\n       python eval/python/evaluate.py\n   fi\nfi\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n####__demo.sh \u3092 \u5b9f\u884c\u3057\u3066\u307f\u308b__\n\n* text8.zip \u3092 \u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u3001\u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306e \u751f\u6210\u4f5c\u696d \u304c \u59cb\u307e\u308b\u6a21\u69d8\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ./demo.sh\nmkdir -p build\ngcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\ngcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wno-unused-result\n--2016-09-25 17:51:02--  http://mattmahoney.net/dc/text8.zip\nResolving mattmahoney.net... 98.139.135.129\nConnecting to mattmahoney.net|98.139.135.129|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 31344016 (30M) [application/zip]\nSaving to: \u2018text8.zip\u2019\n\ntext8.zip                                   100%[=========================================================================================>]  29.89M   664KB/s    in 60s     \n\n2016-09-25 17:52:02 (511 KB/s) - \u2018text8.zip\u2019 saved [31344016/31344016]\n\nArchive:  text8.zip\n  inflating: text8                   \n$ build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\nBUILDING VOCABULARY\nProcessed 17005207 tokens.\nCounted 253854 unique words.\nTruncating vocabulary at min count 5.\nUsing vocabulary of size 71290.\n\n$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\nCOUNTING COOCCURRENCES\nwindow size: 15\ncontext: symmetric\nmax product: 13752509\noverflow length: 38028356\nReading vocab from file \"vocab.txt\"...loaded 71290 words.\nBuilding lookup table...table contains 94990279 elements.\nProcessed 17005206 tokens.\nWriting cooccurrences to disk.........2 files in total.\nMerging cooccurrence files: processed 60666466 lines.\n\n$ build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\nSHUFFLING COOCCURRENCES\narray size: 255013683\nShuffling by chunks: processed 60666466 lines.\nWrote 1 temporary file(s).\nMerging temp files: processed 60666466 lines.\n\n$ build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\nTRAINING MODEL\nRead 60666466 lines.\nInitializing parameters...done.\nvector size: 50\nvocab size: 71290\nx_max: 10.000000\nalpha: 0.750000\n09/25/16 - 05:54.33PM, iter: 001, cost: 0.068953\n09/25/16 - 05:55.01PM, iter: 002, cost: 0.051698\n09/25/16 - 05:55.33PM, iter: 003, cost: 0.046169\n09/25/16 - 05:56.06PM, iter: 004, cost: 0.043052\n09/25/16 - 05:56.37PM, iter: 005, cost: 0.041215\n09/25/16 - 05:57.12PM, iter: 006, cost: 0.040007\n09/25/16 - 05:57.47PM, iter: 007, cost: 0.039140\n09/25/16 - 05:58.23PM, iter: 008, cost: 0.038484\n09/25/16 - 05:59.01PM, iter: 009, cost: 0.037966\n09/25/16 - 05:59.38PM, iter: 010, cost: 0.037545\n09/25/16 - 06:00.18PM, iter: 011, cost: 0.037194\n09/25/16 - 06:00.52PM, iter: 012, cost: 0.036895\n09/25/16 - 06:01.23PM, iter: 013, cost: 0.036641\n09/25/16 - 06:01.56PM, iter: 014, cost: 0.036418\n09/25/16 - 06:02.29PM, iter: 015, cost: 0.036224\n$ python eval/python/evaluate.py\ncapital-common-countries.txt:\nACCURACY TOP1: 50.99% (258/506)\ncapital-world.txt:\nACCURACY TOP1: 25.36% (904/3564)\ncurrency.txt:\nACCURACY TOP1: 4.19% (25/596)\ncity-in-state.txt:\nACCURACY TOP1: 24.68% (575/2330)\nfamily.txt:\nACCURACY TOP1: 40.00% (168/420)\ngram1-adjective-to-adverb.txt:\nACCURACY TOP1: 5.04% (50/992)\ngram2-opposite.txt:\nACCURACY TOP1: 3.04% (23/756)\ngram3-comparative.txt:\nACCURACY TOP1: 26.43% (352/1332)\ngram4-superlative.txt:\nACCURACY TOP1: 8.37% (83/992)\ngram5-present-participle.txt:\nACCURACY TOP1: 11.55% (122/1056)\ngram6-nationality-adjective.txt:\nACCURACY TOP1: 54.17% (824/1521)\ngram7-past-tense.txt:\nACCURACY TOP1: 12.95% (202/1560)\ngram8-plural.txt:\nACCURACY TOP1: 25.23% (336/1332)\ngram9-plural-verbs.txt:\nACCURACY TOP1: 7.13% (62/870)\nQuestions seen/total: 91.21% (17827/19544)\nSemantic accuracy: 26.02%  (1930/7416)\nSyntactic accuracy: 19.73%  (2054/10411)\nTotal accuracy: 22.35%  (3984/17827)\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n___\n\n###__demo.sh \u3067 \u5b9f\u884c\u3055\u308c\u305f\u3053\u3068__\n\n###__\u5b9f\u884c\u624b\u9806__\n\n-1. build/vocab_count -min-count 5 -verbose 2 < text8 > vocab.txt\n\n-2. build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < text8 > cooccurrence.bin\n\n-3. build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n\n-4. build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n\n-5. python eval/python/evaluate.py\n\n####__GloVe GitHUb\u30ec\u30dd\u30b8\u30c8\u30ea src \u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u306e README \u3067 \u89e3\u8aac\u3055\u308c\u3066\u3044\u308b\u901a\u308a__\n\n* https://github.com/stanfordnlp/GloVe/tree/master/src\n\n>This four main tools in this package are:\n>\n> __1) vocab_count__\n>\n> Constructs unigram counts from a corpus, and optionally thresholds the resulting vocabulary based on total vocabulary size or minimum frequency count. \n>This file should already consist of whitespace-separated tokens.\n>\n>  Use something like the Stanford Tokenizer first on raw text.\n>\n> __2) cooccur__\n>\n> Constructs word-word cooccurrence statistics from a corpus. \n>The user should supply a vocabulary file, as produced by vocab_count, and may specify a variety of parameters, as described by running ./build/cooccur.\n>\n>__3) shuffle__\n>\n> Shuffles the binary file of cooccurrence statistics produced by cooccur. \n>For large files, the file is automatically split into chunks, each of which is shuffled and stored on disk before being merged and shuffled togther. \n>\n>The user may specify a number of parameters, as described by running ./build/shuffle.\n>\n> __4) glove__\n>\n> Train the GloVe model on the specified cooccurrence data, which typically will be the output of the shuffle tool.\n>The user should supply a vocabulary file, as given by vocab_count, and may specify a number of other parameters, which are described by running ./build/glove.\n\n####__\u53c2\u8003\u30a6\u30a7\u30d6\u30da\u30fc\u30b8\u300c\u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18\u300d\u3067\u3082\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u89e3\u8aac\u3055\u308c\u3066\u3044\u307e\u3059\u3002__\n\n* [Hatena Blog \u306e\u3093\u3073\u308a\u3057\u3066\u3044\u308b\u30a8\u30f3\u30b8\u30cb\u30a2\u306e\u65e5\u8a18 \uff082015-10-25\uff09 \u300cWord Embedding using GloVe\u300d](http://nonbiri-tereka.hatenablog.com/entry/2015/10/25/223430)\n\n>__\u30ef\u30fc\u30c9\u3068\u30d9\u30af\u30c8\u30eb\u306e\u7d44\u307f\u5408\u308f\u305b\u304c\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u66f8\u304d\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u3002.sh\u3092\u78ba\u8a8d\u3059\u308b\u3068__\n>\n>>```\n>>$BUILDDIR/vocab_count -min-count $VOCAB_MIN_COUNT -verbose $VERBOSE < $CORPUS > $VOCAB_FILE\n>>$BUILDDIR/cooccur -memory $MEMORY -vocab-file $VOCAB_FILE -verbose $VERBOSE -window-size $WINDOW_SIZE < $CORPUS > $COOCCURRENCE_FILE\n>>$BUILDDIR/shuffle -memory $MEMORY -verbose $VERBOSE < $COOCCURRENCE_FILE > $COOCCURRENCE_SHUF_FILE\n>>$BUILDDIR/glove -save-file $SAVE_FILE -threads $NUM_THREADS -input-file $COOCCURRENCE_SHUF_FILE -x-max $X_MAX -iter $MAX_ITER -vector-size $VECTOR_SIZE -binary $BINARY -vocab-file $VOCAB_FILE -verbose $VERBOSE\n>>```\n>\n> \u3053\u306e\u9806\u756a\u3067\u52d5\u4f5c\u3055\u305b\u308b\u3068\u3001\u624b\u52d5\u3067\u30d5\u30a1\u30a4\u30eb\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305d\u3046\u3067\u3059\u3002\n> \n> \u5909\u6570\u306f\u9069\u5b9c\u7f6e\u304d\u63db\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002CORPUS\u306f\u901a\u5e38\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306a\u306e\u3067\u3001\u6587\u5b57\u304c\u5165\u3063\u3066\u3044\u308c\u3070\u554f\u984c\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u3002\uff08\u65e5\u672c\u8a9e\u3060\u3068\u5f62\u614b\u7d20\u89e3\u6790\u306a\u3069\u3092\u3057\u3066split\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u3002\uff09\n\n___\n\n####__demo.sh \u306e\u51fa\u529b\u30d5\u30a1\u30a4\u30eb \u3092 \u78ba\u8a8d__\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE\t\t\tREADME.md\t\tcooccurrence.bin\tdemo.sh\t\t\tsrc\t\t\tvectors.bin\t\tvocab.txt\nMakefile\t\tbuild\t\t\tcooccurrence.shuf.bin\teval\t\t\ttext8\t\t\tvectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n* text8\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ head text8\n anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to kropotkin zeno repudiated the omnipotence of the state its intervention and regimentation and proclaimed the sovereignty of the moral law of the individual the anabaptists of one six th century europe are sometimes considered to be religious forerunners of modern anarchism bertrand russell in his history of western philosophy writes that the anabaptists repudiated all law since they held that the good man will be guided at every moment by the holy spirit from this premise they arrive at communism the diggers or true levellers were an early communistic movement during the time of the english civil war and are considered by some as forerunners of modern anarchism in the modern era the first to use the term to mean something other than chaos was louis armand baron de lahontan in his nouveaux voyages dans l am rique septentrionale one seven zero three where he described\n\n\uff08 \u4e2d\u7565 \uff09\n```\n\n* vocab.txt\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l vocab.txt\n   71290 vocab.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n```\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ head vocab.txt \nthe 1061396\nof 593677\nand 416629\none 411764\nin 372201\na 325873\nto 316376\nzero 264975\nnine 250430\ntwo 192644\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n```\n\n* vectors.txt\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l vectors.txt\n   71291 vectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n\uff08 1\u884c\u76ee \uff09\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '1,1p' vectors.txt \nthe -1.379710 -1.462139 -0.282897 -0.863171 0.464334 0.318731 -0.893265 2.000415 0.353473 -1.233146 -0.223402 1.059758 -1.125059 0.767546 -0.983119 -0.456772 0.137045 -1.352054 0.125428 0.130168 -0.256981 -1.010887 -0.014780 0.616861 -0.240992 1.074628 -0.297288 -0.222200 0.127056 -0.914219 -0.629716 0.307570 -0.331981 0.799037 0.790429 0.203322 -1.355381 -1.428889 0.473966 -1.303657 -0.567251 0.961569 -0.013164 -1.152087 -1.055379 -1.303419 -0.918822 -1.171620 0.146521 0.503275\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n```\n\n\uff08 2\u884c\u76ee \uff09\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '2,2p' vectors.txt \nof -0.473577 -1.222222 -1.115821 -0.453052 0.219024 1.068070 -1.407289 2.534627 0.428462 -0.850468 0.128521 1.567760 -0.564698 0.859289 -0.633664 -0.552737 -0.425765 -1.330973 0.368091 0.927799 -0.793961 -0.898895 0.318299 0.341107 -0.882182 0.119855 0.285407 -1.103808 -0.656047 -0.928127 -0.675922 0.384731 0.087517 0.508578 -0.068728 -0.412215 -1.295717 -1.643162 0.978488 -0.403695 -0.324177 0.359110 0.283563 -1.574671 -0.516422 -0.965030 0.147162 -0.980163 0.984243 0.313542\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n```\n\n\uff08 3\u884c\u76ee \uff09\n\n```{bash:Terminal} \nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '3,3p' vectors.txt \nand -0.371746 -1.289212 -0.320571 -0.952901 0.559102 0.234208 -0.779906 1.527497 0.919236 -0.822447 -0.255847 1.571216 -0.830962 0.497985 -0.050695 0.297993 -1.027054 -1.488593 0.847045 0.021846 -0.474635 -1.270355 0.763695 0.648214 -1.056612 1.253085 0.127453 -0.174881 -0.144203 -0.497339 -0.785462 0.009430 0.461268 0.278945 -0.054788 0.098556 -0.965274 -1.312005 -0.370406 -0.697468 -0.909752 0.665602 0.172027 -1.104685 0.426531 -0.503397 -0.177057 -1.257854 1.528592 0.784191\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n####__vectors.txt \u3092 word2vec\u304b\u3089\u51fa\u529b\u3055\u308c\u308b\u5f62\u5f0f \u306e \u5358\u8a9e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb \u306b\u5909\u63db\u3057\u3066\u3001Python gensim \u306b\u98df\u308f\u305b\u3066\u307f\u308b__\n\n* [kanjirz50\u3055\u3093 Qiita\u8a18\u4e8b\uff082016/04/23\uff09 \u300cGloVe\u304c\u51fa\u529b\u3057\u305f\u30e2\u30c7\u30eb(txt)\u3092gensim.word2vec\u3067\u8aad\u307f\u8fbc\u3081\u308b\u3088\u3046\u306b\u6574\u5f62\u3059\u308b\u3002\u300d](http://qiita.com/kanjirz50/items/9d1c79d64dde46604395)\n\n> __word2vec\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8__\n>\n>\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n>\n>__1\u884c\u76ee\uff1a\u5358\u8a9e\u7a2e\u985e\u6570 \u6b21\u5143\u6570__\n>__2\u884c\u76ee\u4ee5\u964d\uff1a\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 ...__\n>\n>\u4e0a\u8a18\u306e\u3088\u3046\u306a\u30b7\u30f3\u30d7\u30eb\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u306a\u3063\u3066\u3044\u308b\u3002\n>\n\uff1e __GloVe\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8__\n>\n>\u533a\u5207\u308a\u6587\u5b57\u306f\u30b9\u30da\u30fc\u30b9\n>\n>__\u5358\u8a9e1 \u30d9\u30af\u30c8\u30eb1 \u30d9\u30af\u30c8\u30eb2 ...__\n>\n>\u4e0a\u8a18\u306e\u3088\u3046\u306b\u3001word2vec\u306e1\u884c\u76ee\u304c\u6b20\u3051\u3066\u3044\u308b\u3002\n>\u3064\u307e\u308a\u5358\u8a9e\u7a2e\u985e\u6570\u3068\u6b21\u5143\u6570\u30921\u884c\u76ee\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u5f62\u5f0f\u304c\u5909\u63db\u3067\u304d\u308b\u3002\n\n\u4e0a\u8a18\u3001kanjirx50\u3055\u3093 \u306e \u5909\u63db\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u305d\u306e\u307e\u307e\u62dd\u501f\uff08\u4ee5\u4e0b\uff09\u3057\u3066\u3001vectors.txt \u306b 1\u884c\u76ee \u3092 \u8ffd\u8a18\u3059\u308b__ \n\n```{Python:glove2word2vec_format.py}\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\n\ndef main():\n    argvs = sys.argv  # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u683c\u7d0d\u3057\u305f\u30ea\u30b9\u30c8\u306e\u53d6\u5f97\n    argc = len(argvs) # \u5f15\u6570\u306e\u500b\u6570\n\n    if (argc != 2):   # \u5f15\u6570\u304c\u8db3\u308a\u306a\u3044\u5834\u5408\u306f\u3001\u305d\u306e\u65e8\u3092\u8868\u793a\n        print('Usage: # python %s filename' % argvs[0])\n        quit()\n\n    line_count = 0 # \u884c\u6570\n    vector_size = 0 # \u6b21\u5143\u6570\n    # 1\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001\u884c\u6570\u3068\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n    with open(argvs[1], \"r\") as fin:\n        for line in fin:\n            line_count += 1\n        # \u6700\u5f8c\u306b\u6b21\u5143\u6570\u3092\u78ba\u8a8d\n        vector = line.rstrip().split(' ')\n        vector_size = len(vector) - 1\n    # 2\u5ea6\u76ee\u306e\u30d5\u30a1\u30a4\u30eb\u30a2\u30af\u30bb\u30b9\u3067\u3001word2vec\u5f62\u5f0f\u306e\u3082\u306e\u3092\u6a19\u6e96\u51fa\u529b\n    with open(argvs[1], \"r\") as fin:\n        print(line_count, vector_size)\n        for line in fin:\n            print(line, end=\"\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n* vectors.txt \u306e\u683c\u7d0d\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u306b glove2word2vec_format.py \u30b9\u30af\u30ea\u30d7\u30c8\u30d5\u30a1\u30a4\u30eb \u3092 \u914d\u7f6e\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ emacs glove2word2vec_format.py\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ ls\nLICENSE\t\t\t\tbuild\t\t\t\tdemo.sh\t\t\t\tsrc\t\t\t\tvectors.txt\nMakefile\t\t\tcooccurrence.bin\t\teval\t\t\t\ttext8\t\t\t\tvocab.txt\nREADME.md\t\t\tcooccurrence.shuf.bin\t\tglove2word2vec_format.py\tvectors.bin\nHirofumiYashima-no-MacBook:glove hirofumiyashima$\n```\n\n* \u5b9f\u884c\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ python glove2word2vec_format.py vectors.txt > word2vec_formatted_vectors.txt\n```\n\n* 1\u884c\u76ee \u304c \u8ffd\u8a18\u3055\u308c\u305f\n\n```{bash:Terminal}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ wc -l word2vec_formatted_vectors.txt \n   71292 word2vec_formatted_vectors.txt\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '1,1p' word2vec_formatted_vectors.txt \n71291 50\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '2,2p' word2vec_formatted_vectors.txt \nthe -1.379710 -1.462139 -0.282897 -0.863171 0.464334 0.318731 -0.893265 2.000415 0.353473 -1.233146 -0.223402 1.059758 -1.125059 0.767546 -0.983119 -0.456772 0.137045 -1.352054 0.125428 0.130168 -0.256981 -1.010887 -0.014780 0.616861 -0.240992 1.074628 -0.297288 -0.222200 0.127056 -0.914219 -0.629716 0.307570 -0.331981 0.799037 0.790429 0.203322 -1.355381 -1.428889 0.473966 -1.303657 -0.567251 0.961569 -0.013164 -1.152087 -1.055379 -1.303419 -0.918822 -1.171620 0.146521 0.503275\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \nHirofumiYashima-no-MacBook:glove hirofumiyashima$ sed -n '3,3p' word2vec_formatted_vectors.txt \nof -0.473577 -1.222222 -1.115821 -0.453052 0.219024 1.068070 -1.407289 2.534627 0.428462 -0.850468 0.128521 1.567760 -0.564698 0.859289 -0.633664 -0.552737 -0.425765 -1.330973 0.368091 0.927799 -0.793961 -0.898895 0.318299 0.341107 -0.882182 0.119855 0.285407 -1.103808 -0.656047 -0.928127 -0.675922 0.384731 0.087517 0.508578 -0.068728 -0.412215 -1.295717 -1.643162 0.978488 -0.403695 -0.324177 0.359110 0.283563 -1.574671 -0.516422 -0.965030 0.147162 -0.980163 0.984243 0.313542\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ \n```\n\n###__Python \u30a4\u30f3\u30bf\u30d7\u30ea\u30bf \u3092 \u8d77\u52d5\u3057\u3066\u3001gensim.models.Word2Vec.load_word2vec_format\u30e1\u30bd\u30c3\u30c9 \u3067 vectors.txt \u3092 \u30ed\u30fc\u30c9\u3059\u308b__\n\n```{Python:Python 3}\nHirofumiYashima-no-MacBook:glove hirofumiyashima$ python\nPython 3.5.2 (default, Jul 23 2016, 14:25:12) \n[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> from gensim.models import Word2Vec\n>>> \n>>> model = Word2Vec.load_word2vec_format('./word2vec_formatted_vectors.txt', binary=False) \n>>> \n>>> import types\n>>> print(type(model))\n<class 'gensim.models.word2vec.Word2Vec'>\n>>>\n```\n\n* _model_ \u306b \u5199\u50cf\u3055\u308c\u305f \u5358\u8a9e \u3092 \u78ba\u8a8d\n\n```{Python:Python 3}\n>>> vocab_list = list(model.vocab.keys())\n>>> print(len(vocab_list))\n71291\n>>> \n>>> from pprint import pprint\n>>> pprint(vocab_list[0:51])\n['electoral',\n 'dinosauria',\n 'voted',\n 'hogline',\n 'hf',\n 'entorhinal',\n 'kg',\n 'akihito',\n 'adds',\n 'microcosm',\n 'mammuthus',\n 'izabella',\n 'multicellular',\n 'canticle',\n 'shah',\n 'kyiv',\n 'fermionic',\n 'informers',\n 'chennai',\n 'encountering',\n 'albinus',\n 'beans',\n 'transmitters',\n 'expand',\n 'thermostat',\n 'semester',\n 'philospher',\n 'meyer',\n 'dimer',\n 'ogof',\n 'enable',\n 'scurvy',\n 'cinema',\n 'raporto',\n 'shipyards',\n 'maeshowe',\n 'theurgy',\n 'collet',\n 'kosovar',\n 'analytic',\n 'cabot',\n 'canalized',\n 'opacity',\n 'rutledge',\n 'clustering',\n 'hopes',\n 'felipe',\n 'giovani',\n 'socket',\n 'crooner',\n 'schiaparelli']\n>>>\n```\n\n####__word-similarity \u30bf\u30b9\u30af \u3092 \u5b9f\u884c__\n\n```{Python:Python 3}\n>>> print(model.similarity(\"cinema\", \"hopes\"))\n0.13977033022\n>>>\n```\n\n####__word-analogy \u30bf\u30b9\u30af \u3092 \u5b9f\u884c__\n\n```{Python:Python 3}\n>>> model.most_similar(positive=[\"cinema\"], negative=[\"hopes\"])\n[('opera', 0.5363454222679138), ('hindustani', 0.5091257095336914), ('fauvism', 0.5073018074035645), ('iptv', 0.49624091386795044), ('caldera', 0.4948546290397644), ('chinese', 0.4925414025783539), ('federico', 0.49221259355545044), ('pilsner', 0.49128100275993347), ('ballad', 0.4854658842086792), ('funniest', 0.4830397367477417)]\n>>> \n```\n\n###__\u5b9f\u884c\u6210\u529f__\n\n___\n\n##__\u3010 \u53c2\u8003 \u3011__\n\n* [yuku_t\u3055\u3093 Qiita\u8a18\u4e8b \uff082015/12/01\uff09 \u300c\u5358\u7d14\u306a\u5358\u8a9e\u306e\u30d9\u30af\u30c8\u30eb\u8868\u73fe: word2vec, GloVe\u300d](http://qiita.com/yuku_t/items/483b56be83a3a5423b09)\n* [DeepAge \u4eba\u5de5\u77e5\u80fd\u306e\u4eca\u3068\u4e00\u6b69\u5148\u3092\u767a\u4fe1\u3059\u308b\u30e1\u30c7\u30a3\u30a2 \uff082016-09-2\uff09 \u300cWord2Vec\uff1a\u767a\u660e\u3057\u305f\u672c\u4eba\u3082\u9a5a\u304f\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u9a5a\u7570\u7684\u306a\u529b\u300d](https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html#glove)\n\n> __Word2Vec\u306e\u6d3e\u751f\u7cfb\u3084\u985e\u4f3c\u30c4\u30fc\u30eb__\n>\n>__GloVe__\n>\n> Global Vectors for Word Representation\u306a\u7565\u3089\u3057\u3044\u3002\n> Word2Vec\u3088\u308a\u3082\u6027\u80fd\u304c\u9ad8\u3044\u3002 \u5b66\u7fd2\u304c\u901f\u304f\u3001\u5927\u304d\u306a\u30b3\u30fc\u30d1\u30b9\u3067\u3082\u5bfe\u5fdc\u51fa\u6765\u3001\u5c0f\u3055\u306a\u30b3\u30fc\u30d1\u30b9\u30fb\u5c0f\u3055\u306a\u30d9\u30af\u30c8\u30eb\u3067\u3082\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u51fa\u308b\u3002\n", "tags": ["Python", "NLP", "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "word2vec", "GloVe"]}