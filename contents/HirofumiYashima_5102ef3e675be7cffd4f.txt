{"context": "\u6df1\u304f\u8aad\u307f\u8fbc\u3080\u3079\u304d \u8ad6\u6587 \u306e \u512a\u5148\u9806\u4f4d \u3092\u4ed8\u3051\u308b\u4e00\u52a9\u3068\u3057\u3066\u3001\u53c2\u8003\u306b\u306a\u308b\u3002\n\n\n\uff08 GitHub \uff09github.com/arXivTimes/arXivTimes\n\n\n\uff08 \u6a5f\u68b0\u5b66\u7fd2 \u3084 \u6df1\u5c64\u5b66\u7fd2\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3001\u5f37\u5316\u5b66\u7fd2\u3001\u753b\u50cf\u51e6\u7406 \u306e\u30c8\u30d4\u30c3\u30af \u304c \u3042\u308b\uff09\n\n\n\u8ad6\u6587\u4e00\u8a00\u8981\u7d04 \u306e \u30ea\u30b9\u30c8\n\n\n\n\u8ad6\u6587\u4e00\u8a00\u8981\u7d04 \u306e \u4f8b\n\n\nhttps://github.com/arXivTimes/arXivTimes/issues/215\n\n\n\nFrustratingly Short Attention Spans in Neural Language Modeling #215\n\u4e00\u8a00\u3067\u3044\u3046\u3068\nAttention\u3092\u884c\u3046\u5834\u5408\u3001\u96a0\u308c\u5c64\u306e\u30d9\u30af\u30c8\u30eb\u306f\u6b21\u306e\u5358\u8a9e\u306e\u4e88\u6e2c\u30fbAttention\u306e\u7b97\u51fa\u30fb\u5c06\u6765\u306e\u5358\u8a9e\u306b\u6709\u7528\u306a\u60c5\u5831\u306e\u683c\u7d0d\u3001\u3068\u3044\u30463\u3064\u306e\u5f79\u5272\u3092\u62c5\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u3002\u306a\u306e\u3067\u51fa\u529b\u30923\u3064\u306b\u3057\u3066\u5f79\u5272\u5206\u62c5\u3055\u305b\u308b\u30a2\u30a4\u30c7\u30a2\u3002\u4f75\u305b\u3066\u3001\u5358\u7d14\u306b\u904e\u53bb\u306e\u96a0\u308c\u5c64\u3092\u7d50\u5408\u3057\u3066\u5165\u529b\u3059\u308b\u3060\u3051\u3067\u3082\u9ad8\u7cbe\u5ea6\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\n\u8ad6\u6587\u30ea\u30f3\u30af\nhttps://arxiv.org/abs/1702.04521\n\n\n\nhttps://github.com/arXivTimes/arXivTimes/issues/211\n\n\n\nEnergy Saving Additive Neural Network #211\n\u4e00\u8a00\u3067\u3044\u3046\u3068\n\u4e57\u7b97\u306b\u4ee3\u308f\u3063\u3066ef-operation\u3068\u3044\u3046\u3082\u306e\u3092\u5c0e\u5165\u3057\u3066\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u306e\u3088\u3044neural net\u3092\u4f5c\u3063\u305f\u3068\u3044\u3046\u8a71\u3002ef-operation\u3068\u306f\u7b26\u53f7\u306f\u666e\u901a\u306e\u4e57\u7b97\u3068\u540c\u3058\u3060\u3051\u3069\u3001\u7d76\u5bfe\u5024\u306f\u5358\u306a\u308b\u548c\u3068\u3044\u3046\u3082\u306e\u3002MNIST\u3068CIFAR\u3067\u5f93\u6765\u3068\u540c\u7b49\u306e\u7cbe\u5ea6\u3092\u51fa\u3057\u305f\u3068\u3044\u3046\u3002\n\u8ad6\u6587\u30ea\u30f3\u30af\nhttps://arxiv.org/abs/1702.02676\n\n\n\nhttps://github.com/arXivTimes/arXivTimes/issues/208\n\n\n\nStructured Attention Networks #208\n\u4e00\u8a00\u3067\u3044\u3046\u3068\nAttention\u306b\u3064\u3044\u3066\u3001\u5358\u7d14\u306b\u3069\u306e\u5730\u70b9\u306b\u6ce8\u76ee\u3059\u308b\u304b\u3068\u3044\u3046\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u7684\u306a\u8003\u3048\u3067\u306a\u304f\u3001\u898f\u5247\u6027(\u69cb\u9020)\u304c\u3042\u308b\u3068\u4eee\u5b9a\u3057\u3088\u3046\u3068\u3044\u3046\u63d0\u6848\u3002\u5177\u4f53\u7684\u306b\u306f\u3001CRF\u306e\u8003\u3048\u3092\u7528\u3044\u3066Attention\u306e\u5019\u88dc\u3068\u306a\u308b\u300c\u7cfb\u5217\u300d\u306b\u5bfe\u3057\u3066\u5168\u4f53\u3068\u3057\u3066\u6700\u9069\u306b\u306a\u308b\u3088\u3046\u306b\u8a08\u7b97\u3092\u884c\u3046\u3068\u3044\u3046\u8a71\u3002\u3053\u308c\u3067\u7cbe\u5ea6UP\u3092\u78ba\u8a8d\u3002\u305f\u3060\u3057\u3001\u8a08\u7b97\u6642\u9593\u304c\u9045\u304f\u306a\u308b\u3068\u3044\u3046\u30cf\u30f3\u30c7\u304c\u4f34\u3046\u306e\u3067\u6ce8\u610f\u3002\n\u8ad6\u6587\u30ea\u30f3\u30af\nhttps://arxiv.org/abs/1702.00887\n\n\n\nhttps://github.com/arXivTimes/arXivTimes/issues/207\n\n\n\nStacked Attention Networks for Image Question Answering #207\n\u4e00\u8a00\u3067\u3044\u3046\u3068\n\u753b\u50cf\u306b\u95a2\u3059\u308b\u8cea\u554f\u306b\u7b54\u3048\u308b\u7814\u7a76\u3002CNN\u3067\u753b\u50cf\u7279\u5fb4\u3001LSTM\u3067\u8cea\u554f\u30af\u30a8\u30ea\u3001\u4e8c\u3064\u5408\u308f\u305b\u3066\u56de\u7b54\u3059\u308b\u306e\u304c\u9244\u677f\u3060\u304c\u3001\u753b\u50cf\u5185\u306e\u7740\u76ee\u70b9\u3092\u3088\u308a\u306f\u3063\u304d\u308a\u3055\u305b\u308b\u305f\u3081\u3001\u753b\u50cf\u306e\u5404\u9818\u57df\u306b\u5bfe\u3059\u308b\u30af\u30a8\u30ea\u306eAttention\u3092\u53cd\u5fa9\u3057\u3066\u8a08\u7b97\u3059\u308b(Stacked Attention)\u624b\u6cd5\u3092\u63d0\u6848\u3002\u65e2\u5b58\u7cbe\u5ea6\u3092\u5927\u304d\u304f\u66f4\u65b0\u3002\n\u8ad6\u6587\u30ea\u30f3\u30af\nhttps://arxiv.org/abs/1511.02274\n\n\u6df1\u304f\u8aad\u307f\u8fbc\u3080\u3079\u304d \u8ad6\u6587 \u306e \u512a\u5148\u9806\u4f4d \u3092\u4ed8\u3051\u308b\u4e00\u52a9\u3068\u3057\u3066\u3001\u53c2\u8003\u306b\u306a\u308b\u3002\n\n___\n\n####__[\uff08 GitHub \uff09github.com/arXivTimes/arXivTimes](https://github.com/arXivTimes/arXivTimes)__\n\n<img width=\"1081\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 8.57.12.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/314c5bbb-6a56-f82e-6da4-e8145a5f20ab.png\">\n\n__\uff08 \u6a5f\u68b0\u5b66\u7fd2 \u3084 \u6df1\u5c64\u5b66\u7fd2\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3001\u5f37\u5316\u5b66\u7fd2\u3001\u753b\u50cf\u51e6\u7406 \u306e\u30c8\u30d4\u30c3\u30af \u304c \u3042\u308b\uff09__\n\n<img width=\"954\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 8.57.51.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/e06ea0dc-8a87-6483-ef1f-a521e2da0365.png\">\n\n###__\u8ad6\u6587\u4e00\u8a00\u8981\u7d04 \u306e \u30ea\u30b9\u30c8__\n\n<img width=\"1150\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 8.58.31.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/471ec942-6634-48f7-4c22-7604bb0ad84e.png\">\n\n\n###__\u8ad6\u6587\u4e00\u8a00\u8981\u7d04 \u306e \u4f8b__\n\n* https://github.com/arXivTimes/arXivTimes/issues/215\n\n<img width=\"1274\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 8.59.37.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/11fc6a67-139f-0b9f-8589-930bf4652151.png\">\n\n> __Frustratingly Short Attention Spans in Neural Language Modeling #215__\n>\n> __\u4e00\u8a00\u3067\u3044\u3046\u3068__\n>\n> Attention\u3092\u884c\u3046\u5834\u5408\u3001\u96a0\u308c\u5c64\u306e\u30d9\u30af\u30c8\u30eb\u306f\u6b21\u306e\u5358\u8a9e\u306e\u4e88\u6e2c\u30fbAttention\u306e\u7b97\u51fa\u30fb\u5c06\u6765\u306e\u5358\u8a9e\u306b\u6709\u7528\u306a\u60c5\u5831\u306e\u683c\u7d0d\u3001\u3068\u3044\u30463\u3064\u306e\u5f79\u5272\u3092\u62c5\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u3002\u306a\u306e\u3067\u51fa\u529b\u30923\u3064\u306b\u3057\u3066\u5f79\u5272\u5206\u62c5\u3055\u305b\u308b\u30a2\u30a4\u30c7\u30a2\u3002\u4f75\u305b\u3066\u3001\u5358\u7d14\u306b\u904e\u53bb\u306e\u96a0\u308c\u5c64\u3092\u7d50\u5408\u3057\u3066\u5165\u529b\u3059\u308b\u3060\u3051\u3067\u3082\u9ad8\u7cbe\u5ea6\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\n>\n> __\u8ad6\u6587\u30ea\u30f3\u30af__\n>\n>\n> https://arxiv.org/abs/1702.04521\n\n___\n\n* https://github.com/arXivTimes/arXivTimes/issues/211\n\n<img width=\"1280\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 9.06.40.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/7af7e7fb-638f-977c-e698-9f339019bff0.png\">\n\n> __Energy Saving Additive Neural Network #211__\n>\n> __\u4e00\u8a00\u3067\u3044\u3046\u3068__\n>\n> \u4e57\u7b97\u306b\u4ee3\u308f\u3063\u3066ef-operation\u3068\u3044\u3046\u3082\u306e\u3092\u5c0e\u5165\u3057\u3066\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u306e\u3088\u3044neural net\u3092\u4f5c\u3063\u305f\u3068\u3044\u3046\u8a71\u3002ef-operation\u3068\u306f\u7b26\u53f7\u306f\u666e\u901a\u306e\u4e57\u7b97\u3068\u540c\u3058\u3060\u3051\u3069\u3001\u7d76\u5bfe\u5024\u306f\u5358\u306a\u308b\u548c\u3068\u3044\u3046\u3082\u306e\u3002MNIST\u3068CIFAR\u3067\u5f93\u6765\u3068\u540c\u7b49\u306e\u7cbe\u5ea6\u3092\u51fa\u3057\u305f\u3068\u3044\u3046\u3002\n>\n> __\u8ad6\u6587\u30ea\u30f3\u30af__\n>\n>https://arxiv.org/abs/1702.02676\n\n\n\n___\n\n* https://github.com/arXivTimes/arXivTimes/issues/208\n\n<img width=\"1226\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 9.09.11.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/c28ab283-9dbd-3f4b-91e5-91ba87b8f87c.png\">\n\n> __Structured Attention Networks #208__\n>\n> __\u4e00\u8a00\u3067\u3044\u3046\u3068__\n>\n> Attention\u306b\u3064\u3044\u3066\u3001\u5358\u7d14\u306b\u3069\u306e\u5730\u70b9\u306b\u6ce8\u76ee\u3059\u308b\u304b\u3068\u3044\u3046\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u7684\u306a\u8003\u3048\u3067\u306a\u304f\u3001\u898f\u5247\u6027(\u69cb\u9020)\u304c\u3042\u308b\u3068\u4eee\u5b9a\u3057\u3088\u3046\u3068\u3044\u3046\u63d0\u6848\u3002\u5177\u4f53\u7684\u306b\u306f\u3001CRF\u306e\u8003\u3048\u3092\u7528\u3044\u3066Attention\u306e\u5019\u88dc\u3068\u306a\u308b\u300c\u7cfb\u5217\u300d\u306b\u5bfe\u3057\u3066\u5168\u4f53\u3068\u3057\u3066\u6700\u9069\u306b\u306a\u308b\u3088\u3046\u306b\u8a08\u7b97\u3092\u884c\u3046\u3068\u3044\u3046\u8a71\u3002\u3053\u308c\u3067\u7cbe\u5ea6UP\u3092\u78ba\u8a8d\u3002\u305f\u3060\u3057\u3001\u8a08\u7b97\u6642\u9593\u304c\u9045\u304f\u306a\u308b\u3068\u3044\u3046\u30cf\u30f3\u30c7\u304c\u4f34\u3046\u306e\u3067\u6ce8\u610f\u3002\n>\n> __\u8ad6\u6587\u30ea\u30f3\u30af__\n>\n>https://arxiv.org/abs/1702.00887\n\n___\n\n* https://github.com/arXivTimes/arXivTimes/issues/207\n\n<img width=\"1221\" alt=\"\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2017-02-18 9.12.14.png\" src=\"https://qiita-image-store.s3.amazonaws.com/0/43487/ea29c180-94b6-9ad0-2799-0449abc5c1c3.png\">\n\n>__Stacked Attention Networks for Image Question Answering #207__\n>\n> __\u4e00\u8a00\u3067\u3044\u3046\u3068__\n>\n> \u753b\u50cf\u306b\u95a2\u3059\u308b\u8cea\u554f\u306b\u7b54\u3048\u308b\u7814\u7a76\u3002CNN\u3067\u753b\u50cf\u7279\u5fb4\u3001LSTM\u3067\u8cea\u554f\u30af\u30a8\u30ea\u3001\u4e8c\u3064\u5408\u308f\u305b\u3066\u56de\u7b54\u3059\u308b\u306e\u304c\u9244\u677f\u3060\u304c\u3001\u753b\u50cf\u5185\u306e\u7740\u76ee\u70b9\u3092\u3088\u308a\u306f\u3063\u304d\u308a\u3055\u305b\u308b\u305f\u3081\u3001\u753b\u50cf\u306e\u5404\u9818\u57df\u306b\u5bfe\u3059\u308b\u30af\u30a8\u30ea\u306eAttention\u3092\u53cd\u5fa9\u3057\u3066\u8a08\u7b97\u3059\u308b(Stacked Attention)\u624b\u6cd5\u3092\u63d0\u6848\u3002\u65e2\u5b58\u7cbe\u5ea6\u3092\u5927\u304d\u304f\u66f4\u65b0\u3002\n\n> __\u8ad6\u6587\u30ea\u30f3\u30af__\n>\n>https://arxiv.org/abs/1511.02274\n\n", "tags": ["\u8ad6\u6587\u8aad\u307f", "MachineLearning", "NLP", "DeepLearning", "\u4eba\u5de5\u77e5\u80fd"]}