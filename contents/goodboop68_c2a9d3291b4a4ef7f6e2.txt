{"tags": ["Chainer", "Python", "RNN"], "context": "\n\n\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\n#!/usr/bin/env python\n\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, \\\n                        optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n\n\n\u30fb\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\u3001\u524d\u51e6\u7406\ndef load_data(filename):\n    global vocab\n    words = open(filename).read().replace('\\n', '<eos>').strip().split()\n    dataset = np.ndarray((len(words),), dtype=np.int32)\n    for i, word in enumerate(words):\n        if word not in vocab:\n            vocab[word] = len(vocab)\n        dataset[i] = vocab[word]\n    return dataset\n\ntrain_data = load_data(train_path)\neos_id = vocab['<eos>']\n\n\nwords :\u6587\u7ae0\u3092\u8aad\u307f\u8fbc\u307f\u3001\u6587\u672b\u306b<eos>\u30bf\u30b0\u3092\u8ffd\u52a0\u3001\u884c\u982d\u3068\u884c\u672b\u306e\u6539\u884c\u30b3\u30fc\u30c9\u3092\u524a\u9664\u3059\u308b\nvocab :\u5358\u8a9e\u3092\u91cd\u8907\u7121\u3057\u3067\u8ffd\u52a0\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4ed8\u3051\u3066\u4fdd\u5b58\ndataset :\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u3092\u914d\u5217\u3068\u3057\u3066\u4fdd\u5b58\n\n\n\u30fb\u30af\u30e9\u30b9\u5b9a\u7fa9\n\u30fb--init--\u90e8\nclass MyRNN(chainer.Chain):\n    def __init__(self, v, k):\n        super(MyRNN, self).__init__(\n            embed = L.EmbedID(v, k), # Word embedding layer\n            H  = L.Linear(k, k), # L1 layer\n            W = L.Linear(k, v), # L2 layer\n        )\n\n\u30fb--init--\u90e8\u306f\u3001\u30e2\u30c7\u30eb\u306e\u30bb\u30c3\u30c8\u6642\u306b\uff11\u5ea6\u3060\u3051\u7528\u3044\u3089\u308c\u308b\u3002\n\u30fbv,k\u3092\u5f15\u6570\u3068\u3057\u3066\u53d7\u53d6\u308a\u3001\u30ec\u30a4\u30e4\u3092\u5b9a\u7fa9\u3059\u308b\u3002\n\ninput layer : \u8a9e\u5f59\u6570\u3001len{vocab}\u3088\u308a\u3001439(?)\nhidden layer : demb\u3088\u308a100\noutput layer : input layer \u3068\u540c\u3058\n\n\n\u30fb__call__\u90e8\n    def __call__(self, s):\n        accum_loss = None\n        v, k = self.embed.W.data.shape \n        h = Variable(np.zeros((1,k), dtype=np.float32)) # define variable h [ it means x = Variable(x_data, dtype=np.float32) ] \n        for i in range(len(s)): # iterate word in sentenc s = [0,1,2,3] -> 4 iterate\n\n            next_w_id = eos_id if (i == len(s) - 1) else s[i+1] # if next word is in here, else next word is <eos> \n            tx = Variable(np.array([next_w_id], dtype=np.int32)) # tx is next word ID ( = ground truth data ) \n            x_k = self.embed(Variable(np.array([s[i]], dtype=np.int32))) #x_k ==Wx  \n            h = F.tanh(x_k + self.H(h)) # sekf.H(h) is W'h\n            loss = F.softmax_cross_entropy(self.W(h), tx)\n            accum_loss = loss if accum_loss is None else accum_loss + loss\n\n        return accum_loss, x_k, self.W(h)  \n\n\u30fb__call__\u90e8\u306f\u3001\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u5b8c\u4e86\u5f8c\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\nv,k: __init__\u3067\u5b9a\u7fa9\u3057\u305fembed\u30ec\u30a4\u30e4\u306e\u91cd\u307fW\u306e\u884c\u5217\u3092\u5b9a\u7fa9\u3002v:\u884c\u3001k:\u5217\u306e\u6b21\u5143\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u3002\nh:\u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u5b9a\u7fa9\u3001\u4e2d\u9593\u5c64\u306f\u4eca\u56de\u306f100\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u3044\u308b\u3002\ns\u306b\u306f\u53d7\u3051\u53d6\u3063\u305f\u6587\u7ae0\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u3001\u5358\u8a9e\u3054\u3068\u306b\u914d\u5217\u3068\u3057\u3066\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002\u3053\u308c\u3092for i in range(len(s)):\u30eb\u30fc\u30d7\u5185\u3067\u5358\u8a9e\u3054\u3068\u306b\u5206\u5272\u3057\u3001\u51e6\u7406\u3092\u884c\u3046\u3002\n\n\u30fbfor\u5185\u306e\u51e6\u7406\n\u30fbnext_w_id\u306bt+1t+1\u30b9\u30c6\u30c3\u30d7\u3067\u306e\u51fa\u73fe\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u3092\u683c\u7d0d\u3059\u308b\u3002\n\u3082\u3057\u6b21\u306e\u5358\u8a9e\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3001<eos>\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u683c\u7d0d\u3055\u308c\u308b\u3002\n\u305d\u306e\u5f8c\u3001tx\u3068\u3057\u3066\u5909\u6570\u5316\u3055\u308c\u308b\u3002x_k\u3067\u306f\u3001\u4eca\u30b9\u30c6\u30c3\u30d7\u3067\u53c2\u7167\u3057\u3066\u3044\u308b\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5909\u6570\u5316\u3057\u3001word embedding\u3092\u884c\u3046\u3002\n\n\u30fbword embedding\n\n\n\u8f9e\u66f8\u306e\u8a72\u5f53\u5358\u8a9eID\u3092\u53d6\u5f97\u3057\u3001\u305d\u306eID\u90e8\u5206\u306e\u307f\u304c1\u3001\u305d\u308c\u4ee5\u5916\u30920\u3068\u3059\u308b\u3088\u3046\u306a1-hot\u30d9\u30af\u30c8\u30ebxtx_t\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u307e\u305f\u3001\u30b3\u30fc\u30c9\u5185\u306eh (=hth_t)\u306f\u3001t\u22121t-1\u30b9\u30c6\u30c3\u30d7\u306eht\u22121h_{t-1}\u3068xkx_k\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u5f0f\u3067\u6c42\u307e\u308b\u3002\u52a0\u3048\u3066WembedW_{embed}\u3092\u7528\u3044\u3066xkx_k(=x_k)\u3092\u5f97\u308b\u3002xt:dim(xt)=len(vocab)x_t:dim(x_t)=len(vocab)\nxk=Wembed\u2217xk,\u3000dim(xk)=dembx_k = W_{embed} * x_k,\u3000dim(x_k)=dembht=tanh(Wembed\u2217xt+Wh\u2217ht\u22121)h_t=tanh(W_{embed} * x_t + W_h * h_{t-1})\n=tanh(xk+Wh\u2217ht\u22121)=tanh(x_k+ W_h*h_{t-1})\n\n\n\u30fbF.softmax_cross_entropy(self.W(h), tx)\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u306e\u5c24\u5ea6\u304c\u6c42\u307e\u308b\u3002\u8a73\u7d30\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\n\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc:neuralnet\u306e\u65e5\u8a18\n\u3000\n\n\u30fb\u30e1\u30e2\n\u89b3\u6e2c\u30c7\u30fc\u30bfself.W(h).data=zself.W(h).data=z,\u3000\u6559\u5e2b\u30c7\u30fc\u30bftx=tt_x=t\u3068\u3059\u308b\u3068\nz=[0.166479,0.060454,3.714621,...,\u22120.407060],\u3000sum(z)\u22601z=[0.166479, 0.060454, 3.714621, ...,  -0.407060],\u3000sum(z)\u2260 1y=softmax(z)=exp(zi)\u2211iexp(zi),\u3000sum(y)=1y =softmax(z) = \\frac{exp(z_i)}{\\sum_iexp(z_i)},\u3000sum(y)=1\u3067\u3042\u308a\u3001softmax()softmax()\u3092\u901a\u3059\u4e8b\u3067\u3001\u7dcf\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u3055\u308c\u308b\u3002\uff08\uff1d\u78ba\u7387\u3068\u307f\u306a\u305b\u308b\uff09\n\u4f46\u3057\u3001\u30af\u30e9\u30b9\u304c\u591a\u3044\u5834\u5408\u3001\u5024\u304c\u6975\u3081\u3066\u5c0f\u3055\u304f\u306a\u308a\u3001\u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc\u306e\u53ef\u80fd\u6027\u304c\u767a\u751f\u3059\u308b\u3002log\u3092\u901a\u3059\u4e8b\u3067\u3001\u5024\u3092\u305d\u3053\u305d\u3053\u5927\u304d\u304f\u53d6\u308b\u4e8b\u304c\u3067\u304d\u308b\uff08\uff1f\uff09\n\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u901a\u3059\u4e8b\u3067\u3001\u30e2\u30c7\u30eb\u306e\u640d\u5931\u3092\u6e2c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002t,yt,y\u306b\u3064\u3044\u3066t=[0,0,...,0,1,0,...,0],\u3000len(y)=len(t)=len(vocab)t=[0, 0, ..., 0, 1, 0, ... , 0],\u3000len(y)=len(t)=len(vocab)\ncrossentropy(y,t)=\u2212\u2211iti\u2217log2(yi)=\u2211itilog2(yi)crossentropy(y,t) = -\\sum_it_i*log_2(y_i)=\\sum_i\\frac{t_i}{log_2(y_i)}\n\u6559\u5e2b\u30c7\u30fc\u30bftt\u306f1-hot\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u304b\u3089\u3001\u6b63\u89e3\u30af\u30e9\u30b9ti=tkt_i=t_k\u4ee5\u5916\u306e\u5024\u306f0\u3068\u306a\u308b\u3002\u3088\u3063\u3066\u3001\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5909\u5f62\u53ef\u80fd\u3068\u306a\u308b\u3002crossentropy(y\u2032,t)=tklog2(y\u2032k)crossentropy(y',t)=\\frac{t_k}{log_2(y'_k)}\n\u30e2\u30c7\u30eb\u306e\u640d\u5931\u3092\u6587\u7ae0\u5358\u4f4d\u3067\u7d2f\u7a4d\u3057\u3001\u7d2f\u7a4d\u8aa4\u5deeaccum_loss\u3092\u8fd4\u3059\u3002\n\nmain\nfor epoch in range(1):\n    s = []    \n    for pos in range(25): # train_data = [0, 1, 2, 3, 4, 5, 6, 3, 7, 8, 9, 3]\n        id = train_data[pos]\n        s.append(id)        \n        if (id == eos_id):\n            ## s = [0,1,2,3] ( index '3' is <eos>  )\n\n            loss, embedlist,tmp = model(s) ## loss.data.size=1, embed.data.size=100\n\n            model.zerograds()\n            loss.backward()\n            optimizer.update()\n            s = []                \n        if (pos % 100 == 0):\n            print(pos, \"/\", len(train_data),\" finished\")\n    outfile = \"myrnn-\" + str(epoch) + \".model\"\n    serializers.save_npz(outfile, model)\n\nfor epoch in range(n): \u30a8\u30dd\u30c3\u30af\u306e\u7e70\u308a\u8fd4\u3057\u56de\u6570n\nfor pos in range(len(train_data)): \u6587\u7ae0\u306eword\u306e\u6570\u3060\u3051\u7e70\u308a\u8fd4\u3059\n\n\u30fb\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\n-\n\n``` python\n#!/usr/bin/env python\n\nimport numpy as np\nimport chainer\nfrom chainer import cuda, Function, gradient_check, Variable, \\\n                        optimizers, serializers, utils\nfrom chainer import Link, Chain, ChainList\nimport chainer.functions as F\nimport chainer.links as L\n\n```\n\n\u30fb\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\u3001\u524d\u51e6\u7406\n-\n\n\n``` python\ndef load_data(filename):\n    global vocab\n    words = open(filename).read().replace('\\n', '<eos>').strip().split()\n    dataset = np.ndarray((len(words),), dtype=np.int32)\n    for i, word in enumerate(words):\n        if word not in vocab:\n            vocab[word] = len(vocab)\n        dataset[i] = vocab[word]\n    return dataset\n```\n\n``` python\ntrain_data = load_data(train_path)\neos_id = vocab['<eos>']\n```\n\n  - words :\u6587\u7ae0\u3092\u8aad\u307f\u8fbc\u307f\u3001\u6587\u672b\u306b```<eos>```\u30bf\u30b0\u3092\u8ffd\u52a0\u3001\u884c\u982d\u3068\u884c\u672b\u306e\u6539\u884c\u30b3\u30fc\u30c9\u3092\u524a\u9664\u3059\u308b\n  - vocab :\u5358\u8a9e\u3092\u91cd\u8907\u7121\u3057\u3067\u8ffd\u52a0\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4ed8\u3051\u3066\u4fdd\u5b58\n  - dataset :\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u3092\u914d\u5217\u3068\u3057\u3066\u4fdd\u5b58\n\n\n\n\n\u30fb\u30af\u30e9\u30b9\u5b9a\u7fa9\n-\n\n\n\n\u30fb```--init--```\u90e8\n\n``` python\nclass MyRNN(chainer.Chain):\n    def __init__(self, v, k):\n        super(MyRNN, self).__init__(\n            embed = L.EmbedID(v, k), # Word embedding layer\n            H  = L.Linear(k, k), # L1 layer\n            W = L.Linear(k, v), # L2 layer\n        )\n```\n\n\u30fb```--init--```\u90e8\u306f\u3001\u30e2\u30c7\u30eb\u306e\u30bb\u30c3\u30c8\u6642\u306b\uff11\u5ea6\u3060\u3051\u7528\u3044\u3089\u308c\u308b\u3002\n\u30fb```v```,```k```\u3092\u5f15\u6570\u3068\u3057\u3066\u53d7\u53d6\u308a\u3001\u30ec\u30a4\u30e4\u3092\u5b9a\u7fa9\u3059\u308b\u3002\n\n - input layer : \u8a9e\u5f59\u6570\u3001```len{vocab}```\u3088\u308a\u3001439(?)\n - hidden layer : ```demb```\u3088\u308a100\n - output layer : input layer \u3068\u540c\u3058\n\n\n \n\u30fb```__call__```\u90e8\n-\n\n``` python\n\tdef __call__(self, s):\n        accum_loss = None\n        v, k = self.embed.W.data.shape \n        h = Variable(np.zeros((1,k), dtype=np.float32)) # define variable h [ it means x = Variable(x_data, dtype=np.float32) ] \n        for i in range(len(s)): # iterate word in sentenc s = [0,1,2,3] -> 4 iterate\n        \n            next_w_id = eos_id if (i == len(s) - 1) else s[i+1] # if next word is in here, else next word is <eos> \n            tx = Variable(np.array([next_w_id], dtype=np.int32)) # tx is next word ID ( = ground truth data ) \n            x_k = self.embed(Variable(np.array([s[i]], dtype=np.int32))) #x_k ==Wx  \n            h = F.tanh(x_k + self.H(h)) # sekf.H(h) is W'h\n            loss = F.softmax_cross_entropy(self.W(h), tx)\n            accum_loss = loss if accum_loss is None else accum_loss + loss\n            \n        return accum_loss, x_k, self.W(h)  \n```\n\n\n\u30fb```__call__```\u90e8\u306f\u3001\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\u5b8c\u4e86\u5f8c\u306b\u5b9f\u884c\u3055\u308c\u308b\u3002\n\nv,k: ```__init__```\u3067\u5b9a\u7fa9\u3057\u305fembed\u30ec\u30a4\u30e4\u306e\u91cd\u307fW\u306e\u884c\u5217\u3092\u5b9a\u7fa9\u3002v:\u884c\u3001k:\u5217\u306e\u6b21\u5143\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u3002\nh:\u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u5b9a\u7fa9\u3001\u4e2d\u9593\u5c64\u306f\u4eca\u56de\u306f100\u30e6\u30cb\u30c3\u30c8\u3068\u3057\u3066\u3044\u308b\u3002\n```s```\u306b\u306f\u53d7\u3051\u53d6\u3063\u305f\u6587\u7ae0\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u3001\u5358\u8a9e\u3054\u3068\u306b\u914d\u5217\u3068\u3057\u3066\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002\u3053\u308c\u3092```for i in range(len(s)):```\u30eb\u30fc\u30d7\u5185\u3067\u5358\u8a9e\u3054\u3068\u306b\u5206\u5272\u3057\u3001\u51e6\u7406\u3092\u884c\u3046\u3002\n\n####\u30fbfor\u5185\u306e\u51e6\u7406\n\n\u30fb```next_w_id```\u306b$t+1$\u30b9\u30c6\u30c3\u30d7\u3067\u306e\u51fa\u73fe\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u756a\u53f7\u3092\u683c\u7d0d\u3059\u308b\u3002\n\u3082\u3057\u6b21\u306e\u5358\u8a9e\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u3001```<eos>```\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u683c\u7d0d\u3055\u308c\u308b\u3002\n\u305d\u306e\u5f8c\u3001```tx```\u3068\u3057\u3066\u5909\u6570\u5316\u3055\u308c\u308b\u3002```x_k```\u3067\u306f\u3001\u4eca\u30b9\u30c6\u30c3\u30d7\u3067\u53c2\u7167\u3057\u3066\u3044\u308b\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5909\u6570\u5316\u3057\u3001word embedding\u3092\u884c\u3046\u3002\n\n####\u30fbword embedding\n\n>\n - \u8f9e\u66f8\u306e\u8a72\u5f53\u5358\u8a9eID\u3092\u53d6\u5f97\u3057\u3001\u305d\u306eID\u90e8\u5206\u306e\u307f\u304c```1```\u3001\u305d\u308c\u4ee5\u5916\u3092```0```\u3068\u3059\u308b\u3088\u3046\u306a1-hot\u30d9\u30af\u30c8\u30eb$x_t$\u3092\u4f5c\u6210\u3059\u308b\u3002\n - \u307e\u305f\u3001\u30b3\u30fc\u30c9\u5185\u306eh (=$h_t$)\u306f\u3001$t-1$\u30b9\u30c6\u30c3\u30d7\u306e$h_{t-1}$\u3068$x_k$\u3092\u7528\u3044\u3066\u4ee5\u4e0b\u306e\u5f0f\u3067\u6c42\u307e\u308b\u3002\u52a0\u3048\u3066$W_{embed}$\u3092\u7528\u3044\u3066$x_k$(=```x_k```)\u3092\u5f97\u308b\u3002$$x_t:dim(x_t)=len(vocab)$$\n - $$x_k = W_{embed} * x_k,\u3000dim(x_k)=demb$$$$h_t=tanh(W_{embed} * x_t + W_h * h_{t-1})$$\n - $$=tanh(x_k+ W_h*h_{t-1})$$\n\n\n\n\u30fb```F.softmax_cross_entropy(self.W(h), tx)```\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u306e\u5c24\u5ea6\u304c\u6c42\u307e\u308b\u3002\u8a73\u7d30\u306f\u4ee5\u4e0b\u3092\u53c2\u7167\n[\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc:neuralnet\u306e\u65e5\u8a18](http://neuralnet.hatenablog.jp/entry/2016/05/17/010816)\n\u3000\n####\u30fb\u30e1\u30e2\n\u89b3\u6e2c\u30c7\u30fc\u30bf$self.W(h).data=z$,\u3000\u6559\u5e2b\u30c7\u30fc\u30bf$t_x=t$\u3068\u3059\u308b\u3068\n$$z=[0.166479, 0.060454, 3.714621, ...,  -0.407060],\u3000sum(z)\u2260 1$$$$y =softmax(z) = \\frac{exp(z_i)}{\\sum_iexp(z_i)},\u3000sum(y)=1$$\u3067\u3042\u308a\u3001$softmax()$\u3092\u901a\u3059\u4e8b\u3067\u3001\u7dcf\u548c\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u898f\u5316\u3055\u308c\u308b\u3002\uff08\uff1d\u78ba\u7387\u3068\u307f\u306a\u305b\u308b\uff09\n\u4f46\u3057\u3001\u30af\u30e9\u30b9\u304c\u591a\u3044\u5834\u5408\u3001\u5024\u304c\u6975\u3081\u3066\u5c0f\u3055\u304f\u306a\u308a\u3001\u30a2\u30f3\u30c0\u30fc\u30d5\u30ed\u30fc\u306e\u53ef\u80fd\u6027\u304c\u767a\u751f\u3059\u308b\u3002log\u3092\u901a\u3059\u4e8b\u3067\u3001\u5024\u3092\u305d\u3053\u305d\u3053\u5927\u304d\u304f\u53d6\u308b\u4e8b\u304c\u3067\u304d\u308b\uff08\uff1f\uff09\n\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u901a\u3059\u4e8b\u3067\u3001\u30e2\u30c7\u30eb\u306e\u640d\u5931\u3092\u6e2c\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002$t,y$\u306b\u3064\u3044\u3066$$t=[0, 0, ..., 0, 1, 0, ... , 0],\u3000len(y)=len(t)=len(vocab)$$\n$$crossentropy(y,t) = -\\sum_it_i*log_2(y_i)=\\sum_i\\frac{t_i}{log_2(y_i)}$$\n\n\u6559\u5e2b\u30c7\u30fc\u30bf$t$\u306f1-hot\u30d9\u30af\u30c8\u30eb\u3067\u3042\u308b\u304b\u3089\u3001\u6b63\u89e3\u30af\u30e9\u30b9$t_i=t_k$\u4ee5\u5916\u306e\u5024\u306f0\u3068\u306a\u308b\u3002\u3088\u3063\u3066\u3001\u5f0f\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5909\u5f62\u53ef\u80fd\u3068\u306a\u308b\u3002$$crossentropy(y',t)=\\frac{t_k}{log_2(y'_k)}$$\n\n\n\u30e2\u30c7\u30eb\u306e\u640d\u5931\u3092\u6587\u7ae0\u5358\u4f4d\u3067\u7d2f\u7a4d\u3057\u3001\u7d2f\u7a4d\u8aa4\u5dee```accum_loss```\u3092\u8fd4\u3059\u3002\n\nmain\n=\n\n\n``` python\nfor epoch in range(1):\n    s = []    \n    for pos in range(25): # train_data = [0, 1, 2, 3, 4, 5, 6, 3, 7, 8, 9, 3]\n        id = train_data[pos]\n        s.append(id)        \n        if (id == eos_id):\n            ## s = [0,1,2,3] ( index '3' is <eos>  )\n            \n            loss, embedlist,tmp = model(s) ## loss.data.size=1, embed.data.size=100\n            \n            model.zerograds()\n            loss.backward()\n            optimizer.update()\n            s = []                \n        if (pos % 100 == 0):\n            print(pos, \"/\", len(train_data),\" finished\")\n    outfile = \"myrnn-\" + str(epoch) + \".model\"\n    serializers.save_npz(outfile, model)\n```\n\n```for epoch in range(n)```: \u30a8\u30dd\u30c3\u30af\u306e\u7e70\u308a\u8fd4\u3057\u56de\u6570n\n```for pos in range(len(train_data))```: \u6587\u7ae0\u306eword\u306e\u6570\u3060\u3051\u7e70\u308a\u8fd4\u3059\n"}