{"context": " More than 1 year has passed since last update.Spark\u306epython\u7248DataFrame\u306eWindow\u95a2\u6570\u3092\u4f7f\u3063\u3066\u3001\u30ab\u30e9\u30e0\u3092\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0&\u30bd\u30fc\u30c8\u3057\u3064\u3064\u3001\u7d2f\u7a4d\u548c\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3067\u3059\u3002\n\u516c\u5f0f\u306ePython API\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8abf\u3079\u306a\u304c\u3089\u6a21\u7d22\u3057\u305f\u65b9\u6cd5\u306a\u306e\u3067\u3001\u3082\u3063\u3068\u826f\u3044\u65b9\u6cd5\u304c\u3042\u308b\u304b\u3082\u3002\n\u4f7f\u3063\u305fSpark\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.5.2\u3067\u3059\u3002\nhttp://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n\n\u30b5\u30f3\u30d7\u30eb\u30fb\u30c7\u30fc\u30bf\nPostgreSQL\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u3001pyspark\u306bDataFrame\u3068\u3057\u3066\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n$ SPARK_CLASSPATH=postgresql-9.4-1202.jdbc41.jar PYSPARK_DRIVER_PYTHON=ipython pyspark\n(..snip..)\nIn [1]: df = sqlContext.read.format('jdbc').options(url='jdbc:postgresql://localhost:5432/postgres?user=postgres', dbtable='public.foo').load()\n(..snip..)\nIn [2]: df.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\nIn [4]: df.show()\n+---+--------------------+---+\n|  a|                   b|  c|\n+---+--------------------+---+\n|  1|2015-11-22 10:00:...|  1|\n|  1|2015-11-22 10:10:...|  2|\n|  1|2015-11-22 10:20:...|  3|\n|  1|2015-11-22 10:30:...|  4|\n|  1|2015-11-22 10:40:...|  5|\n|  1|2015-11-22 10:50:...|  6|\n|  1|2015-11-22 11:00:...|  7|\n|  1|2015-11-22 11:10:...|  8|\n|  1|2015-11-22 11:20:...|  9|\n|  1|2015-11-22 11:30:...| 10|\n|  1|2015-11-22 11:40:...| 11|\n|  1|2015-11-22 11:50:...| 12|\n|  1|2015-11-22 12:00:...| 13|\n|  2|2015-11-22 10:00:...|  1|\n|  2|2015-11-22 10:10:...|  2|\n|  2|2015-11-22 10:20:...|  3|\n|  2|2015-11-22 10:30:...|  4|\n|  2|2015-11-22 10:40:...|  5|\n|  2|2015-11-22 10:50:...|  6|\n|  2|2015-11-22 11:00:...|  7|\n+---+--------------------+---+\nonly showing top 20 rows\n\n\u30ab\u30e9\u30e0a\u304c\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0\u7528\u3001\u30ab\u30e9\u30e0b\u304c\u30bd\u30fc\u30c8\u7528\u3001\u30ab\u30e9\u30e0c\u304c\u8a08\u7b97\u5bfe\u8c61\u3067\u3059\u3002\n\n\u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7d2f\u7a4d\u548c\n\u30ab\u30e9\u30e0a\u3067\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3064\u3064\u3001\u30ab\u30e9\u30e0b\u3067\u30bd\u30fc\u30c8\u3057\u3001\u30ab\u30e9\u30e0c\u306e\u7d2f\u7a4d\u548c\u3092\u53d6\u308a\u307e\u3059\u3002\n\u307e\u305a\u306fWindow\u306e\u5b9a\u7fa9\nIn [6]: from pyspark.sql.Window import Window\n\nIn [7]: from pyspark.sql import functions as func\n\nIn [8]: window = Window.partitionpartitionBy(df.a).orderBy(df.b).rangeBetween(-sys.maxsize,0)\n\nIn [9]: window\nOut[9]: <pyspark.sql.window.WindowSpec at 0x18368d0>\n\n\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4e0a\u3067pyspark.sql.functions.sum()\u3092\u8a08\u7b97\u3057\u305fColumn\u3092\u4f5c\u6210\nIn [10]: cum_c = func.sum(df.c).over(window)\n\nIn [11]: cum_c\nOut[11]: Column<'sum(c) WindowSpecDefinition UnspecifiedFrame>\n\n\u3053\u306eColumn\u3092\u5143\u306eDataFrame\u306b\u304f\u3063\u3064\u3051\u305f\u65b0\u3057\u3044DataFrame\u3092\u4f5c\u6210\nIn [12]: mod_df = df.withColumn(\"cum_c\", cum_c)\n\nIn [13]: mod_df\nOut[13]: DataFrame[a: int, b: timestamp, c: int, cum_c: bigint]\n\nIn [14]: mod_df.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\n |-- cum_c: long (nullable = true)\n\n\nIn [15]: mod_df.show()\n+---+--------------------+---+-----+\n|  a|                   b|  c|cum_c|\n+---+--------------------+---+-----+\n|  1|2015-11-22 10:00:...|  1|    1|\n|  1|2015-11-22 10:10:...|  2|    3|\n|  1|2015-11-22 10:20:...|  3|    6|\n|  1|2015-11-22 10:30:...|  4|   10|\n|  1|2015-11-22 10:40:...|  5|   15|\n|  1|2015-11-22 10:50:...|  6|   21|\n|  1|2015-11-22 11:00:...|  7|   28|\n|  1|2015-11-22 11:10:...|  8|   36|\n|  1|2015-11-22 11:20:...|  9|   45|\n|  1|2015-11-22 11:30:...| 10|   55|\n|  1|2015-11-22 11:40:...| 11|   66|\n|  1|2015-11-22 11:50:...| 12|   78|\n|  1|2015-11-22 12:00:...| 13|   91|\n|  2|2015-11-22 10:00:...|  1|    1|\n|  2|2015-11-22 10:10:...|  2|    3|\n|  2|2015-11-22 10:20:...|  3|    6|\n|  2|2015-11-22 10:30:...|  4|   10|\n|  2|2015-11-22 10:40:...|  5|   15|\n|  2|2015-11-22 10:50:...|  6|   21|\n|  2|2015-11-22 11:00:...|  7|   28|\n+---+--------------------+---+-----+\nonly showing top 20 rows\n\n\u8a08\u7b97\u3067\u304d\u3066\u3044\u307e\u3059\u306d\u3002\n\n\u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7dcf\u548c\n\u4eca\u5ea6\u306f\u3001\u30ab\u30e9\u30e0a\u306e\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306b\u3001\u30ab\u30e9\u30e0c\u306e\u7dcf\u548c\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\nDataFrame\u3092groupBy()\u3067pyspark.sql.GroupedData\u306b\u3057\u3066\u3001pyspark.sql.GroupedData.sum()\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3055\u3063\u304d\u306esum()\u3068\u3084\u3084\u3053\u3057\u3044\u3051\u3069\u3001\u3053\u3061\u3089\u306fColumn\u30aa\u30d7\u30b8\u30e7\u30af\u30c8\u3092\u5f15\u6570\u306b\u6301\u305f\u3059\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u6ce8\u610f\u3057\u307e\u3059\u3002\nIn [25]: sum_c_df = df.groupBy('a').sum('c')\n\n\u307e\u305f\u3001\u5148\u307b\u3069\u3068\u9055\u3063\u3066\u3053\u308c\u306fWindow\u95a2\u6570\u3067\u306f\u306a\u3044\u306e\u3067\u3001\u8fd4\u3063\u3066\u304f\u308b\u7d50\u679c\u306fDataFrame\u3067\u3059\u3002\n\u3057\u304b\u3082\u3001\u7dcf\u548c\u3092\u683c\u7d0d\u3057\u305f\u30ab\u30e9\u30e0\u540d\u306f\u52dd\u624b\u306b\u6c7a\u307e\u308a\u307e\u3059\u3002\nIn [26]: sum_c_df\nOut[26]: DataFrame[a: int, sum(c): bigint]\n\n\u3046\u30fc\u3093\u3001\u3084\u3084\u3053\u3057\u3044\u3002\n\u3068\u308a\u3042\u3048\u305a\u3001\u5143\u306eDataFrame\u306b\u30ab\u30e9\u30e0\u3068\u3057\u3066\u304f\u3063\u3064\u3051\u307e\u3059\u3002\nIn [27]: mod_df3 = mod_df2.join('a'sum_c_df, 'a'()\n\nIn [28]: mod_df3.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\n |-- cum_c: long (nullable = true)\n |-- sum(c): long (nullable = true)\n\n\nIn [29]: mod_df3.show()\n(..snip..)\n+---+--------------------+---+-------+------+\n|  a|                   b|  c|  cum_c|sum(c)|\n+---+--------------------+---+-------+------+\n|  1|2015-11-22 10:00:...|  1|      1|    91|\n|  1|2015-11-22 10:10:...|  2|      3|    91|\n|  1|2015-11-22 10:20:...|  3|      6|    91|\n|  1|2015-11-22 10:30:...|  4|     10|    91|\n|  1|2015-11-22 10:40:...|  5|     15|    91|\n|  1|2015-11-22 10:50:...|  6|     21|    91|\n|  1|2015-11-22 11:00:...|  7|     28|    91|\n|  1|2015-11-22 11:10:...|  8|     36|    91|\n|  1|2015-11-22 11:20:...|  9|     45|    91|\n|  1|2015-11-22 11:30:...| 10|     55|    91|\n|  1|2015-11-22 11:40:...| 11|     66|    91|\n|  1|2015-11-22 11:50:...| 12|     78|    91|\n|  1|2015-11-22 12:00:...| 13|     91|    91|\n|  2|2015-11-22 10:00:...|  1|      1|    91|\n|  2|2015-11-22 10:10:...|  2|      3|    91|\n|  2|2015-11-22 10:20:...|  3|      6|    91|\n|  2|2015-11-22 10:30:...|  4|     10|    91|\n|  2|2015-11-22 10:40:...|  5|     15|    91|\n|  2|2015-11-22 10:50:...|  6|     21|    91|\n|  2|2015-11-22 11:00:...|  7|     28|    91|\n+---+--------------------+---+-------+------+\nonly showing top 20 rows\n\n\u3046\u307e\u304f\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7dcf\u548c\u304c\u8a08\u7b97\u3067\u304d\u3066\u3044\u307e\u3059\u306d\u3002\n\n\u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e(\u7dcf\u548c - \u7d2f\u7a4d\u548c)\n\u3067\u306f\u3001\u30ab\u30e9\u30e0c\u306b\u3064\u3044\u3066\u7dcf\u548c\u307e\u3067\u306e\u6b8b\u308a\u5024\u3092\u8a08\u7b97\u3057\u307e\u3057\u3087\u3046\u3002\u3064\u307e\u308a\u3001\u7dcf\u548c - \u7d2f\u7a4d\u548c\u3067\u3059\u3002\nIn [30]: diff_sum_c = mod_df3[('sum(c)'] - mod_df3['cum_c']\n\nIn [31]: mod_df4 = mod_df3.withColumn(\"diff_sum_c\", diff_sum_c)\n\nIn [34]: mod_df4.show()\n(..snip..)\n+---+--------------------+---+-------+------+----------+\n|  a|                   b|  c|cum_c_2|sum(c)|diff_sum_c|\n+---+--------------------+---+-------+------+----------+\n|  1|2015-11-22 10:00:...|  1|      1|    91|        90|\n|  1|2015-11-22 10:10:...|  2|      3|    91|        88|\n|  1|2015-11-22 10:20:...|  3|      6|    91|        85|\n|  1|2015-11-22 10:30:...|  4|     10|    91|        81|\n|  1|2015-11-22 10:40:...|  5|     15|    91|        76|\n|  1|2015-11-22 10:50:...|  6|     21|    91|        70|\n|  1|2015-11-22 11:00:...|  7|     28|    91|        63|\n|  1|2015-11-22 11:10:...|  8|     36|    91|        55|\n|  1|2015-11-22 11:20:...|  9|     45|    91|        46|\n|  1|2015-11-22 11:30:...| 10|     55|    91|        36|\n|  1|2015-11-22 11:40:...| 11|     66|    91|        25|\n|  1|2015-11-22 11:50:...| 12|     78|    91|        13|\n|  1|2015-11-22 12:00:...| 13|     91|    91|         0|\n|  2|2015-11-22 10:00:...|  1|      1|    91|        90|\n|  2|2015-11-22 10:10:...|  2|      3|    91|        88|\n|  2|2015-11-22 10:20:...|  3|      6|    91|        85|\n|  2|2015-11-22 10:30:...|  4|     10|    91|        81|\n|  2|2015-11-22 10:40:...|  5|     15|    91|        76|\n|  2|2015-11-22 10:50:...|  6|     21|    91|        70|\n|  2|2015-11-22 11:00:...|  7|     28|    91|        63|\n+---+--------------------+---+-------+------+----------+\nonly showing top 20 rows\n\n\n\u88dc\u8db3\n\u4eca\u56de\u6c17\u4ed8\u304d\u307e\u3057\u305f\u304c\u3001SPARK_CLASSPATH\u3092\u4f7f\u3046\u306e\u306fSpark 1.0\u4ee5\u4e0a\u3067\u306f\u63a8\u5968\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u3002\npyspark\u8d77\u52d5\u6642\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u307e\u3057\u305f\u3002\n15/11/22 12:32:44 WARN spark.SparkConf: \nSPARK_CLASSPATH was detected (set to 'postgresql-9.4-1202.jdbc41.jar').\nThis is deprecated in Spark 1.0+.\n\nPlease instead use:\n - ./spark-submit with --driver-class-path to augment the driver classpath\n - spark.executor.extraClassPath to augment the executor classpath\n\n\n\u3069\u3046\u3082\u3001\u30af\u30e9\u30b9\u30bf\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u306b\u306f\u7570\u306a\u308b\u30b5\u30fc\u30d0\u3067\u3053\u306e\u74b0\u5883\u5909\u6570\u304c\u6b63\u3057\u304f\u4f1d\u308f\u3089\u306a\u3044\u305f\u3081\u3001\u5225\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u3046\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\u3046\u3080\u3080\u3002\n\u3053\u3046\u3044\u3046\u30ed\u30fc\u30ab\u30eb\u3068\u5206\u6563\u74b0\u5883\u306e\u9055\u3044\u3001\u304d\u3061\u3093\u3068\u628a\u63e1\u3057\u3066\u3044\u304b\u306a\u3044\u3068\u306a\u3041\u3002\nSpark\u306epython\u7248DataFrame\u306eWindow\u95a2\u6570\u3092\u4f7f\u3063\u3066\u3001\u30ab\u30e9\u30e0\u3092\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0&\u30bd\u30fc\u30c8\u3057\u3064\u3064\u3001\u7d2f\u7a4d\u548c\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u65b9\u6cd5\u3067\u3059\u3002\n\n\u516c\u5f0f\u306ePython API\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8abf\u3079\u306a\u304c\u3089\u6a21\u7d22\u3057\u305f\u65b9\u6cd5\u306a\u306e\u3067\u3001\u3082\u3063\u3068\u826f\u3044\u65b9\u6cd5\u304c\u3042\u308b\u304b\u3082\u3002\n\u4f7f\u3063\u305fSpark\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f1.5.2\u3067\u3059\u3002\n\nhttp://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n\n\n# \u30b5\u30f3\u30d7\u30eb\u30fb\u30c7\u30fc\u30bf\n\nPostgreSQL\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3057\u3001pyspark\u306bDataFrame\u3068\u3057\u3066\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\n\n```\n$ SPARK_CLASSPATH=postgresql-9.4-1202.jdbc41.jar PYSPARK_DRIVER_PYTHON=ipython pyspark\n(..snip..)\nIn [1]: df = sqlContext.read.format('jdbc').options(url='jdbc:postgresql://localhost:5432/postgres?user=postgres', dbtable='public.foo').load()\n(..snip..)\nIn [2]: df.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\nIn [4]: df.show()\n+---+--------------------+---+\n|  a|                   b|  c|\n+---+--------------------+---+\n|  1|2015-11-22 10:00:...|  1|\n|  1|2015-11-22 10:10:...|  2|\n|  1|2015-11-22 10:20:...|  3|\n|  1|2015-11-22 10:30:...|  4|\n|  1|2015-11-22 10:40:...|  5|\n|  1|2015-11-22 10:50:...|  6|\n|  1|2015-11-22 11:00:...|  7|\n|  1|2015-11-22 11:10:...|  8|\n|  1|2015-11-22 11:20:...|  9|\n|  1|2015-11-22 11:30:...| 10|\n|  1|2015-11-22 11:40:...| 11|\n|  1|2015-11-22 11:50:...| 12|\n|  1|2015-11-22 12:00:...| 13|\n|  2|2015-11-22 10:00:...|  1|\n|  2|2015-11-22 10:10:...|  2|\n|  2|2015-11-22 10:20:...|  3|\n|  2|2015-11-22 10:30:...|  4|\n|  2|2015-11-22 10:40:...|  5|\n|  2|2015-11-22 10:50:...|  6|\n|  2|2015-11-22 11:00:...|  7|\n+---+--------------------+---+\nonly showing top 20 rows\n```\n\n\u30ab\u30e9\u30e0a\u304c\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0\u7528\u3001\u30ab\u30e9\u30e0b\u304c\u30bd\u30fc\u30c8\u7528\u3001\u30ab\u30e9\u30e0c\u304c\u8a08\u7b97\u5bfe\u8c61\u3067\u3059\u3002\n\n\n# \u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7d2f\u7a4d\u548c\n\n\u30ab\u30e9\u30e0a\u3067\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3064\u3064\u3001\u30ab\u30e9\u30e0b\u3067\u30bd\u30fc\u30c8\u3057\u3001\u30ab\u30e9\u30e0c\u306e\u7d2f\u7a4d\u548c\u3092\u53d6\u308a\u307e\u3059\u3002\n\n\u307e\u305a\u306fWindow\u306e\u5b9a\u7fa9\n\n```\nIn [6]: from pyspark.sql.Window import Window\n\nIn [7]: from pyspark.sql import functions as func\n\nIn [8]: window = Window.partitionpartitionBy(df.a).orderBy(df.b).rangeBetween(-sys.maxsize,0)\n\nIn [9]: window\nOut[9]: <pyspark.sql.window.WindowSpec at 0x18368d0>\n```\n\n\u3053\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u4e0a\u3067pyspark.sql.functions.sum()\u3092\u8a08\u7b97\u3057\u305fColumn\u3092\u4f5c\u6210\n\n```\nIn [10]: cum_c = func.sum(df.c).over(window)\n\nIn [11]: cum_c\nOut[11]: Column<'sum(c) WindowSpecDefinition UnspecifiedFrame>\n```\n\n\u3053\u306eColumn\u3092\u5143\u306eDataFrame\u306b\u304f\u3063\u3064\u3051\u305f\u65b0\u3057\u3044DataFrame\u3092\u4f5c\u6210\n\n```\nIn [12]: mod_df = df.withColumn(\"cum_c\", cum_c)\n\nIn [13]: mod_df\nOut[13]: DataFrame[a: int, b: timestamp, c: int, cum_c: bigint]\n\nIn [14]: mod_df.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\n |-- cum_c: long (nullable = true)\n\n\nIn [15]: mod_df.show()\n+---+--------------------+---+-----+\n|  a|                   b|  c|cum_c|\n+---+--------------------+---+-----+\n|  1|2015-11-22 10:00:...|  1|    1|\n|  1|2015-11-22 10:10:...|  2|    3|\n|  1|2015-11-22 10:20:...|  3|    6|\n|  1|2015-11-22 10:30:...|  4|   10|\n|  1|2015-11-22 10:40:...|  5|   15|\n|  1|2015-11-22 10:50:...|  6|   21|\n|  1|2015-11-22 11:00:...|  7|   28|\n|  1|2015-11-22 11:10:...|  8|   36|\n|  1|2015-11-22 11:20:...|  9|   45|\n|  1|2015-11-22 11:30:...| 10|   55|\n|  1|2015-11-22 11:40:...| 11|   66|\n|  1|2015-11-22 11:50:...| 12|   78|\n|  1|2015-11-22 12:00:...| 13|   91|\n|  2|2015-11-22 10:00:...|  1|    1|\n|  2|2015-11-22 10:10:...|  2|    3|\n|  2|2015-11-22 10:20:...|  3|    6|\n|  2|2015-11-22 10:30:...|  4|   10|\n|  2|2015-11-22 10:40:...|  5|   15|\n|  2|2015-11-22 10:50:...|  6|   21|\n|  2|2015-11-22 11:00:...|  7|   28|\n+---+--------------------+---+-----+\nonly showing top 20 rows\n```\n\n\u8a08\u7b97\u3067\u304d\u3066\u3044\u307e\u3059\u306d\u3002\n\n# \u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7dcf\u548c\n\n\u4eca\u5ea6\u306f\u3001\u30ab\u30e9\u30e0a\u306e\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306b\u3001\u30ab\u30e9\u30e0c\u306e\u7dcf\u548c\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\nDataFrame\u3092groupBy()\u3067pyspark.sql.GroupedData\u306b\u3057\u3066\u3001pyspark.sql.GroupedData.sum()\u3092\u4f7f\u3044\u307e\u3059\u3002\n\u3055\u3063\u304d\u306esum()\u3068\u3084\u3084\u3053\u3057\u3044\u3051\u3069\u3001\u3053\u3061\u3089\u306fColumn\u30aa\u30d7\u30b8\u30e7\u30af\u30c8\u3092\u5f15\u6570\u306b\u6301\u305f\u3059\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u6ce8\u610f\u3057\u307e\u3059\u3002\n\n```\nIn [25]: sum_c_df = df.groupBy('a').sum('c')\n```\n\n\u307e\u305f\u3001\u5148\u307b\u3069\u3068\u9055\u3063\u3066\u3053\u308c\u306fWindow\u95a2\u6570\u3067\u306f\u306a\u3044\u306e\u3067\u3001\u8fd4\u3063\u3066\u304f\u308b\u7d50\u679c\u306fDataFrame\u3067\u3059\u3002\n\u3057\u304b\u3082\u3001\u7dcf\u548c\u3092\u683c\u7d0d\u3057\u305f\u30ab\u30e9\u30e0\u540d\u306f\u52dd\u624b\u306b\u6c7a\u307e\u308a\u307e\u3059\u3002\n\n```\nIn [26]: sum_c_df\nOut[26]: DataFrame[a: int, sum(c): bigint]\n```\n\n\u3046\u30fc\u3093\u3001\u3084\u3084\u3053\u3057\u3044\u3002\n\n\u3068\u308a\u3042\u3048\u305a\u3001\u5143\u306eDataFrame\u306b\u30ab\u30e9\u30e0\u3068\u3057\u3066\u304f\u3063\u3064\u3051\u307e\u3059\u3002\n\n```\nIn [27]: mod_df3 = mod_df2.join('a'sum_c_df, 'a'()\n\nIn [28]: mod_df3.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: timestamp (nullable = true)\n |-- c: integer (nullable = true)\n |-- cum_c: long (nullable = true)\n |-- sum(c): long (nullable = true)\n\n\nIn [29]: mod_df3.show()\n(..snip..)\n+---+--------------------+---+-------+------+\n|  a|                   b|  c|  cum_c|sum(c)|\n+---+--------------------+---+-------+------+\n|  1|2015-11-22 10:00:...|  1|      1|    91|\n|  1|2015-11-22 10:10:...|  2|      3|    91|\n|  1|2015-11-22 10:20:...|  3|      6|    91|\n|  1|2015-11-22 10:30:...|  4|     10|    91|\n|  1|2015-11-22 10:40:...|  5|     15|    91|\n|  1|2015-11-22 10:50:...|  6|     21|    91|\n|  1|2015-11-22 11:00:...|  7|     28|    91|\n|  1|2015-11-22 11:10:...|  8|     36|    91|\n|  1|2015-11-22 11:20:...|  9|     45|    91|\n|  1|2015-11-22 11:30:...| 10|     55|    91|\n|  1|2015-11-22 11:40:...| 11|     66|    91|\n|  1|2015-11-22 11:50:...| 12|     78|    91|\n|  1|2015-11-22 12:00:...| 13|     91|    91|\n|  2|2015-11-22 10:00:...|  1|      1|    91|\n|  2|2015-11-22 10:10:...|  2|      3|    91|\n|  2|2015-11-22 10:20:...|  3|      6|    91|\n|  2|2015-11-22 10:30:...|  4|     10|    91|\n|  2|2015-11-22 10:40:...|  5|     15|    91|\n|  2|2015-11-22 10:50:...|  6|     21|    91|\n|  2|2015-11-22 11:00:...|  7|     28|    91|\n+---+--------------------+---+-------+------+\nonly showing top 20 rows\n```\n\n\u3046\u307e\u304f\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7dcf\u548c\u304c\u8a08\u7b97\u3067\u304d\u3066\u3044\u307e\u3059\u306d\u3002\n\n# \u30ab\u30e9\u30e0\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e(\u7dcf\u548c - \u7d2f\u7a4d\u548c)\n\n\u3067\u306f\u3001\u30ab\u30e9\u30e0c\u306b\u3064\u3044\u3066\u7dcf\u548c\u307e\u3067\u306e\u6b8b\u308a\u5024\u3092\u8a08\u7b97\u3057\u307e\u3057\u3087\u3046\u3002\u3064\u307e\u308a\u3001\u7dcf\u548c - \u7d2f\u7a4d\u548c\u3067\u3059\u3002\n\n```\nIn [30]: diff_sum_c = mod_df3[('sum(c)'] - mod_df3['cum_c']\n\nIn [31]: mod_df4 = mod_df3.withColumn(\"diff_sum_c\", diff_sum_c)\n\nIn [34]: mod_df4.show()\n(..snip..)\n+---+--------------------+---+-------+------+----------+\n|  a|                   b|  c|cum_c_2|sum(c)|diff_sum_c|\n+---+--------------------+---+-------+------+----------+\n|  1|2015-11-22 10:00:...|  1|      1|    91|        90|\n|  1|2015-11-22 10:10:...|  2|      3|    91|        88|\n|  1|2015-11-22 10:20:...|  3|      6|    91|        85|\n|  1|2015-11-22 10:30:...|  4|     10|    91|        81|\n|  1|2015-11-22 10:40:...|  5|     15|    91|        76|\n|  1|2015-11-22 10:50:...|  6|     21|    91|        70|\n|  1|2015-11-22 11:00:...|  7|     28|    91|        63|\n|  1|2015-11-22 11:10:...|  8|     36|    91|        55|\n|  1|2015-11-22 11:20:...|  9|     45|    91|        46|\n|  1|2015-11-22 11:30:...| 10|     55|    91|        36|\n|  1|2015-11-22 11:40:...| 11|     66|    91|        25|\n|  1|2015-11-22 11:50:...| 12|     78|    91|        13|\n|  1|2015-11-22 12:00:...| 13|     91|    91|         0|\n|  2|2015-11-22 10:00:...|  1|      1|    91|        90|\n|  2|2015-11-22 10:10:...|  2|      3|    91|        88|\n|  2|2015-11-22 10:20:...|  3|      6|    91|        85|\n|  2|2015-11-22 10:30:...|  4|     10|    91|        81|\n|  2|2015-11-22 10:40:...|  5|     15|    91|        76|\n|  2|2015-11-22 10:50:...|  6|     21|    91|        70|\n|  2|2015-11-22 11:00:...|  7|     28|    91|        63|\n+---+--------------------+---+-------+------+----------+\nonly showing top 20 rows\n```\n\n# \u88dc\u8db3\n\n\u4eca\u56de\u6c17\u4ed8\u304d\u307e\u3057\u305f\u304c\u3001SPARK_CLASSPATH\u3092\u4f7f\u3046\u306e\u306fSpark 1.0\u4ee5\u4e0a\u3067\u306f\u63a8\u5968\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\u3002\npyspark\u8d77\u52d5\u6642\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u307e\u3057\u305f\u3002\n\n```\n15/11/22 12:32:44 WARN spark.SparkConf: \nSPARK_CLASSPATH was detected (set to 'postgresql-9.4-1202.jdbc41.jar').\nThis is deprecated in Spark 1.0+.\n\nPlease instead use:\n - ./spark-submit with --driver-class-path to augment the driver classpath\n - spark.executor.extraClassPath to augment the executor classpath\n  \n```\n\n\u3069\u3046\u3082\u3001\u30af\u30e9\u30b9\u30bf\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u306b\u306f\u7570\u306a\u308b\u30b5\u30fc\u30d0\u3067\u3053\u306e\u74b0\u5883\u5909\u6570\u304c\u6b63\u3057\u304f\u4f1d\u308f\u3089\u306a\u3044\u305f\u3081\u3001\u5225\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4f7f\u3046\u3053\u3068\u304c\u63a8\u5968\u3055\u308c\u3066\u3044\u308b\u3088\u3046\u3067\u3059\u3002\n\n\u3046\u3080\u3080\u3002\n\u3053\u3046\u3044\u3046\u30ed\u30fc\u30ab\u30eb\u3068\u5206\u6563\u74b0\u5883\u306e\u9055\u3044\u3001\u304d\u3061\u3093\u3068\u628a\u63e1\u3057\u3066\u3044\u304b\u306a\u3044\u3068\u306a\u3041\u3002\n", "tags": ["Spark", "DataFrame", "Python"]}